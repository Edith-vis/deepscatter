id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
fb2fb9b5884262e734b74a01d0724b6ad9c1948b	detecting musical sounds in broadcast audio based on pitch tuning analysis	instruments;musical sound detection;content based indexing;content based indexing and retrieval;audio broadcasting;tv broadcasting;signal analysis;audio signal;musical tone;speech;trecvid data set musical sound detection audio broadcasting content based indexing content based retrieval musical tone pitch tuning spectral analysis method audio signal speech;speech acoustic signal detection audio signals electronic music spectral analysis;training data;radio broadcasting;speech music tv broadcasting radio broadcasting tuning instruments frequency training data signal analysis indexing;tuning;indexing;pitch tuning;acoustic signal detection;audio signals;spectral analysis;frequency;experience base;real time application;content based retrieval;music;electronic music;trecvid data set;spectral analysis method	Detecting the presence of musical sounds in broadcast audio is important for content-based indexing and retrieval of auditory and visual information in radio and TV programs. In this paper, we propose a novel approach for musical sounds detection in broadcast audio based on the analysis of the characteristic feature of musical tones, pitch tuning. A spectral analysis method is presented for detecting the evidence of pitch tuning in the audio signal. Unlike the existing methods for discriminating speech and music, the proposed technique is not limited by inadequate training data, and it can deal with the case of music mixed with speech. In addition, the technique can be efficiently implemented for real-time application. Experiments based on TRECVID data set have shown good performance of the proposed technique	pitch correction;pitch shift;real-time clock;real-time computing;sensor;spectrum analyzer	Yongwei Zhu;Qibin Sun;Susanto Rahardja	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262502	computer vision;audio mining;speech recognition;computer science;audio signal;signal processing;multimedia	Visualization	-8.386219447154648	-92.20232061376315	20955
b85788c40177b44d09bff7e637838942fbe4b317	gammatone-domain model combination for consonant recognition in noisy environments	domain model	In this paper, a gammatone-domain model combination method is proposed for consonant recognition in noisy environments. For this task, we first define a gammatone cepstral coefficient (GCC) as the cepstral representation of the averaged envelopes of a gammatone filtered signal. Then, we investigate a proper phonetic unit by comparing monophone, diphone, and triphone acoustic models, where it is determined from consonant recognition experiments that the diphone hidden Markov models (HMMs) provide the best performance. Next, a gammatonedomain model combination method is developed to combine the clean and noise models in the linear gammatone-envelope domain. We then evaluate the performance of the GCC-based feature and the proposed model combination on intervocalic English consonants (VCV) with 24 different consonants. It is experimentally shown that the GCC-based feature achieves a relatively higher recognition rate of 47.46% than the mel-frequency cepstral coefficients (MFCCs). Also, the model combination applied to the GCC-based diphone HMM system relatively increases the accuracy rate by 77.67% under the noisy conditions.	acoustic cryptanalysis;coefficient;domain model;experiment;hidden markov model;markov chain;mel-frequency cepstrum;triphone	Jae Sam Yoon;Ji Hun Park;Hong Kook Kim	2008			artificial intelligence;speech recognition;domain model;computer science;pattern recognition;consonant	AI	-17.680668301553006	-90.08148291916349	20960
50f0d6789174927683653dbe8a18e0b59ea32ae3	tree-based estimation of speaker characteristics for speech recognition	jamforande sprakvetenskap och lingvistik;computer and information science;general language studies and linguistics;speech recognition;data och informationsvetenskap	Speaker adaptation by means of adjustment of speaker characteristic properties, such as vocal tract length, has the important advantage compared to conventional adaptation techniques that the adapted models are guaranteed to be realistic if the description of the properties are. One problem with this approach is that the search procedure to estimate them is computationally heavy. We address the problem by using a multi-dimensional, hierarchical tree of acoustic model sets. The leaf sets are created by transforming a conventionally trained model set using leaf-specific speaker profile vectors. The model sets of non-leaf nodes are formed by merging the models of their child nodes, using a computationally efficient algorithm. During recognition, a maximum likelihood criterion is followed to traverse the tree. Studies of one(VTLN) and four-dimensional speaker profile vectors (VTLN, two spectral slope parameters and model variance scaling) exhibit a reduction of the computational load to a fraction compared to that of an exhaustive grid search. In recognition experiments on children’s connected digits using adult and male models, the one-dimensional tree search performed as well as the exhaustive search. Further reduction was achieved with four dimensions. The best recognition results are 0.93% and 10.2% WER in TIDIGITS and PF-StarSw, respectively, using adult models.	acoustic cryptanalysis;acoustic model;algorithm;algorithmic efficiency;brute-force search;computation;experiment;image scaling;spectral slope;speech recognition;traverse;tract (literature);tree (data structure);tree traversal;vector graphics;word error rate	Mats Blomberg;Daniel Elenius	2009			speaker recognition;speaker diarisation;speech recognition;computer science;machine learning;pattern recognition;information and computer science;statistics	ML	-18.105071218875533	-92.5136813597368	20971
1be7c4d7d2bc1de5626dea1c5bc512161da7dad2	acoustic texttiling for story segmentation of spoken documents	audio streaming;gaussian processes;story segmentation;acoustics;speech processing;texttiling;speech;spoken document processing;glass;spoken document processing story segmentation topic segmentation segmental dynamic time warping texttiling;acoustic representations spoken document story segmentation segmental dynamic time warping lvcsr transcripts audio streams cosine based lexical similarity text blocks tdt2 mandarin corpus acoustic texttiling method lexical texttiling gaussian posteriorgrams;vectors;heuristic algorithms;segmental dynamic time warping;acoustic measurements;speech processing audio streaming gaussian processes;acoustics speech vectors heuristic algorithms acoustic measurements speech processing glass;topic segmentation	We propose an acoustic TextTiling method based on segmental dynamic time warping for automatic story segmentation of spoken documents. Different from most of the existing methods using LVCSR transcripts, this method detects story boundaries directly from audio streams. In analogy to the cosine-based lexical similarity between two text blocks in a transcript, we define the acoustic similarity measure between two pseudo-sentences in an audio stream. Experiments on TDT2 Mandarin corpus show that acoustic TextTiling can achieve comparable performance to lexical TextTiling based on LVCSR transcripts. Moreover, we use MFCCs and Gaussian posteriorgrams as the acoustic representations in our experiments. Our experiments show that Gaussian posteriorgrams are more robust to perform segmentation for the stories each with multiple speakers.	acoustic cryptanalysis;acoustic fingerprint;dynamic time warping;experiment;gaussian blur;similarity measure;speech analytics;streaming media;super robot monkey team hyperforce go!	Lilei Zheng;Cheung-Chi Leung;Lei Xie;Bin Ma;Haizhou Li	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6289073	natural language processing;speech recognition;computer science;speech;gaussian process;speech processing;mathematics;glass;statistics	Vision	-21.869665308631678	-83.23706370967726	20984
7beff52806c57a5622d79ace30e2815fbead199e	adapting asr for under-resourced languages using mismatched transcriptions	asr for under resourced languages;speech hidden markov models adaptation models error analysis training entropy buildings;asr adaptation;mismatched transcriptions;baseline multilingual models asr adaptation under resourced languages speech mismatched transcriptions english letter sequences target language;asr for under resourced languages mismatched transcriptions asr adaptation;speech recognition natural language processing	Mismatched transcriptions of speech in a target language refers to transcriptions provided by people unfamiliar with the language, using English letter sequences. In this work, we demonstrate the value of such transcriptions in building an ASR system for the target language. For different languages, we use less than an hour of mismatched transcriptions to successfully adapt baseline multilingual models built with no access to native transcriptions in the target language. The adapted models provide up to 25% relative improvement in phone error rates on an unseen evaluation set.	automated system recovery;baseline (configuration management);compiler	Chunxi Liu;Preethi Jyothi;Hao Tang;Vimal Manohar;Rose Sloan;Tyler Kekona;Mark Hasegawa-Johnson;Sanjeev Khudanpur	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472797	natural language processing;speech recognition	Robotics	-20.77551950460771	-85.47246623929234	20988
cdfed3a64f8e5cb74f52f73c8db965d50825b0f2	using typography in document image analysis	software;documento;document image analysis;logiciel;optical character recognition;character;segmentation;analyse image document;document;algorithme;tipografia;algorithm;word segmentation;typography;product cycle;reconocimento optico de caracteres;caractere;logicial;caracter;point of view;typographie;character recognition;segmentacion;reconnaissance optique caractere;algoritmo	Even if font usage plays an important role in Document Image A nalysis (DIA), recognition systems generally take the concept o f font management in a weaker sense than in the production cycle. With the point of view of the document recognition community, we show how typographic inform ation (characters bitmap, metrics, etc.) can improve existing analysis metho ds. After a brief survey of font recognition issues, we present the advantages of a fo nt software support in the design of recognition systems. Concrete algorithms are proposed in the subtopics of a posteriori font recognition, monofont Optical C haracter Recognition (OCR), and word segmentation. The reported experiments and results indicate that there are still substantial benefits to expect from the d esign of typographyaware analyzers.	algorithm;bitmap;consensus dynamics;electronic signatures in global and national commerce act;experiment;fo (complexity);heuristic (computer science);image analysis;logic analyzer;optical character recognition;text segmentation;x window system	Frédéric Bapst;Rolf Ingold	1998		10.1007/BFb0053274	text segmentation;computer vision;speech recognition;typography;computer science;artificial intelligence;product lifecycle;optical character recognition;segmentation;character	AI	-32.445208260101914	-86.61899843953525	21060
a6e0d103d430e81f363ae84ab50f6837cd3895e0	duration high-order hidden markov models and training algorithms for speech recognition		The duration high-order hidden Markov model (DHO-HMM) can capture the dynamic evolution of a physical system more precisely than can the first-order hidden Markov model (HMM). The relations among the DHO-HMM, high-order HMM (HOHMM), hidden semi-Markov model (HSMM), and HMM are presented and discussed. Recursive forward and backward probability functions for the partial observation sequence were derived, and were used to calculate the expected number of state transitions and to update the DHO-HMM’s parameters. Viterbi decoding and training algorithms for the DHO-HMM are also presented. Experimental results show that the proposed expectation-maximization (EM) training algorithm can obtain more reliable and accurate estimate of DHO-HMMs than the Viterbi training method. Experimental results also show that the DHO-HMM speech recognizer is superior to the HSMM and the baseline conventional HMM recognizers. In experiments, the DHO-HMM speech recognizer trained by the EM algorithm reduces recognition errors by up to 53% compared with the baseline HMM.	baseline (configuration management);expectation–maximization algorithm;experiment;finite-state machine;first-order predicate;hidden markov model;markov chain;recursion (computer science);semiconductor industry;speech recognition;teaching method;viterbi decoder	Lee-Min Lee	2015	J. Inf. Sci. Eng.		viterbi decoder;expectation–maximization algorithm;viterbi algorithm;computer science;hidden markov model;expected value;recursion;algorithm;artificial intelligence;machine learning;speech recognition;markov model;physical system;pattern recognition	ML	-18.366049588216946	-88.15797984109744	21172
e630172c74d628280ef59ef753234361c6a5841a	multiple time-span feature fusion for deep neural network modeling	hidden markov model hmm feature representation deep bottleneck deep hierarchical bottleneck deep neural network dnn;deep neural network dnn;deep bottleneck;hidden markov model multiple time span feature fusion deep neural network modeling long term information automatic speech recognition multiple time span information feature streams speaker adaptation transformed features deep hierarchical bottleneck features discriminative acoustic modeling syllable error rate mandarin language vietnamese language conversational telephone speech recognition dnn hmm baselines character error rate;deep hierarchical bottleneck;hidden markov model hmm;feature representation;feature extraction hidden markov models neural networks speech recognition speech training acoustics;speech recognition acoustic signal processing error statistics feature extraction hidden markov models natural language processing neural nets sensor fusion	In this paper, we exploit long term information from multiple time-spans for automatic speech recognition. The multiple time-span information is encoded into three different feature streams: speaker-adaptation-transformed features, deep bottleneck features and deep hierarchical bottleneck features. By combining three different time-spans in discriminative acoustic modeling, the character/syllable error rate improves for Mandarin and Vietnamese conversational telephone speech recognition. We obtain 0.8% and 1.9% absolute over DNN-HMM baselines in character error rate and syllable error rate for Mandarin and Vietnamese, respectively. Further analysis also suggests that our proposed feature fusion approach is able to encode finer-grain temporal information than directly using input features of long time-spans in DNN-HMM baselines.	acoustic cryptanalysis;acoustic model;artificial neural network;deep learning;encode;hidden markov model;speech recognition;super robot monkey team hyperforce go!;syllable	Chongjia Ni;Nancy F. Chen;Bin Ma	2014	The 9th International Symposium on Chinese Spoken Language Processing	10.1109/ISCSLP.2014.6936707	natural language processing;speech recognition;computer science;pattern recognition	NLP	-18.091871009599014	-87.864509006679	21227
5f3b74a5a15df39f5956490fb615af62a14e1bfa	anaphoricity in connectives: a case study on german		Anaphoric connectives are event anaphors (or abstract anaphors) that in addition convey a coherence relation holding between the antecedent and the host clause of the connective. Some of them carry an explicitly-anaphoric morpheme, others do not. We analysed the set of German connectives for this property and found that many have an additional nonconnective reading, where they serve as nominal anaphors. Furthermore, many connectives can have multiple senses, so altogether the processing of these words can involve substantial disambiguation. We study the problem for one specific German word, demzufolge, which can be taken as representative for a large group of similar words.	anaphora (linguistics);logical connective;microsoft word for mac;word-sense disambiguation	Manfred Stede;Yulia Grishina	2016		10.18653/v1/W16-0706	natural language processing;linguistics	NLP	-33.208679085267654	-81.90309835791726	21251
c8c24977f9a5bf585f58d4b1bd8df3be34796c97	a japanese natural language toolset implementation for conceptnet	mecab;conceptnet;multilingual ontology;natural language toolkit;cabocha;japanese	"""In recent years, ConceptNet has gained notoriety in the Natural Language Processing (NLP) as a textual commonsense knowledge base (CSKB) for its utilization of k-lines (Liu and Sing, 2004a) which make it suitable for making practical inferences on corpora (Liu and Sing, 2004b). However, until now, ConceptNet has lacked support for many non-English languages. To alleviate this problem, we have implemented a software toolset for the Japanese Language that allows Japanese to be used with ConceptNet's concept inference system. This paper discusses the implementation of this toolset and a possible path for the development of toolsets in other languages with similar features. Introduction ConceptNet is a freely available commonsense knowledge base (CSKB) currently in its 4th major revision (referred to as ConceptNet4). The English version contains over one million sentences mined from the Open Mind Commonsense (OMC) corpus (Liu and Sing, 2004a). From these sentences, over 300,000 elements of common sense (called """"concepts"""") were extracted (Liu and Sing, 2004b). While English is well supported at this point, support for other languages is incomplete or non-existent. After English, the next language, Traditional Chinese, contains only 64,000 concepts. Japanese is much further down the list, containing only 9,527 concepts. Natural Language Interface In order to accommodate other languages, ConceptNet is broken up into several modules, one of which is the Natural Language Interface. This interface abstracts language features away from the other modules in ConceptNet and allows arbitrary languages to be implemented without modifying other modules. To fulfill the language interface, a set of language tools was created. Language Tools Implementation The language tools implementation for Japanese uses a custom B-tree to represent utterances. The top-level node is a special utterance object, with chunk objects forming the second layer, and word objects forming all lower branches. Token objects are used as leaves. The tree is built using data from CaboCha, a Japanese Dependency Structure Analyzer, itself built upon MeCab, a Japanese Part-of-Speech / Morphological Analyzer. The tree structure is built in 5 stages: Stage 1: The input string is sanitized. First, all input is converted to UTF-8. All numeric digits are converted to full-width digits in Unicode range [0xefbc91, 0xefbc9a]. Finally, other punctuation is converted to their corresponding full-width representation. Stage 2: The input string is analyzed by CaboCha using the """"-f1 -n1"""" option string to retrieve information in lattice format. From this analysis, chunk and token information is extracted and used to build the skeleton tree structure in a bottom-up manner. Stage 3: Tokens are combined into word structures by applying operators to the tokens in each chunk. At the end of stage 3, chunk objects contain only word-derived objects as direct children. Operators: Op 1. Wrap nouns and noun suffixes into single noun objects so that compound words can be recognized. Op 2. Wrap adjectives in the nai/negative inflection form into adjective objects to prevent nai/negative from being confused as an independent auxiliary verb. 1 http://chasen.org/~taku/software/cabocha/ 2 http://mecab.sourceforge.net/ 88 Commonsense Knowledge: Papers from the AAAI Fall Symposium (FS-10-02)"""	b-tree;bottom-up parsing;chunking (computing);commonsense knowledge (artificial intelligence);inference engine;knowledge base;list of english terms of venery, by animal;mined;natural language processing;natural language user interface;open mind common sense;programming tool;sourceforge;text corpus;top-down and bottom-up design;tree structure;utf-8;unicode	Tyson Michael Roberts;Rafał Rzepka;Kenji Araki	2010			natural language processing;japanese;computer science;artificial intelligence;database;programming language	NLP	-30.442698251196354	-80.83334014521857	21469
c30c2fb4d60e78dd0ed2ac9138be9a1f800c45a4	clustering parkinson's and age-related voice impairment signal features for unsupervised learning			unsupervised learning	Alice Rueda;Sridhar Krishnan	2018	Advances in Adaptive Data Analysis	10.1142/S2424922X18400077		ML	-6.902022606084171	-83.83442286367143	21558
d498de4c62bb2504a008d3734fd6543ed8226c5f	multi-speaker localization using convolutional neural network trained with noise		The problem of multi-speaker localization is formulated as a multi-class multi-label classification problem, which is solved using a convolutional neural network (CNN) based source localization method. Utilizing the common assumption of disjoint speaker activities, we propose a novel method to train the CNN using synthesized noise signals. The proposed localization method is evaluated for two speakers and compared to a well-known steered response power method.		Soumitro Chakrabarty;Emanuel A. P. Habets	2017	CoRR		computer science;convolutional neural network;machine learning;artificial intelligence;power iteration;disjoint sets	AI	-15.612755570407947	-90.99960527452366	21563
9db480e9586d299395a58f01565bfd818008033f	training ircam's score follower [audio to musical score alignment system]	automatic control;control systems;learning algorithm;audio signal processing;music performance;hidden markov model;real time;music score follower training;filters;hmm training;music event sequence determination hidden markov model audio to musical score alignment system hmm training music score follower training observation modeling gaussian mixture models learning algorithm automatic discriminative training;observers;audio to musical score alignment system;learning systems;hidden markov models multiple signal classification real time systems music control systems learning systems automatic control humans filters curve fitting;automatic discriminative training;gaussian mixture model;multiple signal classification;hidden markov models;machine learning;learning systems audio signal processing music hidden markov models observers gaussian distribution;gaussian mixture models;observation modeling;system development;discriminative training;humans;music event sequence determination;curve fitting;music;gaussian distribution;training algorithm;real time systems	This paper describes our attempt to make the hidden Markov model (HMM) score following system, developed at Ircam, sensible to past experiences in order to obtain better audio to score real-time alignment for musical applications. A new observation modeling based on Gaussian mixture models is developed which is trainable using a learning algorithm we would call automatic discriminative training. The novelty of this system lies in the fact that this method, unlike classical methods for HMM training, is not concerned with modeling the music signal but with correctly choosing the sequence of music events that was performed. Besides obtaining better alignment, the new system's parameters are controllable in a physical manner and the training algorithm learns different styles of music performance as discussed.	algorithm;hidden markov model;loudspeaker time alignment;markov chain;mixture model;real-time locating system	Arshia Cont;Diemo Schwarz;Norbert Schnell	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1415694	speech recognition;computer science;machine learning;pattern recognition;mixture model;hidden markov model;statistics	Robotics	-18.439447706902676	-94.04917623537884	21738
34e2ed91a5d22eaf78bd1a20afd89a8ac8879b96	text-independent speaker verification using 3d convolutional neural networks		In this paper, a novel method using 3D Convolutional Neural Network (3D-CNN) architecture has been proposed for speaker verification in the text-independent setting. One of the main challenges is the creation of the speaker models. Most of the previously-reported approaches create speaker models based on averaging the extracted features from utterances of the speaker, which is known as the d-vector system. In our paper, we propose an adaptive feature learning by utilizing the 3D-CNN s for direct speaker model creation in which, for both development and enrollment phases, an identical number of spoken utterances per speaker is fed to the network for representing the speakers' utterances and creation of the speaker model. This leads to simultaneously capturing the speaker-related information and building a more robust system to cope with within-speaker variation. We demonstrate that the proposed method significantly outperforms the traditional d-vector verification system. Moreover, the proposed system can also be an alternative to the traditional d-vector system which is a one-shot speaker modeling system by utilizing 3D-CNNs.	3d modeling;consistency model;convolutional neural network;feature learning;neural networks;speaker recognition	Amirsina Torfi;Jeremy M. Dawson;Nasser M. Nasrabadi	2018	2018 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2018.8486441	artificial intelligence;convolutional neural network;machine learning;computer science;pattern recognition;feature extraction;speaker diarisation;architecture;speaker recognition;feature learning;speech recognition	Robotics	-4.549922776326237	-87.38118641941819	21764
4b0bf62339e62a5f06b4defdf34aacee11b832ce	efficient ml training of cdhmm parameters based on prior evolution, posterior intervention and feedback	engineering;hidden markov models maximum likelihood estimation feedback speech recognition training data convergence bayesian methods computer science information systems electronic mail;gaussian mixture;conference_paper;maximum likelihood;gaussian processes;parameter estimation natural languages speech recognition feedback hidden markov models maximum likelihood estimation gaussian processes;natural languages;maximum likelihood estimation;baum welch;recognition accuracy efficient ml training cdhmm parameters prior evolution posterior intervention feedback efficient maximum likelihood training procedure gaussian mixture continuous density hidden markov model parameters continuous mandarin chinese speech recognition task user cpu time baum welch algorithm;mandarin chinese;feedback;hidden markov models;speech recognition;parameter estimation;electrical engineering;continuous density hidden markov model	We present an efficient maximum likelihood (ML) training procedure for Gaussian mixture continuous density hidden Markov model (CDHMM) parameters. This procedure is proposed using the concept of approximate prior evolution, posterior intervention and feedback (PEPIF). In a series of experiments for training CDHMMs for a continuous Mandarin Chinese speech recognition task, the new PEPIF procedure achieves a 4-fold speed-up in terms of user CPU time over that of the Baum-Welch algorithm in producing models of given likelihood or recognition accuracy.	approximation algorithm;baum–welch algorithm;central processing unit;experiment;hidden markov model;markov chain;overhead (computing);speech recognition;super robot monkey team hyperforce go!;welch's method	Qiang Hue;Nathan Smith;Bin Ma	2000		10.1109/ICASSP.2000.859131	speech recognition;computer science;machine learning;pattern recognition;mathematics;maximum likelihood;hidden markov model;statistics	ML	-19.231360530635847	-92.11863868632086	21840
d64d41bd85bdd092a1c3af6f6a02949c7c2303d4	an experimental investigation of the input and error correction strategies used by subjects entering digits with the aurix speech recogniser	error correction		error detection and correction;infineon aurix	Kate S. Hone;Robert W. Series;Chris Baber	1995			speech recognition;pattern recognition;error detection and correction;artificial intelligence;computer science	HCI	-17.627197563019397	-86.21917927541118	21843
aa04ae8695167347d5c93d0d92234744e1128176	estimation of multiple pitches in stereophonic mixtures using a codebook-based approach		In this paper, a method for multi-pitch estimation of stereophonic mixtures of multiple harmonic signals is presented. The method is based on a signal model which takes the amplitude and delay panning parameters of the sources in a stereophonic mixture into account. Furthermore, the method is based on the extended invariance principle (EXIP), and a codebook of realistic amplitude vectors. For each fundamental frequency candidate in each of the sources, the amplitude estimates are mapped to entries in the codebook, and the pitch and model order are estimated jointly. The performance of the proposed method is evaluated using mixtures of real signals. Experiments show an increase in performance when knowledge about the panning parameters is utilized together with the codebook of magnitude amplitudes when compared to a state-of-the-art transcription method.	binary prefix;codebook;experiment;pitch detection algorithm;transcription (software)	Martin Weiss Hansen;Jesper Rindom Jensen;Mads Græsbøll Christensen	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952143	amplitude;magnitude (mathematics);codebook;artificial intelligence;pattern recognition;panning (camera);fundamental frequency;time–frequency analysis;harmonic analysis;harmonic;mathematics	Robotics	-11.632631743575956	-92.86659036735617	21884
1a7d7a9e9b2be4ae2749c9bf92c824d872358f46	stress assignment in spanish proper names.	confidence measure;proper names;error rate;multi layer perceptron;k nearest neighbour;memory based learning	In this paper, we propose an approach for Stress Assignment in Spanish Proper Names, based on a Multi-Layer Perceptron (MLP). When assigning stress to a word, we first analyse each vowel in the word and then calculate a Stress-Confidence Measure for it, using a MLP. The system will assign the stress to the vowel with the highest stress-confidence measure. In this paper we present and analyse different alternatives for the inputs to the Multi-Layer Perceptron. In all cases, we consider the number of vowels in the name and the vowel position in the word (taking into account only the vowels in the analysed word). For the rest of inputs, we consider a window of letters. These letters are obtained from the context of the vowel considered and from the word ending, in a similar way to [1]. We propose a Discrimination Measure to analyse the discrimination power for the different input configurations and we validate this measure and present the results obtained in each case. For the best configuration we obtain a 94.9% proper names correctly stressed (5.1% error rate). These results are compared to similar experiments using a Memory based learning approach (k-Nearest Neighbours).	bit error rate;experiment;linear discriminant analysis;memory-level parallelism;perceptron	Rubén San-Segundo-Hernández;Juan Manuel Montero-Martínez;Ricardo de Córdoba;Juana M. Gutiérrez-Arriola	2000			speech recognition;word error rate;computer science;machine learning;proper noun;pattern recognition;linguistics;multilayer perceptron	NLP	-12.333213447926957	-85.99166795685525	21936
5cf1ac41a8a026ee09b0222522e153fffc454a26	neural networks for supervised pitch tracking in noise	supervised learning pitch estimation deep neural networks recurrent neural networks viterbi decoding;viterbi decoding feedforward neural nets learning artificial intelligence probability recurrent neural nets speech coding;viterbi decoding feedforward deep neural network supervised pitch tracking dnn noise conditions pitch determination corrupted harmonic structure pitch extraction supervised learning probabilistic pitch states output noisy speech sequential frame level features recurrent deep neural network rnn temporal dynamics pitch contours;speech hidden markov models probabilistic logic viterbi algorithm signal to noise ratio training	Determination of pitch in noise is challenging because of corrupted harmonic structure. In this paper, we extract pitch using supervised learning, where probabilistic pitch states are directly learned from noisy speech. We investigate two alternative neural networks modeling the pitch states given observations. The first one is the feedforward deep neural network (DNN), which is trained on static frame-level features. The second one is the recurrent deep neural network (RNN) capable of learning the temporal dynamics trained on sequential frame-level features. Both DNNs and RNNs produce accurate probabilistic outputs of pitch states, which are then connected into pitch contours by Viterbi decoding. Our systematic evaluation shows that the proposed pitch tracking approaches are robust to different noise conditions and significantly outperform current state-of-the-art pitch tracking techniques.	deep learning;feedforward neural network;neural networks;pitch (music);pitch detection algorithm;random neural network;recurrent neural network;supervised learning;viterbi algorithm	Kun Han;DeLiang Wang	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6853845	speech recognition;machine learning;pattern recognition;time delay neural network;deep learning	ML	-17.464466444352606	-88.25729385702805	21943
085ca5ef15038cfa9fa41565659164eded77c3b4	unsupervised k-means clustering based out-of-set candidate selection for robust open-set language recognition	databases;pragmatics;acoustics;data collection;speech;feature extraction;robustness	Research in open-set language identification (LID) generally focuses more on accurate in-set language modeling versus improved out-of-set (OOS) language rejection. The main reason for this is the increased cost/resources in collecting sufficient OOS data, versus the in-set languages of interest. Therefore, unknown or OOS language rejection is a challenge. To address this through efficient data collection, we propose a flexible OOS candidate selection method for universal OOS language coverage. Since state-of-the-art i-vector system followed by generative Gaussian back-end achieves effective performance for LID, the selected K candidates are expected to be general enough to represent the entire OOS language space. Therefore, an unsupervised k-means clustering approach is proposed for effective OOS candidate selection. This method is evaluated on a dataset derived from a large-scale corpus (LRE-09) which contains 40 languages. With the proposed selection method, the total OOS training diversity can be reduced by 89% and still achieve better performance on both OOS rejection and overall classification. The proposed method also shows clear benefits for greater data enhancement. Therefore, the proposed solution achieves sustained performance with the advantage of employing a minimum number of OOS language candidates efficiently.	''the legend of zelda:;cluster analysis;k-means clustering;language identification;language model;rejection sampling;repetitive strain;unsupervised learning	Qian Zhang;John H. L. Hansen	2016	2016 IEEE Spoken Language Technology Workshop (SLT)	10.1109/SLT.2016.7846284	natural language processing;speech recognition;feature extraction;computer science;speech;machine learning;data mining;linguistics;robustness;pragmatics;data collection	NLP	-21.87506135897045	-86.9994921801234	21957
8f0f9727262359929f8b7294bd8dfc3e7520d3a5	tracking formant trajectory of tracheoesophageal speech using hidden dynamic model		In this study, a method of tracking the formant trajectory of Tracheoesophageal (TE) speech using the Hidden Dynamic Model (HDM) with dynamic target orientation was put forward based on the characteristics of TE speech. At first, a PIF-LPC algorithm was used to extract the formant parameters of TE speech. The parameters as a weighted dynamic component were then introduced in HDM; afterwards, HDM was solved using the particle filtering algorithm based on the prior distribution of this parameter to track formant trajectory. It was shown in simulation tests that the method was of high precision and good robustness owing to the orientation effect of dynamic targets. With this method, not only the interferences of spurious peaks and merged peaks in TE speech were overcome, but also continuous TE speech signals were tracked.	algorithm;intelligibility (philosophy);lpc;mathematical model;particle filter;path integral formulation;real-time computing;simulation;test engineer	Feng Xue;Gang Lv	2011	Computer and Information Science		speech recognition;computer science	ML	-13.250891131678259	-93.7566062488822	22005
404a68ff9b8e7ff7bc4a989c1565aae10d8c8e4b	cluster-based polynomial-fit histogram equalization (cpheq) for robust speech recognition	robust speech recognition;word error rate;polynomial regression;polynomial regression model;indexing terms;noise robustness;feature vector;automatic speech recognition;probability distribution;speech recognition;cumulant;data fitting;density functional;histogram equalization	Noise robustness is one of the primary challenges facing most automatic speech recognition (ASR) systems. A vast amount of research efforts on preventing the degradation of ASR performance under various noisy environments have been made during the past several years. In this paper, we consider the use of histogram equalization (HEQ) for robust ASR. In contrast to conventional methods, a novel data fitting method based on polynomial regression was presented to efficiently approximate the inverse of the cumulative density functions of speech feature vectors for HEQ. Moreover, a more elaborate attempt of using such polynomial regression models to directly characterizing the relationship between the speech feature vectors and their corresponding probability distributions, under various noise conditions, was proposed as well. All experiments were carried out on the Aurora-2 database and task. The performance of the presented methods were extensively tested and verified by comparison with the other methods. Experimental results shown that for cleancondition training, our method achieved a considerable word error rate reduction over the baseline system, and also significantly outperformed the other methods.	approximation algorithm;automated system recovery;baseline (configuration management);curve fitting;elegant degradation;experiment;feature vector;histogram equalization;polynomial;signal-to-noise ratio;speech recognition;word error rate	Shih-Hsiang Lin;Yao-Ming Yeh;Berlin Chen	2007			probability distribution;speech recognition;index term;feature vector;word error rate;computer science;histogram matching;machine learning;polynomial regression;pattern recognition;adaptive histogram equalization;histogram equalization;statistics;curve fitting;cumulant	ML	-16.3606995432638	-91.85474151163393	22030
c10bdf92f8f8ce93e8789c06cfae45d2c029f924	speech recognition and synthesis technology development at ntt for telecommunications services	technology development;speech synthesis;speaker independent;text to speech;speech recognition;context dependent	This paper describes recent developments at NTT in the areas of speech recognition, speech synthesis, and interactive voice systems as they relate to telecommunications applications. Speaker-independent largevocabulary speech recognition based on context-dependent phone models and LR parser, and high-quality text-to-speech (TTS) conversion using the waveform concatenation method, both realized as software, have enabled interactive voice systems for fast and easy prototyping of telephone-based applications. Practical applications are discussed with examples.		Kazuo Hakoda;Mikio Kitai;Shigeki Sagayama	1997	I. J. Speech Technology	10.1007/BF02208826	voice activity detection;natural language processing;speech technology;speaker recognition;speech recognition;computer science;context-dependent memory;acoustic model;speech synthesis;speech analytics	EDA	-16.097666707319203	-85.66105316128021	22045
94ee6b27a44f30ec98f4d90a4992822678b5b5e3	using morphological analysis to improve japanese pronunciation	morphological analysis			Atsuko Kikuchi;Wayne Lawrence	1994			morphological analysis;speech recognition;artificial intelligence;pattern recognition;computer science;pronunciation	NLP	-15.328571050452787	-85.71927037958362	22069
2c3ca3702b8327c2e7c861daaaeb090df6d6410e	increasing discriminative capability on map-based mapping function estimation for acoustic model adaptation	word error rate;discriminative maximum a posteriori linear regression;dmap based ensemble speaker;maplr;acoustic model adaptation;hidden markov model;acoustics;acoustic modeling;training;discriminative training automatic speech recognition mllr essem maplr map based essem;model adaptation;linear regression;speech;map based mapping function estimation;testing;essem;indexing terms;maximum likelihood estimation;supervised adaptation mode;objective function;automatic speech recognition;mllr;hidden markov models;estimation;map based essem;speech recognition;adaptation models hidden markov models acoustics testing estimation training speech;discriminative training;regression analysis;maximum a posteriori based mapping function estimation;adaptation models;word error rate map based mapping function estimation acoustic model adaptation maximum a posteriori based mapping function estimation discriminative maximum a posteriori linear regression dmap based ensemble speaker supervised adaptation mode objective function;speech recognition maximum likelihood estimation regression analysis	In this study, we propose increasing discriminative power on the maximum a posteriori (MAP)-based mapping function estimation for acoustic model adaptation. Based on the effective and stable learning advantages of MAP-based estimation, we incorporate a discriminative term and derive a new objective function. By applying the new function for online mapping function estimation, we developed discriminative maximum a posteriori (DMAP) linear regression (DMAPLR) and DMAP-based ensemble speaker and speaking environment modeling (DMAP-based ESSEM). We evaluate the DMAPLR and DMAP-based ESSEM on the Aurora-2 task in a supervised adaptation mode. The experimental results show that both DMAPLR and DMAP-based ESSEM consistently provide improvements over their ML-based and MAP-based counterparts irrespective of using one, two, or three adaptation utterances. From the improvements, we confirm the strong effect of increasing discriminative capability on the MAP-based mapping function estimation. Moreover, we verify that including multiple knowledge sources in the objective function can efficiently enhance model adaptation performance. When compared with the baseline result, DMAP-ESSEM achieves a 15.96% (9.21% to 7.74%) average word error rate (WER) reduction using only one adaptation utterance.	acoustic cryptanalysis;acoustic model;baseline (configuration management);digital media access protocol;loss function;optimization problem;web mapping;word error rate	Yu Tsao;Ryosuke Isotani;Hisashi Kawai;Satoshi Nakamura	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947559	estimation;speech recognition;index term;word error rate;computer science;linear regression;speech;machine learning;pattern recognition;software testing;maximum likelihood;hidden markov model;regression analysis;statistics	Robotics	-18.733573126134925	-91.55209276303918	22082
6a7b88c8dc37850f8ffe48dcf7d839c6f0d47873	resnet and model fusion for automatic spoofing detection		Speaker verification systems have achieved great progress in recent years. Unfortunately, they are still highly prone to different kinds of spoofing attacks such as speech synthesis, voice conversion, and fake audio recordings etc. Inspired by the success of ResNet in image recognition, we investigated the effectiveness of using ResNet for automatic spoofing detection. Experimental results on the ASVspoof2017 data set show that ResNet performs the best among all the single-model systems. Model fusion is a good way to further improve the system performance. Nevertheless, we found that if the same feature is used for different fused models, the resulting system can hardly be improved. By using different features and models, our best fused model further reduced the Equal Error Rate (EER) by 18% relatively, compared with the best single-model system.	computer vision;enhanced entity–relationship model;mathematical model;speaker recognition;speech synthesis;spoofing attack	Zhuxin Chen;Zhifeng Xie;Weibin Zhang;Xiangmin Xu	2017			speech recognition;pattern recognition;residual neural network;artificial intelligence;spoofing attack;computer science	AI	-11.021519550216935	-92.20128033753106	22097
6812b46978ff2c882a9a53b8702ef83d24128303	robust connected digit recognition in a car noise environment	decoding;speech;hidden markov models;viterbi algorithm;robustness;algorithm design and analysis;hidden markov models noise decoding speech algorithm design and analysis viterbi algorithm robustness;noise	"""This paper proposes a robust speaker-independent, connected digit recognition system for mobile applications. The system requires a small amount of ROM and low computational cost with high recognition accuracy. In addition, the system can be efficiently implemented on most currently available 32-bit fixed-point DSP chips. To reach these goals, we combined robust speech parameter processing technologies with dual MQ and VQ pairs, which supply discrete gender-dependent HMM to increase the performance of HMMs. The dual MQ/VQ pairs exploit the """"evolution"""" of the speech short-term spectral envelopes with one pair providing error compensation using LSP mean compensated coefficients. Correspondingly, we proposed the dual MQ/HMM and VQ/HMM decoding pair algorithm. In a car noise environment, the system attains an 80% average connected digit recognition accuracy at around 10 dB SNR. A digit accuracy of 93% is obtained at 5 dB SNR."""	32-bit;algorithm;algorithmic efficiency;coefficient;digital signal processor;finite-state machine;fixed-point arithmetic;hidden markov model;mobile app;read-only memory;robustness (computer science);signal-to-noise ratio;speech recognition;vector quantization	Lin Cong;Saf Asghar	2000	2000 10th European Signal Processing Conference		speech recognition;computer science;machine learning;pattern recognition	Mobile	-17.407209403527325	-89.87582333937031	22115
0582055621e60b999c7e84c0bc625fbe23ad1ca3	a new statistical excitation mapping for enhancement of throat microphone recordings		In this paper we investigate a new statistical excitation mapping technique to enhance throat-microphone speech using joint analysis of throatand acoustic-microphone recordings. In a recent study we employed source-filter decomposition to enhance spectral envelope of the throat-microphone recordings. In the source-filter decomposition framework we observed that the spectral envelope difference of the excitation signals of throatand acoustic-microphone recordings is an important source of the degradation in the throat-microphone voice quality. In this study we model spectral envelope difference of the excitation signals as a spectral tilt vector, and we propose a new phone-dependent GMM-based spectral tilt mapping scheme to enhance throat excitation signal. Experiments are performed to evaluate the proposed excitation mapping scheme in comparison with the state-of-the-art throat-microphone speech enhancement techniques using both objective and subjective evaluations. Objective evaluations are performed with the wideband perceptual evaluation of speech quality (ITU-PESQ) metric. Subjective evaluations are performed with the A/B pair comparison listening test. Both objective and subjective evaluations yield that the proposed statistical excitation mapping consistently delivers higher improvements than the statistical mapping of the spectral envelope to enhance the throat-microphone recordings.	acoustic cryptanalysis;elegant degradation;excited state;microphone;pesq;speech enhancement	M. A. Tugtekin Turan;Engin Erzin	2013			speech recognition;excitation;computer science;throat microphone	EDA	-9.792521281939145	-88.21536914385236	22248
dbc7aa90a52a7c11d80074a66f68d11e027d8500	fast unsupervised speaker adaptation through a discriminative eigen-mllr algorithm		We present a new method for unsupervised, fast speaker adaptation that combines the Eigen-MLLR transform approach with discriminative MLLR. We thereby aim to profit both from the performance improvements that are generally provided by a discriminative approach, and from the reliability that Eigen-MLLR has demonstrated in fast adaptation scenarios. We present first evaluation results on the Spoke 4 subset of the 1994 Wall Street Journal (WSJ) database. Our results show that, in fast enrollment scenarios, discriminative Eigen-MLLR allows for clear improvements both over non-discriminative Eigen-MLLR and over discriminative MLLR. We further introduce a method to estimate the weight parameters of Eigen-MLLR discriminatively, and show that this allows for further improvements on the considered data sets.	algorithm;discriminative model;eigen (c++ library);the wall street journal;unsupervised learning	Bart Bakker;Carsten Meyer;Xavier L. Aubert	2005			speech recognition;discriminative model;artificial intelligence;pattern recognition;speaker recognition;speaker diarisation;computer science	NLP	-17.43678946617204	-91.61881518714802	22264
824a373f164101ed5f31d14009cad5237c43ed62	learning style-specific rhythmic structures		The goal of computational music modeling is to construct models that capture the structure of music. We present our work on learning style-specific models for the rhythmic structure on a single line of music. The task by which the model is evaluated is to predict the duration of the next note given the sequence of durations leading to that note. We construct several different models that we train using works by a given composer (Palestrina), and assess the success of our models by looking at the prediction accuracy on unseen works by the same composer. We show that introducing style-specific musical knowledge improves the predictive ability of our models. proficient in transcription, trying to reproduce a musical score, where one musician has never listened to that specific style, and the other is an expert in that style. It is reasonable that the expert will be more accurate and faster in this task. We therefore suggest that methods for transcription of music in a particular style should be supported with a rhythm model that is specific to that style. Such models are more likely to produce accurate and fast (speed is especially important for online systems) rhythm quantifiers. An additional benefit of having a successful rhythm quantifier is that it will help in resolving uncertainty during tempo tracking, since the two problems are highly connected.	composer;dot pitch;hidden markov model;markov chain;online and offline;pitch (music);quantifier (logic);regular expression;transcription (software);video game music	Emir Kapanci;Avi Pfeffer	2003			machine learning;artificial intelligence;rhythm;computer science	ML	-18.80341103999236	-81.41179134382718	22314
72b74bdb857d3047e3a4f1048bf29b90c3454234	a detailed study of word-position effects on emotion expression in speech	emotional expression	We investigate emotional effects on articulatory-acoustic speech characteristics with respect to word location within a sentence. We examined the hypothesis that emotional effect will vary based on word position by first examining articulatory features manually extracted from Electromagnetic articulography data. Initial articulatory data analyses indicated that the emotional effects on sentence medial words are significantly stronger than on initial words. To verify that observation further, we expanded our hypothesis testing to include both acoustic and articulatory data, and a consideration of an expanded set of words from different locations. Results suggest that emotional effects are generally more significant on sentence medial words than sentence initial and final words. This finding suggests that word location needs to be considered as a factor in emotional speech processing.	acoustic cryptanalysis;medial graph;speech processing	Jangwon Kim;Sungbok Lee;Shrikanth (Shri) Narayanan	2009			speech characteristics;speech recognition;statistical hypothesis testing;emotional expression;speech processing;electromagnetic articulography;sentence;computer science	NLP	-11.37650774720463	-82.55167917919955	22335
7782a70d295164843173ad5ab327e3acc9664eae	robust voice activity detection based on pitch and sub-band energy	voice activity detection	A new Voice Activity Detection (VAD) method is proposed to track the various background noises and it can be robust in both stationary and variable noise environments. Many previous VAD methods assume that the background only contains certain kinds of noises, so they could not deal with the noise in practical applications efficiently. In proposed approach, determinate speech, determinate noise and potential speech regions are defined. The first two regions are located with extracted pitch contour information and the ambiguous region will be further retrieved using updated thresholds of sub-bands energy in obtained determinate noise’s frequency domain. Experiments are carried out with an exhaustive comparison to three standard VAD methods: G729b, ETSI AFE and AMR. The result shows that our approach has a more robust performance than others in the real circumstances.	adaptive multi-rate audio codec;ambiguous grammar;analog front-end;contour line;liu jiren;part-of-speech tagging;property (philosophy);signal-to-noise ratio;stationary process;voice activity detection	Zhihao Zhang;Jinlong Lin	2009			voice activity detection;computer science	Vision	-11.74912655831739	-90.29427109595659	22498
8503deae965d93b4c1d695a255e41050b56113cd	playing speech backwards for classification tasks	speech intelligibility;speech analysis testing user interfaces usability signal analysis performance analysis feedback;user interface speech backward playing topic classification task speech skimming;speech based user interfaces;signal classification speech intelligibility speech based user interfaces;signal classification	Literature on speech skimming reports on techniques for playing speech backwards in a way that is still intelligible to the user. However, so far there is no empirical evidence for reasonable parameter settings of the respective algorithms and few examinations have been conducted to verify the usefulness of this feature for actual tasks. We present a user study testing different ways of backward skimming in relation to topic classification. Our evaluation shows a high classification performance and suggests implications for the design of the user interface.	algorithm;statistical classification;usability testing;user interface	Wolfgang Hürst;Tobias Lauer;Cedric Bürfent	2005	2005 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2005.1521572	voice activity detection;natural language processing;speech recognition;computer science;speech processing;intelligibility;speech analytics	SE	-11.783816716725388	-85.08350913981957	22547
4be404315e4795daf405b318c74ee594166e5f34	design and training for combinational neural-logic systems	neural network cantonese speech recognition combinational neural logic system;cantonese speech recognition;logic design;neural nets;building block;electronic book combinational neural logic system design training neural logic gates genetic algorithm cantonese speech commands;neural nets combinational circuits electronic engineering computing electronic publishing genetic algorithms knowledge based systems learning artificial intelligence logic design logic gates;network performance;neural networks flip flops signal processing algorithms combinational circuits fuzzy set theory genetic algorithms speech recognition logic functions backpropagation algorithms design methodology;combinational neural logic system;electronic book;logic gates;network connectivity;speech recognition;genetic algorithm;genetic algorithms;electronic publishing;electronic engineering computing;learning artificial intelligence;logic gate;journal magazine article;knowledge based systems;neural network;combinational circuits	This paper presents the combinational neural-logic system. The basic components, i.e., the neural-logic-and, -or, and -not gates, will be proposed. As different applications have different characteristics, a traditional neural network with a common structure might not handle every application well if some network connections are redundant and cause internal disturbances, which may downgrade the training and network performance. In this paper, the proposed neural-logic gates are the basic building blocks for the applications. Based on the knowledge of the application and the neural-logic design methodology, a combinational neural-logic system can be designed systematically to incorporate the characteristics of the application into the structure of the combinational neural-logic system. It will enhance the training and network performance. The parameters of the combinational neural-logic system will be trained by the genetic algorithm. To illustrate the merits of the proposed approach, the combinational neural-logic system will be realized practically to recognize Cantonese speech commands for an electronic book	artificial neural network;combinational logic;downgrade;e-book;finite-state machine;genetic algorithm;logic gate;logic programming;network performance;speech recognition	Hak Keung Lam;F. H. Frank Leung	2007	IEEE Transactions on Industrial Electronics	10.1109/TIE.2006.885446	genetic algorithm;logic gate;computer science;artificial intelligence;machine learning;artificial neural network;algorithm	Robotics	-23.73194471392611	-88.22667919702157	22662
a5e9205f63883fec5437e310b9dabdd86669abaf	repeating pattern discovery from acoustic musical signals	music;repeating pattern discovery;self-similarity matrix;fractals;pattern classification;music indexing;pop music;acoustic musical signals;automatic repeating pattern extraction;acoustic signal processing;feature extraction;signal classification;constant q transform;audio signal processing;melody similarity;music retrieval;music summary;mel frequency cepstral coefficient;indexing;automation	Music pieces are typically repetitive. The automatic extraction of repeating patterns is useful for music summary, indexing and retrieval. An effective approach for repeating pattern discovery is proposed. In order to represent the melody similarity more accurately, a constant Q transform is used for feature extraction and a novel similarity measure between musical features is proposed. From the self-similarity matrix of the music, an adaptive method is used to extract all the significant repeating patterns. Experiments on pop music indicate the approach is promising.	acoustic cryptanalysis;feature extraction;self-similarity matrix;similarity measure	Muyuan Wang;Lie Lu;HongJiang Zhang	2004	2004 IEEE International Conference on Multimedia and Expo (ICME) (IEEE Cat. No.04TH8763)		constant q transform;speech recognition;audio signal processing;feature extraction;computer science;automation;pattern recognition;music	Visualization	-7.938272234526806	-92.13825063590424	22754
0d27e080bef065d416414be45e9c349190c84888	a comparison of normalization and training approaches for asr-dependent speaker identification		In this paper we discuss a speaker identification approach, called ASR-dependent speaker identification, that incorporates phonetic knowledge into the models for each speaker. This approach differs from traditional methods for performing textindependent speaker identification, such as global Gaussian mixture modeling, that typically ignore the phonetic content of the speech signal. We introduce a new score normalization approach, called phone adaptive normalization, which improves upon our previous speaker adaptive normalization technique. This paper also examines the use of automatically generated transcriptions during the training of our speaker models. Experiments show that speaker models trained using automatically generated transcriptions achieve the same performance as models trained using manually generated transcriptions.	automated system recovery;consistency model;database normalization;experiment;mixture model;speaker recognition	Alex Park;Timothy J. Hazen	2004			phone;normalization (statistics);speech recognition;speaker diarisation;transcription (linguistics);speaker recognition;artificial intelligence;pattern recognition;computer science	ML	-18.547017987731465	-86.04014662352226	22816
57026c11c60c48b3e4accc62361052202315e138	an evaluation of lattice scoring using a smoothed estimate of word accuracy	given word lattice;broadcast news;lattices error analysis automatic speech recognition speech recognition decoding vocabulary viterbi algorithm testing broadcasting minimization methods;word error rate;asr decoding lattice scoring confusion network;smoothed estimation;hypothesis lattice;reference lattice;large vocabulary automatic speech recognition;confusion network;objective function;automatic speech recognition;asr decoding;viterbi algorithm;word sequence;forward backward algorithm;speech recognition;error statistics;lattice scoring;word accuracy;natural language processing;word error rate lattice scoring smoothed estimation word accuracy given word lattice hypothesis lattice reference lattice large vocabulary automatic speech recognition word sequence forward backward algorithms viterbi algorithm objective function arabic broadcast news speech recognition tasks;speech recognition error statistics natural language processing;forward backward algorithms;arabic broadcast news speech recognition tasks	This paper describes a novel approach for estimating the best hypothesis of a given word lattice, the hypothesis lattice, using another word lattice, the reference lattice, and its application to large vocabulary automatic speech recognition. This approach selects the word sequence in the hypothesis lattice which maximizes a smoothed estimate of the word accuracy with respect to the reference lattice. It is shown in the paper that two algorithms similar to the Viterbi and the forward-backward algorithms can be used to estimate the hypothesis which approximately maximizes this objective function. We present in this paper two setups to test the performance of our approach. In the first setup, only one lattice is used as both the reference and the hypothesis lattices. In the second setup, two lattices produced by different systems are used to calculate the best hypothesis. In each setup, we test our approach on two Arabic broadcast news speech recognition tasks. Compared to the baseline results, up to 2.1% relative improvement in the word error rate (WER) is obtained by using our approach.	algorithm;baseline (configuration management);loss function;optimization problem;smoothing;speech recognition;vocabulary;word error rate	Mohamed K. Omar;Lidia Mangu	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.367278	natural language processing;speech recognition;viterbi algorithm;word error rate;computer science;forward–backward algorithm;pattern recognition	NLP	-21.10838049332494	-89.13505070772578	23019
8aacdeee027dd53a43e6c43195045ba88fd6f956	exemplar-based emotional voice conversion using non-negative matrix factorization	speech recognition matrix decomposition newton method speech processing;speech;hidden markov models;vectors;cepstrum;feature extraction;dictionaries;dictionaries speech feature extraction hidden markov models sparse matrices cepstrum vectors;newton set algorithms nonnegative matrix factorization emotional voice conversion technology source speech signal target speech signal;sparse matrices	This paper presents an emotional voice conversion (VC) technology using non-negative matrix factorization, where parallel exemplars are introduced to encode the source speech signal and synthesize the target speech signal. The input source spectrum is decomposed into the source spectrum exemplars and their weights. By replacing source exemplars with target exemplars, the converted spectrum and FO are constructed from the target exemplars and the target FO, which is paired with exemplars. In order to reduce the computational time, we adopted non-negative matrix factorization using active Newton set algorithms to our VC method. We carried out emotional voice conversion tasks, which convert an emotional voice into a neutral voice. The effectiveness of this method was confirmed with objective and subjective evaluations.	algorithm;algorithmic efficiency;basis function;computation;encode;fo (complexity);newton;non-negative matrix factorization;one-to-one (data model);performance;time complexity;vc dimension	Ryo Aihara;Reina Ueda;Tetsuya Takiguchi;Yasuo Ariki	2014	Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific	10.1109/APSIPA.2014.7041640	speech recognition;computer science;pattern recognition;speech processing;communication	Vision	-16.27259943669958	-92.5839455937543	23106
c54d5e8c8ef72302d1e6262742a674833fec3bf8	language model data augmentation for keyword spotting in low-resourced training conditions		This research extends our earlier work on using machine translation (MT) and word-based recurrent neural networks to augment language model training data for keyword search in conversational Cantonese speech. MT-based data augmentation is applied to two language pairs: English-Lithuanian and English-Amharic. Using filtered N-best MT hypotheses for language modeling is found to perform better than just using the 1best translation. Target language texts collected from the Web and filtered to select conversational-like data are used in several manners. In addition to using Web data for training the language model of the speech recognizer, we further investigate using this data to improve the language model and phrase table of the MT system to get better translations of the English data. Finally, generating text data with a character-based recurrent neural network is investigated. This approach allows new word forms to be produced, providing a way to reduce the out-of-vocabulary rate and thereby improve keyword spotting performance. We study how these different methods of language model data augmentation impact speech-to-text and keyword spotting performance for the Lithuanian and Amharic languages. The best results are obtained by combining all of the explored methods.	artificial neural network;convolutional neural network;finite-state machine;language model;machine translation;protologism;recurrent neural network;search algorithm;speech recognition;text corpus;text-based (computing);vocabulary;world wide web	Arseniy Gorin;Rasa Lileikyte;Guangpu Huang;Lori Lamel;Jean-Luc Gauvain;Antoine Laurent	2016		10.21437/Interspeech.2016-1200	natural language processing;speech recognition;pattern recognition	NLP	-20.27748549980572	-84.92023168520761	23158
7e47dd850006de798f5ea14ae2cf19975cb8faee	a first evaluation study of a database of kinetic facial expressions (dafex)	databases;negative affect;expressiveness;user study;quality of facial displays;emotion recognition;embodied conversational agent;facial expression;classification accuracy;kinetics;evaluation studies;emotional expression	"""In this paper we present DaFEx (Database of Facial Expressions), a database created with the purpose of providing a benchmark for the evaluation of the facial expressivity of Embodied Conversational Agents (ECAs). DaFEx consists of 1008 short videos containing emotional facial expressions of the 6 Ekman's emotions plus the neutral expression. The facial expressions were recorded by 8 professional actors (male and female) in two acting conditions (""""utterance"""" and """"no- utterance"""") and at 3 intensity levels (high, medium, low). The properties of DaFEx were studied by having 80 subjects classify the emotion expressed in the videos. High rates of accuracy were obtained for most of the emotions displayed. We also tested the effect of the intensity level, of the articulatory movements due to speech, and of the actors' and subjects' gender, on classification accuracy. The results showed that decoding accuracy decreases with the intensity of emotions; that the presence of articulatory movements negatively affects the recognition of fear, surprise and of the neutral expression, while it improves the recognition of anger; and that facial expressions seem to be recognized (slightly) better when acted by actresses than by actors."""	benchmark (computing);embodied agent	Alberto Battocchi;Fabio Pianesi;Dina Goren-Bar	2005		10.1145/1088463.1088501	facial action coding system;emotional expression;expressivity;multimedia;facial expression;kinetics;affect	AI	-7.9381113559834455	-81.25799035240291	23199
4a1e5377efeaa2124e66cdbd8e0354a0f69b35bc	emotional sentence identification in a story	classifier fusion;support vector machines;emotion identification;extreme learning machine;storytelling	In this paper, we investigate the methods of fusing different classifiers to identify emotional sentences in text. The Extreme Learning Machine (ELM) and Support Vector Machines (SVM) are two classifiers used to predict a sentence neutral or emotional. We use the UniGram, subjective words, and special punctuations, etc. as features. A method of calculating emotion value of a word is presented, and the values are employed to compose the features of an emotional sentence. To further enhance the system performance, we divide the features into three subsets, and train different models of the two classifiers on each feature set. The six models are then combined through a weighted summation fusion method and FoCal fusion method. We evaluate the system performance on a corpus of children's tales, and the experimental results demonstrate that the fusion of models can improve system performance.	n-gram;support vector machine;text corpus	Zhengchen Zhang;Shuzhi Sam Ge;Keng Peng Tee	2012		10.1145/2425296.2425318	natural language processing;support vector machine;speech recognition;computer science;machine learning;pattern recognition	NLP	-12.648265121523737	-87.60076944948595	23201
ed1ce0258852b8281d37484a5c3664f7f5cc01e3	language-specific phonetic structure and the quantisation of the spectral envelope of speech	methode section divisee;mobile radiocommunication;multi stage vector quantisation;speech synthesis;etude theorique;line spectral pair;speech processing;speech analysis;frequence;multi lingual speech processing;tratamiento palabra;traitement parole;speech coding;determination structure;radiocommunication service mobile;algorithme;split vector quantisation;algorithm;codificacion;cuantificacion vectorial;frecuencia;spectre raie;determinacion estructura;vector quantization;mobile telecommunication;structure determination;coding;estudio teorico;line spectral pair frequencies;line spectrum;sintesis palabra;speech structure;bit allocation;theoretical study;radiocomunicacion servicio movil;vector quantisation;frequency;synthese parole;codage;espectro raya;algoritmo;multistage method;quantification vectorielle	In the design of low-bit-rate (LBR) speech coding algorithms, language variability is often considered to be of secondary importance in comparison with other operational factors such as speaker variability and noise. Given that languages dier extensively in the composition of the spectral envelope and that the quantised spectral envelope of speech represents an important part of the bit allocation in speech coding, it is surprising to ®nd that no comprehensive studies have ever been carried out on the role of language in spectral quantisation. This paper addresses this through a series of performance studies of spectral quantisation carried out across a set of language families typical of global mobile telecommunications. The study considers factors of quantiser design such as the size and structure of codebooks, and the quantity of monolingual data used in codebook training. This study found that quantisation distortion is not uniform across languages. It is shown that a signi®cant dierence exists in the behaviour of spectral quantisation across languages, in particular the behaviour of high distortion outliers. Detailed analysis of the spectral distortion data on a phonetic level revealed that the nature of the distribution of spectral energy in phonemes in ̄uenced the behaviour of monolingual codebooks. Some explanations for codebook performance are presented as well as a set of recommendations for codebook design for multi-lingual environments. Ó 2000 Elsevier Science B.V. All rights reserved.	algorithm;codebook;distortion;heart rate variability;quantization (image processing);quantization (physics);quantization (signal processing);spatial variability;speech coding	John J. Parry;Ian S. Burnett;Joe F. Chicharo	2000	Speech Communication	10.1016/S0167-6393(00)00011-X	speech recognition;telecommunications;computer science;emission spectrum;frequency;speech coding;speech processing;mathematics;coding;speech synthesis;vector quantization	AI	-8.708194929747016	-89.28385545878368	23215
2879763acd7084ccd51a0cd0194feef9b70f375c	on modeling context-dependent clustered states: comparing hmm/gmm, hybrid hmm/ann and kl-hmm approaches	hidden markov models acoustics artificial neural networks speech context modeling speech recognition probabilistic logic;speech recognition gaussian processes hidden markov models mixture models neural nets;number context dependent clustered state modeling hybrid hidden markov model artificial neural network context dependent phones hmm gaussian mixture model system kullback leibler divergence based hidden markov model approach media parl database model complexity data sparsity kl hmm approach hybrid hmm ann approach hmm gmm approach;non native speech recognition hmm gmm hybrid hmm ann kullback leibler divergence based hmm context dependent subword units	Deep architectures have recently been explored in hybrid hidden Markov model/artificial neural network (HMM/ANN) framework where the ANN outputs are usually the clustered states of context-dependent phones derived from the best performing HMM/Gaussian mixture model (GMM) system. We can view a hybrid HMM/ANN system as a special case of recently proposed Kullback-Leibler divergence based hidden Markov model (KL-HMM) approach. In KL-HMM approach a probabilistic relationship between the ANN outputs and the context-dependent HMM states is modeled. In this paper, we show that in KL-HMM framework we may not require as many clustered states as the best HMM/GMM system in the ANN output layer. Our experimental results on German part of Media-Parl database show that KL-HMM system achieves better performance compared to hybrid HMM/ANN and HMM/GMM systems with much fewer number of clustered states than is required for HMM/GMM system. The reduction in number of clustered states has broader implications on model complexity and data sparsity issues.	artificial neural network;context-sensitive language;google map maker;hidden markov model;kl-one;kullback–leibler divergence;markov chain;mixture model;sparse matrix	Marzieh Razavi;Ramya Rasipuram;Mathew Magimai-Doss	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6855090	speech recognition;computer science;machine learning;pattern recognition	Robotics	-18.15721089495727	-89.97744579740433	23268
8b8c71c22d648d3128fd7589076e453bc624489d	speaker diarization with i-vectors from dnn senone posteriors			pc speaker;speaker diarisation	Gregory Sell;Daniel Garcia-Romero;Alan McCree	2015			artificial intelligence;speech recognition;pattern recognition;speaker diarisation;computer science	NLP	-14.618740859326882	-87.74931292398377	23308
4f7112656550bf78ab8e756910f97319b21ce48f	crim and lium approaches for multi-genre broadcast media transcription	training acoustics speech training data data models hidden markov models speech recognition;speech recognition natural language processing neural nets;deep neural networks;multi genre broadcast transcription;multi genre broadcast transcription deep neural networks dnn change point detection automatic transcription;crim approaches asr baseline system wer word error rates genre dependent quadgram language model pruned trigram model dnn acoustic models deep neural net multilingual training multigenre broadcast media transcription lium approaches;automatic transcription;change point detection;dnn	The Multi-Genre Broadcast Challenge at ASRU 2015 is a controlled evaluation of speech recognition, speaker diarization, and lightly supervised alignment using BBC TV recordings. CRIM and LIUM teams participated in the speech recognition part of the challenge with a joint submission. This paper presents the CRIM and LIUM's contributions. Each team made different choices to develop its ASR system. By the way, it was expected to compare and to evaluate different approaches to diarization and acoustic modeling, and to get complementary ASR systems for effective merging. CRIM's main contributions are the use of a training scenario similar to multi-lingual training to estimate the deep neural net (DNN) acoustic models with most of the data, the use of a pruned trigram model for search, in addition to the use of a genre-dependent quadgram language model for rescoring the lattice from the search. For LIUM, the focus was on fast decoding with high accuracy. The final word error rates (WER) after merging show that it is possible to get reasonable WER with automatically aligned files. The final global WER of 25.1% corresponds to a WER reduction of about 20% absolute in comparison to the ASR baseline system provided by the organizers.	acoustic cryptanalysis;acoustic model;artificial neural network;automated system recovery;baseline (configuration management);language model;microsoft word for mac;speaker diarisation;speech recognition;transcription (software);trigram;word error rate	Vishwa Gupta;Paul Deléglise;Gilles Boulianne;Yannick Estève;Sylvain Meignier;Anthony Rousseau	2015	2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)	10.1109/ASRU.2015.7404862	natural language processing;speech recognition;computer science;machine learning;change detection;statistics	NLP	-20.9881800256094	-85.89099199940958	23320
132546596bb65736d988482d62759a51190bc2de	chinese word segmentation in msr-nlp	word segmentation;named entity recognition;chinese word segmentation	Word segmentation in MSR-NLP is an integral part of a sentence analyzer which includes basic segmentation, derivational morphology, named entity recognition, new word identification, word lattice pruning and parsing. The final segmentation is produced from the leaves of parse trees. The output can be customized to meet different segmentation standards through the value combinations of a set of parameters. The system participated in four tracks of the segmentation bakeoff -PK-open, PK-close, CTB-open and CTBclosed – and ranked #1, #2, #2 and #3 respectively in those tracks. Analysis of the results shows that each component of the system contributed to the scores. 1 System Description The MSR-NLP Chinese system that participated in the current segmentation bakeoff is not a standalone word segmenter. It is a Chinese sentence analyzer where the leaves of parse trees are displayed as the output of word segmentation. The components of this system are described below. 1.1 Basic segmentation Each input sentence is first segmented into individual characters. 1 These characters and their combinations are then looked up in a dictionary and a word lattice containing lexicalized words only is formed. Each node in the lattice is a feature 1 If an input line contains more than one sentence, a sentence separator is applied to break the line into individual sentences, which are then processed one by one and the results are concatenated to form a single output. 2 The lookup is optimized so that not all possible combinations are tried. matrix that contains the part of speech and other grammatical attributes. Multiple-character words may also have information for resolving segmentation ambiguities. In general, multiple-character words are assigned higher scores than the words they subsume, but words like “才能” are exceptions and such exceptional cases are usually marked in the dictionary. For some of the words that tend to overlap with other words, there is also information as to what the preferred segmentation is. For instance, the preferred segmentation for “会议员” is “会+议员” rather than “会议+员”. Such information was collected from segmented corpora and stored in the dictionary. The scores are later used in word lattice pruning and parse ranking (Wu and Jiang 1998). 1.2 Derivational morphology and named entity recognition After basic segmentation, a set of augmented phrase structure rules are applied to the word lattice to form larger word units which include: • Words derived from morphological processes such as reduplication, affixation, compounding, merging, splitting, etc. • Named entities such as person names, place names, company names, product names, numbers, dates, monetary units, etc. Each of these units is a tree that reflects the history of rule application. They are added to the existing word lattice as single nodes and treated as single words by the parser. The internal structures are useful for various purposes, one of which is the customization of word segmentation: words with such structures can all be displayed as single words or multiple words depending on where the “cuts” are made in the word tree (Wu 2003). 1.3 New word identification The expanded word lattice built in 1.2 is inspected to detect spots of possible OOV new words. Typical spots of this kind are sequences of single charLanguage Processing, July 2003, pp. 172-175. Proceedings of the Second SIGHAN Workshop on Chinese	coding tree unit;concatenation;dictionary;entity;floor and ceiling functions;galaxy morphological classification;lookup table;mathematical morphology;named-entity recognition;natural language processing;parse tree;parsing;phrase structure rules;protologism;public-key cryptography;text corpus;text segmentation	Andi Wu	2003		10.3115/1119250.1119277	natural language processing;speech recognition;computer science;segmentation-based object categorization;pattern recognition	NLP	-26.26552585965452	-81.66397557292895	23330
2a41298643c1e32ec6bfba52912848c65ce9f1dc	auditive learning based chinese f0 prediction	databases;stress;speech synthesis;cost function;anamorphosis;speech processing;auditive learning method;natural languages;testing;speech synthesis auditive learning method fundamental frequency prediction chinese f0 prediction prosody templates syllable pattern extraction spontaneous speech prosody cost function feedback error distribution smoothing testing range testing;prosody templates;chinese f0 prediction;error analysis;feedback;smoothing methods;error distribution;prediction theory;error distribution auditive learning method chinese f0 prediction prosody templates anamorphosis spontaneous speech prosody cost function;speech synthesis humans cost function testing stress clustering methods pattern recognition error analysis smoothing methods databases;error statistics;prosody cost function;speech synthesis humans cost function testing stress clustering methods error analysis smoothing methods databases decision trees;prediction theory speech synthesis learning artificial intelligence natural languages feedback error statistics;humans;spontaneous speech;learning artificial intelligence;clustering methods;decision trees;speech processing speech synthesis natural languages;fundamental frequency	The paper describes a new F0 (fundamental frequency) model based on an auditive learning (AL) method. Being focused on the notion of prosody templates, we confirmed that F0 patterns for a syllable can be extracted from various anamorphoses of F0 contours in spontaneous speech. It is most suitable to use the F0 templates selection method for Chinese F0 prediction with prosody cost function (PCF). Furthermore, an AL method is used to adjust the weight of PCF dynamically in application. Unlike other methods, the approach may give feedback as to exactly what are the crucial parameters determining the successful choice of patterns. The paper also analyzes the error distribution of the F0 prediction results. Both smoothing testing and F0 range testing show that the synthesis results are very close to human speech.		Jianhua Tao;Xing Ni	2003		10.1109/ICME.2003.1221286	natural language processing;speech recognition;anamorphosis;computer science;machine learning;decision tree;pattern recognition;speech processing;feedback;software testing;fundamental frequency;stress;natural language;speech synthesis	NLP	-20.174953262613965	-90.2947460901833	23467
c5b7fa8cd905a55815d048385495b5e9cea72f57	range based multi microphone array fusion for speaker activity detection in small meetings.		This paper presents a method for speaker activity detection in small meetings. The activity of the participants is deduced from audio streams obtained by multiple microphone arrays. One of the novelty of the proposed approach is that it uses a human tracker that relies on scanning laser range finders to localize the participants. First, this additional information is exploited by the beamforming algorithm creating the audio streams for each of the microphone arrays. Then, at each array, the speaker activity detection is performed using Gaussian mixture models that were trained before hand. Finally, a fusion procedure, that also uses the location information, combines the detection results of the different microphone arrays. An experiment reproducing a meeting configuration demonstrates the effectiveness of the system.	algorithm;beamforming;microphone;mixture model	Jani Even;Panikos Heracleous;Carlos Toshinori Ishi;Norihiro Hagita	2011			speech recognition;pattern recognition	Robotics	-11.353503866993755	-94.3448761717527	23479
dba381666e8a1bb595053773d52717de3f439449	quantification of rewriting by the brothers grimm: a comparison of successive versions of three tales	sleeping beauty	A comparison was made of the levels and patterns of emotional tone scores in four successive versions of three stories that have been translated from German by Ellis to illustrate his argument that the Grimm Brothers made extensive revisions from the proported manuscript of the stories to their celebrated first edition versions. This objective analysis was based upon the evaluation, activity, and potency of the emotions connoted by those of the 1000 most frequent English words detected by the computer as occurring in the narratives. The stores were:The King's Daughter and The Enchanted Prince: Frog King, Sleeping Beauty, andThe Little Brother and Little Sister (Hansel and Gretel). Changes in story length, in mean levels of emotional tone, and in patterns of emotional tone across story versions support Ellis's judgement that subsequent revisions were less drastic than the first one, from the manuscript. It was also shown that the stories are quite different from each other in level and pattern of emotional tone.	rewriting	C. W. Anderson;G. E. McMaster	1989	Computers and the Humanities	10.1007/BF02176639	philosophy;computer science;sociology;literature	NLP	-11.823207323953644	-81.51809272288604	23515
58b8b426c6f576fe73cafe8c9b50c17ca0ed6e5c	efficient parameterization for automatic speaker recognition using support vector machines		Recent advances in the field of speaker recognition have proved to highly outperform algorithms. However this performance degrades when limited data are presented. This paper presents examples on how SVM can improve speaker recognition. The main contribution in this approach is the use of new low-dimensional vectors when training data are limited. We show how different kernels function of Support Vector Machines (SVM) can be used to deal a new approach for speaker recognition system. We achieved remarkable results using TIMIT database	algorithm;cepstral mean and variance normalization;experiment;feature vector;radial basis function kernel;speaker recognition;support vector machine;timit;time complexity	Rania Chakroun;Mondher Frikha;Leila Beltaïfa Zouari	2016		10.1007/978-3-319-53480-0_65	support vector machine;timit;artificial intelligence;machine learning;pattern recognition;computer science;speaker recognition;parametrization;training set	AI	-16.261461017660412	-91.6303335267491	23545
5fd0822ec97851ee3751fee3bb31ec9bca3a6a34	traffic density state estimation based on acoustic fusion	state estimation acoustic signal detection road traffic signal classification;road traffic;mfcc traffic state detection acoustic modeling fusion honks;state estimation;mel frequency cepstral coefficient abstracts indexes data collection jamming roads;audio data traffic density state estimation acoustic fusion based approach mel frequency cepstral coefficients cumulative road side signal honk event based classifier honk information based classifier mfcc probabilistic framework;signal classification;acoustic signal detection	In this paper, we propose an acoustic fusion based approach to classify the traffic density states. In particular, we combine the information from mel-frequency cepstral coefficients (MFCC) based classifier, which models the cumulative road side signal and honk event based classifier. Honk based classifier is obtained by modeling the honk statistics for each traffic class, viz., Jam, Medium and Free. We study in detail the discriminative capabilities of honk information based classifier. Decisions from MFCC and honk classifier are then combined in probabilistic framework with an appropriate fusion strategy. We also propose to use prior honk information in-order to further improve the classification results. Classification results show good performance even with 10s of audio data.	acoustic cryptanalysis;algorithm;coefficient;density matrix;jam;mel-frequency cepstrum;oracle fusion middleware;participatory sensing;software deployment;tree accumulation;viz: the computer game	Vikas Joshi;Nithya Rajamani;Naveen Prathapaneni;L. Venkata Subramaniam	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6637693	speech recognition;pattern recognition;data mining	Robotics	-14.132008661853375	-89.20581926807871	23589
72eab14606f1131019c731e337335bbf4ef12551	incorporating voice onset time to improve letter recognition accuracies	voiced voiceless distinction;linguistically motivated acoustic feature;error correcting device;statistically based speech recognizer;alphabet recognition;stops;voice onset time;spectral measures;error correction speech recognition hidden markov models feature extraction statistical analysis linguistics;spectral measure;second pass;error analysis;automatic speech recognition;standards development;speech recognition hidden markov models automatic speech recognition error analysis standards development performance analysis feature extraction measurement standards acoustic measurements error correction;hidden markov models;hmm systems;statistical analysis;standard hmm based first pass;class specific feature extraction;error correction;feature extraction;performance analysis;error rate;speech recognition;letter recognition accuracies;measurement standards;acoustic measurements;distinctive features;error rate reduction;two pass strategy;error correcting device voice onset time letter recognition accuracies statistically based speech recognizer distinctive features two pass strategy standard hmm based first pass second pass class specific feature extraction voiced voiceless distinction stops alphabet recognition linguistically motivated acoustic feature spectral measures error rate reduction hmm systems;linguistics	We consider the possibility of incorporating distinctive features into a statistically based speech recognizer. We develop a two pass strategy for recognition with a standard HMM based first pass followed by a second pass that performs an alternative analysis to extract class-specific features. For the voiced/voiceless distinction on stops for an alphabet recognition task, we show that a linguistically motivated acoustic feature exists (the VOT), provides superior separability to standard spectral measures, and can be automatically extracted from the signal to reduce error rates by 48.7 % over state of the art HMM systems.	acoustic cryptanalysis;extract class;finite-state machine;hidden markov model;linear separability;onset (audio);speech recognition	Partha Niyogi;Padma Ramesh	1998		10.1109/ICASSP.1998.674355	natural language processing;error detection and correction;speech recognition;feature extraction;word error rate;computer science;pattern recognition;voice-onset time;hidden markov model	NLP	-16.01690893019692	-91.15679927227484	23602
565115689873a0e573632d5a18beb290a450fc1b	state based imputation of missing data for robust speech recognition and speech enhancement.	noise estimation;robust speech recognition;hidden markov model;time frequency;speech enhancement;spectral subtraction;probability distribution;artificial neural net work;speech recognition;missing data;signal to noise ratio	Within the context of continuous-density HMM speech recognition in noise, we report on imputation of missing time-frequency regions using emission state probability distributions. Spectral subtraction and local signal–to– noise estimation based criteria are used to separate the present from the missing components. We consider two approaches to the problem of classification with missing data: marginalization and data imputation. A formalism for data imputation based on the probability distributions of individual Hidden Markov model states is presented. We report on recognition experiments comparing state based data imputation to marginalization in the context of connected digit recognition of speech mixed with factory noise at various global signal-to-noise ratios, and wideband restoration of speech. Potential advantages of the approach are that it can be followed by conventional techniques like cepstral features or artificial neural networks for speech recognition.	artificial neural network;cepstrum;circuit restoration;experiment;geo-imputation;hidden markov model;markov chain;missing data;semantics (computer science);signal-to-noise ratio;spectral density;speech enhancement;speech recognition	Ljubomir Josifovski;Martin Cooke;Phil D. Green;Ascension Vizinho	1999			voice activity detection;linear predictive coding;speech recognition;machine learning;speech coding;pattern recognition;acoustic model	ML	-14.992937902330782	-93.18549733893609	23801
6049aa735feba6dda67401b41e79eb6f590325e9	estimating speaking rate in spontaneous discourse	youtube speech corpus speaking rate estimation spontaneous discourse speech waveform convex optimization problem icsi switchboard spontaneous speech corpus;speech estimation interviews feature extraction signal processing algorithms correlation switches;speech processing convex programming estimation theory	In this paper we consider the problem of estimating the speaking rate directly from the speech waveform. We propose an algorithm that poses the speaking rate estimation problem as a convex optimization problem. In contrast to existing methods, we avoid the more difficult task of detecting individual syllables within the speech signal and we avoid heuristics like thresholding a loudness function. The algorithm was evaluated on the ICSI Switchboard spontaneous speech corpus and a speech corpus obtained from publicly-available interviews on Youtube.	algorithm;convex optimization;heuristic (computer science);jonathan james;mathematical optimization;optimization problem;sensor;speech corpus;spontaneous order;telephone switchboard;thresholding (image processing);waveform;whole earth 'lectronic link	Yishan Jiao;Visar Berisha;Ming Tu;Timothy Huston;Julie M. Liss	2015	2015 49th Asilomar Conference on Signals, Systems and Computers	10.1109/ACSSC.2015.7421328	voice activity detection;natural language processing;speech recognition;speech corpus;computer science;communication	ML	-16.007290004281824	-94.36384968220926	23860
ad2daff3619c566f5cf5048ce85f76e092b72c07	automatic speech recognition using hidden conditional neural fields	hidden markov models speech recognition logic gates training feature extraction speech acoustics;hidden markov model;acoustics;training;hmm;speech;continuous phoneme recognition;automatic speech recognition;speech recognition hidden markov models;hidden markov models;hcrf;logic gates;feature extraction;hidden markov model automatic speech recognition hidden conditional neural fields hcrf hcnf continuous phoneme recognition hmm;speech recognition hidden conditional neural fields hidden conditional random fields hmm;conditional random field;speech recognition;hidden conditional random fields;hidden conditional neural fields;logic gate;hcnf;neural network	Hidden Conditional Random Fields(HCRF) is a very promising approach to model speech. However, because HCRF computes the score of a hypothesis by summing up linearly weighted features, it cannot consider non-linearity among features that will be crucial for speech recognition. In this paper, we extend HCRF by incorporating gate function used in neural networks and propose a new model called Hidden Conditional Neural Fields(HCNF). Differently with conventional approaches, HCNF can be trained without any initial model and incorporate any kinds of features. Experimental results of continuous phoneme recognition on TIMIT core test set and Japanese read speach recognition task using monophone showed that HCNF was superior to HCRF and HMM trained in MPE manner.	artificial neural network;conditional random field;hidden markov model;neural oscillation;nonlinear system;speech recognition;timit;test set	Yasuhisa Fujii;Kazumasa Yamamoto;Seiichi Nakagawa	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947488	speech recognition;logic gate;computer science;machine learning;pattern recognition;hidden markov model	Robotics	-17.36682573120272	-88.72436841973777	23890
6039ccb86597c40995e81fdb25e199cf54fe56bc	instrument recognition in polyphonic music based on automatic taxonomies	sound recognition;hierarchical clustering;audio signal processing;probabilistic distances;support vector machines;music segmentation instrument recognition polyphonic music automatic taxonomies music orchestrations hierarchical clustering algorithm audio classification percussion singing voice;maquina vector soporte;audio signal processing acoustic signal processing pattern classification music source separation;acoustic signal processing;instruments multiple signal classification taxonomy music proposals source separation audio recording streaming media frequency clustering algorithms;indexing terms;classification;reconocimiento sonido;hierarchical taxonomy;support vector machines hierarchical taxonomy instrument recognition machine learning pairwise classification pairwise feature selection polyphonic music probabilistic distances;machine vecteur support;synthese signal;automatic recognition;instrument recognition;machine learning;feature extraction;reconnaissance son;pattern classification;sintesis senal;polyphonic music;signal synthesis;support vector machine;extraction caracteristique;pairwise classification;source separation;pairwise feature selection;music;clasificacion;reconocimiento automatico;reconnaissance automatique	We propose a new approach to instrument recognition in the context of real music orchestrations ranging from solos to quartets. The strength of our approach is that it does not require prior musical source separation. Thanks to a hierarchical clustering algorithm exploiting robust probabilistic distances, we obtain a taxonomy of musical ensembles which is used to efficiently classify possible combinations of instruments played simultaneously. Moreover, a wide set of acoustic features is studied including some new proposals. In particular, signal to mask ratios are found to be useful features for audio classification. This study focuses on a single music genre (i.e., jazz) but combines a variety of instruments among which are percussion and singing voice. Using a varied database of sound excerpts from commercial recordings, we show that the segmentation of music with respect to the instruments played can be achieved with an average accuracy of 53%.	acoustic cryptanalysis;algorithm;approximation;cluster analysis;comparison and contrast of classification schemes in linguistics and metadata;computation;gramian matrix;hierarchical clustering;kernel method;mir (computer);microsoft windows;source separation;statistical classification;taxonomy (general);the matrix	Slim Essid;Gaël Richard;Bertrand David	2006	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TSA.2005.860351	support vector machine;speech recognition;computer science;machine learning;pattern recognition	ML	-8.23528592440461	-92.41909100932766	23891
cab7cf13051966985c9846dafc3327c43a9d62fc	identification of the defective transmission devices using the wavelet transform	transformation ondelette;power transmission mechanical;wavelet transforms acoustic devices assembly acoustic sensors vibrometers production facilities feature extraction vector quantization manufacturing acoustic testing;ga;acoustic signal processing wavelet transforms feature extraction learning artificial intelligence vector quantisation failure analysis power transmission mechanical;extraction forme;acoustic signal processing;failure mode;algoritmo genetico;lvq;ga automatic identification feature extraction wavelet transform lvq;failure analysis;wavelet transforms;feature vector;automatic identification;reconnaissance acoustique;cuantificacion vectorial;wavelet transform;vector quantization;extraccion forma;acoustics algorithms artificial intelligence cluster analysis equipment failure analysis information storage and retrieval pattern recognition automated signal processing computer assisted sound spectrography;feature extraction;external acoustic sensors defective transmission device identification wavelet transform failure mode statistical classification feature extraction learning vector quantization;index terms automatic identification;algorithme genetique;genetic algorithm;transformacion ondita;learning artificial intelligence;vector quantisation;identification automatique;pattern extraction;wavelet transformation;learning vector quantization;quantification vectorielle	In this paper, a system is described that uses the wavelet transform to automatically identify the particular failure mode of a known defective transmission device. The problem of identifying a particular failure mode within a costly failed assembly is of benefit in practical applications. In this system, external acoustic sensors, instead of intrusive vibrometers, are used to record the acoustic data of the operating transmission device. A skilled factory worker, who is unfamiliar with statistical classification, helps to determine the feature vector of the particular failure mode in the feature extraction process. In the automatic identification part, an improved learning vector quantization (LVQ) method with normalizing the inputting feature vectors is proposed to compensate for variations in practical data. Some acoustic data, which are collected from the manufacturing site, are utilized to test the effectiveness of the described identification system. The experimental results show that this system can identify the particular failure mode of a defective transmission device and find out the causes of failure successfully.	acoustic cryptanalysis;apache axis;automatic identification and data capture;axis vertebra;computation (action);displacement mapping;failure cause;feature extraction;feature vector;genus axis;learning vector quantization;optic axis of a crystal;pattern matching;population parameter;psychologic displacement;sion's minimax theorem;statistical classification;synthetic intelligence;unbalanced circuit;wavelet transform;sensor (device)	Bingchen Wang;Sigeru Omatu;Toshiro Abe	2005	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2005.121	speech recognition;learning vector quantization;computer science;machine learning;pattern recognition;wavelet transform	Mobile	-4.831827532041864	-93.01105978895396	23906
fbc34ead744ddac9c4cd0ccca643b3311f4ec97e	assessment on impact of various types of dysarthria on acoustic parameters of speech		Dysarthria is one of the major speech impairments that makes speech unintelligible and inaudible. It is a motor speech disorder which occurs due to neurological injury to the motor-speech system and an indication of this is a poor articulation of phonemes. Dysarthria can be of different types: spastic, flaccid, hypokinetic, hyperkinetic, ataxia, mixed etc. Studying the effect of different types of dysarthria on speech, using signal processing techniques is a largely unexplored area. The fundamental frequency, formant frequency and energy of the speech signal are information bearing acoustic parameters of speech. This paper summarizes the impact of spastic, flaccid and hypokinetic types of dysarthria on the above listed acoustic parameters of speech. These parameters are compared with similar parameters for normal speech. A comparison shows that the acoustic parameters of normal speech and impaired speech are very different. By modifying these parameters of impaired speech, the researchers hope to enhance dysarthria affected speech, to make it resemble normal speech as closely as possible.	acoustic cryptanalysis	Mahesh P. Giri;Neela Rayavarapu	2018	I. J. Speech Technology	10.1007/s10772-018-9539-0	speech recognition;ataxia;computer science;formant;neurological injury;dysarthria;normal speech;spastic;motor speech disorders;impaired speech	NLP	-9.710081770733794	-83.92578292820211	23940
19880ce7c1f9775da129b55350ff58ad7e50a1ce	preliminary intelligibility tests of a monaural speech segregation system		Human listeners are able to understand speech in the presence of a noisy background. How to simulate this perceptual ability remains a great challenge. This paper describes a preliminary evaluation of intelligibility of the output of a monaural speech segregation system. The system performs speech segregation in two stages. The first stage segregates voiced speech using supervised learning of harmonic features, and the second stage segregates unvoiced speech by subtracting noise energy that is estimated from voiced intervals and onset/offset based segmentation. Objective evaluation in terms of the match to ideal binary time-frequency masks shows substantial improvements. Tests with human subjects indicate that the system improves intelligibility for young listeners when the input SNR is very low, but does not aid elderly listeners. This preliminary evaluation identifies aspects of the system that should be improved in order to produce consistent improvement in intelligibility in noisy environments.	intelligibility (philosophy);onset (audio);signal-to-noise ratio;simulation;supervised learning;variable shadowing	Ke Hu;Pierre L. Divenyi;Daniel P. W. Ellis;Zhaozhang Jin;Barbara G. Shinn-Cunningham;DeLiang Wang	2008			speech recognition;acoustics;engineering;communication;intelligibility	ML	-10.384257207780246	-84.82959019146078	24062
10cb6539804b41050594a21f36001447dbe55f0d	empirical link between hypothesis diversity and fusion performance in an ensemble of automatic speech recognition systems		Diversity is crucial to reducing the word error rate (WER) when fusing multiple automatic speech recognition (ASR) systems. We present an empirical analysis linking diversity and fusion performance. We transcribed speech from the first 2012 US Presidential debate using multiple ASR systems trained with the Kaldi toolkit. We used the N-best ROVER algorithm to perform hypothesis fusion and measured N-best diversity by the average pairwise WER. We make three key observations. We first note that the WER of the fused hypothesis decreases significantly with increasing diversity of the N-best list. This decrease is greater than the decrease in WER of the oracle hypothesis in the list. N-best lists from systems trained on different data sets are the most diverse and give the lowest WER upon fusion. We then observe that the benefit of diversity depends on the choice of the fusion scheme. We show that confidenceweighted ROVER is able to better exploit diversity than unweighted ROVER and gives lower WERs. We finally explain the above observations by a simple linear relation linking diversity to the ROVER WER. This relation depends on the fusion scheme and also reveals the tradeoff between diversity and average WER of hypotheses in the N-best list.	algorithm;kaldi;speech recognition;word error rate	Kartik Audhkhasi;Andreas M. Zavou;Panayiotis G. Georgiou;Shrikanth (Shri) Narayanan	2013			speech recognition;pattern recognition;fusion;machine learning;computer science;artificial intelligence	NLP	-17.91954338157808	-90.44253705032507	24078
520c82e192b61daab87e673eda434e9ff38fc825	utterance selection techniques for tts systems using found speech		The goal in this paper is to investigate data selection techniques for found speech. Found speech unlike clean, phoneticallybalanced datasets recorded specifically for synthesis contain a lot of noise which might not get labeled well and it might contain utterances with varying channel conditions. These channel variations and other noise distortions might sometimes be useful in terms of adding diverse data to our training set, however in other cases it might be detrimental to the system. The approach outlined in this work investigates various metrics to detect noisy data which degrade the performance of the system on a held-out test set. We assume a seed set of 100 utterances to which we then incrementally add in a fixed set of utterances and find which metrics can capture the misaligned and noisy data. We report results on three datasets, an artificially degraded set of clean speech, a single speaker database of found speech and a multi speaker database of found speech. All of our experiments are carried out on male speakers. We also show comparable results are obtained on a female multi-speaker corpus.	distortion;experiment;signal-to-noise ratio;speech synthesis;test set	Pallavi Baljekar;Alan W. Black	2016		10.21437/SSW.2016-30	speech recognition;utterance;computer science	ML	-13.211842302579047	-89.66011103487494	24234
2318007a7d70c378365a145e8e692563d4694aed	minimum divergence estimation of speaker prior in multi-session plda scoring	adaptation models nist vectors speech estimation covariance matrices training;speaker recognition estimation theory probability;minimum divergence multi session speaker verification plda scoring speaker adaptation;text independent speaker verification minimum divergence estimation multisession plda scoring probabilistic linear discriminant analysis	Probabilistic linear discriminant analysis (PLDA) has shown to be effective for modeling speaker and channel variability in the i-vector space for text-independent speaker verification. This paper shows that the PLDA scoring function could be formulated as model comparison between an adapted PLDA model and the universal PLDA. Based on this formulation, we show that a more robust adaptation could be attained by adapting the PLDA model through the use of minimum divergence estimate of speaker prior in the latent subspace. Experimental results on NIST SRE'10 and SRE'12 dataset confirm that the proposed method is effective in handling multi-session task. Notably, it is free from the covariance shrinkage problem typically found in the standard multi-session PLDA scoring.	linear discriminant analysis;model selection;scoring functions for docking;spatial variability;speaker recognition	Liping Chen;Kong-Aik Lee;Bin Ma;Wu Guo;Haizhou Li;Li-Rong Dai	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854354	speech recognition;computer science;machine learning;pattern recognition	Robotics	-17.6382809370301	-91.84723112632331	24257
b601ec8646625048918ddfb442974d15e4db3d32	l'amorçage sémantique masqué en situation de cocktail party (masked semantic priming in cocktail party situation) [in french]		________________________________________________________________________________________________________ Masked semantic priming in cocktail party situation The present study aimed at testing automatic semantic processing in the auditory modality using the cocktail party situation. Participants had to perform a lexical decision task on a target item embedded in a multi-talker babble. This babble was made of voices pronouncing words sharing semantic features with each other and with the target (priming voices). Other voices pronounced words which were semantically independent (masking voices). Results showed that the observed priming effect was significant only when the number of priming voices was strictly higher than the number of masking voices. This need of intelligibility suggests that semantic processes underlying the observed priming effect were strategic. MOTS-CLES : Amorçage sémantique masqué, situation de cocktail party, traitements sémantiques stratégiques	embedded system;intelligibility (philosophy);modality (human–computer interaction)	Marie Dekerle;Véronique Boulenger;Michel Hoen;Fanny Meunier	2012				ML	-9.434589672241204	-80.64235438485593	24362
062911604ba2c411058205fe8e348c0a4ef295fd	the influence of /s/ quality on ratings of men's sexual orientation: explicit and implicit measures of the 'gay lisp' stereotype		Two experiments examined whether listeners associate frontally normal and misarticulated /s/ with gay-sounding voices, as is suggested by the popular culture stereotype that gay men “lisp”. The first experiment showed that talkers were rated as younger-sounding and gayer-sounding when their speech included tokens with non-canonical variants of /s/ (i.e., a frontally misarticulated token of /s/, a dentalized /s/, or an /s/ produced with an especially high-frequency, compact spectrum). The second experiment showed that listeners recognize voices more quickly when they contain canonical /s/ variants than when they contain non-canonical /s/. Critically, these patterns were robust across different priming conditions in which listeners were presented with either a gay- or a heterosexual-sounding talker prior to the voice-recognition task. Together, these findings confirm experimentally that listeners make the association between non-canonical /s/ variants and male sexual orientation when asked to do so explicitly. However, though gay-sounding voices elicit longer reaction times in a voice-recognition task, we found no evidence that stereotypes about sexual orientation and /s/ production affect implicit processing of talkers' voices.	stereotype (uml)	Sara Mack;Benjamin Munson	2012	J. Phonetics	10.1016/j.wocn.2011.10.002	psychology;speech recognition;communication;social psychology	NLP	-9.151570750758857	-82.02286346211467	24470
c85157f346af097e5c95621917264c6c6ddfd6e9	modeling and analysis of acoustic musical strings using kelly-lochbaum lattice networks	time varying;physical modeling;musical strings;recurrent network;kelly lochbaum structure;plucked string instruments;physical model;simulation model;article;training algorithm;modeling and analysis	Musical strings play important roles in the timbre of a stringed instrument. An instrument installed with different strings can produce very different sounds. To model a plucked-string instrument physically, analyzing its strings may be a good starting point. In this paper, a novel strategy for the modeling and analysis of acoustic musical strings is proposed. In order to obtain the vibration of an acoustic string, a measurement device with multiple sets of electromagnetic pickups is built. A recurrent network based on the Kelly-Lochbaum structure with multiple sets of system parameters is proposed to simulate the time-varying vibrating behavior of a target string. A fast training algorithm combining BPTT and SARPROP is also proposed to train the network automatically. The well-trained recurrent network based model can be regarded as a close simulation model of a real string, and using the model, it becomes easier to understand the characteristics of a string under different conditions when it vibrates.	acoustic cryptanalysis;algorithm;backpropagation through time;experiment;kelly criterion;musical keyboard;recurrent neural network;short-time fourier transform;signal-to-noise ratio;simulation;string (computer science);whole earth 'lectronic link	Sheng-Fu Liang;Alvin Wen-Yu Su	2004	J. Inf. Sci. Eng.		simulation;speech recognition;physical model;computer science;artificial intelligence;simulation modeling	ML	-7.284529832392952	-85.32576759104887	24672
c785d4f73b606779a305ef1898a3bc2bf72b78b1	single sensor audiovisual speech source separation	kernel;speech;redundancy;lips;face;source separation;time frequency analysis	The Kernel Additive Modeling (KAM) is a recent promising framework for the separation of underdetermined convolutive mixture of audio signal. The principle of this method is to estimate the short term Power Spectral Densities (PSD) of the sources directly from the mixture by taking advantage of redundant features in the PSD of the source, such as periodicity or smoothness. The separation itself is then performed with a generalized Wiener filter. This preliminary study aims to evaluate the improvement of using the video of the speaker's face to directly detect such redundancies in the speech that could be used in the KAM framework to perform the extraction of the speech signal.	additive model;generalized wiener filter;quasiperiodicity;source separation	Pierre Narvor;Bertrand Rivet;Christian Jutten	2017	2017 Hands-free Speech Communications and Microphone Arrays (HSCMA)	10.1109/HSCMA.2017.7895583	speech recognition;computer science;pattern recognition;communication	ML	-11.727859530669333	-93.56931899178412	24702
2d867c6987195b9c5867cc36a6aa623bb70e1300	speech pre-enhancement using a discriminative microscopic intelligibility model		We propose a new approach for optimally pre-enhancing speech signals for given noise conditions. Like others, we optimise the predicted intelligibility of the signal, however, we employ a statistical ‘microscopic’ intelligibility model that encodes information about which spectro-temporal speech regions are most informative. Uniquely, our optimisation strategy aims to maximise the discrimination between the correct interpretation and competing incorrect interpretations of the utterance. We present results from studies that use speech-shaped stationary noise maskers and show the new strategy leads to solutions that are more varied than the simple high frequency emphasis employed in many pre-enhancement systems.	information;intelligibility (philosophy);mathematical optimization;stationary process	Maryam Al Dabel;Jon Barker	2014			artificial intelligence;speech recognition;discriminative model;pattern recognition;computer science;intelligibility (communication)	NLP	-11.469146889121852	-90.4606508138145	24707
16257716acf9e35a767fd8daa74584387713e12a	speaker recognition based on discriminative feature extraction - optimization of mel-cepstral features using second-order all-pass warping function	speaker recognition;second order	This paper describes a new framework for designing speaker recognition systems based on the discriminative feature extraction (DFE) method. We apply a mel-cepstral estimation technique to the feature extractor in a Gaussian mixture model (GMM)-based text-independent speaker identification system. The mel-cepstral estimation technique uses the second-order all-pass warping function for frequency transformation. We jointly optimize the frequency warping parameters of the feature extractor and the GMM parameters of the classifier based on a minimum classification error (MCE) criterion. Experimental results show that the frequency warped scale after optimization is different from traditional linear/mel scales; moreover, the proposed system outperforms conventional systems trained with the generalized probabilistic descent (GPD) method in which only the classifier is optimized.	bilinear transform;cepstrum;feature extraction;google map maker;iso/iec 11404;linuxmce;mathematical optimization;mixture model;prototype filter;randomness extractor;speaker recognition;statistical classification	Chiyomi Miyajima;Hideyuki Watanabe;Tadashi Kitamura;Shigeru Katagiri	1999			mixture model;artificial intelligence;feature (machine learning);feature extraction;discriminative model;pattern recognition;probabilistic logic;image warping;computer science;cepstrum;speaker recognition	ML	-15.235197045909093	-92.8085865518376	24749
2c35af210856f4e02512af1524c1541028ddf278	functional centering	functional information structure;grammatical role criterion;forward-looking center;free word order language;fundamental revision;unbound discourse element;empirical evaluation;discourse entity;empirical evidence	Based on empirical evidence from a free word order language (German) we propose a fundamental revision of the principles guiding the ordering of discourse entities in the forward-looking centers within the centering model. We claim that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances, i.e., the distinction between context-bound and unbound discourse elements. This claim is backed up by an empirical evaluation of functional centering.	alfa (xacml);anaphora (linguistics);backup;data-intensive computing;entity;text parser	Michael Strube;Udo Hahn	1996			linguistics;algorithm	NLP	-32.13835659575384	-82.46433357026635	24857
002edc642beafd42ac596cbe5f1d5936d52653a7	variable-activation and variable-input deep neural network for robust speech recognition	robust speech recognition;robust speech recognition deep neural network variable component variable input variable activation;variable input;deep neural network;signal to noise ratio standards vectors noise measurement speech hidden markov models filter banks;speech recognition hidden markov models matrix algebra neural nets polynomials;variable component;variable activation;aurora4 task robust speech recognition variable component deep neural network vcdnn context dependent deep neural network hidden markov model cd dnn hmm polynomial functions signal to noise ratio snr weighting matrix variable parameter dnn variable output dnn vodnn first order environment variable variable activation dnn vadnn variable input dnn vidnn hidden layer activation function	In a previous study, we proposed variable-component deep neural network (VCDNN) to improve the robustness of context-dependent deep neural network hidden Markov model (CD-DNN-HMM). We model the components of DNN a set of polynomial functions of environmental variables, more specifically signal-to-noise ratio (SNR). We refined VCDNN on two types of DNN components: (1) weighting matrix and bias (2) the output of each layer. These two methods are called variable-parameter DNN (VPDNN) and variable-output DNN (VODNN). Although both methods got good gain over the standard DNN, they doubled the number of parameters even with only the first-order environment variable. In this study, we propose two new types of VCDNN, namely variable activation DNN (VADNN) and variable input DNN (VIDNN). The environment variable is applied to the hidden layer activation function in VADNN, and is applied directly to the input in VIDNN. Both DNNs only increase a negligible number of parameters compared to the standard DNN. Experimental results on Aurora4 task show that both methods are effective, and VIDNN can beat all other variations of VCDNN with relative 7.69% word error reduction from the standard DNN with the least increase in number of parameters.	activation function;artificial neural network;concatenation;context-sensitive language;deep learning;environment variable;filter bank;first-order predicate;hidden markov model;markov chain;polynomial;signal-to-noise ratio;speech recognition;t-distributed stochastic neighbor embedding;word error rate	Rui Zhao;Jinyu Li;Yifan Gong	2014	2014 IEEE Spoken Language Technology Workshop (SLT)	10.1109/SLT.2014.7078632	speech recognition;computer science;machine learning;pattern recognition	ML	-17.385784776130432	-89.34615523032453	24882
66fd5b0322503edc0c0dff8529cf9f60c6b546a0	a maximum entropy approach to identifying sentence boundaries	maximum entropy	We present a trainable model for identifying sentence boundaries in raw text. Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of . , ?, and / as either a valid or invalid sentence boundary. The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Romanalphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains. 1 I n t r o d u c t i o n The task of identifying sentence boundaries in text has not received as much attention as it deserves. Many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g. (Brill, 1994; Collins, 1996)). Others perform the division implicitly without discussing performance (e.g. (Cutting et al., 1992)). On first glance, it may appear that using a short list of sentence-final punctuation marks, such a s . , ?, and /, is sufficient. However, these punctuation marks are not used exclusively to mark sentence breaks. For example, embedded quotations may contain any of the sentence-ending punctuation marks and . is used as a decimal point, in email addresses, to indicate ellipsis and in abbreviations. Both / and ? are somewhat less ambiguous * The authors would like to aclmowledge the support of ARPA grant N66001-94-C-6043, ARO grant DAAH0494-G-0426 and NSF grant SBR89-20230. but appear in proper names and may be used multiple times for emphasis to mark a single sentence boundary. Lexically-based rules could be written and exception lists used to disambiguate the difficult cases described above. However, the lists will never be exhaustive, and multiple rules may interact badly since punctuation marks exhibit absorption properties. Sites which logically should be marked with multiple punctuation marks will often only have one ((Nunberg, 1990) as summarized in (White, 1995)). For example, a sentence-ending abbreviation will most likely not be followed by an additional period if the abbreviation already contains one (e.g. note that D. C is followed by only a single . in The president lives in Washington, D.C.). As a result, we believe that manually writing rules is not a good approach. Instead, we present a solution based on a maximum entropy model which requires a few hints about what information to use and a corpus annotated with sentence boundaries. The model trains easily and performs comparably to systems that require vastly more information. Training on 39441 sentences takes 18 minutes on a Sun Ultra Sparc and disambiguating the boundaries in a single Wall Street Journal article requires only 1.4 seconds. 2 P r e v i o u s W o r k To our knowledge, there have been few papers about identifying sentence boundaries. The most recent work will be described in (Pa.lmer and Hearst, To appear). There is also a less detailed description of Pahner and Hearst's system, SATZ, in (Pahuer and Hearst, 1994). 1 The SATZ architecture uses either a decision tree or a neural network to disambiguate sentence boundaries. The neural network achieves 98.5% accuracy on a corpus of Wall Str'eet Journal t~Ve recommend these articles for a more comprehensive review of sentence-boundary identification work than we will be able to provide here.	artificial neural network;cutting stock problem;decimal mark;decision tree;domain-specific language;email;embedded system;ibm notes;lexicon;natural language processing;part-of-speech tagging;principle of maximum entropy;sparc;the wall street journal	Jeffrey C. Reynar;Adwait Ratnaparkhi	1997			natural language processing;speech recognition;computer science;principle of maximum entropy;pattern recognition	NLP	-26.185698722616902	-80.21364803767386	24921
1d96679d345c726a19125243f8c88ef1ce1b27f6	towards automatic identification of singing language in popular music recordings	language model;spectrum;statistical model	The automatic analysis of singing from music is an important and challenging issue within the research target of content-based retrieval of music information. As part of this research target, this study presents a first attempt to automatically identify the language sung in a music recording. It is assumed that each language has its own set of constraints that specify which of the basic linguistic events present in a singing process are allowed to follow another. The acoustic structure of individual languages may, thus, be characterized by statistically modeling those constraints. To this end, the proposed method employs vector clustering to convert a singing signal from its spectrum-based feature representation into a sequence of smaller basic phonological units. The dynamic characteristics of the sequence are then analyzed by using bigram language models. Since the vector clustering is performed in an unsupervised manner, the resulting system does not use sophisticated linguistic knowledge and, thus, is easily portable to new language sets. In addition, to eliminate the interference of background music, we leverage the statistical estimation of a piece’s music background so that the vector clustering is relevant to the solo singing voices in the accompanied signals.	acoustic cryptanalysis;automatic identification and data capture;bigram;cluster analysis;estimation theory;interference (communication);language model;structure of observed learning outcome;unsupervised learning	Wei-Ho Tsai;Hsin-Min Wang	2004			speech recognition;natural language processing;language model;cluster analysis;sound recording and reproduction;artificial intelligence;computer science;statistical model;constructed language;popular music;bigram;singing	NLP	-17.101793513254652	-84.15919259962732	24943
70d7ef86d9a946763a56b65af2241339af68493e	joint variable frame rate and length analysis for speech recognition under adverse conditions	noise robust speech recognition;frame selection;variable frame length;variable frame rate	This paper presents a method that combines variable frame length and rate analysis for speech recognition in noisy environments, together with an investigation of the effect of different frame lengths on speech recognition performance. The method adopts frame selection using an a posteriori signal-to-noise (SNR) ratio weighted energy distance and increases the length of the selected frames, according to the number of non-selected preceding frames. It assigns a higher frame rate and a normal frame length to a rapidly changing and high SNR region of a speech signal, and a lower frame rate and an increased frame length to a steady or low SNR region. The speech recognition results show that the proposed variable frame rate and length method outperforms fixed frame rate and length analysis, as well as standalone variable frame rate analysis in terms of noise-robustness. 2014 Elsevier Ltd. All rights reserved.	experiment;maximal set;signal-to-noise ratio;speaker recognition;speech recognition;variable frame rate	Zheng-Hua Tan;Ivan Kraljevski	2014	Computers & Electrical Engineering	10.1016/j.compeleceng.2014.09.002	inter frame;residual frame;speech recognition;telecommunications;computer science;engineering drawing	AI	-11.564955655567813	-89.14662106231762	24973
779f491908ebcc0452193fa46a8f91f820a9acca	acoustic head gesture recognition and its applications			acoustic cryptanalysis;gesture recognition	Akira Sasou;Yasuharu Hashimoto;Katsuhiko Sakaue	2010			computer vision;gesture recognition;artificial intelligence;computer science	Vision	-6.917511891460102	-84.76325708415513	25008
7603916e6393273093892f3cd29a758d6e4e0073	singing-voice separation from monaural recordings using deep recurrent neural networks		Monaural source separation is important for many real world applications. It is challenging since only single channel information is available. In this paper, we explore using deep recurrent neural networks for singing voice separation from monaural recordings in a supervised setting. Deep recurrent neural networks with different temporal connections are explored. We propose jointly optimizing the networks for multiple source signals by including the separation step as a nonlinear operation in the last layer. Different discriminative training objectives are further explored to enhance the source to interference ratio. Our proposed system achieves the state-of-the-art performance, 2.30⇠2.48 dB GNSDR gain and 4.32⇠5.42 dB GSIR gain compared to previous models, on the MIR-1K dataset.	artificial neural network;discriminative model;interference (communication);neural networks;nonlinear system;recurrent neural network;source separation	Po-Sen Huang;Minje Kim;Mark Hasegawa-Johnson;Paris Smaragdis	2014			speech recognition;artificial intelligence;machine learning;computer science;recurrent neural network;singing;monaural	ML	-15.249398877266179	-90.43251355601808	25012
02011dd3694990b6f718de98cc1a40a920575713	gtt: a general transducer for teaching computational linguistics	theoretical linguistics;general transducer;rule formalism;major emphasis;linguistic data structure;specialized production system;teaching purpose;machine translation;computational linguistics;control facility;tree-to-tree transducer;production system;data structure	The GTI~syst~m is a tree-to-tree transducer developed for teaching purposes in machine translation. The transducer is a specialized production system giving the linguists the tools for expressing infon~ation in a syntax that is close to theoretical linguistics. Major emphasis was placed on developing a system that is user friendly, uniform and legible. This paper describes the linguistic data structure, the rule formalism and the control facilities that the linguist is provided with.	computation;computational linguistics;data structure;graphics address remapping table;grid-tie inverter;machine translation;production system (computer science);semantics (computer science);transducer;usability	P. Shane;J. L. Cochard	1984			computational linguistics;theoretical computer science;computer science;transducer	NLP	-30.80066648945478	-80.88089786590453	25104
3eedfb5f51855b7769055f8bb5cd9e82076820b5	interfacing acoustic models with natural language processing systems	natural language processing;speech recognition	The research presented here focuses on implementation and efficiency issues associated with the use of word graphs for interfacing acoustic speech recognition systems with natural language processing systems. The effectiveness of various pruning methods for graph construction is examined, as well as techniques for word graph compression. In addition, the word graph representation is compared to another predominant interface method, the N-best sentence list.	acoustic cryptanalysis;acoustic model;graph (abstract data type);graph theory;natural language processing;speech recognition	Michael T. Johnson;Mary P. Harper;Leah H. Jamieson	1998			graph (abstract data type);speech recognition;interfacing;natural language processing;speech processing;computer science;sentence;graph;artificial intelligence	NLP	-20.176174373760315	-83.905379028435	25183
1abc840438f167c706ea89403d69d867566d88dd	evaluating speech samples designed for the voice profile analysis scheme for brazilian portuguese (bp-vpas)		The present study aims at evaluating a corpus designed for voice quality settings analysis (Laver, 1980). It addresses the role of keyspeech segments for the identification of voice quality settings. The corpus, recorded by 60 speakers, contained repetitions of 10 keyspeech sentences, and semi-spontaneous speech samples. These samples were evaluated by two expert subjects according to the BP-VPAS Profile (Camargo, Madureira, 2008). These data were analyzed by means of an artificial neural network and statistically by multiple regression analysis. The data reinforce the importance of the description of voice quality based on the phonetic model and the relevant role of keyspeech segments.	artificial neural network;semiconductor industry;spontaneous order;text corpus	Zuleica Antonia de Camargo;Sandra Madureira;Luiz Carlos Rusilo	2011			brazilian portuguese;speech recognition;computer science	NLP	-17.327594990048443	-83.07420348070679	25365
ff4d91ec2431dbebe69486b67a1b925d73b97d51	voice activity detection and speaker localization using audiovisual cues	speaker localization;multimodal analysis;hidden markov models;voice activity detection;user interfaces	This paper proposes a multimodal approach to distinguish silence from speech situations, and to identify the location of the active speaker in the latter case. In our approach, a video camera is used to track the faces of the participants, and a microphone array is used to estimate the Sound Source Location (SSL) using the Steered Response Power with the phase transform (SRP-PHAT) method. The audiovisual cues are combined, and two competing Hidden Markov Models (HMMs) are used to detect silence or the presence of a person speaking. If speech is detected, the corresponding HMM also provides the spatio-temporally coherent location of the speaker. Experimental results show that incorporating the HMM improves the results over the unimodal SRP-PHAT, and the inclusion of video cues provides even further improveEmail addresses: danteab@gmail.com (Dante A. Blauth), vminotto@gmail.com (Vicente P. Minotto), crjung@inf.ufrgs.br (Claudio R. Jung), bowon.lee@hp.com (Bowon Lee), ton.kalker@huawei.com (Ton Kalker) Preprint submitted to Pattern Recognition Letters September 6, 2011 ments.	algorithm;cache coherence;coherence (physics);facial motion capture;graphics processing unit;hidden markov model;markov chain;microphone;multimodal interaction;pattern recognition letters;powered speakers;run time (program lifecycle phase);tls-srp;voice activity detection	Dante A. Blauth;Vicente P. Minotto;Cláudio Rosito Jung;Bowon Lee;Ton Kalker	2012	Pattern Recognition Letters	10.1016/j.patrec.2011.09.002	voice activity detection;speaker recognition;speaker diarisation;speech recognition;computer science;user interface;hidden markov model	Vision	-11.426742260817397	-94.14971581060782	25581
7a0d2307031e52996c52b9bfd4e933be2783f5c4	radiation of musical instruments and improvement of the sound diffusion techniques for synthesized, recorded, or amplified sounds (revisited)		This project is articulated around three principal research axes: a) the collection of radiation data of musical instruments from measurements or from the simulation of their physical functioning; b) development of signal processing methods in order to control the directivity of an array of loudspeakers; c) evaluation of the perceptual cues linked with the variation of the radiation characteristics. In this paper we review some of the developments of this project and new results obtained during the last three years (Causse et al., ISMA, 1992; http://mediatheque.ircam.fr/articles/textes/Causse92/)		René Caussé;Philippe Dérogis	1995			musical;directivity;loudspeaker;perception;signal processing;radiation;acoustics;speech recognition;computer science	EDA	-8.50666992711676	-85.18016796378538	25709
b256e90ecbc84cb3e32e7e3e3c0bf3360689b46e	shared latent subspace modelling within gaussian-binary restricted boltzmann machines for nist i-vector challenge 2014.		This paper presents a novel approach to speaker subspace mod elling based on Gaussian-Binary Restricted Boltzmann Machines (GRBM). The proposed model is based on the idea of shared factors as in the Probabilistic Linear Discriminant A alysis (PLDA). GRBM hidden layer is divided into speaker and channel factors, herein the speaker factor is shared over al l vectors of the speaker. Then Maximum Likelihood Parameter Esti mation (MLE) for proposed model is introduced. Various new scoring techniques for speaker verification using GRBM are proposed. The results for NIST i-vector Challenge 2014 data set are presented.	linear discriminant analysis;speaker recognition	Danila Doroshin;Alexander Yamshinin;Nikolay Lubimov;Marina Nastasenko;Mikhail Kotov;Maxim Tkachenko	2015	CoRR		machine learning;nist;artificial intelligence;probabilistic logic;gaussian;linear discriminant analysis;subspace topology;boltzmann machine;binary number;estimation theory;computer science;pattern recognition	AI	-16.78869871817463	-92.0382141686177	25746
529d2f6adedf827bcc2d14a1a2f6410a5e7e0ef0	can we observe frequency-modulated syllable effects in french dyslexic children?			modulation;syllable	Norbert Maïonchi-Pino;Jean Écalle;Annie Magnan	2013			psychology;cognitive psychology;syllable	HCI	-8.310091451994166	-83.33903271974734	25813
29d7c66af683783a4971a581bf085b1536d58276	learning to segment songs with ordinal linear discriminant analysis	timbre feature extraction clustering algorithms accuracy speech;latent structural repetition feature ordinal linear discriminant analysis supervised learning algorithm feature representation temporally constrained clustering music segmentation;learning music automatic segmentation;time series acoustic signal processing learning artificial intelligence music	This paper describes a supervised learning algorithm which optimizes a feature representation for temporally constrained clustering. The proposed method is applied to music segmentation, in which a song is partitioned into functional or locally homogeneous segments (e.g., verse or chorus). To facilitate abstraction over multiple training examples, we develop a latent structural repetition feature, which summarizes the repetitive structure of a song of any length in a fixed-dimensional representation. Experimental results demonstrate that the proposed method efficiently integrates heterogeneous features, and improves segmentation accuracy.	algorithm;chorusos;cluster analysis;constrained clustering;linear discriminant analysis;ordinal data;supervised learning;verse protocol	Brian McFee;Daniel P. W. Ellis	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854594	feature learning;speech recognition;computer science;machine learning;pattern recognition;supervised learning	Robotics	-5.193316997799544	-88.26001915396644	26001
e8fceab595f4836c9622b6a3ee724700caa93873	speech-to-text conversion in french	speech to text conversion;phone recognition;continuous speech recognition;vocabulary independent recognition;hidden markov models;speech to text;language identification;speaker independent recognition	Speech-to-text conversion of French necessitates that both the acoustic level recognition and language modeling be tailored to the French language. Work in this area was initiated at LIMSI over 10 years ago. In this paper a summary of the ongoing research in this direction is presented. Included are studies on distributional properties of French text materials; problems speciic to speech-to-text conversion particular to French; studies in phoneme-to-grapheme conversion, for continuous, error-free phonemic strings; past work on isolated-word speech-to-text conversion; and more recent work on continuous-speech speech-to-text conversion. Also demonstrated is the use of phone recognition for both language and speaker identiication. The continuous speech-to-text conversion for French is based on a speaker-independent, vocabulary-independent recognizer. In this paper phone recognition and word recognition results are reported evaluating this recognizer on read speech taken from the BREF corpus. The recognizer was trained on over 4 hours of speech from 57 speakers, and tested on sentences from an independent set of 19 speakers. A phone accuracy of 78.7% was obtained using a set of 35 phones. The word accuracy was 88% for a 1139 word lexicon and 86% for a 2716 word lexicon, with a word pair grammar with respective perplexities of 100 and 160. Using a bigram grammar word accuracies of 85.5% and 81.7% were obtained with 5K and 20K word vocabularies, with respective perplexities of 122 and 205.	acoustic cryptanalysis;bigram;finite-state machine;independent set (graph theory);language model;lexicon;perplexity;speech recognition;vocabulary	Jean-Luc Gauvain;Lori Lamel;Gilles Adda;Joseph-Jean Mariani	1994	IJPRAI	10.1142/S021800149400005X	natural language processing;language identification;speaker recognition;speech recognition;word error rate;computer science;hidden markov model	NLP	-21.376836300281862	-84.83940675553974	26026
725f8182eac9bd6f9fa428ba90f43cbce891ac78	improved keyword spotting based on keyword/garbage models	memory management;decoding;speech;training data;computational modeling;hidden markov models;data models	We propose two simple methods to improve the performance of a keyword spotting system. In our application, the users are allowed to change the keywords anytime if they want. Thus we focused on phone-based GMM-HMM models since they do not require keyword-specific training data. However, the GMM-HMM based models usually have very high false alarm rate, i.e., a keyword is not present but the system gives a positive decision. We found that we can utilize the uncertainty of the system when a non-keyword is presented. Two simple methods are proposed to incorporate the uncertainty into the confidence measure. Our experiments show that these two methods can substantially reduce the false alarm rate from 75.05% to 5.71%. Meanwhile, the false reject rate increases from 1.04% to 5.71%.	anytime algorithm;data science;experiment;google map maker;hidden markov model	Qiyu Chen;Weibin Zhang;Xiangmin Xu;Xiaofen Xing	2016	2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)	10.1109/APSIPA.2016.7820743	speech recognition;computer science;machine learning;pattern recognition	Robotics	-20.875348009896115	-88.24478760449158	26064
1cbbbf53f651c8965e528f237a7f7f1c5ae81bd3	a multifaceted approach to music similarity	music similarity;multidimensional scaling;subjective evaluation	Previous work has explored the concept of music similarity measures and a variety of methods have been proposed for calculating such measures. This paper describes a system for music similarity which attempts to model and compare some of the more musically salient features of a set of audio signals. A model for timbre and a model for rhythm are implemented directly from previous work, and a model for song structure is developed. The different models are weighted and combined to provide an overall music similarity measure. The system is tested on a small set of popular music files spanning eleven different genres. The system is tuned to estimate genre boundaries using multidimensional scaling – a technique that allows for quick visualization of similarity data. An “automatic DJ” application, that generates playlists based on the music similarity models, serves as a subjective evaluation for the system.	file spanning;image scaling;multidimensional scaling;similarity measure	Kurt Jacobson	2006			speech recognition;multidimensional scaling;computer science;machine learning;mathematics;multimedia;similarity heuristic	Web+IR	-7.329045154227674	-92.01476761787106	26074
689655056c8afbc8e600ec537e696cda1e0b2a15	i-vectors meet imitators: on vulnerability of speaker verification systems against voice mimicry		Voice imitation is mimicry of another speaker’s voice characteristics and speech behavior. Professional voice mimicry can create entertaining, yet realistic sounding target speaker renditions. As mimicry tends to exaggerate prosodic, idiosyncratic and lexical behavior, it is unclear how modern spectral-feature automatic speaker verification systems respond to mimicry “attacks”. We study the vulnerability of two well-known speaker recognition systems, traditional Gaussian mixture model – universal background model (GMM-UBM) and a state-of-the-art i-vector classifier with cosine scoring. The material consists of one professional Finnish imitator impersonating five wellknown Finnish public figures. In a carefully controlled setting, mimicry attack does slightly increase the false acceptance rate for the i-vector system, but generally this is not alarmingly large in comparison to voice conversion or playback attacks.	automatic sounding;google map maker;mixture model;speaker recognition;speech synthesis	Rosa González Hautamäki;Tomi Kinnunen;Ville Hautamäki;Timo Leino;Anne-Maria Laukkanen	2013			mimicry;speech recognition;imitation;speaker recognition;pattern recognition;artificial intelligence;classifier (linguistics);voice analysis;computer science;vulnerability	ECom	-11.393146815118929	-87.04857368394089	26132
09552537ca52db598441c18d87b3900c29523b55	towards robust indonesian speech recognition with spontaneous-speech adapted acoustic models		This paper presents our work in building an Indonesian speech recognizer to handle both spontaneous and dictated speech. The recognizer is based on the Gaussian Mixture and Hidden Markov Models (GMM-HMM). The model is first trained on 73 hours of dictated speech and 43.5 minutes of spontaneous speech. The dictated speech is read from prepared transcripts by a diverse group of 244 Indonesian speakers. The spontaneous speech is manually labelled from recordings of an Indonesian parliamentary meeting, and is interspersed with noises and fillers. The resulting triphone model is then adapted only to the spontaneous speech using the Maximum A-posteriori Probability (MAP) method. We evaluate the adapted model using separate dictated and spontaneous evaluation sets. The dictated set consists of speech from 20 speakers totaling 14.5 hours. The spontaneous set is derived from the recording of a regional government meeting, consisting of 1085 utterances totaling 48.5 minutes. Evaluation of a MAP-adapted spontaneous set yields a 2.60% absolute increase in Word Accuracy Rate (WAR) over the un-adapted model, outperforming MMI adaptation. Conversely, MMI adaption of the dictated set outperforms the MAP adaptation by achieving an absolute increase of 1.48% in WAR over the un-adapted model. We also demonstrate that fMLLR speaker adaptation is unsuitable for our task due to limited adaptation data. © 2016 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the Organizing Committee of SLTU 2016.	acoustic cryptanalysis;acoustic model;cross-validation (statistics);fmllr;finite-state machine;general-purpose modeling;google map maker;hidden markov model;interpolation;language model;machine translation;markov chain;microsoft word for mac;pc speaker;perplexity;plasma cleaning;pro tools;speech corpus;speech recognition;spontaneous order;transcriber;triphone	Devin Hoesen;Cil Hardianto Satriawan;Dessi Puji Lestari;Masayu Leylia Khodra	2016		10.1016/j.procs.2016.04.045	natural language processing;speech recognition	NLP	-21.002963283013724	-85.00887638517729	26152
855f0705a3668891479548825f2c0456ee801908	an adaptive method for cross-recording speaker diarization		Nowadays, state-of-the-art speaker diarization systems heavily rely on between-recording variability compensation methods to accurately process large collections of recordings. Variability estimation is performed on consequent training datasets, which must be labeled by speaker. One major problem of such systems is the acoustic mismatch between training and target data that degrades performances. Most of the collections contain lots of speakers speaking in various acoustic conditions. In this paper, we investigate how unlabeled speakers can help improve between-recording variability estimation, to overcome the mismatch issue. We propose a scalable unsupervised adaptation framework for two types of variability compensation. The proposed framework consists in adapting a state-of-the-art diarization and linking system, trained on out-of-domain data, using the data of the collection itself. Experiments in mismatch condition are run on two French Television shows, while the initial training dataset is composed of Radio recordings. Results indicate that the proposed adaptation framework reduces the cross-recording DER of 13% in average for variable collection sizes.	acoustic cryptanalysis;heart rate variability;performance;scalability;spatial variability;speaker diarisation	Ga&#x00EB;l Le Lan;Delphine Charlet;Anthony Larcher;Sylvain Meignier	2018	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2018.2844025	speech recognition;task analysis;compensation methods;domain adaptation;scalability;pattern recognition;artificial intelligence;computer science;speaker diarisation;hidden markov model	NLP	-15.410666487263883	-89.6782359904345	26218
2705193d03e56f85c51c7313e4f1b5ba1b0f2005	evaluation of hmm-based visual laughter synthesis	hidden markov models visualization videos databases face speech pipelines;hmm audio visual laughter synthesis;speaker dependent training hmm based visual laughter synthesis evaluation hidden markov model 3d avatar audio visual laughter synthesis extrapolation;speech synthesis audio visual systems avatars extrapolation hidden markov models	In this paper we apply speaker-dependent training of Hidden Markov Models (HMMs) to audio and visual laughter synthesis separately. The two modalities are synthesized with a forced durations approach and are then combined together to render audio-visual laughter on a 3D avatar. This paper focuses on visual synthesis of laughter and its perceptive evaluation when combined with synthesized audio laughter. Previous work on audio and visual synthesis has been successfully applied to speech. The extrapolation to audio laughter synthesis has already been done. This paper shows that it is possible to extrapolate to visual laughter synthesis as well.	extrapolation;hidden markov model;markov chain;speech synthesis	Hüseyin Çakmak;Jérôme Urbain;Joëlle Tilmanne;Thierry Dutoit	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854469	natural language processing;speech recognition	Robotics	-14.978819941913638	-83.068169959662	26246
23c3b2870bb2ff822172f5ea8c65da10c7b0d93f	segmentation, diarization and speech transcription : surprise data unraveled			medical transcription;speaker diarisation;transcription (software)	Marijn Huijbregts	2008				NLP	-15.120474674162617	-86.03134792144735	26308
fbc8708d858c7bbc238722d802f555418799a5a8	using online corpora databases to improve esl writing		This paper summerizes the technical specifications and design principles of an online search tool we developed to help English as a Second Language (ESL) learners in certain aspects of writing. The problem we identified is the confusion caused by either first language interference (L1 interference) whereby literal translation erroneously combines with English words, or because certain words in English come together without a clear rule governing their combination such as collocations. The tool designed employs databases of authentic English texts (corpora) in addition to texts from trusted websites such as governmental and educational websites.	collocation;database;interference (communication);literal (mathematical logic);online search;text corpus	Basem Y. Alkazemi;Grami Mohammad A. Grami	2016	JCP		natural language processing;computer science;artificial intelligence	NLP	-28.815256194811457	-83.59747622847271	26324
cb8823ac74aad64905ed106964f708ad73291c70	binaural speech enhancement using a codebook based approach	hearing aids;speech;speech coding;speech enhancement;noise measurement;ear;estimation	Enhancement of speech in non stationary noise environments has been a major research topic due to its wide range of applications, e.g., in hearing aids. This paper proposes a method for performing dual channel speech enhancement. The proposed method involves the estimation of speech and noise short term predictor (STP) parameters using a codebook based approach, when we have access to binaural noisy signals. The estimated STP parameters are subsequently used for enhancement in a dual channel scenario. Objective measures indicate, that the proposed method is able to improve the speech intelligibility and quality.	binaural beats;codebook;intelligibility (philosophy);kerrison predictor;long short-term memory;multi-channel memory architecture;speech enhancement;stationary process	Mathew Shaji Kavalekalam;Mads Græsbøll Christensen;Jesper Bünsow Boldt	2016	2016 IEEE International Workshop on Acoustic Signal Enhancement (IWAENC)	10.1109/IWAENC.2016.7602908	voice activity detection;speech recognition;acoustics;engineering;speech coding;speech processing;psqm;audiology;intelligibility	Robotics	-10.173085685508976	-88.43765402590617	26325
d2927d6d58d3dbd99ea15960691f9b924c23f506	development and evaluation of hands-free spoken dialogue system for railway station guidance		In this paper, we describe development and evaluation of handsfree spoken dialogue system which is used for railway station guidance. In the application at the railway station, noise robustness is the most essential issue for the dialogue system. To address the problem, we introduce two key techniques in our proposed hands-free system; (a) blind spatial subtraction array (BSSA) as a preprocessing, which can efficiently reduce nonstationary and diffuse noises in real-time, and (b) robust voice activity detection (VAD) based on speech decoding for further improvement of speech recognition accuracy. The experimental assessment of the proposed dialogue system reveals that the combination of real-time BSSA and robust VAD can provide the recognition accuracy of more than 80% under adverse railwaystation noise conditions.	dialog system;dialog tree;emoticon;preprocessor;real-time clock;speech recognition;spoken dialog systems;voice activity detection	Hiroshi Saruwatari;Yu Takahashi;Hiroyuki Sakai;Shota Takeuchi;Tobias Cincarek;Hiromichi Kawanami;Kiyohiro Shikano	2008			speech recognition;noise reduction;microphone array;computer science	NLP	-13.381704687061404	-89.69261052366838	26326
73767cad2a5ece7a110ba6ad670849c311794fdd	power function-based power distribution normalization algorithm for robust speech recognition	ratio of arithmetic mean to geometric mean;normalization algorithm;robust speech recognition;taylor series speech recognition;medium duration observation window;medium duration window power distribution equalization ratio of arithmetic mean to geometric mean;real time;speech processing;parameter estimation noise compensation;equalization;speech;indexing terms;power function;power distribution;real time preprocessing stage power function based power distribution normalization algorithm robust speech recognition spectral power coefficients ratio arithmetic mean parametric power function medium duration observation window parameter estimation noise compensation automatic speech recognition weighted temporal averaging taylor series speech recognition;parametric power function;real time preprocessing stage;automatic speech recognition;cepstral analysis;algorithm theory;arithmetic mean;medium duration window;vector taylor series;ratio arithmetic mean;speech recognition;speech communication;weighted temporal averaging;geometric mean;spectral analysis;power distribution speech recognition real time systems noise robustness arithmetic speech enhancement signal to noise ratio parameter estimation automatic speech recognition taylor series;power function based power distribution;time frequency analysis;speech recognition algorithm theory real time systems;spectral power coefficients;noise;real time systems	A novel algorithm that normalizes the distribution of spectral power coefficients is described in this paper. The algorithm, called power-function-based power distribution (PPDN) is based on the observation that the ratio of arithmetic mean to geometric mean changes as speech is corrupted by noise, and a parametric power function is used to equalize this ratio. We also observe that a longer “medium-duration” observation window (of approximately 100 ms) is better suited for parameter estimation for noise compensation than the briefer window typically used for automatic speech recognition. We also describe the implementation of an online version of PPDN based on exponentially weighted temporal averaging. Experimental results shows that PPDN provides comparable or slightly better results than state of- the-art algorithms such as vector Taylor series for speech recognition while requiring much less computation. Hence, the algorithm is suitable for both real-time speech communication or as a real-time preprocessing stage for speech recognition systems.	additive white gaussian noise;cepstrum;coefficient;computation;estimation theory;feature extraction;frequency band;ibm notes;microsoft windows;online algorithm;parsing;preprocessor;real-time clock;real-time transcription;speech enhancement;speech recognition;utility functions on indivisible goods;vehicle tracking system;waveform;window function	Chanwoo Kim;Richard M. Stern	2009	2009 IEEE Workshop on Automatic Speech Recognition & Understanding	10.1109/ASRU.2009.5373233	speech recognition;computer science;speech;machine learning;pattern recognition;speech processing;mathematics	Vision	-13.084634352958492	-94.15117140183318	26337
2db5d6000087fa99886083c3e2f21f656c97dfe8	when will synthetic speech sound human: role of rules and data	text to speech synthesis;cost effectiveness;speech production;user acceptance	Text-to-speech synthesis research has moved away from building general purpose systems based on an understanding of human language and speech production towards building systems based on statistical algorithms applied to large text and speech corpora, and, recently, towards building such systems for specific domains. Despite substantial progress, the overall quality of even the best systems is often still inadequate for broad user acceptance in applications that cannot also be handled with simple phrase splicing. This tutorial paper analyzes which problems must be addressed to achieve the goal of generating naturalsounding speech in limited domains in a cost-effective way, and the roles of data and rules as we work towards solutions.	algorithm;part-of-speech tagging;speech synthesis;synthetic data;text corpus	Jan P. H. van Santen;Michael W. Macon;Andrew Cronk;John-Paul Hosom;Alexander Kain;Vincent Pagel;Johan Wouters	2000			voice activity detection;natural language processing;speech technology;speech production;speech recognition;cost-effectiveness analysis;linguistics;speech analytics	AI	-23.2495822044408	-85.46265392304295	26380
5a5eb4a14fdc1f5ae75c776c6dde67309dffe20f	a hybrid fragment / syllable-based system for improved oov term detection	vocabulary;actual term weighted value syllable based system oov term detection spoken term detection open vocabulary search large recordings term detection performance in vocabulary terms out of vocabulary terms syllable based search hybrid std system knowledge based subword modeling ability index fusion english conversational telephone speech;triphone index spoken term detection out of vocabulary fragment syllable fusion method;speech recognition;vocabulary natural language processing speech recognition;natural language processing;indexes vocabulary speech hidden markov models nist training speech processing	Spoken term detection (STD) is a task for open vocabulary search in large recordings of speech. Although the term detection performance for in-vocabulary (INV) terms has achieved a great improvement, the detection performance for out of vocabulary (OOV) terms is still disappointing. In this paper, we propose to combine fragment-based with syllable-based search into a hybrid STD system for OOV terms. Syllable is a kind of knowledge-based subword while fragment is data-driven. We initially compare their different modeling ability for OOVs. Considering the potential complementarities between them, we explore two methods of fusion: index fusion (combining the triphone indexes of a fragment-based and a syllable-based system) and result fusion (merging search results of the two systems). After the result fusion, we achieve a 9.4% relative improvement on NIST STD06 English conversational telephone speech (CTS) EvalSet in actual term weighted value (ATWV).	carpal tunnel syndrome;complementarity theory;inverter (logic gate);substring;syllable;triphone;vocabulary	Yong Xu;Wu Guo;Li-Rong Dai	2012	2012 8th International Symposium on Chinese Spoken Language Processing	10.1109/ISCSLP.2012.6423479	natural language processing;speech recognition;computer science;linguistics	NLP	-21.8889701409719	-83.76566443171399	26411
75e8b9212d2c8c4d6df67a5d3db13010925a5478	improving speaker identification in tv-shows using person name detection in overlaid text and speech		This paper is dedicated to the use of auxiliary information in order to help a classical acoustic-based speaker identification system in the specific context of TV shows. The underlying assumption is that auxiliary information could help (1) to rerank n-best speaker hypotheses provided by the acoustic-based only speaker identification system, (2) to provide confidence score to refine a rejection process (open-set identification task), and finally, (3) to identify speakers not covered by the speaker dictionary (out-of-dictionary speakers) used by the speaker identification system (full-set verification task); the last point being one of the main issue when dealing with TV shows. In this paper, the auxiliary information is based on person names detected in overlaid text and speech. Experiments conducted in three different datasets issued from the REPERE evaluation campaign have highlighted the interest of the auxiliary information used here, and notably the use of overlaid person names to identify out-of-dictionary speakers, confirming the key assumptions made.	acoustic cryptanalysis;dictionary;rejection sampling;speaker recognition	Delphine Charlet;Corinne Fredouille;Géraldine Damnati;Grégory Senay	2013			pattern recognition;artificial intelligence;speech recognition;speaker diarisation;speaker recognition;computer science	NLP	-14.629272695134048	-89.88020803257028	26432
7af0aef9c0359b7ef8f1e4819524a881e03d2688	linguistic and logical tools for an advanced interactive speech system in spanish	lenguaje natural;europa;dialogue system;representacion conocimientos;linguistique;architecture systeme;habla;espana;man machine dialogue;interaction;langage naturel;semantics;speech;tratamiento lenguaje;semantica;semantique;systeme conversationnel;linguistica;railway;analisis morfologico;language processing;interactive system;human machine interface;natural language;traitement langage;natural language generation;morphological analysis;sistema conversacional;analyse morphologique;dialogo hombre maquina;arquitectura sistema;espagne;interaccion;parole;europe;spontaneous speech;knowledge representation;ferrocarril;system architecture;representation connaissances;chemin de fer;dialogue manager;human machine interaction;semantic analysis;dialogue homme machine;spain;linguistics	This paper focuses on the increasing need for a more natural and sophisticated human-machine interaction (HMI). The research here presented shows work on the development of a restricted-domain spontaneous speech dialogue system in Spanish. This human-machine interface is oriented towards a semantically restricted domain: Spanish railway information. The paper focuses on the description of the understanding module, which performs the language processing once the dialogue moves have been recognised and transcribed into text. Following the morphological, syntactic and semantic analysis, the module generates a structured representation with the content of the user's intervention. This representation is passed on to the dialogue manager, which generates the system's answer. The dialogue manager keeps the dialogue history and decides what the reaction of the system should be, expressed by a new structured representation. This is sent to the natural language generator, which then builds the sentence to be synthesised.	closing (morphology);dialog system;dialog tree;formal grammar;human–computer interaction;information extraction;information system;lexicon;natural language generation;prototype;semantic analysis (compilers);speech corpus;speech synthesis;spontaneous order;system integration;text corpus;user interface	Jordi Alvarez;Victoria Arranz;Núria Castell;Montserrat Civit	2001		10.1007/3-540-45517-5_58	human–machine interface;natural language processing;interaction;morphological analysis;computer science;speech;semantics;natural language	NLP	-29.005261836273196	-81.57606451172768	26521
57cf233f2d8c89205ef23fb6e0f4d04612661d1d	representing phonological features through a two-level finite state model		Articulatory information has demonstrated to be useful to improve phone recognition performance in ASR systems, being the use of Neural Networks the most successful method to detect articulatory gestures from the speech signal. On the other hand, Stochastic Finite State Automata (SFSA) have been effectively used in many speech-input natural language tasks. In this work SFSA are used to represent phonological features. A hierarchical model able to consider sequences of acoustic observations along with sequences of phonological features is defined. From this formulation a classifier of articulatory features has been derived and then evaluated over a Spanish phonetic corpus. Experimental results show that this is a promising framework to detect and include phonological knowledge into ASR systems.	acoustic cryptanalysis;automaton;finite-state machine;hierarchical database model;natural language;neural networks	Javier Mikel Olaso;M. Inés Torres;Raquel Justo	2011			pattern recognition;speech recognition;natural language processing;artificial intelligence;computer science	NLP	-18.678452497662384	-87.08109101251428	26547
304796cb9deb835a9c6cd9fdd0b4ecf4dc4f6516	voice pathology detection and discrimination based on modulation spectral features	cepstral based features voice pathology detection modulation spectral features modulation frequency representation modulation spectrum voice disorders higher order singular value decomposition dimension reduced representation feature selection process information theoretic criterion voice classes vowel recordings support vector machines;support vector machines;speech processing;singular value decomposition;pathology classification higher order singular value decomposition svd modulation spectrum mutual information pathological voice pathological voice detection;voice disorders;spectrum;frequency modulation pathology acoustics speech mutual information harmonic analysis;confidence interval;frequency modulated;support vector machines information theory modulation singular value decomposition speech processing;mutual information;feature selection;higher order singular value decomposition;cross validation;support vector machine;information theoretic;information theory;modulation;harmonic analysis	In this paper, we explore the information provided by a joint acoustic and modulation frequency representation, referred to as modulation spectrum, for detection and discrimination of voice disorders. The initial representation is first transformed to a lower dimensional domain using higher order singular value decomposition (HOSVD). From this dimension-reduced representation a feature selection process is suggested using an information-theoretic criterion based on the mutual information between voice classes (i.e., normophonic/dysphonic) and features. To evaluate the suggested approach and representation, we conducted cross-validation experiments on a database of sustained vowel recordings from healthy and pathological voices, using support vector machines (SVMs) for classification. For voice pathology detection, the suggested approach achieved a classification accuracy of 94.1±0.28% (95% confidence interval), which is comparable to the accuracy achieved using cepstral-based features. However, for voice pathology classification the suggested approach significantly outperformed the performance of cepstral-based features.	acoustic cryptanalysis;cepstrum;cross-validation (statistics);experiment;feature selection;information theory;modulation;mutual information;singular value decomposition;support vector machine	Maria E. Markaki;Yannis Stylianou	2011	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2010.2104141	support vector machine;speech recognition;information theory;computer science;machine learning;harmonic analysis;pattern recognition;speech processing;mathematics;feature selection;statistics	Vision	-8.770498139737533	-90.1027676731405	26651
df8e57ce61ce0760ba7f64393c63871a85c8122a	some practical considerations in the deployment of a wireless-communication interactive voice response system	wireless communication;interactive voice response	In this paper, we describe the design procedure for a wireless communication interactive voice response system. The application must work in a very noisy environment which has imposed many design constraints. We will address the sensible aspects of three components of the application: the voice activity detector (VAD), the automatic speech recognition (ASR) system, and the confidence measure (CM) determination. In order to get a satisfactory product, it has been necessary to reduce the important mismatch between available linguistic and acoustic resources and the operational environment. Adaptation techniques for the acoustic models of the speech recognition system have proven to be effective to speed up the application deployment time.	acoustic cryptanalysis;interactive voice response;software deployment;speech recognition;voice activity detection	Carmen García-Mateo;Laura Docío Fernández;Antonio Cardenal López	2001			simulation;speech recognition;computer science;machine learning;wireless	Mobile	-14.646265060491093	-90.35034221197418	26685
28fa5f69aa0362ad6a16ee90f62d8028813a3768	using regular tree grammars to enhance sentence realisation	adjoining grammar;tag derivation tree;ftag-based sentence realiser;frtg encoding;feature-based tree;benchmark generator;sentence realisation;feature-based regular tree grammar;feature-based tags;benchmark construction;derivation tree	Feature-based regular tree grammars (FRTG) can be used to generate the derivation trees of a feature-based tree adjoining grammar (FTAG). We make use of this fact to specify and implement both an FTAG-based sentence realiser and a benchmark generator for this realiser. We argue furthermore that the FRTG encoding enables us to improve on other proposals based on a grammar of TAG derivation trees in several ways. It preserves the compositional semantics that can be encoded in feature-based TAGs; it increases efficiency and restricts overgeneration; and it provides a uniform resource for generation, benchmark construction and parsing.	benchmark (computing);constraint satisfaction problem;forward error correction;kaplan–meier estimator;maxwell (microarchitecture);parsing;regular tree grammar;rendering (computer graphics);revision tag;tree-adjoining grammar	Claire Gardent;Benjamin Gottesman;Laura Perez-Beltrachini	2011	Natural Language Engineering	10.1017/S1351324911000076	natural language processing;l-attributed grammar;computer science;theoretical computer science;regular tree grammar;algorithm	NLP	-28.095958468785984	-80.32741175333767	26741
9909c15a8b0f8801032404a362fbcc33b3253c49	a robust speech recognition system against the ego noise of a robot		This paper presents a speech recognition system for a mobile robot that attains a high recognition performance, even if the robot generates ego-motion noise. We investigate noise suppression and speech enhancement methods that are based on prediction of ego-motion and its noise. The estimation of egomotion is used for superimposing white noise in a selective manner based on the ego-motion type. Moreover, instantaneous prediction of ego-motion noise is the core concept to establish the following techniques: ego-motion noise suppression by template subtraction and missing feature theory based masking of noisy speech features. We evaluate the proposed technique on a robot using speech recognition results. Adaptive superimposition of white noise achieves up to 20% improvement of word correct rates (WCR) and the spectrographic mask attains an additional improvement of up to 10% compared to the single channel recognition.	mobile robot;speech enhancement;speech recognition;visual odometry;white noise;zero suppression	Gökhan Ince;Kazuhiro Nakadai;Tobias Rodemann;Hiroshi Tsujino;Jun-ichi Imura	2010			id, ego and super-ego;speech recognition;speaker recognition;robot;computer vision;computer science;artificial intelligence	Robotics	-13.245217263401827	-91.46358070957116	26836
0c2d52d7546966e56e5b9d5f6a147c22615388b6	dynamic character model generation for document keyword spotting	lettre alphabet;presentacion documento;document structure;modelo dinamico;statistical character;phonetique;coreano;keyword;analisis estadistico;modelo markov;estructura documental;caractere statistique;structure document;model generation;real time;document layout;dynamic model;caracter impreso;printed character;palabra clave;caracter estadistico;mot cle;presentation document;korean;markov model;reconnaissance caractere;statistical analysis;modelo 2 dimensiones;coreen;viterbi algorithm;temps reel;modele dynamique;analyse statistique;modele 2 dimensions;pattern recognition;fonetica;tiempo real;letra alfabeto;phonetics;reconnaissance forme;modele markov;reconocimiento patron;letter;caractere imprime;character recognition;two dimensional model;language model;reconocimiento caracter	This paper proposes a novel method of generating statistical Korean Hangul character models in real time. From a set of grapheme average images we compose any character images, and then convert them to P2DHMMs. The nonlinear, 2D composition of letter models in Hangul is not straightforward and has not been tried for machine-print character recognition. It is obvious that the proposed method of character modeling is more advantageous than whole char- acter or word HMMs in regard to the memory requirement as well as the train- ing difficulty. In the proposed method individual character models are synthe- sized in real-time using the trained grapheme image templates. The proposed method has been applied to key character/word spotting in document images. In a series of preliminary experiments, we observed the performance of 86% and 84% in single and multiple word spotting respectively without language models. This performance, we believe, is adequate and the proposed method is effective for the real time keyword spotting applications		Beom-Joon Cho;Bong-Kee Sin	2004		10.1007/978-3-540-27868-9_123	natural language processing;phonetics;speech recognition;letter;viterbi algorithm;computer science;artificial intelligence;markov model;korean;language model	NLP	-22.226952052319753	-90.66262073778569	26957
1f477f60fe679e9db0efbd15918acc5781aff336	optimization of computational time in pattern recognizers	relative comparison;pattern recognition;computational length;supervisor;character recognition;absolute comparison	"""-A sequential organization of the computations arising from pattern recognizers by absolute comparison is suggested in order to reduce the mean computational time involved. This optimization problem is solved by means of a supervising system which exploits the information obtained from the patternvector through a preclassifier : this information has the form of a conditional probability distribution of the classes to which the pattern-vector may belong. The results are extended to pattern recognizers by relative comparison. Pattern recognition Absolute comparison Supervisor Character recognition Relative comparison Computational length I. I N T R O D U C T I O N A N D P R E L I M I N A R Y D E F I N I T I O N S We recall here definitions and results which can be found in the works of Nilsson, """"1 Castan 12) and Viscolani. 131 Let ~ be the set of possible pattern-vectors x and {:¢1, ~¢2 . . . . . :~k}, (k >/3), be the parti t ion of corresponding to a particular recognition problem. Let K = {1, 2 . . . . . k] and K* = Kw{0} . Let us consider a set F, whose elements are totally ordered by the relation < , and the mappings: Z i : : # ~ F , ieK. The recognizer by absolute comparison 12) in (F, < ), with threshold s, associated to the ordered k-tuple (Zt, Z2 . . . . . Xk), is the mapping: Z: .~ ~ K*, {io>O¢>zi(x)>~sand;~j(x)<s, Vj4:i, (I) X(x) = otherwise, where s is a fixed element of F. We assume that """"Ai(x)>~s~zi(x)<s, Vj4:i, V x e ~ , (2) and that x~;~,~Xi(x)>~ s, ¥i~K. (3) Once an algorithm is chosen to evaluate Z(x), the computational length v(x) is the number of mappings of the family {)~L, Z2 . . . . , Z~} which must be computed according to the chosen algorithm in order to classify the pattern-vector x. Gk is the symmetric group over K, *This work was supported by the Consiglio Nazionale delle Ricerche of Italy. 419 ( 7 = ~ G k (71 ( 7 2 """" """" """" k is any permutation and t; is the identity of G k. For every permutation (TeG k we define (a) the following Algorithm Ro. 1 i ~ 1 2 compu te ~(.,(x) 3 i f Z.,(x ) ~> s then Z(x) = (7,; stop e l s e i * i + l ; go to step 2. If the algorithm R., with fixed (7 e Gk, has been chosen and if hypotheses (2) and (3) hold, then x e . ~ i ~ v ( x ) = a i -1, i~K. For every pattern recognition problem, we assume as given the probability distribution P(x) over the set ;~, so that we know the probabilities dP(x) = p~, ieK, (4) J ¢, with"""	algorithm;artificial intelligence;computation;finite-state machine;goto;mathematical optimization;national research council (italy);optimization problem;pattern recognition;time complexity	Bruno Viscolani	1982	Pattern Recognition	10.1016/0031-3203(82)90046-2	speech recognition;computer science;machine learning;pattern recognition;algorithm	Theory	-25.181474187189316	-90.44003552412131	27012
f5700aa2c5ba293f69c1dca89fa9e2d5cdffb073	基於卷積類神經網路之廣播節目音訊事件偵測系統 (automatic audio event detection of broadcast radio programs based on convolution neural networks) [in chinese].		廣播電臺節目中通常包含語音,音樂與其他音訊事件(如笑聲或特效聲)。若能偵 測並切割這些音訊事件,就能進一步對廣播節目進行加值運用。例如,轉寫語音片段的 逐字稿,或是辨認音樂片段的歌名與曲名,以利檢索。針對此問題,在本論文中,我們 首先設計,並以人工標註出一廣播節目音訊事件資料庫,再利用 Convolutional Neural Network (CNN)自動擷取有效的特徵音訊參數,對廣播電臺的音檔做音訊事件偵測與切 割,最後轉成具時間資訊的音訊事件標註檔。實驗方面我們從教育電臺節目中,選出新 聞類與不同性質的談話類節目共 14 個,經人工標注後,獲得總長度共約 60 小時的音 檔,並用來訓練與測試 CNN和傳統 Gaussian Mixture Model(GMM)的效能。實驗結 果顯示以 CNN直接搭配頻譜參數,在偵測語音與非語音,音樂與非音樂或其它與非其 它音訊事件等的錯誤率(equal error rates,EER),分別為 2.27%、12.52%與 9.51%,皆 低於傳統以 GMM搭配 Mel-Frequency Cepstral Coefficients(MFCCs)的 3.65%、15.68% 與 13.25%。 關鍵詞:廣播節目資料庫、音訊事件偵測、卷積類神經網路。 The 2017 Conference on Computational Linguistics and Speech Processing ROCLING 2017, pp. 21-36  The Association for Computational Linguistics and Chinese Language Processing	artificial neural network;computation;computational linguistics;convolution;convolutional neural network;mel-frequency cepstrum;mixture model;speech processing	Jhih-wei Chen;Wu-Hua Hsu;Yuan-Fu Liao	2017			artificial neural network;radio broadcasting;computer network;convolution;computer science	NLP	-15.59200104064706	-86.88646153778332	27091
d57e8536ac10d7b281c58531725357731181431b	the mood of chinese pop music: representation and recognition		Music mood recognition (MMR) has attracted much attention in music information retrieval research, yet there are few MMR studies which focus on non-Western music. In addition, little has been done on connecting the two most adopted music mood representation models: categorical and dimensional. To bridge these gaps, we constructed a new dataset consisting of 818 Chinese Pop (C-Pop) songs, three complete sets of mood annotations in both representations, as well as audio features corresponding to five distinct categories of musical characteristics. The mood space of C-Pop songs was analyzed and compared to that of Western Pop songs. We also explored the relationship between categorical and dimensional annotations and the results revealed that one set of annotations could be reliably predicted by the other. Classification and regression experiments were conducted on the dataset, providing benchmarks for future research on MMR of non-Western music. Based on these analyses, we reflect and discuss the implications of the findings to MMR research.	experiment;feature model;holographic principle;information retrieval;mir (computer);meet-me room;modal logic;multi-master replication;national supercomputer centre in sweden;one-way function;performance;research data archiving;sampling (signal processing)	Xiao Hu;Yi-Hsuan Yang	2017	JASIST	10.1002/asi.23813	mood;music information retrieval;computer science;categorical variable;popular music;speech recognition	Web+IR	-5.197798877130513	-86.12197675690982	27108
ad4f12dfbc02e00b7ee41aba76130935a7d1f95b	a case study on back-end voice activity detection for distributed specch recognition system using support vector machines	kernel;support vector machines;support vector machines cepstral analysis learning artificial intelligence signal classification speech recognition;mel frequency cepstral coefficients voice acivity detection support vector machines dsr system;voice acivity detection;speech;polynomials;mel frequency cepstral coefficient;kernel polynomials speech recognition speech mel frequency cepstral coefficient feature extraction support vector machines;feature extraction;mel frequency cepstral coefficients;dsr system;speech recognition;etsi afe standard back end voice activity detection distributed speech recognition system support vector machines machine learning techniques svm based vad dsr system speech frames nonspeech frames compressed mel frequency cepstral coefficients mfcc back end server side vad performance improvement compression bit rate reduction front end side svm training polynomial kernel classification task aurora 2 speech database noise conditions etsi advanced front end standard	Recently, the Voice Activity Detection (VAD) algorithms based on machine learning techniques have shown impressive results in the area of speech recognition. In this paper, we present a case study and we discuss the performance of VAD based on Support Vector Machines (SVM) for Distributed Speech Recognition (DSR) system. In this case study, the speech and the non-speech frames are detected from the compressed Mel Frequency Cepstral Coefficients (MFCCs), at the back-end (e.g. Server) side, with the aim of improving the VAD performance and reducing the compression bit-rate from the front-end side. By using the trained SVM with polynomial kernel, the SVM-based VAD can produce encouraging detection results. The classification task conducted from the Aurora-2 speech database with different noise conditions shows comparable VAD performance, with respect to ETSI Advanced Front-End (ETSI-AFE) standard.	ansi escape code;algorithm;analog front-end;feature selection;machine learning;mel-frequency cepstrum;polynomial kernel;sensor;signal-to-noise ratio;speech recognition;support vector machine;voice activity detection	Azzedine Touazi;Mohamed Debyeche	2014	2014 Tenth International Conference on Signal-Image Technology and Internet-Based Systems	10.1109/SITIS.2014.54	voice activity detection;speech recognition;computer science;machine learning;pattern recognition;mel-frequency cepstrum	Robotics	-14.18488235613486	-89.74680313776199	27158
0febdd5eeb32f5f561fe325d7047a6904d3cf5d2	online handwritten tibetan syllable recognition based on component segmentation method	component;segmentation;character recognition tv handwriting recognition;online handwritten tibetan syllable recognition;context online handwritten tibetan syllable recognition component segmentation;context	Syllable-based input is more preferable than character-based input for Tibetan people due to the inherent characteristics of Tibetan characters. This paper presents a component segmentation-based recognition method for online handwritten Tibetan syllables. The input syllable is over-segmented into a sequence of sub-structure blocks (stroke blocks) with two-layer segmentation point annotation using horizontal-vertical over-segmentation method. Segmentation hypotheses based on sub-structure block sequences are evaluated by fusing multiple contexts into a principled Tibetan syllable recognition framework. Component-based and character-based bi-gram models are used to represent linguistic contexts. The optimal path is searched to give the component segmentation and syllable recognition results. We evaluated the recognition performance on online handwritten Tibetan syllable database with 827 classes. Experimental results show the effectiveness of the proposed method. Our method achieved the syllable-level recognition rate of 81.23%, and is superior to character segmentation and segmentation-free methods.	handwriting recognition;syllable;text-based (computing)	Long-Long Ma;Jian Wu	2015	2015 13th International Conference on Document Analysis and Recognition (ICDAR)	10.1109/ICDAR.2015.7333723	speech recognition;computer science;pattern recognition;component;segmentation	Robotics	-18.977160785384623	-84.45705500804513	27167
f8c7db7c2cb98f5e8cc2e497f17d03b6c01af982	joint recognition of raag and tonic in north indian music	raag recognition;kernel-density pitch distribution;chromatic pitch;joint recognition;indian music;north indian classical music;different raag category;tonal feature;standard twelve-dimensional pitch class;raag error rate;continuous tonal representation;pitch distribution	In many non-Western musical traditions, such as North Indian classical music (NICM), melodies do not conform to the major and minor modes, and they commonly use tunings that have no fixed reference (e.g., A = 440 Hz). We present a novel method for joint tonic and raag recognition in NICM from audio, based on pitch distributions. We systematically compare the accuracy of several methods using these tonal features when combined with instance-based (nearest-neighbor) and Bayesian classifiers. We find that, when compared with a standard twelve-dimensional pitch class distribution that estimates the relative frequency of each of the chromatic pitches, smoother and more continuous tonal representations offer significant performance advantages, particularly when combined with appropriate classification techniques. Best results are obtained using a kernel-density pitch distribution along with a nearest-neighbor classifier using Bhattacharyya distance, attaining a tonic error rate of 4.2 percent and raag error rate of 10.3 percent (with 21 different raag categories). These experiments suggest that tonal features based on pitch distributions are robust, reliable features that can be applied to complex melodic music.	bayesian network;experiment;mac os x 10.3 panther;naive bayes classifier;nearest neighbour algorithm;nearest-neighbor interpolation;pitch (music)	Parag Chordia;Sertan Sentürk	2013	Computer Music Journal	10.1162/COMJ_a_00194	speech recognition	ML	-22.803552504103813	-89.92744419664469	27205
3266a4fa820ec1f89950f6701b2ffa50a0129979	a hybrid approach to automatic segmentation and labeling for mandarin chinese speech corpus	automatic segmentation;mandarin chinese;hybrid approach;difference set;mandarin speech corpus	In this paper, we propose a hybrid approach to refine the phonetic boundaries in a Mandarin speech corpus. This approach employs different sets of acoustic features for different categories of phonetic transitions, except for the most difficult case of “periodic voiced + periodic voiced”, which is therefore handled by a heuristic scheme. Several experiments are designed to demonstrate the feasibility of the proposed approach.	acoustic cryptanalysis;experiment;heuristic;speech corpus;super robot monkey team hyperforce go!	Cheng-Yuan Lin;Kuan-Ting Chen;Jyh-Shing Roger Jang	2005			mandarin chinese;linguistics;difference set	NLP	-17.36369004125293	-85.33070064320782	27252
9238875bfed425b75af200b1251e483829920717	low delay phone recognition	synthetic mouth gestures low delay phone recognition acts vidas project video assisted with audio coding and representation phonetic recognizer lip synchronization videophone applications very low rate speech coding phonetic vocoding;videotelephony audio coding;conference report;delays	In the frame of the ACTS VIDAS project (VIDeo ASsisted with audio coding and representation) the authors have compared different schemes for low-delay phone recognition. In the VIDAS project, the phonetic recognizer was proposed for lip-synchronization in videophone applications, but the results are also of interest for very low rate speech coding (Phonetic Vocoding) and for driving synthetic mouth gestures in real time.	finite-state machine;speech coding;synthetic intelligence;vocoder	José A. R. Fonollosa;Eloi Botile;José B. Mariño	1998	9th European Signal Processing Conference (EUSIPCO 1998)	10.5281/zenodo.36508	speech recognition;acoustics;computer science;communication	Vision	-14.971054424048802	-85.36016214996323	27299
e85f4f6faa719c4b412bbf1399f594bddf418e3e	speeding up the score computation of hmm speech regognizers with the bucket voronoi intersection algorithm.		With increasing sizes of speech databases, speech recognizers with huge parameter spaces have become trainable. However, the time and memory requirements for high accuracy re-altime speaker-independent continuous speech recognition will probably not be met by the available hardware for a reasonable price for the next few years. This paper describes the application of the Bucket Voronoi Intersection algorithm to the JANUS-2 speech recognizer, which reduces the time for the computation of HMM emission probabilities with large Gaussian mixtures by 50% to 80%. 1. INTRODUCTION Although the computation of Gaussians is only a part (for very large vocabularies, even a small part) of the overall run time, speeding it up does reduce the reaction time of the recognizer, and especially the time for training signiicantly. When computing the log probability of a Gaus-sian mixture, many speech recognizer do not use	computation;database;finite-state machine;hidden markov model;intersection algorithm;log probability;requirement;run time (program lifecycle phase);speech recognition;speech synthesis;vocabulary;voronoi diagram	Jürgen Fritsch;Ivica Rogina;Tilo Sloboda;Alexander H. Waibel	1995			computer science;theoretical computer science;machine learning;pattern recognition	NLP	-21.47717337332653	-87.83955852003686	27324
7d144f8303ebdc077c01550984b10a502853fb24	medrem: an interactive medication reminder and tracking system on wrist devices	wrist;training;smart phones;stakeholders;monitoring;dictionaries;speech recognition	Medication adherence is pivotal for effective health outcomes. One of the main reasons behind poor medication adherence is forgetfulness, and reminder systems are often used in addressing the problem. This paper presents MedRem, a novel medication reminder and tracking system on wearable wrist devices. The system is handy and interactive, and it is enriched with several useful features. To address the limitations of the tiny display size of the wrist devices, MedRem incorporates speech recognition and text-to-speech features along with clever interface design. Users interact with the system using voice commands as well as using the display available on the device. A dictionary based training approach is used on top of the state of the art speech recognition systems to reduce the errors in recognizing the commands from the users. The system is evaluated for both native and non-native English speakers. The error rates for recognizing voice commands are 6.43% and 20.9% for the native and the non-native speakers, respectively, when a off-the-shelf speech recognition system is used. MedRem reduces the error to nearly zero for both types of users through a dictionary based training approach. On average, only 1.25 and 15 training commands are required to achieve this performance for the native and the non-native speakers, respectively.	dictionary;display size;handy board;speech recognition;speech synthesis;tracking system;wearable computer	Md. Abu Sayeed Mondol;Ifat Afrin Emi;John A. Stankovic	2016	2016 IEEE Wireless Health (WH)	10.1109/WH.2016.7764555	speech recognition;engineering;multimedia;communication	HCI	-24.887342227039273	-87.168455607963	27329
bd0086dae7585a92a0d83227385b7dd3fbf2be37	extraction of syllabically rich and balanced sentences for tigrigna language	syllabically rich;syllabically balanced;sentence selection;speech corpus production;conference lecture	The Tigrigna language lacks text and speech corpora for developing speech technologies. In this work, after considering the phonetic nature of Tigrigna, we have gathered and pre-processed an initial and relatively large corpus of sentences. Using the syllable as basic phonetic unit, two different sub-corpora are developed from that initial text corpus, one that is phonetically rich and the other that is balanced. Two different methods are used for that purpose, which are variants of already published methods. Finally, the frequencies of occurrence of the syllabic units in the balanced corpus are contrasted with a previously reported study of Tigrigna phonetics, observing consistency between both.	speech synthesis;syllable;text corpus	Hafte Abera;Climent Nadeu;Sebsibe H. Mariam	2016	2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2016.7732360	natural language processing;speech recognition;computer science	NLP	-19.690010770448946	-82.61488680372753	27332
b6f21859fefabe972e2129a9ee0f1c57e937b723	elirf at mediaeval 2014: query by example search on speech task (quesst)		In this paper, we present the systems that the Natural Language Engineering and Pattern Recognition group (ELiRF) has submitted to the MediaEval 2015 Query by Example Search on Speech Task. All of them are based on a Subsequence Dynamic Time Warping algorithm. The systems use information from outside the task (low-resources systems).	algorithm;dynamic time warping;natural language;pattern recognition;query by example	Marcos Calvo Lafarga;Mayte Giménez;Lluís F. Hurtado;Emilio Sanchis Arnal;Jon Ander Gómez	2014			dynamic time warping;natural language;speech recognition;query by example;computer science	NLP	-15.637437159595656	-86.44190961437913	27381
215cfa5ef96d076cb6ea1868b32211afb19e0a40	a probabilistic model for cursive handwriting recognition using spatial context	bayesian framework;duration model;spatial context;optimisation;context modeling handwriting recognition hidden markov models pattern recognition computational complexity bayesian methods distribution functions maximum likelihood estimation parameter estimation speech analysis;probability;handwriting recognition;image segmentation;bayes methods;computational complexity probabilistic model cursive handwriting recognition spatial context spatial contextual information ambiguous segmentations handwritten patterns optimization problem bayesian framework duration modeling spatial letter configurations real world handwriting dataset recognition performance;contextual information;optimization problem;probabilistic model;computational complexity;image segmentation handwriting recognition handwritten character recognition bayes methods optimisation probability computational complexity;spatial configuration;handwritten character recognition	In this work we introduce a probabilistic model that utilizes spatial contextual information to aid recognition when dealing with ambiguous segmentations of handwritten patterns. The recognition problem is formulated as an optimization problem in a Bayesian framework by explicitly conditioning on the spatial configuration of the letters. As a consequence, and in contrast to HMMs, the proposed model can handle duration modeling without an increase in computational complexity. We test the model on a real-world handwriting dataset and discuss several factors that affect the recognition performance.	computational complexity theory;handwriting recognition;mathematical optimization;optimization problem;statistical model	Jigang Wang;Predrag Neskovic;Leon N. Cooper	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1416275	optimization problem;statistical model;speech recognition;computer science;spatial contextual awareness;machine learning;pattern recognition;probability;mathematics;handwriting recognition;image segmentation;computational complexity theory;statistics	Vision	-19.622637711345938	-92.85804686088245	27413
6c969947d1424a79ea029546170c598fc723a6b2	visual-only discrimination between native and non-native speech	engineering;non native speech identification;binary classification nonnative speech visual only discrimination biometric characteristic specific traits speaking style speech production system vocal tract lip movements linguistics speech processing speech recognition mother tongue foreign language english visual features continuous visual speech mobile phones appearance descriptors video frame vocabulary based histograms;technology;acoustics;accent classification;conference paper;recognition;science technology;visual speech processing non native speech identification accent classification;language identification;vocabulary linguistics mobile radio speaker recognition speech enhancement;engineering electrical electronic;speech visualization mouth speech recognition hidden markov models speech processing vectors;audiovisual speech;visual speech processing	Accent is an important biometric characteristic that is defined by the presence of specific traits in the speaking style of an individual. These are identified by patterns in the speech production system, such as those present in the vocal tract or in lip movements. Evidence from linguistics and speech processing research suggests that visual information enhances speech recognition. Intrigued by these findings, along with the assumption that visually perceivable accent-related patterns are transferred from the mother tongue to a foreign language, we investigate the task of discriminating native from non-native speech in English, employing visual features only. Training and evaluation is performed on segments of continuous visual speech, captured by mobile phones, where all speakers read the same text. We apply various appearance descriptors to represent the mouth region at each video frame. Vocabulary-based histograms, being the final representation of dynamic features for all utterances, are used for recognition. Binary classification experiments, discriminating native and non-native speakers, are conducted in a subject-independent manner. Our results show that this task can be addressed by means of an automated approach that uses visual features only.	binary classification;biometrics;experiment;mobile phone;production system (computer science);speech processing;speech recognition;tract (literature);vocabulary	Christos Georgakis;Stavros Petridis;Maja Pantic	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854519	natural language processing;language identification;speech production;speech recognition;computer science;speech;technology	Vision	-12.33989923148947	-86.01397257229019	27508
094c9e24777f7370624438b2806ba956db616c8a	application of speech recognition to african elephant (loxodonta africana) vocalizations	animals;speaker identification;speech recognition animals acoustic noise speech processing working environment noise feature extraction cepstral analysis robustness biomedical acoustics signal processing algorithms;filter bank;biomedical acoustics;data collection;speech processing;working environment noise;call classification;speaker recognition;frequency spectrum;mel frequency cepstral coefficient;features;african elephant vocalizations speech recognition loxodonta africana speech processing speaker identification call classification features mel frequency cepstral coefficients log energy shifted filter bank infrasound range frequency spectrum;loxodonta africana;african elephant vocalizations;cepstral analysis;shifted filter bank;feature extraction;acoustic noise;signal classification;mel frequency cepstral coefficients;speech recognition;robustness;infrasound range;african elephant;signal processing algorithms;classification accuracy;log energy;cepstral analysis speaker recognition speech processing feature extraction signal classification	This paper presents a novel application of speech processing research, classification of African elephant vocalizations. Speaker identification and call classification experiments are performed on data collected from captive African elephants in a naturalistic environment. The features used for classification are 12 Mel-Frequency Cepstral Coefficients plus log energy computed using a shifted filter bank to emphasize the infrasound range of the frequency spectrum used by African elephants. Initial classification accuracies of 83.8% for call classification and 88.1% for speaker identification were obtained. The long-term goal of this research is to develop a universal analysis framework and robust feature set for animal vocalizations that can be applied to many species.	captive portal;experiment;filter bank;mel-frequency cepstrum;speaker recognition;spectral density;speech processing;speech recognition	Patrick J. Clemins;Michael T. Johnson	2003		10.1109/ICASSP.2003.1198823	speaker recognition;speech recognition;computer science;speech processing;mel-frequency cepstrum	ML	-12.031333939304618	-91.2674089520208	27520
ff32d5b8f016e0eb855e4913622015feb2a853c2	speaker adaptation method for fenonic markov model-based speech recognition	modelo markov;modele ferronique;reconocimiento palabra;intelligence artificielle;systeme adaptatif;markov model;adaptive system;speech recognition;sistema adaptativo;artificial intelligence;inteligencia artificial;reconnaissance parole;modele markov;speaker;locutor;speaker adaptation;locuteur	Abstract#R##N##R##N#This paper proposes a speaker adaptation method for speech recognition based on the discrete hidden Markov models (HMM), especially the fenonic Markov model, which is a framewise model. The method consists largely of two parts—the adaptation of the vector quantization codebook and the adaptation of Markov models. The former employs a method which is based on the difference between the feature parameter distributions of the training speech and the word baseform as the basis for the adaptation, where the two are divided into N segments on the time axis. The latter employs a method based on a linear mapping, which is estimated from the matching between the quantized training speech and the word baseform.#R##N##R##N##R##N##R##N#In this study, a recognition experiment was executed using 150 words with high similarities. Using the speech in which all object words are uttered ten times by a male, the codebook and the Markov models are estimated as the basis for the adaptation. Then the adaptation training is executed for seven males and four females by uttering once 25 words in the object vocabulary. The average error rate, i.e., 25.0 and 45.2 percent, respectively, for the males and females, is improved to 4.1 and 7.8 percent. Thus, the usefulness of the proposed method is demonstrated.	markov chain;markov model;speech recognition	Masafumi Nishimura	1991	Systems and Computers in Japan	10.1002/scj.4690221306	loudspeaker;speech recognition;computer science;artificial intelligence;adaptive system;machine learning;markov model	ML	-19.97959212503707	-89.39241363084477	27641
2229da5511a076b595a82971cd155d27a4a485d3	machine translation versus dictionary and text structure	machine translation	Nowadays, in the age of global communications, the need for on-line, accurate, and cheap translation from one language into another has become absolutely pronounced. This situation can even turns critical when considering translation and interpreting in high risk technology domains. The discrepancy in Codes, Norms, and Standards in various countries as well as delayed information exchange gives rise to increasing disagreement in high-technology and dangerous fields of common engineering interest. The emphasis of this paper is the necessity of realizing the correlation between the main problems and achievements of both machine translation and translation memory systems as well as the possibilities of using such systems as a real tool (automatic workstation) for both analysts and translators. It means consideration of such system pragmatics which demands that we investigate different kinds of linguistic and extra-linguistic knowledge as well as interference of the authors’ mother tongues on the English spec...	dictionary;machine translation	Larissa N. Beliaeva	2003	Journal of Quantitative Linguistics	10.1076/jqul.10.2.193.16711	dynamic and formal equivalence;natural language processing;example-based machine translation;computer science;mathematics;linguistics;machine translation;algorithm	NLP	-32.399819645533846	-85.08197778602073	27693
55d60e5da0f556badc7dc7e64987254b99bb978c	binaural speaker recognition for humanoid robots	databases;humanoid robot;neural nets;speech speech recognition feature extraction databases speaker recognition noise mel frequency cepstral coefficient;speech processing;reverberation binaural speaker recognition humanoid robots automatic speaker recognition speech processing community natural interface human robotic interaction parallel predictive neural networks mel frequency cepstral coefficients multiple talkers speaker spatial position learning;speech;human robot interaction;speaker recognition;mel frequency cepstral coefficient;speech processing humanoid robots human robot interaction neural nets speaker recognition;automatic speaker recognition;humanoid robots;feature extraction;speech recognition;noise;neural network	In this paper, an original study of a binaural speaker identification system is presented. The state of the art shows that, contrarily to monaural and multi-microphone approaches, binaural systems are not so much studied in the specific task of automatic speaker recognition. Indeed, these systems are mostly used for speech recognition, or speaker localization. This study will focus on the benefits of the binaural context in comparison with monaural techniques. It demonstrates the interest of the binaural systems typically used in humanoid robotics. The system is first tested with monaural signals, and then with a binaural sensor, in many signal to noise ratios, speech durations and speaker directions. Up to 11 percent of improvement in recognition ratios of 23 ms frames can be obtained. The used database is a set of audio tracks recorded for 10 speakers, and filtered by HRTFs to obtain binaural signals in the directions of interest, for the binaural training and testing steps. This way, we study the sensitivity of the system to the speaker's location in an environment where a maximum of 10 speakers is present.	acoustic cryptanalysis;binaural beats;experiment;google map maker;humanoid robot;microphone;pc speaker;performance;robotics;speaker recognition;spectral method;speech recognition	Bastien Breteau;Sylvain Argentieri;Jean-Luc Zarader;Zefeng Wang;Karim Youssef	2010	2010 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2010.5723535	speaker recognition;speaker diarisation;speech recognition;acoustics;computer science;humanoid robot;artificial intelligence;artificial neural network	Robotics	-11.806397695121888	-90.16666333861998	27719
1f46ed20e1ef81157b6bca48ccef9b4e6e3e7774	quantized-dialog language model for goal-oriented conversational systems		We propose a novel methodology to address dialog learning in the context of goal-oriented conversational systems. The key idea is to quantize the dialog space into clusters and create a language model across the clusters, thus allowing for an accurate choice of the next utterance in the conversation. This quantizeddialog language model methodology has been applied to the end-to-end goal-oriented track of the latest Dialog System Technology Challenges (DSTC6). The objective is to find the correct system utterance from a pool of candidates in order to complete a dialog between a user and an automated restaurant-reservation system. Our results show that the technique proposed in this paper achieves high accuracy regarding selection of the correct candidate utterance, and outperforms other state-of-the-art approaches based on neural networks.		R. Chulaka Gunasekara;David Nahamoo;Lazaros Polymenakos;Jatin Ganhotra;Kshitij P. Fadnis;Thomas J. Watson	2017	CoRR			NLP	-23.91969473561948	-87.03367543733243	27802
354b27e6e4ef163590e5a4d354e04a2ce0961bae	video realistic mouth animation based on an audio visual dbn model with articulatory features and constrained asynchrony	asynchrony;mouth;learning algorithm;af_avdbn;speech synthesis;mouth images;audio visual dbn model;maximum likelihood estimation criterion;asynchrony mouth animation af_avdbn articulatory features;mouth facial animation hidden markov models maximum likelihood estimation speech processing image converters covariance matrix speech recognition speech synthesis mpeg 4 standard;speech;articulatory features;maximum likelihood estimation;articulatory feature;visualization;maximum likelihood estimate;face recognition;hidden markov models;facial animation;visual features;speech recognition;audio visual;mouth animation;computer animation;speech synthesis computer animation face recognition maximum likelihood estimation;subjective evaluation;speech production;video realistic mouth animation;mouth images video realistic mouth animation audio visual dbn model articulatory features visual feature learning algorithm maximum likelihood estimation criterion;visual feature learning algorithm	This paper presents a mouth animation construction method based on the DBN models with articulatory features (AF_AVDBN), in which the articulatory features of lips, tongue, glottis/velum can be asynchronous within a maximum asynchrony constraint to describe the speech production process more reasonably. Given an audio input and the trained AF_AVDBN models, the optimal visual feature learning algorithm is deduced based on the Maximum Likelihood Estimation criterion. The learned visual features are then used to construct the mouth images for the input speech. Objective and subjective evaluations on the mouth animations of 110 speech sentences show that the learned visual features from the AF_AVDBN models track the real visual features very closely, and the constructed mouth images from the AF_AVDBN models are very much like the real ones.	algorithm;asynchronous i/o;asynchrony (computer programming);feature learning	Dongmei Jiang;Peizhen Liu;Ilse Ravyse;Hichem Sahli;Werner Verhelst	2009	2009 Fifth International Conference on Image and Graphics	10.1109/ICIG.2009.51	facial recognition system;speech recognition;computer science;pattern recognition;maximum likelihood;speech synthesis	Vision	-15.028149292782112	-83.49880215965706	27826
1686f1754e0a0426ab83b8aec4d6ac2e6dcbbd0f	learning to say it well: reranking realizations by predicted synthesis quality	synthetic voice;synthetic speech;paraphrases representative;variable synthesis quality;discriminative paraphrase reranking;language generator;synthetic voice quality;cross-validation study;reranking realization;discriminative reranker;language dialogue system;voice quality;cross validation	This paper presents a method for adapting a language generator to the strengths and weaknesses of a synthetic voice, thereby improving the naturalness of synthetic speech in a spoken language dialogue system. The method trains a discriminative reranker to select paraphrases that are predicted to sound natural when synthesized. The ranker is trained on realizer and synthesizer features in supervised fashion, using human judgements of synthetic voice quality on a sample of the paraphrases representative of the generator’s capability. Results from a cross-validation study indicate that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average, ameliorating the problem of highly variable synthesis quality typically encountered with today’s unit selection synthesizers.	ca-realizer;cross-validation (statistics);dialog system;dialog tree;discriminative model;speech synthesis;supervised learning;synthetic intelligence;eric	Crystal Nakatsu;Michael White	2006			natural language processing;speech recognition;phonation;computer science;linguistics;cross-validation	NLP	-18.777121355354815	-84.65975711797812	27857
94d3a0c77b6ceaab39f9b741f6f8e3e4bfebcc0d	representing and integrating linguistic knowledge	linguistic knowledge	This paper describes a theory of the representation and use of linguistic knowledge in a natural language understanding system. The representation system draws much of its insight from the linguistic theory of Fillmore el al. (1988). This models knowledge of language as a large collection of grammatical constructions, each a description of a linguistic regularity. I describe a representation language for constructions, and principles for encoding linguistic knowledge in this representation. The second part of the theory is a conceptual analyzer which is designed to model the on-line nature of the hunmn language understanding mechanism. I discuss the core of this analyzer, an information-combining operation called integration which combines constructions to produce complete interpretations of an utterance.	knowledge representation and reasoning;natural language understanding;online and offline;syntax (logic);theory;unification (computer science)	Daniel Jurafsky	1990		10.3115/997939.997974	natural language processing;artificial intelligence;computer science	NLP	-32.58739231868722	-80.48599303896421	28002
e67c31656b09e5ad4553daaf7309a2cf5ef7ad3b	compression of small text files using syllables	compression algorithm;word decomposition;mathematics;text file compression;word based compression methods;data compression;huffman coding;compression algorithms;lossless compression;text compression;natural languages;testing;huffman codes;physics;adaptive coding;english documents;block sorting lossless compression;natural languages mathematics physics huffman coding testing data compression writing encoding compression algorithms adaptive coding;writing;syllable based compression algorithms;block sorting lossless compression text file compression huffman coding syllable based compression algorithms english documents word based compression methods word decomposition;encoding;huffman codes data compression file organisation;file systems;file organisation	Summary form only given. We adapted well-known algorithms of adaptive Huffman coding and LZW to use syllables and words instead of characters for text compression. We tested the algorithms on collections of small or middle-sized files. Using syllable-based compression algorithms on English documents gives expected results: they outperform character-based and are outperformed by word-based versions of the same algorithm. According our tests both syllable- and word-based compression methods are sensitive to initial setting of their dictionaries. The decomposition of words into syllables is not trivial and is language dependent. An open issue is the applicability of syllable-based compression for different languages (like German, Rusian, or Hungarian) and its use in conjunction with other algorithms like block-sorting lossless compression	adaptive huffman coding;algorithm;burrows–wheeler transform;data compression;dictionary;lempel–ziv–welch;lossless compression;sorting;syllable;text-based (computing)	Jan Lansky;Michal Zemlicka	2006	Data Compression Conference (DCC'06)	10.1109/DCC.2006.16	data compression;natural language processing;speech recognition;computer science;theoretical computer science;mathematics;lossless compression;statistics	Theory	-22.966932976739535	-81.89942243451127	28014
babecbb967a89955b87ecf05bc85ca54bba94f28	a single case study of articulatory adaptation during acoustic mimicry	p philology linguistics	The distribution of fine-grained phonetic variation can be observed in the speech of members of welldefined social groups. It is evident that such variation must somehow be able to propagate through a speech community from speaker to hearer. However, technological barriers have meant that close and direct study of the articulatory links of this speaker-hearer chain has not, to date, been possible. We present the results of a singlecase study using an ultrasound-based method to investigate temporal and configurational lingual adaptation during mimicry. Our study focuses on allophonic variants of postvocalic /r/ found in speech from Central Scotland. Our results show that our informant was able to adjust tongue gesture timing towards that of the stimulus, but did not alter tongue configuration.	acoustic cryptanalysis	Eleanor Lawson;James M. Scobbie;Jane Stuart-Smith	2011			psychology;speech recognition;acoustics;communication	HCI	-10.588470839801282	-81.42672844547847	28055
d915a70b713d9a62f845a9ad1d9a48649161a1dd	spectral mapping onto probabilistic domain using neural networks and its application to speaker adaptive phoneme recognition	neural network		artificial neural network	Tetsunori Kobayashi;Katsuhiko Shirai	1992			speech recognition;pattern recognition;artificial intelligence;probabilistic logic;computer science;artificial neural network;time delay neural network;speaker recognition	ML	-15.637545749093913	-87.70664885209362	28072
d6c5ed36d894e626d053f2bcea67239a3fc47f4d	coupled hidden markov model for automatic ecg and pcg segmentation		Automatic and simultaneous electrocardiogram (ECG) and phonocardiogram (PCG) segmentation is a good example of current challenges when designing multi-channel decision support systems for healthcare. In this paper, we implemented and tested a Montazeri coupled hidden Markov model (CHMM), where two HMM's cooperate to recreate the “true” state sequence. To evaluate its performance, we tested different settings (two fully connected and two partially connected channels) on a real dataset annotated by an expert. The fully connected model achieved 71% of positive predictability (P+) on the ECG channel and 67% of P+ on the PCG channel. The partially connected model achieved 90% of P+ on the ECG channel and 80% of P+ in the PCG channel. These results validate the potential of our approach for real world multichannel application systems.	decision support system;hidden markov model;markov chain	Jorge Oliveira;Catarina Sousa;Miguel Tavares Coimbra	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952311	phonocardiogram;artificial intelligence;decision support system;predictability;pattern recognition;hidden markov model;computer science;machine learning;segmentation;communication channel	Robotics	-16.396063025052076	-88.99112623639742	28133
3c708d5abdbecca97ee966fbbf26a837977d808b	applying multi- and cross-lingual stochastic phone space transformations to non-native speech recognition	probability;matrix algebra;iterative methods;statistical distributions;universal phone set;hidden markov models;multilingual acoustic modeling;multilingual acoustic modeling non native speech recognition universal phone set;speech recognition;statistical distributions expectation maximisation algorithm hidden markov models iterative methods matrix algebra probability speech recognition;knowledge based mapping techniques cross lingual stochastic phone space transformations nonnative speech recognition multilingual stochastic phone space transformations hybrid hmm mlp automatic speech recognition phone hmm state posterior probability destination phone posterior probability stochastic matrix transformation mlp adaptation mllr adaptation mapping posterior distributions kullback leibler based cost function iterative em algorithm nonnative english database hiwire triphone mapping universal phone posteriors multilingual data asr system;non native speech recognition;expectation maximisation algorithm	In the context of hybrid HMM/MLP Automatic Speech Recognition (ASR), this paper describes an investigation into a new type of stochastic phone space transformation, which maps “source” phone (or phone HMM state) posterior probabilities (as obtained at the output of a Multilayer Perceptron/MLP) into “destination” phone (HMM phone state) posterior probabilities. The resulting stochastic matrix transformation can be used within the same language to automatically adapt to different phone formats (e.g., IPA) or across languages. Additionally, as shown here, it can also be applied successfully to non-native speech recognition. In the same spirit as MLLR adaptation, or MLP adaptation, the approach proposed here is directly mapping posterior distributions, and is trained by optimizing on a small amount of adaptation data a Kullback-Leibler based cost function, along a modified version of an iterative EM algorithm. On a non-native English database (HIWIRE), and comparing with multiple setups (monophone and triphone mapping, MLLR adaptation) we show that the resulting posterior mapping yields state-of-the-art results using very limited amounts of adaptation data in mono-, cross- and multi-lingual setups. We also show that “universal” phone posteriors, trained on a large amount of multilingual data, can be transformed to English phone posteriors, resulting in an ASR system that significantly outperforms a system trained on English data only. Finally, we demonstrate that the proposed approach outperforms alternative data-driven, as well as a knowledge-based, mapping techniques.	acoustic cryptanalysis;automated system recovery;expectation–maximization algorithm;heart rate variability;hidden markov model;iterative method;knowledge-based systems;kullback–leibler divergence;loss function;memory-level parallelism;multilayer perceptron;occam's razor;speech recognition;stochastic matrix;transformation matrix;triphone	David Imseng;Hervé Bourlard;John Dines;Philip N. Garner;Mathew Magimai-Doss	2013	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2013.2260150	probability distribution;speech recognition;computer science;machine learning;pattern recognition;probability;mathematics;iterative method;hidden markov model;statistics	ML	-19.02704970354868	-91.8989650936723	28169
747c34de09f822979f25c8f728d557e04c878a79	techniques for accurate automatic annotation of speech waveforms		We describe techniques used in the development of an automatic annotation system for use with a concatenative text-to-speech synthesis system. The goal of the system is to generate automatically from word-level transcriptions annotations that result in synthetic speech of the same quality as that produced from hand-labelled speech. Our approach in this work has been to use the standard technique of “forced-alignment” to each utterance and to refine both acoustic and pronunciation modelling to achieve greater alignment accuracy. Acoustic models were improved by Bayesian speaker adaptation and the use of confidence measures from N-Best decodings to produce speaker dependent HMMs. Pronunciation modelling improvements involved the use of a large pronunciation dictionary containing multiple pronunciations for many words, pronunciation probabilities, the accommodation of interword silences and using information derived from existing manual annotations to guide the recogniser during decoding. At present, the system can reliably produce time-aligned phonetic alignments for UK accents in which the automatic and manual alignments agree on the segmental labelling 93% of the time. It places boundaries with an r.m.s. error of 14.5 ms from the manual boundary. Subjectively, speech produced using automatic alignments is highly intelligible if not quite as good as that produced from manual alignments.	acoustic cryptanalysis;bayesian network;dictionary;speech synthesis;synthetic intelligence	Stephen Cox;Richard Brady;Peter Jackson	1998			speech recognition;automatic image annotation;image retrieval;waveform;computer science;annotation	NLP	-20.837675996634236	-83.66139055335573	28301
005b5d7c05e40f6bedf1ad87762776bfb7121d0b	drum transcription with non-negative spectrogram factorisation	time varying;instruments;support vector machines;spectrogram;training;spectrum;tut;tin;music	This paper describes a novel method for the automatic transcription of drum sequences. The method is based on separating the target drum sounds from the input signal using non-negative matrix factorisation, and on detecting sound onsets from the separated signals. The separation algorithm factorises the spectrogram of the input signal into a sum of instrument spectrograms, each having a fixed spectrum and a time-varying gain. The spectra are calculated from a set of training signals, and the time-varying gains are estimated with an algorithm stemming from non-negative matrix factorisation. Onset times of the instruments are detected from the estimated time-varying gains. The system gave better results than two state-of-the-art methods in simulations with acoustic signals containing polyphonic drum sequences, and overall hit rate of 96% was accomplished. Demonstrational signals are available at http://www.cs.tut.fi/~paulus/demo/.	acoustic cryptanalysis;algorithm;monoid factorisation;onset (audio);sensor;simulation;spectrogram;stemming;transcription (software)	Jouni Paulus;Tuomas Virtanen	2005	2005 13th European Signal Processing Conference		speech recognition;acoustics;engineering;communication	Comp.	-10.161193525293722	-91.25996720029768	28325
668234bdb228e64d61ac5caff8dd947db2cfb935	air-writing characters modelling and recognition on modified chmm	databases;training;hidden markov models;viterbi algorithm;writing;atmospheric modeling;character recognition	In this paper, we provide two databases DB1 and DB2 of motion characters written in the air, and present an accelerometers and gyroscopes based air-writing characters recognition system. The DB1 of 10 characters was collected by 40 subjects without writing constraints, while DB2 of 36 characters was collected by 49 participants in constrained stroke orders. We preprocessed the raw data with Moving Average filter and Z-score normalization, then utilized a modified CHMM for motion characters modeling and the Viterbi algorithm for recognition. We utilized an ASD rule for states assignment and managed to avoid the underflow issue during training. We analyzed the effect from limitations in writing. Results show that our system can realize real-time use and achieve high accuracy in both the mixed-user and the user-independent style.	angularjs;arithmetic underflow;database;real-time clock;velocity (software development);viterbi algorithm	Songbin Xu;Yang Xue	2016	2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2016.7844452	atmospheric model;speech recognition;viterbi algorithm;computer science;artificial intelligence;machine learning;writing;hidden markov model	Robotics	-15.91810809711658	-82.72913710327562	28397
b7a89d3554354243d067bcbf4c2e20d6bdd9ba13	learning pronunciation from a foreign language in speech synthesis networks		Although there are more than 65,000 languages in the world, the pronunciations of many phonemes sound similar across the languages. When people learn a foreign language, their pronunciation often reflect their native language’s characteristics. That motivates us to investigate how the speech synthesis network learns the pronunciation when multi-lingual dataset is given. In this study, we train the speech synthesis network bilingually in English and Korean, and analyze how the network learns the relations of phoneme pronunciation between the languages. Our experimental result shows that the learned phoneme embedding vectors are located closer if their pronunciations are similar across the languages. Based on the result, we also show that it is possible to train networks that synthesize English speaker’s Korean speech and vice versa. In another experiment, we train the network with limited amount of English dataset and large Korean dataset, and analyze the required amount of dataset to train a resource-poor language with the help of resource-rich languages.	speech synthesis	Y. Lee;T. Kim	2018	CoRR			NLP	-18.269324549276565	-81.34355047037396	28455
beb2a1a9b8c06be839a05a63a85c34ff904549b9	comparative study on corpora for speech translation	corpus based approach;bilingual parallel corpus;bilingualism;traduccion automatica;bootstrap;enfoque basado en corpus;estudio comparativo;speech processing;tratamiento palabra;language translation;traitement parole;conversacion;tratamiento lenguaje;approche basee sur corpus;natural languages;bilinguisme;bilingual experts;in domain utterances;etude comparative;speech translation;traduction automatique;corpus;language processing;bilinguismo;spoken dialog corpus machine translation speech translation;natural languages humans speech recognition global communication strontium speech synthesis parameter estimation state estimation communications technology laboratories;dialog speech;traitement langage;corpora;comparative study;conversation;travel domain;in domain utterances speech to speech translation corpora dialog speech bilingual experts conversational style texts travel domain bilingual parallel corpus;speech to speech translation;speech processing language translation;spoken dialog;conversational style texts;machine translation;automatic translation	This paper investigates issues in preparing corpora for developing speech-to-speech translation (S2ST). It is impractical to create a broad-coverage parallel corpus only from dialog speech. An alternative approach is to have bilingual experts write conversational-style texts in the target domain, with translations. There is, however, a risk of losing fidelity to the actual utterances. This paper focuses on balancing a tradeoff between these two kinds of corpora through the analysis of two newly developed corpora in the travel domain: a bilingual parallel corpus with 420 K utterances and a collection of in-domain dialogs using actual S2ST systems. We found that the first corpus is effective for covering utterances in the second corpus if complimented with a small number of utterances taken from monolingual dialogs. We also found that characteristics of in-domain utterances become closer to those of the first corpus when more restrictive conditions and instructions to speakers are given. These results suggest the possibility of a bootstrap-style of development of corpora and S2ST systems, where an initial S2ST system is developed with parallel texts, and is then gradually improved with in-domain utterances collected by the system as restrictions are relaxed	acoustic cryptanalysis;automatic target recognition;bleu;in the beginning... was the command line;mad;machine translation;parallel text;pull printing;risk assessment;speech synthesis;spontaneous order;test set;text corpus;tomb raider: anniversary;dialog	Gen-ichiro Kikui;Seiichi Yamamoto;Toshiyuki Takezawa;Eiichiro Sumita	2006	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2006.878262	natural language processing;speech recognition;computer science;speech processing;text corpus;linguistics;machine translation	NLP	-23.057907120419614	-86.17284306294385	28480
99f07e194197a55fd017657d4cd1a8d9c349de05	speech synthesis for mixed-language navigation instructions		Text-to-Speech (TTS) systems that can read navigation instructions are one of the most widely used speech interfaces today. Text in the navigation domain may contain named entities such as location names that are not in the language that the TTS database is recorded in. Moreover, named entities can be compound words where individual lexical items belong to different languages. These named entities may be transliterated into the script that the TTS system is trained on. This may result in incorrect pronunciation rules being used for such words. We describe experiments to extend our previous work in generating code-mixed speech to synthesize navigation instructions, with a mixed-lingual TTS system. We conduct subjective listening tests with two sets of users, one being students who are native speakers of an Indian language and very proficient in English, and the other being drivers with low English literacy, but familiarity with location names. We find that in both sets of users, there is a significant preference for our proposed system over a baseline system that synthesizes instructions in English.	baseline (configuration management);device driver;experiment;named entity;named-entity recognition;netware file system;speech synthesis	Khyathi Raghavi Chandu;Sai Krishna Rallabandi;Sunayana Sitaram;Alan W. Black	2017			speech recognition;mixed language;computer science;speech synthesis	NLP	-26.892614109572595	-83.69155521529301	28547
c1b5cc9480670e7604bf8097fa26f8fe3904dd70	hidden markov models applied to on-line handwritten isolated character recognition	optimisation;maximum likelihood estimation hidden markov models character recognition optimisation;modelo markov;maximum likelihood;caracter manuscrito;taux erreur;hidden markov model;manuscript character;maximum vraisemblance;hidden markov models character recognition image restoration signal processing algorithms speech processing filters signal restoration signal processing user interfaces noise level;maximum likelihood estimation;baum welch;maximum likelihood classification;markov model;reconnaissance caractere;hidden markov models;hmm hidden markov models handwritten isolated character recognition baum welch optimization routine maximum likelihood classification average error rate lowercase english alphabet;error rate;modele markov;indice error;caractere manuscrit;character recognition;maxima verosimilitud;reconocimiento caracter	Hidden Markov models are used to model the generation of handwritten, isolated characters. Models are trained on examples using the Baum-Welch optimization routine. Then, given the models for the alphabet, unknown characters can be classified using maximum-likelihood classification. Experiments have been conducted, and an average error rate of 6.9% was achieved over the alphabet consisting of the lowercase English alphabet.	alphabet;baum–welch algorithm;bit error rate;classification;experiment;hidden markov model;markov chain;mathematical optimization;online and offline;optical character recognition;personality character;welch's method	Stephan R. Veltman;Ramjee Prasad	1994	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.287027	speech recognition;computer science;pattern recognition;mathematics;maximum likelihood;hidden markov model;statistics	Vision	-20.078404109080143	-91.64267450672727	28638
31783927e1c95000a6025954eaa69f1b9d186392	stochastically-based semantic analysis for machine translation	communication homme machine;human interaction;traitement automatique de la parole;comprehension du langage;stochastic method;spoken language systems;anglais langue source;traduction automatique;automatic speech processing;natural language;mechanical translation;computational linguistics;information service;spontaneous speech;analyse semantique;linguistique informatique;modele stochastique;human machine interaction;machine translation;semantic analysis	We report our experience of applying a stochastic method for understanding natural language to a multilingual appointment scheduling task, in particular, to the English spontaneous speech task (ESST). The aim of the spoken language systems developed for this task is to translate spontaneous conversational speech among different languages. We have investigated the portability of a stochastic semantic analyser from a setting of human–machine interactions air travel information services (ATIS) and multimodal multimedia automated service kiosk (MASK) into the more open one of human-to-human interactions (ESST). c © 1999 Academic Press	automatic transmitter identification system (television);machine translation;multimodal interaction;natural language;scheduling (computing);software portability;spontaneous order	Wolfgang Minker;Marsal Gavaldà;Alexander H. Waibel	1999	Computer Speech & Language	10.1006/csla.1999.0119	natural language processing;interpersonal relationship;speech recognition;computer science;computational linguistics;linguistics;machine translation;natural language	NLP	-28.78140127718136	-82.18736535862689	28680
f09d43087ce40f92b5576e11b2618ab210645c09	pitch-frequency histogram-based music information retrieval for turkish music	busqueda informacion;medicion automatica;high dimensionality;pitch acoustics;information retrieval;tonie;automatic measurement;mesure automatique;carta de datos;turkish music;haute frequence;histogram;automatic recognition;non western music;histogramme;western music;recherche information;automatic tonic detection;mappage;altura sonida;music information retrieval;signal acoustique;mapping;acoustic signal;alta frecuencia;histograma;high frequency;senal acustica;reconocimiento automatico;reconnaissance automatique;automatic makam recognition	This study reviews the use of pitch histograms in music information retrieval studies for western and non-western music. The problems in applying the pitch-class histogrambased methods developed for western music to non-western music and specifically to Turkish music are discussed in detail. The main problems are the assumptions used to reduce the dimension of the pitch histogram space, such as, mapping to a low and fixed dimensional pitch-class space, the hard-coded use of western music theory, the use of the standard diapason (A4 1⁄4 440 Hz), analysis based on tonality and tempered tuning. We argue that it is more appropriate to use higher dimensional pitch-frequency histograms without such assumptions for Turkish music. We show in two applications, automatic tonic detection and makam recognition, that high dimensional pitchfrequency histogram representations can be successfully used in Music Information Retrieval (MIR) applications without such pre-assumptions, using the data-driven	hard coding;information retrieval;pitch (music);pitch correction	Ali Cenk Gedik;Baris Bozkurt	2010	Signal Processing	10.1016/j.sigpro.2009.06.017	turkish music;speech recognition;computer science;high frequency;histogram;mathematics;statistics	Web+IR	-8.881858014388397	-91.76306825804318	28683
9c6bfc2899e8307c2fbd8789fd85a78452227671	inferential searches of knowledge networks as an approach to extensible language understanding systems	knowledge network;language understanding;noun;production system;semantic network;context free grammar;part of speech	A program has been constructed that implements some inference schemes for inferring the meaning of noun-noun, adjective-noun, and agent-verb-object phrases from the constituent words. Dictionary definitions of words are input directly into a semantic network using a context free grammar implemented in a production system. To interpret a novel phrase (e.g., lawn mower, car tire, car wheel) semantically, an intersection search originating from the nodes representing the constituent words is performed in the semantic network. If an intersection is found, a meaning for the phrase is produced; otherwise the phrase is not interpreted. During this interpretation, only knowledge provided by dictionary definitions and parts of speech of words is used.	context-free grammar;dictionary;extensible programming;inferential theory of learning;natural language understanding;production system (computer science);robotic lawn mower;semantic network	David D. McDonald;Frederick Hayes-Roth	1977	SIGART Newsletter	10.1145/1045343.1045384	natural language processing;noun;noun phrase;speech recognition;part of speech;computer science;phrase structure rules;determiner phrase;linguistics;production system;semantic network;context-free grammar;programming language;stop words	NLP	-31.554011959918665	-80.83898776009235	28811
c46e9ed9794fa895e522e729f21731916544fc67	query-by-example retrieval via fast sequential dynamic time warping algorithm	sequential dtw;query by example retrieving;speech recognition dynamic programming matrix algebra query processing;speech accuracy acoustics eigenvalues and eigenfunctions software algorithms heuristic algorithms feature extraction;wfs dtw algorithm query by example retrieval sequential dynamic time warping algorithm qbe retrieval spoken term detection std dynamic programming algorithm step forward moving strategy distance matrix linear time aligned accumulated distance weighted cumulative distance score parameter weighted fast sequential dtw algorithm pca based silence discriminator term weighted value twv pardat1 corpus;gmm based poste riorgrams;sequential dtw query by example retrieving gmm based poste riorgrams	We introduce a novel approach to Query-by-Example (QbE) retrieval, utilizing fundamental principles of posteriorgram-based Spoken Term Detection (STD), in this paper. Proposed approach is a kind of modification of widely used seg-mental variant of dynamic programming algorithm. Our solution represents sequential variant of DTW algorithm, employing one step forward moving strategy. Each DTW search is carried out sequentially, block by block, where each block represents squared input distance matrix, with size equal to the length of retrieved query. We also examine a way how to speed up sequential DTW algorithm without considerable loss in retrieving performance, by implementing linear time-aligned accumulated distance. The increase of detection accuracy is ensured by weighted cumulative distance score parameter. Therefore, we called this approach Weighted Fast Sequential - DTW (WFS-DTW) algorithm. A novel PCA-based silence discriminator is used along with this algorithm. Evaluation of proposed algorithm is carried out on ParDat1 corpus, using Term Weighted Value (TWV).	algorithm;discriminator;distance matrix;dynamic programming;dynamic time warping;query by example;time complexity	Jozef Vavrek;Peter Viszlay;Eva Kiktová-Vozarikova;Martin Lojka;Jozef Juhár;Anton Cizmar	2015	2015 38th International Conference on Telecommunications and Signal Processing (TSP)	10.1109/TSP.2015.7296440	speech recognition;computer science;machine learning;dynamic time warping;pattern recognition	Vision	-7.635484541977377	-94.01150320031225	28843
73c58e32bc8fa240ced8147970666417734fa5ca	information theoretic analysis of direct articulatory measurements for phonetic discrimination	direct articulatory measurements;electromagnetic measurements;analysis of articulatory measurements;frequency domain analysis;signal representations;signal analysis;speech processing;speech analysis;information theoretic analysis;speech;electromagnetic analysis;information analysis electromagnetic measurements speech analysis mutual information electromagnetic analysis signal analysis signal representations time domain analysis frequency domain analysis signal processing;speech production signal analysis;signal representation schemes;time domain analysis;non parametric mutual information estimation;automatic speech recognition;phonetic discrimination;time domain analysis frequency domain analysis information theory speech speech processing;signal processing;signal representation;articulatory signals information theoretic analysis direct articulatory measurements phonetic discrimination speech production signal analysis signal representation schemes time domain analysis frequency domain analysis phonological classification;mutual information;phonological classification;articulatory signals;frequency domain;information theoretic;information analysis;speech production;information theory;phonological classification analysis of articulatory measurements automatic speech recognition non parametric mutual information estimation	This paper focuses on the analysis of speech production signals (physical measurements from electromagnetic articulograph) from the perspective of phone discrimination. We explore two different signal representation schemes for the articulatory signals, one based on time-domain analysis and the other based on frequency domain. We quantify the amount of discrimination information offered by the speech production signals in identifying the phone labels through mutual information. Mutual information analyses establish that substantial discrimination information is present in the articulatory stream. Furthermore, phonological classification results with articulatory signals indicate higher accuracy compared to the acoustic signal.	acoustic cryptanalysis;domain analysis;kullback–leibler divergence;mutual information;theory	Jorge F. Silva;Vivek Kumar Rangarajan Sridhar;Viktor Rozgic;Shrikanth (Shri) Narayanan	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.366948	natural language processing;speech recognition;computer science;signal processing;mathematics;frequency domain;statistics	Visualization	-8.796789596208516	-90.01498042351342	28910
0e0275b1f31e8606a55dfd95eff165cdd6d2d031	text alignment from bimodal mathematical expression sources	handwritten mathematical recognition;text analysis handwritten character recognition speech recognition string matching;handwriting recognition;mer text alignment bimodal mathematical expression sources handwriting based mathematical expression recognition nbest mathematical expressions audio signal automatic speech recognition system string comparison algorithm hamex dataset;speech and hadwriting merging;bimodal documents	In this paper we propose a new approach to merge mathematical expression recognition results coming from handwriting and speech modalities. Using a bimodal description of mathematical expressions allows taking advantage of the complementarities between both signals, and can disambiguate situations were a single modality would not be clear enough. To combine the signals coming from both modalities, we propose to represent them in the same space as a textual description. First, from the handwriting signal, we generate the Nbest mathematical expressions, each of them is next translated as different possible strings. From the audio signal, an automatic speech recognition system provides a transcript, which is also available as a string. A string comparison algorithm is achieved to select the best mathematical expressions. This bimodal system is evaluated on real bimodal data from the HAMEX dataset and the results are compared to a single modality (handwriting) based system.	algorithm;comparison of programming languages (string functions);complementarity theory;modality (human–computer interaction);speech recognition	Sofiane Medjkoune;Harold Mouchère;Christian Viard-Gaudin;Simon Petit-Renaud	2014	2014 14th International Conference on Frontiers in Handwriting Recognition	10.1109/ICFHR.2014.42	natural language processing;speech recognition;intelligent character recognition;computer science;machine learning;pattern recognition;handwriting recognition	Vision	-21.565721535326848	-83.10926604987247	28997
67e7598c5aed8cde2d885d7c91eedf37d00461b1	improving deep neural networks using state projection vectors of subspace gaussian mixture model as features	bottleneck features;deep neural network;sgmm;state specific vectors	Recent advancement in deep neural network (DNN) has surpassed the conventional hidden Markov model-Gaussian mixture model (HMM-GMM) framework due to its efficient training procedure. Providing better phonetic context information in the input gives improved performance for DNN. The state projection vectors (state specific vectors) in subspace Gaussian mixture model (SGMM) captures the phonetic information in low dimensional vector space. In this paper, we propose to use state specific vectors of SGMM as features thereby providing additional phonetic information for the DNN framework. To each observation vector in the train data, the corresponding state specific vectors of SGMM are aligned to form the state specific vector feature set. Linear discriminant analysis (LDA) feature set are formed by applying LDA to the training data. Since bottleneck features are efficient in extracting useful discriminative information for the phonemes, LDA feature set and state specific vector feature set are converted to bottleneck features. These bottleneck features of both feature sets act as input features to train a single DNN framework. Relative improvement of 8.8% for TIMIT database (core test set) and 9.7% for WSJ corpus is obtained by using the state specific vector bottleneck feature set when compared to the DNN trained only with LDA bottleneck feature set. Also training Deep belief network - DNN (DBN-DNN) using the proposed feature set attains a WER of 20.46% on TIMIT core test set proving the effectiveness of our method. The state specific vectors while acting as features, provide additional useful information related to phoneme variation. Thus by combining it with LDA bottleneck features improved performance is obtained using the DNN framework.	artificial neural network;bayesian network;deep belief network;deep learning;hidden markov model;linear discriminant analysis;markov chain;subspace gaussian mixture model;timit;test set;the wall street journal;word error rate	B MuraliKarthick;S. Umesh	2014	2014 IEEE Spoken Language Technology Workshop (SLT)	10.1109/SLT.2014.7078562	speech recognition;computer science;machine learning;pattern recognition	ML	-17.714622407738165	-89.12674746234013	29006
2376c3ad5c6ece52aae3f3da726e50113629a9ad	easy fuzzy tool for emotion recognition - prototype from voice speech analysis	speech analysis;emotion recognition	In human beings relations it is very important dealing with emotions. Most people is able to deduce the emotion of one person just listening his/her speech. Voice speech characteristics can help us to identify people emotions. Emotion recognition is a very interesting field in modern science and technology but to automate it is not an easy task. Many researchers and engineers are working to recognize this prospective field but the difficulty is that emotions are not clear. They are not a crisp topic. In this paper we propose to use fuzzy reasoning for emotion recognition. We based our work in some previous studies about the specific characteristics of voice speech for each human emotion (speech rate, pitch average, intensity and voice quality). We provide a simple an useful prototype that implements emotion recognition using a fuzzy model. We have used RFuzzy (a fuzzy logic reasoner over a Prolog compiler) and we have obtained a simple and efficient prototype that is able to identify the emotion of a person from his/her voice speech characteristics. We are trying to recognize sadness, happiness, anger, excitement and plain emotion. We have made some experiments and we provide the results that are 90% successful in the identification of emotions. Our tool is constructive, so it can be used not only to identify emotions automatically but also to recognize the people that have an emotion through their different speeches. Our prototype analyzes an emotional speech and obtains the percentage of each emotion that is detected. So it can provide many constructive answers according to our queries demand. Our prototype is an easy tool for emotion recognition that can be modify and improved by adding new rules from speech and face analysis.	compiler;emotion recognition;experiment;fuzzy logic;prolog;prospective search;prototype;sadness;semantic reasoner;speech recognition;speech synthesis;voice analysis	Mahfuza Farooque;Susana Muñoz-Hernández	2009			speech technology;speaker recognition;speech recognition;voice analysis;communication	NLP	-15.490563732729528	-80.27050311321871	29249
29fca5a08ff919cf233bc39e89db68bbc842ac19	how to judge reusability of existing speech corpora for target task by utilizing statistical multidimensional scaling	acoustic model;reusability;statistical mds;multidimensional scaling;task dependency	In order to develop a target speech recognition system with less cost of time and money, reusability of existing speech corpora is becoming one of the most important issues. This paper proposes a new technique to judge the reusab】lity of existing speech co中ora for a target task by utilizing a statistical multidimensional scaling method. In an experiment using twelve tasks in five speech corpora, our proposed method could show high correlation to the cross task recognition performance and judge the reusability of existing speech co叩ora correctly for th巴 target task with low巴r cost	image scaling;multidimensional scaling;speech recognition;speech synthesis;text corpus	Goshu Nagino;Makoto Shozakai;Kiyohiro Shikano	2007			natural language processing;reusability;speech recognition;multidimensional scaling;computer science;machine learning;acoustic model	NLP	-20.219508704634773	-87.17267903500316	29397
1d6cd4e748dee40e575b71c8a9dcee89d364ea30	pitch adaptive training for hmm-based singing voice synthesis	voice equipment hidden markov models learning artificial intelligence speech synthesis;hidden markov models training adaptation models speech training data speech synthesis data models;pitch adaptive training singing voice synthesis hidden markov model speaker adaptive training;speech synthesis speaker integration pitch adaptive training statistical parametric approach hidden markov model context dependent modeled waveform generation hmm based singing voice synthesis system corpus based system singer adaptive training data sparseness problem	A statistical parametric approach to singing voice synthesis based on hidden Markov Models (HMMs) has been growing in popularity over the last few years. The spectrum, excitation, vibrato, and duration of singing voices in this approach are simultaneously modeled with context-dependent HMMs and waveforms are generated from the HMMs themselves. HMM-based singing voice synthesis systems are heavily based on the training data in performance because these systems are “corpus-based.” Therefore, HMMs corresponding to contextual factors that hardly ever appear in the training data cannot be well-trained. Pitch should especially be correctly covered since generated F0 trajectories have a great impact on the subjective quality of synthesized singing voices. We applied the method of “speaker adaptive training” (SAT) to “pitch adaptive training,” which is discussed in this paper. This technique made it possible to normalize pitch based on musical notes in the training process. The experimental results demonstrated that the proposed technique could alleviate the data sparseness problem.	context-sensitive language;database normalization;hidden markov model;markov chain;neural coding;pitch (music);speech synthesis;text corpus	Kanako Shirota;Kazuhiro Nakamura;Kei Hashimoto;Keiichiro Oura;Yoshihiko Nankaku;Keiichi Tokuda	2012	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854062	speech recognition	Robotics	-17.195562223678593	-85.13692315667727	29448
7cb16aca4d408552f5841410bccc3fe9a5ed2ae8	improved mandarin keyword spotting using confusion garbage model	similar pronunciation words;speech recognition hidden markov models natural language processing;decoding;hmm based cm method;hidden markov models mandarin keyword spotting confusion garbage model acoustic keyword spotting mandarin conversational speech chinese characters false alarm rate acoustic kws method hmm based confidence measure method hmm based cm method conversational telephone dataset speech recognition similar pronunciation words;confidence measure;acoustics;acoustic kws method;keyword spotting;vocabulary;false alarm rate;speech;confidence measure keyword spotting similar pronunciation confusion garbage model;mandarin conversational speech;computational modeling;hidden markov models;mandarin keyword spotting;chinese characters;confusion garbage model;hidden markov models speech speech recognition computational modeling acoustics decoding vocabulary;speech recognition;hmm based confidence measure method;conversational telephone dataset;similar pronunciation;acoustic keyword spotting;natural language processing	This paper presents an improved acoustic keyword spotting (KWS) algorithm using a novel confusion garbage model in Mandarin conversational speech. Observing the KWS corpus, we found there are many words with similar pronunciation with predefined keywords, although they have different Chinese characters and different meanings, which easily result in high false alarm rate. In this paper, an improved acoustic KWS method with confusion garbage models was developed that absorbs similar pronunciation words confused with specific keywords for a given task. One obvious advantage of such method is that it provides a flexible framework to implement the selection procedure and reduce false alarm rate effectively for a specific task. The efficiency of the proposed architecture was evaluated under HMM-based confidence measures (CM) methods and demonstrated on a conversational telephone dataset.	acoustic cryptanalysis;algorithm;hidden markov model;speech recognition;super robot monkey team hyperforce go!;usability	Shilei Zhang;Zhiwei Shuang;Qin Shi;Yong Qin	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.901	natural language processing;speech recognition;computer science;speech;constant false alarm rate;computational model;hidden markov model	Robotics	-19.668086121899485	-85.41403836826905	29462
c682c7caef5afc3fd92942cddeb48de12a093812	microphone classification using fourier coefficients	feature vector;machine learning;fourier coefficient;media forensics;fft based microphone classification;logistic regression model	Media forensics tries to determine the originating device of a signal. We apply this paradigm to microphone forensics, determining the microphone model used to record a given audio sample. Our approach is to extract a Fourier coefficient histogram of near-silence segments of the recording as the feature vector and to use machine learning techniques for the classification. Our test goals are to determine whether attempting microphone forensics is indeed a sensible approach and which one of the six different classification techniques tested is the most suitable one for that task. The experimental results, achieved using two different FFT window sizes (256 and 2048 frequency coefficients) and nine different thresholds for near-silence detection, show a high accuracy of up to 93.5% correct classifications for the case of 2048 frequency coefficients in a test set of seven microphones classified with linear logistic regression models. This positive tendency motivates further experiments with larger test sets and further studies for microphone identification.	coefficient;experiment;fast fourier transform;feature vector;logistic regression;machine learning;microphone;programming paradigm;test set	Robert Buchholz;Christian Krätzer;Jana Dittmann	2009		10.1007/978-3-642-04431-1_17	speech recognition;feature vector;computer science;machine learning;pattern recognition;logistic regression;fourier series;statistics	ML	-8.772006642696347	-91.1069142067101	29640
46cf895e9996fcd213a889a0fbd6ab78ad070f6a	progress in nonlinear speech processing, workshop on nonlinear speech processing, wnsp 2005, heraklion, crete, greece, september 20-23, 2005	speech processing	A Review of Glottal Waveform Analysis.- Rahmonic Analysis of Signal Regularity in Synthesized and Human Voice.- Spectral Analysis of Speech Signals Using Chirp Group Delay.- Towards Neurocomputational Speech and Sound Processing.- Extraction of Speech-Relevant Information from Modulation Spectrograms.- On the Detection of Discontinuities in Concatenative Speech Synthesis.- Voice Disguise and Automatic Detection: Review and Perspectives.- Audio-visual Identity Verification: An Introductory Overview.- Text-Independent Speaker Verification: State of the Art and Challenges.- Nonlinear Predictive Models: Overview and Possibilities in Speaker Recognition.- SVMs for Automatic Speech Recognition: A Survey.- Nonlinear Speech Enhancement: An Overview.- The Amount of Information on Emotional States Conveyed by the Verbal and Nonverbal Channels: Some Perceptual Data.	nonlinear programming;nonlinear system;speech processing		2007		10.1007/978-3-540-71505-4	voice activity detection;speaker recognition;speech recognition;acoustics;computer science;voice analysis;speech processing;communication	NLP	-13.508422767391894	-87.54417317963853	29692
f29d9348ce38198ad270d440ba6a5e9f3fc4afb6	the application of semantic classification trees to natural language understanding	speech understanding system;debugging;lenguaje natural;puesta a punto programa;utterance semantic classification trees natural language understanding speech understanding system data structure semantic classification tree semantic rules learning decision trees speech recognizer;sct;linguistique;classification tree analysis training data robustness speech recognition automatic speech recognition buildings natural languages data mining tree data structures debugging;classification tree;decision tree;mise au point programme;learning;building block;semantic classification tree;langage naturel;semantic rules learning;semantics;tratamiento lenguaje;natural languages;utterance;tree data structures;arbol decision;rule learning;trees mathematics;data mining;semantica;semantique;classification;speech recognizer;semantic classification trees;aprendizaje;learning systems;training data;automatic speech recognition;apprentissage;linguistica;natural language understanding;machine learning;language processing;system design;computational linguistics speech recognition natural languages trees mathematics learning systems decision theory;natural language;estructura datos;decision theory;traitement langage;speech recognition;speech understanding;robustness;structure donnee;computational linguistics;classification tree analysis;decision trees;data structure;arbre decision;clasificacion;buildings;linguistics	This article describes a new method for building a natural language understanding (NLU) system, in which the system’s rules are learnt automatically from training data. The method has been applied to design of a speech understanding (SU) system. Designers of such systems rely increasingly on robust matchers to perform the task of extracting meaning from one or several word sequence hypotheses generated by a speech recognizer; a robust matcher processes semantically important islands of words and constituents rather than attempting to parse the entire word sequence. We describe a new data structure, the Semantic Classification Tree (XT), that learns semantic rules from training data and can be a building block for robust matchers for NLU tasks. By reducing the need for handcoding and debugging a large number of rules, this approach facilitates rapid construction of an NLU system. In the case of an SU system, the rules learned by an SCT are highly resistant to errors by the speaker or by the speech recognizer because they depend on a small number of words in each utterance. Our work shows that semantic rules can be learned automatically from training data, yielding successful NLU for a realistic application. Index Items-Speech understanding, semantic classification tree, SCT, machine learning, natural language, decision tree.	data structure;debugging;decision tree learning;finite-state machine;hand coding;machine learning;natural language understanding;parsing;speech recognition	Roland Kuhn;Renato De Mori	1995	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.391397	natural language processing;speech recognition;data structure;computer science;computational linguistics;machine learning;decision tree;semantics;natural language	ML	-22.68662478279338	-81.5704638562283	29737
45603837d78c3412d4278f0f57f5fb0ea0468d2a	using automatically-derived acoustic sub-word units in large vocabulary speech recognition	speech recognition	Although most parameters in a speech recognition system are estimated from data, the unit inventory and lexicon are generally hand crafted and therefore unlikely to be optimal. This paper describes a joint solution to the problems of learning a unit inventory and corresponding lexicon from data. The methodology, which requires multiple training tokens per word, is then extended to handle infrequently observed words using a hybrid system that combines automatically-derived units with phone-based units. The hybrid system outperforms a phone-based system in rst-pass decoding experiments on a large vocabulary conversational speech recognition task.	acoustic cryptanalysis;experiment;hybrid system;lexicon;speech recognition;vocabulary	Michiel Bacchiani;Mari Ostendorf	1998			speech recognition;voxforge;audio mining;acoustic model;speech corpus;speaker recognition;logogen model;speech processing;computer science;speech synthesis	NLP	-19.468743816355154	-86.89951335208805	29942
541ca1f7ab2caede8af274c8c4ab0e738f9c789a	polyphonic transcription based on temporal evolution of spectral similarity of gaussian mixture models	competitive strategy;gaussian mixture;instruments;speech;spectrum;gaussian mixture model;multiple signal classification;hidden markov models;estimation;music;fundamental frequency;harmonic analysis	This paper describes a system to transcribe multitimbral polyphonic music based on a joint multiple-F0 estimation. In a frame level, all possible fundamental frequency (F0) candidates are selected. Using a competitive strategy, a spectral envelope is estimated for each combination composed of F0 candidates under assumption that a polyphonic sound can be modeled by a sum of weighted gaussian mixture models (GMM). Since in polyphonic music the current spectral content depends to a large extent of the immediately previous one, the winner combination is determined taking into account the highest spectral similarity regarding to the past music events which has been selected from a set of combinations that minimize the current spectral distance between input-GMM spectrums. Our system was tested using several pieces of real-world music recordings from RWC Music Database. Evaluation shows encouraging results compared to a recent state-of-the-art method.	google map maker;mixture model;self-replicating machine;strategic management;transcription (software)	Francisco J. Cañadas-Quesada;Pedro Vera-Candeas;Nicolás Ruiz-Reyes;Julio J. Carabias-Orti	2009	2009 17th European Signal Processing Conference		speech recognition;acoustics;pattern recognition;mathematics	AI	-11.401041751249947	-92.97181383408902	29954
0667faea3c692f3a9344e47f532766584d9c837e	ap-stdp: a novel self-organizing mechanism for efficient reservoir computing	spoken english letter recognition self organizing mechanism reservoir computing liquid state machine lsm recurrent spiking neural networks randomly generated reservoir complex recurrent reservoir fixed reservoir temporal input signals internal network dynamics readout layer pattern classification computational tasks activity based probabilistic spiking timing dependent plastic ap stdp mechanism self organizing reservoirs stdp mechanisms synaptic memory principal component analysis pca stdp algorithms spoken english letters ti46 speech corpus reservoir size;self organising feature maps natural language processing pattern classification principal component analysis;reservoirs neurons probabilistic logic tuning hardware standards principal component analysis	The Liquid State Machine (LSM) exploits the computation capability of recurrent spiking neural networks by incorporating a randomly generated reservoir, which is often fixed. This standard choice relaxes the challenging need for training the complex recurrent reservoir. The fixed reservoir is used as a generic kernel to map the temporal input signals to the internal network dynamics, and a readout layer is trained to extract the information embedded in the network dynamics to facilitate pattern classification. However, the question of how to effectively tune the reservoir for given computational tasks remains to be answered. In this paper, we propose a novel Activity-based Probabilistic Spiking-Timing Dependent Plastic (AP-STDP) mechanism for self-organizing reservoirs. Compared to conventional STDP mechanisms, the proposed rule improves tuning efficiency, prevents the saturation of synaptic memory, and boosts performance. We assess the internal representation ability of the proposed self-organizing mechanism via principal component analysis (PCA) and show that the proposed method is advantageous over other STDP algorithms. Using the spoken English letters adopted from the TI46 speech corpus for performance benchmarking, we demonstrate that AP-STDP consistently outperforms other STDP mechanisms regardless of reservoir size, and is able to boost the performance of the isolated spoken English letter recognition by 2.7% with a small reservoir size.	ap computer science principles;algorithm;artificial neural network;computation;embedded system;intranet;liquid state machine;organizing (structure);principal component analysis;procedural generation;randomness;recurrent neural network;reservoir computing;self-organization;speech corpus;spiking neural network;synaptic package manager	Yingyezhe Jin;Peng Li	2016	2016 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2016.7727328	computer science;artificial intelligence;machine learning;reservoir computing	AI	-22.1811811103696	-95.05268003355052	30013
be88b2e2fd7e2016eae493a07ec0f1edf8b6a07a	domain mismatch compensation for speaker recognition using a library of whiteners	libraries;nist;speaker recognition compensation;speech;whitening channel compensation domain mismatch i vectors;speaker recognition;computational modeling;covariance matrices;speaker recognition i vector length normalization data whitening generalization dac13 domain adaptation challenge jhu johns hopkins university nist national institute of standards and technology speech utterance representation whitener library domain mismatch compensation;conferences;speaker recognition covariance matrices libraries speech nist conferences computational modeling	The development of the i-vector framework for generating low dimensional representations of speech utterances has led to considerable improvements in speaker recognition performance. Although these gains have been achieved in periodic National Institute of Standards and Technology (NIST) evaluations, the problem of domain mismatch, where the system development data and the application data are collected from different sources, remains a challenging one. The impact of domain mismatch was a focus of the Johns Hopkins University (JHU) 2013 speaker recognition workshop, where a domain adaptation challenge (DAC13) corpus was created to address this problem. This paper proposes an approach to domain mismatch compensation for applications where in-domain development data is assumed to be unavailable. The method is based on a generalization of data whitening used in association with i-vector length normalization and utilizes a library of whitening transforms trained at system development time using strictly out-of-domain data. The approach is evaluated on the 2013 domain adaptation challenge task and is shown to compare favorably to in-domain conventional whitening and to nuisance attribute projection (NAP) inter-dataset variability compensation.	cluster analysis;decorrelation;domain adaptation;heart rate variability;microphone;spatial variability;speaker recognition;whitening transformation	Elliot Singer;Douglas A. Reynolds	2015	IEEE Signal Processing Letters	10.1109/LSP.2015.2451591	natural language processing;speaker recognition;speaker diarisation;speech recognition;nist;telecommunications;computer science;speech;computational model	NLP	-17.169821835921372	-91.53209177736385	30014
29652a74eeb25541f2042f8c222a6c902bcdb4aa	formant re-synthesis of dysarthric speech		Dysarthria is a motor speech disorder that is often associated with irregular phonation (e.g. vocal fry) and amplitude, incoordination of articulators, and restricted movement of articulators, among other problems. The present study is part of a project on voice transformation systems for dysarthria, with the goal of producing intelligibility-enhanced speech. We report on a procedure in which formants and energies are estimated from dysarthric speech; next, these trajectories are modified to more closely approximate desired targets; finally, transformed speech is generated using formant synthesis. Results indicate that the transformation step enhances intelligibility, and that removal of vocal fry enhances perceived quality. However, the initial step of stylizing the formant trajectories results in a decrement in intelligibility, thereby reducing the net impact of the process.	approximation algorithm;increment and decrement operators;intelligibility (philosophy);speech synthesis	Alexander Kain;Xiaochuan Niu;John-Paul Hosom;Qi Miao;Jan P. H. van Santen	2004			formant;speech recognition;dysarthric speech;computer science	HCI	-9.58018695076827	-85.62374426781496	30178
3198cea77aa7fdb65d1bd6f251e8282a1ef7c7be	a step toward ai tools for quality control and musicological analysis of digitized analogue recordings: recognition of audio tape equalizations		Historical analogue audio documents are indissolubly linked to their physical carriers on which they are recorded. Because of their short life expectancy these documents have to be digitized. During this process, the document may be altered with the result that the digital copy is not reliable from the authenticity point of view. This happens because digitization process is not completely automatized and sometimes it is influenced by human subjective choices. Artificial intelligence can help operators to avoid errors, enhancing reliability and accuracy, and becoming the base for quality control tools. Furthermore, this kind of algorithms could be part of new instruments aiming to ease and to enrich musicological studies. This work focuses the attention on the equalization recognition problem in the audio tape recording field. The results presented in this paper, highlight that, using machine learning algorithms, is possible to recognize the pre-emphasis equalization used to record an audio tape.	algorithm;artificial intelligence;audio signal processing;document;emphasis (telecommunications);machine learning;point of view (computer hardware company)	Edoardo Micheloni;Niccolò Pretto;Sergio Canazza	2017			computer science	HCI	-6.453282442330823	-86.84899308065745	30274
9f38a134f6dd2a7adec946e6548071660221f3e1	on the generation and use of a segment dictionary for speech coding, synthesis and recognition	speech synthesis;acoustic distortion;temporal variability;speech processing;speech coding;bit rate;continuous speech recognition;linear predictive coding;dictionaries speech coding speech synthesis speech recognition linear predictive coding acoustic distortion bit rate pattern matching costs speech processing;pattern matching;dictionaries;speech recognition	A methodology is described to obtain a set of segments and rules that represents adequately the speech performance of a given speaker. This methodology proceeds from an initial set of diphones extracted from a neutral context and modify this set with larger and/or smaller segments depending on the match with natural utterances. Each segment is stored as a sequence of frames coded using LPC coefficients. An estimate of the likelihood of timescale distortion is associated with each frame. It represents knowledge on temporal variability that can be used by synthesis rules and/or pattern matching algorithms. It is then shown how such a segment data base can be used for 1) speech coding at very low bit rate ( ∼ 400 bit/sec), 2) synthesis from unrestricted text, 3) continuous speech recognition.	dictionary;speech coding	Gérard Chollet;J. F. Galliano;J.-P. Lefevre;E. Viara	1983		10.1109/ICASSP.1983.1172018	voice activity detection;speaker recognition;codec2;g.729;audio mining;linear predictive coding;speech recognition;vector sum excited linear prediction;harmonic vector excitation coding;computer science;pattern matching;speech coding;pattern recognition;speech processing;acoustic model;psqm;speech synthesis	NLP	-11.035135593396529	-88.91563462903599	30289
97a467e911071f93c2dbb9ce53a899650eb203e6	analysis and synthesis of fundamental frequency contours for the spoken dialogue in japanese				Keikichi Hirose;Mayumi Sakata;Masafumi Osame;Hiroya Fujisaki	1994			natural language processing;artificial intelligence;fundamental frequency;computer science;literature	NLP	-14.90071061811743	-85.23862980097981	30316
de9ba56a969ad3176a5ade783ccd683224d6d2ad	ar identification of the vocal filter from noisy hyperbaric speech signals			ar (unix)	Laure Charonnat;Joel Crestel;Michel Glutton;Herve Chuberre	1995			speech recognition;artificial intelligence;pattern recognition;computer science	ML	-13.923524247791782	-87.42679312745835	30369
0141fc4ce23837af30a40d01c7bec5e56ec24a1f	incremental learning for spoken affect classification and its application in call-centres	systeme temps reel;modelizacion;distributed system;lenguaje natural;base donnee;systeme reparti;analisis estadistico;support vector machines;centre appel;redundancia;real time;telephone;speech processing;langage naturel;database;tratamiento palabra;traitement parole;call centre;base dato;anns;emotion recognition;aprendizaje probabilidades;classification;modelisation;artificial neural networks;sistema repartido;redundancy;incremental learning;automatic learning;statistical analysis;emotion emotionality;natural language;temps reel;machine exemple support;analyse statistique;affect recognition;pattern recognition;spoken affect classification;tiempo real;apprentissage probabilites;emotion emotivite;real time system;call centres;sistema tiempo real;emocion emotividad;reconnaissance forme;maquina ejemplo soporte;vector support machine;reseau neuronal;reconocimiento patron;speaker;svms;locutor;telefono;modeling;clasificacion;red neuronal;locuteur;probability learning;redondance;neural network	This paper introduces a system for real-time incremental learning in a call-centre environment. The classifier used is a Support Vector Machine (SVM) and it is applied to telephone-based spoken affect classification. A database of 391 natural speech samples depicting angry and neutral speech is collected from 11 speakers. Using this data and features shown to correlate speech with emotional states, a SVM-based classification model is trained. Forward selection is employed on the feature space in an attempt to prune redundant or harmful dimensions. The resulting model offers a mean classification rate of 88.45% for the two-class problem. Results are compared with those from an Artificial Neural Network (ANN) designed under the same circumstances.	algorithm;artificial neural network;concept drift;database;feature vector;natural language;real-time clock;real-time transcription;requirement;stepwise regression;support vector machine;systemverilog;tiff	Donn Morrison;Ruili Wang;W. L. Xu;Liyanage C. De Silva	2005		10.1504/IJISTA.2007.012486	support vector machine;speech recognition;computer science;artificial intelligence;machine learning;redundancy;natural language;artificial neural network	ML	-13.17472274623431	-96.78209431337699	30379
a00cb0505666b3d76586f9561ebb8d5d6cf45039	impact of language on audiovisual speech perception examined by fmri		Both auditory and visual information plays an important role for audiovisual speech perception during face-to-face communication. Several behavioral studies have shown that native English speakers and native Japanese speakers behaved differently in audiovisual speech perception. We hypothesized that there would be differences in neural processing between native English speakers and native Japanese speakers. Twentytwo English language speakers and 22 Japanese speakers watched talker’s face and listened to talker’s speaking during functional magnetic resonance imaging (fMRI) scanning. The lateral occipitotemporal gyrus was associated with visual domain of audiovisual speech perception in native Japanese speakers, but not in native English speakers, suggesting that language environment affects neural processes for audiovisual speech perception.	lateral computing;lateral thinking;resonance	Jun Shinozaki;Kaoru Sekiyama;Nobuo Hiroe;Taku Yoshioka;Masa-aki Sato	2010			communication;speech perception;psychology	NLP	-8.634637174430688	-81.72950183562338	30394
dfded6587c1c5156e8bc19d35b68a826a91b8b67	factor analysis based spatial correlation modeling for speaker verification	frobenius angle;inner product classifier;kernel;log euclidean distance;measurement;support vector machines;gaussian processes;loading;inner product;speech;covariance matrix support vector machines correlation loading speech measurement kernel;euclidean distance;speech spectral distribution;spatial correlation modeling;log euclidean inner product classifier;speaker recognition evaluation;speaker verification;speaker recognition;text independent speaker verification;gaussian mixture model;spatial correlation;factor analysis;covariance matrices;log euclidean inner product classifier factor analysis spatial correlation modeling gaussian mixture model text independent speaker verification speech spectral distribution covariance matrix mean super vector support vector machine speaker recognition evaluation frobenius angle;frobenius angle factor analysis inner product classifier log euclidean distance;pattern classification;support vector machine;correlation;mean super vector;speaker recognition covariance matrices gaussian processes pattern classification;spatial model;covariance matrix	Gaussian mixture models (GMMs) are commonly used in text-independent speaker verification for modeling the spectral distribution of speech. Recent studies have shown the effectiveness of characterizing speaker information using the mean super-vector obtained by concatenating the mean vectors of the GMM. This paper proposes to use the spatial correlation captured by the covariance matrix of the mean super-vector for speaker verification. Factor analysis method is adopted to estimate the covariance of the super-vector. For measuring the similarity between speech utterances in terms of the spatial correlation, we propose two kernel metrics, namely, log-Euclidean inner product and Frobenius angle. For computational simplicity, we introduce an inner product classifier (IPC) with equivalent performance compared to the commonly used support vector machine (SVM). Experiments conducted on the 2006 NIST speaker recognition evaluation (SRE) dataset confirm the efficacy of the proposed factor analysis based spatial modeling technique.	computation;concatenation;experiment;factor analysis;hidden markov model;machine learning;markov chain;matrix analysis;matrix multiplication;mixture model;numerical linear algebra;pattern recognition;spatial variability;speaker recognition;speech recognition;springer (tank);support vector machine	Eryu Wang;Wu Guo;Li-Rong Dai;Kong-Aik Lee;Bin Ma;Haizhou Li	2010	2010 7th International Symposium on Chinese Spoken Language Processing	10.1109/ISCSLP.2010.5684490	speaker recognition;support vector machine;speech recognition;machine learning;pattern recognition;mathematics;statistics	NLP	-16.153141826059926	-92.54536762392337	30616
486bc152f18cadbc1257ce8ec2b692d5c3cbfe5d	vowel quality in spontaneous speech: what makes a good vowel?	spontaneous speech	Clear speech is characterised by longer segmental durations and less target undershoot [9] which results in more extreme spectral features. This paper deals with the clarity of vowels produced in spontaneous speech in a large corpus of task-oriented dialogues. We present an automatic technique for measuring vowel clarity on the basis of a vowel’s spectral characteristics. This technique was evaluated using a perceptual test. Subjects rated the ’goodness’ of vowels with different spectral characteristics with controlled duration and amplitude and these results were compared with an automatic rating. Results indicated that although agreement between subjects and the automatic measurement was poor it was as poor as the agreement between subjects. On the basis of these results we address the following questions: 1. Can subjects reliably judge the clarity of vowels excerpted from spontaneous speech without duration cues? 2. Can a statistical model [3] reliably predict the subjects’ response to such vowels?	spontaneous order;statistical model;stellar classification;text corpus	Matthew P. Aylett;Alice Turk	1998			mid vowel;relative articulation;speech;linguistics	NLP	-11.19853871395733	-83.42080213005767	30671
6a64f404e49922c3b9747687c4cbd3a732f91635	language models and smoothing methods for collections with large variation in document length	smoothing methods bayesian methods computational modeling estimation xml frequency estimation interpolation;document handling;interpolation;smoothing methods information retrieval;smoothing method;information retrieval;bayesian methods;frequency estimation;large variation;smoothing methods;computational modeling;smoothing methods document handling information retrieval natural language processing;estimation;large document length variation;exponential smoothing;xml;large document length variation language models smoothing methods large variation exponential smoothing;natural language processing;language model;language models	In this paper we present a new language model based on an odds formula, which explicitly incorporates document length as a parameter. Furthermore, a new smoothing method called exponential smoothing is introduced, which can be combined with most language models. We present experimental results for various language models and smoothing methods on a collection with large document length variation, and show that our new methods compare favorably with the best approaches known so far.	language model;microsoft outlook for mac;smoothing;time complexity	Najeeb Abdulmutalib;Norbert Fuhr	2008	2008 19th International Workshop on Database and Expert Systems Applications	10.1109/DEXA.2008.33	exponential smoothing;estimation;xml;bayesian probability;interpolation;computer science;pattern recognition;data mining;computational model;statistics;smoothing;language model	NLP	-22.34921019246129	-92.02366583210404	30700
698dfab01116743906ae656bb8215135c1af1255	a three-layer emotion perception model for valence and arousal-based detection from multilingual speech		Automated emotion detection from speech has recently shifted from monolingual to multilingual tasks for human-like interaction in real-life where a system can handle more than a single input language. However, most work on monolingual emotion detection is difficult to generalize in multiple languages, because the optimal feature sets of the work differ from one language to another. Our study proposes a framework to design, implement, and validate an emotion detection system using multiple corpora. A continuous dimensional space of valence and arousal is first used to describe the emotions. A three-layer model incorporated with fuzzy inference systems is then used to estimate two dimensions. Speech features derived from prosodic, spectral, and glottal waveform are examined and selected to capture emotional cues. The results of this new system outperformed the existing state-of-the-art system by yielding a smaller mean absolute error and higher correlation between estimates and human evaluators. Moreover, results for speaker independent validation are comparable to human evaluators.	approximation error;emotion recognition;fuzzy logic;multitier architecture;real life;text corpus;waveform	Xingfeng Li;Masato Akagi	2018		10.21437/Interspeech.2018-1820	speech recognition;valence (chemistry);cognitive psychology;emotion perception;arousal;computer science	NLP	-12.536066139906588	-86.80441504130931	30704
14e4bc9b66da64bae88474a77ccf903809eda8ca	dynamic units of visual speech	distinctive speech movement;visual speech;speech animation;realistic visual speech animation;static visemes;dynamic unit;pleasing speech animation;finite set;dynamic visual speech gesture;dynamic visemes;visual speech articulators;natural language processing;motion capture;applications;computer vision	We present a new method for generating a dynamic, concatenative, unit of visual speech that can generate realistic visual speech animation. We redefine visemes as temporal units that describe distinctive speech movements of the visual speech articulators. Traditionally visemes have been surmized as the set of static mouth shapes representing clusters of contrastive phonemes (e.g. /p, b, m/, and /f, v/). In this work, the motion of the visual speech articulators are used to generate discrete, dynamic visual speech gestures. These gestures are clustered, providing a finite set of movements that describe visual speech, the visemes. Dynamic visemes are applied to speech animation by simply concatenating viseme units. We compare to static visemes using subjective evaluation. We find that dynamic visemes are able to produce more accurate and visually pleasing speech animation given phonetically annotated audio, reducing the amount of time that an animator needs to spend manually refining the animation.	concatenation;concatenative programming language	Sarah L. Taylor;Moshe Mahler;Barry-John Theobald;Iain A. Matthews	2012			computer vision;motion capture;speech recognition;computer science;viseme;information technology;computer graphics (images)	HCI	-15.425878832059004	-82.0280056994803	30935
1df7fee6acd9cb5ce996587f762b351b090785fb	simulation of losses due to turbulence in the time-varying vocal system	artefacto;dynamic articulation;articulatory speech synthesis;time varying;time varying systems speech synthesis synthesizers fluid dynamics kinetic theory energy loss signal synthesis facial animation natural languages medical treatment;speech synthesis;implementation;pressure drop;localization;simulation;simulacion;speech articulation;turbulencia;articulatory synthesizers;localizacion;indexing terms;time varying system;artefact;senal vocal;flow separation;signal vocal;acoustic artifacts;articulation parole;synthetic speech signal;vocal system;localisation;lumped nonlinear resistance;fluid dynamic pressure loss;articulacion palabra;systeme parametre variable;time varying vocal system;high quality articulatory synthesis time varying vocal system flow separation fluid dynamic pressure loss articulatory synthesizers lumped nonlinear resistance dynamic articulation acoustic artifacts synthetic speech signal;kinetic pressure loss;signal acoustique;fluid dynamics;sintesis palabra;acoustic signal;sistema parametro variable;implementacion;vocal signal;kinetics;high quality articulatory synthesis;senal acustica;synthese parole;vocal system articulatory speech synthesis fluid dynamics kinetic pressure loss;turbulence	Flow separation in the vocal system at the outlet of a constriction causes turbulence and a fluid dynamic pressure loss. In articulatory synthesizers, the pressure drop associated with such a loss is usually assumed to be concentrated at one specific position near the constriction and is represented by a lumped nonlinear resistance to the flow. This paper highlights discontinuity problems of this simplified loss treatment when the constriction location changes during dynamic articulation. The discontinuities can manifest as undesirable acoustic artifacts in the synthetic speech signal that need to be avoided for high-quality articulatory synthesis. We present a solution to this problem based on a more realistic distributed consideration of fluid dynamic pressure changes. The proposed method was implemented in an articulatory synthesizer where it proved to prevent any acoustic artifacts	acoustic cryptanalysis;acoustic fingerprint;articulatory phonology;articulatory synthesis;biconnected component;distortion;nonlinear system;reflections of signals on conducting lines;scott continuity;simulation;speech synthesis;synthetic intelligence;tract (literature);transmission line;turbulence	Peter Birkholz;Dietmar Jackel;Bernd J. Kröger	2007	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2006.889731	pressure drop;turbulence;speech recognition;index term;internationalization and localization;acoustics;computer science;flow separation;implementation;manner of articulation;speech synthesis;kinetics;fluid dynamics	Visualization	-8.744671307511801	-86.43200496044749	31079
37b525de1b52b0148b19d221e29f76918f7772db	tracking of speech parameters using arma analysis techniques				Johan de Veth;Lou Boves	1987			artificial intelligence;speech recognition;pattern recognition;computer science	NLP	-13.950024204306427	-87.35912279559031	31146
303b5d5a6fc89057d38318573c74bd0d318de2d9	temporally weighted linear prediction features for tackling additive noise in speaker verification	front end;nist;stabilized weighted linear prediction swlp;stabilized weighted linear prediction swlp additive noise speaker verification;spectrum estimation;additive noise noise robustness cepstral analysis mel frequency cepstral coefficient spectral analysis speech recognition predictive models speech enhancement system performance nist;speech speaker recognition robust linear prediction;additive noise;speech;fft;spectrum;speech enhancement;system performance;noise robustness;linear predictive;speaker verification;speaker recognition;mel frequency cepstral coefficient;spectral subtraction;cepstral analysis;robust linear prediction;fast fourier transforms;speech recognition;weighted linear prediction features;predictive models;speech enhancement fast fourier transforms speaker recognition;preprocessing method;spectral analysis;additive noise corruption;conventional linear prediction;spectrum estimation weighted linear prediction features speaker verification additive noise corruption mel frequency cepstral coefficient speech recognition fft conventional linear prediction speech enhancement spectral subtraction preprocessing method	Text-independent speaker verification under additive noise corruption is considered. In the popular mel-frequency cepstral coefficient (MFCC) front-end, the conventional Fourier-based spectrum estimation is substituted with weighted linear predictive methods, which have earlier shown success in noise-robust speech recognition. Two temporally weighted variants of linear predictive modeling are introduced to speaker verification and they are compared to FFT, which is normally used in computing MFCCs, and to conventional linear prediction. The effect of speech enhancement (spectral subtraction) on the system performance with each of the four feature representations is also investigated. Experiments by the authors on the NIST 2002 SRE corpus indicate that the accuracy of the conventional and proposed features are close to each other on clean data. For factory noise at 0 dB SNR level, baseline FFT and the better of the proposed features give EERs of 17.4% and 15.6%, respectively. These accuracies improve to 11.6% and 11.2%, respectively, when spectral subtraction is included as a preprocessing method. The new features hold a promise for noise-robust speaker verification.	additive model;additive white gaussian noise;baseline (configuration management);coefficient;experiment;fast fourier transform;low-pass filter;mel-frequency cepstrum;nist hash function competition;predictive modelling;preprocessor;signal-to-noise ratio;speaker recognition;spectral density estimation;spectral slope;speech enhancement;speech recognition;utility functions on indivisible goods;wafer-level packaging;white noise	Rahim Saeidi;Jouni Pohjalainen;Tomi Kinnunen;Paavo Alku	2010	IEEE Signal Processing Letters	10.1109/LSP.2010.2048649	speaker recognition;fast fourier transform;speech recognition;computer science;pattern recognition	Vision	-13.72034967796213	-90.73828981929091	31202
4564607bd80be05e7ecdb83069e931a70824a127	model-based duration-difference approach on accent evaluation of l2 learner		This paper aims at using a model-based duration-difference approach to analyze L2 learners’ duration-aspect accent, and, segmental duration characteristics. We use the durational differences deviated from native-English speech duration as an objective measure to evaluate the learner’s timing characteristics. The use of model-based approach provides flexible evaluation without the need to collect any additional English reference speech. The proposed evaluation method was tested on English speech data uttered by native English speakers and Thai-native English learners with different English-study experiences. The experimental results show speaker clusters grouped by English accents and L2 learners’ English-study experiences. These results support the effectiveness of the proposed model-based objective evaluation.		Chatchawarn Hansakunbuntheung;Ananlada Chotimongkol;Sumonmas Thatphithakkul;Patcharika Chootrakool	2012			speech recognition;natural language processing;artificial intelligence;computer science	NLP	-16.632241053928777	-83.04893314802375	31211
82bb3bb057b6986c02f7a177c98b8fa2594a8eb8	face recognition based on separable lattice 2-d hmms using variational bayesian method	image recognition;lattices;hidden markov model;bayes methods;training;bayesian methods;maximum likelihood estimation;training data;bayesian criterion;face recognition;hidden markov models;bayesian criterion face recognition separable lattice 2d hmm variational bayesian method sl2d hmm geometric variation maximum likelihood criterion image recognition over fitting problem generalization ability model marginalization;vectors;hidden markov models bayesian methods image recognition training training data lattices vectors;maximum likelihood estimation bayes methods face recognition hidden markov models;variational bayesian method;variational bayesian method face recognition hidden markov model separable lattice 2 d hmms bayesian criterion;separable lattice 2 d hmms	This paper proposes an image recognition technique based on separable lattice 2-D HMMs (SL2D-HMMs) using the variational Bayesian method. SL2D-HMMs have been proposed to reduce the effect of geometric variations, e.g., size and location. The maximum likelihood criterion had previously been used in training SL2D-HMMs. However, in many image recognition tasks, it is difficult to use sufficient training data, and it suffers from the over-fitting problem. A higher generalization ability based on model marginalization is expected by applying the Bayesian criterion and useful prior information on model parameters can be utilized as prior distributions. Experiments on face recognition indicated that the proposed method improved image recognition.	artificial intelligence;bayesian information criterion;calculus of variations;computer vision;eigenface;eisenstein's criterion;experiment;facial recognition system;hidden markov model;markov chain;model checking;overfitting;variational principle	Kei Sawada;Akira Tamamori;Kei Hashimoto;Yoshihiko Nankaku;Keiichi Tokuda	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288351	training set;bayesian probability;computer science;machine learning;pattern recognition;lattice;mathematics;maximum likelihood;hidden markov model;statistics	Robotics	-18.390156343973178	-92.63400970233586	31244
738fbb9192f747952843d0ec3be0b742751cb69f	gestural coordination differences between intervocalic simple and geminate plosives in moroccan arabic: an ema investigation		Based on EMA data (3 speakers), we investigates the articulatory strategies responsible for the singleton/geminate plosive contrast in Moroccan Arabic. Our data showed that closing and opening phases of the geminate gesture share several articulatory properties with those of its single cognate. These results and our analyses of V-to-V and V-to-C temporal coordination are consistent with the hypothesis that MA geminate plosives can be analysed as two identical overlapped consonants.	closing (morphology)	Chakir Zeroual;Phil Hoole;Adamantios I. Gafos;John H. Esling	2015			arabic;linguistics;computer science	NLP	-10.630533410841563	-81.51543972936015	31277
a28b2862abf6ba470deadb7dc19fad68722cec22	compiling learner corpus data of linguistic output and language processing in speaking, listening, writing, and reading		A learner’s language data of speaking, writing, listening, and reading have been compiled for a learner corpus in this study. The language data consist of linguistic output and language processing. Linguistic output refers to data of pronunciation, sentences, listening comprehension rate, and reading comprehension rate. Language processing refers to processing time and learners’ self-judgment of their difficulty of processing in speaking, listening, and reading and the fluency of their writing. This learner corpus will contribute to making the language learning process more clearly visible.	compiler;corpus linguistics;performance;text corpus	Katsunori Kotani;Takehiko Yoshimi;Hiroaki Nanjo;Hitoshi Isahara	2011			natural language processing;speech recognition;computer science;linguistics;comprehension approach	NLP	-26.822176484007027	-82.72579125225583	31297
71ae54151e6b60ddfda70588f3860b837e803288	quand nasal est plus que nasal: l'articulation orale des voyelles nasales en français (when nasal is more than nasal: oral articulation of french nasal vowels) [in french]		________________________________________________________________________________________________________ When nasal is more than nasal: Oral articulation of French nasal vowels Lingual and labial articulations of oral and nasal vowels of three Metropolitan French (FM) speakers were recorded using an EMA system. Inter-speaker variation in these oral articulations suggest that the role of motor equivalence is important in the acoustic dispersion of this vowel system: the speakers have a similar acoustic output, but use different articulatory strategies to achieve this output. MOTS-CLES : Nasalisation vocalique, production vocalique, français, articulation, EMA	acoustic cryptanalysis;biconnected component;fm broadcasting;turing completeness	Christopher Carignan	2012				HCI	-8.625991878932728	-84.19653892962897	31299
91bd44f9f16fb036cc706327653928c6bce9ec78	natural language processing — nlp 2000		We extend the Integrated Asymmetry hypothesis in considering the relations between principle-based linguistic models and corresponding parsing models with respect to their account of inverse and covert asymmetries. We present an integrated model of language knowledge and language use relying on the logical definition of asymmetry. Our integrated model keeps the relation between the grammar and the parser maximally simple while it keeps constant the specificity of each system. The integration of asymmetry-based parser in IP systems improves their performance.	natural language processing;parsing;sensitivity and specificity	Jan van Leeuwen	2000		10.1007/3-540-45154-4	natural language processing	NLP	-31.88808744462877	-80.99324954617309	31301
3817f4c05b7316a405c2aadf80eb113d8fbb82d6	audio-visual speech recognition using lip movement extracted from side-face images	optical flow;signal to noise ratio;speech recognition;white noise	This paper proposes an audio-visual speech recognition method using lip movement extracted from side-face images to attempt to increase noise-robustness in mobile environments. Although most previous bimodal speech recognition methods use frontal face (lip) images, these methods are not easy for users since they need to hold a device with a camera in front of their face when talking. Our proposed method capturing lip movement using a small camera installed in a handset is more natural, easy and convenient. This method also effectively avoids a decrease of signal-to-noise ratio (SNR) of input speech. Visual features are extracted by optical-flow analysis and combined with audio features in the framework of HMM-based recognition. Phone HMMs are built by the multi-stream HMM technique. Experiments conducted using Japanese connected digit speech contaminated with white noise in various SNR conditions show effectiveness of the proposed method. Recognition accuracy is improved by using the visual information in all SNR conditions, and the best improvement is approximately 6% at 5dB SNR.	audio-visual speech recognition;data-flow analysis;hidden markov model;signal-to-noise ratio;white noise	Tomoaki Yoshinaga;Satoshi Tamura;Koji Iwano;Sadaoki Furui	2003			phone;voice activity detection;audio-visual speech recognition;acoustic model;white noise;speech processing;speech recognition;speech coding;optical flow;computer science	Vision	-12.026487743186028	-90.1434700721338	31327
cd853564c17df5ea416bfd05875fd7a6c1cf469b	a method for evaluating and comparing user simulations: the cramér-von mises divergence	hidden markov models computational modeling machine learning speech recognition laboratories predictive models humans algorithm design and analysis design optimization machine learning algorithms;user modelling;user modelling interactive systems speech recognition statistical analysis;dialog management;score function;spoken dialog system;dialog simulation;user simulation;statistical significance;dialog management user simulation user modelling dialog simulation;statistical analysis;speech recognition;statistical analysis user simulation spoken dialog system quality measure domain specific scoring function cramer von mises divergence;quality measures;critical value;interactive systems;domain specificity	Although user simulations are increasingly employed in the development and assessment of spoken dialog systems, there is no accepted method for evaluating user simulations. In this paper, we propose a novel quality measure for user simulations. We view a user simulation as a predictor of the performance of a dialog system, where per-dialog performance is measured with a domain-specific scoring function. The quality of the user simulation is measured as the divergence between the distribution of scores in real dialogs and simulated dialogs, and we argue that the Cramer-von Mises divergence is well-suited to this task. The technique is demonstrated on a corpus of real calls, and we present a table of critical values for practitioners to interpret the statistical significance of comparisons between user simulations.	dialog system;kerrison predictor;scoring functions for docking;simulation;spoken dialog systems;text corpus	Jason D. Williams	2007	2007 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU)	10.1109/ASRU.2007.4430164	natural language processing;speech recognition;critical value;computer science;machine learning;statistical significance;score;statistics	HPC	-25.547143740588986	-88.29031553919432	31343
483e8e8d19a56405d303cd3dc4b1dc16d1e9fa90	exploiting eigenposteriors for semi-supervised training of dnn acoustic models with sequence discrimination		Deep neural network (DNN) acoustic models yield posterior probabilities of senone classes. Recent studies support the existence of low-dimensional subspaces underlying senone posteriors. Principal component analysis (PCA) is applied to identify eigenposteriors and perform low-dimensional projection of the training data posteriors. The resulted enhanced posteriors are applied as soft targets for training better DNN acoustic model under the student-teacher framework. The present work advances this approach by studying incorporation of sequence discriminative training. We demonstrate how to combine the gains from eigenposterior based enhancement with sequence discrimination to improve ASR using semi-supervised training. Evaluation on AMI meeting corpus yields nearly 4% absolute reduction in word error rate (WER) compared to the baseline DNN trained with cross entropy objective. In this context, eigenposterior enhancement of the soft targets is crucial to enable additive improvement using out-of-domain untranscribed data.	acoustic cryptanalysis;acoustic model;artificial neural network;baseline (configuration management);cross entropy;deep learning;discriminative model;principal component analysis;semi-supervised learning;semiconductor industry;utility functions on indivisible goods;word error rate	Pranay Dighe;Afsaneh Asaei;Hervé Bourlard	2017			speech recognition;artificial intelligence;pattern recognition;computer science	NLP	-16.08318804207431	-90.52015498646978	31388
e0d04a75cf9017bafc61084e0e5b8eb84a3ecb2d	a syllable-based method for vietnamese text compression	text compression;syllable based morphology;dictionary	Text compression is a technique to reduce the size of text file and increase the transfer rate as well as save storage space. Many approaches have been proposed to tackle this problem in several languages such as: English, Chinese, Turkey, Japanese, French, etc. In this paper, we propose a method to compress Vietnamese text using syllables based on morphology and dictionaries. Our method firstly splits a morphosyllable to a consonant and a syllable then we encode it based on dictionaries of consonants and syllables. In our method, based on characteristics of Vietnamese language with six tone-marks, we build six different dictionaries of syllables. We collect a testing set of 20 different text files with different sizes to demonstrate our system. Experimental results show that our system achieves good performance with the compression ratio around 73%. In comparison with WinZIP version 19.51 and WinRAR version 5.212, our method achieves a higher compression ratio while the size of text file is small. So that, our method can apply efficiency to compress for short text such as: SMS messages, text messages on social networks.	data compression;dictionary;encode;mathematical morphology;social network;syllable;winzip	Vu H. Nguyen;Hien T. Nguyen;Hieu Ngoc Duong;Václav Snásel	2016		10.1145/2857546.2857564	natural language processing;speech recognition;computer science;associative array	Web+IR	-22.32688204425066	-81.43857731378579	31456
35fe912e898e0d1d7ca0bf2db282783c4d53806d	gesture recognition portfolios for personalization	psi_visics;personalization;classification;portfolios training gesture recognition vegetation training data standards computational efficiency;classification gesture recognition personalization;selection problem gesture recognition portfolios personalization data speech recognition handwriting recognition single classifier classifier personalization;gesture recognition;speech recognition gesture recognition handwriting recognition	Human gestures, similar to speech and handwriting, are often unique to the individual. Training a generic classifier applicable to everyone can be very difficult and as such, it has become a standard to use personalized classifiers in speech and handwriting recognition. In this paper, we address the problem of personalization in the context of gesture recognition, and propose a novel and extremely efficient way of doing personalization. Unlike conventional personalization methods which learn a single classifier that later gets adapted, our approach learns a set (portfolio) of classifiers during training, one of which is selected for each test subject based on the personalization data. We formulate classifier personalization as a selection problem and propose several algorithms to compute the set of candidate classifiers. Our experiments show that such an approach is much more efficient than adapting the classifier parameters but can still achieve comparable or better results.	experiment;gesture recognition;handwriting recognition;personalization;selection algorithm	Angela Yao;Luc Van Gool;Pushmeet Kohli	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.247	natural language processing;computer vision;speech recognition;biological classification;intelligent character recognition;computer science;pattern recognition;gesture recognition;personalization	Vision	-4.60465505919559	-89.43185309301329	31782
1183d7eb3c0633fdf5444c4de5f90687071e78fa	the bits speech synthesis corpus for german.	unit selection;speech synthesis;ministry of education;quality control	In this paper we announce the new BITS Synthesis Corpus for German. The BITS project is funded by the German Ministry of Education and Science to provide a publicly available synthesis corpus for German. The corpus comprises the voices of four German speakers (two male and two female) and consists of two parts: a set of logatome recordings for controlled diphone synthesis and a set of sentence recordings for unit selection. The paper gives an overview about the basic specifications, the profiles of the speakers, the casting procedure and quality control. Annotation and its organisation are described in detail. The final BITS speech synthesis corpus will be available via BAS and ELDA probably end of 2005.	broadcast auxiliary service;speech synthesis	Tania Ellbogen;Florian Schiel;Alexander Steffen	2004			natural language processing;quality control;speech recognition;computer science;speech synthesis	NLP	-24.92764140164479	-84.03605504640116	31843
e417552dd033d111d85bddf297c1522c4067ea12	error and reject rates in a hierarchical pattern recognizer	decision tree logic error rates hierarchical pattern recognition linguistic pattern recognition pattern recognition reject rates statistical pattern recognition;decision tree;statistical independence;decision tree logic;reject rates;statistical pattern recognition;pattern recognition;error rate;error rates;linguistic pattern recognition;hierarchical pattern recognition	The reject and error rates of a certain hierarchical decision structure are derived under assumptions of statistical independence among the members of the hierarchy. It is shown that high decision reliability can be obtained with much more relaxed requirements on the individual recognizers in the hierarchy.	finite-state machine;requirement	Morton Nadler	1971	IEEE Transactions on Computers	10.1109/T-C.1971.223180	independence;speech recognition;feature;word error rate;computer science;machine learning;decision tree;pattern recognition;statistics	Visualization	-21.355533297208254	-90.35177994023407	31896
3ff1b7ddeaf6dbce814e2938cbcdc71f57f9a782	shared task: statistical machine translation between european languages	statistical machine translation	The ACL-2005 Workshop on Parallel Texts hosted a shared task on building statistical machine translation systems for four European language pairs: French–English, German–English, Spanish–English, and Finnish–English. Eleven groups participated in the event. This paper describes the goals, the task definition and resources, as well as results and some analysis. Statistical machine translation is currently the dominant paradigm in machine translation research. Annual competitions are held for Chinese–English and Arabic–English by NIST (sponsored by the US military funding agency DARPA), which creates a forum to present and compare novel ideas and leads to steady progress in the field. One of the advantages of statistical machine translation is that the currently applied methods are fairly language-independent. Building a new machine translation system for a new language pair is not much more than a matter of running a training process on a training corpus of parallel text (a text in one language paired with a translation in another). It is therefore possible to hold a competition where research groups have only a few weeks to build machine translation systems for language pairs that they have not previously worked on. We effectively demonstrated this with our shared task. For instance, seven teams built Finnish–English machine translation systems, a language pair that was certainly not of their immediate concern before. In contrast to the bigger NIST competition, we wanted to keep the barrier of entry as low as possible. We provided not only training data from the Europarl corpus (Koehn, 2005), but also additional resources: sentence and word alignments, the decoder Pharaoh1 (Koehn, 2004b), and a language model, so that participation was feasible even as a graduate level class project. Using about 15 million words of translated text, participants were asked to build a phrase-based statistical machine translation system. The focus of the task was to build a probabilistic phrase translation table, since most of the other resources were provided — for more on phrase-based statistical machine translation, refer to Koehn et al. (2003). The participants’ systems were compared by how well they translated 2000 previously unseen test sentences from the same domain. The shared task operated within an extremely short timeframe. The workshop and hence the shared task was accepted on February 22, 2005 and announced on March 3. The official test data was made available on April 3, results were due one week later. Despite this tight schedule, eleven research groups participated and built a total of 32 machine translation systems for the four language pairs.	europarl corpus;language model;language-independent specification;parallel text;programming paradigm;statistical machine translation;test data;text corpus	Philipp Koehn;Christof Monz	2005		10.3115/1654449.1654474	computer-assisted translation;natural language processing;example-based machine translation;computer science;theoretical computer science;evaluation of machine translation;machine translation;rule-based machine translation;programming language;machine translation software usability	NLP	-25.44626593098402	-81.29497766553717	31980
578170d1ff7fdcd817786b2bee6b25295fa4af00	svitchboard 1: small vocabulary tasks from switchboard		We present a conversational telephone speech data set designed to support research on novel acoustic models. Small vocabulary tasks from 10 words up to 500 words are defined using subsets of the Switchboard-1 corpus; each task has a completely closed vocabulary (an OOV rate of 0%). We justify the need for these tasks, de- scribe the algorithm for selecting them from a large cor- pus, give a statistical analysis of the data and present baseline whole-word hidden Markov model recognition results. The goal of the paper is to define a common data set and to encourage other researchers to use it.	telephone switchboard;vocabulary	Simon King;Chris D. Bartels;Jeff A. Bilmes	2005				Vision	-21.30372139329933	-85.33863909318227	32025
646a8ea5484ed2cf9b7524dae3e702d978f9bddb	generative modeling and classification of dialogs by a low-level turn-taking feature	generative modeling;dialog analysis;generic model;classification;gaussian mixture model;feature extraction	In the last few years, a growing attention has been paid to the problem of human–human communication, trying to devise artificial systems able to mediate a conversational setting between two or more people. In this paper, we propose an automatic system based on a generative structure able to classify dialog scenarios. The generative model is composed by integrating a Gaussian mixture model and a (observed) Markovian influence model, and it is fed with a novel low-level acoustic feature termed steady conversational period (SCP). SCPs are built on duration of continuous slots of silence or speech, taking also into account conversational turn-taking. The interactional dynamics built upon the transitions among SCPs provides a behavioral blueprint of conversational settings without relying on segmental or continuous phonetic features, and may be important for predicting the evolution of typical conversational situations in different dialog scenarios. The model has been tested on an extensive set of real, dyadic and multi-person conversational settings, including a recent dyadic dataset and the AMI meeting corpus. Comparative tests are made using conventional acoustic features and classification methods, showing that the proposed scheme provides superior classification performances for all conversational settings in our datasets. Moreover, we prove that our approach is able to characterize the nature of multi-person conversation (namely, the role of the participants) in a very accurate way, thus demonstrating great versatility. & 2011 Elsevier Ltd. All rights reserved.	acoustic cryptanalysis;algorithm;blueprint;cluster analysis;dialog system;discriminant;dyadic transformation;encode;feature extraction;first-order predicate;generative modelling language;generative model;high- and low-level;human-readable medium;interaction;markov chain;mixture model;objectivity/db;performance;semantic prosody;state space;statistical classification	Marco Cristani;Anna Pesarin;Carlo Drioli;Alessandro Tavano;Alessandro Perina;Vittorio Murino	2011	Pattern Recognition	10.1016/j.patcog.2011.01.013	natural language processing;speech recognition;feature extraction;biological classification;computer science;machine learning;pattern recognition;mixture model	Vision	-12.957921702732602	-84.44515133690246	32148
68495a23e511085f641d78fac4f8130d6dcd3b4d	mismatched crowdsourcing based language perception for under-resourced languages		Mismatched crowdsourcing is a technique for acquiring automatic speech recognizer training data in under-resourced languages by decoding the transcriptions of workers who don't know the target language using a noisy-channel model of cross-language speech perception. All previous mismatched crowdsourcing studies have used English transcribers; this study is the first to recruit transcribers with a different native language, in this case, Mandarin Chinese. Using these data we are able to compute statistical models of cross-language perception of the tones and phonemes from transcribers based on phone distinctive features and tone features. By analyzing the phonetic and tonal variation mappings and coverages compared with the dictionary of the target language, we evaluate the different native languages’ effect on the transcribers’ performances. © 2016 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the Organizing Committee of SLTU 2016.	channel (communications);compiler;crowdsourcing;dictionary;finite-state machine;noisy channel model;performance;speech recognition;statistical model;super robot monkey team hyperforce go!;transcription (software)	Wenda Chen;Mark Hasegawa-Johnson;Nancy F. Chen	2016		10.1016/j.procs.2016.04.025	natural language processing;speech recognition;computer science	NLP	-20.3028297322206	-84.98258853020096	32168
909ec23bfa63e6c255d45e20be29460027e6a126	fast signal recognition and detection using art1 neural networks and nonlinear preprocessing units based on time delay embeddings.			artificial neural network;broadcast delay;nonlinear system;preprocessor	Radu Dogaru;A. T. Murgan;Cristina Comaniciu	1996			speech recognition;computer science;machine learning;pattern recognition;time delay neural network	ML	-15.701565400198769	-87.62308380714981	32193
9dd49948bd86e6e7e15244a7e4fdc6c7baad5b88	combining voiceprint and face biometrics for speaker identification using sdws.	speaker identification	The biometric system that uses multiple biometric traits promises higher identification accuracy than identification in either individual domain. To reach this goal, special attention should be paid to the strategies for combining voiceprint and face experts. We propose an improved weighted sum rule based on the scores difference (SDWS) between the genuine speaker class and the mistaken speaker class labeled by each classifier, and demonstrate that the performance of multibiometric system can be further improved by SDWS. The tests were conducted on a multi-modal database with 54 users We compare our approach with other existing methods and show that SDWS improved performance by about 7.8-13.3%, much better than the others.	biometrics;modal logic;speaker recognition;sum rule in quantum mechanics;weight function	Dongdong Li;Yingchun Yang;Zhaohui Wu	2005			speaker recognition;speech recognition;computer science	AI	-12.793788539865226	-89.07931738200591	32256
88183f84e128f751d50c1d34e5fe5a2cd65dc890	extension of hvs semantic parser by allowing left-right branching	language understanding;speech processing grammars;spoken dialog systems hidden vector state parser semantic parsing statistical based spoken dialog system czech human human train timetable corpus;spoken dialog system;spoken dialog systems;spoken language understanding semantic analysis spoken dialog systems;speech processing;semantic parsing;czech human human train timetable corpus;statistical significance;hidden vector state;grammars;left right;natural languages information retrieval automata cybernetics testing cultural differences global communication maximum a posteriori estimation probability helium;statistical based spoken dialog system;semantic analysis;hidden vector state parser;spoken language understanding	The hidden vector state (HVS) parser is a popular method for semantic parsing. It is used in the language understanding module of the statistical based spoken dialog system. This paper presents an extension of the HVS semantic parser. It enables the parser to generate broader class of semantic trees. This modification can be used to improve the performance of the parser by generating not only the right-branching trees (like original HVS parser) but also limited left-branching trees and their combinations. The extension retains simplicity and properties of the original HVS parser. We tested the method on Czech human-human train timetable corpus. The modified HVS parser yields statistically significant improvement. The accuracy of the system increased from 50.4% to 58.3% absolutely.	dialog system;human visual system model;natural language understanding;parsing;schedule;spoken dialog systems	Filip Jurcícek;Jan Svec;Ludek Müller	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518779	natural language processing;parser combinator;speech recognition;lalr parser;canonical lr parser;computer science;glr parser;speech processing;statistical significance;recursive descent parser;ll parser;top-down parsing;statistics;lr parser;simple lr parser	NLP	-19.96064613005311	-86.160746381029	32292
ca8f0adb9f096f9e80cef9048fc148c94d8fb100	creation of an annotated german broadcast speech database for spoken document retrieval		In this paper we present a semi-automatic method for creating annotated data sets from German-language broadcast resources for which audio files as well as transcripts are available on the Internet. The transcripts are required to be reasonably accurate, but not perfect. Our approach is implemented by a integrated bundle of data processing tools, which support the human annotator in the creation of an annotated data set specialized for research in the area of spoken document classification and retrieval. Annotation decisions that would require prohibitively large amounts training data or system development time to make automatically are taken over by the human annotator. Annotation decisions which are easily automated and tedious for humans are shouldered by the computer. Using our method we can process and annotate the data approximately ten times faster that it was possible by hand. The data is downloaded and the transcripts are normalized by a series of filters as well as a semi-automatic digit to text conversion. Then, the system makes use of the Bayesian Information Criterion (BIC) to segment the audio data and Automatic Speech Recognition (ASR) to forced-alignment of the speech signal with written transcripts. We demonstrate the method with the concrete example of our Deutsche Welledatabase of programs from theKalenderblattradio series.	bayesian information criterion;document classification;document retrieval;internet;semiconductor industry;speech recognition	Stefan Eickeler;Martha Larson;Wolff Rüter;Joachim Köhler	2002			natural language processing;the internet;artificial intelligence;speech recognition;computer science;database;information retrieval;bayesian information criterion;document retrieval;document classification;data processing;data set;broadcasting;annotation	NLP	-22.383188090593734	-84.37191426238037	32321
23c3d17391fc129ad9045242834f111b6acf9504	error analysis of field trial results of a spoken dialogue system for telecommunications applications	dialogue system;telephone;field trial;speech recognition			Shingo Kuroiwa;Kazuya Takeda;Masaki Naito;Naomi Inoue;Seiichi Yamamoto	1995	IEICE Transactions		natural language processing;speech recognition;computer science	NLP	-15.682359059295573	-85.81472842795588	32327
c795998556933b94224e6d60e219e80a50b6b3b3	a study of large vocabulary speech recognition decoding using finite-state graphs	finite state graph;memory management;decoding;large vocabulary continuous speech recognition;acoustics;vocabulary;wfst representation;transducers;wfst;finite state machines;hidden markov models;viterbi algorithm;weighted finite state transducer;graphical representation;grpdecoder wfst finite state graph;viterbi decoder;error rate;speech recognition;compiled search network;decoding hidden markov models speech recognition transducers viterbi algorithm acoustics memory management;grpdecoder benchmarking large vocabulary speech recognition decoding finite state graph compiled search network viterbi decoder wfst representation graphical representation;vocabulary finite state machines speech recognition viterbi decoding;viterbi decoding;large vocabulary speech recognition decoding;grpdecoder benchmarking;grpdecoder	The use of weighted finite-state transducers (WFSTs) has become an attractive technique for building large vocabulary continuous speech recognition decoders. Conventionally, the compiled search network is represented as a standard WFST, which is then directly fed into a Viterbi decoder. In this work, we use the standard WFST representations and operations during compiling the search network. The compiled WFST is then equivalently converted to a new graphical representation, which we call finite-state graph (FSG). The resulting FSG is more tailored to Viterbi decoding for speech recognition and more compact in memory. This paper presents our effort to build a state-of-the-art WFST-based speech recognition system, which we call GrpDecoder. Benchmarking of GrpDecoder is carried out separately on two languages - English and Mandarin. The test results show that GrpDecoder which uses the new FSG representation in searching is superior to HTK's HDecode and IDIAP's Juicer for both languages, achieving lower error rates for a given recognition speed.	compiler;htk (software);mathematical optimization;rtfm;sound juicer;speech recognition;super robot monkey team hyperforce go!;transducer;viterbi decoder;vocabulary	Zhijian Ou;Ji Xiao	2010	2010 7th International Symposium on Chinese Spoken Language Processing	10.1109/ISCSLP.2010.5684837	natural language processing;speech recognition;computer science;pattern recognition;finite-state machine;viterbi decoder;hidden markov model;statistics	NLP	-20.989955163465076	-86.2383926900129	32330
2999bdd86b64f76b8d11a435bb914e0a5a2e14f0	improving communication of visual signals by text-to-speech software	well-documented aid;text-to-speech software;tts application;availability criterion;accessibility criterion;printed signal;theoretical framework;important criterion;improving communication;expository text;alternative auditory rendering;visual signal;specific information	well-documented aid;text-to-speech software;tts application;availability criterion;accessibility criterion;printed signal;theoretical framework;important criterion;improving communication;expository text;alternative auditory rendering;visual signal;specific information	speech synthesis	Robert F. Lorch;Julie Lemarié	2013		10.1007/978-3-642-39194-1_43	speech recognition;computer science;artificial intelligence;communication	HCI	-7.27295960510666	-84.08290595660618	32336
1a1c7fec7385b85c89c64ed5aaf8413ba3e945b8	sound event recognition with probabilistic distance svms	event recognition;support vector machine svm divergence distance probabilistic distance sound characterization sound event recognition subband temporal envelope ste;gamma modeling;kernel;speech signal;audio signal processing;closed form solution;probability;support vector machines;classification method sound event recognition subband probabilistic distance svm speech signal audio signal unique spectro temporal signature probabilistic distance support vector machine subband temporal envelope ste kernel technique spd gamma modeling computational cost reduction mel frequency cepstral coefficient mfcc;support vector machine svm;audio signal;speech;subband probabilistic distance svm;wavelet transforms;mel frequency cepstral coefficient;ste;wavelet transform;kernel support vector machines probabilistic logic speech recognition mel frequency cepstral coefficient wavelet transforms speech;sound event recognition;sound characterization;kernel technique;classification method;speech recognition;spd;support vector machine;probabilistic logic;probabilistic distance support vector machine;support vector machines audio signal processing probability speech recognition;subband temporal envelope;computational cost reduction;probabilistic distance;mfcc;divergence distance;subband temporal envelope ste;unique spectro temporal signature	Unlike other audio or speech signals, sound events have a relatively short time span. They are usually distinguished by their unique spectro-temporal signature. This paper proposes a novel classification method based on probabilistic distance support vector machines (SVMs). We study a parametric approach to characterizing sound signals using the distribution of the subband temporal envelope (STE), and kernel techniques for the subband probabilistic distance (SPD) under the framework of SVM. We show that generalized gamma modeling is well devised for sound characterization, and that the probabilistic distance kernel provides a closed form solution to the calculation of divergence distance, which tremendously reduces computational cost. We conducted experiments on a database of ten types of sound events. The results show that the proposed classification method significantly outperforms conventional SVM classifiers with Mel-frequency cepstral coefficients (MFCCs). The rapid computation of probabilistic distance also makes the proposed method an obvious choice for online sound event recognition.	algorithmic efficiency;coefficient;computation;experiment;kernel (operating system);mel-frequency cepstrum;support vector machine	H. D. Tran;Haizhou Li	2011	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2010.2093519	support vector machine;speech recognition;computer science;machine learning;pattern recognition;mathematics;linguistics;mel-frequency cepstrum;statistics;wavelet transform	ML	-10.088606966045225	-92.06383182933354	32344
b8c506c348a163b270e5e60922f41176759dd6f5	multi-lingual geoparsing based on machine translation		Our method for multi-lingual geoparsing uses monolingual tools and resources along with machine translation and alignment to return location words in many languages. Not only does our method save the time and cost of developing geoparsers for each language separately, but also it allows the possibility of a wide range of language capabilities within a single interface. We evaluated our method in our LanguageBridge prototype on location named entities using newswire, broadcast news and telephone conversations in English, Arabic and Chinese data from the Linguistic Data Consortium (LDC). Our results for geoparsing Chinese and Arabic text using our multi-lingual geoparsing method are comparable to our results for geoparsing English text with our English tools. Furthermore, experiments using our machine translation approach results in accuracy comparable to results from the same data that was translated manually.	bitext word alignment;cost efficiency;display resolution;experiment;linguistic data consortium;machine translation;named entity;prototype;spatial variability	Xu Chen;Han Zhang;Judith Gelernter	2015	CoRR		natural language processing;speech recognition;computer science;linguistics	NLP	-22.254975613297137	-83.61756647161256	32440
448091a94bab1ebe597dc5690843f1db16fc8f94	large-margin conditional random fields for single-microphone speech separation		Conditional random field (CRF) formulations for singlemicrophone speech separation are improved by large-margin parameter estimation. Speech sources are represented by acoustic state sequences from speaker-dependent acoustic models. The large-margin technique improves the classification accuracy of acoustic states by reducing generalization error in the training phase. Non-linear mappings inspired from the mixturemaximization (MIXMAX) model are applied to speech mixture observations. Compared with a factorial hidden Markov model baseline, the improved CRF formulations achieve better separation performance with significantly fewer training data. The separation performance is evaluated in terms of objective speech quality measures and speech recognition accuracy on the reconstructed sources. Compared with the CRF formulations without large-margin parameter estimation, the improved formulations achieve better performance without modifying the statistical inference procedures, especially when the sources are modeled with increased number of acoustic states.	acoustic cryptanalysis;acoustic model;baseline (configuration management);conditional random field;estimation theory;generalization error;hidden markov model;markov chain;microphone;speech recognition	Yu Ting Yeung;Tan Lee;Cheung-Chi Leung	2014			artificial intelligence;speech recognition;random field;pattern recognition;computer science;conditional random field;microphone	ML	-17.602861864718705	-91.76363372690095	32472
27af00095480902321ce7f5125d38e0dc4535b7a	communication strategies for a computerized caregiver for individuals with alzheimer's disease	automated system;correct speech recognition;automatic detection;health care;generic automated system;daily task;inaugural work;computerized caregiver;current direction;communication strategy;acoustic noise;human caregiver	Currently, health care costs associated with aging at home can be prohibitive if individuals require continual or periodic supervision or assistance because of Alzheimer’s disease. These costs, normally associated with human caregivers, can be mitigated to some extent given automated systems that mimic some of their functions. In this paper, we present inaugural work towards producing a generic automated system that assists individuals with Alzheimer’s to complete daily tasks using verbal communication. Here, we show how to improve rates of correct speech recognition by preprocessing acoustic noise and by modifying the vocabulary according to the task. We conclude by outlining current directions of research including specialized grammars and automatic detection of confusion.	acoustic cryptanalysis;preprocessor;speech recognition;vocabulary	Frank Rudzicz;Rozanne Wilson;Alex Mihailidis;Elizabeth Rochon;Carol Leonard	2012				NLP	-17.245489681858537	-82.61986486092383	32521
614db929f5b32bc7dca590d701098f04413d56de	robust hidden semi-markov modeling of array cgh data	comparative genomic hybridization data;genomics;hidden semimarkov modeling;probability;coriell cell lines;generic model;array cgh;array cgh data;hidden markov model;hidden semi markov model;training;glioblastoma multiforme;tumours;comparative genomic hybridization data hidden semimarkov modeling probability distribution genomic hybridization data coriell cell lines glioblastoma multiforme data tumors;data analysis;computational modeling;hidden semi markov models;maximum likelihood estimate;hidden markov models;mixture model;hidden markov models training robustness data models computational modeling equations mathematical model;probability distribution;molecular biophysics;tumours bioinformatics cellular biophysics data analysis genomics hidden markov models molecular biophysics probability;robust method;tumors;mathematical model;robustness;discriminative training;student s t distribution array cgh data copy number variation hidden semi markov models discriminative training;glioblastoma multiforme data;array based comparative genomic hybridization;genomic hybridization data;copy number variation;cellular biophysics;student s t distribution;data models;bioinformatics;cell line	As an extension to hidden Markov models, the hidden semi-Markov models allow the probability distribution of staying in the same state to be a general distribution. Therefore, hidden semi-Markov models are good at modeling sequences with succession of homogenous zones by choosing appropriate state duration distributions. Hidden semi-Markov models are generative models. Most times they are trained by maximum likelihood estimation. To compensate model mis-specification and provide protection against outliers, hidden semi-Markov models can be trained discriminatively given a labeled training set at the expense of increased training complexity. As an alternative to discriminative training, in this paper, we consider model mis-specification and outliers by adopting robust methods. Specifically, we use Student's t mixture models as the emission distributions of hidden semi-Markov models. The proposed robust hidden semi-Markov models are used to model array based comparative genomic hybridization data. Experiments conducted on the benchmark data from the Coriell cell lines, and the glioblastoma multiforme data illustrate the reliability of the technique.	benchmark (computing);computer-generated holography;discriminative model;generative model;hidden markov model;markov chain;mixture model;semiconductor industry;succession;test set	Jiarui Ding;Sohrab P. Shah	2010	2010 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2010.5706637	probability distribution;data modeling;computer science;bioinformatics;machine learning;student's t-distribution;hidden semi-markov model;probability;mixture model;mathematical model;maximum likelihood;copy-number variation;data analysis;generative model;computational model;cell culture;hidden markov model;statistics;robustness	ML	-20.557410293092698	-94.33558228966724	32570
547e0858112ef2ffd4743ca13a391cd7abd80774	on the development of a dictation machine for spanish: divo.	bottom up;hidden markov model;word recognition;speaker dependent	∗ This work has been partially supported by INSERSO (Ministerio de Asuntos Sociales) and CYCIT grant #TIC94-0119. ABSTRACT The first prototype of a low cost dictation machine for Spanish is described (DIVO). The main characteristics of our recognition approach are: bottomup, hypothesis-verification strategy; large vocabulary, speaker dependent, isolated word recognition. Its modular structure is the cue for quick development and testing of different implementation alternatives. Two of them are presented: one is based in Static phoneme Modeling (SM) and the other uses Discrete Hidden Markov Modeling (DHMM). The system runs on a standard PC compatible (286 or higher) equipped with a DSP board and is fully voice controlled. This first version of the system can address multiple vocabulary sets of up to 2000 words each, with immediate response and reasonable performance. Modules for increasing vocabulary and performance are being developed.	digital signal processor;hidden markov model;ibm pc compatible;markov chain;personal computer;prototype;vocabulary	Javier Macías Guarasa;Manuel A. Leandro;José Colás Pasamontes;Alvaro Villegas;Santiago Aguilera;José Manuel Pardo	1994			natural language processing;speech recognition;pattern recognition	Vision	-22.54224754175283	-85.54908452319651	32620
c85ae0320ecb2dc8dc564f7cba01b71b3f2901d2	two web applications for exploring melodic patterns in jazz solos		This paper presents two novel user interfaces for investigating the pattern content in monophonic jazz solos and exemplifies how these interfaces could be used for research on jazz improvisation. In jazz improvisation, patterns are of particular interest for the analysis of improvisation styles, the oral transmission of musical language, the practice of improvisation, and the psychology of creative processes. The ongoing project “Dig That Lick” is devoted to addressing these questions with the help of a large database of jazz solo transcriptions generated by automated melody extraction algorithms. To expose these transcriptions to jazz researchers, two prototypes of user interfaces were designed that work currently with the 456 manually transcribed jazz solos of the Weimar Jazz Database. The first one is a Shiny application that allows exploring a set of 653 of the most common patterns by eminent players. The second one is a web interface for a general two-staged pattern search in the Weimar Jazz Database featuring regular expressions. These applications aim on the one hand at an expert audience of jazz researchers to facilitate generating and testing hypotheses about patterns in jazz improvisation, and on the other hand at a wider audience of jazz teachers, students, and fans.	algorithm;keyword extraction;pattern search (optimization);regular expression;structure of observed learning outcome;user interface;web application	Klaus Frieler;Frank Höger;Martin Pfleiderer;Simon Dixon	2018				Web+IR	-16.535920776674633	-80.40541155458558	32684
34ad9c01253b9d1d012ced0fcec7d8661b59425d	english to turkish example-based machine translation with synchronous sstc	software;synchronous generators;parallel corpora english to turkish example based machine translation synchronous sstc example based machine translation corpus based method translated text;translated text;english to turkish example based machine translation;information technology;speech;natural languages;natural languages synchronous generators tree data structures information technology programming character generation computational linguistics software speech;tree data structures;synchronous sstc machine translation example based machine translation;character generation;corpus based method;computational linguistics;example based machine translation;synchronous sstc;programming;parallel corpora;machine translation	Example based machine translation (EBMT) is a corpus-based method which is based on using previously translated text. In this project, English to Turkish EBMT system has been developed which uses the Synchronous SSTC for the representation of the sentences in the parallel corpora. This method proves to be effective especially in structurally different languages such as English and Turkish.	example-based machine translation;parallel text;tesla coil	N. Deniz Alp;Çigdem Turhan	2008	Fifth International Conference on Information Technology: New Generations (itng 2008)	10.1109/ITNG.2008.64	natural language processing;programming;speech recognition;example-based machine translation;computer science;speech;computational linguistics;machine translation;tree;natural language;information technology	Robotics	-22.88427133664929	-82.02027240952033	32790
639b51460b1a80f6dff9dc176f7523f1ed75b9dc	multi-stream spectral representation for statistical parametric speech synthesis	hidden markov models decision trees speech natural languages context speech synthesis pragmatics;speech synthesis decision trees hidden markov models spectral analysis;hmm based speech synthesis;factorised speech representation;over smoothing;factorised speech representation hmm based speech synthesis sub band over smoothing;sub band;single spectrum stream multistream spectral representation statistical parametric speech synthesis hidden markov model hmm based synthesis very large decision tree boundary frequency	In statistical parametric speech synthesis such as Hidden Markov Model (HMM) based synthesis, one of the problems is in the over-smoothing of parameters, which leads to a muffled sensation in the synthesised output. In this paper, we propose an approach in which the high frequency spectrum is modelled separately from the low frequency spectrum. The high frequency band, which does not carry much linguistic information, is clustered using a very large decision tree so as to generate parameters as close as possible to natural speech samples. The boundary frequency can be adjusted at synthesis time for each state. Subjective listening tests show that the proposed approach is significantly preferred over the conventional approach using a single spectrum stream. Samples synthesised using the proposed approach sound less muffled and more natural.	decision tree;frequency band;hidden markov model;markov chain;natural language;smoothing;spectral density;spectral method;speech synthesis	Kayoko Yanagisawa;Ranniery Maia;Yannis Stylianou	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472661	natural language processing;speech recognition;computer science;pattern recognition	Robotics	-10.85669946202357	-86.56316482944138	33048
d647eae8a75433d76fa84c0b9bde52940b881157	speech recognition system based on visual feature for the hearing impaired	speech recognition genetic algorithms handicapped aids hidden markov models;speech recognition auditory system hidden markov models speech enhancement educational institutions nose mouth genetics robustness deafness;hidden markov model;training;speech;global optimisation;hearing impaired;visualization;handicapped aids;visual feature;biological cells;hidden markov models;speech recognition system;visual cues;hidden markov model speech recognition visual feature genetic algorithm;visual features;speech recognition;genetic algorithm;genetic algorithms;genetic algorithm speech recognition system visual feature hearing impaired hidden markov model global optimisation;high speed;gallium	The movements of talkers' face, nose, mouth and throat are known to convey visual cues and represent several different kinds of informationl, and that can improve speech recognition rate, especially for persons with speech-impairments. We proposed a new speech recognition method using these visual features and hidden Markov model (HMM). Based on global optimisation, a new genetic algorithm (GA) for training HMM was proposed. Six chinese vowels were taken as the experimental data, ten handicapped speakers were taken as the testee. Recognition experiments show that the method is effective and high speed and accuracy for speech recognition. At present, the average recognition rate is 91.47% using improved HMM and 88.96% using the classic training HMM algorithm, So the features has very good robustness and the improved HMM is very good.	experiment;genetic algorithm;global optimization;hidden markov model;markov chain;mathematical optimization;speech recognition	Xu Wang;Zhiyan Han;Jian Wang;Mingtao Guo	2008	2008 Fourth International Conference on Natural Computation	10.1109/ICNC.2008.550	speech recognition;computer science;pattern recognition;communication	Robotics	-13.717087925817259	-86.0834180577338	33049
757a9c440a7e4ce2983cb32230e3a593589ef462	identifying `cover songs' with chroma features and dynamic programming beat tracking	dynamic programming;dynamic programming beat tracking;audio signal processing;instruments;chroma features;cross correlation;signal analysis;search methods;beat by chroma representation cover songs chroma features dynamic programming beat tracking music collections automatic search methods music audio recordings chroma feature vectors spectral energy;music audio signal processing dynamic programming;search method;dynamic program;audio recording;feature vector;music audio recordings;beat tracking;multiple signal classification;dynamic programming instruments multiple signal classification music audio recording spatial databases search methods signal analysis digital audio players robustness;beat by chroma representation;database searching;mean reciprocal rank;spatial databases;automatic search methods;robustness;acoustic signal analysis;cover songs;correlation music database searching acoustic signal analysis dynamic programming;chroma feature vectors;correlation;music collections;electrical engineering;digital audio players;database search;music;local alignment;spectral energy	Large music collections, ranging from thousands to millions of tracks, are unsuited to manual searching, motivating the development of automatic search methods. When different musicians perform the same underlying song or piece, these are known as `cover' versions. We describe a system that attempts to identify such a relationship between music audio recordings. To overcome variability in tempo, we use beat tracking to describe each piece with one feature vector per beat. To deal with variation in instrumentation, we use 12-dimensional `chroma' feature vectors that collect spectral energy supporting each semitone of the octave. To compare two recordings, we simply cross-correlate the entire beat-by-chroma representation for two tracks and look for sharp peaks indicating good local alignment between the pieces. Evaluation on several databases indicate good performance, including best performance on an independent international evaluation, where the system achieved a mean reciprocal ranking of 0.49 for true cover versions among top-10 returns.	database;dynamic programming;feature vector;smith–waterman algorithm;spatial variability	Daniel P. W. Ellis;Graham E. Poliner	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.367348	mean reciprocal rank;speech recognition;feature vector;audio signal processing;computer science;multiple signal classification;dynamic programming;signal processing;music;multimedia;correlation;statistics;robustness	Visualization	-7.978563087233053	-92.62196450300824	33094
f9a039e8a98e840466cf176fec3a409d1b6ddaae	linear discriminant analysis f-ratio for optimization of tespar & mfcc features for speaker recongnition	optimization technique;average f ratio;linear discriminate analysis;indexing terms;f ratio;mel frequency cepstral coefficient;asr;automatic speaker recognition;tespar;arithmetic mean;rbf neural network;signal processing;radial basis function neural network;comparative study;signal to noise ratio;mfcc	This paper deals with implementing an efficient optimization technique for designing an Automatic Speaker Recognition (ASR) System, which uses average F-ratio score of TESPAR(Time Encoded Signal Processing And Recognition) and MFCC(Mel frequency Cepstral Coefficients) features, to yield high recognition accuracy even in adverse noisy conditions. A new ranking scheme is also proposed in order to stabilize the rank of features in various noise levels by taking Arithmetic Mean of the F-Ratio scores obtained from various levels of Signal to Noise Ratio (SNR). The result is presented for a Text-Dependent ASR system with 20 speaker database. An RBF (Radial Basis Function) Neural Network is used for Recognition purpose. Also a comparative study has been performed for recognition accuracies of optimized MFCC and TESPAR features and we conclude that new proposed average F-Ratio technique has resulted in better accuracy compared to simple F-ratio in noisy environment and also we came to know that TESPAR features are more redundant compared to MFCC. Index Terms ASR, F-Ratio, Average F-Ratio, TESPAR, RBF Neural Network, MFCC.	artificial neural network;automated system recovery;coefficient;f-ratio;linear discriminant analysis;mathematical optimization;mel-frequency cepstrum;radial basis function;signal processing;signal-to-noise ratio;speaker recognition	K. Anitha Sheela;K. Satya Prasad	2007	Journal of Multimedia	10.4304/jmm.2.6.34-43	speech recognition;computer science;machine learning;signal processing;pattern recognition;mel-frequency cepstrum;statistics	ML	-14.341231768315831	-90.78314807237716	33241
8c02bd985731b860640a0516468816423e5ee24b	automatic generation of a pronunciation dictionary based on a pronunciation network		In this paper, we propose a method for automatically generating a pronunciation dictionary based on a pronunciation neural network that can predict plausible pronunciations (alternative pronunciations) from the canonical pronunciation. This method can generate multiple forms of alternative pronunciations using the pronunciation network for words that only occur a few times in the database and even for unseen words. Experimental results on spontaneous speech show that the automatically-derived pronunciation dictionaries give consistently higher recognition rates and require less computational time for recognition than a conventional dictionary.	acoustic cryptanalysis;acoustic model;artificial neural network;computation;dictionary;spontaneous order;time complexity	Toshiaki Fukada;Yoshinori Sagisaka	1997			speech recognition;pronunciation;computer science	NLP	-19.081074104538217	-86.15142288095244	33248
f25528cde666c7118a43a64c7d8780e580637bf8	國語電話語音辨認之強健性特徵參數及其調整方法 (robust feature parameters and their adaptation methods for telephone mandarin speech recognition) [in chinese]			speech recognition;super robot monkey team hyperforce go!	Yifen Huang;Hsiao-Chuan Wang	1999			mandarin chinese;speech recognition;computer science	NLP	-15.853654795615803	-85.9644538710206	33278
9352dacd4ab772db9ab16e3bdedeeddc7fb25d36	enhanced human-computer speech interface using wavelet computing	speech interfaces;natural language interfaces;human computer interaction;time frequency;speech;speech enhancement;enhanced human computer speech interface;wavelet coefficients enhanced human computer speech interface wavelet computing thresholding algorithm shrink function time frequency spectrogram discontinuities;thresholding algorithm;time frequency analysis human computer interaction natural language interfaces;wavelet transforms;waveshrink;wavelet transform;speech enhancement speech transforms wavelet transforms wavelet coefficients signal to noise ratio noise;threshold selection;wavelet computing;transforms;shrink function speech enhancement waveshrink threshold selection;shrink function;signal to noise ratio;time frequency analysis;wavelet coefficients;time frequency spectrogram discontinuities;noise	In this paper, we design an enhanced human-computer speech interface by wavelet transform. By using a new thresholding algorithm and shrink function, we improve the efficiency of the speech interface. This shrink function tries to decrease sharp time-frequency spectrogram discontinuities by attenuating the wavelet coefficients instead of setting them to zero. This attenuation will be done regarding to the wavelet coefficients distance from the threshold. By applying a lot of tests we evaluated our system for three different languages. The results confirmed the improvement in performances and achievements of our system.	algorithm;coefficient;performance;spectrogram;speech synthesis;thresholding (image processing);wavelet transform	Saeed Ayat	2008	2008 IEEE Conference on Virtual Environments, Human-Computer Interfaces and Measurement Systems	10.1109/VECIMS.2008.4592749	wavelet;speech recognition;time–frequency analysis;second-generation wavelet transform;computer science;cascade algorithm;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;wavelet transform	Visualization	-7.912369152982892	-88.5934609647315	33321
b400dd6008a943e4e6e09c73e9fe9e1c11055cbc	recognition of facial movements and hand gestures using surface electromyogram(semg) for hci based applications	muscle activity;human computer interaction;application software;hci;scattering;computer applications;emg;face recognition;command and control;speech recognition;system testing;prosthetic hand;face recognition human computer interaction application software speech recognition command and control systems computer applications muscles scattering system testing prosthetic hand;electromyogram;command and control systems;muscles	This research reports the recognition of facial movements during unvoiced speech and the identification of hand gestures using surface Electromyogram (sEMG). The paper proposes two different methods for identifying facial movements and hand gestures, which can be useful for providing simple commands and control to computer, an important application of HCI. Experimental results demonstrate that the features of sEMG recordings are suitable for characterising the muscle activation during unvoiced speech and subtle gestures. The scatter plots from the two methods demonstrate the separation of data for each corresponding vowel and each hand gesture. The results indicate that there is small inter-experimental variation but there are large intersubject variations. This inter-subject variation may be attributable to anatomical differences and different speed and style of speaking for the different subjects. The proposed system provides better results when is trained and tested by individual user. The possible applications of this research include giving simple commands to computer for disabled, developing prosthetic hands, use of classifying sEMG for HCI based systems.	electromyography;human–computer interaction	Sridhar Poosapadi Arjunan;Dinesh Kant Kumar	2007	9th Biennial Conference of the Australian Pattern Recognition Society on Digital Image Computing Techniques and Applications (DICTA 2007)	10.1109/DICTA.2007.4426768	facial recognition system;command and control;computer vision;application software;speech recognition;computer science;computer applications;scattering;system testing	HCI	-5.439615348407448	-81.44403361496983	33357
e520e3b66b04bcd142a838f639e34415dc4063fb	a new synthetic speech/sound control language.		The Multi-layered Speech/Sound Synthesis Control Language (MSCL) proposed herein facilitates the synthesizing of several speech modes such as nuance, mental state and emotion, and allows speech to be synchronized to other media easily. MSCL is a multi-layered linguistic system and encompasses three layers: and semantic level layer (The S-layer), interpretation level layer (The I-layer), and parameter level layer (The P-layer). The S-layer is the description level of semantics such as emotional and emphasized speech. The I-layer is the description level of prosodic feature controls and interprets The S-layer scripts to for control on I-layer level. The P-layer represents prosodic parameters for speech synthesis. This multi-level description system is convenient for both laymen and professional users. MSCL also encompasses many e ective prosodic feature control functions such as a time-varying pattern description function, absolute and relative control forms, and SDS(Speaker Dependent Scale). MSCL enables more emotional and expressive synthetic speech than conventional TTS systems. This paper describes these functions and the e ective prosodic feature controls possible with MSCL.	control function (econometrics);dialog system;html;latex;mental state;open road tolling;semantic prosody;speech synthesis;synthetic intelligence	Osamu Mizuno;Shin'ya Nakajima	1998			speech technology;acoustics;speech corpus	NLP	-14.60805880973287	-83.53876970817622	33511
1ae16e4910d92c1b64fbecbeb2c8939ea7859348	robust i-vector extraction tightly coupled with voice activity detection using deep neural networks		This paper describes an extended framework of i-vector feature extraction for improving speaker recognition under noisy conditions. In such speaker recognition applications, voice activity detection (VAD) has been a pre-processing step independent from the succeeding i-vector extraction, just for excluding noisy sounds other than voice of a target speaker. In the proposed framework, i-vector extraction is tightly coupled with VAD. It first estimates voice quality in an utterance as frame- wise voice posteriors using deep neural network-based sound classification, and then it extracts an i-vector of the utterance on the basis of the voice posteriors. The proposed method is able to produce a reliable speaker feature under noisy conditions with help of LSTM-based soft decision VAD. Speaker verification experiments on NIST 2016 SRE evaluation set demonstrate a 14.7% reduction in equal error rates.	artificial neural network;deep learning;experiment;feature extraction;long short-term memory;preprocessor;speaker recognition;voice activity detection	Hitoshi Yamamoto;Koji Okabe;Takafumi Koshinaka	2017	2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)	10.1109/APSIPA.2017.8282114	voice activity detection;noise measurement;speaker recognition;feature extraction;artificial neural network;nist;utterance;artificial intelligence;pattern recognition;computer science	Vision	-16.2520993045459	-89.23807694578767	33595
79d73c4d832cae10eef6ee48ee9c8b1c707f8b5a	icarus: source generator based real-time recognition of speech in noisy stressful and lombard effect environments	background noise;noise adaptation;robust speech recognition;algorithm performance;lombard effect;real time;source generator theory;speech processing;additive noise;ruido aditivo;traitement parole;stress equalization;bruit additif;speech enhancement;speech under stress;senal vocal;signal vocal;automatic speech recognition;resultado algoritmo;temps reel;performance algorithme;present day;real time speech processing;speech recognition;tiempo real;ruido fondo;reconnaissance parole;vocal signal;bruit fond	Abstract   The problem of real-time automatic speech recognition in an adverse environment is addressed. Though much research has been performed in the area of speech recognition, only limited success has been demonstrated for real-time recognition in noisy stressful environments. The primary reason for this is that the performance of present day recognition algorithms are predicated on the assumptions of the environmental settings in which the algorithms have been formulated and implemented. In this paper, we discuss the effects of additive background noise on speech quality and recognition parameters, and propose a source generator based framework to address stress and noise. Using this framework, a computationally efficient real-time recognition system called ICARUS is developed. The speech recognition system incorporates direct processing steps to address the effects of additive noise on the speech signal and stress on the speech production system. Central issues which are addressed include (i) improved characterization of speech spoken in noisy situations involving both parameter estimation methods and analysis of varying speech characteristics spoken in adverse environments (i.e., stress and Lombard effect), (ii) exploration of signal processing strategies tailored to such speech, and (iii) demonstration of real-time system performance of the proposed methods. The proposed recognition system was formulated using a digital signal processing platform. Performance evaluations showed an improvement in speech feature representation under stressed speaking conditions, with an average improvement in recognition rate of +17.28% across eleven noisy stressful speaking conditions.	real-time clock	John H. L. Hansen;Douglas A. Cairns	1995	Speech Communication	10.1016/0167-6393(95)00007-B	voice activity detection;speaker recognition;speech recognition;computer science;speech processing;background noise	NLP	-12.931319107907342	-90.08090775291559	33761
50b754225193b7399aa61294e691a617a4f18fd0	language recognition with support vector machines		Support vector machines (SVMs) have become a popular tool for discriminative classification. Powerful theoretical and computational tools for support vector machines have enabled significant improvements in pattern classification in several areas. An exciting area of recent application of support vector machines is in speech processing. A key aspect of applying SVMs to speech is to provide a SVM kernel which compares sequences of feature vectors–a sequence kernel. We propose the use of sequence kernels for language recognition. We apply our methods to the NIST 2003 language evaluation task. Results demonstrate the potential of the new SVM methods.	kernel (operating system);language identification;speech processing;support vector machine	William M. Campbell;Elliot Singer;Pedro A. Torres-Carrasquillo;Douglas A. Reynolds	2004			support vector machine;machine learning;artificial intelligence;computer science	ML	-16.77672226135372	-90.61062003458132	33849
2cd4182d3e8e3c993a9769697016f5f77f2659af	maximum likelihood and minimum classification error factor analysis for automatic speech recognition	analyse parole;modelizacion;training analysis;correlacion;vocabulaire;metodo estadistico;optimisation;metodo analisis;covariance analysis;analisis factorial;probability;high dimensionality;analisis estadistico;modelo markov;north america;metodo reduccion;america del norte;maximum likelihood;amerique du nord;gaussian processes;amerique;new jersey;maximization;reconocimiento palabra;hidden markov model;analisis palabra;speech processing;speech analysis;vocabulary;maximum vraisemblance;tratamiento palabra;calcul erreur;traitement parole;statistical method;vocabulario;correlation methods;indexing terms;etats unis;maximum likelihood estimation;analisis automatico;classification;estados unidos;senal vocal;modelisation;feature vector;error analysis;signal vocal;automatic speech recognition;methode analyse;automatic analysis;automatic recognition;markov model;analyse factorielle;maximum likelihood estimate;hidden markov models;statistical analysis;expectation maximization;gradient descent;factor analysis;analysis method;methode statistique;feature extraction;high dimensional data;analyse statistique;signal classification;analyse correlation;classification error;analyse automatique;algorithme em;expectation;gradient methods;calculo error;speech recognition;psychanalyse didactique;discriminative training;methode reduction;algoritmo em;error analysis speech recognition maximum likelihood estimation signal classification feature extraction hidden markov models correlation methods covariance analysis optimisation gradient methods gaussian processes probability;reconnaissance parole	Hidden Markov models (HMM’s) for automatic speech recognition rely on high-dimensional feature vectors to summarize the short-time properties of speech. Correlations between features can arise when the speech signal is nonstationary or corrupted by noise. We investigate how to model these correlations using factor analysis, a statistical method for dimensionality reduction. Factor analysis uses a small number of parameters to model the covariance structure of high dimensional data. These parameters can be chosen in two ways: 1) to maximize the likelihood of observed speech signals, or 2) to minimize the number of classification errors. We derive an expectation–maximization (EM) algorithm for maximum likelihood estimation and a gradient descent algorithm for improved class discrimination. Speech recognizers are evaluated on two tasks, one small-sized vocabulary (connected alpha-digits) and one medium-sized vocabulary (New Jersey town names). We find that modeling feature correlations by factor analysis leads to significantly increased likelihoods and word accuracies. Moreover, the rate of improvement with model size often exceeds that observed in conventional HMM’s.	dimensionality reduction;expectation–maximization algorithm;factor analysis;finite-state machine;gradient descent;hidden markov model;markov chain;speech recognition;speech synthesis;vocabulary	Lawrence K. Saul;Mazin G. Rahim	2000	IEEE Trans. Speech and Audio Processing	10.1109/89.824696	speech recognition;computer science;pattern recognition;maximum likelihood;hidden markov model;statistics	ML	-20.080958422857663	-91.97592622670489	33850
c25bd147a47697ced2353edea3bc54870758193c	text-continuous speech recognition based on ica and geometrical learning	continuous speech recognition;hmm;ica;independent component analysis;feature extraction;mel frequency cepstral coefficient;training data;principal component analysis;signal analysis;hidden markov models;speech recognition	We investigate the use of independent component analysis (ICA) for speech feature extraction in digits speech recognition systems. We observe that this may be true for recognition tasks based on Geometrical Learning with little training data. In contrast to image processing, phase information is not essential for digits speech recognition. We therefore propose a new scheme that shows how the phase sensitivity can be removed by using an analytical description of the ICA-adapted basis functions. Furthermore, since the basis functions are not shift invariant, we extend the method to include a frequency-based ICA stage that removes redundant time shift information. The digits speech recognition results show promising accuracy. Experiments show that the method based on ICA and Geometrical Learning outperforms HMM in a different number of training samples.	hidden markov model;independent computing architecture;speech recognition	Wenming Cao;Tiancheng He;Shoujue Wang	2006	2006 IEEE International Conference on Granular Computing	10.1109/GRC.2006.1635877	independent component analysis;training set;speech recognition;feature extraction;computer science;speech;machine learning;invariant;pattern recognition;markov model;redundancy;hidden markov model;shift-invariant system;principal component analysis	Robotics	-10.817566705756242	-93.97728407980365	33869
55a3e69c6a6a6502d0a32b789b72d8fd6b2f42b7	conceptual analysis of noun groups in english	front end;english language;noun;conceptual analysis;left to right;natural language understanding;time use	An expectation-based system, NGP, f o r parsing Engl ish noun groups i n to the Conceptual Dependency representat ion is descr ibed. The system is a par t of ELI (Engl ish Language I n t e rp re te r ) which is used as the f ron t end to several na tu ra l language understanding systems and is capable of handl ing a wide range of sentences of considerable complex i ty . NGP processes the input from l e f t to r i g h t , one word at a t ime, using l i n g u i s t i c and world knowledge to f i nd the meaning of a noun group. D ic t ionary en t r i es fo r i n d i v i d u a l words contain much of the program's knowledge. In a d d i t i o n , a l i m i t e d a b i l i t y f o r the handl ing of s l i g h t l y i nco r rec t sentences and unknown words is incorpora ted. 0. I n t roduc t i on Every na tu ra l language processor has to have the a b i l i t y to i n t e r p r e t noun phrases. This paper describes a set of programs ca l led NGP (Noun Group Processor) which is an i n t e g r a l par t of ELI , the Engl ish Language In te rp re te r (Riesbeck and Schank 1976) which serves as the f r o n t end to three of the Yale na tu ra l language understanding systems, SAM, PAM and WEIS. SAM is a system capable of understanding s to r i es such as var ious newspaper repor ts by using s c r i p t s (Schank and Abelson 1975, 1977; Cu l l i ng fo rd 1975, 1977). PAM is an understanding system which uses general knowledge about peoples' goals and plans (Wilensky 1976). WEIS is a system which understands and c l a s s i f i e s a great v a r i e t y of i so la ted newspaper headlines on i n t e r n a t i o n a l r e l a t i o n s . Thus, our task was to process not only noun phrases of considerable complexity but also to i n t e r p r e t newspaper headl ines, which are not always grammatical ly c o r r e c t . The fo l l ow ing two examples i l l u s t r a t e the kind of sentences our system is able to handle. 1. A CONNECTICUT MAN, JOHN DOE, AGE 23, OF 342 COLLEGE AVENUE, NEW HAVEN WAS PRONOUNCED DEAD AT THE SCENE BY DR. DANA BLAUCHARD, MEDICAL EXAMINER. 2. FUNERAL OF INDIA 'S SHASTRI ATTENDED BY USSR KOSYGIN AND USA HUMPHREY. This work was supported in par t by the Advanced Research Projects Agency of the Department of Defense and monitored under the Of f i ce of Naval Research under cont rac t N00014-75-C-1111 To process such a la rge scope of sentences the program makes extensive use of i t s knowledge of the problem domain and the redundancy of n a t u r a l language expressions. This saves e f f o r t and permits co r rec t processing of such i r r e g u l a r i t i e s of input t ex t as missing commas and a r t i c l e s , or s l i g h t l y i nco r rec t word order. I t also provides f o r the a b i l i t y to ignore unknown words or ( i n some cases) to make p laus ib le i n t e r p r e t a t i o n s of unknown words. This knowledge is kept in the d i c t i o n a r y . The con t ro l mechanisms remain domain independent. NGP is a p roduc t i on l i ke system which uses expectat ions as i t s basic con t ro l mechanism. The problem wi th every p roduc t i on l i ke system is the tendency f o r the accumulation of a la rge number of expectat ions f i g h t i n g fo r a chance to be tes ted . In t h i s work I have t r i e d to develop a theory of how var ious expectat ions are organized and processed, which, I be l i eve , is in f ac t a theory of how people process na tu ra l language. The basic gu id ing p r i n c i p l e fo r t h i s theory was i t s i n t u i t i v e p l a u s i b i l i t y . !• Noun Group Semantics In t h i s paper we w i l l discuss two classes of noun groups according to the conceptual s t r uc tures they generate: PP P ic tu re Producers and CTP Concept Producers. PP's are def ined by Schank (Schank 1975) as concepts which tend to produce p ic tu res of r e a l wor ld items in the mind of a hearer. For example,	commonsense knowledge (artificial intelligence);eli;eve;fo (complexity);linear algebra;naruto shippuden: clash of ninja revolution 3;natural language processing;natural language understanding;neo geo pocket;numerical aperture;numerically controlled oscillator;parsing;problem domain;rp (complexity);rise of nations;theory;tree accumulation;uc browser	Anatole Gershman	1977			natural language processing;noun;computer science;english;front and back ends;proper noun;language transfer;gerund;linguistics	AI	-31.18367579693349	-82.52778707967987	34019
2a664a7afa830909b579918c599973658a30578e	study of prosodic cues of emotions for second lang. speakers using computational methods	computational method	This Paper addresses the question of perceptual detection and acoustic cues analysis in emotional behaviors of free and forced corpus in humans having the language (English) as the second to talk. Our aim is to define the appropriate calculated values of different emotions for further work in computation. We validated emotions here through a test to find robust cues for emotion detection. Most of the researches held lately have been mostly on predefined emotions or to those who speak English as the mother tongue. We here have conducted the tests on subjects who do not speak English as the first language, but instead as second language. In order to get the most close even the most appropriate values we conducted two tests with two conditions: one in which we gave the subject to read a text sample in the behavior it was meant to be read or the way he/she wanted to, second we pumped up the subjects to individually react to a certain behavior or attitude to ours, labeling some of the emotions predicted/proposed and for some new ones as well. Keywords—Second Language, Emotion Detection, TABASCO, Appraisal Theory, Primary Emotions (Anger, Fear, Sadness and joy), PRAAT, Prosodic Cues	acoustic cryptanalysis;computation;emotion recognition;humans;praat;sadness;text corpus	Wajahat M. Qazi;Irfan M. Ahmad;S. M. Bais-e-Aalam	2004			speech recognition;computer science	NLP	-14.658186552158417	-80.89070242091812	34027
95edc3943321eedfdd67dd8e8f15e00e428286f2	using speech assessment technique for the validation of taiwanese speech corpus		This research focuses on validating a Taiwanese speech corpus by using speech recognition and assessment to automatically find the potentially problematic utterances. There are three main stages in this work: acoustic model training, speech assessment and error labeling, and performance evaluation. In the acoustic model training stage, we use the ForSD (Formosa Speech Database) ,provided by Chang Gung University (CGU), to train hidden Markov models (HMMs) as the acoustic models. Monophone, biphone (right context dependent), and triphone HMMs are tested. The recognition net is based on free syllable decoding. The best syllable accuracies of these three types of HMMs are 27.20%, 43.28%, and 45.93% respectively. In the speech assessment and error labeling stage, we use the trained triphone HMMs to assess the unvalidated parts of the dataset. And then we split the dataset as low-scored dataset, mid-scored dataset, and high-score dataset by different thresholds. For the low-scored dataset, we identify and label the possible cause of having such a lower score. We then extract features from these lower-scored utterances and train an SVM classifier to further examine if each of these low-scored utterances is to be removed. In the performance evaluation stage, we evaluate the effectiveness of finding problematic utterances by using 2 subsets of ForSD, TW01, and TW02 as the 使用語音評分技術輔助台語語料的驗證 83 training dataset and one of the following: the entire unprocessed dataset, both mid-scored and high-scored dataset, and high-scored dataset only. We use these three types of joint dataset to train and to evaluate the performance. The syllable accuracies of these three types of HMMs are 40.22%, 41.21%, 44.35% respectively. From the previous result, the disparity of syllable accuracy between the HMMs trained by unprocessed dataset and processed dataset can be 4.13%. Obviously, it proves that the processed dataset is less problematic than unprocessed dataset. We can use speech assessment automatically to find the potential problematic utterances.	acoustic cryptanalysis;acoustic model;binocular disparity;hidden markov model;markov chain;performance evaluation;speech corpus;speech recognition;support vector machine;syllable (computing);triphone	Yu-Jhe Li;Chung-Che Wang;Liang-Yu Chen;Jyh-Shing Roger Jang;Ren-Yuan Lyu	2013	IJCLCLP		speech recognition;speech corpus	NLP	-18.426656441655666	-85.62006149031659	34038
1d3dc9520905eb05c47b7554c58ed5eddfc90d92	combining rtl and ltr hmms to recognise handwritten farsi words of small- and medium-sized vocabularies		In this study, a method for holistic recognition of handwritten Farsi words is proposed, which fuses the outputs of right-to-left (RtL) and left-to-right (LtR) hidden Markov models (HMMs). The experimental results on 16,000 images of 200 names of Iranian cities, from the ‘Iranshahr 3’ are presented and compared with those methods using only RtL or LtR models. Experimental results show that the main sources of error are similar beginnings or similar endings of the words. Since RtL and LtR models when dealing with the words behave differently, there is notable error diversity between the two classifiers in such a way that their combination increases the recognition rate. Compared to the RtL-HMM, the product of output scores of the RtL and LtR-HMMs reduces the classification error to about 6, 6 and 3%, for three different feature sets. A subjective error analysis on the results is also provided.	vocabulary	Seyed Ali Asghar AbbasZadeh Arani;Ehsanollah Kabir;Reza Ebrahimpour	2018	IET Computer Vision	10.1049/iet-cvi.2017.0645	speech recognition;fuse (electrical);artificial intelligence;mathematics;pattern recognition;hidden markov model	Vision	-20.049058547696625	-80.7167131370192	34208
92f203ce701cdc0f7226363986afe233c5fb393b	robust methodology for tts enhancement evaluation	publications robust methodology for tts enhancement evaluation;katedra kybernetiky;synteza řeci;kybernetika;informacni a řidici systemy;automaticke řizeni;poslechove testy;publikace robust methodology for tts enhancement evaluation;uměla inteligence;statisticka významnost;clanek;article;hodnoceni	The paper points to problematic and usually neglected aspects of using listening tests for TTS evaluation. It shows that simple random selection of phrases to be listened to may not cover those cases which are relevant to the evaluated TTS system. Also, it shows that a reliable phrase set cannot be chosen without a deeper knowledge of the distribution of differences in synthetic speech, which are obtained by comparing the output generated by an evaluated TTS system to what stands as a baseline system. Having such knowledge, the method able to evaluate the reliability of listening tests, as related to the estimation of possible invalidity of listening results-derived conclusion, is proposed here and demonstrated on real examples.	baseline (configuration management);netware file system;speech synthesis;synthetic intelligence	Daniel Tihelka;Martin Gruber;Zdenek Hanzlícek	2013		10.1007/978-3-642-40585-3_56	natural language processing;speech recognition	NLP	-19.709179393495845	-83.16888187653166	34238
a059a01a15e6d634fa42c476b054ad96df882d0a	analysis of temporal aggregation and dimensionality reduction on feature sets for speaker identification in wireless acoustic sensor networks		In this paper we analyze the impact of temporal feature aggregation and feature dimensionality reduction on the performance of speaker identification tasks. We investigate these two processing steps in the context of communication layer constraints, such as limited bitrate, and privacy constraints at node level, of a wireless acoustic sensor network. To this end, we extract Modulation-MFCC features and state-of-the-art i-vectors for speaker identification, and investigate temporal aggregation and dimensionality reduction in the feature extraction process. In the evaluation, we use clean data as well as reverberant data to assess the feature sets for different application scenarios. It is found that temporal aggregation has a positive effect on increasing speaker identification performance while respecting the aforementioned privacy constraints and that Linear Discriminant Analysis can be successfully employed for dimensionality reduction.	acoustic cryptanalysis;dimensionality reduction;feature extraction;linear discriminant analysis;modulation;speaker recognition	Alexandru Nelus;Sebastian Gergen;Rainer Martin	2017	2017 IEEE 19th International Workshop on Multimedia Signal Processing (MMSP)	10.1109/MMSP.2017.8122277	artificial intelligence;pattern recognition;wireless sensor network;computer science;linear discriminant analysis;modulation;wireless;dimensionality reduction;feature extraction;mel-frequency cepstrum	ML	-13.917806883138121	-91.675197330087	34349
b425b8dc714a58895744b2b322c3a88a0c4355d7	tv advertisements detection and clustering based on acoustic information	databases;pattern clustering;audio signal processing;initial coarse energy detection;acoustic signal detection clustering algorithms tv broadcasting databases multimedia communication monitoring mel frequency cepstral coefficient detectors image edge detection bayesian methods;acoustic information;acoustics;tv advertisements clustering;acoustic signal processing;data mining;acoustic indexing commercial advertisements detection and clustering;multimedia systems;distance measurement;acoustic indexing;pattern clustering acoustic signal processing audio signal processing multimedia systems;indexation;multimedia communication;commercial advertisements detection and clustering;clustering algorithms;energy detection;multimedia indexing;tv;dynamic time warping;personalized user content;commercial advertisements;tv advertisements detection;dynamic time warping tv advertisements detection tv advertisements clustering acoustic information commercial advertisements multimedia indexing personalized user content initial coarse energy detection	Detection and clustering of commercial advertisements plays an important role in multimedia indexing as well as in the creation of personalized user content. Its aim is at detecting individual commercials within a broadcast and grouping together all repetitions of the same commercial over time. Several algorithms to tackle the detect task using either video and audio or only video cues have been found in existing literature, but none has been found for clustering.In this paper we present an acoustic-only system to perform both the detection and clustering of commercials. Detection is done in three steps, incrementally refining an initial coarse energy detection, while cluster is performed at a later stage over all previously detected commercials to find out how many times each commercial appears. Our detection system achieves 82% precision and recall using only acoustic information. For the clustering step, three algorithms are compared, obtaining best results using a modified dynamic Time Warping approach, which achieves 100% recall and 99% precision.	acoustic cryptanalysis;algorithm;automatic differentiation;cluster analysis;database;dynamic time warping;image processing;performance;personalization;precision and recall;sensor;statistical classification;zero suppression	David Conejero;Xavier Anguera Miró	2008	2008 International Conference on Computational Intelligence for Modelling Control & Automation	10.1109/CIMCA.2008.162	audio signal processing;computer science;machine learning;dynamic time warping;multimedia;cluster analysis;world wide web	SE	-8.246887576286607	-93.83469017281068	34459
2c15035c5fc0d66057ea23ca18ca4684739ed502	evaluation of two simultaneous continuous speech recognition with ica bss and mft-based asr	missing feature theory;acoustic modeling;independent component analysis;feature vector;continuous speech recognition;word recognition;frequency domain	An adaptation of independent component analysis (ICA) and missing feature theory (MFT)-based ASR for two simultaneous continuous speech recognition is described. We have reported on the utility of a system with isolated word recognition, but the performance of the MFT-based ASR is affected by the configuration, such as an acoustic model. The system needs to be evaluated under a more general condition. It first separates the sound sources using ICA. Then, spectral distortion in the separated sounds is estimated to generate missing feature masks (MFMs). Finally, the separated sounds are recognized by MFT-based ASR. We estimate spectral distortion in the temporal-frequency domain in terms of feature vectors, and we generate MFMs. We tested an isolated word and the continuous speech recognition with a cepstral and spectral feature. The resulting system outperformed the baseline robot audition system by 13 and 6 points respectively on the spectral features.	automated system recovery;independent computing architecture;media foundation;speech recognition	Ryu Takeda;Shun'ichi Yamamoto;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno	2007		10.1007/978-3-540-73325-6_38	independent component analysis;speech recognition;feature vector;word recognition;computer science;machine learning;pattern recognition;frequency domain	ML	-12.012253991849734	-91.20061692336616	34466
ee6bead28de27bde833ac1a26435ac9ccb089e66	botest: a framework to test the quality of conversational agents using divergent input examples		Quality of conversational agents is important as users have high expectations. Consequently, poor interactions may lead to the user abandoning the system. In this paper, we propose a framework to test the quality of conversational agents. Our solution transforms working input that the conversational agent accurately recognises to generate divergent input examples that introduce complexity and stress the agent. As the divergent inputs are based on known utterances for which we have the 'normal' outputs, we can assess how robust the conversational agent is to variations in the input. To demonstrate our framework we built ChitChatBot, a simple conversational agent capable of making casual conversation.		Elayne Ruane;Théo Faure;Ross Smith;Dan Bean;Julie Carson-Berndsen;Anthony Ventresque	2018		10.1145/3180308.3180373	human–computer interaction;conversation;chatbot;natural language processing;computer science;artificial intelligence;dialog system;casual	HCI	-26.54158985749905	-86.70561462429643	34479
461ed45e62dbccf15f6ca7dcdb94b0d86a30242d	spline filters for end-to-end deep learning		We propose to tackle the problem of end-to-end learning for raw waveform signals by introducing learnable continuous time-frequency atoms. The derivation of these filters is achieved by defining a functional space with a given smoothness order and boundary conditions. From this space, we derive the parametric analytical filters. Their differentiability property allows gradientbased optimization. As such, one can utilize any Deep Neural Network (DNN) with these filters. This enables us to tackle in a front-end fashion a large scale bird detection task based on the freefield1010 dataset known to contain key challenges, such as the dimensionality of the inputs data (> 100, 000) and the presence of additional noises: multiple sources and soundscapes.	artificial neural network;audio signal processing;bioinformatics resource centers;color gradient;deep learning;dilation (morphology);end-to-end principle;front and back ends;ibm notes;loss function;mathematical optimization;numerical linear algebra;particle filter;randomized algorithm;spline (mathematics);waveform	Randall Balestriero;Romain Cosentino;Hervé Glotin;Richard G. Baraniuk	2018			wavelet;artificial neural network;deep learning;smoothness;computer science;parametric statistics;waveform;curse of dimensionality;pattern recognition;spline (mathematics);artificial intelligence	ML	-14.806147532515162	-97.53138934715399	34487
23bcd56ce3aa0df417c4972f420db45df0a56e9e	transmissions and transitions: a study of two common assumptions in multi-band asr	speech processing;speech analysis;automatic speech recognition multi band asr full band approach frequency space division sub bands phonetic feature transmission phonetic transition lags multi band speech analysis band merging syllabic model phonetic model time alignment;feature extraction speech recognition speech processing;automatic speech recognition;automatic speech recognition speech analysis merging reverberation spatial databases computer science frequency conversion speech recognition signal processing signal generators;feature extraction;speech recognition	Is multi-band ASR inherently inferior to a full-band approach because phonetic information is lost due to the division of the frequency space into sub-bands? Do the phonetic transitions in sub-bands occur at different times? The first statement is a common objection of the critics of multi-band ASR, and the second, a common assumption by multi-band researchers. This paper is dedicated to finding answers to both these questions. To study the first point, we calculate phonetic feature transmission for sub-bands. Not only do we fail to substantiate the above objection, but we observe the contrary. We confirm the second hypothesis by analyzing the phonetic transition lags in each sub-band. These results reinforce our view that multi-band speech analysis provides useful information for ASR, particularly when band merging takes place at the end state for a phonetic or syllabic model, allowing sub-bands to be independently time-aligned within the model.	automated system recovery;automatic system recovery;voice analysis	Nikki Mirghafori;Nelson Morgan	1998		10.1109/ICASSP.1998.675364	voice activity detection;natural language processing;speech recognition;feature extraction;computer science;phonetic search technology;speech processing	SE	-10.393920122699049	-90.56089024938035	34544
b6d623afbc02123f69cfc3d79789c9de76ae486a	real-time flight test track filtering and association using kalman filter and qda classifier		In this paper an on-line track filtering and association algorithm for flight test was proposed. Firstly, a K-means clustering based scheme was used for track initialization and initial state and corresponding covariance matrix estimation for second-order extended Kalman filter. After that, track filtering and association and frequency estimation of Dutch roll were achieved through interactive use of the second-order extended Kalman-filter and the Kalman-predictor based QDA minimum error rate Bayesian classifier. Experimental results had shown that the algorithm can initialize the track reliably, filter and associate tracks in real time and estimate the frequency of flight testing precisely.	kalman filter;real-time clock	Kundong Wang;Yao Ge	2016		10.1007/978-3-319-48036-7_49	naive bayes classifier;covariance matrix;initialization;cluster analysis;kalman filter;filter (signal processing);extended kalman filter;dutch roll;artificial intelligence;computer science;pattern recognition	Robotics	-12.313953852285218	-93.2072039610016	34602
8e8050c5a1a3127efce26a53212b9dab018e3abe	tone recognition for chinese speech: a comparative study of mandarin and cantonese	linguistics speech recognition speech processing natural languages;speech processing;natural languages;automatic continuity;normalization automatic continuous tone recognition chinese speech recognition mandarin cantonese;comparative study;speech recognition;point of view;speech recognition spatial databases testing automatic speech recognition natural languages hidden markov models training data neural networks;linguistics	The paper presents a comparative study on automatic continuous tone recognition for Mandarin and Cantonese. Compared with Mandarin, Cantonese has a much more complex tone system. The effects of F/sub 0/ normalization on the tone recognition of Mandarin and Cantonese are studied. Furthermore, the two tone systems are compared from an engineering point of view. Tone recognition accuracies of 71.50% and 83.06% have been obtained for Cantonese and Mandarin respectively. These results compare favorably with results reported for other tone recognition experiments on the same (for Cantonese) and similar (for Mandarin) databases.	database;experiment;point of view (computer hardware company);speech recognition;super robot monkey team hyperforce go!	Gang Peng;Hongying Zheng;William S.-Y. Wang	2004	2004 International Symposium on Chinese Spoken Language Processing	10.1109/CHINSL.2004.1409629	natural language processing;speech recognition;computer science;comparative research;speech processing;linguistics;natural language	NLP	-16.58310708101422	-85.53294351742194	34637
8bec59ee5743257c0c5ee263052998f20e133c5a	continuous speech recognition using modified stack decoding algorithm	continuous speech recognition	This paper attempts to recognize a speech sequence with a series of words. We represent words with HMM models and use a modified version of stack decoding method [1][2] as the primary algorithm to recognize words. This type of method allows us to recognize a streaming sequence of speech signal as we receive it. We introduced a heuristic function that works with stack decoding algorithm. We also observed how model parameters affect the performance of the algorithm.	algorithm;heuristic (computer science);hidden markov model;speech recognition	David Lee	2005			voice activity detection;natural language processing;speech recognition;computer science;pattern recognition;speech processing	NLP	-19.207288964182524	-87.65281850671495	34756
fd903ae5b576851785ca672aebe627d462bbdb52	the performance comparison of fitting feature with segment model in speaker identification	noise removal fitting feature speaker identification cepstra vocal tract stochastic errors speech signals speaking rate speech production process environmental noise;speaker identification;hidden markov models stochastic resonance speech enhancement speech recognition shape noise shaping stochastic processes polynomials educational institutions computer science;speech processing;vocal tract;performance comparison;noise abatement;speaker recognition;feature vector;stochastic processes;feature extraction;acoustic noise;speech production;feature extraction speaker recognition acoustic noise noise abatement stochastic processes speech processing	Abstiact In general, cepstra are used to manfest the shape of the vocal fract. But it ignores the various stochartic errors of the vocal fract, which can be observedfrom different speech signals obtainedfrom the same word pronounced repearedly even i f ignoring ihe noise and the speaking rate. Now we make use of a method to eliminate the various stochastic errors to some extent, and the feature vectors can represent the essential characteristics of the vocal tract more effectively. In this paper, we analyze ihe speech-production process and utilize a segment model to implement the elimination of the various stochastic errors. The experiments show: besides the elimination of the stochastic errors, the proposed method also can remove ihe noise in the environment.	experiment;feature vector;speaker recognition;tract (literature)	Chenggong Yu;Yingchun Yang;Zhaohui Wu	2003		10.1109/ICSMC.2003.1245647	vocal tract;speaker recognition;speech production;speech recognition;feature vector;feature extraction;computer science;noise;pattern recognition;speech processing;noise control	AI	-11.57506258499098	-89.7614057297274	34788
70e58ea8d235bc0b4a2542023c974b2fa0f93fa5	towards a unified model for low bit-rate speech coding using a recognition-synthesis approach	unified model;speech coding	This paper proposes a recognition-synthesis approach to sp eech coding which uses an underlying formant trajectory model for both recognition and synthesis. It is argued that this “unified” approach to coding has the potential to achieve low data rates whilst preserving speech quality and important paralinguistic information. A simple coding scheme is described which establishes the principles of this approach. In this scheme, the formant analysis method described in [1] is first applied to the input speech. The formant features are then i nput to a lineartrajectory segmental hidden Markov model recognizer [2] in order to locate segment boundaries. The formant parameters for each segment are coded using a linear trajectory description, and used to drive the JSRU parallel-formant synthesizer [3] to reproduce the utterance at the receiver. The coding met hod has been tested on utterances from a variety of speakers. In the current system, which has not yet been optimised for coding efficiency, speech is typically coded at 600-1000 bits/s with good intelligibility, whilst preserving speaker characteristics.	algorithmic efficiency;finite-state machine;hidden markov model;intelligibility (philosophy);markov chain;speech coding;unified model;while	Wendy J. Holmes	1998			speech recognition;artificial intelligence;unified model;speech coding;pattern recognition;low bit;computer science	NLP	-11.297905393479057	-87.57014872726522	34820
a7d24bec959b258d9ebf32bb1c0e8db826eb4ff9	aoidos: a system for the automatic scansion of poetry written in portuguese		Scansion is the activity of determining the patterns that give verses their poetic rhythm. In Portuguese, this means discovering the number of syllables that the verses in a poem have and fitting all verses to this measure, while attempting to pronounce syllables so that an adequate stress pattern is produced. This article presents Aoidos, a rule-based system that takes a poem written in the Portuguese language and performs scansion automatically, further providing an analysis of rhymes. The system works by making a phonetic transcription of a poem, determining the number of poetic syllables that the verses in the poem should have, fitting all the verses according to this measure and looking for verses that rhyme. Experiments show that the system attains a high accuracy rate (above 98%).		Adiel Mittmann;Aldo von Wangenheim;Alckmar Luiz dos Santos	2016		10.1007/978-3-319-75487-1_46	computer science;rhyme;scansion;poetry;artificial intelligence;natural language processing;rhythm;phonetic transcription;linguistics;portuguese	NLP	-25.656009662304275	-80.51078637714858	34910
ffd4d61329a215ca1f6ffed5ae82dd4c67cdb184	recipe sub-goals and graphs: an evaluation by cooks	multimedia;instruction enhancement;cooking recipes	Following recipes can be difficult for cooks. Many recipes use technical culinary language and condense their instructions into brief sentences, cooks may also get lost in long paragraphs as they jump around the recipe to find tasks to perform in parallel. Multimedia content has been shown to increase the confidence of cooks but few comparative evaluations have been reported. In this study we evaluated the effect of adding pictures of interim goal states to a plain text recipe and the effect of presenting recipe steps in a dependency graph representation. Initial results indicate that cooks value pictures of interim goals states to compare their ingredients against, and prefer a graph representation of a recipe because it supports the cook's non-linear path through recipe instructions.	graph (abstract data type);graph (discrete mathematics);nonlinear system	Lucy Buykx;Helen Petrie	2012		10.1145/2390776.2390789	computer science;artificial intelligence;algorithm	HCI	-33.52230253562703	-86.50140323573741	34962
14c296d4adc2d396ed71505e3c12b64f7fe02e12	categorization of digital ink elements using spectral features	unsupervised clustering;fourier transform;digital ink	Desriptors for digital ink are usually sequences of features that evolve with time. Since handwriting is an oscillatory process, it is reasonable to think that there is much information in the frequency spectra of such signals. In particular, ink elements of different nature, like text and graphics, might present very different spectra due to different gestural behaviours needed to draw them. Therefore, the descriptor we propose is the Fourier transform of the angle difference between successive ink segments. On a database containing text and symbols, an unsupervised clustering is performed based on this descriptor and clear clusters corresponding to text-only and graphic-only elements emerge.	categorization;cluster analysis;graphics;text-based user interface	José A. Rodríguez;Gemma Sánchez;Josep Lladós	2007		10.1007/978-3-540-88188-9_18	fourier transform;computer vision;speech recognition;computer science;machine learning;multimedia	Graphics	-6.755804029252022	-91.91039622775948	34995
35ec059f6d4069124bdaa92eea77ab2bb640d5d6	generalization performance in parsec - a structured connectionist parsing architecture		This paper presents PARSEC-a system for generating connectionist parsing networks from example parses. PARSEC is not based on formal grammar systems and is geared toward spoken language tasks. PARSEC networks exhibit three strengths important for application to speech processing: 1) they learn to parse, and generalize well compared to hand-coded grammars; 2) they tolerate several types of noise; 3) they can learn to use multi-modal input. Presented are the PARSEC architecture and performance analyses along several dimensions that demonstrate PARSEC's features. PARSEC's performance is compared to that of traditional grammar-based parsing systems.	connectionism;formal grammar;modal logic;parsec;parsing;speech processing	Ajay N. Jain	1991			natural language processing;speech recognition;computer science	NLP	-17.691569975301107	-86.8501965267275	35118
0a3ac5c0e40661262f8f574cd3f6954c910ecbba	mechanical versus perceptual constraints as determinants of articulatory strategy	speech production	This paper summarizes the results of a series of experiments conducted to investigate various aspects of normal pharyngeal articulation and the nature of pharyngeal coarticualtion. Video fiberscopic imaging, electromagnetography and acoustic analysis techniques were used to obtain empirical and quantitative data on the use of the pharynx in speech production. The overall results suggest that mechanical constraints determine to a great extent the articulatory strategy used by the speaker to achieve the perceptual/acoustic contrast essential for the process of speech encoding.	acoustic cryptanalysis;biconnected component;experiment;speech coding	Ahmed M. Elgendy;Louis C. W. Pols	2001			speech production;economics;linguistics	HCI	-9.62508446508383	-83.42112590452203	35141
6c96ec4df8ca940187b595ffd1999f1f5233abc8	unsupervised accent classification for deep data fusion of accent and language information	accent classification;tf idf;dialect identification;ut podcast;nlp	Automatic Dialect Identification (DID) has recently gained substantial interest in the speech processing community. Studies have shown that the variation in speech due to dialect is a factor which significantly impacts speech system performance. Dialects differ in various ways such as acoustic traits (phonetic realization of vowels and consonants, rhythmical characteristics, prosody) and content based word selection (grammar, vocabulary, phonetic distribution, lexical distribution, semantics). The traditional DID classifier is usually based on Gaussian Mixture Modeling (GMM), which is employed as baseline system. We investigate various methods of improving the DID based on acoustic and text language sub-systems to further boost the performance. For acoustic approach, we propose to use i-Vector system. For text language based dialect classification, a series of natural language processing (NLP) techniques are explored to address word selection and grammar factors, which cannot be modeled using an acoustic modeling system. These NLP techniques include: two traditional approaches, including N-Gram modeling and Latent Semantic Analysis (LSA), and a novel approach based on Term Frequency–Inverse Document Frequency (TFIDF) and logistic regression classification. Due to the sparsity of training data, traditional text approaches do not offer superior performance. However, the proposed TF-IDF approach shows comparable performance to the i-Vector acoustic system, which when fused with the i-Vector system results in a final audio-text combined solution that is more discriminative. Compared with the GMM baseline system, the proposed audio-text DID system provides a relative improvement in dialect classification performance of +40.1% and +47.1% on the self-collected corpus (UT-Podcast) and NIST LRE-2009 data, respectively. The experiment results validate the feasibility of leveraging both acoustic and textual information in achieving improved DID performance. © 2015 Elsevier B.V. All rights reserved.	acoustic cryptanalysis;acoustic model;baseline (configuration management);google map maker;latent semantic analysis;logistic regression;n-gram;natural language processing;podcast;sms language;semantic prosody;sparse matrix;speech processing;tf–idf;unsupervised learning;vocabulary	John H. L. Hansen;Gang Liu	2016	Speech Communication	10.1016/j.specom.2015.12.004	natural language processing;speech recognition;computer science;linguistics;tf–idf	NLP	-17.91234896765376	-87.18453385844172	35227
2eaf4130dccfd375cea7d1037f2215eceb0bb99b	when a dog is a cat and how it changes your pupil size: pupil dilation in response to information mismatch		In the present study, we investigate pupil dilation as a measure of lexical retrieval. We captured pupil size changes in reaction to a match or a mismatch between a picture and an auditorily presented word in 120 trials presented to ten native speakers of Swedish. In each trial a picture was displayed for six seconds, and 2.5 seconds into the trial the word was played through loudspeakers. The picture and the word were matching in half of the trials, and all stimuli were common high-frequency monosyllabic Swedish words. The difference in pupil diameter trajectories across the two conditions was analyzed with Functional Data Analysis. In line with the expectations, the results indicate greater dilation in the mismatch condition starting from around 800 ms after the stimulus onset. Given that similar processes were observed in brain imaging studies, pupil dilation measurements seem to provide an appropriate tool to reveal lexical retrieval. The results suggest that pupillometry could be a viable alternative to existing methods in the field of speech and language processing, for instance across different ages and clinical groups.	dilation (morphology);functional data analysis;loudspeaker;medical imaging;onset (audio)	Lena F. Renner;Marcin Wlodarczak	2017			speech recognition;pupil;size pupil;dilation (morphology);computer science	HCI	-8.461740892150102	-82.11526249345225	35230
4c2f94b555df1ee7f6e0859e7e70b6afd1d92d29	prosodic structure and phonetic processing: a cross-linguistic study	cognitive psychology;psycholinguistics;psychophysics	Dutch and Spanish differ in how predictable the stress pattern is as a function of the segmental content: it is correlated with syllable weight in Dutch but not in Spanish. In the present study, two experiments were run to compare the abilities of Dutch and Spanish speakers to separately process segmental and stress information. It was predicted that the Spanish speakers would have more difficulty focusing on the segments and ignoring the stress pattern than the Dutch speakers. The task was a speeded classification task on CVCV syllables, with blocks of trials in which the stress pattern could vary versus blocks in which it was fixed. First, we found interference due to stress variability in both languages, suggesting that the processing of segmental information cannot be performed independently of stress. Second, the effect was larger for Spanish than for Dutch, suggesting that that the degree of interference from stress variation may be partially mitigated by the predictability of stress placement in the language.	experiment;heart rate variability;interference (communication);statistical classification;syllable	Christophe Pallier;Anne Cutler;Núria Sebastián-Gallés	1997				NLP	-10.59634452357037	-81.11591410907224	35269
d16321e7c2e0a0bb21166aea6b6879423735b4c1	a new method of the automatically marked chinese part of speech based on gaussian prior smoothing maximum entropy model	maximum entropy methods;parameters sparse phenomenon;gaussian prior smoothing method gaussian prior smoothing maximum entropy model natural language processing parameters sparse phenomenon over fit training data;gaussian processes;smoothing method;speech processing;gaussian prior smoothing method;smoothing methods;over fit training data;part of speech;maximum entropy model;smoothing methods entropy computer science probability distribution hidden markov models educational institutions natural language processing training data speech processing natural languages;natural language processing;speech processing gaussian processes maximum entropy methods natural language processing smoothing methods;maximum entropy;gaussian prior smoothing maximum entropy model	With its many virtues, maximum entropy (ME) model has been favored in natural language processing. Because of the limitation of the training data, the parameters sparse phenomenon is serious in Chinese part of speech. The model is prone to over fit training data, therefore some smoothing method should be applied on maximum entropy model. While several smoothing methods for maximum entropy models have been proposed to address this problem, Gaussian prior smoothing method has an outstanding performance. Based on this smoothing maximum entropy model and characteristics of Chinese, a new Chinese part-of-speech system is presented. Result of experiment shows that it works well.	entropy (information theory);experiment;natural language processing;smoothing;sparse matrix	Wei Zhao;Faxing Zhao;Wenhui Li	2007	Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)	10.1109/FSKD.2007.86	maximum-entropy markov model;speech recognition;binary entropy function;computer science;principle of maximum entropy;machine learning;pattern recognition;speech processing;maximum entropy spectral estimation;statistics;smoothing	NLP	-21.731296590413358	-92.02065497720719	35389
822b3125a92a706c14cf5d1ee7bf80f3e303405b	using visual speech information and perceptually motivated loss functions for binary mask estimation		This work is concerned with using deep neural networks for estimating binary masks within a speech enhancement framework. We first examine the effect of supplementing the audio features used in mask estimation with visual speech information. Visual speech is known to be robust to noise although not necessarily as discriminative as audio features, particularly at higher signal-to-noise ratios. Furthermore, most DNN approaches to mask estimate use the cross-entropy (CE) loss function which aims to maximise classification accuracy. However, we first propose a loss function that aims to maximise the hit minus false-alarm (HIT-FA) rate of the mask, which is known to correlate more closely to speech intelligibility than classification accuracy. We then extend this to a hybrid loss function that combines both the CE and HIT-FA loss functions to provide a balance between classification accuracy and HIT-FA rate of the resulting masks. Evaluations of the perceptually motivated loss functions are carried out using the GRID and larger RM-3000 datasets and show improvements to HIT-FA rate and ESTOI across all noises and SNRs tested. Tests also found that supplementing audio with visual information into a single bimodal audio-visual system gave best performance for all measures and conditions tested.	artificial neural network;cross entropy;deep learning;intelligibility (philosophy);loss function;signal-to-noise ratio;speech enhancement	Danny Websdale;Ben P. Milner	2017		10.21437/AVSP.2017-9	discriminative model;artificial neural network;binary number;speech enhancement;intelligibility (communication);pattern recognition;computer science;artificial intelligence	ML	-14.303560246737128	-91.0818431625981	35412
baacc572bea81e8cc670234cb4621ab05c7871b1	visual speech synthesis in spanish using an optical flow algorithm	optical flow;speech synthesis		algorithm;optical flow;speech synthesis	Edson Bárcenas;Jorge Galán;Carolina Soto;Juan F. Urbina;Sandra Vásquez;Pedro R. Vizcaya	2001			speech recognition;computer science;optical flow;speech synthesis	Robotics	-14.823000768906562	-85.75912920480688	35677
9e7ff44e4e7f056fae2e03072d9073e6fe68232a	significance of the modified group delay feature in speech recognition	analyse parole;class separability;traitement signal;processus gauss;group delay;evaluation performance;speech recognition feature extraction fourier transforms data mining speech processing delay effects resonance speech coding wrapping signal processing;robustness class separability feature extraction feature selection gaussian mixture models gmms group delay function hidden markov models hmms phase spectrum;performance evaluation;phase spectra;retardo grupo;features extraction;fourier transform;spectral representation;formant;retard groupe;pitch acoustics;spectrum analysis;cepstral features;analyse spectre;hidden markov model;analisis palabra;analisis espectro;speech processing feature extraction fourier transforms speaker recognition;evaluacion prestacion;signal analysis;speech processing;speech analysis;modele markov variable cachee;generalized derivation;tratamiento palabra;traitement parole;analisis de senal;pitch periodicity effects;tonie;spectrum;probabilistic approach;separability;speech perception;senal vocal;verbal perception;language recognition;speaker recognition;signal vocal;continuous speech recognition;phase spectrum;fourier transform magnitude;gaussian mixture model;cepstral analysis;percepcion verbal;hidden markov models;separabilidad;reconocimiento voz;group delay function;gaussian mixture models gmms;analyse cepstrale;fourier transformation;representation signal;enfoque probabilista;approche probabiliste;feature extraction;signal processing;altura sonida;robustesse;fourier transforms;signal representation;transformation fourier;hidden markov models hmms;transition phase;reconnaissance locuteur;pattern recognition;phase transitions;signal acoustique;speech recognition;transicion fase;robustness;separabilite;feature selection;teoria mezcla;acoustic signal	Spectral representation of speech is complete when both the Fourier transform magnitude and phase spectra are specified. In conventional speech recognition systems, features are generally derived from the short-time magnitude spectrum. Although the importance of Fourier transform phase in speech perception has been realized, few attempts have been made to extract features from it. This is primarily because the resonances of the speech signal which manifest as transitions in the phase spectrum are completely masked by the wrapping of the phase spectrum. Hence, an alternative to processing the Fourier transform phase, for extracting speech features, is to process the group delay function which can be directly computed from the speech signal. The group delay function has been used in earlier efforts, to extract pitch and formant information from the speech signal. In all these efforts, no attempt was made to extract features from the speech signal and use them for speech recognition applications. This is primarily because the group delay function fails to capture the short-time spectral structure of speech owing to zeros that are close to the unit circle in the z-plane and also due to pitch periodicity effects. In this paper, the group delay function is modified to overcome these effects. Cepstral features are extracted from the modified group delay function and are called the modified group delay feature (MODGDF). The MODGDF is used for three speech recognition tasks namely, speaker, language, and continuous-speech recognition. Based on the results of feature and performance evaluation, the significance of the MODGDF as a new feature for speech recognition is discussed	cepstrum;group delay and phase delay;image warping;language identification;performance evaluation;quasiperiodicity;short-time fourier transform;speaker recognition;spectral density;speech processing;speech recognition;syllable;wrapping (graphics)	Rajesh Mahanand Hegde;Hema A. Murthy;Venkata Ramana Rao Gadde	2007	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2006.876858	voice activity detection;fourier transform;linear predictive coding;speech recognition;computer science;machine learning;signal processing;pattern recognition;speech processing;mathematics;hidden markov model	ML	-8.622669234460975	-89.95270183425319	35925
1342b370b47902e0504bf3391290bbf991208f84	accent rating by native and non-native listeners	educational institutions speech databases analysis of variance speech processing computers reliability;native listeners accent rating nonnative listeners native language listener foreign accent native english finnish german mandarin english sentences native english talkers;natural language processing;native vs non native listeners perceptual evaluation	This study investigates the influence of listener native language with respect to talker native language on perception of degree of foreign accent in English. Listeners from native English, Finnish, German and Mandarin backgrounds rated the accentedness of native English, Finnish, German and Mandarin talkers producing a controlled set of English sentences. Results indicate that non-native listeners, like native listeners, are able to classify non-native talkers as foreign-accented, and native talkers as unaccented. However, while non-native talkers received higher accentedness ratings than native talkers from all listener groups, non-native listeners judged talkers with non-native accents less harshly than did native English listeners. Similarly, non-native listeners assigned higher degrees of foreign accent to native English talkers than did native English listeners. It seems that non-native listeners give accentedness ratings that are less extreme, or closer to the centre of the rating scale in both directions, than those used by native listeners.	rating scale;super robot monkey team hyperforce go!	Mirjam Wester;Cassie Mayo	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6855098	natural language processing;speech recognition;computer science	Mobile	-11.44142816128673	-82.06599923855478	36034
56b09d25877285b7d87bb76de2200fb5f623d596	malay anaphor and antecedent candidate identification: a proposed solution	malay text;knowledge poor;anaphor;pronoun;antecedent	This paper discusses on Malay language anaphor and antecedent candidate determination using the knowledge-poor techniques. The process to determine the candidate for anaphor and antecedent is important because the usage of pronouns in a text is not always considered as an anaphor. Sometimes pronoun referred to something outside the context or does not refer to any situation in the text. Such a situation is also exhibited in the use of pronouns in Malay language. Therefore, certain rules must be issued to identify the antecedent and anaphor candidate. Pronoun usage in Malay language does indicate the gender of the person, but to distinguish the status of the person such as imperial family, honorable people and common people. Thus, generic rules that have been used by other languages cannot simply be adapted for Malay language. The proposed solution concerns with the distance of each candidate and location of the Subject-Verb-Object (SVO) used to determine the anaphor candidate. As such, syntactic information, semantic information and distance of anaphor-antecedent are seen important to determine the antecedent candidate.	anaphora (linguistics)	Noorhuzaimi Karimah Mohd Noor;Shahrul Azman Mohd. Noah;Mohd Juzaiddin Ab Aziz;Mohd Pouzi Hamzah	2012		10.1007/978-3-642-28493-9_16	natural language processing;malay;machine learning;syntax;artificial intelligence;computer science;pronoun	Crypto	-30.99937977269459	-85.99393469148507	36152
8c29a60d42b38b5e1cb1cdb0ae67c489dbce8bbe	vision-based sign language recognition using sign-wise tied mixture hmm	sign language recognition;probability density;sign language;chinese sign language;parameter estimation	In this paper, a new sign-wise tied mixture HMM (SWTMHMM) is proposed and applied in vision-based sign language recognition (SLR). In the SWTMHMM, the mixture densities of the same sign model are tied so that the states belonging to the same sign share a common local codebook, which leads to robust model parameters estimation and efficient computation of probability densities. For the sign feature extraction, an effective hierarchical feature description scheme with different scales of features to characterize sign language is presented. Experimental results based on 439 frequently used Chinese sign language (CSL) signs show that the proposed methods can work well for the medium vocabulary SLR in the unconstrained environment.	background subtraction;codebook;computation;estimation theory;feature extraction;hidden markov model;language identification;vocabulary	Liang-Guo Zhang;Gaolin Fang;Wen Gao;Xilin Chen;Yiqiang Chen	2004		10.1007/978-3-540-30542-2_127	natural language processing;probability density function;speech recognition;sign language;computer science;estimation theory;statistics	Vision	-18.418623776343928	-92.26811588333014	36232
9b34dd764b94eb3371fc76fd3333c23b3af463fd	a statistical approach to semi-supervised speech enhancement with low-order non-negative matrix factorization		Compared to generic source separation, NMF for speech enhancement is relatively underexplored. When applied to the latter problem, NMF is bereft of performance consistency (across runs and data samples), esp. with small-sized dictionaries. This limitation raises the need for higher-order representations, leading to increased computational costs. In this paper, we propose a statistical-estimation technique that attempts to bridge this gap. Our approach combines multiple low-order NMF decompositions of noisy speech to increase the overall enhancement performance. We show PESQ improvements of up to 0.24 beyond what is achievable by a single NMF parametrization and, at iso-performance levels, major reductions in computational cost.	algorithmic efficiency;computation;dictionary;estimation theory;non-negative matrix factorization;pesq;semi-supervised learning;semiconductor industry;source separation;speech enhancement;statistical model	Shoaib Mohammed;Ivan Tashev	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952215	pattern recognition;computer science;parametrization;noise measurement;artificial intelligence;pesq;spectrogram;source separation;signal-to-noise ratio;speech enhancement;non-negative matrix factorization	Vision	-17.022459845386926	-92.15638062225486	36245
acd1b92ef208ad21d7de3615cbaa89e273e18bf2	implementation of a practical query-by-singing/humming (qbsh) system and its commercial applications	engines feature extraction harmonic analysis multiple signal classification accuracy electronic mail;engines feature extraction servers software smart phones accuracy digital signal processing;query processing digital signal processing chips embedded systems information retrieval systems music;query processing;signal representation acoustic signal processing feature extraction music query processing;user humming dataset query by singing system query by humming system qbsh system polyphonic music retrieval music retrieval system server side software stack dsp module digital signal processor stand alone embedded platform smart phone laptop karaoke polyphonic music dataset;acoustic signal processing;dtw qbsh polyphonic music predominantmelody extraction harmonic structure matching engine;embedded systems;feature extraction;signal representation;information retrieval systems;digital signal processing chips;music;monophonic music datasets query by singing humming system qbsh system polyphonic music tracks aac files mp3 files reference database reference db melody extraction error method polyphonic music signals harmonic structure matching engine modified dynamic time warping dtw asymmetric path chroma scale representation smart phone laptop karaoke polyphonic music datasets	This paper proposes a practical query-by-singing/humming (QbSH) system that retrieves polyphonic music such as an MP3 and its commercial applications. The performance of music retrieval system is mainly affected by the server. This paper discusses developing the state-of-the-art server side software stack which has several managers and plug-in engines. It also describes implementation of digital signal processor (DSP) module for stand-alone embedded platforms. The paper shows three different models for its commercial applications like smart phone, laptop and karaoke. We evaluate the performance of the proposed system with polyphonic music datasets using users' humming datasets as the input query.	digital signal processor;embedded system;laptop;mp3;plug-in (computing);server (computing);server-side;signal processing;smartphone	Chai-Jong Song;Hochong Park;Chang-Mo Yang;Sei-Jin Jang;Seok-Pil Lee	2013	2013 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/TCE.2013.6531124	speech recognition;feature extraction;computer science;music;multimedia	Mobile	-8.703580446280915	-94.20419973090256	36282
3607fa3a697412c3b77698505ded1f45f722fdeb	dictionary look-up with katakana variant recognition		The Japanese language has rich variety and quantity of word variant. Since 1980s, it has been recognized that this richness becomes an obstacle against natural language processing. A complete solution, however, has not been presented yet. This paper proposes a method to recognize Katakana variants—a major type of word variant in Japanese—in the process of dictionary look-up. For a given set of variant generation rules, the method executes variant generation and entry retrieval simultaneously and efficiently. We have developed the seven-layered rule set (216 rules in total) according to the specification manual of UniDic-2.1.0 and other sources. An experiment shows that the spelling-variant generator with 102 rules in the first five layers is almost perfect. Another experiment shows that the form-variant generator with all 216 rules is powerful and 77.7% of multiple spellings of Katakana loanwords are unnecessary (i.e., can be removed). This result means that the proposed method can drastically reduce the number of variants that we have to register into a dictionary in advance.	algorithm;data dictionary;lookup table;natural language processing	Satoshi Sato	2012			natural language processing;obstacle;artificial intelligence;computer science;katakana	NLP	-23.029729575842293	-80.28346300297683	36313
57f6084ce85740d2c57693a4aa7a030985224ee2	bayesian phonotactic language model for acoustic unit discovery		Recent work on Acoustic Unit Discovery (AUD) has led to the development of a non-parametric Bayesian phone-loop model where the prior over the probability of the phone-like units is assumed to be sampled from a Dirichlet Process (DP). In this work, we propose to improve this model by incorporating a Hierarchical Pitman-Yor based bigram Language Model on top of the units' transitions. This new model makes use of the phonotactic context information but assumes a fixed number of units. To remedy this limitation we first train a DP phone-loop model to infer the number of units, then, the bigram phone-loop is initialized from the DP phone-loop and trained until convergence of its parameters. Results show an absolute improvement of 1–2%on the Normalized Mutual Information (NMI) metric. Furthermore, we show that, combined with Multilingual Bottleneck (MBN) features the model yields a same or higher NMI as an English phone recogniser trained on TIMIT.	acoustic cryptanalysis;bigram;language model;mutual information;non-maskable interrupt;timit	Lucas Ondel;Lukás Burget;Jan Cernocký;Santosh Kesiraju	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7953258	phone;computer science;normalization (statistics);machine learning;data modeling;language model;pattern recognition;hidden markov model;mutual information;artificial intelligence;bigram;dirichlet process	ML	-18.392709128327425	-90.3735351173839	36465
6756e4a64a4fcf4a456205fab921254d512938f6	towards confidence measures on fundamental frequency estimations	confidence measures;estimation;pitch;fundamental frequency	The fundamental frequency is one of the prosodic parameters, and many algorithms have been developed for estimating the fundamental frequency of speech signals. Most of them provide good results on good quality speech signals, but their performance degrades when dealing with noisy signals. Moreover, although some provide a probability for the voicing decision, none of them indicate how reliable the estimated fundamental frequency is. In this paper, we investigate the computation of a confidence (or reliability) measure on the estimated fundamental frequency values. A neural network based approach is proposed for computing the posterior probability that the estimated fundamental frequency is correct. Experiments are conducted on the PTDB-TUG pitch-tracking database, using three fundamental frequency estimation algorithms.	algorithm;artificial neural network;computation;experiment;spectral density estimation	Boyuan Deng;Denis Jouvet;Yves Laprie;Ingmar Steiner;Aghilas Sini	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7953229	estimation;speech recognition;mathematics;pitch;fundamental frequency;statistics	ML	-14.078138547166198	-93.6639114702109	36504
094469c60a3460f801667ec791ede10bc6a9f96b	improving listeners' experience for movie playback through enhancing dialogue clarity in soundtracks	center channel extraction;speech processing;dialogue clarity;5 1 downmix;quality of experience	This paper presents a method for improving users' quality of experience through processing of movie soundtracks. The dialogue clarity enhancement algorithms were introduced for detecting dialogue in movie soundtrack mixes and then for amplifying the dialogue components. The front channel signals (left, right, center) are analyzed in the frequency domain. The selected partials in the center channel signal, which yield high disparity between left and right channels, are detected as dialogue. Subsequently, the dialogue frequency components are boosted to achieve an increased dialogue intelligibility. Techniques for reduction of artifacts in the processed signal are also introduced. It is done through smoothing in the time domain and in the frequency domain, applied to reduce unpleasant artifacts. The results of objective and subjective tests are provided, which prove that an increased dialogue intelligibility is achieved with the aid of the proposed algorithm. The algorithm is particularly applicable in mobile devices while listening in changing conditions and in the presence of noise.		Kuba Lopatka;Andrzej Czyzewski;Bozena Kostek	2016	Digital Signal Processing	10.1016/j.dsp.2015.08.015	speech recognition;computer science;speech processing;multimedia	NLP	-8.105572288328942	-88.37505069565887	36518
5e2b08bb4974c403c07f86926ac5179990b1fe06	a neural network model for speech intelligibility quantification	cognitive science;speech intelligibility;blind estimation;spectrum;subjects outside of the university themes;hybrid model;hilbert transform;back propagation network;feature extraction;principal component analysis;indexation;speech envelope;speech transmission index;neural network model;intelligibility;human perception;blind system identification;qc221 246 acoustics sound;artificial neural network;neural network	A neural network based model is developed to quantify speech intelligibility by blind-estimating speech transmission index, an objective rating index for speech intelligibility of transmission channels, from transmitted speech signals without resort to knowledge of original speech signals. It consists of a Hilbert transform processor for speech envelope detection, a Welch average periodogram algorithm for envelope spectrum estimation, a principal components analysis (PCA) network for speech feature extraction and a multi-layer back-propagation network for non-linear mapping and case generalisation. The developed model circumvents the use of artificial test signals by exploiting naturally occurring speech signals as probe stimuli, reduces measurement channels from two to one and hence facilitates in situ assessment of speech intelligibility. From a cognitive science viewpoint, the proposed method might be viewed as a successful paradigm of mimicking human perception of speech intelligibility using a hybrid model built around artificial neural networks. # 2005 Elsevier B.V. All rights reserved.	algorithm;artificial neural network;backpropagation;cognitive science;envelope detector;feature extraction;hilbert transform;intelligibility (philosophy);layer (electronics);network model;nonlinear system;principal component analysis;programming paradigm;software propagation;spectral density estimation;welch's method	Francis F. Li;Trevor J. Cox	2007	Appl. Soft Comput.	10.1016/j.asoc.2005.05.002	voice activity detection;linear predictive coding;speech recognition;computer science;machine learning;speech coding;speech processing;intelligibility;artificial neural network	AI	-10.471089259036003	-88.20115781595487	36714
f1c465e216215e300c97badb1fc98f47b72cdf6f	direct, modular and hybrid audio to visual speech conversion methods - a comparative study	comparative study	A systematic comparative study of audio to visual speech conversion methods is described in this paper. A direct conversion system is compared to conceptually different ASR based solutions. Hybrid versions of the different solutions will also be presented. The methods are tested using the same speech material, audio preprocessing and facial motion visualization units. Only the conversion blocks are changed. Subjective opinion score evaluation tests prove the naturalness of the direct conversion is the best.	preprocessor	György Takács	2009			speech recognition;computer science;comparative research;speech coding;multimedia	HCI	-15.282556613209893	-84.59269999841496	36789
d15cc9e5b9acb09a85ae81e0efa55c73b9aa05dc	human and machine consonant recognition	front end;filtering;syllabe;43 71 e;filtrage;filter bank;banc filtre;hidden markov model;speech processing;filtrado;modele markov variable cachee;additive noise;tratamiento palabra;ruido aditivo;traitement parole;43 72 n;parameterization;bruit additif;probabilistic approach;parametrizacion;reduccion ruido;speaker recognition;automatic speech recognition;accuracy;histogram;automatic recognition;cepstral analysis;precision;hidden markov models;reconocimiento voz;syllable;histogramme;43 72 d;analyse cepstrale;enfoque probabilista;approche probabiliste;noise reduction;banco filtro;43 71 g;consonant;reduction bruit;reconnaissance locuteur;speaker dependent;speech recognition;43 72 n automatic speech recognition;consonante;reconnaissance parole;silaba;histograma;parametrisation;consonant identification;consonne;reconocimiento automatico;reconnaissance automatique;noise	Three traditional ASR parameterizations matched with Hidden Markov Models (HMMs) are compared to humans for speaker-dependent consonant recognition using nonsense syllables degraded by highpass filtering, lowpass filtering, or additive noise. Confusion matrices were determined by recognizing the syllables using different ASR front ends, including Mel-Filter Bank (MFB) energies, Mel-Filtered Cepstral Coefficients (MFCCs), and the Ensemble Interval Histogram (EIH). In general the MFB recognition accuracy was slightly higher than the MFCC, which was higher than the EIH. For syllables degraded by lowpass and highpass filtering, automated systems trained on the degraded condition recognized the consonants as well as humans. For syllables degraded by additive speech-shaped noise, none of the automated systems recognized consonants as well as humans. The greatest advantage displayed by humans was in determining the correct voiced/unvoiced classification of consonants in noise. 2005 Elsevier B.V. All rights reserved. PACS: 43.71.E; 43.71.G; 43.72.D; 43.72.N	additive white gaussian noise;confusion matrix;filter bank;hidden markov model;low-pass filter;markov chain;mel-frequency cepstrum;picture archiving and communication system;utility functions on indivisible goods	Jason J. Sroka;Louis D. Braida	2005	Speech Communication	10.1016/j.specom.2004.11.009	speech recognition;computer science;accuracy and precision;hidden markov model	AI	-13.565613423878204	-91.64796866786497	36809
9f06fb50776e4da22fc70d54f654c8c94aa1936d	a constraint-based grammar of case : to correctly predict case phrases occurring without their head verb	conference paper	The current paper argues that the phenomenon in Japanese that case phrases occur without their head verb before the finite complementizer would falsify the HPSG valence/content analysis, for example, in Sag 1997, Pollard and Sag 1994, if no phantom relation corresponding to a verb is used in the syntax or semantics. The HPSG valence/content analysis is that the content of a case phrase structure-shares with a part of the content of its immediately larger constituent only through the valence of its head verb. In the framework of Koga 2000, which does not assume this, a syntax & semantics phrasal rule is proposed to specify the inherent meaning of a case phrase plus the finite complementizer, and not more than that inherent meaning. The semantics of every case form is specified independently of its head verb in Koga 2000. Koga's 2000 constraint-based grammar of case was implemented on unicorn3 parser developed at University of Illinois at Urbana-Champaign. 1 A Problem to the HPSG Valence/Content Analysis on the Assumption of Compositionality 1.1. Case Phrases Occurring Without Their Head Verb before the Finite Complementizer Case phrases occur without their head verb before the finite complementizer /to/ in Japanese, as in B's response to A's utterance in (1).	constraint-based grammar;head-driven phrase structure grammar;phantom reference;phrase structure rules	Hiroki Koga	2003			natural language processing;computer science;specifier;linguistics;communication	NLP	-31.054604502689035	-83.02858770942396	36821
4552a77983ee230c202d1170c1813e672fca1198	on the quantitative and qualitative speech changes of the czech radio broadcasts news within years 1969-2005	automatic speech recognition	In this paper we introduce the quantitative and qualitative#N#characteristics of the Czech Radio Broadcasts News during a#N#period of significant political and social changes in the Czech#N#Republic (1969 - 2005). The research is mainly focused on the#N#quantitative features of speech that can be determined from the#N#results of automatic speech recognition system. We describe the#N#used archive transcription system and selected characteristics#N#of the macro- and micro- structure of the Radio Broadcasts#N#News; namely the changes in studio vs. out-of-studio speech#N#ratio, distribution of speakers by male and female, moderators#N#and guest-speakers, changes in the use of signature tunes#N#(including jingles), approximate use of phrasal introductory#N#and closing language specific for the time periods, speech#N#speed changes, average silence length, coordinative vs.#N#subordinate conjunctions ratio and the most frequent semantic#N#words. The sample of data consists of 6,580 hours of news#N#broadcasting and 48,721,952 lexical words.		Michaela Kucharová;Svatava Skodová;Ladislav Seps;Václav Lábus;Jan Nouza;Marek Bohac	2013		10.1007/978-3-642-40585-3_46	speech recognition;computer science;artificial intelligence;linguistics;multimedia	NLP	-12.845052770655652	-82.46093813898753	36871
42cd21fc975c706139191ddb1856790954352c73	the swedat project and swedia database for phonetic and acoustic research	databases;swedish dialects;phonetic acoustic research;user friendly interfaces;dialectology;acoustics;speech processing;speech;materials;data mining;swedia database;swedat project;finland;internet;user interfaces information retrieval systems internet linguistics speech processing;dialectology phonetics acoustics forensic phonetics;full fledged e science database;databases loudspeakers speech analysis internet acoustics forensics aging data handling data security biometrics;information retrieval systems;forensic phonetics;phonetics;finland swedat project swedia database full fledged e science database internet user friendly interfaces phonetic acoustic research swedish dialects;user interfaces;tagging;linguistics	The project described here may be seen as a continuation of an earlier project, SweDia 2000, aimed at transforming the database collected in that project to a full-fledged e-science database. The database consists of recordings of Swedish dialects from 107 locations in Sweden and Swedish speaking parts of Finland. The goal of the present project is to make the material searchable in a flexible and simple way to make it available to a much wider sector of the research community than is the case at present. The database will be accessible over the Internet via user-friendly interfaces specifically designed for this type of data. Other more specialized research interfaces will also be designed to facilitate phonetic acoustic research and orientation of the database.	acoustic cryptanalysis;acoustic fingerprint;continuation;e-science;internet;usability	Jonas Lindh;Anders Eriksson	2009	2009 Fifth IEEE International Conference on e-Science	10.1109/e-Science.2009.15	natural language processing;speech recognition;computer science;world wide web	DB	-23.474476056764164	-84.85695931151366	36957
0271b9dd1ecf244881f1fd4674f6cd02fe3b5d4c	a massively parallel model of speech-to-speech dialog translation: a step toward interpreting telephony		This paper describes the overall picture of 4iDMDIALOG. 4>DMDIALOG is a real-time Japanese-English speech-tospeech dialog translation system that accepts speakerindependent continuous speech inputs. The scientific focus of the project is to model the cognitive process of simultaneaus . interpreters. As a result, the architecture of the system is very different from machine translation systems. Our model assumes hybridized parallelism as a basie computation mechanism, and the process of translation is highly interactive due to the dynamic participation of knowledge from morphophonetic-level to discourse-level. An almost concurrent parsing and generation scheme provides a simultaneaus interpretation capability which is essential to interpretlog telephony. 4iDMDIALOG has been publicly demonstrated at the Center for Machine Translation at Camegie Mellon University.	cognition;computation;interpreter (computing);machine translation;parallel computing;parsing;real-time transcription;dialog	Hiroaki Kitano;Hideto Tomabechi;Teruko Mitamura;Hitoshi Iida	1989			speech recognition;massively parallel;artificial intelligence;natural language processing;computer science;dialog box;telephony	NLP	-29.432517570897787	-82.74528019357408	36963
df095871bf87e37fe4e373220cde0882393046c3	detection of polyphonic music note onsets by application of the bayesian theory of surprise		In this paper we present an onset detection algorithm that consists of two parts, the detection of transient peaks in an audio spectrum and the classification of the peaks, adapting a model derived from the Bayesian Theory of Surprise. The model is an unsupervised, robust adaptation of conjugate priors, providing the distributions of beliefs about the number of the transient peaks, in a time space as well as in a frequency space. The novelty points marked by the model are then classified according to their relevance in order to filter out non-onset events, caused for example by a background noise. It has been evaluated using a collection of over 170 music excerpts. Our experiments show that the new model can provide an overall performance close to the current state of the art solutions. We discuss the advantages of the presented approach and the ways to overcome its shortcomings and the possible directions of future research.	algorithm;experiment;onset (audio);relevance;unsupervised learning	Piotr Holonowicz;Perfecto Herrera	2010			psychology;speech recognition;acoustics;communication	ML	-11.111395629381635	-90.84799774569701	36989
09bf7afcaf7072c5cd37cff7e019ec5e51c14162	the effect of age and native speaker status on synthetic speech intelligibility			intelligibility (philosophy);synthetic intelligence	Catherine I. Watson;Wei Liu;Bruce A. MacDonald	2013			first language;speech recognition;computer science;intelligibility (communication)	NLP	-9.401975992686923	-83.7247740861152	37004
82e7f197bb9197d36c6311cb4aecdde350a16ba6	multilingual data selection for low resource speech recognition		Feature representations extracted from deep neural networkbased multilingual frontends provide significant improvements to speech recognition systems in low resource settings. To effectively train these frontends, we introduce a data selection technique that discovers language groups from an available set of training languages. This data selection method reduces the required amount of training data and training time by approximately 40%, with minimal performance degradation. We present speech recognition results on 7 very limited language pack (VLLP) languages from the second option period of the IARPA Babel program using multilingual features trained on up to 10 languages. The proposed multilingual features provide up to 15% relative improvement over baseline acoustic features on the VLLP languages.	acoustic cryptanalysis;baseline (configuration management);elegant degradation;speech recognition	Samuel Thomas;Kartik Audhkhasi;Jia Cui;Brian Kingsbury;Bhuvana Ramabhadran	2016		10.21437/Interspeech.2016-598	natural language processing;speech recognition;pattern recognition	NLP	-18.437566758569623	-88.85460851509737	37020
c3e336253030ff7eb711cee3132ea78668f6fe72	continuous f0 in the source-excitation generation for hmm-based tts: do we need voiced/unvoiced classification?	hmm based synthesis;multi band mixed excitation;generators;hidden markov model;speech signal processing continuous f0 source excitation generation hmm based tts voice classification source excitation state specific msd prior;speech processing;source excitation generation;voiced unvoiced decision;hmm based synthesis continuous f0 voiced unvoiced decision aperiodicity multi band mixed excitation;aperiodicity;hidden markov models mathematical model equations indexes generators;state specific msd prior;speech processing hidden markov models;indexes;voice classification;hidden markov models;source excitation;continuous f0;indexation;speech signal processing;mathematical model;hmm based tts	Most HMM-based TTS systems use a hard voiced/unvoiced classification to produce a discontinuous F0 signal which is used for the generation of the source-excitation. When a mixed source excitation is used, this decision can be based on two different sources of information: the state-specific MSD-prior of the F0 models, and/or the frame-specific features generated by the aperiodicity model. This paper examines the meaning of these variables in the synthesis process, their interaction, and how they affect the perceived quality of the generated speech The results of several perceptual experiments show that when using mixed excitation, subjects consistently prefer samples with very few or no false unvoiced errors, whereas a reduction in the rate of false voiced errors does not produce any perceptual improvement. This suggests that rather than using any form of hard voiced/unvoiced classification, e.g., the MSD-prior, it is better for synthesis to use a continuous F0 signal and rely on the frame-level soft voiced/unvoiced decision of the aperiodicity model.	experiment;hidden markov model;netware file system	Javier Latorre;Mark J. F. Gales;Sabine Buchholz;Kate Knill;Masatsune Tamura;Yamato Ohtani;Masami Akamine	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947410	database index;speech recognition;computer science;mathematical model;hidden markov model	Robotics	-10.837389061564915	-86.35199639451525	37158
be1553b7b16d7d0f72f618ad401988476239a822	improvement of lipreading performance using discriminative feature and speaker adaptation		In this paper, we apply a general and discriminative feature ”GIF” (Genetic Algorithm based Informative feature) to lipreading (visual speech recognition), and improve the lipreading performance using speaker adaptation. The feature extraction method consists of two transforms, which convert an input vector into GIF for recognition. In the speaker adaptation, MAP (Maximum A Posteriori) adaptation is used to adapt a recognition model to a target speaker. Recognition experiments of continuous digit utterances were conducted using an audio-visual corpus CENSREC-1-AV [1] including more than 268,000 lip images. At first, we compared the GIF-based method with the baseline method employing conventional eigenlip features, using two kinds of images: pictures in the database around speakers’ mouth, and extracted images only containing lips. Secondly, we evaluated the effectiveness of speaker adaptation for lipreading. The result of comparison shows that the GIFbased approach achieved slightly better than the baseline method. And it is found using the mouth-around images is more suitable than lip-only images. Furthermore, the result of speaker adaptation shows that speaker adaptation significantly improved recognition accuracy in the GIF-based method; after the adaptation, the recognition rate drastically increased from approximately 30% to 70%.	baseline (configuration management);experiment;feature extraction;gif;genetic algorithm;information;map;speech recognition	Seko Takumi;Naoya Ukai;Satoshi Tamura;Satoru Hayamizu	2013			discriminative model;speech recognition;computer science;artificial intelligence;pattern recognition	NLP	-16.1597624151159	-89.00565414238469	37198
2a89070081eb2f9e1d9fd5402f9d00448777370f	learning the basic units in american sign language using discriminative segmental feature selection	phonemic model;natural language interfaces;natural languages natural language interfaces;reduced model complexity;machine perception;feature selection machine learning american sign language;american sign language;wrist;training;vocabulary;speech;internal structure;natural languages;data mining;model complexity;handicapped aids;hidden markov models;machine learning;reduced model complexity american sign language discriminative segmental feature selection natural language phonemic model machine perception;feature extraction;natural language;deafness;speech recognition;feature selection;humans;scalability;signal processing algorithms;discriminative segmental feature selection;handicapped aids hidden markov models natural languages deafness scalability machine learning signal processing algorithms speech recognition vocabulary humans;human perception	The natural language for most deaf signers in the United States is American Sign Language (ASL). ASL has internal structure like spoken languages, and ASL linguists have introduced several phonemic models. The study of ASL phonemes is not only interesting to linguists, but also useful for scalability in recognition by machines. Since machine perception is different than human perception, this paper learns the basic units for ASL directly from data. Comparing with previous studies, our approach computes a set of data-driven units (fenemes) discriminatively from the results of segmental feature selection. The learning iterates the following two steps: first apply discriminative feature selection segmentally to the signs, and then tie the most similar temporal segments to re-train. Intuitively, the sign parts indistinguishable to machines are merged to form basic units, which we call ASL fenemes. Experiments on publicly available ASL recognition data show that the extracted data-driven fenemes are meaningful, and recognition using those fenemes achieves improved accuracy at reduced model complexity.	discriminative model;feature selection;machine perception;natural language;scalability	Pei Yin;Thad Starner;Harley Hamilton;Irfan A. Essa;James M. Rehg	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4960694	natural language processing;speech recognition;computer science;machine learning;natural language;feature selection	Vision	-18.239203737803667	-86.83090591221087	37202
4fcd4dae597c6e0f90560d5ea9853fc423ec417a	innovations in czech audio-visual speech synthesis for precise articulation	publications innovations in czech audio visual speech synthesis for precise articulation;katedra kybernetiky;kybernetika;informacni a řidici systemy;automaticke řizeni;uměla inteligence;publikace innovations in czech audio visual speech synthesis for precise articulation	This paper presents new steps toward animation of precise articulation. The acquisition of audio-visual corpus for Czech and new method for parameterization of visual speech was designed to obtain exact speech data. The parameterization method is primarily suitable for training a data driven visual speech synthesis systems. The audio-visual corpus includes also specially designed test part. Furthermore, the paper presents the collection of suitable text material for test of visual speech perception and also the procedure how can be the test performed. The synthesis method based on the selection of visual unit and animation model of talking head is extended. The synthesis system is objectively and subjectively evaluated.	biconnected component;speech synthesis	Zdenek Krnoul;Milos Zelezný	2007			natural language processing;speech recognition;engineering;communication	Graphics	-15.332112075114237	-84.19611294348992	37219
57f68dfd9ae0a4600fce3c808dcc505f9b9f3778	comparison of two frequency lowering algorithms for digital hearing aids		A considerable percentage of listeners with severe hearing loss have audiograms where the losses are high for high frequencies and low for low frequencies. For these patients, lowering the speech spectrum to the frequencies where there is some residual hearing could be a good solution to be implemented for digital hearing aids. In this paper we have presented two different frequency–lowering algorithms: frequency compression and frequency shifting. Preliminary results have shown a slight better performance of the frequency shifting method relatively to the frequency compression method. I. INTRODUCTION There are several kinds of hearing impairment. The origin of the sensorineural hearing losses can be due to defects in the cochlea, auditory nerve or both. These problems reduce the dynamic range of hearing. The threshold of hearing is elevated, but the threshold of discomfort (at which the loudness become uncomfortable) is almost the same as for normal–hearing listeners, or even may be lower. For some range of frequencies, the threshold of hearing is so high than it is equal to the threshold of discomfort, i.e., it is impossible for the listener hearing any sound at those frequencies. Hearing loss is more common for high–frequency and mid–frequency sounds (1 to 3 kHz) than for low– frequency. Frequently, there are only small losses at low frequencies (below 1 kHz) but almost absolute deafness above 1.5 or 2 kHz. These facts lead researchers to lower the spectrum of speech in order to match the residual low–frequency hearing of listeners with high–frequency impairments. Slow playback, vocoding, and zero–crossing rate division are some of the methods that have been employed in the last decades. All of these methods involve signal distortion, more or less noticeable, generally depending on the amount of the frequency shifting. Many of the lowering schemes have altered perceptually important characteristics of speech, such as temporal and rhythmic patterns, pitch and durations of segmental elements. Hicks et al. [1] have done one of the most remarkable investigations about frequency lowering. Their technique involve pitch–synchronous, monotonic compression of the short–term spectral envelope, while at the same time	algorithm;distortion;dynamic range;heterodyne;noise shaping;pitch (music);vocoder;zero-crossing rate	Alan M. Marotta;Francisco J. Fraga	2003			acoustics;speech recognition;computer science	ML	-7.848843524687781	-86.61997543175329	37322
69283803bdbca0bd6de05aee42628de1a216e270	classification of sound clips by two schemes: using onomatopoeia and semantic labels	database indexing;databases;feature extraction acoustics labeling speech music databases classification algorithms;feature vectors;pattern clustering audio databases audio signal processing database indexing matrix algebra pattern classification;pattern clustering;unsupervised clustering;audio representation;labeling scheme;audio signal processing;unit document cooccurrence matrix;latent perceptual indexing;audio clips;document analysis;semantic audio;latent document analysis;acoustics;audio classification;database;speech;matrix algebra;indexing terms;onomatopoeia;feature vector;sound clip classification;latent document analysis audio classification audio representation indexing semantic audio onomatopoeia;indexing;space use;feature extraction;indexation;classification algorithms;classification accuracy sound clip classification onomatopoeia semantic labels latent perceptual indexing feature vectors database unsupervised clustering technique unit document cooccurrence matrix audio clips reduced rank matrix approximation;classification system;pattern classification;audio databases;classification accuracy;reduced rank matrix approximation;music;unsupervised clustering technique;semantic labels;co occurrence matrix;labeling	Using the recently proposed framework for latent perceptual indexing of audio clips, we present classification of whole clips categorized by two schemes: high-level semantic labels and the mid-level perceptually motivated onomatopoeia labels. First, feature-vectors extracted from the clips in the database are grouped into reference clusters using an unsupervised clustering technique. A unit-document co-occurrence matrix is then obtained by quantizing the feature-vectors extracted from the audio clips into the reference clusters. The audio clips are then mapped to a latent perceptual space by the reduced rank approximation of this matrix. The classification experiments are performed in this representation space using corresponding semantic and onomatopoeic labels of the clips. Using the proposed method, classification accuracy of about sixty percent was obtained when tested on the BBC sound effects library using over twenty categories. Having the two labeling schemes together in a single framework makes the classification system more flexible as each scheme addresses the limitation of the other. These aspects are the main motivation of the work presented here.	categorization;cluster analysis;co-occurrence matrix;document-term matrix;experiment;high- and low-level;low-rank approximation	Shiva Sundaram;Shrikanth (Shri) Narayanan	2008	2008 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2008.4607691	statistical classification;speech recognition;feature vector;computer science;machine learning;pattern recognition;information retrieval	Vision	-9.089629699571141	-92.78444201704136	37461
99b7cd02b80bf944c212ca277d4bb4cbfb16eafe	fpga based assembling of facial components for human face construction	field programmable gate array;three step search;indexing terms;pattern recognition;hardware description language	This paper aims at VLSI realization for generation of a new face from textual description. The FASY (FAce SYnthesis) System is a Face Database Retrieval and new Face generation System that is under development. One of its main features is the generation of the requested face when it is not found in the existing database. The new face generation system works in three steps – searching phase, assembling phase and tuning phase. In this paper the tuning phase using hardware description language and its implementation in a Field Programmable Gate Array (FPGA) device is presented.	field-programmable gate array;http 404;hardware description language;usability	Santanu Halder;Debotosh Bhattacharjee;Mita Nasipuri;Dipak Kumar Basu;Mahantapas Kundu	2006	CoRR		computer vision;speech recognition;index term;computer science;theoretical computer science;machine learning;pattern recognition;hardware description language;field-programmable gate array	AI	-8.434811187685147	-97.092598372826	37601
3830c7cb82204e036b36306d3e85608d0352ef24	neuro-fuzzy clustering techniques for complex acoustic scenarios	objet;representation;analisis escena;cluster;analyse scene;time frequency plane;mixture;auditory space;melange;concepcion sistema;amas;sistema;signal analysis;time frequency;simulation;frequence;environmental conditions;plan temps frequece;ruido;object;espacio acustico;analisis de senal;simulacion;reseau neuronal flou;time;scenario acoustic;acoustic scenario;condition;espace acoustique;frecuencia;temps;time frequency representation;system design;feature extraction;system;neuro fuzzy;bruit;condicion;tecnica;signal acoustique;arquitectura;ejemplo;acoustic signal;monton;systeme;methode domaine temps frequence;reseau neuronal;neurofuzzy network;frequency;auditory scene analysis;example;architecture;objeto;metodo dominio tiempo frecuencia;red neuronal;technique;conception systeme;analyse signal;senal acustica;time frequency domain method;mezcla;noise;representacion;neural network;scene analysis;tiempo;exemple	An apparatus is described capable of detecting a specific sound among a sound mixture and of identifying the environmental acoustic scenario, to automatically adapt the apparatus to changing environmental conditions. Such an apparatus could be useful, for example, in the design of an ASR system robust to environmental noise. Starting from a time-frequency representation of the incoming signal, neuro-fuzzy clustering is performed on proper features extracted from time-frequency objects.	acoustic coupler;acoustic cryptanalysis;automatic system recovery;cluster analysis;file spanning;finite impulse response;fuzzy clustering;microsoft windows;neuro-fuzzy;sensor;test set;time–frequency representation	Rinaldo Poluzzi;Alberto Savi;Davide Vago;Giuseppe Martina	2003	Neural Computing & Applications	10.1007/s00521-003-0389-5	speech recognition;feature extraction;computer science;noise;artificial intelligence;object;architecture;machine learning;frequency;system;auditory scene analysis;representation;mixture;artificial neural network;cluster	ML	-4.625559528296079	-92.96823236420958	37617
d01357de0b444c2645c09fddd4412101e7146798	the prolongation-type speech non-fluency detection based on the linear prediction coefficients and the neural networks		The goal of the paper is presenting a speech prolongation detection method based on the linear predicion coefficients obtained by the Levinson-Durbin method. The application “Dabar”, which was made for this aim, has an ability of setting the coefficients computed by the implemented methods as an input of the Kohonen networks with different size of the output layer. Three different types of the neural networks were used to classify fluency of the utterances: RBF networks, linear networks and Multi-Layer Perceptrons. The Kohonen network (SOM) was used to reduce the LP coefficients representation to the winning neurons vector. After that the vector was splitted into subvectors whom represents 400ms utterances. These utterances were fragments of the Polish speech without the silence. The research was based on 202 fluent utterances and 140 with the prolongations on Polish phonems. The classifying success reached 75% of certainty.	coefficient;linear predictive coding;neural networks	Adam Kobus;Wieslawa Kuniszyk-Józkowiak;Elzbieta Smolka;Ireneusz Codello;Waldemar Suszynski	2013		10.1007/978-3-319-00969-8_87	speech recognition;machine learning;pattern recognition;time delay neural network	ML	-15.81224371049338	-87.54126521811855	37791
1ab7508b3c52a42ca90ffab150dcb09353c0b040	visual stimulus placement comparison for turkish p300 speller	natural language interfaces;reliability;brain computer interface;electroencephalography brain computer interfaces electric potential neurophysiology visualization abstracts reliability;visualization;natural language processing brain computer interfaces character recognition equipment electroencephalography natural language interfaces;abstracts;character recognition equipment;electric potential;neurophysiology;brain computer interfaces;electroencephalography;electroencephalogram;eeg visual stimulus placement turkish p300 speller turkish character brain computer interface electroencephalogram;natural language processing	A P300 speller is designed, that is implemented with Turkish characters. The designed P300 speller is used as a brain computer interface via electroencephalogram (EEG). For this speller, the visual stimulus has been prepared for two different letter placements. The first of these placements, which is very common in the literature, is the square formed letter sequence. The second placement of letters is designed to consist of a gap on top and at the bottom rows of letters in the form of parallelogram. For the two different letter placements, data are collected from the subjects. The data are evaluated for the placement changes' effects on the designed P300 speller.	brain–computer interface;electroencephalography	Suleyman Orken;R. Koray Çiftçi	2012	2012 20th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2012.6204593	natural language processing;brain–computer interface;speech recognition;computer science;neurophysiology	Visualization	-5.941451421869837	-83.48055300924506	37806
2824c75ad48ca4795f79c0f4a0c9b05affbf2b6a	rhythmic structure of english and japanese: a constraint based analysis of nursery rhymes and haiku		Studies on English verses have claimed that the position in which a pause is inserted has a great effect on determining their rhythm. The position in which a pause appears follows a certain rule for the verses to be ‘rhythmic’, however, it is not yet obvious whether Japanese Haiku also has such tendencies or preferences. In this paper, we analysed and compared the metrical structure of English verses with that of Haiku within the Optimality Theory framework. We claim that the constraints employed to account for English verses can also account for the preference of the patterning of Haiku.	haiku	Yukiko Ishikawa;Haruko Miyakoda	2015			optimality theory;mathematics;communication;rhythm;linguistics;haiku	NLP	-10.9935723377859	-80.51669901606324	37834
f966d579cd13a933d898090f51fd6c990a7447bc	a kaldi-dnn-based asr system for italian	libraries;google;decoding;training libraries google switches wireless sensor networks decoding monos devices;training;am training procedure kaldi dnn italian kaldi asr engine acoustic model training procedure;asr dnn children speech;monos devices;speech recognition acoustic signal processing natural language processing;switches;wireless sensor networks	In this paper, the KALDI ASR engine adapted to Italian is described and the results obtained so far on some children speech ASR experiments are reported. We give a brief overview of KALDI, we describe in detail its DNN implementation, we introduce the acoustic model (AM) training procedure and we end describing some experiments on Italian children speech together with the final test procedures.	acoustic cryptanalysis;acoustic model;automated system recovery;context-sensitive language;discriminative model;experiment;kaldi;language model;mixture model	Piero Cosi	2015	2015 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2015.7280336	speech recognition;wireless sensor network;network switch;computer science	NLP	-20.383969796964333	-85.86311415960904	37938
e02b8175799bcebe43e1620b67dccf1f94b1c5a1	automatic speech recognition of english-isizulu code-switched speech from south african soap operas		We introduce a new English-isiZulu code-switched speech corpus compiled from South African soap opera broadcasts. isiZulu itself is currently under-resourced, and automatic speech recognition is made even more challenging by the high prevalence of codeswitching in spontaneous speech. Analysis of the corpus reflects effects common in conversational isiZulu, such as vowel deletion and cross-language prefixes and suffixes. Baseline monolingual and code-switched automatic speech recognition systems are developed, including a new language model configuration that explicitly includes switching transitions. For code-switched speech, a system with language-dependent acoustic models and language-dependent language models linked by switching transitions leads to best performance, although word error rates overall remain very high. c © 2016 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the Organizing Committee of SLTU 2016.	acoustic cryptanalysis;acoustic model;automated system recovery;compiler;computation;language model;speech corpus;speech recognition;spontaneous order;test set;text corpus	Ewald van der Westhuizen;Thomas R Niesler	2016		10.1016/j.procs.2016.04.039	speech production;audio mining;speech recognition;speech corpus;computer science;speech;acoustic model;speech error;speech synthesis;speech analytics	NLP	-21.33204387339706	-84.598846571046	37996
dc3167e8a8e284d39810082d13fcb56bda4c6645	on the evaluation of inversion mapping performance in the acoustic domain	evaluation	The two measures typically used to assess the performance of an inversion mapping method, where the aim is to estimate what articulator movements gave rise to a given acoustic signal, are root mean squared (RMS) error and correlation. In this paper, we investigate whether “task-based” evaluation using an articulatory-controllable HMM-based speech synthesis system can give useful additional information to complement these measures. To assess the usefulness of this evaluation approach, we use articulator trajectories estimated by a range of different inversion mapping methods as input to the synthesiser, and measure their performance in the acoustic domain in terms of RMS error of the generated acoustic parameters and with a listening test involving 30 participants. We then compare these results with the standard RMS error and correlation measures calculated in the articulatory domain. Interestingly, in the acoustic evaluation we observe one method performs with no statistically significant difference from measured articulatory data, and cases where statistically significant differences between methods exist which are not reflected in the results of the two standard measures. From our results, we conclude such task-based evaluation can indeed provide interesting extra information, and gives a useful way to compare inversion methods.	acoustic cryptanalysis;hidden markov model;speech synthesis	Korin Richmond;Zhen-Hua Ling;Junichi Yamagishi;Benigno Uria	2013			speech recognition	Comp.	-10.872032770490062	-83.96457680302025	38010
f12b1bcaa34022c9c6a5995cef9e16a2ebf1e785	speeding up dynamic search methods in speech recognition	desciframiento;dynamic programming;programacion dinamica;tiempo busqueda;algoritmo busqueda;decodage;decoding;algorithme recherche;heuristic method;search algorithm;search method;metodo heuristico;dynamic program;intelligence artificielle;temps recherche;heuristic search;reconocimiento voz;aggregation operator;programmation dynamique;speech recognition;artificial intelligence;methode heuristique;inteligencia artificial;reconnaissance parole;search time	In speech recognition huge hypothesis spaces are generated. To overcome this problem dynamic programming can be used. In this paper we examine ways of speeding up this search process even more using heuristic search methods, multi-pass search and aggregation operators. The tests showed that these techniques can be applied together, and their combination could significantly speed up the recognition process. The run-times we obtained were 22 times faster than the basic dynamic search method, and 8 times faster than the multi-stack decoding method. In speech recognition enormous hypothesis spaces arise. To handle them we can use dynamic programming, where we can avoid calculating the same values several times, which leads to a dramatic speed-up of a speech recognizer system. But this is not enough for real-world applications, hence we have to look for other ways of making improvements while preserving the recognition accuracy. Here we carry out experiments using search heuristics, aggregation operators and multi-pass search, and apply ideas for speeding up the heuristic search. 1 The Speech Recognition Problem We have a speech signal given by a series of observations A = a1 . . . at, and a set of phoneme sequences W . We look for the word ŵ ∈ W = arg max P (w|A) which, via Bayes’ theorem, is equivalent to ŵ = arg max(P (A|w) ·P (w))/P (A). P (A) is the same for all w, so ŵ = arg max P (A|w)P (w). Let w be o1o2 . . . on, as oj is the jth phoneme of w. Let A1, . . . , An be non-overlapping segments of A. We assume that the phonemes are independent, i.e. P (A|w) can be obtained from P (A1|o1), . . . , P (An|on). To calculate P (A|w), we can use aggregation operators at two levels: g1 supplies the P (Aj |oj) values as g1(P (atj−1 |oj), . . . , P (atj |oj)), while g2 is used to construct P (A|w) as g2(P (A1|o1), . . . , P (An|on)). Instead of a probability p we will use a cost c = −ln p. g1 will be the addition operator. A hypothesis is a pair of phoneme series and segment series. The dynamic programming method uses a table with the ai speech frames indexing the columns and the phoneme-sequences indexing the rows. A cell holds the lowest cost of the hypotheses having its phoneme-sequence and ending at its frame. To compute the value of a cell we take the value of an earlier frame and its M. Ali and F. Esposito (Eds.): IEA/AIE 2005, LNAI 3533, pp. 98–100, 2005. c © Springer-Verlag Berlin Heidelberg 2005 Speeding Up Dynamic Search Methods in Speech Recognition 99 phoneme-sequence without its last phoneme, and add up the cost of this last phoneme on the interleaving frames. The result is the minimum of these sums. 2 Speeding Up the Recognition Process The dynamic programming search technique, despite its effectiveness, tends to be quite slow. In this section we discuss some methods that speed it up while keeping the recognition accuracy at an acceptable rate. Heuristic Search Methods. These techniques fill only a part of the table. So the result will not always be optimal, but we can get a notable speed-up with little or no loss in accuracy. The multi-stack decoding algorithm fills a fixed number (stack size) of cells (the ones with the lowest costs) for a row. The Viterbi beam search fills the cell with the best value, and the cells close to it defined by a beam width parameter. Here we used the multi-stack approach. Speed-Up Improvements. In earlier works [1] we presented some speed-up ideas for the multi-stack decoding algorithm, which we also want to use here. i) One possibility is to combine multi-stack decoding with a Viterbi beam search. At each column, belonging to one time instance, we fill only a fixed number of cells, and also discard those which are far from the best-scoring value. ii) Another approach is based on the fact that the later the time instance, the fewer hypotheses (and filled cells) are need. Thus we filled s ·mi cells belonging to the ai frame, where 0 < m < 1 and s is the original stack size parameter. iii) Actually, we need to fill more cells at those speech frames close to pronounced phoneme bounds. We trained an ANN to estimate whether a given time instance was a phoneme bound or not. Then we constructed a function that approximates the stack size based on the output of this ANN. Multi-pass Search. Multi-pass methods work in several steps: in the first pass the worse hypotheses are discarded because of some condition requiring low computational time. We reduced the number of phoneme groups for this reason. In later passes only the remaining hypotheses are examined, but with a more detailed phoneme grouping. The last pass (P0) uses the original phoneme set. To create the phoneme-sets first a distance function of the original ph1, . . . , phm phonemes is defined: d(phi, phj) is based on the ratio of phi-s classified as phj and vice versa. We can use the higher value (d) or the average (d) as the metric. The distance between phoneme-groups can be the minimum distance between their phones (Dmin), or the maximum (Dmax) [2]. The recognition steps using the resulting phoneme-sets were P1 and P2.	a* search algorithm;beam search;call stack;column (database);computation;dynamic programming;experiment;finite-state machine;forward error correction;heuristic (computer science);international ergonomics association;jumbo frame;lecture notes in computer science;speech recognition;speedup;springer (tank);time complexity	Gábor Gosztolya;András Kocsor	2005		10.1007/11504894_16	heuristic;computer science;artificial intelligence;machine learning;dynamic programming;incremental heuristic search;algorithm;search algorithm	AI	-20.622414617273513	-87.98531338300187	38026
41a760b38f6c108d64e971fd5b81581d9dcdfb4b	bilingual lexical data contributed by language teachers via a web service: quality vs. quantity		about twenty technical domains. These terms are associated with their French translations (95% of which are correct) and examples of use (about 85% correct). In the second year, emphasis has been on quality rather than on quality: about 6000 high-quality entries have been contributed by the same number of students and classes. Some desirable extensions are in progress, e.g. to add English when this language is not included in the original language pair, and to synchronize with off-line contributions prepared on a PDA or a hand-held calculator.	collocation;lexical database;lexico;mobile device;online and offline;personal digital assistant;testbed;usability;web service;windows 95	Valérie Bellynck;Christian Boitet;John Kenwright	2009	Polibits		natural language processing;speech recognition;computer science;multimedia	NLP	-27.57144500137417	-84.3688508916731	38031
437375581a23a374b46e09ce369b858aa2a29960	estimates for the measurement and articulatory error in mri data from sustained vowel production		Spatial data of the human vocal tract (VT), larynx, and thorax can be obtained by magnetic resonance imaging (MRI) during steady, sustained phonation. Long acquisition time increases the resolution as well as the errors due to involuntary motion of VT. We discuss two experiments with a single test subject to find a suitable 3D MRI data acquisition procedure in terms of subject’s articulatory steadiness.	data acquisition;experiment;resonance;tract (literature)	Daniel Aalto;Jarmo Malinen;Martti Vainio;Jani Saunavaara;Pertti Palo	2011			speech recognition;acoustics;communication	ML	-8.073224652148083	-84.43505310693963	38047
d88e68fbd46663da23804fb25475853931d93399	learning novel objects for extended mobile manipulation	object recognition;tecnologia industrial tecnologia mecanica;tecnologia electronica telecomunicaciones;out of vocabulary;robocup home;mobile manipulation;object learning;tecnologias;grupo a	We propose a method for learning novel objects from audio visual input. The proposed method is based on two techniques: outof-vocabulary (OOV) word segmentation and foreground object detection in complex environments. A voice conversion technique is also involved in the proposed method so that the robot can pronounce the acquired OOV word intelligibly. We also implemented a robotic system that carries out interactive mobile manipulation tasks, which we call “extended mobile manipulation”, using the proposed method. In order to evaluate the robot as a whole, we conducted a task “Supermarket” adopted from the RoboCup@Home league as a standard task for real-world applicaT. Nakamura (B) · T. Nagai The University of Electro-Communications, 1-5-1 Chofugaoka, Chofu-shi, Tokyo, Japan e-mail: naka_t@apple.ee.uec.ac.jp K. Sugiura · N. Iwahashi National Institute of Information and Communications Technology, 3-5 Hikaridai, Seika, Soraku, Kyoto, Japan T. Toda Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan H. Okada · T. Omori Tamagawa University, 6-1-1 Tamagawagakuen, Machida, Tokyo, Japan tions. The results reveal that our integrated system works well in real-world applications.	email;mobile manipulator;object detection;robot;text segmentation;vocabulary	Tomoaki Nakamura;Komei Sugiura;Takayuki Nagai;Naoto Iwahashi;Tomoki Toda;Hiroyuki Okada;Takashi Omori	2012	Journal of Intelligent and Robotic Systems	10.1007/s10846-011-9605-1	simulation;engineering;artificial intelligence;cognitive neuroscience of visual object recognition	Robotics	-27.039596416895595	-91.45413341801074	38192
8421faf1045fbd7bd43ceac06dc1f6e15dcc0e35	interactive voice-controller applied to home automation	libraries;voice equipment;flash memory;lighting control;multi stage recognition phases;sunplus spce061a;training;embedded sound controller;home appliances;speech;voice controller;network routing;zigbee home automation interactive voice controller;interactive;voice control lighting system;speech recognition library functions;zigbee;microchip 2 4 ghz zigbee modules;voice control lighting system interactive voice controller home automation embedded sound controller sunplus spce061a microchip 2 4 ghz zigbee modules multi stage recognition phases speech recognition library functions;interactive voice controller;speech recognition;personal area networks;interactive systems;voice equipment home automation interactive systems lighting control personal area networks speech recognition;home automation zigbee automatic control lighting control speech recognition home appliances keyboards libraries software design multimedia systems;home automation	With the rapid development of broadband networks and multimedia technologies, P2P video-on-demand service is increasingly gaining popularity over the past few years. However, little work has been done on an important design issue for P2P video-on-demand systems, i.e., determining the maximal service capacity of a system. In this paper, we first propose an optimal rate allocation scheme to maximize the service capacity of a P2P video-on-demand system. The proposed rate allocation scheme determines the parent/child relationship among peers and the link rates between peers. Then we develop an admission control policy based on the proposed rate allocation scheme. Our simulation results showed that the proposed scheme demonstrates high efficiency compared to other rate allocation schemes.	algorithm;home automation;maximal set;peer-to-peer;scalability;simulation;speaker recognition;user interface;voice command device	Jinn-Kwei Guo;Chun-Lin Lu;Ju-Yun Chang;Yi-Jing Li;Ya-Chi Huang;Fu-Jiun Lu;Ching-Wen Hsu	2009	2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2009.320	embedded system;home automation;routing;real-time computing;computer science;speech;operating system;interactivity	HPC	-23.533442104904722	-87.21053051988082	38294
04e0b8c8d7421f46062449cd0f182a25a9696bf1	study on headphone hearing loss prevention methods based on the melody structure of music on portable music player		In this paper, we examine the volume control system in an application to prevent the onset of headphone hearing loss due to music listening using a portable music player. First, the possibility that lower the volume to a safe level without be noticed by the user (difference of perceived attenuating level) are evaluated. As a result, the notice happened when volume level was attenuated 11 ± 3 dB, and it not be affected by changing attenuation rate of volume level. Based on the above experiments, we examine the melody structure of music by attenuate the volume separately for melody structure (Intro part, Verse, Chorus, Interlude). As a result, it was found that the intro part was hard to notice the volume attenuation, and in the part of the chorus, it was easy to notice the volume attenuation. Next, we evaluated difference of the perceived attenuating level considering frequency characteristics. As a result for the low frequency, it was difficult to notice the volume attenuation, and the high frequency tends to notice the volume attenuation.	chorusos;control system;experiment;headphones;onset (audio);verse protocol	Daiki Okabe;Yoshihisa Nakatoh	2018	2018 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2018.8326297	electronic engineering;acoustics;attenuation;notice;active listening;computer science;hearing loss;headphones	Visualization	-7.338867269948956	-86.54019414343257	38298
f98b706ea18bf9b63b1da303ce25b197f15f1edd	experiments with automatic segmentation for czech speech synthesis	statistical approach;modelo markov oculto;katedra kybernetiky;analisis estadistico;speech synthesis;modele markov cache;kybernetika;proceso markov;hidden markov model;automatic segmentation;modele markov variable cachee;informacni a řidici systemy;automaticke řizeni;inicializacion;segmentation;probabilistic approach;publications experiments with automatic segmentation for czech speech synthesis;publikace experiments with automatic segmentation for czech speech synthesis;hidden markov models;statistical analysis;enfoque probabilista;approche probabiliste;processus markov;analyse statistique;uměla inteligence;markov process;sintesis palabra;speaker;locutor;segmentacion;initialization;initialisation;locuteur;synthese parole	This paper deals with the automatic segmentation for Czech Concatenative speech synthesis. Statistical approach to speech segmentation using hidden Markov models (HMMs) is applied in the baseline system [1]. Several experiments that concern various issues in the process of building the segmentation system, such as speech parameterization or HMM initialization problems, are described here. An objective comparison of various experimental automatic and manual segmentations is performed to find out the best settings of the segmentation system with respect to our single-female-speaker continuous speech corpus.	acoustic cryptanalysis;baseline (configuration management);coefficient;experiment;hidden markov model;inventory;iterative method;lu decomposition;markov chain;microsoft windows;netware file system;speech corpus;speech segmentation;speech synthesis;synthetic intelligence;triphone;wave packet	Jindrich Matousek;Daniel Tihelka;Josef Psutka	2003		10.1007/978-3-540-39398-6_41	loudspeaker;initialization;speech recognition;computer science;machine learning;segmentation-based object categorization;pattern recognition;markov process;scale-space segmentation;segmentation;hidden markov model;statistics	NLP	-20.77206600137041	-91.26811736470549	38334
73ae0c903f97a80ce7298d39270d82a3a7113a00	two-stage system for robust neutral/lombard speech recognition	indexing terms;speech recognition	Performance of current speech recognition systems is significantly deteriorated when exposed to strongly noisy environment. It can be attributed to background noise and Lombard effect (LE). Attempts for LE-robust systems often display a tradeoff between LE-specific improvements and the portability to neutral speech. Therefore, towards LE-robust recognition, it seems effective to use a set of conditionsdedicated subsystems driven by a condition classifier, rather than attempting for one universal recognizer. Presented paper focuses on a design of a two-stage recognition system (TSR) comprising talking style classifier (neutral/LE) followed by two style-dedicated recognizers differing in input features. First, the binary neutral/LE classifier is built, with a particular interest in developing suitable features for the classification. Second, performance of common speech features (MFCC, PLP), LE-robust features (Expolog) and newly proposed features is compared in neutral/LE digit recognition tasks. In addition, robustness to the changes of average speech pitch and various noise backgrounds is evaluated. Third, the TSR is built, employing two recognizers, each using style-specific features. Comparison of the proposed system with either neutralspecific or LE-specific recognizer on a joint neutral/LE speech shows an improvement 6.5 4.2 % WER on neutral and 48.1 28.4 % WER on LE Czech utterances.	finite-state machine;pl/p;software portability;speech recognition;speech synthesis;statistical classification;traffic sign recognition;word error rate	Hynek Boril;Petr Fousek;Harald Höge	2007			speech recognition;index term;computer science;pattern recognition	NLP	-15.16809888548282	-89.72164165716778	38483
078bbae0bdd27b2c8e269c369595cc7016595f29	generating musical expression of midi music with lstm neural network		Musicians aim to express emotions through musical performances. Technically, musical expression is mainly created by variances in tempo and dynamics. The purpose of this paper is to investigate the possibility of generating dynamics and expressive tempo for plain (inexpressive) MIDI files by means of a long short-term memory (LSTM) artificial neural network. Two neural network models (for dynamics and tempo separately) were built with the use of Keras deep learning library and trained on a dataset consisting of Chopin's mazurkas. The trained models are capable of generating expressive performance of inexpressive mazurka represented in MIDI format. The generated performances are evaluated by comparing the resulting dynamics and tempo graphs to human performances and by a survey testing the ease of differentiation between human and generated performance. The conclusion of the research is that expression generated with LSTM network can be very similar to human expression and convincing for listeners.	antivirus software;artificial neural network;computer performance;computer program;deep learning;eternal sonata;human reliability;keras;long short-term memory;midi;recurrent neural network	Maria Klara Jedrzejewska;Adrian Zjawinski;Bartlomiej Stasiak	2018	2018 11th International Conference on Human System Interaction (HSI)	10.1109/HSI.2018.8431033	musical expression;deep learning;artificial neural network;machine learning;midi;recurrent neural network;information technology;logic gate;artificial intelligence;graph;computer science	Robotics	-11.139071463901741	-86.76531537737274	38568
85040104ab21d29e411a7153f89efa11bba1ea2a	automatic bilingual legacy-fonts identification and conversion system		The digital text written in an Indian script is difficult to use as such. This is because, there are a number of font formats available for typing, and these font-formats are not mutually compatible. Gurmukhi alone has more than 225 popular ASCII-based fonts whereas this figure is 180 in case of Devanagari. To read the text written in a particular font, that font is required to be installed on that system. This paper describes a language and font-detection system for Gurmukhi and Devanagari. It also explains a font conversion system for converting the ASCII based text into Unicode. Therefore, the proposed system works in two stages: the first stage suggests a statistical model for automatic language-detection (i.e., Gurmukhi or Devanagari) and fontdetection; the second stage converts the detected text into Unicode as per font detection. Though we could not train our systems for some fonts due to nonavailability of font converters but system and its architecture is open to accept any number of languages/fonts in the future. The existing system supports around 150 popular Gurmukhi font encodings and more than 100 popular Devanagari fonts. We have demonstrated the effectiveness of font detection is 99.6% and Unicode conversion is 100% in all the cases.	language identification;statistical model;unicode	Gurpreet Singh Lehal;Tejinder Singh Saini;Saini Pretpal Kaur Buttar	2014	Research in Computing Science		unicode;arithmetic;speech recognition;computer science;unicode font;communication	NLP	-26.350142498518654	-81.83684593364623	38682
cde9d0a6007918cf0e4bd6a56342c8387337ac0e	analysis of two algorithms for telephone speech recognition	speech recognition	The telephone nctwork presents speech recognition devices with a band-limited, noisy, and, in some cases, distorted speech signal. A series of experiments were performed to quantify the effects of these transformations on two current recognition algorithms: a) an acoustic segmentation algorithm and b) an acoustic classiiication algorithm. The data used in these experiments are a subsei of the TIMIT speech database and a telephone net work version of the identical TIMIT utterances (N-TIMIT). In this paper, we pre~nt insertion and deletion results for the segmenter (for both conditions, compared to hand transcriptions) as weil as pattems observed in segmentation errors as a function of data set. Also presented will be the results of the classification algorithm for both databases.	acoustic cryptanalysis;acoustic model;algorithm;bandlimiting;database;emoticon;experiment;speech recognition;timit;telephone line	Benjamin Chigier;Erik Urdang;Judith Spitz	1989			voice activity detection;artificial intelligence;audio mining;speech recognition;speech technology;speech corpus;speaker recognition;acoustic model;pattern recognition;computer science;speech processing	ML	-14.135741314224308	-88.58144975320297	38688
1117dedfe694d8f5b483b2422f580a68cfb96c95	similarity structure in visual phonetic perception and optical phonetics	euclidean distance;speech perception;optical recording;multidimensional scaling;consonant vowel;3 dimensional	This study was undertaken to examine relationships between the similarity structures of optical phonetic measures and visual phonetic perception. For this study, four talkers who varied in visual intelligibility were recorded simultaneously with a 3-dimensional optical recording system and a video camera. Subjects perceptually identified the talkers’ consonant-vowel nonsense syllable utterances in a forced-choice identification task. Then, perceptual confusion matrices were analyzed using multidimensional scaling, and Euclidean distances among stimulus phonemes were obtained. Physical Euclidean distances between phonemes were computed on the raw 3-dimensional optical recordings for the phonemes used in the perceptual testing. Multilinear regression was used to generate a transformation vector between physical and perceptual distances. Then, correlations were computed between transformed physical and perceptual distances. These correlations ranged between .77 and .81 (59% and 66% variance accounted for), depending on the vowel context. This study showed that the relatively raw representations of the physical stimuli were effective in accounting for visual speech perception, a result consistent with the hypothesis that perceptual representations and similarity structures for visual speech are modality-specific.	confusion matrix;euclidean distance;image scaling;intelligibility (philosophy);modality (human–computer interaction);multidimensional scaling;syllable	Lynne E. Bernstein;Jintao Jiang;Abeer Alwan;Edward T. Auer	2001			speech recognition;acoustics;mathematics;communication	ML	-9.699527807383589	-83.33575011666207	38743
64ed4eec27a888f8bd9184082aba74d0b6d88214	recognition of classroom lectures in european portuguese		Classroom lectures may be very challenging for automatic speech recognizers, because the vocabulary may be very specific and the speaking style very spontaneous. Our first experiments using a recognizer trained for Broadcast News resulted in word error rates near 60%, clearly confirming the need for adaptation to the specific topic of the lectures, on one hand, and for better strategies for handling spontaneous speech. This paper describes our efforts in these two directions: the different domain adaptation steps that lowered the error rate to 45%, with very little transcribed adaptation material, and the exploratory study of spontaneous speech phenomena in European Portuguese, namely concerning filled pauses.	domain adaptation;experiment;finite-state machine;speech synthesis;spontaneous order;vocabulary	Isabel Trancoso;Ricardo Nunes;Luís Neves;Céu Viana;Helena Moniz;Diamantino Caseiro;Ana Isabel Mata	2006			word error rate;exploratory research;speech recognition;european portuguese;automatic speech;domain adaptation;computer science;vocabulary	NLP	-18.029765296448595	-83.4430016427528	38772
c332aa5ed33b13e343ae00739805e5ee6569265d	kana-kanji conversion system with input support based on prediction	input support;character string;kana-kanji conversion system;system dictionary;certainty factor;kana character string;proposed system;key input operation;greater factor;prediction method feature	1 I n t r o d u c t i o n TOSHIBA developed the world's first Japanese word processor in 1978. Unlike languages based on an alphabet , Japanese uses /,housands of Ica nji characters of varying comp]exity. Hence, l,o arrange all of l~a'~:ii chm'acl;ers on keyboard is; difficult. On the other hand, kana dlaracters which are phonetic scripl,s of Japanese have 83 variations; these can be arranged on keyb o a ' & As a result, conversion from kana notat ions to kanji ones, whal, is called kana-ka,~:ji conversion, has been used. Since Japanese is not writl,en separately by words, segmental,ion of typed kana characl,er strings has ambiguil,y. And ~m ambiguil,y in conversion exists, too; a kana notat ion may correspond l,o some different t:a, nji notations. These make t,:ana-kanji conversion challenging. We have made efforts l,o raise a precision of kana-kaTt:ji conversion, thinking l,h~t high precision can provide a be t te r input enviromnent for the user. A t)rccision of our kana-kanji conversion system reaches 95-98% for several kinds o[' texts in our previous experiments. Neverthe]ess, this approach is not; enough in the situations where fasl; typing is hard, e.g., lbr beginners who m'e not familiar wil,h keyboard or for palm-size comlmters. Thus, new method to reduce key input; operations is needed. We propose a k(vna-kanji conversion system witll input suppor t based on prediction. This system is composed o[' two parts: prediction of succeeding ka'nji character strings t]'om typed kana ones, and ordinary hrvna-kanji conversion. It; automatical ly shows candidates of ka'nji character strings which the user intends to input. The candidates change as /;lie usc, r inputs ka.na characters. If no approt)riate ctloice is presenl;e(t, the candidates aul;Olnatictd]y disappear when the ne×t kana character is entered. Our system, l,here['ore, can be used in the same manner as an ordinary ka'na-kanji conversion system, and allows the user to save time and efforts for key input without learning new key operations. We have been considered two issues to generate accurate candidates: (i) How Co determine where t;yped kana character strings are segmented; since Japanese is not wri t ten separately by words, determinat ion of positions where words start is needed to retrieve dictionaries. (ii) How to determine when prediction candidates are presented; if all of retrieval results are always shown, a sysl,em cannot be convenielfl,. Surveying previous works Dora the view on above issues, we found l,hal, the Reacl,ive Keyboard has been proposed (Darragh el, al., 1980). It accelerates typewri t ten communication with	ana (programming language);dictionary;doraemon;emoticon;experiment;expert system;hardware random number generator;japanese input methods;string (computer science);text corpus;typing	Yumi Ichimura;Yoshimi Saito;Kazuhiro Kimura;Hideki Hirakawa	2000			natural language processing;speech recognition;computer science;artificial intelligence	NLP	-26.609287432802716	-83.06040960271075	38808
b06fd351d10eccb830b75deb6061ffa3382f4166	combining tdnn and hmm in a hybrid system for improved continuous-speech recognition	word error rate;large vocabulary tasks hybrid system improved continuous speech recognition tdnn hmm speaker dependent darpa resource management task combined system normalized neural network output scores hidden markov model emission probabilities maximum likelihood estimation mean square error word error rate training state of the art hmm system time delay neural network viterbi framework;neural nets;hidden markov model;resource manager;maximum likelihood estimation;error statistics maximum likelihood estimation speech recognition neural nets hidden markov models learning artificial intelligence telecommunications computing;continuous speech recognition;time delay neural network;maximum likelihood estimate;hidden markov models;telecommunications computing;mean square error;hybrid system;hidden markov models neural networks management training error analysis resource management mean square error methods maximum likelihood estimation training data data mining delay effects;speaker dependent;speech recognition;error statistics;learning artificial intelligence;task management;neural network	The paper presents a hybrid continuous-speech recognition system that leads to improved results on the speaker dependent DARPA Resource Management task. This hybrid system, called the combined system, is based on a combination of normalized neural network output scores with hidden Markov model (HMM) emission probabilities. The neural network is trained under mean square error and the HMM is trained under maximum likelihood estimation. In theory, whatever criterion may be used, the same word error rate should be reached if enough training data is available. As this is never the case, the idea of combining two different criteria, each of them extracting complementary characteristics of the feature is interesting. A state-of-the-art HMM system will be combined with a time delay neural network (TDNN) integrated in a Viterbi framework. A hierarchical TDNN structure is described that splits training into subtasks corresponding to subsets of phonemes. This structure makes training of TDNNs on large-vocabulary tasks manageable on workstations. It will be shown that the combined system, despite the low accuracy of the hierarchical TDNN, achieves a word error rate reduction of 15% with respect to our state-of-the-art HMM system. This reduction is obtained with a 10% increase only in the number of parameters.	artificial neural network;broadcast delay;hidden markov model;hybrid system;markov chain;mean squared error;speech recognition;time delay neural network;vocabulary;word error rate;workstation	Christian Dugast;Laurence Devillers;Xavier L. Aubert	1994	IEEE Trans. Speech and Audio Processing	10.1109/89.260364	speech recognition;computer science;machine learning;pattern recognition;maximum likelihood;hidden markov model	ML	-19.614191082122428	-88.12477538074192	38873
476961e42331b4e28bd3ccb53442703dc82add8d	analysis on mishearing of speech signals whose constant intervals are periodically eliminated, and its characteristics				Jun Shirataki;Manabu Ishihara	1995	JRM	10.20965/jrm.1995.p0141	speech processing;speech recognition;computer science	ECom	-11.771825332798826	-88.08308140863342	38886
63abb608093cdbd0b015453e657a35c13f0e874f	acquiring variable length speech bases for factorisation-based noise robust speech recognition	noise robustness;noise robustness spectral factorization speech recognition;speech recognition;speech speech recognition noise context spectrogram mathematical model correlation;spectral factorization	Studies from multiple disciplines show that spectro-temporal units of natural languages and human speech perception are longer than short-time frames commonly employed in automatic speech recognition. Extended temporal context is also beneficial for separation of concurrent sound sources such as speech and noise. However, the length of patterns in speech varies greatly, making it difficult to model with fixed-length units. We propose methods for acquiring variable length speech atom bases for accurate yet compact representation of speech with a large temporal context. Bases are generated from spectral features, from assigned state labels, and as a combination of both. Results for factorisation-based speech recognition in noisy conditions show equal or better separation and recognition quality in comparison to fixed length units, while model sizes are reduced by up to 40%.	monoid factorisation;natural language;speech recognition	Antti Hurmalainen;Tuomas Virtanen	2013	21st European Signal Processing Conference (EUSIPCO 2013)		voice activity detection;speaker recognition;linear predictive coding;speech recognition;computer science;speech coding;pattern recognition;speech processing;acoustic model;communication	NLP	-10.41856524255322	-90.59433429264166	38887
bb612f9c8f8e953d57ab5c75900b3d8814dc5dac	automatic staging of audio with emotions	human computer interaction;speech synthesis;speech synthesis emotion recognition human computer interaction;speech;emotion recognition;abstracts;geneemo text dramatisation text to speech technology human computer interactions expressive speech audio automatic staging;speech recognition;markov processes;speech speech synthesis human computer interaction markov processes speech recognition affective computing abstracts;affective computing	"""Current day text-to-speech technologies are mature enough to be acceptable in quality for the users. There is still a large gap between a synthesised speech and a real human speech due to lack of expressions and emotions. Geneemo is a technology for automatic addition of emotions and expressions to any audio. The process of staging the text is to dramatize it. The text is enriched and transformed into a performance. Similarly, """"staging the audio"""" refers to extending text dramatisation to audio by enriching emotionally neutral audio content into a natural human speech with real expressions. The audio can be generated by any text-to-speech technology. The aim of the project is to make human computer interactions as natural as possible with expressive speech. This also opens up a portfolio of applications replacing real human voices."""	disk staging;human computer;interaction;speech synthesis;speech technology	Lakshmi Saheer;Milos Cernak	2013	2013 Humaine Association Conference on Affective Computing and Intelligent Interaction	10.1109/ACII.2013.124	voice activity detection;natural language processing;speech technology;trace;audio mining;speech recognition;speech corpus;computer science;speech;motor theory of speech perception;speech processing;acoustic model;affective computing;linguistics;markov process;communication;speech synthesis;speech analytics	AI	-15.204893447283736	-84.94688990774101	38894
4831606d67855c03e61f32aa3a080522b5bee793	unsupervised estimation of the human vocal tract length over sentence level utterances	unsupervised estimation;speech waveform;speaker clustering potential unsupervised estimation human vocal tract length sentence level utterances gender independent estimation speech waveform x ray vowel data multiple sentence utterances male timit speaker female timit speaker correlation analyses body heights error criteria noniterative closed form estimator solutions;impedance;sentence level utterances;image processing;speech processing;speech analysis;frequency estimation;gender independent estimation;speech;shape measurement;data engineering;vocal tract length;speaker clustering potential;male timit speaker;poles and zeros;physiology;parameter estimation physiology speech speech processing;error criteria;signal processing;female timit speaker;human voice;noniterative closed form estimator solutions;multiple sentence utterances;correlation analyses;human vocal tract length;human voice impedance poles and zeros frequency estimation speech analysis shape measurement signal processing image processing data engineering speech processing;parameter estimation;body heights;body height;x ray vowel data;x rays	This paper describes a method for the unsupervised and gender-independent estimation of the average human vocal tract length from the speech waveform, and reports results obtained on Fant’s X-ray vowel data as well as results from experiments performed on multiple sentence utterances of 86 male and 78 female TIMIT speakers, including correlation analyses between the vocal tract length estimates and given body heights. The investigated error criteria that make non-iterative, closed-form estimator solutions possible are all found to achieve good speaker clustering potential for both male and female subgroups.	cluster analysis;experiment;iterative method;timit;tract (literature);unsupervised learning;waveform	Burhan Necioglu;Mark A. Clements;Thomas P. Barnwell	2000		10.1109/ICASSP.2000.861821	pole–zero plot;speech recognition;human voice;image processing;computer science;speech;electrical impedance;signal processing;speech processing;estimation theory	NLP	-9.177032193721278	-89.79188700437366	38948
2fe8a4936b7e3abf5a07d7aac23b0833bdeda070	on the role of missing data imputation and nmf feature enhancement in building synthetic voices using reverberant speech	speech synthesis;dereverberation;speech enhancement;glotthmm;nonnegative matrix factorization;missing data imputation	In this paper, we study the role of a recently proposed feature enhancement technique in building HMM-based synthetic voices using reverberant speech data. The feature enhancement technique studied combines the advantages of missing data imputation and non-negative matrix factorization (NMF) based methods in cleaning up the reverberant features. Speaker adaptation of a clean average voice using noisy data is generally better than building a speaker dependent voice using the noisy data. In this paper, we show that the proposed feature enhancement technique can further improve the spectral match between the enhanced feature adapted voice and a clean speaker dependent voice.	feature model;geo-imputation;hidden markov model;missing data;non-negative matrix factorization;plasma cleaning;signal-to-noise ratio;speech synthesis;synthetic intelligence	Dhananjaya N. Gowda;Heikki Kallasjoki;Reima Karhila;Cristian Contan;Kalle J. Palomäki;Mircea Giurgiu;Mikko Kurimo	2014			speech recognition;computer science;machine learning;pattern recognition;speech synthesis;non-negative matrix factorization	Mobile	-14.42868916211217	-92.02244636803492	38950
1ca4670e9967fcb91ef1826f52d4d621fa7a950d	listening while speaking: speech chain by deep learning		Despite the close relationship between speech perception and production, research in automatic speech recognition (ASR) and text-to-speech synthesis (TTS) has progressed more or less independently without exerting much mutual influence on each other. In human communication, on the other hand, a closed-loop speech chain mechanism with auditory feedback from the speaker's mouth to her ear is crucial. In this paper, we take a step further and develop a closed-loop speech chain model based on deep learning. The sequence-to-sequence model in close-loop architecture allows us to train our model on the concatenation of both labeled and unlabeled data. While ASR transcribes the unlabeled speech features, TTS attempts to reconstruct the original speech waveform based on the text from ASR. In the opposite direction, ASR also attempts to reconstruct the original text transcription given the synthesized speech. To the best of our knowledge, this is the first deep learning model that integrates human speech perception and production behaviors. Our experimental results show that the proposed approach significantly improved the performance more than separate systems that were only trained with labeled data.	automatic system recovery;concatenation;deep learning;speech recognition;speech synthesis;transcription (software);waveform	Andros Tjandra;Sakriani Sakti;Satoshi Nakamura	2017	2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)	10.1109/ASRU.2017.8268950	natural language processing;motor theory of speech perception;architecture;voice activity detection;speech analytics;artificial intelligence;auditory feedback;deep learning;speech corpus;computer science;speech perception;speech recognition	NLP	-17.67952533995178	-87.70080888649017	38985
8bdd8da44a5a673d0c036a78282bb8eac4bc0e50	music in our ears: the biological bases of musical timbre perception	female;models neurological;judgment;bioacoustics;support vector machines;auditory perception;acoustics;recognition psychology;male;sound;acoustic stimulation;auditory cortex;adult;music perception;algorithms;humans;neurophysiology;perception;computational biology;music;sensory perception;psychophysics	Timbre is the attribute of sound that allows humans and other animals to distinguish among different sound sources. Studies based on psychophysical judgments of musical timbre, ecological analyses of sound's physical characteristics as well as machine learning approaches have all suggested that timbre is a multifaceted attribute that invokes both spectral and temporal sound features. Here, we explored the neural underpinnings of musical timbre. We used a neuro-computational framework based on spectro-temporal receptive fields, recorded from over a thousand neurons in the mammalian primary auditory cortex as well as from simulated cortical neurons, augmented with a nonlinear classifier. The model was able to perform robust instrument classification irrespective of pitch and playing style, with an accuracy of 98.7%. Using the same front end, the model was also able to reproduce perceptual distance judgments between timbres as perceived by human listeners. The study demonstrates that joint spectro-temporal features, such as those observed in the mammalian primary auditory cortex, are critical to provide the rich-enough representation necessary to account for perceptual judgments of timbre by human listeners, as well as recognition of musical instruments.	auditory area;base;cerebral cortex;eprs gene;instrument - device;judgment;machine learning;mammals;mechatronics;nonlinear system;primary auditory cortex	Kailash Patil;Daniel Pressnitzer;Shihab A. Shamma;Mounya Elhilali	2012		10.1371/journal.pcbi.1002759	pitch (music);speech recognition;order;perception;neurophysiology;psychophysics	ML	-7.3945194323451116	-81.71968478943064	39000
f46720cf340a652992ff1b915dac55c47b0f8333	spectral difference for statistical model-based speech enhancement in speech recognition	speech enhancement;noise reduction;speech recognition;spectral difference	In this paper, we propose a statistical model-based speech enhancement technique using the spectral difference scheme for the speech recognition in virtual reality. In the analyzing step, two principal parameters, the weighting parameter in the decision-directed (DD) method and the long-term smoothing parameter in noise estimation, are uniquely determined as optimal operating points according to the spectral difference under various noise conditions. These optimal operating points, which are specific according to different spectral differences, are estimated based on the composite measure, which is a relevant criterion in terms of speech quality. An efficient mapping function is also presented to provide an index of the metric table associated with the spectral difference so that operating points can be determined according to various noise conditions for an on-line step. In the on-line speech enhancement step, different parameters are chosen on a frame-by-frame basis under the metric table of the spectral difference. The performance of the proposed method is evaluated using objective and subjective speech quality measures in various noise environments. Our experimental results show that the proposed algorithm yields better performances than conventional algorithms.	algorithm;computation;online and offline;performance;smoothing;speech enhancement;speech recognition;statistical model;virtual reality	Soojeong Lee;Joon-Hyuk Chang	2016	Multimedia Tools and Applications	10.1007/s11042-016-4122-7	speech recognition;pattern recognition;statistics	ML	-13.039762205696798	-93.64154311335814	39156
c61ebd34f00799979cdd0cc16efa96eb5b8720d2	amplitude and duration interdependence in the perceived intensity of complex tactile signals		The dependency of the perceived intensity of a short stimulus on its duration is well established in vision and audition. No such phenomenon has been reported for the tactile modality. In this study naive observers were presented with pink noise vibrations enveloped in a Gabor wavelet. Characteristic durations ranging between 100 and 700 ms and intensities ranging from 0.3 and 3.0 10−3 m/s were presented to the fingertip. Using a two alternative forced choice staircase procedure, the points of subjective equivalence were estimated for the 400 ms long reference stimulus. Similarly to vision and audition, lower intensities were consistently reported for shorter stimuli. The observed relationship could be interpreted as reflecting a mechanism of haptic constancy with respect to exploration speed.	gabor wavelet;haptic technology;interdependence;modality (human–computer interaction);pink noise;turing completeness	Séréna Bochereau;Alexander Terekhov;Vincent Hayward	2014		10.1007/978-3-662-44193-0_13	acoustics;communication	HCI	-6.870325567995396	-81.42859518985944	39194
2ab012651369120cb4e33d7039e7e693c1a9ec01	基於深層類神經網路之音訊事件偵測系統(deep neural networks for audio event detection)[in chinese]		現實生活中常有許多聲音事件會一起發生,而聲音會重疊在一起,使得傳統(Gaussian Mixture Model ,GMM)方法很難準確辨認這些重疊的聲音事件。因此,本文提出以深層 類神經網絡(Deep Neural Network, DNN)來檢測這些互相干擾的聲音事件,並據此參加 Detection and Classification of Acoustic Scenes and Events 2016 (DCASE2016) 比賽, DCASE2016 評比提供的音訊資料,內有兩種場景,包括居家與戶外,共有 18 種含有背 景的聲音事件。實驗結果顯示使用 DNN 與傳統 GMM 比較,其場景偵測錯誤率可從 0.91 降至 0.86、F1 分數並從 23.4%提升到 26.8%。此外針對室內環境的音訊事件偵測,錯誤 率可從 1.06 降至 0.86,F1 分數並從 8.9%提升到 27.7%。最後在戶外環境的音訊偵測情 境中,錯誤率可從 1.03 降至 0.96, F1 分數從 17.6%降到 12.8%。因為 DACSE2016 比 賽主要看錯誤率,所以整體而言 DNN 方法還是明顯比 GMM 方法好。	deep learning;google map maker;mixture model;neural networks	Jhih-wei Chen;Chia-Hsin Liu;Yuan-Fu Liao	2016			artificial intelligence;pattern recognition;artificial neural network;computer science;speech recognition	ML	-15.319124568988402	-87.73395964895337	39198
e3ca4364e3ff842f6b88879c61d56a23b76fc2d2	on the information rate of speech communication		The key to the success of speech-based technology is an understanding of human speech communication. While significant advances have been made, a unified theory of speech communication that is both comprehensive and quantitative is yet to emerge. In this paper we approach speech communication from an information theoretical perspective. Without relying on prior knowledge of speech production, language, or auditory processing, we develop a new methodology for measuring the information rate of speech. Instead we rely on having recordings of multiple talkers saying the same utterance. In general, our results are consistent with a linguistic understanding of speech communication.	information theory	Steven Van Kuyk;W. Bastiaan Kleijn;Richard Christian Hendriks	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7953233	cued speech;voice activity detection;motor theory of speech perception;speech analytics;speech corpus;computer science;natural language processing;intelligibility (communication);speech technology;speech recognition;speech processing;artificial intelligence	Visualization	-13.178131222851254	-84.2235953636108	39206
659f80392e37e82e3a20c3ab931f8feb47d88418	the acoustic characteristics of japanese identical vowel sequences in connected speech		It has been pointed out that a word which contains an identical vowel sequence may not be distinguished, especially in faster speech, from a word which has a long vowel and phonologically the same mora-count. However, this indication has been made without any objective evidence and it is argued that a quantitative study is needed to investigate the acoustic characteristics of identical vowel sequence in comparison with those of a long vowel. This paper reports the results of production experiments for this purpose done by 10 native speakers of Japanese. The results will be discussed from the viewpoints of glottal stop insertion between an identical vowel sequence and durational ratio comparison. More specifically, the actual number of glottal stop insertions on citation and sentence levels will be clearly shown and the acoustic features of the glottal stop such as its duration will be investigated. In addition, the ratios of the whole token, consonant + vowel(s) and the vowel part of identical vowel sequence examples will be made clear and compared with those of long vowel examples in both citation and sentence reading forms.	acoustic cryptanalysis;experiment	Tsutomu Sato;John A. Maidment	2000			speech recognition;artificial intelligence;connected speech;mid vowel;pattern recognition;computer science;vowel	NLP	-10.57037990029572	-82.18934824602087	39587
584acdb496e5a0650e5b60e490d2079007f0a352	efficient speaker detection via target dependent data reduction	filters filtering speaker recognition speech processing speech analysis algorithm design and analysis computer architecture throughput pipelines feature extraction;architectural design;distributed speech processing architecture design;speech processing;speaker detection;data filtering;target dependent data reduction time critical information extraction data filtering distributed speech processing architecture design speaker recognition speaker detection;speech processing feature extraction filtering theory speaker recognition;speaker recognition;system design;feature extraction;target dependent data reduction;dependent data;point of view;time critical information extraction;filtering theory	Systems designed to extract time-critical information from large volumes of unstructured data must include the ability, both from an architectural and algorithmic point of view, to filter out unimportant data that might otherwise overwhelm the available resources. This paper presents an approach for data filtering to reduce computation in the context of a distributed speech processing architecture designed to detect or identify speakers. Here, filtering means either dropping and ignoring data or passing it on for further processing. The goal of the paper is to show that when the filter is designed to select and pass on a subset of the input data that best preserves the ability to recognize a specific desired speaker, or group of speakers, a large percentage of the data can be ignored while being able to preserve most of the accuracy	algorithm;computation;speech processing;window of opportunity	Upendra V. Chaudhari;Olivier Verscheure;Juan M. Huerta;Xiang Li;Ganesh N. Ramaswamy;Lisa Amini	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262696	speaker recognition;speech recognition;feature extraction;computer science;machine learning;pattern recognition;speech processing;systems design	Robotics	-10.305307719231111	-93.08485058451708	39627
20635a61280d419cb6a65d63a0d115e5fc3c7435	rhythmic similarity of music based on dynamic periodicity warping	musical pieces;modified knn classification approach;rhythm;audio signal processing;information retrieval rhythmic similarity dynamic periodicity warping musical pieces periodicity spectra warping path linearity modified knn classification approach;rhythm computer science shape measurement multiple signal classification informatics linearity music information retrieval fourier transforms bars timing;information retrieval;dynamic periodicity warping;information retrieval rhythm similarity music;periodicity spectra;indexing terms;rhythmic similarity;music information retrieval;signal classification;similarity;classification accuracy;music;warping path linearity;signal classification audio signal processing information retrieval	This paper introduces a new way to measure rhythmic similarity between two musical pieces using periodicity spectra. In order to detect similarity for pieces of different tempi, the linearity of the warping path between their spectra serves as a measure of their rhythmic similarity. Using a modified kNN classification approach on two datasets, the proposed measure provides comparable classification accuracy (82.1%) to the best of widely used measures (85.5%) for the first dataset; For the second dataset, which is characterized by a large variance of tempi, the proposed measure outperforms all reference measures, reaching an accuracy of 69.0%, while the best of the other measures reaches 53.8%. Moreover, the presented technique works fully automatically, and no information regarding tempo is needed.	quasiperiodicity	Andre Holzapfel;Yannis Stylianou	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518085	speech recognition;similarity;audio signal processing;computer science;rhythm;machine learning;pattern recognition;music;mathematics	Visualization	-7.959615236055722	-92.05761775484521	39646
4532829e55f7863f43ca65c1371f2a19bc3208ed	improved speaker segmentation and segments clustering using the bayesian information criterion.	bayesian information criterion	Detection of speaker, channel and environment changes in a continuous audio stream is important in various applications (e.g., broadcast news, meetings/teleconferences etc.). Standard schemes for segmentation use a classi er and hence do not generalize to unseen speaker / channel / environments. Recently S.Chen introduced new segmentation and clustering algorithms, using the so-called BIC. This paper presents more accurate and more e cient variants of the BIC scheme for segmentation and clustering. Speci cally, the new algorithms improve the speed and accuracy of segmentation and clustering and allow for a real-time implementation of simultaneous transcription, segmentation and speaker tracking.	algorithm;bayesian information criterion;cluster analysis;real-time clock;scheme;simultaneous multithreading;streaming media;transcription (software);viavoice	Alain Tritschler;Ramesh A. Gopinath	1999			determining the number of clusters in a data set;bayesian information criterion	Vision	-14.922918514932979	-88.5450143625361	39751
322f314e195362f99f06ae663cdb878b083666a1	noise-robust cellular phone speech recognition using codec-adapted speech and noise models	codecs;speech;noise robustness;error analysis;multiaccess communication codecs error analysis motorcycles speech;motorcycles;word recognition;speech recognition;speech detection;multiaccess communication	In this paper, we introduce a rapid and noise-robust CODEC adaptation for cellular phone speech recognition in adverse environment. Our proposal is to use CODEC-dependent noise models (CDNM) in addition to CODEC-dependent speech models (CDSM) for reducing speech detection errors caused by the background noise and compensating nonlinear speech distortion due to the low-bitrate CODECs. In this approach, the CODEC type is automatically determined through the recognition process by comparing likelihoods of CODEC-dependent models. Experiments on 3k words recognition of noisy speech showed 25% improvement in accuracy, and 46% reduction in early starting point detection errors over the conventional CODEC-independent approach.	codec;distortion;mobile phone;nonlinear system;speech recognition	Tsuneo Kato;Masaki Naito;Tohru Shimizu	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5743710	voice activity detection;adaptive multi-rate audio codec;speaker recognition;codec2;codec;speech recognition;word recognition;word error rate;computer science;speech;speech coding;speech processing;acoustic model;psqm	Vision	-13.926312756947002	-91.47304136855483	39828
e3d30e1ef57a5c1548fb6a9847db42bdf0c5c2af	language modeling for what-with-where on goog-411	human language technologies;language modeling architectures;voice search;presentation;goog 411;what with where queries;10th annual conference of the international speech communication association;interspeech 2009;speech recognition;speech communication;hierarchical language model;language model	This paper describes the language modeling architectures and recognition experiments that enabled support of ’what-withwhere’ queries on GOOG-411. First we compare accuracy trade-offs between a single national business LM for business queries and using many small models adapted for particular cities. Experimental evaluations show that both approaches lead to comparable overall accuracy. Differences in the distributions of errors also lead to improvements from a simple combination. We then optimize variants of the national business LM in the context of combined business and location queries from the web, and finally evaluate these models on a recognition test from the recently fielded ’what-with-where’ system.	experiment;language model	Charl Johannes van Heerden;Johan Schalkwyk;Brian Strope	2009			natural language processing;cache language model;speech recognition;computer science;speech;machine learning;linguistics;modeling language;language model	NLP	-22.898488804256296	-83.52265907493226	39847
4243fefb4b6403cfe304941e72e612b62c600f45	voice activity detection using source separation techniques.	voice activity detection;source separation			Tomohiko Taniguchi;Shoji Kajita;Kazuya Takeda;Fumitada Itakura	1997			voice activity detection;speech recognition;computer science	Vision	-13.895245133275148	-87.30689515635461	39871
14077fea58056729c51c7fdbaed7edffcffdd043	speech enhancement using compressed sensing		This paper proposes an approach based on the compressed sensing for speech enhancement. It attempts to exploit the fact that it is relatively easy to find sparse representation for speech signal but the same cannot hold true for noise. Thus for a given speech signal a sparse vector is derived which extracts the speech signal from the given noisy speech signal. Approach is further modified to ensure the speech enhancement across the varying segments of speech signal, which is present due to language, emotion and speech uttered by a person. Experimental results show that the proposed approach can be an alternative to the existing approaches for speech enhancement.	compressed sensing;sparse approximation;sparse matrix;speech enhancement	Vinayak Abrol;Pulkit Sharma;Anil Kumar Sao	2013			speech recognition;compressed sensing;speech enhancement;computer science	NLP	-14.05062158039308	-92.78863267284852	40084
257d417c65d45fc43368f544acebd3ba55667d11	voting for two speaker segmentation	change detection;indexing terms;speaker recognition;computational complexity;error rate	The process of locating the end points of each speakers voice in an audio file and then clustering segments based in speaker identity is called speaker segmentation. In this paper we present a method for two speaker segmentation, though it can be extended to more than two speakers. Most methods for speaker segmentation and clustering start with an initial computationally inexpensive speaker segmentation method, followed by a more accurate segment clustering. In this paper we describe a simple algorithm that improves the accuracy of the segment clustering while not increasing the computational complexity. Since the clustering is done iteratively, the improvement in each segment clustering step results in a significant overall increase in segmentation accuracy and cluster purity. We borrow ideas from speaker recognition to perform segment clustering by frame based voting. We look at each frame as an independent classifier deciding which speaker generated that segment. These ’classifiers’ are combined by voting to make a decision as to which segments should be clustered together. This simple change leads to 56.9% decrease in error rates on a segmentation task for the SWITCHBOARD corpus.	algorithm;cluster analysis;computational complexity theory;image segmentation;pc speaker;pure function;speaker recognition	Narayanaswamy Balakrishnan;Rashmi Gangadharaiah;Richard M. Stern	2006			speaker recognition;speaker diarisation;speech recognition;index term;word error rate;computer science;machine learning;pattern recognition;scale-space segmentation;computational complexity theory;change detection	Vision	-17.369873850020934	-94.17583029972161	40101
1ee269603ae0dba3176614fa7ef14d106380c773	difficulty understanding speech in noise by the hearing impaired: underlying causes and technological solutions	training;auditory system;speech;noise measurement;speech recognition;signal to noise ratio;signal processing algorithms	A primary complaint of hearing-impaired individuals involves poor speech understanding when background noise is present. Hearing aids and cochlear implants often allow good speech understanding in quiet backgrounds. But hearing-impaired individuals are highly noise intolerant, and existing devices are not very effective at combating background noise. As a result, speech understanding in noise is often quite poor. In accord with the significance of the problem, considerable effort has been expended toward understanding and remedying this issue. Fortunately, our understanding of the underlying issues is reasonably good. In sharp contrast, effective solutions have remained elusive. One solution that seems promising involves a single-microphone machine-learning algorithm to extract speech from background noise. Data from our group indicate that the algorithm is capable of producing vast increases in speech understanding by hearing-impaired individuals. This paper will first provide an overview of the speech-in-noise problem and outline why hearing-impaired individuals are so noise intolerant. An overview of our approach to solving this problem will follow.	acquired immunodeficiency syndrome;cochlear implants;cochlear implant;hearing aids;microphone device component;sensorineural hearing loss (disorder);solutions;algorithm;hearing impairment	Eric W. Healy;Sarah E. Yoho	2016	2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2016.7590647	psychology;speech recognition;computer science;noise measurement;speech;communication;signal-to-noise ratio;audiology	SE	-6.44669794677363	-86.59754423326488	40116
2e189db86d27f9b444a78176902503f4e4a197f9	discriminative optimisation of large vocabulary recognition systems	recognition accuracy;optimisation;speech recognition;mmie training algorithm;si-284 training set;system structure optimization;continuous-density hmm-based large-vocabulary speech recognition system;alternative utterance hypotheses;vocabulary;computational complexity;discriminative optimization;parameter optimization;mixture components;wall street journal database;word lattices;maximum mutual information estimation criterion;hidden markov models;confusable speech segments;iterative mixture splitting procedure;hidden markov model;databases;maximum likelihood estimation;training data;lattices;mutual information	This paper describes a framework for optimising the structure and parameters of a continuous density HMM-based large vocabulary recognition system using the Maximum Mutual Information Estimation (MMIE) criterion. To reduce the computational complexity of the MMIE training algorithm, confusable segments of speech are identified and stored as word lattices of alternative utterance hypotheses. An iterative mixture splitting procedure is also employed to adjust the number of mixture components in each state during training such that the optimal balance between number of parameters and available training data is achieved. Experiments are presented on various test sets from the Wall Street Journal database using the full SI-284 training set. These show that the use of lattices makes MMIE training practicable for very complex recognition systems and large training sets. Furthermore, experimental results demonstrate that MMIE optimisation of system structure and parameters can yield useful increases in recognition accuracy.	algorithm;computational complexity theory;discriminative model;experiment;hidden markov model;iterative method;mathematical optimization;mutual information;test set;the wall street journal;vocabulary	Valentin Valtchev;Philip C. Woodland;Steve J. Young	1996			speech recognition;computer science;machine learning;pattern recognition	ML	-18.665997133585016	-91.95831449900476	40218
fd08c47d9e354a86baff828e0f1d06de147b2fc6	an automatic input protocol recommendation method for tailored switch-to-speech communication aid systems	protocols;speech;accuracy;simulation automatic input protocol recommendation method switch to speech communication aid system tailoring switch to speech interface interactive communication disabled people support system voluntary movement disabilities body part motion eye movement switch input switch operation variation bandwidth limitation efficient input protocol switch operation pronunciation mapping learnable protocol individual switch operation requirement customized protocol user interface protocol design alphabet table knowledge input speed learnability switch to pronunciation conversion ambiguity conversion error n gram language model;hidden markov models;protocols accuracy switches speech speech recognition hidden markov models character recognition;speech recognition;user interfaces biomechanics eye handicapped aids learning artificial intelligence medical signal processing natural language processing protocols speech processing speech synthesis;switches;character recognition	A switch-to-speech interface can provide a means of interactive communication as a support system for people with disabilities with voluntary movements. Any motion of a part of the body, such as eye movements, can be used for the switch input. The number of possible switch operations varies from person to person, but the bandwidth is generally quite limited. Therefore, efficient input protocols are needed to map the switch operations to pronunciations. Meanwhile, the protocol must be easily learnable so that anyone can use it. To this end, we propose a protocol recommendation method that can accept individual requirements in switch operations. This method suggests a customized protocol for each user of the interface that is both speedy to enter and easy to remember. The two main ideas in the protocol design are utilizing the knowledge about the alphabet table that everyone already knows and improving the input speed and learnability by allowing ambiguity in the switch to pronunciation conversion. The conversion errors due to the ambiguity are offset by an N-gram language model. The performance of the protocols was evaluated through simulations and the measured values obtained from research participants, and the advantage of the proposed method is shown.	bandwidth (signal processing);baseline (configuration management);communications protocol;experiment;language model;learnability;n-gram;requirement;simulation;usability;vii	Fuming Fang;Takahiro Shinozaki;Takao Kobayashi	2014	Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific	10.1109/APSIPA.2014.7041638	speech recognition;computer science;artificial intelligence;communication	Robotics	-25.083814420914774	-87.21268941895143	40385
a51cb857ae83cb3723d15697cc0142c15b01bb36	classification of audio events in broadcast news	broadcast news;speaker identification;audio signal processing;fuzzy membership function;speech feature extraction switches data preprocessing information retrieval indexing multimedia communication tv broadcasting radio broadcasting niobium compounds;speaker recognition;experimental results audio events classification news report commercials music at t hierarchical segmentation semantically meaningful units abstraction levels coarse level audio data preprocessing news segments speaker identification system classifier performance fuzzy membership functions;levels of abstraction;feature extraction;signal classification;signal classification broadcasting audio signal processing fuzzy systems speaker recognition feature extraction;broadcasting;fuzzy systems	This paper describes our approach to discriminate news report from others such as commercials and music in broadcast news programs based on audio information. The reported work here is part of the effort at AT&T to hierarchically segment broadcast news programs into semantically meaningful units at different levels of abstraction. At the coarse level, using the described approach we preprocess the audio data to pass only the news segments as input to a speaker identification system. To develop a lightweight preprocessing scheme for efficiency, we adopted a set of audio features that are simple to compute yet, based on our observation, statistically capture the intrinsic properties of the audio events to be classified. To improve the performance of the classifier, fuzzy membership functions associated with the features are introduced. Preliminary experimental results are reported which demonstrate the usefulness of the approach.	preprocessor;principle of abstraction;speaker recognition	Zhu Liu;Qian Huang	1998		10.1109/MMSP.1998.738963	speaker recognition;speaker diarisation;audio mining;speech recognition;audio signal processing;feature extraction;computer science;machine learning;pattern recognition;broadcasting	Web+IR	-8.927576228720104	-92.61259854059409	40566
318c3014cd1b6c4f528cf32116e31dd74069fef2	multiple-feature fusion based onset detection for solo singing voice	gaussian mixture model	Onset detection is a challenging problem in automatic singing transcription. In this paper, we address singing onset detection with three main contributions. First, we outline the nature of a singing voice and present a new singing onset detection approach based on supervised machine learning. In this approach, two Gaussian Mixture Models (GMMs) are used to classify audio features of onset frames and non-onset frames. Second, existing audio features are thoroughly evaluated for this approach to singing onset detection. Third, feature-level and decision-level fusion are employed to fuse different features for a higher level of performance. Evaluated on a recorded singing database, the proposed approach outperforms state-of-the-art onset detection algorithms significantly.	algorithm;machine learning;mixture model;onset (audio);solo;supervised learning;transcription (software)	Chee-Chuan Toh;Bingjun Zhang;Ye Wang	2008			machine learning;speech recognition;mixture model;fusion;computer science;artificial intelligence;singing;pattern recognition	Vision	-10.962994081357698	-91.4074954848662	40571
dcd896db918bfa0aaa2ef367179f130c422bf2cb	a wind-noise suppressor based on wind-onset detection and spectral gain modification	speech processing signal detection spectral analysis;wind noise suppressor speech distortion speech onset elimination spectral smoothness speech presence likelihood input noisy speech snr estimation spectral gain modification wind onset detection;speech noise wind noise measurement speech enhancement spectrogram gain	A wind-noise suppressor based on wind-onset detection and spectral gain modification is proposed. Wind onsets are detected based on an estimated SNR to find frequencies of sharply increasing components in each frame of an input noisy speech. The detection result is modified by speech presence likelihood based on spectral smoothness to eliminate speech onsets. To suppress wind noise with little speech distortion, spectral gains are made smaller at the detected wind-onsets. Subjective evaluation results show that the 5-grade MOS for the proposed wind-noise suppressor reaches 3.4 and is 0.56 higher than that by a conventional noise suppressor with a statistically significant difference.	distortion;onset (audio);signal-to-noise ratio;speech synthesis	Masanori Kato;Akihiko Sugiyama	2014	2014 14th International Workshop on Acoustic Signal Enhancement (IWAENC)	10.1109/IWAENC.2014.6953355	electronic engineering;linear predictive coding;speech recognition;acoustics	ML	-9.836424999886324	-89.07603023968198	40607
7bb6e51efddade2256220bf3b452d55fd5a0ef6a	japanese learning system for native chinese speakers focusing on differences between chinese and japanese	japanese language learning system;japanese learning system;kanji meaning;natural languages;chinese language;japanese language;native chinese speaker;kanji sound learning;linguistics;kanji writing;chinese speaker;native chinese speakers;computer aided instruction;loudspeakers;language learning;information technology;writing;shape;tellurium	"""In this paper, we describe development of a Japanese learning system for native Chinese speakers focusing on the differences between Chinese and Japanese. First, we analyze the differences between both languages and propose how to give questions for Chinese speakers. Then, we describe a Japanese language learning system """"JaLeSy-CJ"""" to support native Chinese speakers with learning sound, writing, and meaning of Kanji"""		Sa Lu;Naoko Yamashita;Hiroyuki Tominaga;Toshihiro Hayashi;Toshinori Yamasaki	2006	Sixth IEEE International Conference on Advanced Learning Technologies (ICALT'06)	10.1109/ICALT.2006.207	loudspeaker;natural language processing;speech recognition;shape;computer science;east asian languages;tellurium;linguistics;natural language;writing;information technology;chinese	NLP	-16.581912933764407	-81.21446684282813	40624
6c044715a2452604f231d6f5d7f7a0810c11db63	unsupervised single-channel speech separation via deep neural network for different gender mixtures	neural networks;speech processing;training;speech;hidden markov models;speech recognition;signal to noise ratio	In this study, we propose a regression approach via deep neural network (DNN) for unsupervised speech separation in a single-channel setting. We rely on a key assumption that two speakers could be well segregated if they are not too similar to each other. A dissimilarity measure between two speakers is then proposed to characterize the separation ability between competing speakers. We demonstrate that the distance between speakers of different genders is large enough to warrant a possible separation. We finally propose a DNN architecture with dual outputs, one representing the female speaker group and the other characterizing the male speaker group. Trained and tested on the Speech Separation Challenge corpus our experimental results show that the proposed DNN approach achieves large performance gains over the state-of-the-art unsupervised techniques without using specific knowledge about the mixed target and interfering speakers and even outperforms the supervised GMM-based method.	artificial neural network;computational auditory scene analysis;deep learning;google map maker;signal-to-noise ratio;unsupervised learning	Yannan Wang;Jun Du;Li-Rong Dai;Chin-Hui Lee	2016	2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)	10.1109/APSIPA.2016.7820736	speech recognition;computer science;pattern recognition;communication	AI	-15.712428923879711	-90.58377475829015	40682
3baf2b7ac65265c584d48924721bd807c037c3c9	detection of depression in adolescents based on statistical modeling of emotional influences in parent-adolescent conversations	depression diagnosis;support vector machines acoustic signal detection emotion recognition gaussian processes medical disorders mixture models optimisation signal classification speech processing statistical analysis;speech speech processing accuracy mel frequency cepstral coefficient feature extraction conferences;speech processing;emotional influence model depression diagnosis speech classification conversation modeling;speech classification;speech;conversation modeling;mel frequency cepstral coefficient;accuracy;feature extraction;emotional influence speech based depression detection technique acoustic speech parameter representative speech recording higher order influence model hoim coefficient emotional transition parameter conversational speech recording parent adolescent conversation gaussian mixture model gmm nondepressed category classification depressed category classification acoustic parameters support vector machine svm model based technique mfcc optimisation teo feature optimisation statistical modeling;emotional influence model;conferences	The current benchmark speech-based depression detection techniques rely on acoustic speech parameters collected from large sets of representative speech recordings. This study for the first time investigates depression detection based on the higher order influence model (HOIM) coefficients and emotional transition parameters derived from a relatively small set of conversational speech recordings representing 63 different parent-adolescent conversations of time duration 20 minutes each. The adolescents included 29 (24 female and 5 male) individuals diagnosed with major depressive disorder and 34 (24 female and 8 male) healthy individuals. The mental state of parents was not assessed. The model-based depression diagnosis was compared with benchmark techniques based on acoustic speech parameters (mel frequency cepstral coefficients (MFCC) and Teager energy operator (TEO)). The classification into depressed on non-depressed categories was performed using the Gaussian Mixture Model (GMM) for the acoustic parameters and the support vector machine (SVM) for the HOIM features. The model based technique led to the highest average classification accuracy of 94% of for the HOIM of order 4, whereas the best benchmark techniques scored 70% for the optimized MFCCs and 71% for the optimized TEO features.	acoustic cryptanalysis;benchmark (computing);coefficient;energy operator;mel-frequency cepstrum;mental state;mixture model;statistical model;support vector machine	Melissa N. Stolar;Margaret Lech;Nicholas B. Allen	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178117	speech recognition;feature extraction;computer science;speech;pattern recognition;speech processing;acoustic model;accuracy and precision	Robotics	-11.643350191803075	-88.46573971623364	40733
cf8548986df7edd6c06901c8d010038204752ad8	implementation and evaluation of statistical parametric speech synthesis methods for the persian language	clustergen statistical parametric speech synthesis persian language persian speech synthesis system statistical speech synthesis hmm based speech synthesis unit selection quality ranking;unit selection;speech synthesis;speech synthesis hidden markov models natural language processing;hidden markov models speech speech synthesis databases training cmos integrated circuits adaptation models;persian language;ccr test;statistical parametric;hmm based speech synthesis;hidden markov models;text to speech;natural language processing;ccr test text to speech statistical parametric speech synthesis persian language	Scattered and little research in the field of Persian speech synthesis systems has been performed during the last ten years. Comprehensive framework that properly implements and adapts statistical speech synthesis methods for Persian has not been conducted yet. In this paper, recent statistical parametric speech synthesis methods including CLUSTERGEN, traditional HMM-based speech synthesis and its STRAIGHT version, are implemented and adapted for Persian language. CCR test is carried out to compare these methods with each other and with unit selection method. Listeners Score samples based on CMOS. The methods were ranked by averaging the CCR scores. The results show that STRAIGHT-based system produces the best quality. Traditional HMM-based and unit selection are second and third in quality ranking. These approximately produce the same quality. Finally CLUSTERGEN produces the worst quality among these four systems.	cmos;hidden markov model;speech synthesis	Sara Bahaadini;Hossein Sameti;Soheil Khorram	2011	2011 IEEE International Workshop on Machine Learning for Signal Processing	10.1109/MLSP.2011.6064608	natural language processing;speech recognition;speech corpus;computer science;communication	EDA	-20.07482156632426	-82.69727636908713	40740
40b562d46e30a5d91399af92c7557cd997e72494	towards environment-independent spoken language systems	codebook entry;recognition accuracy;input utterance;instantaneous snr;language system;cepstral domain;desk-top microphone;em technique;additive correction;cmu continuous-speech speaker-independent recognition;cepstral vector;noise;additives;signal processing;accuracy;speech recognition;algorithms	"""In this paper we discuss recent results from our efforts to make SPHINX, the CMU continuous-speech speakerindependent recognition system, robust to changes in the environment. To deal with differences in noise level and spectral tilt between close-talking and desk-top microphones, we describe two novel methods based on additive corrections in the cepstral domain. In the first algorithm, an additive correction is imposed that depends on the instantaneous SNR of the signal. In the second technique, EM techniques are used to best match the cepstral vectors of the input utterances to the ensemble of codebook entries representing a standard acoustical ambience. Use of these algorithms dramatically improves recognition accuracy when the system is tested on a microphone other than the one on which it was trained. Introduction There are many sources of acoustical distortion that can degrade the accuracy of speech-recognition systems. For example, obstacles to robustness include additive noise from machinery, competing talkers, etc., reverberation from surface reflections in a room, and spectral shaping by microphones and the vocal tracts of individual speakers. These sources of distortion cluster into two complementary classes: additive noise (as in the first two examples) and distortions resulting from the convolution of the speech signal with an unknown linear system (as in the remaining three). A number of algorithms for speech enhancement have been proposed in the literature. For example, Boll [3] and Beroufi et al. [2] introduced the spectral subtraction of DFT coefficients, and Porter and Boll [11] used MMSE techniques to estimate the DFT coefficients of corrupted speech. Spectral equalization to compensate for convolved distortions was introduced by Stockham et al. [13]. Recent applications of spectral subtraction and spectral equalization for speech recognition systems include the work of Van Compemolle [5] and Stem and Acero [12]. Although relatively successful, the above methtxls all depend on the assumption of independence of the spectral estimates across fr~uencies. Erell and Weimranb [6] demonstrated improved performance with an MMSE estimator in which correlation among frequencies is modeled explicitly. 157 Acero and Stem [1] proposed an approach to environment normalization in the cepstral domain, going beyond the noise stripping problem. In this paper we present two algorithms for speech normalization based on additive corrections in the cepstral domain and compare them to techniques that operate in the frequency domain. We have chosen the cepstral domain rather than the frequency domain so that can we work directly with the parameters that SPHINX uses, and because speech can be characterized with a smaller number of parameters in the cepstral domain than in the frequency domain. The first algorithm, SNR-dependent cepstral normalization (SDCN) is simple and effective, but it cannot be applied to new microphones without microphonespecific training. The second algorithm, codeword-dependent cepstral normalization (CDCN) uses the speech knowledge represented in a codebook to estimate the noise and spectral equalization necessary for the environmental normalization. We also describe an interpolated SDCN algorithm (iSDCN) which combines the simplicity of SDCN and the normalization capabilities of CDCN. These algorithms are evaluated with a number of microphones using an alphanumeric database in which utterances were recorded simultaneously with two different microphones. Experimental Procedures The alphanumeric database and system used for these experiments has been described previously [12] [l]. Briefly, the database contain.q utterances that were recorded simultaneously in stereo using both the close-talking Sennheiser HMD224 microphone (CLSTK), a standard in previous DARPA evaluations, and a desk-top Crown PZM6fs microphone (CRPZM). The recordings with the CRPZM exhibit not only background noise but also key clicks from workstations, interference from other talkers, and reverberation. The task has a vocabulary of 104 words that are highly confusable, A simplified version of SPHINX with no grammar was used. Baseline recognition results obtained by gaining and testing SPHIlqX using this database are shown in the first two columns of Table 1. With no processing, training and testing using the CRPZM degrades recognition accuracy by about 60 percent relative to that obtained by training and testing on the CLSTK. Although most of the """"new"""" errors introduced by the CRPZM were confusions of silence or noise segments with weak phonetic events, a significant percentage was also due to crosstalk [12]. It can also be seen that the """"cross conditions"""" (training on one microphone and testing using the other) produce a very large degradation in recognition accuracy. Independent Compensation for Noise and Filtering In this section we examine the performance of SPHINX under some of the techniques that have been used in the literature to combat noise and spectral ilk: multi-style training, short-time liftering, spectral subtraction, and spectral equalization. Multi-Style Training Multi-style training is a technique in which the training set includes data representing different conditions so that the resulting HMM models are more robust to this variability. This simple approach has been used successfully in the field of speech styles [10] and speaker independence [9]. The price one must pay for the robustness is a degradation in performance for cases in which the training and testing are done with the same condition. with s =1.0 which Jtmqua [8] found to be optimum, and the ban@ass liftering method defined by Juang [7]. Unfortunately, we found that application of these techniques produced essentially no improvement for clean speech and only a very small improvement for corrupted speech. Since the frequency-warping transformation in SPHINX alters the variances of the coefficients, some other set of weights may prove more effective. Spectral Subtraction and Equalization In spectral subtraction and equalization it is assumed that the speech signal x(t) is degraded by linear filtering and/or uncorrelated additive noise, as depicted in Fig. 1. The goal of the compensation is to reverse the effects of these degradations. x(t) ~ h(t) """"Clean"""" Linear speech Distortion ~ D ~ p g r a y(t) ded eech n(t) Additive Noise Figure 1: Model of the degradation. An experiment was carried out in which all the speech recorded from the CLSTK and the CRPZM microphones were used in training (Table 1). As expected, robustness is gained by using multi-style training, but at the expense of sacrificing performance with respect to the case of train and test on the same conditions. TRAIN CLSTK CRPZM MULTI Test CLSTK 85.3% 36.9% 78.3% Test CRPZM 18.6% 76.5% 6'9.7% Table 1: Comparison of recognition accuracy of SPHINX under different training and testing conditions. CLSTK is the Sennheiser HMD224, CRPZM is the Crown PZM6sf and MULTI means that the data from both microphones were used in training Liftering Many studies have examined several potential distortion measures for speech recognition in noise. Most of these measures involve unequal weighthags of the mean-square distance between cepstral coefficients of the reference and test utterances. The motivation for weighting distances between cepstral vectors is twofold: it provides some variance normalization for the coefficients and it makes the system more robust to noise and spectral tilt by giving less weight to the low-order cepstral coefficients. We tried in our system several weighting measures that have been proposed in the literature including the inverse of the intra-eluster variance as defined by Tokhura [14], the exponential lifter 158 Using the notation of Fig. 1, we can characterize the power spectral density (PSD) of the processes involved as Py(f) = Px(f) IH(f)12 + Pn(f) (1) Spectral equalization techniques attempt to compensate for the filter h(t), while spectral subtraction techniques attempt to remove the effects of the noise from the signal. We compare the performance of the following different implementations of spectral subtraction and equalization techniques in Table 2. • A spectral equalization algorithm (EQUAL) that is similar to the approach of [13]. It compensates for the effects of the linear fiJtering, but not the additive noise, as described in [12]. • A direct implementation of the original power spectral subtraction rule (PSUB) on 32 frequency bands obtained via a real D F r of the cepstmm vector. The restored cepstrurn is obtained with an inverse DFT. • An implementation of BoU's algorithm (MMSE1) [4], in which a transformation is applied to all the frequency bands of the CRPZM speech that minimizes the mean squared error relative to the CLSTK speech. The log-power correction in each frequency band depended only on the instantaneous SNR in that band. • An implementation of magnitude spectral subtraction (MSUB) described in [12] that incorporates overand under-subtraction depending on the SNR as suggested by [2]. In [12] it was noted that a cascade of the EQUAL and MSUB algorithms did not yield any further improvement in recognition accuracy because they interact nonlinearly. The different criteria used in PSUB, MSUB, produce different curves that relate the effective SNR of the input and output. Some of these curves are shown in Figure 2. ~zo[ •..-'~ _sl . Igh SN. MSUB . ~ il~ 10["""" MMSSE1 . . . ~ ' / / """" I . . . . . P S U e . . : ~ t """" .**..*'.° y l , , . . , . . . . .IO~.: . . / /I .15 ~ / / Figure 2: Input-Output transformation curves for PSUB, MSUB and MMSE1. The SNR is defined as the log-power of the signal in a frequency band minus the log-power of the noise in that band. The transformation for MSUB is not a single curve but a family of curves that depend on the total SNR for a given frame. TRAIN CLSTK CLSTK CRPZM CRPZM TEST CLSTK CRPZM CLSTK CRPZM BASE 85.3% 18.6% 36.9%"""	additive model;additive white gaussian noise;algorithm;audio normalization;bilinear transform;cepstrum;code word;codebook;coefficient;column (database);convolution;crosstalk;crown group;database normalization;discrete fourier transform;distortion;elegant degradation;expectation–maximization algorithm;experiment;frequency band;heart rate variability;hidden markov model;inferring horizontal gene transfer;input/output;interference (communication);interpolation;linear system;mean squared error;microphone;noise (electronics);noise figure;noise shaping;nonlinear system;race condition;reflection (computer graphics);robustness (computer science);signal-to-noise ratio;speaker recognition;spectral density;speech enhancement;speech recognition;sphinx;sphinx4;statistical model;stellar classification;test set;time complexity;utility functions on indivisible goods;vocabulary;workstation	Alex Acero;Richard M. Stern	1990			speech recognition;food additive;computer science;noise;signal processing;accuracy and precision	ML	-12.900477317617883	-91.10487495935911	41071
55672796f6726f0f17c86d08d347ee9cdaf3b5ea	large vocabulary speech recognition using subword units	vocabulaire;context dependency;algorithmique;modelo markov;reconocimiento palabra;speech processing;vocabulary;speaker dependency;tratamiento palabra;traitement parole;estimation a posteriori;vocabulario;maximum a posteriori estimation;a posteriori estimation;continuous speech recognition;markov model;subword hmms;estimacion a posteriori;algorithmics;algoritmica;speech recognition;reconnaissance parole;modele markov;task dependency	Research in large vocabulary speech recognition has been intensively carried out worldwide, in the past several years, spurred on by advances in algorithms, architectures and hardware. In the United States, the D A R P A community has focused efforts on studying several continuous speech recognition tasks including Naval Resource Management , a 991 word task, ATIS (Air Travel Information System), a speech understanding task with an open vocabulary (in practice on the order of several thousand words) and a natural language component , and Wall Street Journal, a voice dictation task with a vocabulary on the order of 20,000 words. Although we have learned a great deal about how to build and efficiently implement large vocabulary speech recognition systems, there remain a whole range of fundamenta l questions for which we have no definitive answers. In this paper we review the basic structure of a large vocabulary speech recognition system, address the basic system design issues, discuss the considerations in the selection of training material, choice of subword unit, method of training and adaptation of models of subword units, integration of language model, and implementation of the overall system, and report on some recent results, obtained at AT&T Bell Laboratories, on the Resource Management	algorithm;automatic transmitter identification system (television);information system;language model;natural language;speech recognition;substring;systems design;the wall street journal;vocabulary	Chin-Hui Lee;Jean-Luc Gauvain;Roberto Pieraccini;Lawrence R. Rabiner	1993	Speech Communication	10.1016/0167-6393(93)90025-G	natural language processing;speech recognition;computer science;maximum a posteriori estimation;speech processing;linguistics;markov model;algorithmics	ML	-21.18632959544809	-86.52757391201477	41072
970fa2c73195734d39f6313aa0e2ff0ae10169cb	a neural network keyword search system for telephone speech		In this paper we propose a pure “neural network” (NN) based keyword search system developed in the IARPA Babel program for conversational telephone speech. Using a common keyword search evaluation metric, “actual term weighted value” (ATWV), we demonstrate that our NN-keyword search system can achieve a performance similar to a comparible but more complex and slower “hybrid deep neural network hidden markov model” (DNN-HMM Hybrid) based speech recognition system without using either an HMM decoder or a language model.	artificial neural network;deep learning;hidden markov model;language model;markov chain;search algorithm;speech recognition	Kevin Kilgour;Alexander H. Waibel	2014		10.1007/978-3-319-11581-8_7	natural language processing;speech recognition;computer science;communication	NLP	-18.907502289661828	-87.51734544419743	41226
a7ff14e1d8318b6052aa1abe5e6431d53383dd1a	new method for delexicalization and its application to prosodic tagging for text-to-speech synthesis	text to speech synthesis;speech synthesis;voice quality;delexicalization;prosody	This paper describes a new flexible delexicalization method based on glottal excited parametric speech synthesis scheme. The system utilizes inverse filtered glottal flow and all-pole modelling of the vocal tract. The method provides a possibility to retain and manipulate all relevant prosodic features of any kind of speech. Most importantly, the features include voice quality, which has not been properly modeled in earlier delexicalization methods. The functionality of the new method was tested in a prosodic tagging experiment aimed at providing word prominence data for a text-to-speech synthesis system. The experiment confirmed the usefulness of the method and further corroborated earlier evidence that linguistic factors influence the perception of prosodic prominence.	speech synthesis;tract (literature)	Martti Vainio;Antti Suni;Tuomo Raitio;Jani Nurminen;Juhani Järvikivi;Paavo Alku	2009			natural language processing;speech recognition;phonation;speech corpus;computer science;linguistics;prosody;speech synthesis	NLP	-10.188723848991438	-84.99488330084664	41304
6ffe82b820ea45ce627fd5a4ec53fb7a6c7b9782	building a vocabulary self-learning speech recognition system		This paper presents initial studies on building a vocabulary selflearning speech recognition system that can automatically learn unknown words and expand its recognition vocabulary. Our recognizer can detect and recover out-of-vocabulary (OOV) words in speech, then incorporate OOV words into its lexicon and language model (LM). As a result, these unknown words can be correctly recognized when encountered by the recognizer in future. Specifically, we apply the word-fragment hybrid system framework to detect the presence of OOV words. We propose a better phoneme-to-grapheme (P2G) model so as to correctly recover the written form for more OOV words. Furthermore, we estimate LM scores for OOV words using their syntactic and semantic properties. The experimental results show that more than 40% OOV words are successfully learned from the development data, and about 60% learned OOV words are recognized in the testing data.	finite-state machine;hybrid system;language model;lexicon;speech recognition;vocabulary	Long Qin;Alexander I. Rudnicky	2014			artificial intelligence;speech recognition;semantic property;hybrid system;pattern recognition;syntax;computer science;language model;test data;vocabulary;lexicon	NLP	-20.172522884612622	-84.84105554292309	41531
afe1698e4008dff6a3feb104ddb8b4d5dee66a61	controlled active procedures as a tool for linguistic engineering	linguistic engineering;semantic representation;rule writing;elementary problem;parsing problem;controlled active procedure;random nl text;cap grammar formalism;parsing strategy;rule application;problem reduction method;natural language;data structure	Controlled active procedures are productions that are grouped under and activated by units called 'scouts'. Scouts are controlled by units called 'missions', which also select relevant sections from the data structure for rule application. Following the problem reduction method, the parsing problem is subdivided into ever smaller subproblems, each one of which is represented by a mission. The elementary problems are represented by scouts. The CAP grammar formalism is based on experience gained with natural language (NL) analysis and translation by computer in the Sonderforschungsbereich I00 at the University of Saarbrdcken over the past twelve years and dictated by the wish to develop an efficient parser for random NL texts on a sound theoretical basis. The idea has ripened in discussions with colleagues from the EUROTRA-project and is based on what Heinz-Dieter Maas has developed in the framework of the SUSYII system. In the present paper, CAP is introduced as a means of linguistic engineering (cf. Simmons 1985), which covers aspects like rule writing, parsing strategies, syntactic and semantic representation of meaning, representation of lexical knowledge etc. Survey of some ideas behind CAP The data structure used in CAP is a type of chart called S-graph (see Maas 1985). Charts are used in parsing quite frequently (cf. Kay 1977, Varile 1983). The S-graph is an acyclic directed graph with exactly one start node and one end node. Each arc carries non-structural information and may carry structural information that is also represented as an S-graph. The non-structural information is a set of property/value-pairs called 'decoration'. It includes a) a morDhosvntactic t_vpe (MS), i.e. the terminal o r non-terminal category b) a surface-syntactic function (SF) c) a~]9_e~nt~ctic function (DSF) d) a semantic relation (SR) e) a W~i.C~ f) information specific to an MS The structure of the complex NP 'trouble with Max' is visible to the user as Fig. i.	categorial grammar;chart;data structure;directed acyclic graph;directed graph;elementary;emoticon;eurotra;formal grammar;nl (complexity);name mangling;natural language;ontology components;parsing;s-graph;semantics (computer science)	Heinz-Dirk Luckhardt;Manfred Thiel	1986			natural language processing;data structure;computer science;artificial intelligence;linguistics;natural language;algorithm	AI	-30.517175215951518	-82.42218358543661	41593
1f70ebdc1bb8c728184b45a36dae6d3b59f5f9c9	combining speaker identification and bic for speaker diarization	speaker diarization	This paper describes recent advances in speaker diarization by incorporating a speaker identification step. This system builds upon the LIMSI baseline data partitioner used in the broadcast news transcription system. This partitioner provides a high cluster purity but has a tendency to split the data from a speaker into several clusters, when there is a large quantity of data for the speaker. Several improvements to the baseline system have been made. Firstly, a standard Bayesian information criterion (BIC) agglomerative clustering has been integrated replacing the iterative Gaussian mixture model (GMM) clustering. Then a second clustering stage has been added, using a speaker identification method with MAP adapted GMM. A final post-processing stage refines the segment boundaries using the output of the transcription system. On the RT-04f and ESTER evaluation data, the improved multi-stage system provides between 40% and 50% reduction of the speaker error, relative to a standard BIC clustering system.	baseline (configuration management);bayesian information criterion;cluster analysis;google map maker;iterative method;mixture model;pure function;speaker diarisation;speaker recognition;transcription (software);video post-processing	Xuan Zhu;Claude Barras;Sylvain Meignier;Jean-Luc Gauvain	2005			artificial intelligence;speech recognition;mixture model;cluster analysis;speaker diarisation;pattern recognition;computer science;hierarchical clustering;bayesian information criterion	ML	-17.78913228713246	-93.12009995747563	41616
feb594f9f4c1365fe8d20d727c7e8b27fa8d63b1	a writer identification system for on-line whiteboard data	traitement signal;processus gauss;on line handwriting;learning;caracter manuscrito;reconnaissance scripteur;manuscript character;arriere plan;writer recognition;writer identification;aprendizaje;apprentissage;background;gaussian mixture model;feature extraction;gaussian mixture models;signal processing;teoria mezcla;difference set;universal background model;gaussian process;extraction caracteristique;proceso gauss;mixture theory;procesamiento senal;caractere manuscrit;theorie melange	In this paper we address the task of writer identification of on-line handwriting captured from a whiteboard. Different sets of features are extracted from the recorded data and used to train a text and language independent on-line writer identification system. The system is based on Gaussian mixture models (GMMs) which provide a powerful yet simple means of representing the distribution of the features extracted from the handwritten text. The training data of all writers are used to train a universal background model (UBM) from which a client specific model is obtained by adaptation. Different sets of features are described and evaluated in this work. The system is tested using text from 200 different writers. A writer identification rate of 98.56% on the paragraph and of 88.96% on the text line level is achieved.	online and offline	Andreas Schlapbach;Marcus Liwicki;Horst Bunke	2008	Pattern Recognition	10.1016/j.patcog.2008.01.006	speech recognition;computer science;artificial intelligence;machine learning;signal processing;pattern recognition;mixture model;mathematics;statistics	Vision	-21.57547351586562	-90.93587987140506	41731
07f83309f8a42952a94747d297a6baaaf8a7123a	training of on-line handwriting text recognizers with synthetic text generated using the kinematic theory of rapid human movements	hidden markov models handwriting recognition;on line handwriting text recognizers hidden markov model sigma lognormal parameters sigma lognormal model synthetic handwritten words automatic generation rapid human movements kinematic theory synthetic text;comunicacion en congreso;sigma lognormal model on line handwritten text recognition synthetic generation kinematic theory;hidden markov models kinematics training adaptation models signal to noise ratio trajectory handwriting recognition	A method for automatic generation of synthetic handwritten words is presented which is based in the Kinematic Theory and its Sigma-lognormal model. To generate a new synthetic sample, first a real word is modelled using the Sigma-lognormal model. Then the Sigma-lognormal parameters are randomly perturbed within a range, introducing human-like variations in the sample. Finally, the velocity function is recalculated taking into account the new parameters. The synthetic words are then used as training data for a Hidden Markov Model based on-line handwritten recognizer. The experimental results confirm the great potential of the kinematic theory of rapid human movements applied to writer adaptation.	finite-state machine;hidden markov model;markov chain;online and offline;randomness;synthetic data;synthetic intelligence;velocity (software development)	Daniel Martín-Albo;Réjean Plamondon;Enrique Vidal	2014	2014 14th International Conference on Frontiers in Handwriting Recognition	10.1109/ICFHR.2014.97	speech recognition;computer science;machine learning;pattern recognition	Robotics	-15.861581764816334	-83.15962132960277	41744
2ba8eff07b978574afa4d36c8759652b45ce378b	a maximum a posterior-based reconstruction approach to speech bandwidth expansion in noise	speech narrowband wideband noise vectors hidden markov models;speech processing maximum likelihood estimation signal reconstruction signal representation;maximum a posterior based reconstruction approach babble noise car noise noise corrupted narrowband speech clean narrowband speech vts noise adaptation algorithm vector taylor series narrowband input signals map criterion matching process narrowband input sentence narrowband segments speech signal representation wideband representation narrowband representation speaker independent database narrowband speech signal speech bandwidth expansion algorithm;vector taylor series speech bandwidth expansion maximum a posterior corpus model noise reduction	We propose a novel bandwidth expansion algorithm for extending narrowband speech signal to wideband by exploiting segment examples pre-stored in a speaker independent database. Both narrowband and wideband representation of speech signals are pre-stored in the corpus and they are dynamically chopped into variable length segments. Narrowband segments are used dynamically to explain a given narrowband input sentence while the wideband expanded version of the input sentence is constructed correspondingly. The matching process in the narrowband favors a longer segment patch by the chosen Maximum A Posterior (MAP) criterion. As a result, the multiple choices in matching process are significantly reduced with the MAP criterion in decoding. The approach is further generalized to deal with noise corrupted narrowband input signals and the well-known Vector Taylor Series (VTS) noise adaptation algorithm is incorporated into the matching and bandwidth expansion process. A series of experiments is performed to validate the approach on both clean and noise corrupted narrowband speech where both car noise and babble noise corrupted samples are tested.	algorithm;bandwidth expansion;experiment;signal-to-noise ratio;text corpus;vehicle tracking system;whole earth 'lectronic link	Hyunson Seo;Hong-Goo Kang;Frank K. Soong	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854773	speech recognition;speech processing;mathematics	Robotics	-16.200642083711177	-93.42293555131151	41792
69a31abbf434b6e31cfebbb999d695ab11d07c19	speech emotion recognition using deep neural network and extreme learning machine		Speech emotion recognition is a challenging problem partly because it is unclear what features are effective for the task. In this paper we propose to utilize deep neural networks (DNNs) to extract high level features from raw data and show that they are effective for speech emotion recognition. We first produce an emotion state probability distribution for each speech segment using DNNs. We then construct utterance-level features from segment-level probability distributions. These utterancelevel features are then fed into an extreme learning machine (ELM), a special simple and efficient single-hidden-layer neural network, to identify utterance-level emotions. The experimental results demonstrate that the proposed approach effectively learns emotional information from low-level features and leads to 20% relative accuracy improvement compared to the stateof-the-art approaches.	artificial neural network;deep learning;emotion recognition;high- and low-level;high-level programming language;speech recognition	Kun Han;Dong Yu;Ivan Tashev	2014			speech recognition;machine learning;time delay neural network	AI	-16.817523603648173	-87.9562338778538	41829
2ecf52e418c477be9c0a96c5536158a830e35d51	automatic data-driven frequency-warped cepstral feature design for micro-doppler classification		Micro-Doppler signature analysis and speech processing share a common approach as both rely on the extraction of features from the signal's time-frequency distribution for classification. As a result, features, such as the mel-frequency cepstrum coefficients (MFCCs), which have shown success in speech processing, have been proposed for use in micro-Doppler classification. MFCCs were originally designed to take into account the auditory properties of the human ear by filtering the signal using a filter bank spaced according to the mel-frequency scale. However, the physics underlying radar micro-Doppler is unrelated to that of human hearing or speech. This work shows that the mel-scale filter bank results in the loss of frequency components significant to the classification of radar micro-Doppler. A novel method for frequency-warped cepstral feature design is proposed as a means for optimizing the efficacy of features in a data-driven fashion specifically for micro-Doppler analysis. It is shown that the performance of the proposed frequency warped cepstral coefficients outperforms MFCC based on both simulated and measured data sets for four-class and eight-class human activity classification problems.	activity recognition;coefficient;computation;electronic signature;filter bank;mathematical optimization;mel scale;mel-frequency cepstrum;ps-algol;radar;software release life cycle;speech processing;time complexity;time–frequency representation;type signature	Baris Erol;Moeness G. Amin;Sevgi Zubeyde Gurbuz	2018	IEEE Transactions on Aerospace and Electronic Systems	10.1109/TAES.2018.2801378	mathematics;filter (signal processing);electronic engineering;feature extraction;radar;filter bank;mel-frequency cepstrum;cepstrum;doppler radar;speech processing;pattern recognition;artificial intelligence	ML	-10.341711944447427	-90.38247434736944	41836
5589a65d6e96dba79d01966419cf88ca5aa65d44	hmm/ann based spectral peak location estimation for noise robust speech recognition	neural nets;location estimation;time frequency;time frequency analysis spectral analysis signal representation speech recognition hidden markov models neural nets;speech;spectrum;estimation algorithm;noise robustness;activity pattern;temporal constraints;hidden markov models;ikbal;hidden markov models noise robustness speech recognition acoustic noise humans frequency estimation time frequency analysis spectrogram yield estimation autocorrelation;signal representation;bourlard;speech recognition;mathew;pac speech signal spectral representation spectral peak location estimation noise robust speech recognition hmm ann spectrogram time frequency patterns peak estimation temporal constraints ergodic topology spectro temporal activity pattern feature stap phase autocorrelation spectrum;multi layer perceptron;spectral analysis;time frequency analysis	In this paper, we present an HMM/ANN based algorithm to estimate the spectral peak locations. This algorithm makes use of distinct time-frequency (TF) patterns in the spectrogram for estimating the peak locations. Such a use of TF patterns is expected to impose temporal constraints during the peak estimation task, thereby yielding a smoother estimate of the peaks over time. Additionally, the algorithm uses an ergodic topology for the HMM/ANN, thus allowing an estimation of a varying number of peak locations over time. The usefulness of the proposed algorithm is evaluated in the framework of a recently introduced noise robust feature called the spectro-temporal activity pattern (STAP) feature. Interestingly, the recently introduced phase autocorrelation (PAC) spectrum, with enhanced spectral peaks and smoothed spectral valleys, turns out to be more appropriate for this algorithm than the regular spectrum.	algorithm;autocorrelation;ergodic hypothesis;ergodicity;hidden markov model;smoothing;space-time adaptive processing;spectrogram;speech recognition	Shajith Ikbal;Hervé Bourlard;Mathew Magimai-Doss	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1415148	speech recognition;time–frequency analysis;computer science;machine learning;pattern recognition;artificial neural network;hidden markov model	Vision	-12.216924051884737	-91.6342449257664	41838
9a7f05b68376b2fc369c541b6e5ad3a6607b61cc	modeling spoken dialog systems under the interactive pattern recognition framework	dialog system;new formulation;ipr problem;interactive pattern recognition framework;new interactive pattern recognition;pattern recognition system;proposed formulation;new dialog;dialog manager action;spanish dialog task;ipr paradigm;ipr approach	dialog system;new formulation;ipr problem;interactive pattern recognition framework;new interactive pattern recognition;pattern recognition system;proposed formulation;new dialog;dialog manager action;spanish dialog task;ipr paradigm;ipr approach	dialog system;pattern recognition;spoken dialog systems	M. Inés Torres;José-Miguel Benedí;Raquel Justo;Fabrizio Ghigi	2012		10.1007/978-3-642-34166-3_57	computer science;artificial intelligence;data mining;dialog system;communication	Vision	-27.251100678079645	-85.67340146108627	41865
bd887069c459decfc3fa207c88895cf713684e74	training the tilt intonation model using the jema methodology	gradient descent;closed form solution;fundamental frequency	This paper focuses on the estimation of the Tilt intonation model [1]. Usually, Tilt events are detected using a first estimation which is improved using gradient descent techniques. To speed up the search we propose to use a closed form expression for some of the Tilt parameters. The gradient descent search is used only for the time related parameters because a close expression cannot be found. Furthermore, the original Tilt proposal estimates the Tilt events sentence by sentence. Here we propose to estimate the events of the whole training corpus at the same time, using what we call the JEMA methodology. This approach increases the consistency of the estimation producing better intonation models. It has been tested on two different languages: Slovenian and Spanish. The experimental results reveal that the Tilt model is appropriate for these languages and that the JEMA methodology produces better prosodic models.	gradient descent	Matej Rojc;Pablo Daniel Agüero;Antonio Bonafonte;Zdravko Kacic	2005			speech recognition;artificial intelligence;speedup;computer science;pattern recognition;fundamental frequency;gradient descent;closed-form expression;sentence	NLP	-18.61585030143483	-84.85605795764046	41924
1c21b0f568a725d0aa247fc3b65c97b6f245332b	acoustic and kinematic characteristics of vowel production through a virtual vocal tract in dysarthria	dysarthria;sensorimotor learning;auditory feedback;articulatory resynthesis	Broadening our understanding of the components and processes of speech sensorimotor learning is crucial to furthering methods of speech neurorehabilitation. Recent research in limb sensorimotor control has used virtual environments to study learning in novel sensorimotor working spaces. Comparable experimental paradigms have yet to be undertaken to study speech learning. We present acoustic and kinematic data obtained from participants producing vowels in unfamiliar articulatory-acoustic working spaces using a virtual vocal tract. Talkers with dysarthria and healthy controls were asked to produce vowels using an electromagnetic articulograph-driven speech synthesizer for participantcontrolled auditory feedback. The aim of the work was to characterize performance within and between groups to generate hypotheses regarding experimental manipulations that may bolster our understanding of speech sensorimotor learning. Results indicate that dysarthric talkers displayed relatively reduced acoustic working spaces and somewhat more variable acoustic targets compared to controls. Kinematic measures of articulatory dynamics, particularly peak speed and movement jerk-cost, were idiosyncratic and did not dissociate talker groups. These findings suggest that individuals with dysarthria and healthy talkers may use idiosyncratic movement strategies in learning to control a virtual vocal tract, but that dysarthric talkers may nonetheless exhibit acoustic limitations that parallel deficits in speech intelligibility.	acoustic cryptanalysis;intelligibility (philosophy);speech synthesis;tract (literature);virtual reality	Jeffrey Berry;Andrew Kolb;Cassandra North;Michael T. Johnson	2014			speech recognition	HCI	-8.504400594077111	-82.03169820002248	41980
72e9655a1c0426b5de557baa7958a457b8b5e122	svm classification using sequences of phonemes and syllables	phoneme;syllabe;text;analisis datos;reconocimiento palabra;communication ecrite;chaine caractere;fonema;phonem;intelligence artificielle;comunicacion escrita;texte;classification;data analysis;syllable;machine exemple support;cadena caracter;speech recognition;artificial intelligence;analyse donnee;written communication;inteligencia artificial;reconnaissance parole;document classification;vector support machine;classification accuracy;silaba;texto;clasificacion;character string	In this paper we use SVMs to classify spoken and written documents. We show that classification accuracy for written material is improved by the utilization of strings of sub-word units with dramatic gains for small topic categories. The classification of spoken documents for large categories using sub-word units is only slightly worse than for written material, with a larger drop for small topicc ategories. Finally it is possible, without loss, to train SVMs on syllables generated from written material and use them to classify audio documents. Our results confirm the strong promise that SVMs hold for robust audio document classification, and suggest that SVMs can compensate for speech recognition error to an extent that allows a significant degree of topic independence to be introduced into the system.		Gerhard Paass;Edda Leopold;Martha Larson;Jörg Kindermann;Stefan Eickeler	2002		10.1007/3-540-45681-3_31	natural language processing;speech recognition;string;biological classification;computer science;artificial intelligence;machine learning;pattern recognition;data analysis;syllable;writing	Vision	-20.796344414375426	-81.20875635922344	42087
731a3fb639af77dae062989a411cf32821a9d7fe	exploring auditory network composition during free listening to audio excerpts via group-wise sparse representation	functional magnetic resonance imaging;brain;highlevel features;signal representation acoustic signal processing audio signal processing biomedical mri brain music;acoustics;low level feature;semantics;matrix decomposition;feature extraction;audio content analysis auditory network composition audio excerpts group wise sparse representation media channels distribution channels advanced audio analysis approaches multimedia field high level semantics low level acoustic features functional magnetic resonance imaging music speech excerpts whole brain fmri signal representation brain auditory networks;semantic gap;dictionaries;functional magnetic resonance imaging semantic gap low level feature highlevel features;semantics acoustics feature extraction dictionaries brain sparse matrices matrix decomposition;sparse matrices	With the growing number of audio excerpts through various media and distribution channels, advanced audio analysis approaches have received significant interest in the multimedia field. However, current audio analysis approaches are still far from satisfactory due to the semantic gaps between the low-level acoustic features and high-level semantics perceived by human brain. In order to alleviate the problem, this paper propose a novel computational framework to bridge acoustic features with high-level semantic features derived from functional magnetic resonance imaging (fMRI) signals which record the brain's response during free listening to music/speech excerpts, and to explore the brain auditory network composition of acoustic features for different types of music/speech excerpts. Specifically, we identify meaningful brain networks and corresponding brain activities representing high-level semantic features via a novel group-wise sparse representation of whole brain fMRI signals. Then we associate the brain activities with specific low-level acoustic features and analyze the auditory network composition of acoustic features for different types of music/speech excerpts. Experimental results demonstrate that multiple acoustic features are involved in the brain auditory networks during free listening to music/speech excerpts. Meanwhile, there is considerable variability of auditory network composition of acoustic features for different types of music/speech. Our results provide new insights of how to narrow the semantic gaps in audio content analysis.	acoustic cryptanalysis;computation;electroencephalography;han unification;high- and low-level;ibm notes;resonance;sparse approximation;sparse matrix;spatial variability	Shijie Zhao;Junwei Han;Xi Jiang;Xintao Hu;Jinglei Lv;Shu Zhang;Bao Ge;Lei Guo;Tianming Liu	2016	2016 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2016.7552952	natural language processing;speech recognition;sparse matrix;feature extraction;computer science;semantics;matrix decomposition;algorithm;semantic gap	Robotics	-5.326581748921966	-88.02279424298152	42181
197a8110016e4eea6045a6adbb409e8f1a7bc278	on lexicon creation for turkish lvcsr	word error rate;out of vocabulary;speech recognition;high frequency	In this paper, we address the lexicon design problem in Turkish large vocabulary speech recognition. Although we focus only on Turkish, the methods described here are general enough that they can be considered for other agglutinative languages like Finnish, Korean etc. In an agglutinative language, several words can be created from a single root word using a rich collection of morphological rules. So, a virtually infinite size lexicon is required to cover the language if words are used as the basic units. The standard approach to this problem is to discover a number of primitive units so that a large set of words can be created by compounding those units. Two broad classes of methods are available for splitting words into their sub-units; morphology-based and data-driven methods. Although the word splitting significantly reduces the out of vocabulary rate, it shrinks the context and increases acoustic confusibility. We have used two methods to address the latter. In one method, we use word counts to avoid splitting of high frequency lexical units, and in the other method, we recompound splits according to a probabilistic measure. We present experimental results that show the methods are very effective to lower the word error rate at the expense of lexicon size.	acoustic cryptanalysis;acoustic model;galaxy morphological classification;lexicon;morphological parsing;speech analytics;speech recognition;text segmentation;vocabulary;word error rate	Kadri Hacioglu;Bryan L. Pellom;Tolga Çiloglu;Özlem Öztürk;Mikko Kurimo;Mathias Creutz	2003			natural language processing;cohort model;speech recognition;word error rate;computer science;high frequency;linguistics	NLP	-20.24881714090621	-82.67078472569902	42190
9c665958d5c34096763ab03d5bdf57f923d4fc9b	cochlear implant filterbank design and optimization: a simulation study	engineering;speech intelligibility;electrical electronic;technology;acoustics;speech;filter transfer functions;recognition;science technology;stimulation;channel bank filters;vocoders;vocoders channel bank filters cochlear implants q factor;number;optimal combination cochlear implant filterbank design hearing function profoundly deaf patients essential processing step frequency analysis filterbank architectures filter test speech intelligibility ci hearing tone excited vocoder gammatone filter dapgf differentiated all pole gtf ozgf one zero gtf butf filter butterworth filterbank parameters filter order filter quality factor channel number;vocoders cochlear implants filter transfer functions speech;channels;models;speech band pass filters measurement silicon auditory system speech processing optimization;cochlear implants;q factor;noise	Cochlear implants (CIs) are devices capable of restoring hearing function in profoundly-deaf patients to an acceptable degree of performance. An essential processing step in any cochlear implant is frequency analysis, which is usually performed via banks of filters. Here, we simulate and test the suitability of different filters and filterbank architectures for CIs with respect to their performance in speech intelligibility. Four different filters were implemented in an established model of CI hearing, the tone-excited vocoder, namely: GTF (Gammatone Filter), DAPGF (Differentiated All-Pole GTF), OZGF (One-Zero GTF) and BUTF (Butterworth). Three filterbank parameters, the filter order ( N), the filter quality factor ( Q) and the number of channels ( Ch), and their combinations were tested using objective and subjective metrics. Simulation results show that all filters tested are suitable for CI implementation, but that the choice of Q and N parameter values is crucial. For most conditions, optimal ( N,Q) combinations were within few units away from the combination (2, 4).	butterworth filter;cochlear implant;filter bank;frequency analysis;generalized timing formula;intelligibility (philosophy);simulation;vocoder	Stefano Cosentino;Tiago H. Falk;David McAlpine;Torsten Marquardt	2014	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2013.2290502	speech recognition;acoustics;stimulation;noise;speech;linguistics;q factor;technology	ML	-8.859257894151517	-87.79980859957082	42325
42281b3d4d797a59c86bc8b2a1e00b1acbb3c586	the lectra corpus - classroom lecture transcriptions in european portuguese		This paper presents the recent extension of the LECTRA corpus, a speech corpus of university lectures in European Portuguese that will be partially available. Eleven additional hours of various lectures were transcribed, following the previous multilayer annotations, and now comprising about 32 hours. This material can be used not only for the production of multimedia lecture contents for e-learning applications, enabling hearing impaired students to have access to recorded lectures, but also for linguistic and speech processing studies. Lectures present challenges for automatic speech recognition (ASR) engines due to their idiosyncratic nature as spontaneous speech and their specific jargon. The paper presents recent ASR experiments that have clearly shown performance improvements on this domain. Together with the manual transcripts, a set of upgraded and enriched force-aligned transcripts was also produced. Such transcripts constitute an important advantage for corpora analysis, and for studying several speech tasks.	automated system recovery;experiment;jargon;java annotation;list of cax companies;speech corpus;speech processing;speech recognition;spontaneous order;text corpus	Isabel Trancoso;Rui Martins;Helena Moniz;Ana Isabel Mata;Céu Viana	2008			scarcity;natural language processing;speech recognition;speech characteristics;artificial intelligence;language model;transcription (linguistics);acoustic model;european portuguese;computer science;linguistics;portuguese	NLP	-22.705134170267037	-84.30388319123256	42339
352ed137a8c6362c33d8afedcd6616bf0c21ed73	informed source separation: source coding meets source separation	quantization;probabilistic model informed source separation source coding constrained entropy quantization;decoding;blind source separation;source coding source separation probabilistic logic quantization acoustics;acoustics;informed source separation;probabilistic model;source coding blind source separation decoding;probabilistic logic;source separation;constrained entropy quantization;source coding;probabilistic model based source separation informed source separation source coding decoding source separation inspired strategies coding based iss	We consider the informed source separation (ISS) problem where, given the sources and the mixtures, any kind of side-information can be computed during a so-called encoding stage. This side-information is then used to assist source separation, given the mixtures only, at the so-called decoding stage. State of the art ISS approaches do not really consider ISS as a coding problem and rely on some purely source separation-inspired strategies, leading to performances that can at best reach those of oracle estimators. On the other hand, classical source coding strategies are not optimal either, since they do not benefit from the mixture availability. We introduce a general probabilistic framework called coding-based ISS (CISS) that consists in quantizing the sources using some posterior source distribution from those usually used in probabilistic model-based source separation. CISS benefits from both source coding, thanks to the source quantization, and source separation, thanks to the use of the posterior distribution that depends on the mixture. Our experiments show that CISS based on a particular model considerably outperforms for all rates both the conventional ISS approach and the source coding approach based on the same model.	data compression;experiment;performance;source separation;statistical model	Alexey Ozerov;Antoine Liutkus;Roland Badeau;Gaël Richard	2011	2011 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)	10.1109/ASPAA.2011.6082285	statistical model;speech recognition;acoustics;quantization;variable-length code;computer science;machine learning;blind signal separation;linguistics;probabilistic logic;statistics;source code	Vision	-21.317970291807825	-94.1456586979354	42364
b1431331b8c798aade4e13613348204c2df877be	designing the model human cochlea: an ambient crossmodal audio-tactile display	auditory information;sensory aids;user interface;sensory aids human factors user interfaces models and principles music;biological system modeling;auditory system;form factor;development process;music auditory displays ear haptic interfaces human factors;auditory displays;discrete vibration signal;visualization;vibrotactile stimuli;multiple signal classification;sensory substitution model;ear;human factors;coils;tactile display;deafness;multiple discrete vibrotactile channel;vibration measurement;voice coil;deaf;humans;chair form factor;model human cochlea;humans auditory displays deafness music auditory system vibration measurement biological system modeling multiple signal classification visualization coils;haptic interfaces;sensory substitution;user interfaces;music;ambient crossmodal audio tactile display;models and principles;sensory substitution model model human cochlea ambient crossmodal audio tactile display sensory substitution technique auditory information vibrotactile stimuli music discrete vibration signal chair form factor voice coil multiple discrete vibrotactile channel deaf;sensory substitution technique	We present a model human cochlea (MHC), a sensory substitution technique and system that translates auditory information into vibrotactile stimuli using an ambient, tactile display. The model is used in the current study to translate music into discrete vibration signals displayed along the back of the body using a chair form factor. Voice coils facilitate the direct translation of auditory information onto the multiple discrete vibrotactile channels, which increases the potential to identify sections of the music that would otherwise be masked by the combined signal. One of the central goals of this work has been to improve accessibility to the emotional information expressed in music for users who are deaf or hard of hearing. To this end, we present our prototype of the MHC, two models of sensory substitution to support the translation of existing and new music, and some of the design challenges encountered throughout the development process. Results of a series of experiments conducted to assess the effectiveness of the MHC are discussed, followed by an overview of future directions for this research.	accessibility;align (company);audio media;authorization;bands;biometric device;biometrics;blinded;cardiomyopathies;cochlear structure;computer form factor;content-control software;dimensions;emoticon;excerpts;experiment;fm broadcasting;fetal measurements domain;frequency band;hearing impaired persons;human body weight;ieee xplore;kilohertz;large;less than;model of hierarchical complexity;multimodal interaction;parsing;pitch (music);prototype;recliner;sensory substitution;vibration - physical agent;vocoder	Maria Karam;Frank A. Russo;Deborah I. Fels	2009	IEEE Transactions on Haptics	10.1109/TOH.2009.32	speech recognition;acoustics;computer science;engineering;human factors and ergonomics;operating system;user interface	HCI	-6.287603013079043	-83.6079398425476	42411
122f5cb50f0d583f2ab1ba795b62a5492e4afe2f	analysis and evaluation of discriminant analysis techniques for multiclass classification of human vocal emotions		Many of the classification problems in human computer interaction applications involve multi class classification. Support Vector Machines excel at binary classification problems and cannot be easily extended to multi class classification. The use of Discriminant analysis how ever is not experimented widely in the area of Speech emotion recognition. In this paper Linear Discriminant Analysis and Regularized Discriminant Analysis are implemented over Berlin and Spanish emotional speech databases. Prosody and spectral features are extracted from the speech database and are applied individually and also with feature fusion. Based on the results obtained, LDA classification performance is poor than RDA due to the singularity problem. The results are analysed using ROC Curves.	linear discriminant analysis;multiclass classification	Swarna Kuchibhotla;H. D. Vankayalapati;B. S. Yalamanchili;Koteswara Rao Anne	2014		10.1007/978-3-319-11218-3_30	speech recognition;pattern recognition	ML	-12.271332725276654	-87.97648785741187	42441
492089fd2a44a2dc5eda43d4fd033674b16ec752	bayesian on-line spectral change point detection: a soft computing approach for on-line asr	mcra;jac compensation;on line environment learning;bayesian on line inference for spectral change point detection;smart phones and mobile hand held devices;non stationary noise tracking and estimate;on line asr;highly non stationary unknown test conditions;frame dynamic;real world application;dsr;minimum search window;boscpd	Current automatic speech recognition (ASR) works in off-line mode and needs prior knowledge of the stationary or quasi-stationary test conditions for expected word recognition accuracy. These requirements limit the application of ASR for real-world applications where test conditions are highly non-stationary and are not known a priori. This paper presents an innovative frame dynamic rapid adaptation and noise compensation technique for tracking highly non-stationary noises and its application for on-line ASR. The proposed algorithm is based on a soft computing model using Bayesian on-line inference for spectral change point detection (BOSCPD) in unknown non-stationary noises. BOSCPD is tested with the MCRA noise tracking technique for on-line rapid environmental change learning in different non-stationary noise scenarios. The test results show that the proposed BOSCPD technique reduces the delay in spectral change point detection significantly compared to the baseline MCRA and its derivatives. The proposed BOSCPD soft computing model is tested for joint additive and channel distortions compensation (JAC)-based on-line ASR in unknown test conditions using non-stationary noisy speech samples from the Aurora 2 speech database. The simulation results for the on-line AR show significant improvement in recognition accuracy compared to the baseline Aurora 2 distributed speech recognition (DSR) in batch-mode.	automatic system recovery;online and offline;soft computing	Md Foezur Rahman Chowdhury;Sid-Ahmed Selouani;Douglas D. O'Shaughnessy	2012	I. J. Speech Technology	10.1007/s10772-011-9116-2	simulation;speech recognition	NLP	-13.45013937472698	-93.7876494839394	42619
63e5c38b6f3cad673c29d2998779a7bc31b3151a	single-channel speech separation based on modulation frequency	computational auditory scene analysis;speaker identification;modulation frequency;frequency modulation;cross correlation;modulation frequency speech analysis speech recognition time frequency analysis;speech processing;speech analysis;resource manager;instantaneous frequency;component cluster;correlation methods;indexing terms;speech processing correlation methods frequency modulation signal reconstruction source separation speaker recognition;speaker recognition;grid database;single channel;frequency modulation speech analysis speech enhancement speech recognition image analysis phase modulation speech coding target tracking amplitude modulation signal processing;speech recognition;signal reconstruction;darpa resource management database single channel speech signal separation frequency modulation computational auditory scene analysis speaker identification component cluster speech recognition grid database;source separation;time frequency analysis;fundamental frequency;darpa resource management database;single channel speech signal separation	This paper describes an algorithm that performs a simple form of computational auditory scene analysis to separate multiple speech signals from one another on the basis of the modulation frequencies of the components. The most novel aspect of the algorithm is the use of the cross-correlation of the instantaneous frequencies of the components of a signal to identify and separate those components that are likely have been produced by a common sound source. The putative desired target speech signal is reconstructed by choosing those components that have the greatest mutual correlation, and then using extrinsic information such as fundamental frequency or speaker identification to determine which component clusters belong to which speaker. The system was evaluated by comparing speech recognition accuracy of a target speech signal that was extracted from a mixture of two speakers. It was found that recognition accuracy obtained when the separation was based on cross-correlation of changes in instantaneous frequency was better than the accuracy obtained when the separation was performed on the basis of fundamental frequency alone, for both the DARPA Resource Management Database and the Grid database used in the 2006 Speech Separation Challenge.	algorithm;computation;computational auditory scene analysis;covox speech thing;cross-correlation;instantaneous phase;modulation;speaker recognition;speech recognition	Lingyun Gu;Richard M. Stern	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4517537	frequency modulation;speaker recognition;speech recognition;computer science;speech processing	Robotics	-11.561072988009409	-92.07419571766248	42695
831f6218b21b4c4f2a512ead7aae04bcd094bffe	optimum reference construction and updating for speaker recognition systems.	speaker recognition			Nikos Fakotakis;Evangelos Dermatas;George K. Kokkinakis	1987			natural language processing;speaker recognition;speaker diarisation;speech recognition;computer science;pattern recognition	Theory	-14.844286571984384	-87.46093020497398	42711
78090327bcb4d622ae168723f2e95d226b110372	combining hmm and weighted deviation linear transformation for highband speech parameter estimation	gaussian mixture model;hidden markov model;parameter estimation;linear transformation	A hidden Markov model (HMM)-based parameter estimation scheme is proposed for wideband speech recovery. In each Markov state, the estimation efficiency is improved using a new mapping function derived from the weighted least squares of vector deviations. The experimental results reveal that the performance of the proposed scheme is superior to that combining the HMM and Gaussian mixture model (GMM).	estimation theory;hidden markov model	Hwai-Tsu Hu;Chu Yu	2009	IEICE Transactions		econometrics;computer science;machine learning;pattern recognition;mixture model;gaussian process;weighting;mathematics;linear map;estimation theory;least squares;hidden markov model;statistics	Vision	-14.595208927991505	-93.98553315066094	42729
9b364d55a74e2161c384f6defc748007dd3322d4	active selection with label propagation for minimizing human effort in speaker annotation of tv shows		In this paper an approach minimizing the human involvement in the manual annotation of speakers is presented. At each iteration a selection strategy choses the most suitable speech track for manual annotation, which is then associated with all the tracks in the cluster that contains it. The study makes use of a system that propagates the speaker track labels. This is done using a agglomerative clustering with constraints. Several different unsupervised active learning selection strategies are evaluated. Additionally, the presented approach can be used to efficiently generate sets of speech tracks for training biometric models. In this case both the length of the speech track for a given person and its purity are taken into consideration. To evaluate the system the REPERE video corpus was used. Along with the speech tracks extracted from the videos, the optical character recognition system was adapted to extract names of potential speakers. This was then used as the ’cold start’ for the selection method.	biometrics;cluster analysis;iteration;optical character recognition;pure function;selection (genetic algorithm);software propagation;unsupervised learning	Mateusz Budnik;Johann Poignant;Laurent Besacier;Georges Quénot	2014			eval;speech recognition;active learning;pattern recognition;cluster analysis;computer science;machine learning;hierarchical clustering;biometrics;artificial intelligence;annotation	NLP	-17.886031524792273	-80.25714433614817	42759
3490f2b309161b73f1f3c9f907f50e230008ffb1	combining perceptually-motivated spectral shaping with loudness and duration modification for intelligibility enhancement of hmm-based synthetic speech in noise		This paper presents our entry to a speech-in-noise intelligibility enhancement evaluation: the Hurricane Challenge. The system consists of a Text-To-Speech voice manipulated through a combination of enhancement strategies, each of which is known to be individually successful: a perceptually-motivated spectral shaper based on the Glimpse Proportion measure, dynamic range compression, and adaptation to Lombard excitation and duration patterns. We achieved substantial intelligibility improvements relative to unmodified synthetic speech: 4.9 dB in competing speaker and 4.1 dB in speech-shaped noise. An analysis conducted across this and other two similar evaluations shows that the spectral shaper and the compressor (both of which are loudness boosters) contribute most under higher SNR conditions, particularly for speech-shaped noise. Duration and excitation Lombard-adapted changes are more beneficial in lower SNR conditions, and for competing speaker noise.	dynamic range;hidden markov model;intelligibility (philosophy);loudspeaker;noise shaping;signal-to-noise ratio;speech synthesis;synthetic intelligence	Cassia Valentini-Botinhao;Junichi Yamagishi;Simon King;Yannis Stylianou	2013			speech recognition;dynamic range compression;loudness;intelligibility (communication);hidden markov model;computer science	HCI	-12.708217841562705	-90.14629405366607	42791
16124f94057c03770dba75dd3e2bdb67601c9710	web resource selection for dialogue system generating natural responses		Using Web information in example-based dialogue systems is considered to be a good way to increase the topical relevance of system responses. However, Web content is mostly written documents, not colloquial utterances. To alleviate the discrepancy in style between written and spoken language, we suggest that the corpus should be selected subsets of the Web, viz., online bulletin boards. The naturalness provided by casual utterances is especially important when the goal of a dialogue is chatting rather than formal question answering. By appropriately selecting the information source, our textbased dialogue system can generate friendly responses to users. These characteristics were evaluated with a questionnaire.	dialog system;dialog tree;discrepancy function;information source;norm (social);online chat;question answering;relevance;viz: the computer game;web content;web resource;world wide web	Masashi Inoue;Takuya Matsuda;Shoichi Yokoyama	2011		10.1007/978-3-642-22098-2_114	natural language processing;knowledge management	NLP	-27.626416824336403	-83.8544260813892	42882
027bc1c8fc69c14cc9067b5f76d775f4d6cbc8d2	the voicing feature for stop consonants: acoustic phonetic analyses and automatic speech recognition experiments	automatic speech recognition	We examine the distinctive feature [voice] that separates the voiced from the unvoiced sounds for the case of stop consonants. We conduct acoustic-phonetic analyses on a large database and demonstrate the superior separability using a temporal measure (voice onset time ; VOT) rather than spectral measures. We describe several algorithms to estimate the VOT automatically from continuous speech and compare them on a speech recogn ition problem to reduce error rates by as much as 53 % over a baseline HMM based system.	acoustic cryptanalysis;algorithm;baseline (configuration management);experiment;hidden markov model;linear separability;onset (audio);speech recognition;stellar classification	Padma Ramesh;Partha Niyogi	1998			artificial intelligence;voice;speech recognition;distinctive feature;voice-onset time;hidden markov model;pattern recognition;acoustic model;computer science	NLP	-12.610214224455586	-90.00547338834525	42938
4fa3aa7dbd3fd712aca5d1b6b975a9015ecd4db3	a method for automatic and dynamic estimation of discourse genre typology with prosodic features	institutional repositories;agglomerative clustering;bottom up;fedora;indexing terms;vital;discriminant analysis;typology;vtls;prosody;work in progress;discourse genre;ils	This paper presents a work-in-progress on the automatic analysis of discourse genre in non-elicited speech. The study is focused on the development of bottom-up methods for automatic validation of discourse typologies found in linguistic descriptions (prosodic, syntactic, pragmatic and/or contextual and situational cues). The linguistic classification examined here opposes five discourse genres +/controlled. To test this a priori classification under prosodic criteria, we propose a method that provides an automatic and dynamic estimation of discourse genre typology i.e. of prosodic similarities between discourse genres. This is achieved in a two-step procedure : a set of discriminant prosodic patterns is estimated and then used to raise a typology of discourse genres based on prosodic similarity criterion. The discriminant analysis reveals that a small number of prosodic patterns is sufficient to discriminate the 5 discourse genres. The typological analysis reveals some multilevel caterogical oppositions on a continuous prosodic scale that can be interpreted in terms of +/controlled speech.	biological anthropology;linear discriminant analysis;top-down and bottom-up design	Nicolas Obin;Anne Lacheret;Christophe Veaux;Xavier Rodet;Anne-Catherine Simon	2008			natural language processing;speech recognition;index term;typology;computer science;work in process;top-down and bottom-up design;hierarchical clustering;linguistics;prosody;linear discriminant analysis	NLP	-13.721111825745114	-80.99096281063338	42998
9e110d84e2918da83df74767dbfc1308827685d5	judgments of relative prominence for adjacent and non-adjacent accents			graph (discrete mathematics)	Jacques M. B. Terken;Karin van den Hombergh	1992			artificial intelligence;pattern recognition;computer science	NLP	-13.40288384375286	-84.54752163187213	43076
38a7f51fef7eabf02e601d66f151aa68293f76d2	estimation of vocal-tract shape from speech spectrum and speech resynthesis based on a generative model		Precise control of articulatory parameters is difficult and prevents a physical model from generating natural sounding speech signals. To determine vocal-tract shape from speech, this paper presents an inversion method for simultaneously estimating the cross-sectional area and length of the vocal tract. In addition, we performed speech resynthesis from a time-series of estimated vocal-tract shapes. The vocal-tract shape is determined through an iterative procedure that gradually optimizes the parameter values to produce the target speech spectrum. The vocal-tract shape is updated using a sensitivity function that represents the change in formant frequency caused by a small perturbation of the vocal-tract shape. When combined with a perturbation relationship of speech spectrum parameters (i.e., cepstrum parameters) and formants, our method effectively optimizes the vocal-tract shape. We quantitatively examined the accuracy using area function data for 10 isolated vowels. The results showed that the average area error was 0.43 cm and the average length error was 0.23 cm. This indicates that the vocaltract shape was determined with satisfactory accuracy. We also performed an estimation experiment for continuous speech and synthesized speech from the estimated vocal-tract shape.	automatic sounding;cepstrum;cross-sectional data;generative model;inverse transform sampling;iterative method;speech synthesis;time series;tract (literature)	Tokihiko Kaburagi	2014			generative model;vocal tract;computer science;artificial intelligence;pattern recognition	Vision	-9.322601941502139	-86.35099537761889	43181
890d50d63eaa1b53408c94267116b69668d7ee6c	generation of suprasegmental information for speech using a recurrent neural network and binary gravitational search algorithm for feature selection	binary particle swarm optimization;prosody generation;modified mos scale;feature selection;recurrent neural network;binary gravitational search algorithm	Suprasegmental (prosody) features of discourse provide a vehicle by which speakers reflect their mental purposes to listeners. Generating suitable prosody information is critical to expressing messages and improving the intelligibility and naturalness of synthetic speech. Generic prosody generators should provide information about pitch frequency (F 0) contours, energy levels, word durations, and inter-word pause durations for speech synthesizers. The present study used a recurrent neural network (RNN) for prosody generation. The inputs of this RNN were word-level and syllable-level linguistic features. To provide data efficiently for the RNN-based prosody generator in the training, validation, and test phases, automatic segmentation and labeling of phonemes were performed. The number of inputs to the RNN was reduced by employing a binary gravitational search algorithm (BGSA) for feature selection (FS). The proposed prosody generator provided 12 output prosodic parameters for the current syllable for representing pitch contour, log-energy contour, inter-syllable pause duration, duration of syllable, duration of the vowel in the syllable, and vowel onset time. Experimental results demonstrated the success of the RNN-based prosody generator in synthesizing the six prosodic elements with acceptable root mean square error (RMSE). By using a BGSA-based FS unit, a lighter neural model was achieved with a 53 % reduction in the number of weight connections, producing RMSEs with acceptable degradation over the no-FS unit prosody generator. The performance of the BGSA-based FS method was compared with a binary particle swarm optimization (BPSO) algorithm, and the BGSA showed slightly better results. A modified mean opinion score scale was used to evaluate the intelligibility and naturalness of synthesized speech using the proposed method.	acoustic cryptanalysis;artificial neural network;dialog system;effective descriptive set theory;elegant degradation;emoticon;energy level;feature selection;global storage architecture;intelligibility (philosophy);interactive voice response;lexico;mathematical optimization;mean squared error;multimodal interaction;natural language processing;netware file system;onset (audio);particle swarm optimization;pitch (music);random neural network;real-time clock;recurrent neural network;search algorithm;semantic prosody;simulation;speech recognition;speech synthesis;syllable;synthetic intelligence;usability;word-sense disambiguation	Mansour Sheikhan	2013	Applied Intelligence	10.1007/s10489-013-0505-x	natural language processing;speech recognition;computer science;recurrent neural network;machine learning;feature selection	NLP	-17.7545891669492	-84.90272675354072	43338
7abb47728a582c0a458573b7a577d5ceddd4484e	diacritics restoration for arabic dialect texts		Vocalization, diactritization or diacritics restoration is one of the major challenges in Arabic natural language processing. Algiers dialect is also concerned by this issue. In this paper, we present an automatic diacritization system for standard and dialect Arabic texts based on statistical approach. The idea is to use available tools in statistical machine translation to build such a system which basically does not require any linguistic knowledge. We began by working on Modern Standard Arabic (MSA) texts for many reasons: Algiers dialect is an Arabic language which obeys to almost the same writing rules of MSA. Availability of diacritized texts in MSA allows to test our system on a large amount of data, which is not the case for Algiers dialect. Finally, we worked first on MSA texts because of the available results for many works in this field.	circuit restoration;natural language processing;statistical machine translation	Salima Harrat;Mourad Abbas;Karima Meftouh;Kamel Smaïli	2013			natural language processing;speech recognition;linguistics	NLP	-25.33472231863681	-81.2912716540117	43413
943d2b30df957824c4797cf3bc7d4fd2435e9c0f	integration of acoustic and visual cues in prominence perception		This study concerns the perception of prominence in auditoryvisual speech perception. We constructed A/V stimuli from five-syllable sentences in which every syllable was a candidate for receiving stress. All syllables were of uniform length, and the F0 contours were manipulated using the Fujisaki model, moving a peak of F0 from the beginning to the end of the utterance. The peak was either aligned with the center of the syllable or the boundary between syllables, yielding a total of nine positions. Likewise, a video showing the upper part of a speaker’s face exhibiting one single raise of eyebrows was aligned with the audio, hence yielding nine positions for the visual cue, with the maximum displacement of the eyebrows coinciding with syllable centers or boundaries. Another series of stimuli was produced with head nods as the visual cue. In addition stimuli with constant F0 with or without video were created. 22 German native subjects rated the strength of each of the five syllables in a stimulus on a scale from 1-3. Results show that the acoustic prominence outweighs the visual one, and that the integration of both in a single syllable is the strongest when the movement as well as the F0 peak are aligned with the center of the syllable. However, F0 peaks aligned with the right boundary of the accented syllable, as well as visual peaks aligned with the left one also boost prominence considerably. Nods had an effect similar in magnitude as eye brow movements, however, results suggest that they rather have to be aligned with the right boundary of the syllable than the left one.	acoustic cryptanalysis;displacement mapping;fujisaki model;syllable	Hansjörg Mixdorff;Angelika Hönemann;Sascha Fagel	2013			communication;perception;sensory cue;syllable;speech perception;psychology;utterance	Vision	-9.819388330292181	-81.70119409374688	43435
31c876bd7c535b5950481be83e77b9a1b4af8422	towards online adaptation and personalization of key-target resizing for mobile devices	dynamic change;mobile device;generic model;data collection;text input;gold standard;personalization;key target resizing;relative error;online adaptation;error correction;soft keyboard	Software (soft) keyboards are becoming increasingly popular on mobile devices. To attempt to improve soft keyboard input accuracy, key-target resizing algorithms that dynamically change the size of each key's target area have been developed. Although methods that employ personalized touch models have been shown to outperform general models, previous work has relied upon laboratory-based offline calibration to collect the data necessary to build these models. Such approaches are unrealistic and interuptive, and it is unlikely that offline calibration can be applied in a realistic usage setting, as hundreds or thousands of touch points are necessary to build the models. To combat this problem, this paper explores the possibility of online adaptation of key-target resizing algorithms. In particular, we propose and examine three online data collection methods that can be used to build and dynamically update personalized key-target resizing models. Our results suggest that a data collection methodology that makes inference based on vocabulary and error correction behavior is able to perform on par with gold standard personalized models, while reducing relative error rate by 10.4% over general models. This approach is simple, computationally inexpensive, and calculable via information that the system already has access to. Additionally, we show that these models can be built quickly, requiring less than one week's worth of text input by an average mobile device user.	algorithm;approximation error;computability;error detection and correction;information;mac os x 10.4 tiger;mobile device;multi-touch;online and offline;personalization;plover;vocabulary	Tyler Baldwin;Joyce Yue Chai	2012		10.1145/2166966.2166969	approximation error;simulation;error detection and correction;human–computer interaction;gold standard;computer science;artificial intelligence;operating system;machine learning;personalization;mobile device;multimedia;world wide web;statistics;data collection	HCI	-26.680571864221292	-84.33303128409392	43460
72e13457cff9e37a5ae8a97a3f40c3895edb1cbf	speech translation in pedagogical environment using additional sources of knowledge		A key aspect in the development of statistical translators is the synergic combination of different sources of knowledge. This work describes the effect and implications that would have adding additional other-than-voice information in a voice translation system. In the model discussed the additional information serves as the bases for the log-linear combination of several statistical models. A prototype that implements a real-time speech translation system from Spanish to English that is adapted to specific teaching-related environments is presented. In the scenario of analysis a teacher as speaker giving an educational class could use a real time translation system with foreign students. The teacher could add slides or class notes as additional reference to the voice translation system. Should notes be already translated into the destination language the system could have even more accuracy. We present the theoretical framework of the problem, summarize the overall architecture of the system, show how the system is enhanced with capabilities related to capturing the additional information; and finally present the initial performance results.	log-linear model;machine translation;prototype;real-time transcription;statistical model;synergy	Jesús Tomás;Alejandro Canovas;Jaime Lloret Mauri;Miguel Garcia;Jose L. Abad	2010			machine translation;speech translation;speech recognition;artificial intelligence;natural language processing;speech corpus;computer science	HCI	-27.849101476795262	-84.8228259225011	43546
5c4372077a382e538e7f9b7326adc2ff252d4db3	the role of the newly introduced word types in the translations of novels	computer science;text analysis;information technology	The project detailed in the article is able to find the vocabulary rich segments of novels in different languages. The method used here takes into account the frequency of the words of the text, and based on this information we are able to create artificial texts with the same parameters. Since the original and the artificial texts share parameters they are comparable and we can find those segments of the original text which are richer in vocabulary then it is expected as compared to a random selection of the words. The advantages of finding these vocabulary rich segments of the text, beyond that they give an insight of the development of the vocabulary of a novel, is that in any translation, adaptation process it is a great advantage being familiar with these sections of the text.	vocabulary	Maria Csernoch	2010	Proceedings of the International Multiconference on Computer Science and Information Technology		natural language processing;speech recognition;computer science;linguistics;law;information technology	NLP	-25.407132580007666	-80.23940625051996	43596
4ea1ce8d202301d6de76345c1d1c7c416da417f8	evaluation of a pitch estimation algorithm for speech emotion recognition			emotion recognition;pitch detection algorithm	Nicola Vanello;Nicola Martini;Matteo Milanesi;H. Keiser;Marcello Calisti;Leonardo Bocchi;Claudia Manfredi;Luigi Landini	2009			emotion recognition;speech recognition;computer science	NLP	-13.835494005379573	-87.4079567538194	43607
7ea5a83c07c8bf4140c2b4b3660d6cef98f7a529	the pronunciation of orthographic in standard austrian german		A graphemic representation of vowel mutated a is first documented in Middle High German texts. As a consequence of a strict letter-to-letter pronunciation, graphemic <ä> was pronounced as [ɛ(ː)] in Early New High German and was subsequently prescribed in pronunciation dictionaries. Especially the pronunciation of long [ɛː] has become an issue of debate. Many scholars point out that [ɛː] had merged with [eː]. In the current investigation, it is tested whether speakers of Standard Austrian German merge the vowels orthographically represented as <ä, äh> and <e, ee, eh>. An acoustic analysis of all <ä, äh> and <e, ee, eh> of read and spontaneous speech of ten speakers was performed. No statistically significant differences were obtained, thus, a merger is assumed. The qualitative analysis proved some occasional realisations of [ɛː] in read speech. It is concluded that some speakers adhere to a prescriptive norm which is sporadically activated in formal contexts.	acoustic cryptanalysis;dictionary;orthographic projection;spontaneous order	Bettina Hobel;Sylvia Moosmüller;Christian H. Kasess	2015			merge (version control);vowel;linguistics;middle high german;orthographic projection;pronunciation;psychology;norm (social);german	NLP	-11.095615115681078	-81.2208882924236	43643
bdbf387da340240968d4163a9d78b9728349c320	the klattgrid speech synthesizer	speech analysis	We present a new speech synthesizer class, named KlattGrid, for the Praat program [3]. This synthesizer is based on the original description of Klatt [1, 2]. New aspects of a KlattGrid in comparison with other Klatt-type synthesizers are that a KlattGrid • is not frame-based but time-based. You specify parameters as a function of time with any precision you like. • has no limitations on the number of oral formants, nasal formants, nasal antiformants, tracheal formants or tracheal antiformants that can be defined. • has separate formants for the frication part. • allows varying the form of the glottal flow function as a function of time. • allows for any number of formants and bandwidths to be modified during the open phase of the glottis. • uses no beforehand quantization of amplitude parameters. • is fully integrated into the freely available speech analysis program Praat [3].	flow network;praat;quantization (signal processing);sampling (signal processing);speech synthesis;voice analysis	David Weenink	2009			speech technology;chemistry;speech processing;speech synthesis	ML	-8.929642688915935	-86.77260208341373	43708
767c994a9398572868092473406bae16435b71ba	discriminative training of finite state decoding graphs		Automatic Speech Recognition systems integrate three main knowledge sources: acoustic models, pronunciation dictionary and language models. In contrast to common practices, where each source is optimized independently, then combined in a finite-state search space, we investigate here a training procedure which attempts to adjust (some of) the parameters after, rather than before, combination. To this end, we adapted a discriminative training procedure originally devised for language models to the more general case of arbitrary finitestate graphs. Preliminary experiments performed on a simple name recognition task demonstrate the potential of this approach and suggest possible improvements.	acoustic cryptanalysis;dictionary;discriminative model;experiment;language model;speech recognition	Shiuan-Sung Lin;François Yvon	2005			artificial intelligence;speech recognition;discriminative model;pattern recognition;decoding methods;computer science;inspection time;metre (music);graph	NLP	-20.91422667336572	-88.79386140213225	43764
eee7dc52500981c86baca7a51538e609fba28b64	discovering musical structure in audio recordings	traitement signal;base donnee;signal audio;musica;audio signal;database;base dato;intelligence artificielle;sound recording;musique;enregistrement son;music analysis;signal processing;registro sonido;pattern recognition;artificial intelligence;inteligencia artificial;reconnaissance forme;reconocimiento patron;procesamiento senal;music;senal audio	Music is often described in terms of the structure of repeated phrases. For example, many songs have the form AABA, where each letter represents an instance of a phrase. This research aims to construct descriptions or explanations of music in this form, using only audio recordings as input. A system of programs is described that transcribes the melody of a recording, identifies similar segments, clusters these segments to form patterns, and then constructs an explanation of the music in terms of these patterns. Additional work using spectral information rather than melodic transcription is also described. Examples of successful machine “listening” and music analysis are presented.	machine listening;medical transcription;sound card;transcription (software)	Roger B. Dannenberg;Ning Hu	2002		10.1007/3-540-45722-4_6	speech recognition;acoustics;computer science;artificial intelligence;audio signal;signal processing;music;musicality	AI	-5.064374522628584	-92.65989230599972	43829
298d3acf86930e2191acdfd37bec54333bde5462	aligning transcripts to automatically segmented handwritten manuscripts	document structure;alignement;modelo markov oculto;handwriting recognition;document analysis;image processing;estructura documental;modele markov cache;caracter manuscrito;hidden markov model;base donnee tres grande;structure document;frase;gauchissement;automatic segmentation;manuscript character;procesamiento imagen;segmentation;traitement image;sentence;analyse documentaire;reconnaissance ecriture;realite terrain;torcimiento;alineamiento;pattern recognition;analisis documental;realidad terreno;ground truth;phrase;reconnaissance forme;very large databases;reconocimiento patron;dynamic time warping;caractere manuscrit;alignment;segmentacion;warping	Training and evaluation of techniques for handwriting recognition and retrieval is a challenge given that it is difficult to create large ground-truthed datasets. This is especially true for historical handwritten datasets. In many instances the ground truth has to be created by manually transcribing each word, which is a very labor intensive process. Sometimes transcriptions are available for some manuscripts. These transcriptions were created for other purposes and hence correspondence at the word, line, or sentence level may not be available. To be useful for training and evaluation, a word level correspondence must be available between the segmented handwritten word images and the ASCII transcriptions. Creating this correspondence or alignment is challenging because the segmentation is often errorful and the ASCII transcription may also have errors in it. Very little work has been done on the alignment of handwritten data to transcripts. Here, a novel Hidden Markov Model based automatic alignment algorithm is described and tested. The algorithm produces an average alignment accuracy of about 72.8% when aligning whole pages at a time on a set of 70 pages of the George Washington collection. This outperforms a dynamic time warping alignment algorithm by about 12% previously reported in the literature and tested on the same collection.	algorithm;dynamic time warping;ground truth;handwriting recognition;hidden markov model;markov chain;smoothing;transcription (software)	Jamie L. Rothfeder;R. Manmatha;Toni M. Rath	2006		10.1007/11669487_8	image warping;natural language processing;computer vision;speech recognition;image processing;ground truth;computer science;artificial intelligence;document structure description;machine learning;dynamic time warping;pattern recognition;database;handwriting recognition;segmentation;hidden markov model	Vision	-23.4100379113555	-80.8336702686816	43863
5222b735b96e37643987849b03b5d29a8c8c8358	the stability of mouth movements for multiple talkers over multiple sessions		To examine the stability of visible speech articulation (a potentially useful biometric) we examined the degree of similarity of a speaker’s mouth movements when uttering the same sentence on six different occasions. We tested four speakers of differing language background and compared withinand across speaker variability. We obtained mouth motion data using an inexpensive 3D close range sensor and commercial face motion capture software. These data were exported as c3d files and the analysis was based on guided principal components derived from custom Matlab scripts. We showed that within-speaker repetitions were more similar than between speaker ones; that language background did not affect the stability of the utterances and that the patterns of articulation from different speakers were relatively distinctive.	biconnected component;biometrics;matlab;motion capture;spatial variability	Chris Irwin Davis;Jeesun Kim;Vincent Aubanel;Gregory Zelic;Yatin Mahajan	2015			audiology;psychology;mouth movements	HCI	-8.380505872214917	-81.90290127632834	43951
3b174252d44efd705a20efd0e3de646da8e16804	kmrcrelat algorithm for finding repeated words in sequences: application on biological sequences	relational search;relational patterns;repeated words	Searching repeated words in sequences is a problem treated in several ways. There are two categories of methods for searching word repetition: Methods for searching exact words and methods for searching approximate words. The exact words search methods allow searching words by not tolerating any errors or differences between words. The approximate words search methods consist in finding the words with K differences (or errors) from targets words M. When words represent continuations of letters or characters belonging to an alphabet S, these errors can be presented by substitution, suppression or insertion of characters. Approximate search methods are more usually used in bioinformatic because they offer greater flexibility allowing to find more words. There are several sequences analysis techniques. The most frequently used one consists of comparing the sequences by aligning them. In this paper, we first clearly delimit our work by studying different techniques. Then, we present a new fast and efficient algorithm derived from two former algorithms. KMRCRelat uses the concept of relational words. In fact, we present a word by its components and the relations between them. We only consider components and their successors relations.	algorithm	Nabil El-Kadhi	2005	J. Comput. Meth. in Science and Engineering		natural language processing;machine learning;mathematics;algorithm;stop words	Theory	-22.30117255297476	-80.52293435528144	43960
25ef251f791f59f0a4457fd4562fdba9adac0465	from discontinuous to continuous f0 modelling in hmm-based speech synthesis		The accurate modelling of fundamental frequency, or F0, in HMM-based speech synthesis is a critical factor in achieving high quality speech. However, it is also difficult because F0 values are normally considered to depend on a binary voicing decision such that they are continuous in voiced regions and undefined in unvoiced regions. A widely used solution is to use a multi-space probability distribution HMM (MSDHMM), which directly models discontinuous F0 observations. An alternative solution, continuous F0 modelling, has been recently proposed and shown to be more effective in achieving natural synthesised speech. Here, continuous F0 observations are assumed to always exist and hence they can be modelled by standard HMMs. This paper describes a general mathematical framework for discontinuous F0 modelling, of which MSDHMM is a special case, and compares it to continuous F0 modelling. Various aspects associated with continuous F0 modelling, the use of a single F0 stream, globally tied distributions (GTD) and the assumption of a continuous unvoiced F0, are discussed in theory and examined in experiments. Both objective measures and subjective listening tests demonstrate that the introduction of continuous unvoiced F0 is vital for achieving speech quality improvement.	display resolution;experiment;hidden markov model;speech synthesis;undefined behavior;uniform theory of diffraction	Kai Yu;Blaise Thomson;Steve J. Young	2010			probability distribution;voice;hidden markov model;mathematics;speech recognition;pattern recognition;fundamental frequency;active listening;artificial intelligence;speech synthesis;special case	Web+IR	-13.746658801321372	-94.22546028866716	43976
82c47b9cf923eee0679328840fcd7bfe50aded62	exploiting inter-microphone agreement for hypothesis combination in distant speech recognition	hypothesis combination;speech recognition microphones;multi microphone;distant speech recognition;lattices microphones speech recognition microwave integrated circuits computer numerical control speech decoding;confusion networks;confusion networks distant speech recognition hypothesis combination multi microphone;multimicrophone hypothesis combination approach intermicrophone agreement distant speech recognition	A multi-microphone hypothesis combination approach, suitable for the distant-talking scenario, is presented in this paper. The method is based on the inter-microphone agreement of information, extracted at speech recognition level. Particularly, temporal information is exploited to organize the clusters that shape the resulting confusion network, and to reduce the global hypothesis search space. As a result, a single combined confusion network is generated from multiple lattices. The approach offers a novel perspective to solutions based on confusion network combination. The method was evaluated in a simulated domestic environment equipped with largely spaced microphones. The experimental evidence suggests that results, comparable or, in some cases, better than the state of the art, can be achieved under optimal configurations with the proposed method.	algorithm;experiment;microphone;sensor;speech recognition	Cristina Guerrero;Maurizio Omologo	2014	2014 22nd European Signal Processing Conference (EUSIPCO)		voice activity detection;speech recognition;acoustics;computer science;communication	Robotics	-14.55188050201851	-90.04516736264739	44046
4b498065114b61b0bca81426c58f24caee37a5f7	speech and language resources within speech recognition and synthesis systems for serbian and kindred south slavic languages	south slavic languages;speech and language resources;speech technologies	"""Unlike other new technologies, most speech technologies are heavily language dependent and have to be developed separately for each language. The paper gives a detailed description of speech and language resources for Serbian and kindred South Slavic languages developed during the last decade within joint projects of the Faculty of Technical Sciences, Novi Sad, Serbia and the company """"AlfaNum"""". It points out the advantages of simultaneous development of speech synthesis and recognition as complementary speech technologies, and discusses the possibility of reuse of speech and language resources across kindred languages."""	speech recognition	Vlado Delic;Milan Secujski;Niksa Jakovljevic;Darko Pekar;Dragisa Miskovic;Branislav M. Popovic;Stevan Ostrogonac;Milana Bojanic;Dragan Knezevic	2013		10.1007/978-3-319-01931-4_42	natural language processing;speech recognition;computer science;linguistics;natural language	NLP	-24.127653524622186	-83.8837225041002	44070
8dcf65b37a8e23937be21a26ddd93bbecacb668d	multi-channel deep clustering: discriminative spectral and spatial embeddings for speaker-independent speech separation		The recently-proposed deep clustering algorithm represents a fundamental advance towards solving the cocktail party problem in the single-channel case. When multiple microphones are available, spatial information can be leveraged to differentiate signals from different directions. This study combines spectral and spatial features in a deep clustering framework so that the complementary spectral and spatial information can be simultaneously exploited to improve speech separation. We find that simply encoding inter-microphone phase patterns as additional input features during deep clustering provides a significant improvement in separation performance, even with random microphone array geometry. Experiments on a spatial-ized version of the wsj0-2mix dataset show the strong potential of the proposed algorithm for speech separation in reverberant environments.	algorithm;cluster analysis;microphone	Zhongqiu Wang;Jonathan Le Roux;John R. Hershey	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461639	discriminative model;cocktail party effect;spatial analysis;cluster analysis;computer science;pattern recognition;time–frequency analysis;microphone array;artificial intelligence;communication channel	Vision	-15.142483393943339	-90.51209337282421	44096
b23d52c0cb19eca357413599a89a1c7b39639532	an automatic broadcast system for a weather report radio program	speech synthesis;speech processing;speech rate conversion;weather forecasting;speech based user interfaces;voice synthesizer recording sentence set speech rate conversion templates;voice synthesizer;radio broadcasting;recording sentence set;speech synthesis method speech rate converter automatic broadcast system five point mos scale mean opinion score voice synthesizer nhk weather report radio program japan broadcasting corporation natural sounding synthesized speech speech databases recording sentence sets;speech databases meteorology synthesizers typhoons human voice;templates;natural language processing;weather forecasting natural language processing radio broadcasting speech processing speech synthesis speech based user interfaces	Here we describe a speech-synthesis method using templates that can generate recording-sentence sets for speech databases and produce natural sounding synthesized speech. Applying this method to the Japan Broadcasting Corporation (NHK) weather report radio program reduced the size of the recording-sentence set required to just a fraction of that needed by a comparable method. After integrating the recording voice of the generated recording-sentence set into the speech database, speech was produced by a voice synthesizer using templates. In a paired-comparison test, 66% of the speech samples synthesized by our system using templates were preferred to those produced by a conventional voice synthesizer. In an evaluation test using a five-point mean opinion score (MOS) scale, the speech samples synthesized by our system scored 4.97, whereas the maximum score for commercially available voice synthesizers was 3.09. In addition, we developed an automatic broadcast system for the weather report program using the speech-synthesis method and speech-rate converter. The system was evaluated using real weather data for more than 1 year, and exhibited sufficient stability and synthesized speech quality for broadcast purposes.	automatic sounding;database;speech synthesis	Hiroyuki Segi;Reiko Takou;Nobumasa Seiyama;Tohru Takagi;Yuko Uematsu;Hideo Saito;Shinji Ozawa	2013	IEEE Transactions on Broadcasting	10.1109/TBC.2013.2272406	radio broadcasting;voice activity detection;codec2;g.729;linear predictive coding;speech recognition;acoustics;weather forecasting;telecommunications;computer science;speech coding;speech processing;acoustic model;psqm;speech synthesis	Visualization	-18.90056139748588	-83.91201265484194	44184
182321ba8df64384f113ea0849e1b877be7d6ceb	subjective intelligibility testing and perceptual study of thai initial and final consonants		We methodically design and develop a subjective intelligibility testing of Thai speech based on the diagnostic rhyme test (DRT). The Thai DRT (TDRT) consists of 2 test sets, one for initials and the other final consonants. The test for initials is designed to equally compare 21 phonemes pairwise, which results in 210 stimulus pairs. The TDRT for finals compares 8 final phonemes, yielding 84 stimulus pairs. These tests are wellconstructed using real words. TDRT have two main advantages. It allows us to evaluate percent intelligibility responses in each stimulus pair and to systematically compare confusion responses across all phonemes. To test the validity of our method and to further our investigation, we carry out the subjective intelligibility test on twenty eight Thai listeners using TDRT, which varies in 4 SNR levels (6, 12, 18, and 24dB). Average intelligibility scores and confusion matrices for initial and final consonants are analyzed.	confusion matrix;intelligibility (philosophy);signal-to-noise ratio;six-hour clock	Charturong Tantibundhit;Chutamanee Onsuwan;Sumonmas Thatphithakkul;Patcharika Chootrakool;Krit Kosawat;Nattanun Thatphithakkul;Tanawan Saimai;Nantaporn Saimai	2011			perception;speech recognition;intelligibility (communication);computer science	ML	-11.39982719440877	-83.95109910671368	44277
3e6e1e480fb8711296a0056d0ea06e5de98b71f5	machine translation of low-resource spoken dialects: strategies for normalizing swiss german		The goal of this work is to design a machine translation (MT) system for a low-resource family of dialects, collectively known as Swiss German, which are widely spoken in Switzerland but seldom written. We collected a significant number of parallel written resources to start with, up to a total of about 60k words. Moreover, we identified several other promising data sources for Swiss German. Then, we designed and compared three strategies for normalizing Swiss German input in order to address the regional diversity. We found that character-based neural MT was the best solution for text normalization. In combination with phrase-based statistical MT, our solution reached 36% BLEU score when translating from the Bernese dialect. This value, however, decreases as the testing data becomes more remote from the training one, geographically and topically. These resources and normalization techniques are a first step towards full MT of Swiss German dialects.	bleu;machine translation;normalization property (abstract rewriting);switzerland;text normalization;text-based (computing)	Pierre-Edouard Honnet;Andrei Popescu-Belis;Claudiu Musat;Michael Baeriswyl	2018	CoRR		text normalization;machine translation;swiss german language;artificial intelligence;natural language processing;bleu;spelling;phrase;speech recognition;computer science	NLP	-24.08950524848483	-82.8771740473709	44292
6c57e30341a2b8ca95d8f27218780778fc7ac988	investigation of different acoustic modeling techniques for low resource indian language data	databases;acoustics;training;neural nets gaussian processes hidden markov models mixture models natural language processing;training data;hidden markov models;bottleneck dnn sgmm low resource data indian languages hindi tamil;feature extraction;mandi database different acoustic modeling techniques low resource indian language data deep neural network dnn subspace gaussian mixture model sgmm continuous density hidden markov models cdhmm;training data models feature extraction acoustics hidden markov models databases training data;data models	In this paper, we investigate the performance of deep neural network (DNN) and Subspace Gaussian mixture model (SGMM) in low-resource condition. Even though DNN outperforms SGMM and continuous density hidden Markov models (CDHMM) for high-resource data, it degrades in performance while modeling low-resource data. Our experimental results show that SGMM outperforms DNN for limited transcribed data. To resolve this problem in DNN, we propose to train DNN containing bottleneck layer in two stages: First stage involves extraction of bottleneck features. In second stage, the extracted bottleneck features from first stage are used to train DNN having bottleneck layer. All our experiments are performed using two Indian languages (Tamil & Hindi) in Mandi database. Our proposed method shows improved performance when compared to baseline SGMM and DNN models for limited training data.	acoustic cryptanalysis;acoustic model;artificial neural network;baseline (configuration management);deep learning;directory information tree;experiment;hidden markov model;integrated information theory;markov chain;subspace gaussian mixture model;timit;viz: the computer game	R. Sriranjani;B MuraliKarthick;S. Umesh	2015	2015 Twenty First National Conference on Communications (NCC)	10.1109/NCC.2015.7084860	speech recognition;computer science;machine learning;pattern recognition	AI	-18.227960313919866	-89.37107851677926	44362
9e4e2b1a2e6779b57d3f3d112f5d38c5188dce52	hmm-based hierarchical unit selection combining kullback-leibler divergence with likelihood criterion	databases;dynamic programming;statistical criterion;unit selection;speech synthesis;cost function;maximum likelihood;hidden markov model;kullback leibler divergence;speech segmentation;hmm;kld;maximum likelihood estimation;diversity reception;speech synthesis hidden markov models maximum likelihood estimation;phone sized units;frame level units;hidden markov models;frame level units hmm based hierarchical unit selection kullback leibler divergence likelihood criterion hidden markov model statistical criterion frame sized speech segments maximum likelihood criterion concatenative synthesis system phone sized units cost function criterion;kld speech synthesis hmm;frame sized speech segments;cost function criterion;concatenative synthesis system;maximum likelihood criterion;signal synthesis;diversity reception hidden markov models speech synthesis context modeling cost function signal synthesis synthesizers databases dynamic programming flowcharts;synthesizers;context modeling;flowcharts;hmm based hierarchical unit selection;likelihood criterion	This paper presents a hidden Markov model (HMM) based unit selection method using hierarchical units under statistical criterion. In our previous work we tried to use frame sized speech segments and maximum likelihood criterion to improve the performance of traditional concatenative synthesis system using phone sized units and cost function criterion. In this paper, hierarchical units which consist of phone level units and frame level units are adopted to achieve better balance between the coverage rate of candidate unit and the number of concatenation points during synthesis. Besides, Kullback-Leibler divergence (KLD) between candidate and target phoneme HMMs is introduced as a part of the final criterion for unit selection. The listening result proves that these two approaches can improve the performance of synthetic speech effectively.	concatenation;concatenative synthesis;hidden markov model;kullback–leibler divergence;loss function;markov chain;synthetic intelligence	Zhen-Hua Ling;Ren-Hua Wang	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.367302	speech recognition;computer science;machine learning;pattern recognition;maximum likelihood;hidden markov model;statistics	Robotics	-19.243105768292228	-91.89976666452893	44446
06a8c3c92d1297e96e6fc3405c9aaeb4da91d7b5	concatenative synthesis based on a harmonic model	modelizacion;time scale;speech synthesis;etude theorique;pitch acoustics;concatenative speech synthesizer concatenative synthesis harmonic model recorded speech units prosody unit boundaries mismatch pitch modification time scale modification smoothing algorithms harmonic coding speech frame phase coherence shape invariance harmonic relation sine waves analysis frame synthesis frame boundary;speech processing;tratamiento palabra;traitement parole;armonica;speech coding;harmonic;tonie;algorithme;modelisation;algorithm;codificacion;forma sinusoidal;harmonique;sinusoidal shape;speech synthesis smoothing methods harmonic analysis coherence synthesizers shape measurement speech coding pulse shaping methods databases spectral shape;smoothing;altura sonida;prosodie;forme sinusoidale;coding;estudio teorico;alisamiento;sintesis palabra;theoretical study;speech coding speech synthesis;prosody;modeling;lissage;synthese parole;prosodia;codage;algoritmo	One of the currently most successful approaches to synthesizing speech, concatenative synthesis, combines recorded speech units to build full utterances. However, the prosody of the stored units is often not consistent with that of the target utterance and must be altered. Furthermore, several types of mismatch can occur at unit boundaries and must be smoothed. Thus, both pitch and time-scale modification techniques as well as smoothing algorithms play a crucial role in such concatenation based systems. In this paper, we describe novel approaches to each of these issues. First, we present a conceptually simple technique for pitch and time-scale modification of speech. Our method is based upon a harmonic coding of each speech frame, and operates entirely within the original sinusoidal model [1]. Crucially, it makes no use of “pitch pulse onset times.” Instead, phase coherence, and thus shape invariance, is ensured by exploiting the harmonic relation existing between the sine waves used to code each analysis frame so that their phases at each synthesis frame boundary are consistent with those derived during analysis. Secondly, a smoothing algorithm, aimed specifically at correcting phase mismatches at unit boundaries, is described. Results are presented showing our prosodic modification techniques to be highly suitable for use within a concatenative speech synthesizer.	algorithm;concatenation;concatenative programming language;concatenative synthesis;onset (audio);pitch (music);semantic prosody;sinusoidal model;smoothing;speech synthesis	Darragh O'Brien;Alex I. C. Monaghan	2001	IEEE Trans. Speech and Audio Processing	10.1109/89.890067	speech recognition;systems modeling;acoustics;computer science;speech coding;harmonic;speech processing;prosody;coding;speech synthesis;smoothing	Graphics	-9.006915542980156	-86.72117749774641	44702
cd31807cc53019fdd845c340814a7bed0eeb7259	recognition of assamese phonemes using rnn based recognizer	mlp;cmn;rnn;mfcc	This paper discusses a novel technique for the recognition of Assamese phonemes using Recurrent Neural Network (RNN) based phoneme recognizer. A Multi-Layer Perceptron (MLP) has been used as phoneme segmenter for the segmentation of phonemes from isolated Assamese words. Two different RNN based approaches have been considered for recognition of the phonemes and their performances have been evaluated. MFCC has been used as the feature vector for both segmentation and recognition. With RNN based phoneme recognizer, a recognition accuracy of 91% has been achieved. The RNN based phoneme recognizer has been tested for speaker mismatched and channel mismatched conditions. It has been observed that the recognizer is robust to any unseen speaker. However, its performance degrades in channel mismatch condition. Cepstral Mean Normalization (CMN) has been used to overcome the problem of performance degradation effectively.	finite-state machine;random neural network	Utpal Bhattacharjee	2011		10.1007/978-3-642-31980-8_14	natural language processing;speech recognition;pattern recognition	NLP	-16.526934541288558	-89.11683761354152	44772
0d9f88856b844a074d845be6e0f07532c0284f07	influence of various asymmetrical contextual factors for tts in a low resource language	hmm;contextual modelling;hts hmm contextual modelling;speech hidden markov models high temperature superconductors speech synthesis databases acoustics speech recognition;speech synthesis hidden markov models natural language processing speech recognition;hts;lllcr asymmetrical contextual factors tts low resource language generalized statistical framework hidden markov model speech recognition hmm based speech synthesis hts method india gujarati language speech synthesis quality gujarati speech speech intelligibility triphone pentaphone gujarati hts left left left centre right contextual factors	The generalized statistical framework of Hidden Markov Model (HMM) has been successfully applied from the field of speech recognition to speech synthesis. In this paper, we have applied HMM-based Speech Synthesis (HTS) method to Gujarati (one of the official languages of India). Adaption and evaluation of HTS for Gujarati language has been done here. In addition, to understand the influence of asymmetrical contextual factors on quality of synthesized speech, we have conducted series of experiments. Evaluation of different HTS built for Gujarati speech using various asymmetrical contextual factors is done in terms of naturalness and speech intelligibility. From the experimental results, it is evident that when more weightage is given to left phoneme in asymmetrical contextual factor, HTS performance improves compared to conventional symmetrical contextual factors for both triphone and pentaphone case. Furthermore, we achieved best performance for Gujarati HTS with left-left-left-centre-right (i.e., LLLCR) contextual factors.	experiment;hidden markov model;high-throughput satellite;intelligibility (philosophy);markov chain;netware file system;speech recognition;speech synthesis;triphone	Nirmesh J. Shah;Mohammadi Zaki;Hemant A. Patil	2014	2014 International Conference on Asian Language Processing (IALP)	10.1109/IALP.2014.6973509	natural language processing;speech recognition;speech corpus;computer science;machine learning;hidden markov model;high-temperature superconductivity	NLP	-18.49651067333629	-85.09661013872534	44824
38d9e10b721f8f3219407135d6d24363e9f6c4da	on large vocabulary continuous speech recognition of highly inflectional language - czech	large vocabulary continuous speech recognition	A system for large vocabulary continuous speech recognition of highly inflectional language is introduced. Word-based recognition approach is compared with a morpheme-based recognition system. An experiment involving Czech N-best rescoring has been performed with encouraging results.	speech recognition;vocabulary	Pavel Ircing;Pavel Krbec;Jan Hajic;Josef Psutka;Sanjeev Khudanpur;Frederick Jelinek;William J. Byrne	2001			natural language processing;speech recognition;computer science;linguistics	NLP	-20.569242496613732	-84.7833617804023	44945
342d0e5798f4b69673f8ba5460a84f889c527d9b	tonal description of polyphonic audio for music content processing	key estimation;sound and music computing;content analysis and indexing;tonal description	We present a method to extract a description of the tonal aspects of music from polyphonic audio signals. We define this tonal description using different levels of abstraction, differentiating between low-level signal descriptors and high-level textual labels. We also establish different temporal scales for description, defining some features as being attached to a certain time instant, and other global descriptors as related to a wider segment. The description is validated by estimating the key of a piece. We also propose the description as a tonal representation of the polyphonic audio signal to measure tonal similarity between audio excerpts and to establish the tonal structure of a musical piece.		Emilia Gómez	2006	INFORMS Journal on Computing	10.1287/ijoc.1040.0126	speech recognition;computer science	Web+IR	-7.3429562781639675	-91.96419269836068	45319
e5fd740fc41d8ca292434c8ae4f1e053fec9a1ad	specificity and definiteness in sentence and discourse structure	generique specifique;pragmatics;contrastive analysis;noun phrase;defini indefini;filologias;semantics;definite indefinite;semantique;generic specific;discourse structure;sentence;linguistica;pragmatique;anglais;syntagme nominal;reference;discours;turc;english;phrase;grupo a;discourse;analyse contrastive	The paper gives a contrastive analysis of the two semantic categories specificity and definiteness. It argues against the traditional picture that assumes that specific expressions are a subclass of indefinite NPs. The paper rather assumes that the two categories are independent of each other. Definiteness expresses the discourse pragmatic property of familiarity, while specificity mirrors a more finely grained referential structure of the items used in the discourse. A specific NP indicates that it is referentially anchored to another discourse object. This means that the referent of the specific expression is linked by a contextually salient function to the referent of another expression.	coherence (physics);email;embedded system;emoticon;fach;modal logic;sensitivity and specificity;the matrix	Klaus von Heusinger	2002	J. Semantics	10.1093/jos/19.3.245	noun phrase;philosophy;english;semantics;linguistics;sociology;pragmatics	NLP	-33.57476730852347	-81.97902414758842	45361
8eff559c781ccfab32c4b513dbe831e5f8d55753	note and timbre classification by local features of spectrogram		In recent years, very large scale online music databases containing more than 10 million tracks became prevalent as the fostered availability of streaming and downloading services via the World-Wide Web. The set of access schemes, or Music Information Retrieval (MIR), still poses several and partially solved problems, especially the personalization of the access, such as query by humming, melody, mood, style, genre, instrument, etc. Generally the previous approaches utilized the spectral features of the music track and extracted several high-level features such as pitch, cepstral coefficients, power, and the time-domain features such as onset, tempo, etc. In this work, however, the low-level local features of the spectrogram partitioned by means of the Bark scale are utilized to extract the quantized time-frequency-power features to be used by a Support Vector Machine to classify the notes (melody) and the timbre (instrument) of 128 instruments of General Midi standard. A database of 3-second sound clips of notes C4 to C5 on 7 sound cards using two software synthesizers is constructed and used for experimental note and timbre classification. The preliminary results of 13-category music note and 16-category timbre classifications are promising and their performance scores are surpassing the previously proposed methods.	bark scale;cepstrum;coefficient;database;download;faceted classification;general midi;high- and low-level;information retrieval;list of online music databases;online music store;onset (audio);personalization;pitch (music);query by humming;sound card;spectrogram;support vector machine;world wide web	Erhan Guven;Ahmet Murat Ozbayoglu	2012		10.1016/j.procs.2012.09.051	speech recognition;artificial intelligence	Web+IR	-8.840322426199299	-93.91572411697183	45390
c7872c7986c9369fb9b4189cf288fb1ffb4eccc6	harmonic sound stream segregation using localization and its application to speech stream segregation	extraction information;computational auditory scene analysis;analisis escena;analyse scene;architecture systeme;segregation;metodologia;information extraction;forme onde;reconocimiento palabra;integration information;implementation;signal distortion;localization;fuente sonora;segregacion;audition binaurale;spectrum;distorsion signal;binaural hearing;frecuencia fundamental;methodologie;experimental result;harmonic structure;ejecucion;information integration;forma onda;integracion informacion;resultado experimental;speech stream segregation;source sonore;speech recognition;analyse scene auditive;arquitectura sistema;waveform;reconnaissance parole;methodology;resultat experimental;system architecture;sound source;auditory scene analysis;frequence fondamentale;spectrum distortion;fundamental frequency;audicion binaural;scene analysis;extraction informacion;distorsion senal	Sound stream segregation is essential to understand auditory events in the real world. In this paper, we present a new method of segregating a series of harmonic sounds. The harmonic structure and sound source direction are used as clues for segregation. The direction information of the sources is used to extract fundamental frequencies of individual harmonic sounds, and harmonic sounds are segregated according to the extracted fundamental frequencies. Sequential grouping of harmonic sounds is achieved by using both sound source directions and fundamental frequencies. An application of the harmonic stream segregation to speech stream segregation is presented. It provides effective speech stream segregation using binaural microphones. Experimental results show that the method reduces the spectrum distortions and the fundamental frequency errors compared to an existing monaural system, and that it can segregate three simultaneous harmonic streams with only two microphones.		Tomohiro Nakatani;Hiroshi G. Okuno	1999	Speech Communication	10.1016/S0167-6393(98)00079-X	spectrum;speech recognition;waveform;internationalization and localization;sound localization;computer science;information integration;methodology;computational auditory scene analysis;fundamental frequency;auditory scene analysis;implementation;information extraction	HCI	-4.91981887675793	-92.69863387296726	45420
e8a51227021e8c01f7f89be53e8cf8c6db504c9f	parallel phrasing across cultures: emotional vs. informational effects	text analysis;natural languages;psychology;visualization parallel processing professional communication educational institutions accuracy presses testing;grammars;parallel phrasing cultures visual formatting readers information recall parallel text informational effects emotional effects;visual formatting emotion response information recall parallel phrasing;text analysis cultural aspects grammars natural languages psychology;cultural aspects	This is a work-in progress paper, describing a series of experiments conducted to determine the actual effects of parallel phrasing vs. non-parallel phrasing on readers. Traditionally, it has been assumed that parallel text enhances information recall more than non-parallel text, but no distinct evidence for this emerges in the data. However, the data from another series of experiments indicates that parallel text has an emotional effect on readers distinct from non-parallel text.	experiment;parallel text	Nicole Amare;Alan Manning	2013	IEEE International Professonal Communication 2013 Conference	10.1109/IPCC.2013.6623894	natural language processing;text mining;speech recognition;computer science;linguistics;natural language	Visualization	-16.787709698780173	-81.15257720075418	45439
64aff099496444008f1ea9cc04b1eea77b9a6a27	singing voice separation using mono-channel mask		Separating singing voice from monaural song recording is a highly difficult task. Still it is important because it has many applications such as singer identification, lyrics recognition, and melody extraction. Difficulty arises due to many musical instruments involved and time-varying spectral overlap between singing voice and music. The goal of singing voice separation is to extract singing voice from the given monaural song recording with minimum artefacts and musical interference. We propose a three stage system for singing voice separation which helps to improve intelligibility and perceptual quality of the separated output. In the first stage, modified sub-harmonic summation algorithm finds pitch of the singing voice and its harmonic components. Here, we create a binary mask. In the second stage, frames i.e. the masked spectral amplitudes are classified as singing and non-singing frames by using a combination of Gammatone frequency cepstral coefficients (GFCC) and Mel-frequency cepstral coefficients (MFCC) features. Lastly, mono-channel mask is created and signal amplitude correction is done using kurtosis measure. We synthesize the estimate of singing voice using both binary mask and mono-channel mask. It is observed that the singing voice separated using mono-channel mask improves the GNSDR score. Performance of the proposed system is compared with the other methods, where it presents excellent improvement in terms of GNSDR. It produces higher GNSDR scores in case of two different datasets.	spectral mask	Pallavi P. Ingale;Sanjay L. Nalbalwar	2018	I. J. Speech Technology	10.1007/s10772-018-9509-6	artificial intelligence;speech recognition;lyrics;pattern recognition;intelligibility (communication);harmonic;mel-frequency cepstrum;computer science;pitch detection algorithm;communication channel;singing;monaural	EDA	-10.24938644303692	-89.99775139649785	45445
cf600facc07dc14c90fce0ce97fb0b6f6859c8b0	an experiment with a non-head-mounted microphone for discrete utterance recognition (dur)	phase change materials;microphones;mouth;magnetic heads;vocabulary;transducers;speech enhancement;frequency response;noise cancellation;microphones magnetic heads mouth transducers phase change materials speech enhancement laboratories vocabulary frequency response noise cancellation	Typically, DUR systems use a head-mounted microphone in order to keep the distance between the mouth and microphone - and therefore the input conditions - constant over time. These devices, unfortunately, also limit user-flexibility and may be quite uncomfortable. A goal of the current work is to ascertain the feasibility of a two-microphone, shoulder-mounted system. The experiment was based on data recorded simultaneously, in PCM, from the reference head-mounted microphone and the experimental system. Results based on machine DUR for two different vocabularies and two combination algorithms are presented. One very simple combination algorithm gave results which compared quite favorably to the performance of the head-mounted transducer.	microphone	Wendy J. Kessler;N. Rex Dixon;Harvey F. Silverman	1984		10.1109/ICASSP.1984.1172354	frequency response;speech recognition;transducer;noise-canceling microphone;active noise control	Vision	-8.703825429466065	-87.26718260368017	45516
3ad98a5ec605473b7a6153a2eab1413d0c18a191	how syllables group in chinese	stress;microphones;rhythm;connected speech;speech processing;prosodic hierarchy;presses;foot;speech;lexical stress chinese connected speech speech rhythm prosodic hierarchy syllable organization;speech rhythm;lexical stress;natural languages speech acoustic pulses educational institutions stress microphones pulse generation foot auditory system rhythm;speech processing natural language processing;syllable organization;organizations;chinese;natural language processing	In connected speech, syllables are believed to be prosodically organized into groups. Such groups are thought of as either the basic units of speech rhythm or a prosodic hierarchy. In this study we investigated the nature of syllable organization by examining syllable duration, tonal undershoot and FO height in Chinese as related to speaking mode, group position in sentence, syllable position in group, and number of syllables in the group. The results showed polysyllabic shortening and constituent-edge lengthening previously reported for languages with lexical stress despite the fact that no lexical stress was involved in the present study.	syllable	Maolin Wang;Yi Xu	2008	2008 6th International Symposium on Chinese Spoken Language Processing	10.1109/CHINSL.2008.ECP.88	speech recognition;computer science;speech;rhythm;speech processing;linguistics;stress;chinese;foot	NLP	-11.29163316589072	-81.4991918976997	45564
8b187a2585cefecc67222ff478e67d379f0739f9	a singing voice/music separation method based on non-negative tensor factorization and repeat pattern extraction	repet;median filter;ntf;unsupervised signal processing;source separation	In this paper, a novel singing voice/music separation method is proposed based on the non-negative tensor factorization NTF and repeat pattern extraction technique REPET to separate the mixture into an audio signal and a background music. Our system consists of three stages. Firstly, we use the NTF to decompose the mixture into different components, and similarity detection is applied to distinguish the components from each other, in order to classify the components into two classes as the voice including voice/periodic music and the block music/voice; next we utilize the REPET to extract the background music one step further for the two classes, and the final background music is estimated by adding the two backgrounds together, the left is added together as the singing voice; finally the music spectrum and the voice spectrum are filtered by harmonic filter and percussive filter respectively. To improve the performance further, wiener filter is used to separate the voice and music. Our method can improve the separation performance compared with the other state-of-the-art methods on the MIR-1K dataset.	pattern recognition	Yong Zhang;Xiaohong Ma	2015		10.1007/978-3-319-25393-0_32	median filter;speech recognition;computer science	Vision	-9.67717200362246	-90.95751967779648	45618
03665f9d053632fdec804c4003d6a8958e892360	affine invariant features and their application to speech recognition	speech recognition cepstral analysis mel frequency cepstral coefficient robustness pattern recognition loudspeakers speech processing vectors data engineering error analysis;japanese isolated word recognition;cepstral features;affine invariant feature;acoustics;sequence data;speech processing;transforms algebra cepstral analysis speech recognition;speech;data engineering;indexing terms;data mining;vocal tract length;mel frequency cepstral coefficient;error analysis;cepstral analysis;vectors;algebra;loudspeakers;mfcc delta;affine transformation;transforms;word recognition;pattern recognition;error rate;speech recognition;robustness;frequency warping;mfcc delta affine invariant features speech recognition sequence data affine transformation algebraic calculation vocal tract length frequency warping cepstral features japanese isolated word recognition;affine invariant features;speech recognition affine invariant feature frequency warping speaker normalization;invariant feature;speaker normalization;algebraic calculation	This paper proposes a set of affine invariant features (AIFs) for sequence data. The proposed AIFs can be calculated directly from the sequence data, and their invariance to affine transformation is proved mathematically through algebraic calculation. We apply the AIFs to speech recognition. Since the vocal tract length (VTL) difference causes to frequency warping which can be approximated well by affine transform on cepstral features [1], the AIFs of cepstral sequence provide robust features for VTL variations. We experimentally examine the invariance of AIFs of speech signals, and apply AIFs for Japanese isolated word recognition. The experimental results show that the combination of AIFs with MFCC or MFCC+Δ can lead to higher recognition rates than MFCC or MFCC+Δ only. Especially in the mismatched experiments, the combination with AIFs can reduce the error rates about 30% when compared to MFCC or MFCC+Δ only. The AIFs are expected to have other applications than speech recognition, since their invariance is general.	approximation algorithm;bilinear transform;cepstrum;experiment;linear algebra;speech recognition;tract (literature);virtual tape library	Yu Qiao;Masayuki Suzuki	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4960662	loudspeaker;speech recognition;index term;word recognition;word error rate;computer science;speech;pattern recognition;speech processing;affine transformation;mathematics;robustness	Vision	-14.843368313283806	-92.81064877789687	45689
033c8e842faa7e395ae75d9bbcc18da20c99f2c3	bridging the gap between speech technology and natural language processing: an evaluation toolbox for term discovery systems	word segmentation;spoken term discovery;evaluation	The unsupervised discovery of linguistic terms from either continuous phoneme transcriptions or from raw speech has seen an increasing interest in the past years both from a theoretical and a practical standpoint. Yet, there exists no common accepted evaluation method for the systems performing term discovery. Here, we propose such an evaluation toolbox, drawing ideas from both speech technology and natural language processing. We first transform the speech-based output into a symbolic representation and compute five types of evaluation metrics on this representation: the quality of acoustic matching, the quality of the clusters found, and the quality of the alignment with real words (type, token, and boundary scores). We tested our approach on two term discovery systems taking speech as input, and one using symbolic input. The latter was run using both the gold transcription and a transcription obtained from an automatic speech recognizer, in order to simulate the case when only imperfect symbolic information is available. The results obtained are analysed through the use of the proposed evaluation metrics and the implications of these metrics are discussed.	acoustic cryptanalysis;bridging (networking);finite-state machine;medical transcription;natural language processing;simulation;software metric;speech recognition;speech technology;transcription (software);unsupervised learning	Bogdan Ludusan;Maarten Versteegh;Aren Jansen;Guillaume Gravier;Xuan-Nga Cao;Mark Johnson;Emmanuel Dupoux	2014			natural language processing;text segmentation;speech recognition;computer science;evaluation;linguistics	NLP	-20.427778269089885	-82.99917978966059	45730
478ecee69493d3f67e030ff9439b9f8aa42e180f	oldenburg logatome speech corpus (ollo) for speech recognition experiments with humans and machines	speech intelligibility;automatic speech recognition;semantic information;speech recognition	This paper introduces the new OLdenburg LOgatome speech corpus (OLLO) and outlines design considerations during its creation. OLLO is distinct from previous ASR corpora as it specifically targets (1) the fair comparison between human and machine speech recognition performance, and (2) the realistic representation of intrinsic variabilities in speech that are significant for automatic speech recognition (ASR) systems. To enable an unbiased human-machine comparison, OLLO is designed for recognition of individual phonemes that are embedded in logatomes, specifically, three-phoneme sequences with no semantic information. A balanced set of target-phonemes important for human and automatic speech recognition has been chosen, drawing on pilot ASR studies and cross-fertilization from the field of human speech intelligibility testing. Several intrinsic variabilities in speech are represented in OLLO, by recording from 40 speakers from four German dialect regions, and by covering six articulation characteristics. Results from preliminary phonetic time-labeling and ASR experiments are promising and consistent with corpus variabilities.	automated system recovery;biconnected component;embedded system;experiment;intelligibility (philosophy);speech corpus;speech recognition;text corpus	Thorsten Wesker;Bernd T. Meyer;Kirsten Wagener;Jörn Anemüller;Alfred Mertins;Birger Kollmeier	2005			voxforge;natural language processing;speech technology;speech recognition;speech corpus;computer science;intelligibility	NLP	-19.314728955505046	-83.61692916331208	45933
22a75a87ab033cd57173d26f8880ff336a34d3cb	jokestega: automatic joke generation-based steganography methodology	information security;information hiding;linguistic steganography;textual jokes;text steganography;puns;joke based steganography	This paper presents a novel steganography methodology, namely Automatic Joke Generation Based Steganography Methodology Jokestega, that pursues textual jokes in order to hide messages. Basically, Jokestega methodology takes advantage of recent advances in Automatic Jokes Generation AJG techniques to automate the generation of textual steganographic cover. In a corpus of jokes, one may judge a number of documents to be the same joke although letters, locations, and other details are different. Generally, joke and puns could be retold with totally different vocabulary, while still retaining their identities. Therefore, Jokestega pursues the common variations among jokes to conceal data. Furthermore, when someone is joking, anything may be said which legitimises the use of joke-based steganography. This makes employing textual jokes very attractive as steganographic carrier for camouflaging data. It is worth noting that Jokestega follows Nostega paradigm, which implies that joke-cover is noiseless. The validation results demonstrate the effectiveness of Jokestega.	steganography	Abdelrahman Desoky	2012	IJSN	10.1504/IJSN.2012.052529	pun;computer science;artificial intelligence;information security;internet privacy;information hiding;computer security	Vision	-31.93618817858775	-88.32852924754825	45990
7f9c095c33597135b203d3ad04d772aab7b6eb69	music interpolation by wiener filter and its application to music arrangement	interpolation;audio signal processing;interpolation vectors correlation wiener filter stochastic processes matrix decomposition;wiener filters;trees mathematics;generative theory of tonal music;wiener filters audio signal processing interpolation music trees mathematics;general methods;interpolation method;tree structure;arrangement;wiener filter;stochastic model;tonal music wiener filter music arrangement music interpolation method automatic composition automatic arrangement stochastic model melody generation method tree structure generative theory;music;wiener filter music interpolation arrangement	When composers and arrangers compose music, they depend not only on their sense, but also on music theory. Therefore, there is possibility of automatic composition and automatic arrangement by computers. Some studies on automatic composition using computers have been studied continuously for 50 years. For example, an automatic music composition based on counterpoint and imitation using stochastic models and a melody generation method using a tree structure of generative theory of tonal music, are cited as previous studies. In this paper, we propose a method of music interpolation by applying the Wiener filter to scores with missing parts. In addition, we propose a method of music arrangement based on the proposed interpolation method.	automatic control;color gradient;computer;interpolation;stochastic process;theory;tree structure;video game music;wiener filter	Yoshifumi Mizuno;Akira Tanaka;Masaaki Miyakoshi	2011	2011 IEEE International Conference on Granular Computing	10.1109/GRC.2011.6122642	speech recognition;audio signal processing;interpolation;machine learning;music;mathematics;tree structure;wiener filter;statistics	Robotics	-8.51364301088898	-88.08597961052003	46030
53448fde78e44770fd31adfc4221de4d64e0f988	managing information at linguistic interfaces	interface term;dialogue translation system;linguistic constraint;integrated system;linguistic interface;communicative role;linguistic information	A large spoken dialogue translation system imposes both engineering and linguistic constraints on the way in which linguistic information is communicated between modules. We describe the design and use of interface terms, whose formal, functional and communicative role has been tested in a sequence of integrated systems and which have proven adequate to these constraints.	computational semantics;machine translation;prototype;requirement;semantic analysis (compilers);system requirements	Johan Bos;C. J. Rupp;Bianka Buschbeck-Wolf;Michael Dorna	1998			natural language processing;integrable system;computer science;knowledge management	NLP	-28.830587773568638	-82.10504009641522	46037
582664467e621621b3fdb266ce8aa07cf1a3bcb4	evaluation of tree-structured piecewise linear transformation-based noise adaptation on aurora2 database	piecewise linear;linear transformation;tree structure	This paper uses the AURORA2 task to investigate the performance of our proposed tree-structured piecewise-linear transformation (PLT) noise adaptation. In our proposed method, an HMM that best matches the input speech is selected based on the likelihood maximization criterion by tracing a tree structured HMM space that is prepared in the training step, and the selected HMM is further adapted by linear transformation. Experimental results show that our method achieves a significant improvement for the AURORA2 database.	expectation–maximization algorithm;hidden markov model;piecewise linear continuation;racket	Zhipeng Zhang;Tomoyuki Ohya;Sadaoki Furui	2004			thermal shock;chip;artificial intelligence;pattern recognition;computer science;lead frame;mathematical optimization;polishing;piecewise linear function;electrical conductor;composite material;silicon	AI	-18.721941068441435	-92.8289192429681	46047
0cd954a0270087222f3244566156b64255b8d38d	extending the task of diarization to speaker attribution	diarization;090609 signal processing;clustering;speaker attribution;joint factor analysis;cross likelihood ration	In this paper we extend the concept of speaker annotation within a single-recording, or speaker diarization, to a collection wide approach we call speaker attribution. Accordingly, speaker attribution is the task of clustering expectantly homogenous intersession clusters obtained using diarization according to common cross-recording identities. The result of attribution is a collection of spoken audio across multiple recordings attributed to speaker identities. In this paper, an attribution system is proposed using mean-only MAP adaptation of a combined-gender UBM to model clusters from a perfect diarization system, as well as a JFA-based system with session variability compensation. The normalized cross-likelihood ratio is calculated for each pair of clusters to construct an attribution matrix and the complete linkage algorithm is employed to conduct clustering of the inter-session clusters. A matched cluster purity and coverage of 87.1% was obtained on the NIST 2008 SRE corpus.	algorithm;cluster analysis;linkage (software);pure function;spatial variability;speaker diarisation	Houman Ghaemmaghami;David Dean;Robbie Vogt;Sridha Sridharan	2011			psychology;speaker diarisation;speech recognition;pattern recognition;communication	NLP	-14.925370050366562	-89.37330304691265	46068
8a4b0dd0e2b52011b9db3d22b11853afbda45f50	eeg probability mapping while listening to a text: a group and a single case study			electroencephalography	P. Rappelsberger;D. Lacroix;K. Steinberger;K. Thau;H. Petsche	1991		10.1007/978-3-642-93503-9_181	natural language processing;speech recognition	NLP	-7.195768834471039	-83.67169089795367	46077
21e048c767930d41a2b6447fade7f381899e1824	restoring the residual speaker information in total variability modeling for speaker verification.	i-vector;residual space;score combination;speaker verification;total variability	In this paper, we introduce the residual space into the Total Variability Modeling by assuming that the speaker super-vectors are not totally contained in a linear subspace of low dimension. Thus the feature reduction carried out by Probabilistic Principal Component Analysis(PPCA) leads to information loss including information of speaker as well as channel. We add the residual factor to restore the missing speaker information which is lost during the PPCA process. To utilize the recovered information effectively, we propose two fusion methods that combine the principal components with the residual factor. We compare the fusion results that are obtained with direct scoring and Support Vector Machines for classification, respectively. The experiments on NIST SRE 2006 show that the performance can be improved consistently by involving the residual factor, e.g. the best result achieves 6% relative improvement on Equal Error Rate(EER) compared to the baseline system.	baseline (configuration management);experiment;heart rate variability;speaker recognition;support vector machine	Ce Zhang;Rong Zheng;Bo Xu	2011			machine learning;pattern recognition	Vision	-14.61746765624948	-91.66371930234176	46193
f6e27c6a5e0161ecc635eab2100ea6573f1b0bda	a new training method for multi-phone speech units for use in a hidden markov model speech recognition system	hidden markov model;speech recognition		hidden markov model;markov chain;speech recognition;teaching method	Jade Goldstein-Stewart;Akio Amano;Hideki Murayama;Mariko Izawa;Akira Ichikawa	1990			phone;artificial intelligence;sequence labeling;viterbi algorithm;hidden markov model;pattern recognition;speech recognition;acoustic model;computer science;speaker recognition;markov model;maximum-entropy markov model	ML	-16.060114105743093	-87.08231193465484	46262
52e600f221e98a41a58cd3aa60a63a8d3f6c95ce	windowing functions for the average magnitude difference function pitch extractor	upper bound speech computer buffers history jitter steady state equations uncertainty frequency government;history;uncertainty;government;speech;upper bound;jitter;frequency;computer buffers;steady state	This paper describes the fundamental properties of the Average Magnitude Difference Function, (AMDF) when used for pitch extraction on human speech, such as the ability to work in the absence of the fundamentals, and responses to pitch and formant resonances. Requirements for the algorithm's correct functioning are discussed, including reasonable bounds on window size and requirements on a nonlinear post processor. Finally, three experiments are described: 1) comparison of rectangular window versus exponential window AMDF, 2) comparison of the accuracy of the indicated pitch versus window length, and 3) effects of inverse filtering on the AMDF.	pitch (music);randomness extractor;window function	B. A. Fette;R. Gibson;E. Greenwood	1980		10.1109/ICASSP.1980.1170961	speech recognition;jitter;uncertainty;computer science;speech;frequency;mathematics;steady state;government;statistics	ML	-10.72889854897721	-84.23183193147554	46344
a3cb6659695fe5d9875b932fb43f65e21940a4f6	toward a universal synthetic speech spoofing detection using phase information	databases;speech synthesis;blizzard challenge universal synthetic speech spoofing detection phase information speaker verification synthetic voice speech driven biometric access system synthetic speech detector standard sv system statistical text to speech systems gaussian mixture model based binary classifier gmm copy synthesized signals wall street journal database relative phase shift acoustic parameters canonical mel frequency cepstral coefficients parameters mfcc phase aware vocoders synthetic tts signals;training;speech;vocoders speech mel frequency cepstral coefficient harmonic analysis speech synthesis training databases;mel frequency cepstral coefficient;vocoders;vocoders biometrics access control gaussian processes mixture models speech synthesis;synthetic speech detection bio moda voi voice biometrics anti spoofing phase information;harmonic analysis	In the field of speaker verification (SV) it is nowadays feasible and relatively easy to create a synthetic voice to deceive a speech driven biometric access system. This paper presents a synthetic speech detector that can be connected at the front-end or at the back-end of a standard SV system, and that will protect it from spoofing attacks coming from state-of-the-art statistical Text to Speech (TTS) systems. The system described is a Gaussian Mixture Model (GMM) based binary classifier that uses natural and copy-synthesized signals obtained from the Wall Street Journal database to train the system models. Three different state-of-the-art vocoders are chosen and modeled using two sets of acoustic parameters: 1) relative phase shift and 2) canonical Mel Frequency Cepstral Coefficients (MFCC) parameters, as baseline. The vocoder dependency of the system and multivocoder modeling features are thoroughly studied. Additional phase-aware vocoders are also tested. Several experiments are carried out, showing that the phase-based parameters perform better and are able to cope with new unknown attacks. The final evaluations, testing synthetic TTS signals obtained from the Blizzard challenge, validate our proposal.	acoustic cryptanalysis;baseline (configuration management);binary classification;biometrics;discrete cosine transform;experiment;extrapolation;google map maker;mel-frequency cepstrum;mixture model;netware file system;sensor;solid-state drive;speaker recognition;speech synthesis;spoofing attack;synthetic intelligence;systemverilog;the wall street journal;vocoder;web server	Jon Sánchez;Ibon Saratxaga;Inma Hernáez;Eva Navas;Daniel Erro;Tuomo Raitio	2015	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2015.2398812	voice activity detection;speech recognition;computer science;speech;harmonic analysis;speech processing;speech synthesis	Vision	-11.16070448988837	-92.20848704711923	46433
5206010c13b9664c3e876782717358688214748c	method of moments learning for left-to-right hidden markov models	method of moments;biological system modeling;maximum likelihood estimation;hidden markov models signal processing algorithms method of moments indexes biological system modeling maximum likelihood estimation;indexes;hidden markov models;signal processing algorithms;left to right hidden markov models method of moments	We propose a method-of-moments algorithm for parameter learning in Left-to-Right Hidden Markov Models. Compared to the conventional Expectation Maximization approach, the proposed algorithm is computationally more efficient, and hence more appropriate for large datasets. It is also asymptotically guaranteed to estimate the correct parameters. We show the validity of our approach with a synthetic data experiment and a word utterance onset detection experiment.	expectation–maximization algorithm;hidden markov model;markov chain;onset (audio);synthetic data	Y. Cem Sübakan;Johannes Traa;Paris Smaragdis;Daniel J. Hsu	2015	2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)	10.1109/WASPAA.2015.7336940	forward algorithm;database index;markov chain;maximum-entropy markov model;markov kernel;method of moments;variable-order bayesian network;markov property;viterbi algorithm;computer science;generalized method of moments;machine learning;hidden semi-markov model;forward–backward algorithm;pattern recognition;mathematics;maximum likelihood;markov algorithm;markov process;markov model;hidden markov model;statistics;variable-order markov model	ML	-19.613436779521106	-92.8295453393936	46438
8e9137d8acebe03d62637cc968ce79ca73b20c48	optimal unit stitching in a unit selection singing synthesis system		Unit Selection based speech synthesis systems are currently the best performing, producing natural sounding speech with minimal CPU load. One of the important reasons behind their success is the amount of recordings that are now commonly used in synthesis applications. However, in the case of singing applications, it is quite hard for a database to cover a large phonetic space due to the relative inefficiency of the recording process. Thus, due to the reduced catalogue of units, singing unit selection systems are more likely to produce spectral discontinuity artefacts. Taking advantage of the quasi stable nature of articulation during singing, we propose a novel unit stitching method. The method was implemented into the system that was used for the ”Fill-In the Gap” Singing Synthesis Challenge.	automatic sounding;biconnected component;central processing unit;database;reflections of signals on conducting lines;speech synthesis	Marius Cotescu	2016		10.21437/Interspeech.2016-1390	speech recognition	SE	-9.90319898234397	-87.27494976237966	46450
57922b10ce0465352210bf6d0b82786d0df8432d	fuzzy logic speech/non-speech discrimination for noise robust speech processing	metodo cuadrado menor;gradient descent method;zero crossing rate;speech transmission;processus gauss;regle inference;transmission parole;evaluation performance;methode moindre carre;sistema experto;base de connaissances;performance evaluation;protocole transmission;least squares method;automovil;metodo descenso;gradient method;evaluacion prestacion;speech processing;rule based;subband decomposition;logique floue;database;tratamiento palabra;traitement parole;base dato;logica difusa;least square method;backpropagation;noise robustness;fuzzy logic;inference rule;methode gradient;retropropagation;protocolo transmision;gradient bajada;subbanda;descomposicion subbanda;metodo gradiente;gradient descent;automobile;fonction appartenance;backpropagation algorithm;subband;motor car;distributed speech recognition;descente gradient;membership function;base de donnees;algorithme retropropagation;base conocimiento;rapport signal bruit;relacion senal ruido;gaussian process;funcion pertenencia;systeme expert;decomposition sous bande;descent method;signal to noise ratio;transmision palabra;proceso gauss;discriminacion;retropropagacion;sous bande;fuzzy system;training algorithm;discrimination;methode descente;regla inferencia;knowledge base;expert system;algoritmo retropropagacion;transmission protocol	This paper shows a fuzzy logic speech/non-speech discrimination method for improving the performance of speech processing systems working in noise environments. The fuzzy system is based on a Sugeno inference engine with membership functions defined as combination of two Gaussian functions. The rule base consists of ten fuzzy if then statements defined in terms of the denoised subband signal-tonoise ratios (SNRs) and the zero crossing rates (ZCRs). Its operation is optimized by means of a hybrid training algorithm combining the leastsquares method and the backpropagation gradient descent method for training membership function parameters. The experiments conducted on the Spanish SpeechDat-Car database shows that the proposed method yields clear improvements over a set of standardized VADs for discontinuous transmission (DTX) and distributed speech recognition (DSR) and also over recently published VAD methods.	ansi escape code;adaptive multi-rate audio codec;algorithm;analog front-end;backpropagation;dtx (form factor);discontinuous transmission;experiment;fuzzy control system;fuzzy logic;g.729;gradient descent;inference engine;least squares;rule-based system;serial digital video out;signal-to-noise ratio;speech processing;speech recognition;voice activity detection;zero crossing	Rafael Culebras;Javier Ramírez;Juan Manuel Górriz;José C. Segura	2006		10.1007/11758501_55	gradient descent;knowledge base;speech recognition;computer science;artificial intelligence;fuzzy number;backpropagation;speech processing;mathematics;least squares;expert system;algorithm	AI	-13.490352911683038	-96.34281100018674	46460
38101b154051c55b6506b615946d249e6c98dfba	computer-aided mandarin pronunciation learning system		In this era of globalization, learning multiple languages is becoming necessary. Learning to speak a new language involves knowing how to correctly pronounce words. In many cases, corrections to pronunciation mistakes come from language teachers who can typically give students only limited time and attention. With the rapid development of automatic speech recognition (ASR) technologies, computer can now accurately transcribe spoken words.	speech recognition;super robot monkey team hyperforce go!	Man-Hung Siu;Ka-Ming Wong;Man-Yan Ching;Mei-Sum Lau	2000			mandarin chinese;computer-aided;speech recognition;computer science;pronunciation	NLP	-20.446304260548562	-84.96063242557172	46523
32e922faa0c8c2556a00c45e8375cf09070c7e05	multi-task learning for speech recognition: an overview		Generalization is a common issue for automatic speech recognition. A successful method used to improve recognition results consists of training a single system to solve multiple related tasks in parallel. This overview investigates which auxiliary tasks are helpful for speech recognition when multi-task learning is applied on a deep learning based acoustic model. The impact of multi-task learning on speech recognition related tasks, such as speaker adaptation, or robustness to noise, is also examined.	acoustic cryptanalysis;acoustic model;computer multitasking;deep learning;multi-task learning;speech recognition	Gueorgui Pironkov;Stéphane Dupont;Thierry Dutoit	2016			speech technology;speech recognition	NLP	-15.931519822822619	-89.06509834678313	46555
4d15fe5bc46efa9514d72f97d0a640c2f74db6ea	topic extraction with multiple topic-words in broadcast-news speech	broadcast news;statistical measures;broadcasting data mining speech recognition frequency mutual information information retrieval indexing content based retrieval hidden markov models natural languages;newspapers;information retrieval;χ 2 based model;natural languages;data mining;spl chi sup 2 based model newspapers multiple topic words continuous speech recognition japanese broadcast news speech news content topic extraction model headlines articles relevance statistical measures mutual information experiments;continuous speech recognition;multiple topic words;japanese broadcast news speech;hidden markov models;statistical analysis;indexing;news content;feature extraction;experiments;articles;speech recognition;mutual information;statistical analysis broadcasting speech recognition information retrieval natural languages information theory feature extraction;topic extraction model;relevance;broadcasting;frequency;content based retrieval;information theory;headlines	This paper reports on topic extraction in Japanese broadcastnews speech. We studied, using continuous speech recognition, the extraction of several topic-words from broadcast-news. A combination of multiple topic-words represents the content of the news. This is a more detailed and more flexible approach than using a single word or a single category. A topic-extraction model shows the degree of relevance between each topic-word and each word in the article. For all words in an article, topic-words which have high total relevance score are extracted. We trained the topicextraction model with five years of newspapers, using the frequency of topic-words taken from headlines and words in articles. The degree of relevance between topic-words and words in articles is calculated on the basis of statistical measures, i.e., mutual information or the χ-value. In topicextraction experiments for recognized broadcast-news speech, we extracted five topic-words from the 10-best hypotheses using a χ-based model and found that 76.6% of them agreed with the topic-words chosen by subjects.	experiment;mutual information;off topic;relevance;speech recognition	Katsutoshi Ohtsuki;T. Matsutoka;Shoichi Matsunaga;Sadaoki Furui	1998		10.1109/ICASSP.1998.674434	natural language processing;search engine indexing;speech recognition;relevance;information theory;feature extraction;computer science;frequency;pattern recognition;natural language;mutual information;broadcasting;statistics	NLP	-22.610408111099975	-83.07726775443066	46570
f63dfb6bac906798a52bf992f3ec44977a12cc0b	a rapid adaptation algorithm for tracking highly non-stationary noises based on bayesian inference for on-line spectral change point detection		This paper presents an innovative rapid adaptation technique for tracking highly non-stationary acoustic noises. The novelty of this technique is that it can detect the acoustic change points from the spectral characteristics of the observed speech signal in rapidly changing non-stationary acoustic environments. The proposed innovative noise tracking technique will be very suitable for joint additive and channel distortions compensation (JAC) for on-line automatic speech recognition (ASR). The Bayesian on-line change point detection (BOCPD) approach is used to implement this technique. The proposed algorithm is tested using highly non-stationary noisy speech samples from the Aurora2 speech database. Significant improvement in minimizing the delay in adaptation to new acoustic conditions is obtained for highly non-stationary noises compared to the most popular baseline noise tracking algorithm MCRA and its derivatives.	acoustic cryptanalysis;algorithm;baseline (configuration management);distortion;online and offline;speech recognition;stationary process;utility functions on indivisible goods	Md Foezur Rahman Chowdhury;Sid-Ahmed Selouani;Douglas D. O'Shaughnessy	2011			artificial intelligence;pattern recognition;change detection;computer science;machine learning;bayesian inference	Vision	-13.341844290235962	-93.78179897132965	46589
410eda4c3b1b564a441fb5da7096bbc09f1bdbe9	automatic speaker recognition using time alignment of spectrograms	automatic speaker recognition	Abstract   New techniques for automatic speaker recognition from telephone speech are described. The recognition is based on spectral analysis of fixed sentence-long utterances. From the whole utterance the form the time-dependent spectrogram. Normalization procedures are applied to the spectrogram to take account of spectral distortions introduced by the telephone transmission system and of amplitude variations in the utterance. The decision on the speaker's identity is arrived at by comparing the spectrogram of a sample utterance with stored reference spectrograms and calculating a measure of dissimilarity between the spectrograms. To perform this spectrogram comparison, it is necessary to take into account differences in speaking rate and to bring corresponding speech events of the utterances into exact time synchronisation. The details of this so-called time alignment are described with respect to the requirements of speaker recognition. Time alignment is based on a measure of dissimilarity between speech events and is carried out by a dynamic programming algorithm which minimizes timing differences between corresponding speech events.  A set of utterances spoken by cooperative speakers and transmitted via conventional dialed-up telephone lines was used for the evaluation of the system. Different versions of the recognition procedure were evaluated and compared. For identification as well as for verification, error rates of 2% or less have been obtained.		Hermann Ney	1982	Speech Communication	10.1016/0167-6393(82)90033-4	speaker recognition;speaker diarisation;speech recognition;computer science;pattern recognition	ML	-11.90385126187776	-90.44999605828454	46724
ac3419e5d9d7b86c731a144dac6099e9d419ac96	maximum a posteriori linear regression for speaker adaptation with the prior of mean	silicon;speech;hidden markov models;estimation;covariance matrices;speech recognition;adaptation models	An efficient method for speaker adaptation (SA) is proposed in this paper. Let the relationship between the mean parameters of adapted model and the mean parameters of the speaker independent (SI) model be represented by sets of linear transformations like that of maximum likelihood linear regression (MLLR) approach, we try to estimate the transformations by maximum a posteriori (MAP) criterion. The prior mean distribution is considered in the estimation. The experiments on Mandarin speech recognition show the proposed approach is superior to the MLLR approach when only little speech is available for speaker adaptation.	experiment;speech recognition;super robot monkey team hyperforce go!	Chih-Heng Lin;Wern-Jun Wang	2000	2000 10th European Signal Processing Conference		speech recognition;computer science;machine learning;pattern recognition	ML	-18.15237058922576	-92.0364953363719	46767
b80ca42dbf51bd85c4a8c831147c6b4a73d18a8e	classification of myoelectric signal for sub-vocal hindi phoneme speech recognition				Munna Khan;Mosarrat Jahan	2018	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-161067	hindi;mathematics;speech recognition	Robotics	-14.231765196387473	-86.88204910299073	46904
62c6b608dfb23c66f8ebeec21206c35536bcc840	speaker classification using composite hypothesis testing and list decoding	verification;test hypothese;desciframiento;list decoding;speaker identification;gaussian mixture;equal error rate speaker classification composite hypothesis testing hypothesis testing list decoding j simple hypotheses a prior distribution nontarget speaker gaussian mixtures process speaker verification;decodage;decoding;gaussian processes;decoding hidden markov models signal processing system testing nist databases parameter estimation maximum likelihood estimation random variables speaker recognition;database management systems;bayes methods;test hipotesis;prior distribution;maximum likelihood estimation;classification;speaker verification;speaker recognition;identification;equal error rate;reconnaissance locuteur;error statistics;identificacion;composite hypothesis;verificacion;speaker recognition composite hypothesis list decoding;clasificacion;decision rule;hypothesis test;national institute of standards and technology;maximum likelihood estimation speaker recognition gaussian processes decoding database management systems error statistics bayes methods	Speaker classification is seen as a hypothesis testing problem of J simple hypotheses and a composite hypothesis. The simple hypotheses represent target speakers while the composite hypothesis represents nontarget speakers. The simple hypotheses have well-defined distributions that are estimated from training signals. The distribution of the signal under the composite hypothesis is assumed to belong to a given family. The parameter of that distribution is assumed random with a prior distribution that is estimated from a large set of speakers. This formulation converts the problem to that of testing J+1 simple hypotheses. Signals corresponding to target and nontarget speakers are assumed Gaussian mixtures processes. Once the system has been trained, list decoding is applied in which a test signal is associated with a list of possible speakers. The probability that the correct speaker is on the list is maximized for a given average number of incorrect speakers on the list. Results from speaker identification and speaker verification experiments are reported. In speaker identification using a National Institute of Standards and Technology (NIST) database with 174 target speakers, over 77% correct identification was achieved for an average of less than two erroneous speakers on the list. Speaker verification experiments on a similar database yielded results, expressed in terms of the equal-error-rate, of 6.7% and 10.1% using two decision rules.	boolean algebra;experiment;list decoding;speaker recognition	William J. J. Roberts;Yariv Ephraim;Howard W. Sabrin	2005	IEEE Transactions on Speech and Audio Processing	10.1109/TSA.2004.838536	identification;list decoding;speaker recognition;statistical hypothesis testing;verification;speech recognition;prior probability;biological classification;computer science;pattern recognition;decision rule;gaussian process;mathematics;maximum likelihood;statistics	ML	-20.74823647168903	-92.43326684692755	46926
40f62c6c1e61e2c20d11bd2c27542972b994eb9a	generating electronica: a virtual producer and virtual dj	computational creativity;electronic dance music;generative music	We present a virtual Electronic Dance Music (EDM) producer that generates stylistically accurate Breakbeat music. GESMI, the Generative Electronica Statistical Modeling Instrument, uses a corpus of human-analysed Breakbeat tracks dating from 1994--2010: the system learns relevant musical information in order to produce a wide variety of music within the genre using knowledge culled from the transcriptions.	statistical model;text corpus;virtual dj	Arne Eigenfeldt	2013		10.1145/2466627.2481234	computational creativity;programming;electronic dance music;computer science;music visualization;multimedia;pop music automation	Web+IR	-30.053304899757894	-88.61658904489711	46929
9b7865a408d5bd5007b21ebf23ccf5e6b8120a0c	audio-visual voice activity detection using diffusion maps	diffusion maps;mouth;detectors;speech processing;speech;audio visual speech processing;transient analysis;voice activity detection audio visual speech processing diffusion maps;video signals audio signals audio visual systems feature extraction learning artificial intelligence speech recognition;speech lips detectors noise transient analysis mouth speech processing;lips;voice activity detection;noise;adjacent frames audio visual voice activity detection diffusion maps nonstationary noise transient interferences video signal acoustic environment audio signals supervised learning procedure labeled training data set feature extraction procedure low dimensional representation	The performance of traditional voice activity detectors significantly deteriorates in the presence of highly nonstationary noise and transient interferences. One solution is to incorporate a video signal which is invariant to the acoustic environment. Although several voice activity detectors based on the video signal were recently presented, merely few detectors which are based on both the audio and the video signals exist in the literature to date. In this paper, we present an audio-visual voice activity detector and show that the incorporation of both audio and video signals is highly beneficial for voice activity detection. The algorithm is based on a supervised learning procedure, and a labeled training data set is considered. The algorithm comprises a feature extraction procedure, where the features are designed to separate speech from nonspeech frames. Diffusion maps is applied separately and similarly to the features of each modality and builds a low dimensional representation. Using the new representation, we propose a measure for voice activity which is based on a supervised learning procedure and the variability between adjacent frames in time. The measures of the two modalities are merged to provide voice activity detection based on both the audio and the video signals. Experimental results demonstrate the improved performance of the proposed algorithm compared to state-of-the-art detectors.	acoustic cryptanalysis;algorithm;diffusion map;feature extraction;heart rate variability;modality (human–computer interaction);sensor;spatial variability;supervised learning;test set;transient (computer programming);voice activity detection	David Dov;Ronen Talmon;Israel Cohen	2015	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2015.2405481	voice activity detection;diffusion map;detector;g.729;audio mining;speech recognition;acoustics;computer science;noise;speech;speech coding;speech processing;acoustic model;audio signal flow	Vision	-12.38398260757689	-91.88174406888388	46963
37e712f6662894801866c3bd588a9035b60a944d	gabor filterbank features for robust speech recognition		Several research studies have shown that the robustness and performance of speech recognition systems can be improved using phy- siologically inspired filterbank based on Gabor filters. In this paper, we proposed a feature extraction method based on 59 two-dimensional Ga- bor filterbank. The use of these set of filters aims to extracting specific modulation frequencies and limiting the redundancy on feature level. The recognition performance of our feature extraction method is evaluated in isolated words extracted from TIMIT corpus. The obtained results de- monstrate that the proposed extraction method gives better recognition rates to those obtained using the classic methods MFCC, PLP and LPC.	filter bank;gabor filter;speech recognition	Ibrahim Missaoui;Zied Lachiri	2014		10.1007/978-3-319-07998-1_76	speech recognition;computer science;pattern recognition	ML	-12.578257938540705	-90.12367121328238	46964
09437c537b6fa3990cba8b8e8528cfc9e015f99d	joint optimization of anatomical and gestural parameters in a physical vocal tract model	lips optimization anatomical parameters gestural parameters physical vocal tract model acoustic information target speaker matching x ray information mri information analysis by synthesis approach vocaltractlab vtl acoustic distance objective function ground truth parameters teeth;speech synthesis optimisation speech;acoustics;optimization shape acoustics adaptation models computational modeling speech linear programming;speaker inversion optimization vocal tract model gestures;speech;computational modeling;shape;linear programming;optimization;adaptation models	We describe a method for adapting a physical vocal tract model's anatomical and gestural parameters using acoustic information to match a target speaker. Physical vocal tract models are hard to adjust to match a speaker, as doing so requires information which is difficult to capture, such as X-Ray or MRI information. We propose an analysis-by-synthesis approach to adjust the parameters of the VocalTractLab (VTL) physical vocal tract model, optimizing on an acoustic distance objective function. We compare our method with one which does not adjust anatomy parameters, just gestural parameters, and find that the proposed method results in a net improvement. We also test our method's ability to recreate a synthetic speaker for which the ground truth parameters are known, and find that the method can reproduce the speaker if parameters pertaining to teeth and lips are fixed.	acoustic cryptanalysis;computational anatomy;ground truth;loss function;mathematical optimization;optimization problem;speech coding;synthetic intelligence;tract (literature);virtual tape library;x-ray (amazon kindle)	Christopher Liberatore;Ricardo Gutierrez-Osuna	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178772	speech recognition;shape;computer science;linear programming;speech;mathematics;computational model	Vision	-9.146320407653386	-86.62361497320526	46968
aacc0c39cec62263c14391e89dd76eb55ce51dfa	whispered speech recognition using deep denoising autoencoder	automatic speech recognition asr;deep learning;whispered speech;deep denoising autoencoder ddae	Recently Deep Denoising Autoencoders (DDAE) have shown state-of-the-art performance on various machine learning tasks. In this paper, the authors extended this approach to whispered speech recognition which is one of the most challenging problems in Automatic Speech Recognition (ASR). Namely, due to the profound differences between acoustic characteristics of neutral and whispered speech, the performance of traditional ASR systems trained on neutral speech degrades significantly when whisper is applied. This mismatch between training and testing is successfully alleviated with the new proposed system based on deep learning, where DDAE is applied for generating whisper-robust cepstral features. This system was tested and compared in terms of word recognition accuracy with conventional Hidden Markov Model (HMM) speech recognizer in an isolated word recognition task with a real database of whispered speech (WhiSpe). Three types of cepstral coefficients were used in the experiments: MFCC (Mel-Frequency Cepstral Coefficients), TECC (Teager-Energy Cepstral Coefficients) and TEMFCC (Teager-based Mel-Frequency Cepstral Coefficients). The experimental results showed that the proposed system significantly improves whisper recognition accuracy and outperforms traditional HMM-MFCC baseline, resulting in an absolute 31% improvement of whisper recognition accuracy. The highest word recognition rate of 92.81% in whispered speech was achieved with TECC feature.	acoustic cryptanalysis;autoencoder;automated system recovery;baseline (configuration management);coefficient;computer vision;deep learning;experiment;finite-state machine;google map maker;htk (software);hidden markov model;information retrieval;machine learning;markov chain;mel-frequency cepstrum;natural language;noise reduction;optical character recognition;performance;speech recognition;word error rate	Dorde T. Grozdic;Slobodan Jovicic;Misko Subotic	2017	Eng. Appl. of AI	10.1016/j.engappai.2016.12.012	speech recognition;computer science;machine learning;pattern recognition;deep learning	AI	-16.419455075698494	-89.0530334261698	47156
0988bcb09c182017edea462e19b125657aadcf23	a hierarchical, context-dependent neural network architecture for improved phone recognition	mlp;hmm ann;decoding;neural nets;hidden markov model;training;phone recognition;timit phone recognition mlp hmm ann bottleneck;error analysis;bottleneck technique context dependent neural network architecture phone recognition hmm ann hybrid model timit database;artificial neural networks;hidden markov models;hidden markov models training artificial neural networks error analysis speech recognition neurons decoding;speech recognition hidden markov models neural nets;speech recognition;context dependent;neurons;timit;bottleneck;artificial neural network;neural network	In this paper we combine three simple refinements proposed recently to improve HMM/ANN hybrid models. The first refinement is to apply a hierarchy of two nets, where the second net models the contextual relations of the state posteriors produced by the first network. The second idea is to train the network on context-dependent units (HMM states) instead of context-independent phones or phone states. As the latter refinement results in a lot of output neurons, combining the two methods directly would be problematic. Hence the third trick is to shrink the output layer of the first net using the bottleneck technique before applying the second net on top of it. The phone recognition results obtained on the TIMIT database demonstrate that both the context-dependent and the 2-stage modeling methods can bring about marked improvements. Using them in combination, however, results in a further significant gain in accuracy. With the bottleneck technique a further improvement can be obtained, especially when the number of context-dependent units is large.	artificial neural network;context-sensitive language;hidden markov model;network architecture;refinement (computing);timit	László Tóth	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947489	speech recognition;computer science;machine learning;context-dependent memory;pattern recognition;artificial neural network	Vision	-17.516165041889323	-88.55983953607081	47227
05be39faffa9e8c9d2daf687e529b1c1695164e7	inter-language interference in vot production by l2-dominant bilinguals: asymmetries in phonetic code-switching	biological patents;language use;biomedical journals;the australian standard research classification 210000 science general;text mining;europe pubmed central;citation search;voice onset time;citation networks;research articles;second language;abstracts;open access;life sciences;clinical guidelines;full text;speech production;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Speech production research has demonstrated that the first language (L1) often interferes with production in bilinguals' second language (L2), but it has been suggested that bilinguals who are L2-dominant are the most likely to suppress this L1-interference. While prolonged contextual changes in bilinguals' language use (e.g., stays overseas) are known to result in L1 and L2 phonetic shifts, code-switching provides the unique opportunity of observing the immediate phonetic effects of L1-L2 interaction. We measured the voice onset times (VOTs) of Greek-English bilinguals' productions of /b, d, p, t/ in initial and medial contexts, first in either a Greek or English unilingual mode, and in a later session when they produced the same target pseudowords as a code-switch from the opposing language. Compared to a unilingual mode, all English stops produced as code-switches from Greek, regardless of context, had more Greek-like VOTs. In contrast, Greek stops showed no shift toward English VOTs, with the exception of medial voiced stops. Under the specifically interlanguage condition of code-switching we have demonstrated a pervasive influence of the L1 even in L2-dominant individuals.	adult-onset still disease;interference (communication);medial graph;network switch;onset (audio);switch device component	Mark Antoniou;Catherine T. Best;Michael D. Tyler;Christian Kroos	2011	Journal of phonetics	10.1016/j.wocn.2011.03.001	psychology;speech production;text mining;speech recognition;computer science;linguistics;voice-onset time;communication	HCI	-10.995328381328248	-81.02831995299591	47235
b982b3948736bbb07291c763608a5caad505379e	multichannel text-to-speech system for electronic mail applications	text to speech		email;speech synthesis	Piero Pierucci;Enzo Mumolo;Corrado Labonia	1987			speech recognition;multimedia;computer science;speech synthesis	Theory	-15.177940487164058	-85.88775263513394	47287
21a0110d6188b917e8aa43e0a4df402b80b5e2e9	neural networks with hidden markov models in skeleton-based gesture recognition		In Gesture Recognition (GR) tasks, a system with a traditional use of Hidden Markov Models (HMMs) usually serves as a baseline. Their performance is often not so good and therefore somehow overlooked. However, in recent years, especially in Automatic Speech Recognition (ASR), there are advanced methods proposed for this type of model which have been shown to improve significantly recognition results. Among them, the use of Neural Networks (NNs) instead of Gaussian Mixture Models (GMMs) for estimating emission probabilities of HMMs has been considered as one of biggest advances [1,2,3]. This fact implies that the performance of HMM-based models on GR need to be revised. For this reason, in this study, we show that by carefully tailoring NNs to a traditional HMM-based GR system, we can improve significantly the performance, hence, achieving very competitive results on a skeleton-based GR task which is defined by using Microsoft Research Cambridge 12 (MSRC-12) data [4]. It should be pointed out that, it is straightforward to apply our proposed techniques to more complicated GR tasks such as Sign Language Recognition [5], where basically a sequence of sign gestures need to be transcribed.	artificial neural network;gesture recognition;hidden markov model;markov chain	Hai-Son Le;Ngoc-Quan Pham;Duc-Dung Nguyen	2014		10.1007/978-3-319-11680-8_24	speech recognition;signature recognition	ML	-18.298318344629536	-88.68999121480189	47322
0f6c93d29d2c79fa76f61156557c391f4910ffff	feature extraction for spectral continuity measures in concatenative speech synthesis	speech synthesis;feature extraction	The quality of concatenative speech synthesis depends on the cost function employed for unit selection. Effective cost functions for spectral continuity are difficult to define and standard measures often do not accurately reflect human perception of discontinuity across a concatenated join. In this study the performance of a number of standard distance measures are compared for the task of detecting audible discontinuities in concatenated speech. Feature sets derived from the phase spectrum are also investigated. Feature extraction based on wavelet analysis is proposed to overcome some of the limitations of the standard measures tested. Receiver Operating Characteristic (ROC) curves are constructed for each measure from the results of a perceptual experiment and are used to rank the performance of each measure. Results indicate that phase spectra is comparable to magnitude spectra as a join cost for spectral continuity. Measures based on wavelet transform coefficients outperform all other measures tested.	acoustic cryptanalysis;coefficient;concatenation;feature extraction;feature model;loss function;rca spectra 70;receiver operating characteristic;reflections of signals on conducting lines;scott continuity;sensor;spectral density;speech synthesis;stationary process;wavelet transform;window function	Barry Kirkpatrick;Darragh O'Brien;Ronan Scaife	2006			artificial intelligence;speech recognition;pattern recognition;feature extraction;computer science;speech synthesis	ML	-10.593494201346632	-90.47076515630695	47396
bd35b2e99d1226a3c91e8decf9cf6fad032e201d	the edinburgh speech production facility doubletalk corpus	ema;spontaneous speech;discourse	The DoubleTalk articulatory corpus was collected at the Edinburgh Speech Production Facility (ESPF) using two synchronized Carstens AG500 electromagnetic articulometers. The first release of the corpus comprises orthographic transcriptions aligned at phrasal level to EMA and audio data for each of 6 mixed-dialect speaker pairs. It is available from the ESPF online archive. A variety of tasks were used to elicit a wide range of speech styles, including monologue (a modified Comma Gets a Cure and spontaneous story-telling), structured spontaneous dialogue (Map Task and Diapix), a wordlist task, a memory-recall task, and a shadowing task. In this session we will demo the corpus with various examples.	archive;orthographic projection;speech corpus;spontaneous order;text corpus	James M. Scobbie;Alice Turk;Christian Geng;Simon King;Robin J. Lickley;Korin Richmond	2013			natural language processing;speech recognition;speech corpus;computer science;linguistics	NLP	-24.722385802850063	-84.32516962375584	47403
683248046fd04f4e45bf597d522e569496a9a0cc	neighboring digits pattern training method in quickly-spoken connected mandarin digits speech recognition	speaking rate;deletion errors;discriminability;neighboring digits pattern;hidden markov model hmm;mandarin digit string speech recognition mdssr	Deletion errors are most usually occurred in connected Mandarin digit string speech recognition when speaking rate is fast, and are the main reasons leading to the increasing of the recognition error rate and the decline of the recognition accuracy. In this paper, a new training method named neighboring digits pattern is given based on sufficient statistics of recognition errors of the traditional system in order to eliminate most of deletion errors which seriously affect the system recognition rate. The training process is presented and the performance evaluation is given. The result analysis demonstrates that the new method can reduce the deletion errors effectively and improve the system recognition rate from 96.4% to 98.3%.	cognitive dimensions of notations;computation;hidden markov model;performance evaluation;speech recognition;super robot monkey team hyperforce go!;teaching method	Chunyi Guo;Runzhi Li;Ming Fan;Kejun Liu	2011	Journal of Multimedia	10.4304/jmm.6.3.300-307	speech recognition;computer science;pattern recognition	AI	-19.164531710631678	-86.58073708898259	47886
92057fcdd4a75a6b5c2dbfb9532153b198aa9562	meeting recognition with asynchronous distributed microphone array using block-wise refinement of mask-based mvdr beamformer		This paper addresses a front-end system for speech recognition of spontaneous conversational speech signals that are recorded with asynchronous distributed microphones such as smartphones. In our previous work, we proposed combining blind synchronization and a state-of-the-art microphone array speech enhancement technique, e.g., a time-frequency mask based minimum variance distortionless response (MVDR) beamformer. This approach has provided reasonably high recognition performance even if we use asynchronous microphones. However, because the previous speech enhancement method was applied in a full-batch mode, it has been difficult to track speaker position movement in a real meeting conversation. To make it possible to handle the speaker movement, this paper describes our attempt to refine the mask-based MVDR beamformer in a blockwise manner, and reports that such a refinement reduces the word error rate from 31.4% to 28.8% for real meeting recordings.	batch processing;beamforming;end system;microphone;refinement (computing);smartphone;speech enhancement;speech recognition;spontaneous order;synchronization (computer science);word error rate	Shoko Araki;Nobutaka Ono;Keisuke Kinoshita;Marc Delcroix	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462458	conversation;time–frequency analysis;synchronization;artificial intelligence;word error rate;speech enhancement;pattern recognition;microphone array;asynchronous communication;computer science	Robotics	-11.485460912095654	-94.29755662217576	47912
31726efd66cf2f24b5b21ef8f02a9ce339ab6d62	auto-segmentation based vad for robust asr	auto-segmentation;endpointing;speech recognition;vad		automated system recovery;voice activity detection	Yu Shi;Frank K. Soong;Jian-Lai Zhou	2006			speech recognition;artificial intelligence;pattern recognition;computer science;segmentation	Crypto	-14.560563780320525	-87.59200835598412	48044
e65b0683d74b807718fd0e05de6614a93e23cef8	generalized time-series active search with kullback&#8211;leibler distance for audio fingerprinting	audio fingerprinting;distance measure;gaussian processes;audio search;time series;generation time;probabilistic model;audio coding;gaussian mixture model;statistical analysis;time series audio coding gaussian processes search problems statistical analysis;search problems;audio search audio fingerprinting;fingerprint recognition distortion measurement streaming media robustness acoustic distortion histograms digital audio players acoustic measurements search methods testing;gaussian mixture models generalized time series active search kullback leibler distance audio fingerprinting statistical fingerprint modeling;kullback leibler distance;kullback leibler	In this letter, a new audio fingerprinting approach is presented. We investigate to improve robustness by more precise statistical fingerprint modeling with common component Gaussian mixture models (CCGMMs) and Kullback-Leibler (KL) distance, which is more suitable to measure the dissimilarity between two probabilistic models. To address the resulting complexity, generalized time-series active search is proposed, which supports a wide variety of distance measures between two CCGMMs, including L1, L2, KL, etc. Experiments show that the new approach with KL distance increases robustness to distortions (including low-quality MP3 compression, small room echo, and play-and-record) while achieving efficient search	acoustic fingerprint;distortion;experiment;fingerprint (computing);kullback–leibler divergence;mp3;mixture model;plug and play;radio fingerprinting;statistical model;time series	Hui Lin;Zhijian Ou;Xi Xiao	2006	IEEE Signal Processing Letters	10.1109/LSP.2006.874394	speech recognition;pattern recognition;mathematics;kullback–leibler divergence;statistics	Vision	-8.217284525061105	-92.79922212056175	48060
f63e3ac6c185da7041d9826fe9864b6971fcd200	a clustering method and radius tuning by end users	ocr clustering method radius tuning end users intra class step inter class step training set best radius distance metrics prototype library japanese test set;distance metrics;radius tuning;top down;optical character recognition;clustering methods libraries optical character recognition software character recognition prototypes character generation laboratories optical tuning testing shape;intra class step;training set;end users;best radius;common criteria;clustering method;number of clusters;distance metric;ocr;japanese test set;prototype library;inter class step	"""In this paper we describe a top-down clustering method consisting of an intra class step and an inter class step. In the intra class step all the samples for each category are initially divided into a small number of clusters, then the largest cluster is split and its members reallocated. The largest cluster is decided based on a new concept, """"Volume"""" of a cluster that is a hybrid of existing two common criteria for splitting: number of members in a cluster, and variance of a cluster. In the inter class step recognition is done for all the training set to assign best radius to each prototype. The radii are used as a normalizing factor in the computation of distance metrics. In our experiments we generated a prototype library by clustering characters written by Americans. When we used another training set written by Japanese only for tuning radii of the American library, the recognition rate of Japanese test set increased from 87.9% to 92.1%. The radii can be tuned even by OCR end users when the application domain is quite different from that of the initial clustering by OCR developers."""	cluster analysis	Hiroyasu Takahashi;K. Moidin Mohiuddin	1995		10.1109/ICDAR.1995.601999	complete-linkage clustering;training set;end user;speech recognition;metric;k-medians clustering;computer science;machine learning;top-down and bottom-up design;optical character recognition	EDA	-23.301494835899625	-90.19928903057726	48090
3ba38bd669cffb9846a995960de68c0e31dccbb4	automatic gender recognition in normal and pathological speech		The design of voice pathology automatic detection systems is gaining attention in the last few years for presenting advantages compared to traditional diagnosis methods. However, the performance of these systems is influenced by aspects related to the inter-speaker variability, and specially to the heterogeneity introduce by gender differences. To overcome that, a gender recognizer may be employed as a preprocessing stage in order to stratify the speakers and further adjust the detectors to the specific characteristics of each target group. Nevertheless, the reliability of gender recognizers on pathological speech has not been investigated. Having this in mind, the present paper studies the effectiveness of an automatic gender recognizer, based on mel frequency cepstral coefficients and gaussian mixture models, on normal and pathological speech. The analysis is carried out parameterizing the speech, the glottal waveform extracted from speech via inverse filtering, and a vocal tract model. The experiments were carried out using sustained vowels taken from the Saarbrücken and UPM voice disorders databases, and suggest that the gender might be effectively classified when using the proposed methodology. They also suggest that gender recognizers can be successfully employed as a preprocessing stage for a more accurate design of gender-dependent pathology detection systems.	coefficient;database;experiment;finite-state machine;gender hci;inverse filter;mel-frequency cepstrum;mixture model;preprocessor;sensor;spatial variability;tract (literature);waveform	Jorge Andrés Gómez García;Juan Ignacio Godino-Llorente;Germán Castellanos-Domínguez	2013			speech recognition;pattern recognition;artificial intelligence;pathological;computer science	NLP	-10.556508944840608	-85.81185578217145	48108
73cb51dc3569b609625661775a4e960c06cbb9ab	recognition of emotions in interactive voice response systems	k nearest neighbor;support vector machine;neural network;decision tree	This paper reports emotion recognition results from speech signals, with particular focus on extracting emotion features from the short utterances typical of Interactive Voice Response (IVR) applications. We focus on distinguishing anger versus neutral speech, which is salient to call center applications. We report on classification of other types of emotions such as sadness, boredom, happy, and cold anger. We compare results from using neural networks, Support Vector Machines (SVM), K-Nearest Neighbors, and decision trees. We use a database from the Linguistic Data Consortium at University of Pennsylvania, which is recorded by 8 actors expressing 15 emotions. Results indicate that hot anger and neutral utterances can be distinguished with over 90% accuracy. We show results from recognizing other emotions. We also illustrate which emotions can be clustered together using the selected prosodic features.	artificial neural network;decision tree;emotion recognition;interactive voice response;k-nearest neighbors algorithm;linguistic data consortium;sadness;support vector machine	Sherif M. Yacoub;Steven J. Simske;Xiaofan Lin;John Burns	2003			speech recognition;support vector machine;decision tree;sadness;artificial neural network;interactive voice response;artificial intelligence;pattern recognition;k-nearest neighbors algorithm;anger;boredom;computer science	NLP	-12.42841532697932	-87.83718759674647	48134
84f9c93b4b6adbc1a970470afb9c993dbbccd5cd	automatic scoring of short handwritten essays in reading comprehension tests	analisis imagen;lenguaje natural;image recognition;reconocimiento imagen;document image analysis;handwriting recognition;document analysis;image processing;caracter manuscrito;lexicon;manuscript character;heuristic method;extraction forme;langage naturel;reading comprehension;procesamiento imagen;hombre;automatic essay scoring;metodo heuristico;intelligence artificielle;test bed;segmentation;computational method;coupling method;traitement image;enfant;artificial neural networks;analyse documentaire;reconnaissance ecriture;automatic recognition;extraccion forma;nino;feature extraction;natural language;human;reconnaissance image;palabra;child;word recognition;error rate;analisis documental;artificial intelligence;image analysis;word;methode heuristique;inteligencia artificial;contextual handwriting recognition;lexico;reseau neuronal;analisis semantico;analyse semantique;caractere manuscrit;analyse image;latent semantic analysis;pattern extraction;red neuronal;segmentacion;reconocimiento automatico;semantic analysis;artificial neural network;mot;reconnaissance automatique;homme;neural network;lexique	Reading comprehension is largely tested in schools using handwritten responses. The paper describes computational methods of scoring such responses using handwriting recognition and automatic essay scoring technologies. The goal is to assign to each handwritten response a score which is comparable to that of a human scorer even though machine handwriting recognition methods have high transcription error rates. The approaches are based on coupling methods of document image analysis and recognition together with those of automated essay scoring. Document image-level operations include: removal of pre-printed matter, segmentation of handwritten text lines and extraction of words. Handwriting recognition is based on a fusion of analytic and holistic methods together with contextual processing based on trigrams. The lexicons to recognize handwritten words are derived from the reading passage, the testing prompt, answer rubric and student responses. Recognition methods utilize children's handwriting styles. Heuristics derived from reading comprehension research are employed to obtain additional scoring features. Results with two methods of essay scoring-both of which are based on learning from a human-scored set-are described. The first is based on latent semantic analysis (LSA), which requires a reasonable level of handwriting recognition performance. The second uses an artificial neural network (ANN) which is based on features extracted from the handwriting image. LSA requires the use of a large lexicon for recognizing the entire response whereas ANN only requires a small lexicon to populate its features thereby making it practical with current word recognition technology. A test-bed of essays written in response to prompts in statewide reading comprehension tests and scored by humans is used to train and evaluate the methods. End-to-end performance results are not far from automatic scoring based on perfect manual transcription, thereby demonstrating that handwritten essay scoring has practical potential.		Sargur N. Srihari;Jim Collins;Rohini K. Srihari;Harish Srinivasan;Shravya Shetty;Janina Brutt-Griffler	2008	Artif. Intell.	10.1016/j.artint.2007.06.005	natural language processing;el niño;speech recognition;latent semantic analysis;feature extraction;word recognition;intelligent character recognition;word error rate;computer science;artificial intelligence;machine learning;word;natural language;segmentation;artificial neural network;testbed	AI	-24.401403419785446	-81.57075820900202	48148
1f38d1f50f7b0ec82bb12ec387a621bd8072bb96	evaluating prosody of mandarin speech for language learning	reference data;regression tree;language learning	Abstract This paper proposes an approach to automatically evaluate the prosody of Chinese Mandarin speech for language learning. In this approach, we grade the appropriateness of prosody of speech units according to a model speech corpus from a teacher’s voice. To this end, we build two models, which are the prosody model and the scoring model. The prosody model that is built from the teacher’s speech predicts the reference prosody for the learning text. The scoring model compares the student’s prosody with the reference prosody and gives a prosody rating score. Both the prosody model and the scoring model are built using regression tree. To make the two prosodies comparable, we transform the student’s prosody into the teacher’s prosody space. To build the scoring model, we derive from the corpus a reference data set, in which prosody rating is associated with prosody parameters. During speech evaluation, the student’s prosody is first transformed into the teacher’s prosody space and then evaluated by the scoring model. Experiments show that our model works well for speech of new speakers.	semantic prosody;super robot monkey team hyperforce go!	Minghui Dong;Haizhou Li;Tin Lay Nwe	2006	Journal of Chinese Language and Computing		natural language processing;mandarin chinese;speech recognition;artificial intelligence;language acquisition;computer science;chinese speech synthesis;prosody	NLP	-18.00236610320032	-82.75935590726382	48155
13afbea8ddc73325bd7729c146f867090cc80d8d	effects of phrasal position and metrical structure on alignment patterns of nuclear pitch accents in german: acoustics and articulation		In this production study, we investigate the effects of phrasal position and metrical structure on alignment patterns of nuclear rising pitch accents in German. In the acoustic domain, peak alignment varies across open and closed syllables. In the articulatory domain, these effects of metrical structure disappear. The peak shows a stable coordination pattern with the vocalic target of the accented syllable. However, effects of phrasal position occurred in both domains leading to a leftward shift of the peak towards the accented vowel. We conclude that alignment is best understood as coordinative structures between tones and oral constriction gestures instead of cooccurrence of nearby-landmarks on the acoustic surface.	acoustic cryptanalysis;biconnected component;syllable	Henrik Niemann;Doris Mücke	2015			acoustics;linguistics;speech recognition;computer science;german	NLP	-10.58889652927563	-81.66499799509572	48156
0819682993c1a79561efa6fca29ad5dda582f5af	speaker identification under noisy environments by using harmonic structure extraction and reliable frame weighting	speaker identification;voice reliability;indexing terms;noise robustness;gaussian mixture model;error rate;voice extraction;extraction method	We present methods for automatic speaker identification in noisy environments. To improve noise robustness of speaker identification, we developed two methods, the harmonic structure extraction method and the reliable frame weighting method. The harmonic structure extraction method enables the speaker of input speech signals to be identified after environmental noise has been reduced. This method first extracts harmonic components of the speech from the sound mixtures and then resynthesizes a clean speech signal by using a sinusoidal model driven by harmonic components. The reliable frame weighting method then determines how each frame of the resynthesized speech is reliable (i.e. little influenced by environmental noises) by using two Gaussian mixture models for the speech and noise. The speaker can be robustly identified by attaching importance to reliable frames. Experimental results with thirty speakers showed that our method was able to reduce the influences of environmental noise and achieved an error rate of 10.7%, while the error rate for a conventional method was 18.9%.	mixture model;sinusoidal model;speaker recognition	Hiromasa Fujihara;Tetsuro Kitahara;Masataka Goto;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno	2006			speech recognition;index term;word error rate;computer science;pattern recognition;mixture model;world wide web	NLP	-13.337868359365029	-91.72614301976587	48190
bec304111ca5c29a1a0449e68acada0c9d135def	biometrics writer recognition for arabic language : analysis and classification techniques using subwords features	qa75 electronic computers computer science	......................................................................................................................... i Acknowledgment .......................................................................................................... v Abbreviations ............................................................................................................... vi Abbreviation................................................................................................................. vi Table of	acknowledgment index;biometrics;handwritten biometric recognition	Makki Jasim Radhi Maliki	2015			speech recognition;computer science;artificial intelligence;communication	NLP	-14.742979389095346	-86.1373076207395	48221
354a8e6bc85094200be4c9749c9c0b3e28ce2cf7	local pairwise linear discriminant analysis for speaker verification		Linear discriminant analysis—probabilistic linear discriminant analysis (LDA-PLDA) is a standard and effective backend in the field of speaker verification. The object of LDA is to perform dimensionality reduction while minimizing within-class covariance and maximizing between-class covariance. For a target class (or speaker), our task is to make a binary decision about whether a test utterance is from a specific target speaker. Generally, the nontarget test utterances that are close to the target speaker are easily misjudged. Inspired by this idea, we propose a local pairwise linear discriminant analysis (LPLDA) algorithm. This new method focuses on maximizing the local pairwise covariance, which represents the local structure between the target class samples and neighboring nontarget class samples, instead of the between-class covariance, which represents the global structure of the data. Experiments on the NIST SRE 2010, 2014, and 2016 database show that, the proposed LPLDA-PLDA backend has significant performance improvements over the LDA-PLDA backend.	algorithm;dimensionality reduction;linear discriminant analysis;speaker recognition	Liang He;Xianhong Chen;Can Xu;Jia Liu;Michael T. Johnson	2018	IEEE Signal Processing Letters	10.1109/LSP.2018.2869107	artificial intelligence;mathematics;dimensionality reduction;pattern recognition;nist;pairwise comparison;linear discriminant analysis;binary decision diagram;covariance	ML	-16.84549673397598	-92.66154470435657	48293
75170fb204c9d0c1d4547e4a35f8be2496d62913	discriminative linear transforms for feature normalization and speaker adaptation in hmm estimation	modelizacion;sistema interactivo;vocabulaire;linear estimation;transforms hidden markov models speaker recognition;speech recognition system discriminative linear transform feature normalization speaker adaptation hidden markov model estimation maximum mutual information criterion discriminative training procedure speaker adaptive training;correlation modeling;maximum mutual information;large vocabulary conversational speech recognition;learning;information mutuelle maximale;speech processing;vocabulary;estimacion lineal;modele markov variable cachee;tratamiento palabra;traitement parole;conversacion;vocabulario;probabilistic approach;systeme conversationnel;speaker recognition;aprendizaje;discriminant analysis;analyse discriminante;modelisation;estimation lineaire;analisis discriminante;apprentissage;hidden markov models;reconocimiento voz;speaker adaptive training;adaptacion locutor;interactive system;enfoque probabilista;approche probabiliste;discriminative training adaptive training correlation modeling;transformation lineaire;hidden markov models maximum likelihood estimation speech recognition linear discriminant analysis mutual information power system modeling loudspeakers automatic speech recognition vocabulary acoustic applications;transforms;estimacion parametro;conversation;linear transformation;speech recognition;discriminative training;reconnaissance parole;parameter estimation;estimation parametre;modeling;speaker adaptation;adaptive training;transformacion lineal;adaptation locuteur	Linear transforms have been used extensively for training and adaptation of HMM-based ASR systems. Recently procedures have been developed for the estimation of linear transforms under the Maximum Mutual Information (MMI) criterion. In this paper we introduce discriminative training procedures that employ linear transforms for feature normalization and for speaker adaptive training. We integrate these discriminative linear transforms into MMI estimation of HMM parameters for improvement of large vocabulary conversational speech recognition systems.	discriminative model;hidden markov model;mutual information;speech recognition;vocabulary	Stavros Tsakalidis;Vlasios Doumpiotis;William J. Byrne	2002	IEEE Transactions on Speech and Audio Processing	10.1109/TSA.2005.845806	speaker recognition;speech recognition;systems modeling;computer science;machine learning;pattern recognition;speech processing;linear map;estimation theory;hidden markov model	Vision	-20.070869112235112	-91.54121364195072	48637
d350a43e5f4c4842feff3bc1d074affb3af9c32d	a new system for text-to-speech conversion, and its application to swedish	text to speech		speech synthesis	Mats Ljungqvist;Anders Lindström;Kjell Gustafson	1994			speech recognition;natural language processing;computer science;artificial intelligence;speech synthesis	NLP	-15.59758774111349	-85.46194034907604	48662
798ba1a698d8d56939c069e2cb5a194e5f69857c	an acoustic profile of consonant reduction	spectral analysis;speech processing;acoustic profile;consonant reduction;durational data;intervocalic consonants;intervocalic vowels;read speech;speaking style;spectral data;spontaneous speech	Vowel reduction has been studied for years. It is a universal phenomenon that reduces the distinction of vowels in informal speech and unstressed syllables. How consonants behave in situations where vowels are reduced is much less well known. In this paper we compare durational and spectral data (for both intervocalic consonants and vowels) segmented from read speech with otherwise identical segments from spontaneous speech. On a global level, it shows that consonants reduce like vowels when the speaking style becomes informal. On a more detailed level there are differences related to the type of the consonant.	acoustic cryptanalysis;natural language;spontaneous order	R. J. J. H. van Son;Louis C. W. Pols	1996			sampling;gravity;consonant harmony;speech processing;stress;obstruent;manner of articulation;speech synthesis	NLP	-10.715383335536467	-82.4468934336384	48703
009976ad3de9f93e98fd829e51f4b301c4c261fe	frequency content of breath pressure and implications for use in control	wind controller;pressure sensor;frequency response;breath control;breath sensors;similarity measure	The breath pressure signal applied to wind music instruments is generally considered to be a slowly varying function of time. In a context of music control, this assumption implies that a relatively low digital sample rate (100-200 Hz) is sufficient to capture and/or reproduce this signal. We tested this assumption by evaluating the frequency content in breath pressure, particularly during the use of extended performance techniques such as growling, humming, and flutter tonguing. Our results indicate frequency content in a breath pressure signal up to about 10 kHz, with especially significant energy within the first 1000 Hz. We further investigated the frequency response of several commercially available pressure sensors to assess their responsiveness to higher frequency breath signals. Though results were mixed, some devices were found capable of sensing frequencies up to at least 1.5 kHz. Finally, similar measurements were conducted with Yamaha WX11 and WX5 wind controllers and results suggest that their breath pressure outputs are sampled at about 320 Hz and 280 Hz, respectively.	frequency response;responsiveness;sampling (signal processing);sensor;wow and flutter measurement;yamaha v9958	Gary P. Scavone;Andrey R. da Silva	2005			frequency response;acoustics;pressure sensor	HCI	-5.141135236221719	-83.18695641728365	48711
33e78be9787e715c7bacde2165b1f85395a6d68c	a generalized reconstruction algorithm for ellipsis resolution	correspondence relation;syntactic reconstruction;ellipsis structure;ellipsis resolution;empty verbal head;appropriate interpretation;bare ellipsis;uniform computational approach;vp ellipsis;ellipsis phenomenon;unrealized head verb;generalized reconstruction algorithm	We present an algorithm which assigns interpretations to several major types of elli psis structures through a generalized procedure of syntactic reconstruction. Elli psis structures are taken to be sequences of lexically realized arguments and/or adjuncts of an empty verbal head. Reconstruction is characterized as the specification of a (partial) correspondence relation between the unrealized head verb of an elided clause and its argument and adjuncts on one hand, and the head of a non-elided antecedent sentence and its arguments and adjuncts on the other. The algorithm generates appropriate interpretations for cases of VP elli psis, pseudo-gapping, bare elli psis (stripping), and gapping. It provides a uniform computational approach to a wide range of elli psis phenomena, and it has significant advantages over several other approaches to elli psis which have recently been suggested in the computational and linguistic literature.	algorithm;computation;vp/css	Shalom Lappin;Hsue-Hueh Shih	1996			natural language processing;computer science;verb phrase ellipsis;linguistics;algorithm	NLP	-33.04289485026683	-81.96982631587828	48775
0df5faa913b75eaaefc809a4ebe56a57c413d5ad	emotional accompaniment generation system based on harmonic progression	correlacion;evaluation performance;onset rate;specified emotion emotional accompaniment generation system harmonic progression music piece onset rate emotion based accompaniment generation perfect positive spearman correlation perceived emotion;performance evaluation;evaluacion prestacion;acoustique musicale;armonica;harmonic;onset rate accompaniment harmonic progression melody music emotion;musical acoustics;musical instrument;instrumento musical;harmonique;emotion emotionality;acustica musical;music emotion;harmonic progression;tambor;instrument musique;emotion emotivite;melodia;emocion emotividad;correlation;tambour;music;melody;melodie;accompaniment;drum	A music piece consists of melody and accompaniment in many genres. In this paper, we present a system to automatically generate accompaniment that evokes specific emotions for a given melody. In particular, we propose harmonic progression and onset rate as two key features for emotion-based accompaniment generation. The former refers to the progression of chords, and the latter refers to the number of music events (such as notes and drums) in a unit time. The harmonic progression and the onset rate are altered according to the specified emotion represented by the valence and arousal parameters, respectively. The performance of the system is evaluated subjectively, and the result shows a perfect positive Spearman correlation between the specified emotion and the perceived emotion.	color gradient;onset (audio);table (database);vii	Pei-Chun Chen;Keng-Sheng Lin;Homer H. Chen	2013	IEEE Transactions on Multimedia	10.1109/TMM.2013.2267206	melody;speech recognition;tambour;harmonic progression;musical acoustics;harmonic;music;drum;correlation	SE	-8.902490790856923	-85.5909594831446	48856
9f5f31f3ad58195e86dc7da3456b467b79c4fae6	blind normalization of speech from different channels and speakers.	non linear dynamics;time series	This paper describes representations of time-dependent signals that are invariant under any invertible time-independent transformation of the signal time series. Such a representation is created by rescaling the signal in a non-linear dynamic manner that is determined by recently encountered signal levels. This technique may make it possible to normalize signals that are related by channel-dependent and speaker-dependent transformations, without having to characterize the form of the signal transformations, which remain unknown. The technique is illustrated by applying it to the time-dependent spectra of speech that has been filtered to simulate the effects of different channels. The experimental results show that the rescaled speech representations are largely normalized (i.e., channelindependent), despite the channel-dependence of the raw (unrescaled) speech.	nonlinear system;normalization (image processing);signal-to-noise ratio;simulation;time series	David N. Levin	2002	CoRR		normalization (statistics);machine learning;speech recognition;invertible matrix;artificial intelligence;nonlinear system;pattern recognition;invariant (mathematics);computer science;communication channel	ML	-10.22735572556494	-90.71739509227447	48893
2d92acd22fd5e8e3bb6feda20b2c75cc0c7a98f5	automatic learning: an approach to the adaptation of a speech recognition system to one or several speakers	reconnaissance multilocuteur;multispeaker recognition;learning;reconocimiento de la palabra;aprendizaje;apprentissage;mot isole;palabra aislada;speech recognition;reconnaissance parole;isolated word;speaker;locutor;locuteur	As part of a system for the automatic recognition of isolated words in a large vocabulary on the basis of an analytical approach, we considered the automatic speaker-adaptation of the system. This was carried out by means of an automatic learning procedure of the speakers' reference patterns, and by automatically adjusting the parameters of the system. This learning relies on a time alignment algorithm using acoustic-phonetic features which are little speakerdependent. The learning session was successfully tested on 18 speakers out of 20 (10 women and 10 men) and the reference patterns thus obtained yielded good results during the recognition phase. We have now undertaken an analysis of the vowels uttered by 15 speakers based upon descriptive statistics and statistical interpretation in order to design procedures of normalization and of automatic generation of a speaker's vowel reference patterns.		Christine Pister-Bourjot;Jean Paul Haton	1987	Speech Communication	10.1016/0167-6393(87)90068-9	loudspeaker;natural language processing;speech recognition;computer science;linguistics	NLP	-16.817021973866787	-83.68305490013003	49075
9641a1306660439fc435fafe823a9671851b5f33	modelling the influence of pitch duration on the induction of tonality from pitch-use		The patterns of duration of the pitches used in pieces of music are thought to be strongly correlated with prooles of pitch salience identiied with keys. However, it is not clear whether this correlation is causal. This report compares diierent ways of describing pitch duration in classifying pitch use into tonal centres.	causal filter;pitch (music);strongly correlated material	Niall J. L. Griffith	1994			tonality;acoustics;computer science	ML	-9.640401105143855	-81.63319863942247	49087
fb469ffcf976e4f3d94f2a9233813ca833a537a8	adaptation of hidden markov models using model-as-matrix representation	training;computational modeling;hidden markov models;vectors;mathematical model;adaptation models;two dimensional principal component analysis 2dpca generalized low rank approximations of matrices matrix variate distribution speaker adaptation speech recognition;hidden markov models adaptation models vectors training mathematical model covariance matrix computational modeling;covariance matrix	In this paper, we describe basis-based speaker adaptation techniques using the matrix representation of training models. Bases are obtained from training models by decomposition techniques for matrix-variate objects: two-dimensional principal component analysis (2DPCA) and generalized low rank approximations of matrices (GLRAM). The motivation for using matrix representation is that the sample covariance matrix of training models can be more accurately computed and the speaker weight becomes a matrix. Speaker adaptation equations are derived in the maximum-likelihood (ML) framework, and the adaptation equations can be solved using the maximum-likelihood linear regression technique. Additionally, novel applications of probabilistic 2DPCA and GLRAM to speaker adaptation are presented. From the probabilistic 2DPCA/GLRAM of training models, speaker adaptation equations are formulated in the maximum a posteriori (MAP) framework. The adaptation equations can be solved using the MAP linear regression technique. In the isolated-word experiments, the matrix representation-based methods in the ML and MAP frameworks outperformed maximum-likelihood linear regression adaptation, MAP adaptation, eigenvoice, and probabilistic PCA-based model for adaptation data longer than 20 s. Furthermore, the adaptation methods using probabilistic 2DPCA/GLRAM showed additional performance improvement over the adaptation methods using 2DPCA/GLRAM for small amounts of adaptation data.	approximation;experiment;hidden markov model;markov chain;matrix representation;principal component analysis;the matrix	Yongwon Jeong	2012	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2012.2202649	covariance matrix;speech recognition;machine learning;pattern recognition;mathematical model;mathematics;computational model;hidden markov model;statistics	ML	-18.17380596901159	-92.73840292085919	49184
8663b35bda4a12a7d3ee8e2ee31595c354e404a8	tempo- and transposition-invariant identification of piece and score position		We present an algorithm that, given a very small snippet of an audio performance and a database of musical scores, quickly identifies the piece and the position in the score. The algorithm is both tempoand transposition-invariant. We approach the problem by extending an existing tempoinvariant symbolic fingerprinting method, replacing the absolute pitch information in the fingerprints with a relative representation. Not surprisingly, this leads to a big decrease in the discriminative power of the fingerprints. To overcome this problem, we propose an additional verification step to filter out the introduced noise. Finally, we present a simple tracking algorithm that increases the retrieval precision for longer queries. Experiments show that both modifications improve the results, and make the new algorithm usable for a wide range of applications.	algorithm;experiment;fingerprint (computing)	Andreas Arzt;Gerhard Widmer;Reinhard Sonnleitner	2014			speech recognition;computer science;machine learning;data mining;world wide web	Vision	-7.604478964125397	-94.10120169033074	49186
7a05d35cbead001cdc55aa831501f0124c4ed26a	very low bit-rate f0 coding for phonetic vocoder using msd-hmm with quantized f0 context	quantization;very low bit rate f0 coding;speech synthesis;generic algorithm;hidden markov models speech quantization speech recognition speech coding context;hidden markov model;hmm based parameter generation algorithm very low bit rate f0 coding msd hmm quantized f0 context speaker dependent phonetic vocoder hidden markov model scalar quantization hmm based speech synthesis;speech;very low bit rate speech coding;speech coding;hmm based speech synthesis;scalar quantization;hidden markov models;multi space distribution hmm;vocoders;vocoders hidden markov models speech synthesis;msd hmm;speaker dependent;speech recognition;f0 context;context dependent;quantized f0 context;phonetic vocoder;context;multi space distribution hmm phonetic vocoder hmm based speech synthesis very low bit rate speech coding f0 context;speaker dependent phonetic vocoder;hmm based parameter generation algorithm	This paper presents a very low bit-rate F0 coding technique for speaker-dependent phonetic vocoder based on hidden Markov model (HMM) using quantized F0 context. In the proposed technique, the input F0 sequence is converted into F0 symbol sequence at a phoneme level using scalar quantization. The quantized F0 symbols are used in the decoding process as the prosodic context for the HMM-based speech synthesis. The synthetic speech is generated from the context-dependent labels and input speaker's pre-trained HMMs by using the HMM-based parameter generation algorithm. By taking account account of preceding and succeeding phonemes and F0 symbols as the contextual factors, we can generate smooth F0 trajectory similar to that of the original with only a small number of quantization bits. Experimental results demonstrate that the proposed technique can generate F0 contour with acceptable quality even when the bit-rate is less than 50 bps.	algorithm;context-sensitive language;hidden markov model;markov chain;quantization (signal processing);speech synthesis;synthetic intelligence;vocoder	Takashi Nose;Takao Kobayashi	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947538	speech recognition;genetic algorithm;quantization;computer science;speech;context-dependent memory;speech coding;pattern recognition;hidden markov model	Robotics	-18.6077737056488	-87.67276527445001	49192
f88305bcef4daf955c6b9c1f38c6e63a6c011e39	speech recognition in noisy environments using a switching linear dynamic model for feature enhancement	automatic speech recognition;indexing terms;speech recognition;feature extraction;index terms	The performance of automatic speech recognition systems strongly decreases whenever the speech signal is disturbed by background noise. We aim to improve noise robustness focusing on all major levels of speech recognition: feature extraction, feature enhancement, and speech modeling. Different auditory modeling concepts, speech enhancement techniques, training strategies, and model architectures are implemented in an in-car digit and spelling recognition task. We prove that joint speech and noise modeling with a global Switching Linear Dynamic Model (SLDM) capturing the dynamics of speech, and a Linear Dynamic Model (LDM) for noise, prevails over state-of-theart speech enhancement techniques. Furthermore we show that the baseline recognizer of the Interspeech Consonant Challenge 2008 can be outperformed by SLDM feature enhancement for almost all of the noisy testsets.	baseline (configuration management);feature extraction;finite-state machine;mathematical model;speech enhancement;speech recognition	Björn W. Schuller;Martin Wöllmer;Tobias Moosmayr;Gerhard Rigoll	2008			speech recognition;viscoelasticity;deformation (engineering);synthetic resin;artificial intelligence;pattern recognition;computer science;speech enhancement	ML	-15.836955883995364	-89.1053008951066	49212
1a22ae1d49208010ade4455b5b19adb743caca73	speech recognition with factorial-hmm syllabic acoustic models	temporal dynamics;factorial hidden markov model;dynamic programming algorithm;real time;acoustic modeling;speech segmentation;indexing terms;automatic speech recognition;speech recognition	Approaches in Automatic Speech Recognition based on classic acoustic models seem not to exploit all the information lying in a speech signal; furthermore decoding procedures have real time constraints preventing the system to achieve optimal alignment between acoustic models and signal. In this paper, we present an approach to speech recognition in which Factorial Hidden Markov Models (FHMM) are used as syllabic acoustic models. An alignment algorithm is used for unit decoding. As applicative domain we choose numbers (range 0-999,999) uttered in Italian. Syllabic accuracy in our model is 84.81%, correctness on numbers is 77.74%. Aim of the experiment is to show that the performances of FHMMs lie in the ability to retrieve the presence of two different temporal dynamics in a speech segments: the former with a quasi-segmental timing, the latter presenting a quasi-syllabic trend. Moreover, we evaluate a unit decoding process based on a dynamic programming algorithm in order to exploit the acoustic models performances at best.	acoustic cryptanalysis;acoustic model;algorithm;applicative programming language;correctness (computer science);dynamic programming;hidden markov model;markov chain;performance;speech recognition	Gianpaolo Coro;Francesco Cutugno;Fulvio Caropreso	2007			natural language processing;speech recognition;index term;computer science;machine learning;dynamic programming;acoustic model;speech segmentation	NLP	-20.504082849992127	-88.09299718397943	49382
58179db117fca0ba18b96ccd5935026eb1bec822	asr-driven top-down binary mask estimation using spectral priors	speech recognition least mean squares methods speech enhancement;baseline recognition system;robust automatic speech recognition;least mean squares methods;snr;speech;ideal binary mask;speech enhancement;spectral priors;automatic speech recognition;low level features;mask estimation robust automatic speech recognition ideal binary mask;hidden markov models;linguistic information;estimation;speech recognition;aurora4 corpus;asr driven top down binary mask estimation;interfering noise estimation;speech hidden markov models estimation signal to noise ratio speech recognition speech enhancement;signal to noise ratio;mask estimation;mmse based method;linguistic information asr driven top down binary mask estimation spectral priors low level features interfering noise estimation snr baseline recognition system aurora4 corpus mmse based method automatic speech recognition speech enhancement	Typical mask estimation algorithms use low-level features to estimate the interfering noise or instantaneous SNR. We propose a simple top-down approach to mask estimation. The estimated mask is based on a specific hypothesis of the underlying speech without using information about the interference or the instantaneous SNR. In this pilot study, we observe a 9% reduction in word error over a baseline recognition system on the Aurora4 corpus, though much greater gains could theoretically be achieved through improvements to the model selection process. We also present SNR improvement results showing our method performs as well as a standard MMSE-based method, demonstrating that speech recognition can aid speech enhancement. Thus, the relationship between recognition and enhancement need not be one way: linguistic information can play a significant role in speech enhancement.	algorithm;automated system recovery;baseline (configuration management);high- and low-level;interference (communication);model selection;signal-to-noise ratio;speech enhancement;speech recognition;top-down and bottom-up design	William Hartmann;Eric Fosler-Lussier	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288964	speech recognition;computer science;pattern recognition;signal-to-noise ratio;hidden markov model	Robotics	-20.426502018345996	-89.93448206984841	49406
3dea35d256e4a99035d4d843acd40a078b3af1bb	基於決策樹演算法之台語連音變調預估模組 (a prediction module for taiwanese tone sandhi based on the decision tree algorithm) [in chinese]		Taiwanese tone sandhi problem is one of the important research issues for Taiwanese Text-to-Speech systems. In word level, we can use the general tone sandhi rules to deal with the Taiwanese tone sandhi problem. The tone sandhi becomes more difficult in sentence level because of that the general tone sandhi rules for words may not apply at each word in a sentence. In this paper we proposed a module to deal with the Taiwanese tone sandhi problem for Chinese to Taiwanese Text-to-Speech systems. We adopt Decision tree C5.0 algorithm accompanied with three Special Cases generated from training data to predict the tone sandhi of each syllable. In this module, the accuracy of the inside test and outside test are 93.42% Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012)	c4.5 algorithm;computation;computational linguistics;decision tree model;speech processing;syllable	Neng-Huang Pan;Ming-Shing Yu;Pei-Chun Tsai	2012			tone sandhi;decision tree learning;speech recognition;computer science	NLP	-21.08409227212124	-82.00722295640223	49408
e8684fe3482a420b3827c67ccb60960ebf49da00	a new speech recognition method based on vq-distortion measure and hmm	vector quantisation hidden markov models speech recognition;ieee trans;probability;speech recognition method;speaker dependent digit recognition;hidden markov model;performance;natural languages;distortion measurement;code standards;accuracy speech recognition method vector quantization hidden markov model vq distortion based hmm vq distortion measure ieee trans mixtured distribution density hmm performance speaker dependent digit recognition;vq distortion based hmm;accuracy;hidden markov models;vector quantization;stochastic processes;viterbi algorithm;vq distortion measure;speaker dependent;speech recognition;vector quantizer;mixture distribution;measurement standards;vector quantisation;mixtured distribution density hmm;information theory;speech recognition hidden markov models stochastic processes natural languages distortion measurement viterbi algorithm information theory probability code standards measurement standards	A speech recognition method which integrates a VQ (vector quantization)-distortion measure and a discrete HMM (hidden Markov model) is proposed. This VQ-distortion-based HMM uses a VQ-distortion measure at each state instead of the discrete output probability used by a discrete HMM. Although this method is regarded as a refined version of the VQ-distribution based recognition method proposed by D.K. Burton et al (IEEE Trans. vol. ASSP-33, no.4, p.837-49 of 1985), it is also considered as a special case of a mixtured distribution density HMM. The authors describe the relationship between the VQ-distortion-based HMM and conventional HMMs, and compare their speech recognition performance through experiments on speaker-dependent digit recognition. A recognition accuracy of 100% using the new method was obtained. >		Seiichi Nakagawa;Hideyuki Suzuki	1993		10.1109/ICASSP.1993.319401	speech recognition;performance;viterbi algorithm;mixture distribution;computer science;machine learning;pattern recognition;probability;mathematics;accuracy and precision;natural language;vector quantization;hidden markov model;statistics	NLP	-20.10049646353971	-92.16227116417463	49536
65dfbae5f561ce03690c058ba1f6c3825fa4c1e4	consonant recognition by modular construction of large phonemic time-delay neural networks	time delay neural network	Alex Waibel Carnegie-Mellon University Pittsburgh, PA 15213, A TR Interpreting Telephony Research Laboratories Osaka, Japan In this paperl we show that neural networks for speech recognition can be constructed in a modular fashion by exploiting the hidden structure of previously trained phonetic subcategory networks. The performance of resulting larger phonetic nets was found to be as good as the performance of the subcomponent nets by themselves. This approach avoids the excessive learning times that would be necessary to train larger networks and allows for incremental learning. Large time-delay neural networks constructed incrementally by applying these modular training techniques achieved a recognition performance of 96.0% for all consonants.	connectionism;image scaling;modular design;neural networks;speech recognition	Alexander H. Waibel	1988			speech recognition;computer science;artificial intelligence;machine learning;time delay neural network	ML	-17.747259973272165	-86.90233357661945	49580
748c114ae8b5d87333c344f9c65b328541ac434a	speech activity detection and automatic prosodic processing unit segmentation for emotion recognition		In speech communication emotions play a great role in expressing information. These emotions are partly given as reactions to our environment, to our partners during a conversation. Understanding these reactions and recognizing them automatically is highly important. Through them, we can get a clearer picture of the response of our partner in a conversation. In Cognitive InfoCommunication this kind of information helps us to develop robots, devices that are more aware of the need of the user, making the device easy and enjoyable to use. In our laboratory we conducted automatic emotion classification and speech segmentation experiments. In order to develop an automatic emotion recognition system on the basis of speech, an automatic speech segmenter is also needed to separate the speech segments needed for the emotion analysis. In our former research we found that the intonational phrase can be a proper unit of emotion analysis. In this paper speech detection and segmentation methods are developed. For speech detection, Hidden Markov Models are used with various noise and speech acoustic models. The results show that the procedure is able to detect speech in the sound signal with more than 91% accuracy and segment it into intonational phrases.	acoustic cryptanalysis;emotion recognition;experiment;hidden markov model;markov chain;robot;speech segmentation	David Sztahó;Klára Vicsi	2014	Intelligent Decision Technologies	10.3233/IDT-140199	speech recognition;pattern recognition	NLP	-12.90803503775519	-83.84823989030825	49614
0d95e655f9d03e4d884ec74f0eafacda4d6b4fb4	production of neutral tone on disyllabic words by two-year-old mandarin-speaking children		This study examined the production of neutral tone in disyllabic words by two-year-old Mandarin-speaking children. The results showed that children were fully aware of the neutral tone sandhi rule phonologically at the age of two. However, they cannot phonetically produce neutral tone well. In particular, children made off-standard production with higher pitch register, wider pitch range and longer duration, while made correct production with correct pitch pattern but the duration ratio between the initial syllable and the final syllable is slightly larger than the adults’. The difficulty of the neutral tone production is closely related to the type of the preceding tone and the coordination of articulation for disyllabic neutral tone words.	super robot monkey team hyperforce go!	Jun Gao;Aijun Li	2017		10.1007/978-3-030-00126-1_9	mandarin chinese;communication;tone sandhi;syllable;psychology	NLP	-11.186214677493355	-81.52571919720411	49873
9ee2262ccf560c05fb91d3b01702c8c97fe89d38	jacobian adaptation with improved noise reference for speaker verification.	speaker verification	Jacobian Adaptation (JA) of the acoustic models is a fast adaptation technique that has been used in Automatic Speech Recognition (ASR) systems to adapt the models from the training to the testing noise conditions. This technique has been tested in previous works with both Mel-Frequency Cepstrum Coefficients (MFCC) and Frequency Filtering (FF) parameters and good speech recognition results were obtained. In this work we have used the JA technique in a speaker verification system. In the previous implementations of JA for speech recognition only one reference of the training noise conditions was used to adapt all the models. In speaker recognition systems the utterances of each speaker are only used to train his/her model. Therefore, the training noise reference can be improved by estimating a specific reference for each speaker because each speaker model is trained in different noise conditions. With this approach, which we call Model-dependent Noise Reference Jacobian Adaptation (MNRJA), a better noise estimation is obtained and therefore a better adaptation of the speaker models. In our speaker verification tests the MNRJA approach outperformed the conventional implementation of the JA technique.	acoustic cryptanalysis;coefficient;jacobian matrix and determinant;mel-frequency cepstrum;speaker recognition;speech recognition	Jan Anguita;Javier Hernando;Alberto Abad	2004			natural language processing;speech recognition;computer science;machine learning	NLP	-13.92425057365027	-92.05256561233251	50078
5aeb35e7ba19309ba299f8275ccd7fbe1895204f	improved open-vocabulary spoken content retrieval with word and subword lattices using acoustic feature similarity☆	spoken term detection;pseudo relevance feedback;random walk;spoken content retrieval	Improved open-vocabulary spoken content retrieval with word and subword lattices using acoustic feature similarity Hung-yi Lee a,∗, Po-wei Chou b, Lin-shan Lee a a Graduate Institute of Communication Engineering, National Taiwan University, No. 1, Sec. 4, Roosevelt Road, Taipei 10617, Taiwan b Department of Electrical Engineering, National Taiwan University, No. 1, Sec. 4, Roosevelt Road, Taipei 10617, Taiwan	acoustic cryptanalysis;electrical engineering;silk road;substring;vocabulary	Hung-yi Lee;Po-wei Chou;Lin-Shan Lee	2014	Computer Speech & Language	10.1016/j.csl.2013.12.003	natural language processing;speech recognition;computer science;pattern recognition;random walk;statistics	NLP	-24.338005046847176	-85.32786427061293	50201
675d821baea2931df33cb2115776114c8324ea4e	incremental learning of new user formulations in automatic directory assistance	incremental learning	Directory Assistance for business listings is a challenging task: one of its main problems is that customers formulate their requests for the same listing with great variability. Since it is difficult to reliably predict a priori the user formulations, we have proposed a procedure for detecting, from field data, user formulations that were not foreseen by the designers. These formulations can be added, as variants, to the denominations already included in the system to reduce its failures. In this work, we propose an incremental procedure that is able to filter a huge amount of calls routed to the operators, collected every month, and to detect a limited number of phonetic strings that can be included as new formulation variants in the system vocabulary. The results of our experiments, tested on 9 months of calls that the system was unable to serve automatically, show that the incremental procedure, using only additional amount of data collected every month, is able to stay close to the (upper bound) performance of the not incremental one, and offers the possibility of periodically updating the system formulation variants of every city.	experiment;routing;sensor;spatial variability;telephone number;test set;vocabulary	M. Andorno;Luciano Fissore;Pietro Laface;Mario Nigra;Cosmin Popovici;Franco Ravera;Claudio Vair	2003			computer science;machine learning;database;multimedia	ML	-21.800933698280115	-85.81893833459799	50251
158339600441dc8996568a213d2805e5c68c849f	recognizing preliminary sentences in dialogue interpretation	speech acts	In traditional plan-based dialogue interpretation systems, speech-acts are directly used for identifying the speaker's domain plans and little analysis is performed of the role of sentences in dialogue. This may lead to the activation of a large number of hypotheses on an agent's domain plans. In this paper, we describe how to interpret background sentences occurring in a dialogue by using knowledge coming from the linguistic and domain levels, and from a model of the user. We consider two kinds of utterances: the rst one justiies the performance of subsequent speech-acts; the second represents information to be used for constraining the interpretation process of the other speech-act.	internationalized domain name	Liliana Ardissono;Guido Boella;Dario Sestero	1995		10.1007/3-540-60437-5_13	natural language processing;speech recognition;linguistics	NLP	-25.93623117433754	-83.07286949080094	50286
1dcbf3563de2701685b17e0f83d251292a449b70	an articulatory feature-based tandem approach and factored observation modeling	mlp;phone classification;hidden markov model;multilayer perceptrons;speech processing;state tying structures;multilayer perceptron;articulatory feature;asr;automatic speech recognition;hidden markov models automatic speech recognition multilayer perceptrons speech recognition feature extraction concatenated codes computer science standards development acoustic signal processing speech processing;hidden markov models;feature concatenation approach;feature concatenation approach articulatory feature based tandem approach factored observation modeling multilayer perceptron mlp automatic speech recognition asr phone classification hidden markov model state tying structures;factored observation modeling;speech recognition hidden markov models multilayer perceptrons speech processing;speech recognition;multilayer perceptrons speech recognition;articulatory feature based tandem approach	The so-called tandem approach, where the posteriors of a multilayer perceptron (MLP) classifier are used as features in an automatic speech recognition (ASR) system has proven to be a very effective method. Most tandem approaches up to date have relied on MLPs trained for phone classification, and appended the posterior features to some standard feature hidden Markov model (HMM). In this paper, we develop an alternative tandem approach based on MLPs trained for articulatory feature (AF) classification. We also develop a factored observation model for characterizing the posterior and standard features at the HMM outputs, allowing for separate hidden mixture and state-tying structures for each factor. In experiments on a subset of Switchboard, we show that the AF-based tandem approach is as effective as the phone-based approach, and that the factored observation model significantly outperforms the simple feature concatenation approach while using fewer parameters.	anisotropic filtering;concatenation;effective method;experiment;hidden markov model;markov chain;memory-level parallelism;multilayer perceptron;speech recognition;telephone switchboard	Özgür Çetin;Arthur Kantor;Simon King;Chris D. Bartels;Mathew Magimai-Doss;Joe Frankel;Karen Livescu	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.366995	speech recognition;computer science;machine learning;pattern recognition;multilayer perceptron;hidden markov model	Vision	-18.78614099562484	-91.12656625502655	50434
739f618f0f96a3b1d4e62cd98f2d3d930b546fe9	automatic recognition and transcription of pitman's handwritten shorthand - an approach to shortforms	dynamic programming;programacion dinamica;shorthand;stenographie pitman;documento manuscrito;reconnaissance caractere;anglais;programmation dynamique;english;stenographie;ingles;document manuscrit;character recognition;reconocimiento caracter;manuscript document	Abstract   A verbatim written transcript of speech would have many applications in the office, in verbatim reporting and as an aid for the deaf. Unfortunately, the automatic recognition of unlimited vocabulary speech is not likely to be possible for a number of years. An alternative strategy, investigated at Southampton University, is to attempt the less complex task of automatically transcribing handwritten notes made using the Pitman shorthand notation.  Pitman shorthand outlines can be split into two classes of characters, shortforms (comprising over 90 of the most frequently used words and phrases in the English language) and vocalised outlines which can represent any word pseudo-phonetically. The shortforms represent as much as 50% of normal shorthand and are recognised directly using a dynamic programming technique with typical recognition accuracy of over 90%. Vocalised outlines are recognised using a syntactic method which interacts with a knowledge source derived from analysis of a large number of shorthand outlines. This paper describes the recognition strategy for Pitman shorthand shortforms which uses the dynamic programming template matching technique.	transcription (software)	Graham Leedham;Andy C. Downton	1987	Pattern Recognition	10.1016/0031-3203(87)90008-2	natural language processing;speech recognition;computer science;artificial intelligence;english;dynamic programming;mathematics	Vision	-24.75520703323066	-82.54617858411152	50582
487202979334f773ad417f3d12a29c0a305b5d36	a fast algorithm for unsupervised incremental speaker adaptation	feature vectors;adaptation data;error reduction;information systems;bayes methods;speech processing;unsupervised incremental speaker adaptation;bayesian methods;speech coding;contracts;acoustic signal processing;maximum likelihood estimation;telephony;speaker adaptation algorithms;speaker recognition;feature vector;hidden markov models;vectors;acoustic adaptation methods;word error reduction;maximum likelihood estimation speaker recognition bayes methods real time systems speech coding acoustic signal processing speech processing;fast algorithm;experiments;speech recognition;real time applications;parameter estimation;modified bayesian codebook reestimation;speaker adaptation;real time application;labeling;maximum likelihood estimation bayesian methods information systems telephony labeling speech recognition hidden markov models vectors contracts optimization methods;ml estimation unsupervised incremental speaker adaptation fast algorithm speaker adaptation algorithms adaptation data parameter estimation real time applications modified bayesian codebook reestimation feature vectors experiments word error reduction acoustic adaptation methods;ml estimation;real time systems;optimization methods	Speaker adaptation algorithms often require a rather large amount of adaptation data in order to estimate the new parameters reliably. In this paper, we investigate how adaptation can be performed in real{time applications with only a few seconds of speech from each user. We propose a modi ed Bayesian codebook reestimation which does not need the computationally intensive evaluation of normal densities and thus speeds up the adaptation remarkably, e.g. by a factor of 18 for 24{dimensional feature vectors. We performed experiments in two real{time applications with very small amounts of adaptation data, and achieved a word error reduction of up to 11%.	algorithm;codebook;experiment;feature vector	Michael Schüßler;Florian Gallwitz;Stefan Harbeck	1997		10.1109/ICASSP.1997.596113	speaker recognition;speech recognition;feature vector;computer science;machine learning;pattern recognition;speech processing	ML	-18.837707163208403	-91.70395556394564	50595
19c29a452c3faef3de35c732b369820e57112a16	performance evaluation for voice conversion systems	speaker identification;performance evaluation;voice conversion;evaluation criteria	In the present work, we introduce a new performance evaluation measure for assessing the capacity of voice conversion systems to modify the speech of one speaker (source) so that it sounds as if it was uttered by another speaker (target). This measure relies on a GMM-UBM-based likelihood estimator that estimates the degree of proximity between an utterance of the converted voice and the predefined models of the source and target voices. The proposed approach allows the formulation of an objective criterion, which is applicable for both evaluation of the virtue of a single system and for direct comparison (benchmarking) among different voice conversion systems. To illustrate the functionality and the practical usefulness of the proposed measure, we contrast it with four well-known objective evaluation criteria.	google map maker;performance evaluation;utility;xs (perl)	Todor Ganchev;Alexandros Lazaridis;Iosif Mporas;Nikos Fakotakis	2008		10.1007/978-3-540-87391-4_41	speaker recognition;speech recognition	NLP	-15.16003363554747	-90.22670143117392	50609
321e7f77804de89a28b86494afe4ca3b8833a45b	a study of sensor arrangements for detecting movements and inclinations of tongue point during speech			sensor	Kohichi Ogata;Yorinobu Sonoda	1994			speech recognition;tongue;computer science	Vision	-6.922560352969091	-84.66709973741206	50686
07c065d6f3f11642e12243eb6e698b39fd14fa2d	mandarin listeners can learn non-native lexical tones through distributional learning		In a previous study we have found that non-tone language speakers are able to form lexical tone categories through extracting frequency distribution in training, but only when attention is directed towards the distribution [12]. This study extends the distributional learning literature by investigating how tone language speakers’ linguistic experience with tones affects their distributional learning of non-native lexical tones. Native Mandarin listeners were presented with a Thai lexical tone minimal pair distributed either unimodally (promoting formation of a single category), or bimodally (promoting two category formation). Assessment of performance in a discrimination task before and after exposure showed that the Bimodal Distribution group improved significantly from Pretest to Posttest whereas the Unimodal group did not. These results suggest that tone language speakers capitalise on their experience in using pitch phonemically to form the appropriate number of lexical tone categories based on the distribution that they hear.	distributional semantics;super robot monkey team hyperforce go!	Jia Hoong Ong;Denis Burnham;Paola Escudero	2015				NLP	-11.398807845190232	-81.66356619877358	50717
d161bf07ef42fc6040726031a8fc6cac123d9e02	post-focus pitch register lowering as a phrasal marker - an acoustic study of focus and phrasing in shanghai chinese		This paper reports an experiment designed to investigate whether post-focus tonal realization is constrained by prosodic structure in Shanghai Chinese (SH). Previous studies have shown that in SH, focus expanded the f0 range of bi-syllabic tone sandhi words while pitch register lowering, instead of pitch range compression, was consistently found on post-focus bi-syllabic tone sandhi words. The present study examined durational adjustments and f0 realizations of bi-syllabic tone sandhi words at different prosodic levels under different focus conditions. The results showed that focus expanded the pitch range of the target syllables at both the levels of Prosodic Word and Major Word, but postfocus pitch register lowering was only found at the level of Major Word. Based on the results, this study concludes that post-focus tonal realization in SH is constrained by prosodic structure and post-focus pitch register lowering is a phrasal marker.	acoustic cryptanalysis;chinese room;pitch (music)	Lei Sun;Yiya Chen	2015			speech recognition;engineering	NLP	-11.124553283184506	-81.58506404327859	50796
215a272715365166f4a400952f109546af345135	on the suitability of state-of-the-art music information retrieval methods for analyzing, categorizing and accessing non-western and ethnic music collections	busqueda informacion;science general;traitement signal;evaluation performance;access;performance evaluation;information retrieval;evaluacion prestacion;document musical;musical score;documento musical;classification;base de donnees audio;carte autoorganisatrice;latin music;algorithme;etat actuel;algorithm;non western music;self organising feature maps;recherche information;feature extraction;signal processing;state of the art;music information retrieval;self organizing map;estado actual;self organized map;audio databases;extraction caracteristique;analisis sonido;ethnic music;sound analysis;latin american;audio analysis;procesamiento senal;analyse son;genre;algoritmo	With increasing amounts of music being available in digital form, research in music information retrieval has turned into a dominant field to support organization of and easy access to large collections of music. Yet, most research is focussed traditionally on Western music, mostly in the form of mastered studio recordings. This leaves the question whether current music information retrieval approaches can also be applied to collections of non-Western and in particular ethnic music with completely different characteristics and requirements. In this work we analyze the performance of a range of automatic audio description algorithms on three music databases with distinct characteristics, specifically a Western music collection used previously in research benchmarks, a collection of Latin American music with roots in Latin American culture, but following Western tonality principles, as well as a collection of field recordings of ethnic African music. The study quantitatively shows the advantages and shortcomings of different feature representations extracted from music on the basis of classification tasks, and presents an approach to visualize, access and interact with ethnic music collections in a structured way. & 2009 Elsevier B.V. All rights reserved.	accessibility;algorithm;audio description;categorization;database;information retrieval;list of online music databases;requirement	Thomas Lidy;Carlos Nascimento Silla;Olmo Cornelis;Fabien Gouyon;Andreas Rauber;Celso A. A. Kaestner;Alessandro L. Koerich	2010	Signal Processing	10.1016/j.sigpro.2009.09.014	programming;speech recognition;self-organizing map;feature extraction;biological classification;computer science;latin americans;machine learning;signal processing;multimedia;pop music automation;audio analyzer	Web+IR	-5.4949104372295725	-92.09747853857105	50826
89fc5b65e877cb0377828930a9cc5da6893c10ed	an analysis of deep neural networks in broad phonetic classes for noisy speech recognition		The introduction of Deep Neural Network (DNN) based acoustic models has produced dramatic improvements in performance. In particular, we have recently found that Deep Maxout Networks, a modification of DNNs’ feed-forward architecture that uses a max-out activation function, provides enhanced robustness to environmental noise. In this paper we further investigate how these improvements are translated into the different broad phonetic classes and how does it compare to classical Hidden Markov Models (HMM) based back-ends. Our experiments demonstrate that performance is still tightly related to the particular phonetic class being stops and affricates the least resilient but also that relative improvements of both DNN variants are distributed unevenly across those classes having the type of noise a significant influence on the distribution. A combination of the different systems DNN and classical HMM is also proposed to validate our hypothesis that the traditional GMM/HMM systems have a different type of error than the Deep Neural Networks hybrid models.	activation function;database;decision model and notation;deep learning;google map maker;hidden markov model;linear algebra;neural networks;speech recognition	Fernando de-la-Calle-Silos;Ascensión Gallardo-Antolín;Carmen Peláez-Moreno	2016		10.1007/978-3-319-49169-1_9	time delay neural network	ML	-17.428529467859413	-88.85481776287908	50868
c49242965eddfb580ca7f1a2f597b5e1b9d36655	semi-supervised acoustic model retraining for medical asr		Training models for speech recognition usually requires accurate word-level transcription of available speech data. For the domain of medical dictations, it is common to have “semi-literal” transcripts available: large numbers of speech files along with their associated formatted episode report, whose content only partially overlaps with the spoken content of the dictation. We present a semi-supervised method for generating acoustic training data by decoding dictations with an existing recognizer, confirming which sections are correct by using the associated report, and repurposing these audio sections for training a new acoustic model. The effectiveness of this method is demonstrated in two applications: first, to adapt a model to new speakers, resulting in a 19.7% reduction in relative word errors for these speakers; and second, to supplement an already diverse and robust acoustic model with a large quantity of additional data (from already known voices), leading to a 5.0% relative error reduction on a large test set of over one thousand speakers.	acoustic cryptanalysis;acoustic model;approximation error;automated system recovery;cluster analysis;finite-state machine;human error;inter-rater reliability;language model;literal (mathematical logic);semi-supervised learning;semiconductor industry;speech recognition;test set;transcription (software);word error rate	Greg P. Finley;Erik Edwards;Wael Salloum;Amanda Robinson;Najmeh Sadoughi;Nico Axtmann;Maxim Korenevsky;Michael Brenndoerfer;Mark Miller;David Suendermann-Oeft	2018		10.1007/978-3-319-99579-3_19	dictation;retraining;decoding methods;acoustic model;speech recognition;training set;test set;computer science	NLP	-20.541068104713183	-83.87545401525756	51005
1d772d02d133edcf55415443fe3215a8c62466a4	frequency analysis system of auditory nerves with auditory brainstem response (abr) by harmonious tone			auditory processing disorder;frequency analysis	Hideto Ide;Masao Ohtsuka	1993	JRM	10.20965/jrm.1993.p0407		ML	-7.369776977423463	-83.78189912349663	51008
98f9df1a3175b88a0063311d6b8df6b55128186b	metrics for evaluating dialogue strategies in a spoken language system	real time;spoken language systems;dialogue manager	In this paper, we describe a set of metrics for the evaluation of different dialogue management strategies in an implemented real-time spoken language system. The set of metrics we propose tries to offer useful insights in evaluating how particular choices in the dialogue management can affect the overall quality of the man-machine dialogue. The evaluation makes use of established metrics: the transaction success, the contextual appropriateness of system answers, the calculation of normal and correction turns in a dialogue. We also define a new metric, the implicit recovery, which allows to measure the ability of a dialogue manager to deal with errors by different levels of analysis. We report evaluation data from several experiments, and we compare two different approaches to dialogue repair strategies using the set of metrics we argue for.	dialog system;dialog tree;experiment;human–computer interaction;norm (social);real-time transcription;systems design	Morena Danieli;Elisabetta Gerbino	1996	CoRR		natural language processing;computer science;knowledge management	NLP	-26.400556585635314	-86.59012883688163	51038
92754c3b3a23bfae0c593a1c6c5255e277710612	analysis of amperometric biosensor curves using hidden-markov-modells	modelo markov oculto;modele markov cache;amperometry;hidden markov model;signal analysis;amperometrie;biodetecteur;mesure position;analisis de senal;biosensor;medicion posicion;amperometria;hidden markov modells;position measurement;biodetector;analyse signal	In this paper a HMM based method for analysing curves of amperometric biosensors is presented. In addition to our multi-HMM approach, we propose an enhanced single-HMM approach, which is used to detect the curve type, the end of the measurement and the correct measurement position. Besides, a problem-specific modified variant of the Baum-Welch algorithm is presented. The multi- and single-HMM approach yielded good accuracies and recognition rates. The modified Baum-Welch algorithm outperformed the standard Baum-Welch.	hidden markov model;markov chain	Jörg Weitzenberg;Stefan Posch;Manfred Rost	2002		10.1007/3-540-45783-6_23	econometrics;computer science;artificial intelligence;amperometry;hidden markov model;biosensor;statistics	Vision	-14.564851525118325	-94.68334933331619	51086
50bbd510e8e0848c239a42ca6b0c4ad8175b12dd	designing very compact decision trees for grapheme-to-phoneme transcription	decision tree	Decision trees are a popular technique for automatic generation of a phonetic transcription for a given word spelling. We investigate different methods of decision tree design to obtain more compact trees and at the same time better graphemeto-phoneme transcription quality. We evaluate different approaches to decision tree question selection and pruning using one English and two German grapheme-to-phoneme transcription tasks. In particular, we present a method of automatic generation of decision tree questions from the training data that significantly improves decision tree design.	decision tree;transcription (software)	Anne K. Kienappel;Reinhard Kneser	2001			pruning;artificial intelligence;decision tree;pattern recognition;incremental decision tree;phonetic transcription;alternating decision tree;grapheme;computer science;machine learning;training set;spelling	NLP	-21.019260291687637	-80.32655017892345	51148
8cf06ebf0ff0fcce3277bb9466e8c40c5b89c45e	word-dependent acoustic-labial weights in hmm-based speech recognition	error rate;speech recognition	This paper describes a novel approach for weight-ing the contribution of the acoustic and visual sources of information in a bimodal connected speech recognition system. We consider that a dierent acoustic-labial weight is attached to each recognition unit. The values of the weighting vector are optimised in order to minimise error rate on a learning set. Experiments are performed on a two-speakers audiovisual database, composed of connected letters, with two diierent acoustic-labial speech recognition systems. For both speakers and both systems, the weights op-timisation allows us to increase the recognition rate of our bimodal system.	acoustic cryptanalysis;database;hidden markov model;operational amplifier;speech recognition	Pierre Jourlin	1997			word error rate;speaker recognition;hidden markov model;acoustic model;artificial intelligence;speech recognition;pattern recognition;computer science	AI	-14.893328582044107	-87.84692376918002	51173
d1ec77f96fbaccdb63abb1fc63ec6a9ee07d5574	prosody in finger braille and teletext receiver for finger braille		In this paper, we introduce durational rules in text-to-FingerBraille. Finger Braille is one of the communication methods for the deaf blind and it seems to be the medium most suited to real-time communication and for expressing the feelings of the speaker because of its prosody existing similarly to spoken languages. First, we analyzed duration between two Braille codes in Finger Braille and found that it can be changed according to the structure and meaning of the sentences. Second, we construct durational rules in Finger Braille based on these results. Third, the effectiveness of the rule was examined in listening experiments with a deaf blind person. As a result, it is suggested that durational prosody help listeners to have clear understanding. Finally, we made a prototype of Finger Braille receiver for teletext broadcasting system as a practical application applying this rule.	code;experiment;finger tree;prototype;real-time transcription;refreshable braille display;semantic prosody;teletext	Yasuo Horiuchi;Akira Ichikawa	2001			natural language processing;speech recognition;computer science	HCI	-25.232585557254296	-86.73084375792656	51256
52786f52c6943c0768acaf89cc6dd5b5b8fc7724	within-utterance correlation for speech recognition	computer and information science;speech recognition;data och informationsvetenskap	Relations between non-adjacent parts of an utterance are commonly regarded as an important source of information for speech recognition. However, they have not been very much used in speech recognition systems. In this paper, we include this information by joint distributions of pairs of phones occurring in the same utterance. In addition to relations between acoustic events, we also have incorporated relations between spectral and prosodically oriented information, such as phone duration, position in utterance and fundamental frequency. Preliminary recognition results on N-best rescoring show 10% word error reduction compared to a baseline Viterbi decoder.	acoustic cryptanalysis;baseline (configuration management);graph (discrete mathematics);information source;speech recognition;viterbi decoder	Mats Blomberg	1999			voice activity detection;natural language processing;speech recognition;computer science;speech error;communication;speech analytics	NLP	-17.671038457857232	-84.30583330883795	51334
b7b551d57d32c618fae26882a26656b39070f830	context-dependent mlps for lvcsr: tandem, hybrid or both?		Gaussian Mixture Model (GMM) and Multi Layer Perceptron (MLP) based acoustic models are compared on a French large vocabulary continuous speech recognition (LVCSR) task. In addition to optimizing the output layer size of the MLP, the effect of the deep neural network structure is also investigated. Moreover, using different linear transformations (time derivatives, LDA, CMLLR) on conventional MFCC, the study is also extended to MLP based probabilistic and bottle-neck TANDEM features. Results show that using either the hybrid or bottleneck TANDEM approach leads to similar recognition performance. However, the best performance is achieved when deep MLP acoustic models are trained on concatenated cepstral and context-dependent bottle-neck features. Further experiments reveal the importance of the neighbouring frames in case of MLP based modeling, and that its gain over GMM acoustic models is strongly reduced by more complex features.	acoustic cryptanalysis;acoustic model;artificial neural network;cepstrum;concatenation;context-sensitive language;deep learning;experiment;google map maker;memory-level parallelism;mixture model;perceptron;quad flat no-leads package;speech analytics;speech recognition;tandem computers;vocabulary	Zoltán Tüske;Ralf Schlüter;Hermann Ney;Martin Sundermeyer	2012			speech recognition;tandem;pattern recognition;artificial intelligence;computer science	ML	-17.452744809605008	-88.74340209294311	51337
00f7858b32fe735db049e0509c1573886a38fe12	implementation and evaluation of an hmm-based thai speech synthesis system	speech synthesis	This paper describes a novel approach to the realization of Thai speech synthesis. Spectrum, pitch, and phone duration are modeled simultaneously in a unified framework of HMM, and their parameter distributions are clustered independently by using a decision-tree based context clustering technique with different styles. A group of contextual factors which affect spectrum, pitch, and state duration, i.e., tone type, part of speech, are taken into account especially for a tonal language. The evaluation of the synthesized speech shows that tone correctness is significantly improved in some clustering styles, moreover the implemented system gives the better reproduction of prosody (or naturalness, in some sense) than the unit-selection-based system with the same speech database.	cluster analysis;correctness (computer science);decision tree;hidden markov model;pitch (music);semantic prosody;speech synthesis;unified framework	Suphattharachai Chomphan;Takao Kobayashi	2007			speech recognition;artificial intelligence;hidden markov model;pattern recognition;computer science;speech synthesis	NLP	-17.53623186014606	-84.57527379330145	51484
c971534dbc6e9e2c14a7085ab9ef4bbae661f79b	algorithms for statistical translation of spoken language	experimental tests;lenguaje natural;alignement;algoritmo busqueda;translating;analisis estadistico;algorithme recherche;search problems speech recognition language translation statistical analysis computational linguistics;search algorithm;langage naturel;language translation;search method;tratamiento lenguaje;natural languages system testing speech recognition search problems search methods vocabulary performance evaluation text recognition educational technology training data;indexing terms;traduction;speech translation;statistical analysis;language processing;word alignment;natural language;analyse statistique;traitement langage;alineamiento;palabra;speech recognition;traduccion;word;search problems;computational linguistics;bilingual corpus;alignment;speech recognition spoken language translation statistical translation experimental results translation model language model bigram model m gram model lexical model alignment model quasi monotone alignment approach inverted alignment approach alignment template approach search method bilingual corpus vocabulary text transcription;language model;mot	We describe three approaches to statistical translation and present experimental results. The statistical translation approach uses two types of information: a translation model and a language model. The language model used is a bigram or general m-gram model. The translation model is decomposed into a lexical model and an alignment model. There are three approaches that are presented and tested in detail: the quasi-monotone alignment approach, the inverted alignment approach, and the alignment template approach. For each of these three approaches, a suitable search method is presented. The system has been tested on a limited-domain spoken-language task for which a bilingual corpus is available: the Verbmobil task (German-English, 7000-word vocabulary). We present experimental results for each of the three approaches. The experimental tests were performed on both the text transcription and the speech recognizer output.		Hermann Ney;Sonja Nießen;Franz Josef Och;Hassan Sawaf;Christoph Tillmann;Stephan Vogel	2000	IEEE Trans. Speech and Audio Processing	10.1109/89.817451	natural language processing;speech recognition;transfer-based machine translation;example-based machine translation;index term;computer science;computational linguistics;word;rule-based machine translation;natural language;language model;search algorithm	Arch	-23.768664809773117	-80.3376947701611	51493
4ff0579f18caff30fe9620e71fbdc22ffe33f09a	pronunciation change in conversational speech and its implications for automatic speech recognition	prononciation;phonetic transcription;phonetic variation;linguistique appliquee;automatic speech recognition;discours spontane;variation phonetique;pronunciation;feature extraction;reconnaissance de la parole;conversation;acoustic phonetics;speech recognition;computational linguistics;spontaneous speech;phonetique acoustique;linguistique informatique;transcription phonetique;applied linguistics	Pronunciations in spontaneous speech differ significantly from citation form and pronunciation modeling for automatic speech recognition has received considerable attention in the last few years. Most methods describe alternate pronunciations of a word using multiple entries in a dictionary or using a network of phones, assuming implicitly that a deviation from the canonical pronunciation results in a ‘‘complete’’ change as described by the alternate pronunciation. We investigate this implicit assumption about pronunciation change in conversational speech and demonstrate here that in most cases, the change is only partial; a phone is not completely deleted or substituted by another phone but is modified only partially. Evidence supporting this conclusion comes from the three-way analysis of features extracted from the acoustic signal for use in a speech recognition system, canonical pronunciations from a dictionary, and careful phonetic transcriptions produced by human labelers. Most often, when a deviation from the canonical pronunciation is marked, neither the canonical nor the manually labeled phones represent the actual acoustics adequately. Further analysis of the manual phonetic transcription reveals a significant number (>20%) of instances where even human labelers disagree on the identity of the surface-form. In light of this evidence, two methods are suggested for accommodating such partial pronunciation change in the automatic recognition of spontaneous speech and experimental results are presented for each method. 2003 Elsevier Ltd. All rights reserved. This work was supported by the US Department of Defense via Contract No MDA90499C3525. * Corresponding author. E-mail addresses: murat@research.att.com (M. Sarac lar), khudanpur@jhu.edu (S. Khudanpur). 0885-2308/$ see front matter 2003 Elsevier Ltd. All rights reserved. doi:10.1016/j.csl.2003.09.005 2 M. Sarac lar, S. Khudanpur / Computer Speech and Language xxx (2003) xxx–xxx ARTICLE IN PRESS	acoustic cryptanalysis;design by contract;dictionary;speech recognition;spontaneous order;transcription (software)	Murat Saraclar;Sanjeev Khudanpur	2004	Computer Speech & Language	10.1016/j.csl.2003.09.005	natural language processing;speech recognition;feature extraction;computer science;computational linguistics;applied linguistics;linguistics	NLP	-12.021525054858829	-82.52313109585438	51607
49e33673cb7b675efe4fd550b941a517ad281bf5	a correntropy-based voice to midi transcription algorithm	kernel;internet correlation methods information retrieval;fluctuations;band pass filters;query by humming;information retrieval;accuracy time frequency analysis correlation band pass filters kernel estimation fluctuations;correlation methods;correntropy based voice;query by humming systems;accuracy;automatic voice to midi transcription algorithm;internet;estimation;monophonic singing transcription;signal envelope;correlation function;correlation;note segmentation method;time frequency analysis;transcription system;internet correntropy based voice automatic voice to midi transcription algorithm note segmentation method signal envelope correlation function transcription system query by humming systems monophonic singing transcription	In this paper we describe an automatic voice-to-MIDI transcription procedure. In particular we propose a note segmentation method based on the analysis of the signal envelope and its derivative. The pitch of the segmented note is extracted with a novel generalized correlation function called correntropy function achieving high accuracy with the same computational cost of traditional correlation based methods. The performances of our transcription system have been measured on examples extracted by some repositories available on internet; they consist in sung melodies and hummed queries. Results show the ability of our transcription procedure to cope with query-by-humming systems, as well as with monophonic singing transcription. Performances can be easily evaluated by downloading from the authors web site the original PCM files and the corresponding MIDI files produced by the proposed transcription algorithm.	algorithm;algorithmic efficiency;download;image noise;inference engine;information retrieval;internet;midi;medical transcription;numerical analysis;performance;pitch detection algorithm;query by humming;transcription (software);video post-processing;web application	Mario Antonelli;Antonello Rizzi	2008	2008 IEEE 10th Workshop on Multimedia Signal Processing	10.1109/MMSP.2008.4665216	estimation;kernel;the internet;speech recognition;time–frequency analysis;computer science;pattern recognition;band-pass filter;accuracy and precision;correlation function;correlation;statistics	Comp.	-7.5208858699579295	-92.88022838665756	51633
3cd33b6400525166047b2cd133cbd971f2d9ea1e	bayesian adaptive learning of the parameters of hidden markov model for speech recognition	forward backward;bayesian methods hidden markov models speech recognition state estimation maximum likelihood estimation computer science prototypes parameter estimation inference algorithms;discrete hidden markov model;gaussian mixture;theoretical framework;prior parameter specification;gaussian processes;helium;hidden markov model;moment estimates;bayes methods;prototypes;semi continuous hmm;state observer;bayesian adaptive learning;bayesian methods;prior densities;maximum likelihood estimation;state estimation;segmental map algorithm;hidden markov models;empirical bayes method;segmental quasi bayes algorithm;speaker adaptive hmm bayesian adaptive learning hidden markov model speech recognition discrete hidden markov model semi continuous hmm gaussian mixture state observation densities forward backward map maximum a posteriori segmental map algorithm segmental quasi bayes algorithm state specific mixture coefficients prior densities moment estimates prior parameter specification;adaptive learning;speech recognition;inference algorithms;gaussian mixture state observation densities;computer science;gaussian processes speech recognition bayes methods learning artificial intelligence hidden markov models maximum likelihood estimation adaptive estimation;maximum a posteriori;parameter estimation;learning artificial intelligence;speaker adaptation;article;speaker adaptive hmm;adaptive estimation;forward backward map;state specific mixture coefficients	In this paper a theoretical framework for Bayesian adaptive learning of discrete HMM and semi continuous one with Gaussian mixture state observation densities is presented Corre sponding to the well known Baum Welch and segmental k means algorithms respectively for HMM training formulations of MAP maximum a posteriori and segmental MAP estima tion of HMM parameters are developed Furthermore a computationally e cient method of the segmental quasi Bayes estimation for semi continuous HMM is also presented The important issue of prior density estimation is discussed and a simpli ed method of moment estimate is given The method proposed in this paper will be applicable to some prob lems in HMM training for speech recognition such as sequential or batch training model adaptation and parameter smoothing etc	baum–welch algorithm;hidden markov model;k-means clustering;markov chain;semiconductor industry;smoothing;speech recognition;welch's method;whole earth 'lectronic link	Qiang Huo;Chorkin Chan;Chin-Hui Lee	1995	IEEE Trans. Speech and Audio Processing	10.1109/89.466661	bayesian probability;computer science;maximum a posteriori estimation;machine learning;pattern recognition;gaussian process;prototype;maximum likelihood;estimation theory;helium;state observer;adaptive learning;hidden markov model;statistics	ML	-19.31389481357494	-92.74275921642976	51648
2a5f4d1c08d70e9396b4ca9e0bfde9fe05fc238d	low-latency incremental speech transcription in the synface project	real time;computer and information science;look ahead;low latency;classification accuracy;data och informationsvetenskap	In this paper, a real-time decoder for low-latency online speech transcription is presented. The system was developed within the Synface project, which aims to improve the possibilities for hard of hearing people to use conventional telephony by providing speech-synchronized multimodal feedback. This paper addresses the specific issues related to HMM-based incremental phone classification with real-time constraints. The decoding algorithm described in this work enables a trade-off to be made between improved recognition accuracy and reduced latency. By accepting a longer latency per output increment, more time can be ascribed to hypothesis look-ahead and by that improve classification accuracy. Experiments performed on the Swedish SpeechDat database show that it is possible to generate the same classification as is produced by non-incremental decoding using HTK, by adopting a latency of approx. 150 ms or more.	algorithm;approximation;htk (software);hidden markov model;medical transcription;multimodal interaction;real-time clock;real-time transcription;transcription (software)	Alexander Seward	2003			voice activity detection;speaker recognition;speech recognition;computer science;machine learning;low latency	NLP	-21.34109691903006	-87.98861716879478	51697
1d340fe19026b0359bde23fcd7299a99a240bd15	robust cnn-based speech recognition with gabor filter kernels		As has been extensively shown, acoustic features for speech recognition can be learned from neural networks with multiple hidden layers. However, the learned transformations may not sufficiently generalize to test sets that have a significant mismatch to the training data. Gabor features, on the other hand, are generated from spectro-temporal filters designed to model human auditory processing. In previous work, these features are used as inputs to neural networks, which improved word accuracy for speech recognition in the presence of noise. Here we propose a neural network architecture called a Gabor Convolutional Neural Network (GCNN) that incorporates Gabor functions into convolutional filter kernels. In this architecture, a variety of Gabor features served as the multiple feature maps of the convolutional layer. The filter coefficients are further tuned by back-propagation training. Experiments used two noisy versions of the WSJ corpus: Aurora 4, and RATS re-noised WSJ. In both cases, the proposed architecture performs better than other noise-robust features that we have tried, namely, ETSI-AFE, PNCC, Gabor features without the CNN-based approach, and our best neural network features that don’t incorporate Gabor functions.	acoustic cryptanalysis;analog front-end;artificial neural network;aurora;backpropagation;coefficient;convolutional neural network;gabor atom;gabor filter;network architecture;software propagation;speech recognition;the wall street journal	Shuo-Yiin Chang;Nelson Morgan	2014			pattern recognition;architecture;convolutional neural network;speech recognition;artificial intelligence;training set;artificial neural network;computer science;filter design;gabor filter	ML	-16.902692306083075	-88.97348926316461	51753
7701ef14129c91174179e5166dfa978401919d55	combination of multiple classifiers using probabilistic dictionary and its application to postcode recognition	low frequency;single character level;probabilistic dictionary;postcode level;combination of multiple classifiers;pattern recognition;numeral recognition;postcode recognition;handwritten character recognition	Combination of multiple classi ers is regarded as an e+ective strategy for achieving a practical system of handwritten character recognition. A great deal of research on the methods of combining multiple classi ers has been reported to improve the recognition performance of single characters. However, in a practical application, the recognition performance of a group of characters (such as a postcode or a word) is more signi cant and more crucial. With the motivation of optimizing the recognition performance of postcode rather than that of single characters, this paper presents an approach to combine multiple classi ers in such a way that the combination decision is carried out at the postcode level rather than at the single character level, in which a probabilistic postcode dictionary is utilized as well to improve the postcode recognition ability. It can be seen from the experimental results that the proposed approach markedly improves the postcode recognition performance and outperforms the commonly used methods of combining multiple classi ers at the single character level. Furthermore, the sorting performance of some particular bins with respect to the postcodes with low frequency of occurrence can be improved signi cantly at the same time. ? 2002 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.	data dictionary;handwriting recognition;optical character recognition;pattern recognition;postal;sorting;systems design	Yue Lu;Chew Lim Tan	2002	Pattern Recognition	10.1016/S0031-3203(01)00237-0	speech recognition;computer science;machine learning;pattern recognition;low frequency	Vision	-12.80831118170784	-89.06590872948992	51754
b4a3997b15cc252109d92f79a3d4e5b19b4f4c10	pronunciation ambiguity vs. pronunciation variability in speech recognition	acoustic signal processing;automatic speech recognition;speech recognition humans predictive models speech processing speech analysis acoustics surface treatment context modeling natural languages automatic speech recognition;hidden variables;hidden variable pronunciation ambiguity pronunciation variability alternate pronunciation spontaneous speech citation form pronunciation modeling automatic speech recognition phonetic units canonical pronunciations manual phonetic transcription conversational speech ambiguity human labelers surface form acoustic evidence pronunciation change alternate phone baseform surface form pair;speech recognition;spontaneous speech;acoustic signal processing speech recognition linguistics;linguistics	"""It is widely acknowledged that pronunciations in spontaneous speech di er signi cantly from citation form. For this reason, pronunciation modeling has received considerable attention in recent automatic speech recognition literature. Most of the attention however has focussed on describing an alternate pronunciation as a di erent sequence of phonetic units using the same inventory of phones which describe canonical pronunciations. Analysis of manual phonetic transcription of conversational speech reveals a large number (>20%) of cases of genuine ambiguity: instances where human labelers disagree on the identity of the surface form. In this paper, we investigate and characterize the acoustic evidence in the context of this ambiguity. We show that when a pronunciation change occurs, it is often the case that neither the canonical nor the alternate phone represent the acoustics very well. Based on this analysis, two methods for accommodating pronunciation ambiguity are developed. The rst method attempts to resolve the ambiguity by separately modeling each baseform/surfaceform pair. The second method treats the surface form as a hidden variable and \averages out"""" the ambiguity."""	acoustic cryptanalysis;hidden variable theory;spatial variability;speech recognition;spontaneous order;transcription (software)	Murat Saraclar;Sanjeev Khudanpur	2000		10.1109/ICASSP.2000.862073	natural language processing;speech recognition;speech corpus;computer science;speech;hidden variable theory;speech analytics	NLP	-11.922848076475626	-82.71940466734355	51763
72dcb5ed67596465377f8e3ae9c4ec981a4c3a4c	arida: an arabic interlanguage database and its applications: a pilot study		This paper describes a pilot study in which we collected a small learner corpus of Arabic, developed a tagset for error annotation and performed simple Computer-aided Error Analysis (CEA) on the data. For this study, we adapted the French Interlanguage Database (FRIDA) (Granger, 2003a) tagset to the data. We chose FRIDA in order to keep our tagging in line with a known standard. The paper describes the need for learner corpora, the learner data we have collected, the tagset we have developed, its advantages and disadvantages, the preliminary CEA results, other potential applications of the error-annotated corpus of Arabic, and the error frequency distribution of both proficiency levels as well as our ongoing work.	corpus linguistics;error analysis (mathematics);interference (communication);nearest neighbor search;service-level agreement;text corpus;vocabulary	Anna Feldman;Ghazi Abuhakema;Eileen Fitzpatrick	2008			language transfer;natural language processing;artificial intelligence;lexis;second-language acquisition;corpus linguistics;first language;computer science;interlanguage;vocabulary;grammar	NLP	-25.00185407466594	-83.51002197304074	51912
3053b11bf013c80718ca7069cb60c148993e36ef	a case based approach to expressivity-aware tempo transformation	raisonnement base sur cas;razonamiento fundado sobre caso;case base reasoning;musical note;articulo;musica;acoustique musicale;intelligence artificielle;music tempo transformation;musical acoustics;sound recording;musique;tempo transformation;emotion emotionality;enregistrement son;expressive performance;acustica musical;nota musical;note musique;registro sonido;artificial intelligence;emotion emotivite;melodia;emocion emotividad;inteligencia artificial;case based reasoning;case based reasoning expressive performance;music;melody;melodie	The research presented in this paper focuses on global tempo transformations of monophonic audio recordings of saxophone jazz performances. We are investigating the problem of how a performance played at a particular tempo can be rendered automatically at another tempo, while preserving naturally sounding expressivity. Or, differently stated, how does expressiveness change with global tempo. Changing the tempo of a given melody is a problem that cannot be reduced to just applying a uniform transformation to all the notes of a musical piece. The expressive resources for emphasizing the musical structure of the melody and the affective content differ depending on the performance tempo. We present a case-based reasoning system called TempoExpress for addressing this problem, and describe the experimental results obtained with our approach.	automatic sounding;case-based reasoning;expressive power (computer science);performance;reasoning system	Maarten Grachten;Josep Lluís Arcos;Ramón López de Mántaras	2006	Machine Learning	10.1007/s10994-006-9025-9	case-based reasoning;melody;speech recognition;computer science;artificial intelligence;musical acoustics;music	AI	-7.72287363513675	-87.0289795667555	51925
574bc71422c4c29f766f3238db41ce438d46f5e8	an ultrasound study of coronal places of articulation in central arrernte: apicals, laminals and rhotics		This study presents ultrasound data from six female speakers of Arrernte, a language which has four coronal places of articulation: dental, alveolar, retroflex, and (alveo-)palatal. We present tongue contours for stop, nasal and lateral productions of each of these four coronal places – /t̪ t ʈ c/, /n̪ n ɳ ɲ/ and / l ̪ l ɭ ʎ/ – as well as for the two contrastive rhotics of Arrernte, the alveolar trill /r/ and the retroflex glide /ɻ/. Results show that the palatal is characterized by a high and front tongue position, and the dental is characterized by a relatively low and flat tongue in the mid-to-front portion. Consistent with previous studies, the alveolar and the retroflex are difficult to discriminate, and potential differences are discussed. The rhotics are characterized by a low front portion of the tongue, and a retracted back portion of the tongue. The front portion of the tongue is lower for the alveolar trill than for the retroflex glide; and for most speakers, the back of the tongue is more retracted for the trill than for the glide. The back of the tongue is more retracted for the rhotics than for the corresponding stops and nasals, although there is evidence this part of the tongue patterns similarly for the laterals and the rhotics. It is suggested that this posterior constriction for the various liquid sounds arises from the interaction between bio-mechanical properties of the tongue and manner of articulation requirements. Crown Copyright 2017 Published by Elsevier Ltd. All rights reserved.	biconnected component;british informatics olympiad;crown group;glide os;lateral thinking;requirement	Marija Tabain;Richard Beare	2018	J. Phonetics	10.1016/j.wocn.2017.09.006	anatomy;coronal plane;psychology;communication;liquid consonant;manner of articulation;ultrasound;place of articulation;tongue	HCI	-9.292994773884455	-81.68147792403248	51972
dff2273a46b8161343025de4b19cc5e36d4f61d7	automated speaker recognition in real world conditions: controlling the uncontrollable	speaker recognition	The current development of automatic speaker recognition technology may provide a new method to augment or replace the traditional method offered by qualified experts using aural and spectrographic analysis. The most promising of these automated technologies are based on statistical hypothesis testing methods involving likelihood ratios. The null hypothesis is generated using a universal background model composed of a large population of speakers. However, techniques with excellent performance in standardized evaluations (NIST trials) may not work perfectly in the real world. By defining and controlling the input speech samples carefully, we show quantitative differences in performance for different factors affecting a speaker population, and discuss on-going efforts to improve the accuracy rate for use in real world conditions. In this paper we will address two issues related to the factors that affect the system performance, namely the speech signal duration and the signalto-noise ratio.	signal-to-noise ratio;speaker recognition	Hirotaka Nakasone	2003			speech recognition;natural language processing;speaker diarisation;computer science;speaker recognition;artificial intelligence	HCI	-13.587083007793169	-90.67273570033413	52022
8b8c3c0d60bf19dfba1c824b8a64609ad878f88f	phoneme set and pronouncing dictionary creation for large vocabulary continuous speech recognition of vietnamese		This paper describes our study on solving two basic problems of large vocabulary continuous speech recognition (LVCSR) of Vietnamese, which can be used as a standard reference for Vietnamese researchers and other researchers who are interested in Vietnamese language. First, a standard phoneme set is proposed with its corresponding grapheme-to-phoneme map. This phoneme set is the core to solve other problems related to LVCSR of Vietnamese. Then the creation of standard pronouncing dictionary based on the grapheme-to-phoneme map and the analysis of Vietnamese syllable is also described. Finally, we present the results on LVCSR using different types of pronouncing dictionary, which show some interesting aspects of Vietnamese language such as the structure of Vietnamese syllable and the effect of tone in the relationship with syllable.	dictionary;speech recognition;vocabulary	Thien Chuong Nguyen;Josef Chaloupka	2013		10.1007/978-3-642-40585-3_50	natural language processing;speech recognition;linguistics	ML	-22.395085656794183	-82.34836109783726	52240
6f465d5f545a942624a7756bc3c26baa7902d300	on the use of quality measures for text-independent speaker recognition		The use of quality information on automatic recognition systems is studied. From an apparent definition of what constitutes a quality measure, a framework for the successful exploitation of the quality information is derived. Potential applications are also introduced at different phases of the recognition process, namely: enrollment, scoring and multi-level fusion stages. Traditional likelihood scoring stage is further developed providing guidelines for the practical application of the proposed ideas. Preliminary experiments corroborate the benefits of the proposed quality-guided recognition approach. In particular, a frame-level quality measure meeting a goodness criterion based on deviation from the fundamental frequency is used, obtaining encouraging initial results.	experiment;speaker recognition	Daniel Garcia-Romero;Julian Fiérrez;Joaquín González-Rodríguez;Javier Ortega-Garcia	2004			speech recognition;speaker diarisation;speaker recognition;computer science	Vision	-17.94839760856378	-84.91762816437391	52244
45c30c3420e595e402400a666441c5ab67d62283	the effect of formant trajectories and phoneme durations on vowel intelligibility	background noise;speech intelligibility speech enhancement;speech intelligibility;formant trajectory;speech synthesis;speech enhancement formant trajectory phoneme duration vowel intelligibility clear speech conversational speech speech intelligibility;acoustics;speech processing;speech enhancement speech synthesis frequency speech processing speech analysis moon natural languages background noise noise measurement bandwidth;speech analysis;auditory system;speech;natural languages;speech enhancement;indexing terms;data mining;noise measurement;trajectory;moon;vowel intelligibility;speech enhancement speech intelligibility speech analysis speech processing;phoneme duration;bandwidth;frequency;clear speech;conversational speech	We examined how much listeners can benefit from listening to “clear” (CLR) speech compared to “conversational” (CNV) speech, both spoken at different speaking rates. Vowel intelligibilities of four front vowels (/i:/, /I/, /E/, and /ei/) in background noise were measured with four speaking styles (CNV/SLOW, CNV, CLR, and CLR/FAST). Results showed only tense vowels of CLR speech had a significant difference between CNV and CLR speaking styles, after energy and F0 contour were normalized. We synthesized hybrid (HYB) speech whose formant features were equal to those of CLR speech, while all other features were taken from CNV speech. Primary conclusions from this study are (1) naturally-spoken fast CLR speech was not as intelligible as CLR speech, (2) enhancing formant frequencies to resemble those of CLR speech was effective at improving vowel intelligibility, and (3) spectral tilt and formant bandwidths were not contributing factors to the CLR speech benefit.	intelligibility (philosophy)	Akiko Amano-Kusumoto;John-Paul Hosom	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4960674	speech recognition;index term;computer science;natural satellite;noise measurement;speech;trajectory;frequency;speech processing;background noise;natural language;speech synthesis;intelligibility;bandwidth	NLP	-10.981850679711505	-84.27817085006441	52280
d2f829a618da5eb7e7271136f4301508b79e2b81	comparative study of three time-frequency representations with applications to a novel correlation method	time frequency analysis correlation heart pattern recognition pattern matching application software acoustic noise fourier transforms redundancy character recognition;cardiology;time frequency;extrapolation;acoustic signal processing;correlation methods;acoustic signal processing cardiology time frequency analysis correlation methods signal representation extrapolation medical signal processing pattern matching;time frequency representation;pattern matching;signal representation;time frequency analysis;medical signal processing;pattern matching time frequency representations correlation method signal representation time frequency analysis significant feature extrapolation opening snap third heart sound time frequency transforms pattern recognition	The effect of three time-frequency representations on a novel correlation algorithm is studied. By representing a signal in the time-frequency domain, a redundant representation of the signal is obtained. The algorithm presented relies on such redundancies to extrapolate some significant features of the signal. The developed scheme has been applied to heart sound analysis using real recordings from patients, where the opening snap (OS) is distinguished from the third heart sound (S3). The results for the three time-frequency transforms are compared and very encouraging results have been obtained with S-transform.	algorithm;extrapolation;operating system;s transform;time–frequency analysis	Ervin Sejdic;Jin Jiang	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1326337	multidimensional signal processing;computer vision;speech recognition;time–frequency analysis;discrete-time signal;analog signal;computer science;pattern recognition;mathematics;frequency domain	Robotics	-7.915302667328525	-91.8544470693283	52286
fd46bd78a51d6a638a523ec437b5104291517b3a	a novel feature extraction algorithm for classification of bird flight calls	spectrogram;feature extraction birds mel frequency cepstral coefficient hidden markov models spectrogram classification algorithms;mel frequency cepstral coefficient;hidden markov models;birds;feature extraction;classification algorithms	Acoustic monitoring of birds in the vicinity of wind turbines is becoming an important public policy issue. Acoustic monitoring involves preprocessing, feature extraction and classification. A novel Spectrogram-based Image Frequency Statistics (SIFS) feature extraction algorithm has been developed. Features extracted from proposed algorithms were then combined with various classification algorithms such as k-NN, Multilayer Perceptron (MLP) and Hidden Markov Models (HMM) and Evolutionary Neural Network (ENN). SIFS and MMS algorithms, combined with ENN, provided the most accurate results. Proposed algorithms were tested with real data collected during spring migration around Lake Erie in Ohio.	acoustic cryptanalysis;artificial neural network;feature extraction;hidden markov model;k-nearest neighbors algorithm;markov chain;multilayer perceptron;preprocessor;quad flat no-leads package;spectrogram;statistical classification;superheterodyne receiver	Selin Bastas;Mohammad Wadood Majid;Golrokh Mirzaei;Jeremy Ross;Mohsin M. Jamali;Peter V. Gorsevski;Joseph P. Frizado;Verner P. Bingman	2012	2012 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2012.6271580	statistical classification;speech recognition;feature extraction;computer science;machine learning;spectrogram;pattern recognition;hidden markov model	EDA	-7.028990180928264	-90.36908025713414	52339
df776516a37e700c97bb7c67bc5255bfc96dcd7f	mpe-based discriminative linear transforms for speaker adaptation	minimum phone error;confusion network;discriminative linear transforms;linear transformation;speech recognition;computational linguistics;parameter estimation;speaker adaptation;linguistique informatique;language model;conversational telephone speech	In this paper, the use of discriminative linear transforms (DLT) is investigated to construct speaker adaptive speech recognition systems, where a discriminative criterion rather than ML is used for transform parameter estimation. The minimum phone error (MPE) criterion is investigated for DLT estimation, by making use of a so-called weak-sense auxiliary function to derive the estimation formulae. An implementation based on lattices is used for DLT statistics accumulation, where the use of a weakened language model allows more confusion data to be included. To improve DLT estimation for unsupervised adaptation, a method of incorporating word correctness information of the supervision into transform estimation is developed. The confidence scores calculated by confusion network decoding are used to represent the word correctness and weight the numerator statistics during DLT estimation. This makes the DLT estimation less sensitive to errors in the supervision. Experiments on transcription of read newspaper data and on conversational telephone speech transcription have shown the improvements of DLT over MLLR for both supervised and unsupervised adaptation, and the effectiveness of confidence scores for improving both normal and DLT-based MLLR adaptation.	hp multi-programming executive	Lan Wang;Philip C. Woodland	2008	Computer Speech & Language	10.1016/j.csl.2007.09.001	speech recognition;computer science;computational linguistics;machine learning;pattern recognition;linear map;estimation theory;language model	NLP	-18.965180254165922	-91.85141402068646	52359
2a7cd1cb6f5da6c0ac547a2fd814bc68a7131094	speech trajectory discrimination using the minimum classification error learning	traitement signal;hidden markov models polynomials maximum likelihood estimation state estimation stochastic processes speech analysis space stations power generation speech recognition;estado estacionario;learning algorithm;phonetic classification;modelo markov;maximum likelihood;metodo descenso;hidden markov model;gradient method;speech processing;maximum vraisemblance;tratamiento palabra;traitement parole;maximum likelihood estimation pattern classification hidden markov models speech recognition;algorithme apprentissage;state dependence;indexing terms;maximum likelihood estimation;stationary state;methode gradient;performance improvement;markov model;hidden markov models;metodo gradiente;trajectory generation;signal processing;pattern classification;etat stationnaire;speech recognition;classification phonetique;discriminative training;stochastic model;modele markov;minimum classification error;descent method;algoritmo aprendizaje;procesamiento senal;modelo estocastico;mce trained trended hmm speech trajectory discrimination minimum classification error learning maximum likelihood training algorithm state dependent polynomial coefficients stochastic trajectory model trended hidden markov model hmm smoothness constrained state bound speech trajectories piecewise constant degraded trajectories speech class phonetic classification;modele stochastique;maxima verosimilitud;training algorithm;methode descente;erreur classification minimale	In this paper, we extend the maximum likelihood (ML) training algorithm to the minimum classification error (MCE) training algorithm for discriminatively estimating the state-dependent polynomial coefficients in the stochastic trajectory model or the trended hidden Markov model (HMM) originally proposed in [2]. The main motivation of this extension is the new model space for smoothness-constrained, state-bound speech trajectories associated with the trended HMM, contrasting the conventional, stationary-state HMM, which describes only the piecewise-constant “degraded trajectories” in the observation data. The discriminative training implemented for the trended HMM has the potential to utilize this new, constrained model space, thereby providing stronger power to disambiguate the observational trajectories generated from nonstationary sources corresponding to different speech classes. Phonetic classification results are reported which demonstrate consistent performance improvements with use of the MCE-trained trended HMM both over the regular ML-trained trended HMM and over the MCEtrained stationary-state HMM.	algorithm;coefficient;discriminative model;hidden markov model;linuxmce;markov chain;polynomial;stationary process;stationary state;statistical classification	Rathinavelu Chengalvarayan;Li Deng	1998	IEEE Trans. Speech and Audio Processing	10.1109/89.725317	speech recognition;computer science;machine learning;pattern recognition;mathematics;maximum likelihood;hidden markov model;statistics	ML	-19.83034100930261	-92.05434702674395	52419
766173f4be9063d1bcd5246b40e2e235a6220a0f	feature and signal enhancement for robust speaker identification of g.729 decoded speech	wireless security;g 729 coding distortion;ensemble systems;robust speaker identification	For wireless remote access security, there is an emerging need for biometric speaker identification systems (SID) to be robust to speech coding distortion. This paper presents results on a Gaussian mixture model (GMM) based SID system that is trained on clean speech and tested on the decoded speech of the G.729 codec. To mitigate the performance loss due to mismatched training and testing conditions, five robust features, two enhancement approaches and three fusion strategies are used. The first enhancement method is feature compensation based on the affine transform. The second is the McCree signal enhancement approach based on the spectral envelope information in the G.729 bit stream. Ensemble systems using decision level, score fusion and Borda count are studied. The best performance is obtained by performing signal enhancement, feature compensation and decision level fusion. This results in an identification success rate (ISR) of 89.8%.	g.729	Kalpesh Raval;Ravi P. Ramachandran;Sachin S. Shetty;Brett Y. Smolenski	2012		10.1007/978-3-642-34500-5_41	speech recognition;computer science;pattern recognition	ML	-13.994794891636143	-90.8679595564744	52511
744b549b32a03cb9c3d423ba9ed7c48cff54838e	lexical affect sensing: are affect dictionaries necessary to analyze affect?	spontaneous dialogues;statistical method;affect dictionaries;or phrases;lexical modality;lexical affect sensing;emotion detection	Recently, there has been considerable interest in the automated recognition of affect from written and spoken language. In this paper, we investigate how information on a speaker’s affect may be inferred from lexical features using statistical methods. Dictionaries of affect offer great promise to affect sensing since they contain information on the affective qualities of single words or phrases that may be employed to estimate the emotional tone of the corresponding dialogue turn. We investigate to what extent such information may be extracted from general-purpose dictionaries in comparison to specialized dictionaries of affect. In addition, we report on results obtained for a dictionary that was tailored to our corpus.	dictionary;experiment;general-purpose markup language;general-purpose modeling;real-time clock;speech corpus;text corpus;turing completeness;word lists by frequency;wordnet	Alexander Osherenko;Elisabeth André	2007		10.1007/978-3-540-74889-2_21	psychology;natural language processing;speech recognition;communication	NLP	-19.00112645600899	-82.20251266452149	52555
5166596f739b20d59b2489910d635bea341e91c2	bimodal recurrent neural network for audiovisual voice activity detection.		Voice activity detection (VAD) is an important preprocessing step in speech-based systems, especially for emerging handfree intelligent assistants. Conventional VAD systems relying on audio-only features are normally impaired by noise in the environment. An alternative approach to address this problem is audiovisual VAD (AV-VAD) systems. Modeling timing dependencies between acoustic and visual features is a challenge in AV-VAD. This study proposes a bimodal recurrent neural network (RNN) which combines audiovisual features in a principled, unified framework, capturing the timing dependency within modalities and across modalities. Each modality is modeled with separate bidirectional long short-term memory (BLSTM) networks. The output layers are used as input of another BLSTM network. The experimental evaluation considers a large audiovisual corpus with clean and noisy recordings to assess the robustness of the approach. The proposed approach outperforms audio-only VAD by 7.9% (absolute) under clean/ideal conditions (i.e., high definition (HD) camera, close-talk microphone). The proposed solution outperforms the audio-only VAD system by 18.5% (absolute) when the conditions are more challenging (i.e., camera and microphone from a tablet with noise in the environment). The proposed approach shows the best performance and robustness across a varieties of conditions, demonstrating its potential for real-world applications.	av-test;acoustic cryptanalysis;artificial neural network;long short-term memory;microphone;modality (human–computer interaction);preprocessor;random neural network;recurrent neural network;robustness (computer science);tablet computer;text corpus;unified framework;voice activity detection	Fei Tao;Carlos Busso	2017		10.21437/Interspeech.2017-1573	speech recognition;voice activity detection;pattern recognition;artificial intelligence;recurrent neural network;computer science	AI	-14.644157840962176	-89.35057875001822	52650
c85361016417ee89ecf291d1ea0f3f5a8a0da059	developing and testing general models of spoken dialogue system peformance		The design of methods for performance evaluation is a major open research issue in the area of spoken language dialogue systems. This paper presents the PARADISE methodology for developing predictive models of spoken dialogue performance, and shows how to evaluate the predictive power and generalizability of such models. To illustrate the methodology, we develop a number of models for predicting system usability (as measured by user satisfaction), based on the application of PARADISE to experimental data from two different spoken dialogue systems. We compare both linear and tree-based models. We then measure the extent to which the models generalize across different systems, different experimental conditions, and different user populations, by testing models trained on a subset of the corpus against a test set of dialogues. The results show that the models generalize well across the two systems, and are thus a first approximation towards a general performance model of system usability.	dialog system;open research;order of approximation;performance evaluation;population;predictive modelling;spoken dialog systems;test set;usability	Marilyn A. Walker;Candace A. Kamm;Julie E. Boland	2000			experimental data;artificial intelligence;natural language processing;generalizability theory;spoken language;computer science;predictive power;open research;test set;usability	NLP	-25.282530537386226	-88.45963990833235	52738
246a1fe44700435f129997aa033480146069ae3b	mlp based phoneme detectors for automatic speech recognition	broadcast news;detectors;broadcast news task mlp based phoneme detector automatic speech recognition phoneme posterior probability estimation multilayer perceptron based phoneme detector phonetic event detector acoustic signal segmental conditional random field scrf;segmental conditional random fields phoneme posteriors multi layer perceptrons;probability;hidden markov model;acoustics;multilayer perceptrons;speech processing;posterior probability;multi layer perceptrons;speech recognition multilayer perceptrons probability speech processing;automatic speech recognition;hidden markov models;acoustics feature extraction detectors hidden markov models viterbi algorithm;viterbi algorithm;feature extraction;segmental conditional random fields;conditional random field;speech recognition;multi layer perceptron;phoneme posteriors	Phoneme posterior probabilities estimated using Multi-Layer Perceptrons (MLPs) are extensively used both as acoustic scores and features for speech recognition. In this paper we explore a different application of these posteriors - as phonetic event detectors for speech recognition. We show how these detectors can be built to reliably capture phonetic events in the acoustic signal by integrating both acoustic and phonetic information about sound classes. These event detectors are used along with Segmental Conditional Random Fields (SCRFs) to improve the performance of speech recognition systems on the Broadcast News task.	acoustic cryptanalysis;acoustic fingerprint;conditional random field;memory-level parallelism;perceptron;sensor;speech recognition	Samuel Thomas;Patrick Nguyen;Geoffrey Zweig;Hynek Hermansky	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947485	detector;speech recognition;feature extraction;viterbi algorithm;computer science;machine learning;pattern recognition;probability;posterior probability;multilayer perceptron;conditional random field;hidden markov model	Vision	-16.340474286085517	-89.83655661682951	52783
192034345d646a8b085982260ff5e62e4cf22b97	estimation of speech embedded in a reverberant and noisy environment by independent component analysis and wavelets	convolution;speech analysis working environment noise independent component analysis acoustic noise microphones humans frequency band pass filters speech enhancement ear;real world experiments embedded speech estimation reverberant environment noisy environment independent component analysis wavelets speech signal enhancement linear convolutive mixture statistically independent sound sources ica adaptive auditory filter banks pitch tracking computer simulations;independent component analysis;speech enhancement;acoustic filters;wavelet transforms;principal component analysis;tracking;noise;tracking speech enhancement noise principal component analysis wavelet transforms convolution acoustic filters	In this paper, we develop a system for enhancement of the speech signal with highest energy from a linear convolutive mixture of n statistically independent sound sources recorded by m microphones, where m<n. In this system we use the concept of independent component analysis (ICA) along with adaptive auditory filter banks and pitch tracking. Computer simulations and real-world experiments carried out in an actual room and measured through objective and subjective measures confirm the validity of the proposed algorithm.	bank (environment);embedded system;embedding;experiment;filter bank;independent computing architecture;independent component analysis;microphone;pitch detection algorithm;simulation;wavelet	Allan Kardec Barros;Tomasz M. Rutkowski;Fumitada Itakura;Noboru Ohnishi	2002	IEEE transactions on neural networks	10.1109/TNN.2002.1021889	independent component analysis;speech recognition;computer science;noise;machine learning;speech processing;mathematics;tracking;convolution;wavelet transform;principal component analysis	ML	-10.393047982567124	-90.1893981480795	52818
e24a5c56e9e4c4f5678c5242fa3653b02e979143	constraints on the generation of adjunct clauses	natural language generation system;generation system;adjunct clause;satisfying account;computational framework;adjunct construction;infinitival relative clause;particular english construction;rationale clause;purpose clause;descriptive linguistics;satisfiability	"""This paper presents an analysis of a family of particular English constructions, all of which roughly express """"purpose"""". In particular we look at the purpose clause, rationale .clause, and infinitival relative clause. We (1) show that couching the analysis in a computational framework, specifically generation, provides a more satisfying account than analyses based strictly on descriptive linguistics, (2) describe an implementation of our analysis in the natural language generation system MUMBLE-86, and (3) discuss how our architecture improves upon the techniques used by other generation systems for handling these and other adjunct constructions."""	computation;design rationale;natural language generation	Alison K. Huettner;Marie M. Vaughan;David D. McDonald	1987			natural language processing;dependent clause;linguistics;algorithm;satisfiability	NLP	-32.42711866721351	-81.54607806683458	52948
c757325dc67fa9a59245cb3912904f3ab76e09fb	enhance the word vector with prosodic information for the recurrent neural network based tts system		Word embedding, which is a dense and low-dimensional vector representation of word, is recently used to replace of the conventional prosodic context as an input feature to the acoustic model of a TTS system. However, these word vectors trained from text data may encode insufficient information related to speech. This paper presents a post-filtering approach to enhance the raw word vectors with prosodic information for the TTS task. Based on a publicly available speech corpus with manual prosodic annotation, a post-filter can be trained to transform the raw word vectors. Experiment shows that using the enhanced word vectors as an input to the neural network-based acoustic model improves the accuracy of the predicted F0 trajectory. Besides, we also show that the enhanced vectors provide better initial values than the raw vectors for error back-propagation of the network, which results in further improvement.		Xin Wang;Shinji Takaki;Junichi Yamagishi	2016		10.21437/Interspeech.2016-390	natural language processing;speech recognition;pattern recognition	NLP	-18.178453834465365	-87.48651625650517	52965
b1545c36643cb2dc402a8037beeac8d4f14f1c03	a study of f1 correlation with f0 in a tone language: case of thai	pragmatics;speaker characterization;empirical study;acoustic space;thai language;forensic applications;acoustic space pitch tone formant frequencies;formant frequencies;acoustics;speech processing;forensic applications f1 correlation tone language thai language f0 variations phonemic tones laryngeal phenomenon speaker characterization;laryngeal phenomenon;frequency measurement;tone language;resonant frequency;tone;pragmatics correlation acoustics tongue graphics resonant frequency frequency measurement;tongue;speech processing natural language processing;f0 variations;f1 correlation;pitch;correlation;natural language processing;graphics;phonemic tones	This is an empirical study of vowels of Thai language as spoken in Bangkok in terms of two parameters, namely - tones, levels and contours as reflected in F0 variations, and lower formant, F1 which is high for open/ low vowels and low for close/ high vowels. We examine each one of the nine long vowels with all the five phonemic tones to find out the correlation between the pitch of voice, a laryngeal phenomenon and the first formant which is part of the resonance phenomenon. The results give us some interesting hypotheses to be explored further with data from different tonal and non-tonal languages. F1-F0 correlations, if established will have significant implications for TTS studies, for comparison across speakers for speaker characterization and forensic applications.	netware file system;resonance	Sulaganya Punyayodhin;Deepshikha Misra;Ritu Yadav;Vaishna Narang	2010	2010 International Conference on Asian Language Processing	10.1109/IALP.2010.85	speech recognition;formant;resonance;computer science;graphics;acoustic space;speech processing;pitch;linguistics;empirical research;correlation;pragmatics	NLP	-10.766208888418067	-83.52223707118759	53084
1a01678c44ca20bddeda663975adefbca8db9ca4	phonological pun-derstanding		Many puns create humor through the relationship between a pun and its phonologically similar target. For example, in “Don’t take geologists for granite” the word “granite” is a pun with the target “granted”. The recovery of the target in the mind of the listener is essential to the success of the pun. This work introduces a new model for automatic target recovery and provides the first empirical test for this task. The model draws upon techniques for automatic speech recognition using weighted finite-state transducers, and leverages automatically learned phone edit probabilities that give insight into how people perceive sounds and into what makes a good pun. The model is evaluated on a small corpus where it is able to automatically recover a large fraction of the pun targets.	algorithm;experiment;microsoft word for mac;n-gram;privacy-enhanced electronic mail;ran raz;speech recognition;syllable;text corpus;transducer	Aaron Jaech;Rik Koncel-Kedziorski;Mari Ostendorf	2016			natural language processing;computer science;artificial intelligence;pun	NLP	-18.552313118932837	-81.39733295837618	53091
3598c33763a1bb3907ddcb9f51c75317f018f000	a warped bandwidth expansion filter	loudness;auditory system;speech enhancement;iir filters speech enhancement loudness hearing;nonlinear bandwidth expansion warped filter biological representation peripheral auditory system hearing speech enhancement formant bandwidths critical band scale perceived loudness;bandwidth filters auditory system frequency resonance linear predictive coding distortion measurement neural engineering speech enhancement psychology;hearing;iir filters	A warped filter is presented as a new speech enhancement method to adjust formant bandwidths on a critical band scale. The warped filter enhances perceived loudness without adding signal energy by exploiting the psychoacoustic nature of the auditory system. The critical band concept in auditory theory states that when the energy in a signal remains constant, loudness increases when the energy spreads beyond a critical bandwidth. A warped filter is proposed and developed to elevate the perceived loudness of clean speech by applying nonlinear bandwidth expansion to the formant regions of vowels in accordance with the critical band scale. The filter has been inspired and motivated by the biological representation of loudness in the peripheral auditory system and the critical band concept of hearing.	bandwidth expansion;critical band;kalman filter;nonlinear system;peripheral;psychoacoustics;speech enhancement;the filter	Marc A. Boillot;John G. Harris	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1415051	loudness;speech recognition;critical band	Robotics	-8.059696762249521	-86.50966453720268	53133
356e2034e323025c82fde6ddfc7fa4261724cd61	covariance updates for discriminative training by constrained line search	line search;acoustic modeling;indexing terms;conference paper;speech recognition;discriminative training	We investigate the recent Constrained Line Search algorithm for discriminative training of HMMs and propose an alternative formula for variance update. We compare the method to standard techniques on a phone recognition task.	discriminative model;line search;search algorithm	Peter Bell;Simon King	2008			speech recognition;index term;computer science;machine learning;pattern recognition;line search	AI	-18.678248211306723	-92.1165374937222	53270
10a47036a981d7986ace6ddf3065f7b4c629a0d3	cmu spoken document retrieval in trec-8: analysis of the role of term frequency tf	term frequency	The participation of Carnegie Mellon University in the TREC-8 Spoken Document Retrieval Track used the basic same Sphinx speech recognition system as in TREC-7. Due to some unfortunate defaults in the parameter setup files, the speech recognizer did not perform in a reasonable manner. We will not analyze the results of the speech recognizer runs, as we believe the results contained abnormal types of errors, and insights or improvements on these errors would not generalize. A thorough examination of the speech recognition condition is given in [3]. However, we did evaluate a slightly modified weighting scheme in the reference (R1) and baseline (B1) conditions, which is described below.	baseline (configuration management);document retrieval;finite-state machine;speech recognition;sphinx	Matthew Siegler;Rong Jin;Alexander G. Hauptmann	1999			information retrieval;natural language processing;coating;document retrieval;metaboric acid;computer science;artificial intelligence	Web+IR	-23.48398608375794	-82.90491728369513	53380
4f5ae459f5d2b6625ba16c6ecab4654634e4850b	the rwth 2010 quaero asr evaluation system for english, french, and german	sublexical components rwth 2010 quaero asr evaluation system broadcast conversational speech data recognition broadcast news recognition automatic speech recognition system english language french language german language word error rates multilayer perception mlp;broadcast news;information resources;word error rate;performance evaluation;hidden markov model;multilayer perceptrons;training;speech;multilayer perceptron;training speech recognition mel frequency cepstral coefficient speech training data hidden markov models;mel frequency cepstral coefficient;training data;automatic speech recognition;hidden markov models;multilayer perceptrons automatic speech recognition;broadcast conversation;speech recognition;system development;broadcasting;speech recognition broadcasting information resources multilayer perceptrons natural language processing performance evaluation;natural language processing;language model	Recognizing Broadcast Conversational (BC) speech data is a difficult task, which can be regarded as one of the major challenges beyond the recognition of Broadcast News (BN).	automated system recovery	Martin Sundermeyer;Markus Nußbaum-Thom;Simon Wiesler;Christian Plahl;Amr El-Desoky Mousa;Stefan Hahn;David Nolden;Ralf Schlüter;Hermann Ney	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5946920	natural language processing;training set;speech recognition;word error rate;computer science;speech;multilayer perceptron;broadcasting;hidden markov model	Robotics	-20.26161554245563	-86.84595397029389	53579
c08ab8f7b47e39855222c5345dbc27253111a844	description of acoustic variations by tree-based phone modeling.				Satoru Hayamizu;Kai-Fu Lee;Hsiao-Wuen Hon	1990			speech recognition	EDA	-14.690060598364354	-86.68330095296506	53652
9b79004ceb841adb28b164ca2d4020d285df1a0e	bayesian analysis of phoneme confusion matrices	speech processing belief networks matrix algebra;ieee transactions;fluid mechanics and acoustics;reliability;elektroteknik och elektronik;confusion matrix cells parametric bayesian approach statistical analysis phoneme confusion matrices mutual information;electrical engineering electronic engineering information engineering;bayes methods;speech processing;speech;stromningsmekanik och akustik;estimation;speech recognition;mutual information;parameter estimation;bayes methods speech ieee transactions speech processing reliability estimation	This paper presents a parametric Bayesian approach to the statistical analysis of phoneme confusion matrices measured for groups of individual listeners in one or more test conditions. Two different bias problems in conventional estimation of mutual information are analyzed and explained theoretically. Evaluations with synthetic datasets indicate that the proposed Bayesian method can give satisfactory estimates of mutual information and response probabilities, even for phoneme confusion tests using a very small number of test items for each phoneme category. The proposed method can reveal overall differences in performance between two test conditions with better power than conventional Wilcoxon significance tests or conventional confidence intervals. The method can also identify sets of confusion-matrix cells that are credibly different between two test conditions, with better power than a similar approximate frequentist method.	approximation algorithm;characterization test;confusion matrix;mutual information;newton's method;synthetic intelligence	Leijon Leijon;Gustav Eje Henter;Martin Dahlquist	2016	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2015.2512039	estimation;speech recognition;computer science;speech;machine learning;pattern recognition;reliability;speech processing;mathematics;mutual information;estimation theory;statistics	ML	-14.382402775569764	-94.09334109077093	53676
5748a7844d6f07bfa7e97b1e2bec205d0ca526cf	use of automatic speech recognition systems for multimedia applications		The need to retrieve information in multimedia content increases the demand for systems that use automatic speech recognition. A speech recognition system enables the computer to interpret audio signals, generating approximate textual transcriptions. These systems are based on probabilistic models that create a robust and correct model for human speech. In this paper it is presented a speech recognition systems architecture and a description of its basic components: the acoustic model, language model, lexical and decoder. The training process of acoustic and language models is also presented. Finally, it its presented how these systems can be used in several applications.	acoustic cryptanalysis;acoustic model;approximation algorithm;language model;lexicon;speech recognition;systems architecture	Marcos Valadão Gualberto Ferreira;Jairo Francisco de Souza	2017		10.1145/3126858.3131630	audio mining;speech analytics;multimedia;voice activity detection;speaker recognition;speech corpus;acoustic model;speech recognition;cache language model;speech processing;computer science	NLP	-16.19328288415106	-86.22425805127975	53694
32471d663c37829ec43103df73ccc14314955c5b	speaker recognition with rival penalized em training	rpem speaker recognition gmm;gaussian processes;gmm;speaker recognition;rpem;speaker recognition gaussian processes pattern recognition;pattern recognition;speaker recognition training data models vectors testing convergence mel frequency cepstral coefficient;recognition rate speaker recognition rival penalized em training gaussian mixture model rival penalized em algorithm pattern recognition problem incremental mode gaussian mixture component reduced order speaker model	The paper considers speaker recognition with Gaussian mixture models trained by a rival penalized EM (RPEM) algorithm. Although RPEM was applied successfully to several pattern recognition problems, our attempt to apply the algorithm in its original form to speaker recognition was not successful. We modified it by adding a discriminative threshold to prevent over penalty on mixture components, and using it with batches of feature vectors rather than the original incremental mode. We applied the modified RPEM to train speaker models with the number of Gaussian mixture components adapted individually to each speaker and used it to perform some basic speaker recognition experiments. The experiments are very reassuring about using the modified RPEM as a training method for GMM based speaker recognition. In settings with limited amount of training data, not only that the algorithm showed nice convergence to reduced order speaker models, but the resulting reduced models achieved better recognition rates than the initial higher order models.	algorithm;experiment;feature vector;google map maker;mixture model;pc speaker;pattern recognition;speaker recognition;teaching method	Avi Matza;Yuval Bistritz	2011	2011 IEEE International Workshop on Machine Learning for Signal Processing	10.1109/MLSP.2011.6064597	speaker recognition;speaker diarisation;speech recognition;computer science;machine learning;pattern recognition	Vision	-18.457835790706614	-92.3053911024823	53766
4a73017b2058968d4767b15e0954b1da2ae8bc95	snr-dependent mixture of plda for noise robust speaker verification		This paper proposes a mixture of SNR-dependent PLDA models to provide a wider coverage on the i-vector spaces so that the resulting i-vector/PLDA system can handle test utterances with a wide range of SNR. To maximise the coordination among the PLDA models, they are trained simultaneously via an EM algorithm using utterances contaminated with noise at various levels. The contribution of a training i-vector to individual PLDA models is determined by the posterior probability of the utterance’s SNR. Given a test i-vector, the marginal likelihoods from individual PLDA models are linear combined based on the the posterior probabilities of the test utterance and the targetspeaker’s utterance. Verification scores are the ratio of the marginal likelihoods. Results based on NIST 2012 SRE suggest that this soft-decision scheme is particularly suitable for the situations where the test utterances exhibit a wide range of SNR.	expectation–maximization algorithm;marginal model;signal-to-noise ratio;speaker recognition	Man-Wai Mak	2014			artificial intelligence;speech recognition;pattern recognition;computer science	NLP	-17.09425385790351	-91.5383814650347	53799
a45b284ab45042462d41395c6642e984b9096521	parameter estimation of a plucked string synthesis model using a genetic algorithm with perceptual fitness calculation	signal image and speech processing;physical modeling synthesis;sound synthesis;quantum information technology spintronics;genetic algorithm;parameter estimation;plucked string synthesis	We describe a technique for estimating control parameters for a plucked string synthesis model using a genetic algorithm. The model has been intensively used for sound synthesis of various string instruments but the fine tuning of the parameters has been carried out with a semiautomatic method that requires some hand adjustment with human listening. An automated method for extracting the parameters from recorded tones is described in this paper. The calculation of the fitness function utilizes knowledge of the properties of human hearing.	automatic control;automatic sounding;digital filter;discretization;estimation theory;experiment;filter design;fitness function;genetic algorithm;newton's method;sampling (signal processing);sensitivity and specificity;short-time fourier transform;signal processing;software release life cycle;synthetic data;synthetic intelligence;test case	Janne Riionheimo;Vesa Välimäki	2003	EURASIP J. Adv. Sig. Proc.	10.1155/S1110865703302100	speech recognition;genetic algorithm;computer science;estimation theory;statistics	Robotics	-8.803014837395306	-86.95631098029206	53835
1df74b328530ba675e9f2fc6bc9d57ce7f677e1b	a modeling of singing voice robust to accompaniment sounds and its application to singer identification and vocal-timbre-similarity-based music information retrieval	busqueda informacion;modelizacion;processus gauss;evaluation performance;predominant melody;gaussian mixture models singer identification vocal timbre similarity based music information retrieval polyphonic musical audio signals musical instruments accompaniment sound reduction frame selection reliability predominant melody;audio signal processing;instruments;performance evaluation;information retrieval system;gaussian processes;automatic system;signal audio;information retrieval;music information retrieval mir;probability density function;evaluacion prestacion;speech processing;audio signal;singing voice;tratamiento palabra;traitement parole;armonica;useful information;speech;informacion util;harmonic;vocal timbre similarity music information retrieval mir singer identification singing voice vocal;sistema de recuperacion de informacion;musical instruments;voz de canto;vocal;similitude;polyphonic musical audio signals;speaker recognition;musical sound;identificacion sistema;voix chantee;modelisation;feature vector;systeme de recherche d information;gaussian mixture model;automatic recognition;musical instrument;instrumento musical;sistema automatico;harmonique;system identification;recherche information;robustness music information retrieval instruments timbre multiple signal classification signal processing feature extraction;feature extraction;gaussian mixture models;periodic structures;singer identification;frame selection reliability;music information retrieval;similarity;instrument musique;reconnaissance locuteur;systeme automatique;signal acoustique;information retrieval audio signal processing feature extraction gaussian processes;accompaniment sound reduction;teoria mezcla;vocal timbre similarity;acoustic signal;gaussian process;similitud;son musical;proceso gauss;mixture theory;vocal timbre similarity based music information retrieval;modeling;theorie melange;music;identification systeme;senal acustica;information utile;senal audio;sonido musical	This paper describes a method of modeling the characteristics of a singing voice from polyphonic musical audio signals including sounds of various musical instruments. Because singing voices play an important role in musical pieces with vocals, such representation is useful for music information retrieval systems. The main problem in modeling the characteristics of a singing voice is the negative influences caused by accompaniment sounds. To solve this problem, we developed two methods, accompaniment sound reduction and reliable frame selection . The former makes it possible to calculate feature vectors that represent a spectral envelope of a singing voice after reducing accompaniment sounds. It first extracts the harmonic components of the predominant melody from sound mixtures and then resynthesizes the melody by using a sinusoidal model driven by these components. The latter method then estimates the reliability of frame of the obtained melody (i.e., the influence of accompaniment sound) by using two Gaussian mixture models (GMMs) for vocal and nonvocal frames to select the reliable vocal portions of musical pieces. Finally, each song is represented by its GMM consisting of the reliable frames. This new representation of the singing voice is demonstrated to improve the performance of an automatic singer identification system and to achieve an MIR system based on vocal timbre similarity.	experiment;feature vector;google map maker;information retrieval;mixture model;sinusoidal model	Hiromasa Fujihara;Masataka Goto;Tetsuro Kitahara;Hiroshi G. Okuno	2010	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2010.2041386	speaker recognition;melody;speech recognition;acoustics;computer science;machine learning;harmonic analysis;mixture model;gaussian process;speech processing;statistics	Web+IR	-9.356952564697236	-89.49035184021649	53974
1e71c210320bd9db42dec2255e7ec66862dd3608	development of a large spontaneous speech database of agglutinative hungarian language		In this paper, a large Hungarian spoken language database is introduced. This phonetically-based multi-purpose database contains various types of spontaneous and read speech from 333 monolingual speakers (about 50 minutes of speech sample per speaker). This study presents the background and motivation of the development of the BEA Hungarian database, describes its protocol and the transcription procedure, and also presents existing and proposed research using this database. Due to its recording protocol and the transcription it provides a challenging material for various comparisons of segmental structures of speech also across languages.	database;multi-purpose viewer;privacy;speaker recognition;speech technology;spontaneous order;text corpus;transcription (software)	Tilda Neuberger;Dorottya Gyarmathy;Tekla Etelka Gráczi;Viktória Horváth;Mária Gósy;András Beke	2014		10.1007/978-3-319-10816-2_51	natural language processing;linguistics	NLP	-22.90127558219058	-84.58755352593325	54137
d90774aebd2609100279e22ebd40feea3d0a8cfe	spoken emotion recognition using hierarchical classifiers	gaussian mixture;man machine interaction;acoustic analysis;modele markov cache;hidden markov model;emotion recognition;spectrum;multilayer perceptron;classification;recognition;gaussian mixture model;emotion;signal processing;prosodie;spectral information;traitement du signal;interaction homme machine;perceptron;prosody;reconnaissance;human machine interaction;analyse acoustique;neural network;hierarchical classifiers	The recognition of the emotional state of speakers is a multi-disciplinary research area that has received great interest over the last years. One of the most important goals is to improve the voice-based human–machine interactions. Several works on this domain use the prosodic features or the spectrum characteristics of speech signal, with neural networks, Gaussian mixtures and other standard classifiers. Usually, there is no acoustic interpretation of types of errors in the results. In this paper, the spectral characteristics of emotional signals are used in order to group emotions based on acoustic rather than psychological considerations. Standard classifiers based on Gaussian Mixture Models, Hidden Markov Models and Multilayer Perceptron are tested. These classifiers have been evaluated with different configurations and input features, in order to design a new hierarchical method for emotion classification. The proposed multiple feature hierarchical method for seven emotions, based on spectral and prosodic information, improves the performance over the standard classifiers and the fixed features. © 2010 Elsevier Ltd. All rights reserved.	acoustic cryptanalysis;artificial neural network;cluster analysis;computer-aided industrial design;emotion recognition;feature vector;hidden markov model;hierarchical classifier;interaction;markov chain;mixture model;multilayer perceptron;pict;pid;quad flat no-leads package;semantic prosody;spatial variability;spectral density estimation	Enrique Marcelo Albornoz;Diego H. Milone;Hugo Leonardo Rufiner	2011	Computer Speech & Language	10.1016/j.csl.2010.10.001	random subspace method;natural language processing;spectrum;speech recognition;emotion;computer science;perceptron;machine learning;signal processing;pattern recognition;prosody;multilayer perceptron;hidden markov model	AI	-14.049543351260638	-87.23449430792321	54138
6d535c216e75485eb2cba383d87a2320bfe11dd2	an improved uncertainty decoding scheme with weighted samples for multi-channel dnn-hmm hybrid systems	microphones;decoding;uncertainty;acoustics;distortion;hidden markov models;feature extraction	In this paper, we improve a recently-proposed uncertainty decoding scheme for DNN-HMM (deep neural network — hidden Markov model) hybrid systems, which models acoustic features as random variables. This uncertainty decoding strategy averages DNN outputs produced by a finite set of feature samples to approximate the posterior likelihoods of the context-dependent HMM states. As main innovation, we propose a weighted (instead of arithmetic) DNN-output averaging based on a minimum classification error criterion and apply it to a new probabilistic distortion model for multi-channel front-end signal enhancement schemes. The experimental evaluation is performed on the 8-channel REVERB Challenge task using a DNN-HMM hybrid system with spatial filtering of the microphone signals. It is shown that the recognition accuracy of the DNN-HMM hybrid system improves by incorporating uncertainty decoding and that the proposed weighted DNN-output averaging further reduces the word error rate scores.	acoustic cryptanalysis;approximation algorithm;artificial neural network;context-sensitive language;deep learning;distortion;hidden markov model;hybrid system;markov chain;microphone;word error rate	Christian Huemmer;Ramón Fernández Astudillo;Walter Kellermann	2017	2017 Hands-free Speech Communications and Microphone Arrays (HSCMA)	10.1109/HSCMA.2017.7895556	speech recognition;machine learning;pattern recognition;mathematics	AI	-14.995378641411035	-91.06352426883062	54168
da4114c204cd69f3b0c775eb22d6598f49981b47	multi-source far-distance microphone selection and combination for automatic transcription of lectures	automatic speech recognition;speech to text;word error rate;indexing terms;signal to noise ratio	In this work, we present our progress in multi-source far field automatic speech-to-text transcription for lecture speech. In particular, we show how the best of several far field channels can be selected based on a signal-to-noise ratio criterion, and how the signals from multiple channels can be combined at either the waveform level using blind channel combination or at the hypothesis level using confusion network techniques to improve the accuracy of a far field lecture transcription system. Using the techniques described here, we ran a series of experiments on the test set used by the US National Institute of Standards and Technologies for the RT-05S evaluation. For the multiple distant microphones (MDM) task of RT-05S, our system achieved a word error rate of 38.5% which represents an improvement of over 13% absolute compared to the best reported results in the RT-05S evaluation.	experiment;master data management;microphone;multi-source;signal-to-noise ratio;speech recognition;test set;transcription (software);waveform;word error rate	Matthias Wölfel;Christian Fügen;Shajith Ikbal;John W. McDonough	2006			speech recognition;pattern recognition;microphone;artificial intelligence;word error rate;test set;near and far field;waveform;communication channel;signal-to-noise ratio;multi-source;computer science	NLP	-13.911797988400908	-90.02309379278623	54197
395a02c67a349438e55e6340e2f1a71598576e83	language- and talker-dependent variation in global features of native and non-native speech	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;uk phd theses thesis;life sciences;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	We motivate and present a corpus of scripted and spontaneous speech in both the native and the non-native language of talkers from various language backgrounds. Using corpus recordings from 11 native English and 11 late Mandarin-English bilinguals we compared speech timing across native English, native Mandarin, and Mandarin-accented English. Findings showed similarities across native Mandarin and native English in speaking rate and in reduction of the number of acoustic relative to orthographic syllables. The two languages differed in silence-to-speech ratio and in the number of words between pauses, possibly reflecting phrase-level structural differences between English and Mandarin. Non-native English had a significantly slower speaking rate and lower rate of syllable reduction than both native English and native Mandarin. But, non-native English was similar to native English in terms of silence-to-speech ratio and was similar to native Mandarin in terms of words per pause. Finally, some talker-specificity in terms of (non)optimal speech timing appeared to transfer from native to non-native speech within the Mandarin-English bilinguals. These findings provide an empirical base for testing how language-dependent, structural features combine with general features of non-native speech production and with talker-dependent features in determining foreign-language speech production.	acoustic cryptanalysis;body of uterus;languages;orthographic projection;sensitivity and specificity;speaking (activity);spontaneous order;super robot monkey team hyperforce go!;syllable;text corpus	Ann R. Bradlow;Lauren Ackerman;L. Ann Burchfield;Lisa Hesterberg;Jenna Luque;Kelsey Mok	2011	Proceedings of the ... International Congress of Phonetic Sciences. International Congress of Phonetic Sciences		natural language processing;speech recognition;computer science;linguistics	NLP	-11.448704972487871	-81.64871088375268	54271
558cc62c7dddb872df560adb1b492554896a6b57	vector-field-smoothed bayesian learning for fast and incremental speaker/telephone-channel adaptation	error reduction;learning;telephone;prior information;linguistique appliquee;apprentissage;automatic recognition;bayesian learning;signal processing;reconnaissance de la parole;adaptation;adaptive method;traitement du signal;word recognition;map estimation;computational linguistics;speaker;vector field;speaker adaptation;linguistique informatique;locuteur;reconnaissance automatique;applied linguistics	Abstract   This paper describes an on-line adaptation method that combines maximum a posteriori (MAP) estimation for intra-class training (the training scheme incorporates new training samples with prior information) with vector field smoothing (VFS) for inter-class smoothing. Results of experiments comparing recognition performance of MAP/VFS with MAP adaptation for speaker adaptation and simultaneous adaptation of speaker and telephone channel show that fast and incremental adaptation can be achieved even with a relatively small number of training samples (under 10 words) due to VFS's ability to consistently enhance MAP adaptation. High word error reduction rates, which in the experiments were 22% for speaker adaptation in a large-vocabulary isolated-word recognition task (vocabulary size=2575) and 48% for simultaneous adaptation of speaker and telephone channel in a 100-isolated-word recognition task, can be achieved through word-by-word incremental adaptation using 10-word data.	smoothing	Jun-ichi Takahashi;Shigeki Sagayama	1997	Computer Speech & Language	10.1006/csla.1996.0025	loudspeaker;natural language processing;vector field;speech recognition;word recognition;computer science;computational linguistics;applied linguistics;signal processing;pattern recognition;linguistics;bayesian inference;adaptation	ML	-19.885499318878622	-91.70687401529837	54333
f1c0e20789552cd69761181c58293d7da1ab5e05	two is better than one: improving multilingual collaboration by giving two machine translation outputs	collaboration;multilingual communication;machine translation	Machine translation (MT) creates both opportunities and challenges for multilingual collaboration: While MT enables collaborators to communicate via their native languages, it can introduce errors that make communication difficult. In the current paper, we examine whether displaying two alternative translations for each message will improve conversational grounding and task performance. We conducted a laboratory experiment in which monolingual native English speakers collaborated with bilingual native Mandarin speakers on a map navigation task. Each dyad performed the task in one of three communication conditions: MT with single output, MT with two outputs, and English as a common language. Dyads given two translations for each message communicated more efficiently, and performed better on the task, than dyads given one translation. Our findings show the value of providing multiple translations in multilingual collaboration, and suggest design features of future MT-based collaboration tools.	machine translation;super robot monkey team hyperforce go!	Ge Gao;Bin Xu;David C. Hau;Zheng Yao;Dan Cosley;Susan R. Fussell	2015		10.1145/2675133.2675197	natural language processing;speech recognition;computer science;machine translation;communication;management;collaboration	NLP	-27.860742585687557	-86.48587860129976	54362
bf752bdd92c226eceaffed27b9099c4c2eba103a	environmental adaptation based on first order approximation	additive noise;low pass filter;low pass filters speech recognition acoustic noise approximation theory noise pollution;approximation theory;adaptive algorithm;first order;acoustic noise;robustesse;reconnaissance de la parole;adaptation;noise pollution;additive noise working environment noise jacobian matrices testing speech enhancement databases convolution low pass filters equations cepstral analysis;error rate;speech recognition;robustness;low pass filters;tidigits corpus environmental adaptation first order approximation automatic speech recognition additive noise convolutional noise additive adaptation algorithm low pass filter channel noise error rate reduction	In this paper, we propose an algorithm that compensates for both additive and convolutional noise. The goal of this method is to achieve an efficient environmental adaptation to realistic environments both in terms of computation time and memory. The algorithm described in this paper is an extension of an additive noise adaptation algorithm presented in [1]. Experimental results are given on a realistic database recorded in a car. This database is further filtered by a low pass filter to combine additive and channel noise. The proposed adaptation algorithm reduces the error rate by 75 % on this database, when compared to our baseline system without environmental adaptation.	additive white gaussian noise;algorithm;baseline (configuration management);computation;database;low-pass filter;noise (electronics);order of approximation;time complexity;utility functions on indivisible goods	Christophe Cerisara;Luca Rigazio;Robert Boman;Jean-Claude Junqua	2001		10.1109/ICASSP.2001.940805	speech recognition;low-pass filter;computer science;machine learning	ML	-14.804389889527426	-91.93654030721638	54387
811954510b02ccc269d09083dde42c909061ea3e	speaker information enhancement	speaker information enhancement	This study consists of two experiments*. Experiment A investigates speaker-information distribution in the parametric domain. Experiment B compares different weighting strategies for speaker-information enhancement. The results indicate that weightings based on speaker-information distribution in the parametric domain yield better speaker recognition performance.	speaker recognition	Fangxin Chen	2000			artificial intelligence;speech recognition;pattern recognition;speaker diarisation;speaker recognition;computer science	Robotics	-13.551112169239078	-90.2745444901148	54430
d73ca7d9e9b9e70dcfc4ec2636e1b8490ea9b888	the siwis database: a multilingual speech database with acted emphasis	speech to speech translation;speech corpus;emphasis;bilingual speakers	We describe here a collection of speech data of bilingual and trilingual speakers of English, French, German and Italian. In the context of speech to speech translation (S2ST), this database is designed for several purposes and studies: training CLSA systems (cross-language speaker adaptation), conveying emphasis through S2ST systems, and evaluating TTS systems. More precisely, 36 speakers judged as accentless (22 bilingual and 14 trilingual speakers) were recorded for a set of 171 prompts in two or three languages, amounting to a total of 24 hours of speech. These sets of prompts include 100 sentences from news, 25 sentences from Europarl, the same 25 sentences with one acted emphasised word, 20 semantically unpredictable sentences, and finally a 240-word long text. All in all, it yielded 64 bilingual session pairs of the six possible combinations of the four languages. The database is freely available for non-commercial use and scientific research purposes.	netware file system;session id	Jean-Philippe Goldman;Pierre-Edouard Honnet;Robert A. J. Clark;Philip N. Garner;Maria Ivanova;Alexandros Lazaridis;Hui Liang;Tiago Macedo;Beat Pfister;Manuel Sam Ribeiro;Eric Wehrli;Junichi Yamagishi	2016		10.21437/Interspeech.2016-1003	natural language processing;emphasis;speech recognition;speech corpus;computer science;speech;linguistics	NLP	-23.191231570282152	-84.11986199173063	54432
00c7b37ad903430676bd6afb825f7ce1a8d4cecf	dynamically combining syntax and semantics in natural language processing	natural language understanding;natural language processing	A controversy has existed over the interaction of syntax and semantics in natural language understanding systems. According to theories of integrated parsing, syntactic and semantic processing should take place simultaneously, with the parsing process driven by a single rule base which contains both syntactic and semantic knowledge. This is in sharp contrast to traditional linguistic approaches to language analysis, in which syntact,ic and semantic processing are performed separately from one another, driven by completely separate sets of syntactic and semantic rules. This paper presents an approach to natural language understanding which is a compromise between these two views. It is an integrated approach, in the sense that syntactic and semantic processing take place at the same time. However, unlike previous integrated systems, the approach described here uses largely separate bodies of syntactic and semantic knowledge, which are combined only at the time of processing.	natural language processing;natural language understanding;parsing;rule-based system	Steven L. Lytinen	1986			natural language processing;language identification;semantic role labeling;semantic interpretation;semantic computing;semantic web rule language;universal networking language;deep linguistic processing;computer science;syntactic predicate;semantic compression;programming language	AI	-31.57470592902253	-80.97852631896227	54473
1f2b3725807ce939dc4cfd45694dfea1cf2ea827	discriminative acoustic model for improving mispronunciation detection and diagnosis in computer-aided pronunciation training (capt)	capt;data-driven phonological rule extraction;discriminative training;mispronunciation detection and diagnosis	In this study, we propose a discriminative training algorithm to jointly minimize mispronunciation detection errors (i.e., false rejections and false acceptances) and diagnosis errors (i.e., correctly pinpointing mispronunciations but incorrectly stating how they are wrong). An optimization procedure, similar to Minimum Word Error (MWE) discriminative training, is developed to refine the ML-trained HMMs. The errors to be minimized are obtained by comparing transcribed training utterances (including mispronunciations) with Extended Recognition Networks [3] which contain both canonical pronunciations and explicitly modeled mispronunciations. The ERN is compiled by handcrafted rules, or data-driven rules. Several conclusions can be drawn from the experiments: (1) data-driven rules are more effective than hand-crafted ones in capturing mispronunciations; (2) compared with the ML training baseline, discriminative training can reduce false rejections and diagnostic errors, though false acceptances increase slightly due to a small number of false-acceptance samples in the training set.	acoustic cryptanalysis;acoustic model;algorithm;baseline (configuration management);compiler;discriminative model;experiment;mathematical optimization;minimal working example;test set	Xiaojun Qian;Frank K. Soong;Helen M. Meng	2010			artificial intelligence;pattern recognition;computer-aided;speech recognition;discriminative model;computer science;acoustic model;small number;training set;pronunciation	Vision	-19.026936695858225	-85.59568911477531	54513
f79ad1cda48fa05f5f2e3521d434d34543b4ef22	use of articulatory synthesis for analysis of voice disorders			articulatory synthesis	Peter Howell;Mark Williams	1990			articulatory synthesis;speech recognition;computer science	Logic	-13.227618909032518	-85.75467784753869	54715
f4e7777cb89888254082aad212039facf70a55ea	a study of style-induced vowel variability: laboratory versus spontaneous speech in spanish	analyse parole;voz hablada;voix parlee;analisis palabra;speech processing;speech analysis;tratamiento palabra;traitement parole;vocal;spanish;voyelle;spontaneous speech;espagnol;vowel;speaking voice;espanol	Abstract   The paper is focussed on the vocalic differences between spontaneous and laboratory speech in Spanish. The first and second formants of 954 vowel utterances (477 in laboratory and 477 in spontaneous speech) have been measured. They constitute clusters in the  F  1 / F  2  space. The paper describes inter- and intra-cluster variabilities caused by communication situations changes. In spontaneous speech, the formants values show (1) a marked schwa-tendency; (2) increasing intra-cluster variability. Both phenomena result in lowered differenciation of the sounds in spontaneous speech.	heart rate variability;spontaneous order	Bernard Harmegnies;Dolors Poch-Olivé	1992	Speech Communication	10.1016/0167-6393(92)90048-C	speech recognition;computer science;speech processing;linguistics;spanish	NLP	-10.550852636103736	-82.51907899152255	54746
72480e08d220a68a4d9667a2f6ed254eba08c916	phone segmentation tool with integrated pronunciation lexicon and czech phonetically labelled reference database		Phonetic segmentation is the procedure which is used in many applications of speech processing, both as a subpart of automated systems or as the tool for an interactive work. In this paper we are presenting the latest development in our tool of automated phonetic segmentation. The tool is based on HMM forced alignment realized by publicly available HTK toolkit. It is implemented into the environment of Praat application and it can be used with several optional settings. The tool is designed for segmentation of the utterances with known orthographic records while phonetic contents are obtained from the pronunciation lexicon or from orthoepic record generated by rules for new unknown words. Second part of this paper describes small Czech reference database precisely labelled on phonetic level which is supposed to be used for the analysis of the accuracy of automatic phonetic segmentation.	bibliographic database;htk (software);hidden markov model;lexicon;orthographic projection;praat;speech processing	Petr Pollák;Jan Volín;Radek Skarnitzl	2008			natural language processing;speech recognition;phone;artificial intelligence;hidden markov model;database;speech processing;computer science;czech;lexicon;segmentation;pronunciation	NLP	-22.554758267373582	-84.15252701431147	54749
7720aedfff3d4e5e25edbc495ad84fa23bb4b888	on the applicability of a user satisfaction-based reward for dialogue policy learning		Finding a good dialogue policy using reinforcement learning usually relies on objective criteria for modelling the reward signal, e.g., task success. In this contribution, we propose to use user satisfaction instead represented by the metric Interaction Quality (IQ). Comparing the user satisfaction-based reward to the baseline of task success, we show that IQ is a real alternative for reward modelling: designing a reward function using IQ may result in a similar or even better performance than using task success. This is demonstrated in a user simulator evaluation using a live IQ estimation module.		Stefan Ultes;Juliana Miehle;Wolfgang Minker	2017		10.1007/978-3-319-92108-2_22	machine learning;reinforcement learning;policy learning;computer science;artificial intelligence	NLP	-27.604052027161778	-87.11322793177064	54818
4e4ef3270abf62570db73633de0600f540229b5a	detection of agreement and disagreement in broadcast conversations	prosodic feature;training data;durational feature;english broadcast conversation shows;agreement detection;various lexical;random downsampling;disagreement detection;ensemble downsampling;english broadcast conversation data	We present Conditional Random Fields based approaches for detecting agreement/disagreement between speakers in English broadcast conversation shows. We develop annotation approaches for a variety of linguistic phenomena. Various lexical, structural, durational, and prosodic features are explored. We compare the performance when using features extracted from automatically generated annotations against that when using human annotations. We investigate the efficacy of adding prosodic features on top of lexical, structural, and durational features. Since the training data is highly imbalanced, we explore two sampling approaches, random downsampling and ensemble downsampling. Overall, our approach achieves 79.2% (precision), 50.5% (recall), 61.7% (F1) for agreement detection and 69.2% (precision), 46.9% (recall), and 55.9% (F1) for disagreement detection, on the English broadcast conversation data.	conditional random field;decimation (signal processing);ensemble kalman filter;sampling (signal processing);sensor	Wen Wang;Sibel Yaman;Kristin Precoda;Colleen Richey;Geoffrey Raymond	2011			natural language processing;speech recognition;computer science	NLP	-19.609753575538242	-80.78261491083869	54821
bf42368990ebd108e254b35f97bd9f22854909c7	multi-resolution spectral entropy feature for robust asr	wideband;acoustic modelling;spectral entropy feature extraction;multiresolution spectral entropy feature;hidden markov model;additive noise;snr;speech;plp features acoustic modelling perceptual linear prediction multiresolution spectral entropy feature robust asr automatic speech recognition spectrum formant positions spectral entropy feature extraction normalized spectrum total entropy overlapping sub bands entropy feature time derivatives additive wide band noise snr;robust asr;spectrum;sivadas;entropy feature time derivatives;noise robustness;perceptual linear prediction;mel frequency cepstral coefficient;automatic speech recognition;misra;cepstral analysis;hidden markov models;ikbal;normalized spectrum total entropy;feature extraction;acoustic noise;spectrum formant positions;bourlard;feature extraction speech recognition spectral analysis entropy;hybrid system;speech recognition;context dependent;entropy automatic speech recognition cepstral analysis mel frequency cepstral coefficient multiresolution analysis noise robustness additive noise hidden markov models wideband acoustic noise;entropy;spectral analysis;multi resolution;additive wide band noise;multiresolution analysis;plp features;overlapping sub bands;artificial neural network	Recently, entropy measures at different stages of recognition have been used in automatic speech recognition (ASR) tasks. In a recent paper, we proposed that formant positions of a spectrum can be captured by a multi-resolution spectral entropy feature. In this paper, we suggest modifications to the spectral entropy feature extraction approach and compute the entropy contribution from each sub-band to the total entropy of the normalized spectrum. Further, we explore the ideas of overlapping sub-bands and the time derivatives of the spectral entropy feature. The modified feature is robust to additive wide-band noise and performs well at low SNRs. Finally, in the TANDEM framework, we show that the system using combined entropy and PLP (perceptual linear prediction) features works better than the baseline PLP feature for additive wide-band noise at different SNRs.	automated system recovery;baseline (configuration management);entropy (information theory);feature extraction;pl/p;speech recognition;tandem computers;utility functions on indivisible goods	Hemant Misra;Shajith Ikbal;Sunil Sivadas;Hervé Bourlard	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1415098	multiresolution analysis;spectrum;entropy;speech recognition;feature extraction;computer science;speech;machine learning;context-dependent memory;noise;pattern recognition;mathematics;recurrence period density entropy;signal-to-noise ratio;maximum entropy spectral estimation;hidden markov model;hybrid system	Robotics	-12.395581164037573	-91.07810555298232	54911
cc8e0244f489888b4485f2876928eee44888001f	an improved preprocessor for the automatic transcription of broadcast news audio stream.	broadcast news;automatic segmentation;speech recognition;near real time	This paper deals with the preprocessing of the broadcast news (BN) audio stream for the automatic transcription purposes. The preprocessing consists of the automatic segmentation followed by the broad-class segment identification. The former is capable of detecting speaker and/or acoustic changes in the BN audio stream with the precision being 82.75%. The latter acts as a filter that removes nonspeech parts. The performance of the proposed system was evaluated on the multi-lingual pan-European COST278 BN database containing data in 6 languages. The preprocessing and segmentation module operates in a near-real-time way, with the total delay of 12 seconds. Its practical functionality was evaluated on the Czech part of the BN database. The automatically segmented signal was directly sent to the large vocabulary speech recognition system operating with a 200K-word Czech lexicon. The difference in performance between automatically and manually segmented BN streams was only minimal 1.12%.	acoustic fingerprint;database;lexicon;preprocessor;real-time clock;real-time computing;sensor;speech recognition;streaming media;transcription (software);vocabulary	Jindrich Zdánský;Petr David;Jan Nouza	2004			speech recognition;computer science;multimedia;world wide web	Web+IR	-22.404245860017753	-84.0190023130354	54916
2ff4a63c5f8498e44418197113b4ab715979edcf	harmonic-percussive source separation with deep neural networks and phase recovery		Harmonic/percussive source separation (HPSS) consists in separating the pitched instruments from the percussive parts in a music mixture. In this paper, we propose to apply the recently introduced Masker-Denoiser with twin networks (MaD TwinNet) system to this task. MaD TwinNet is a deep learning architecture that has reached state-of-the-art results in monaural singing voice separation. Herein, we propose to apply it to HPSS by using it to estimate the magnitude spectrogram of the percussive source. Then, we retrieve the complex-valued short-time Fourier transform of the sources by means of a phase recovery algorithm, which minimizes the reconstruction error and enforces the phase of the harmonic part to follow a sinusoidal phase model. Experiments conducted on realistic music mixtures show that this novel separation system outperforms the previous state-of-the art kernel additive model approach.	additive model;algorithm;deep learning;experiment;high performance storage system;neural networks;short-time fourier transform;source separation;spectrogram;wizardry: proving grounds of the mad overlord	Konstantinos Drossos;Paul Magron;Stylianos Ioannis Mimilakis;Tuomas Virtanen	2018	2018 16th International Workshop on Acoustic Signal Enhancement (IWAENC)	10.1109/IWAENC.2018.8521371	machine learning;artificial intelligence;architecture;computer science;fourier transform;time–frequency analysis;deep learning;harmonic analysis;source separation;spectrogram;harmonic;pattern recognition	ML	-14.936932815595764	-91.33185522304024	54931
2a6a8645294f2f9ddb6f9e6ec26b0d2d72569481	estimation of speaking style in speech corpora focusing on speech transcriptions		Recent developments in computer technology have allowed the construction and widespread application of large-scale speech corpora. To foster ease of data retrieval for people interested in utilising these speech corpora, we attempt to characterise speaking style across some of them. In this paper, we first introduce the 3 scales of speaking style proposed by Eskenazi in 1993. We then use morphological features extracted from speech transcriptions that have proven effective in style discrimination and author identification in the field of natural language processing to construct an estimation model of speaking style. More specifically, we randomly choose transcriptions from various speech corpora as text stimuli with which to conduct a rating experiment on speaking style perception; then, using the features extracted from those stimuli and the rating results, we construct an estimation model of speaking style by a multi-regression analysis. After the cross validation (leave-1-out), the results show that among the 3 scales of speaking style, the ratings of 2 scales can be estimated with high accuracies, which prove the effectiveness of our method in the estimation of speaking style.	computer;data retrieval;natural language processing;randomness;speech synthesis;text corpus	Raymond Shen;Hideaki Kikuchi	2014			natural language processing;speech recognition;speech corpus;speech;linguistics	AI	-19.089884885738215	-82.56371595391394	54953
03a74dfd03139978f57bb0b18cb641007f44cfc1	rhythm-flexible voice conversion without parallel data using cycle-gan over phoneme posteriorgram sequences		Speaking rate refers to the average number of phonemes within some unit time, while the rhythmic patterns refer to duration distributions for realizations of different phonemes within different phonetic structures. Both are key components of prosody in speech, which is different for different speakers. Models like cycle-consistent adversarial network (Cycle-GAN) and variational auto-encoder (VAE) have been successfully applied to voice conversion tasks without parallel data. However, due to the neural network architectures and feature vectors chosen for these approaches, the length of the predicted utterance has to be fixed to that of the input utterance, which limits the flexibility in mimicking the speaking rates and rhythmic patterns for the target speaker. On the other hand, sequence-to-sequence learning model was used to remove the above length constraint, but parallel training data are needed. In this paper, we propose an approach utilizing sequence-to-sequence model trained with unsupervised Cycle-GAN to perform the transformation between the phoneme posteriorgram sequences for different speakers. In this way, the length constraint mentioned above is removed to offer rhythm-flexible voice conversion without requiring parallel data. Preliminary evaluation on two datasets showed very encouraging results.	artificial neural network;autoencoder;encoder;feature vector;semantic prosody;variational principle	Cheng-chieh Yeh;Po-chun Hsu;Ju-Chieh Chou;Hung-yi Lee;Lin-Shan Lee	2018	CoRR		speech recognition;computer science;feature vector;decoding methods;artificial neural network;training set;prosody;data modeling;rhythm;utterance	NLP	-17.719804261838682	-87.78309596760421	55015
89ad16acfa3ac61fe96639ffe19d5849cec78d8b	unintuitive phonetic behavior in tswana post-nasal stops		This article describes the phonetic process of post-nasal devoicing in Tswana. We propose a multi-agent exemplar model with various interaction schemes which include factors like functional and social biases in order to account for this counter-intuitive phenomenon. Our novel hybrid multi-agent modeling framework facilitates investigation of sound change by combining the sociophonetic model of Nettle [22] and the exemplar-based model of Wedel [26] into a single unified model.	multi-agent system;nettle;unified model	Jagoda Bruni;Daniel Duran;Grzegorz Dogil	2015			speech recognition;tswana;computer science	AI	-14.570865171614773	-80.9508529945064	55043
8e3041254a138a79bed7b6e11b6eaaea936af68a	applying tree kernel to chinese syntactic parsing reranking	tree kernel;syntactic parsing;kernel;pediatrics;reranking;probability density function;tree data structures;chinese syntactic parsing;data mining;tree kernel tracking chinese syntactic parsing reranking;learning systems;grammars;candidate parsing tree chinese syntactic parsing reranking tree kernel;chinese syntactic parsing reranking;production;kernel method;computational linguistics;candidate parsing tree;tracking;kernel computational linguistics learning systems training data machine learning algorithms partial transmit sequences;tree data structures grammars	Chinese Syntactic Parsing is often divided into two stages, basic model to generate k-best candidate parsing trees and reranking stage to find the best one in these trees, and we focus on the second stage-reranking. Tree kernel has been used in Syntactic parsing in English(Collins 2002) which makes a good improvement in performance. This paper makes tree kernel method practical for Chinese Syntactic Parsing, and improves tree kernel from three points: the first is introducing rules weights rw to distinct different rules, the second is introducing similar rules converging, and the third is introducing a parameter λ to scale the relative importance of tree fragments with their size and solve the peak problem which is a common kernel. Experiments show that these three adjustments to tree kernel increase reranking performance a lot.	kernel (operating system);kernel method;machine learning;parsing expression grammar;stochastic context-free grammar	Xiaodong Zheng;Liang Chen;Baobao Chang	2009	2009 International Conference on Natural Language Processing and Knowledge Engineering	10.1109/NLPKE.2009.5313750	natural language processing;speech recognition;computer science;bottom-up parsing;s-attributed grammar;pattern recognition;tree kernel;top-down parsing	NLP	-22.1256078294199	-89.76898711713743	55081
ec35ee1a1e63bd46b0e415d9a29aa2e424f2a8c8	planning for problem formulation in advice-giving dialogue	natural language front-end;specific aspect;dialogue module;overlapping activity;problem formulation;financial investing;problem formulation activity;expert problem solver;person-machine advice-giving dialogue;advice-giving dialogue;natural language;front end	We d i s t ingu i sh th ree main , over lapping act ivi t ies in an advice-giv ing dialogue: p r o b l e m fo rmula t ion , resolut ion, and exp lana t ion . T h i s pape r focuses on a p rob lem formu la t ion ac t iv i ty in a d ia logue m o d u l e which in te rac t s on one side wi th an exper t p rob lem solver for financial inves t ing and on the o the r side wi th a na t u r a l l anguage front-end. Several strategies which reflect specific aspects of person-machine advice-giving dialogues are realized by incorporating planning at a high-level of dialogue.	exptime;high- and low-level;loic;linear algebra;numerical aperture;oracle rac;re-order buffer;solver;wilhelm pape	Paul Decitre;Thomas Grossi;Cléo Jullien;Jean-Philippe Solvay	1987			simulation;computer science;artificial intelligence;front and back ends;linguistics;natural language	AI	-29.742549502412924	-81.53965171756467	55215
99b479b5dced570f138acd5c8f1a294e7c47ff5e	end-to-end training approaches for discriminative segmental models	neural networks;training;mel frequency cepstral coefficient;computational modeling;hidden markov models;predictive models;fasteners	Recent work on discriminative segmental models has shown that they can achieve competitive speech recognition performance, using features based on deep neural frame classifiers. However, segmental models can be more challenging to train than standard frame-based approaches. While some segmental models have been successfully trained end to end, there is a lack of understanding of their training under different settings and with different losses.	speech recognition	Hao Tang;Weiran Wang;Kevin Gimpel;Karen Livescu	2016	2016 IEEE Spoken Language Technology Workshop (SLT)	10.1109/SLT.2016.7846309	speech recognition;computer science;machine learning;pattern recognition;predictive modelling;mel-frequency cepstrum;computational model;artificial neural network;hidden markov model	Vision	-17.48678103757069	-88.64440342132016	55231
fed0130009125962f1b0b654cd3b5765b2eece4b	a fast speaker adaptation method using aspect model.		We propose a fast speaker adaptation method using an aspect model. The performance of speaker independent (SI) model is very sensitive to environments such as microphones, speakers, and noises. Speaker adaptation techniques try to obtain near speaker dependent (SD) performance with only small amounts of specific data and are often based on initial SI model. One of the most important purposes for adaptation algorithms is to modify a large number of parameters with only a small amount of adaptation data. The number of free parameters to be estimated from adaptation data can be reduced by using aspect model. In this paper, we introduce an aspect model into an acoustic model for rapid speaker adaptation. A formulation of probabilistic latent semantic analysis (PLSA) is extended to continuous density HMM. We carried out an isolated word recognition experiment on Korean database, and the results are compared to those of conventional expectation maximization (EM) algorithm, maximum a posteriori (MAP) and maximum likelihood linear regression (MLLR).	acoustic cryptanalysis;acoustic model;expectation–maximization algorithm;hidden markov model;microphone;probabilistic latent semantic analysis	Seongjun Hahm;Akinori Ito;Shozo Makino;Motoyuki Suzuki	2008			speaker recognition;speaker diarisation;speech recognition;computer science	ML	-18.01542741988735	-91.77138471869713	55424
444e297ea9ce07ccaac64446ba5d0b7d1a0319b0	the phonological syllable plays a role in lexical access in korean visual word recognition	korean visual word recognition;lexical decision task;lexical access;syllable based sublexical unit;visual word recognition;inhibitory effect;inhibitory effect phonological syllable lexical access korean visual word recognition visual word recognition syllable based sublexical unit;lexical processing;speech recognition;phonological syllable;frequency natural languages delay error analysis psychology computer science education software computational modeling pattern recognition;computer simulation	Recently, the influence of phonological information in visual word recognition has attracted a great amount of studies. Especially, languages with a clear syllable boundary, such as Spanish and French, have reported the syllable-based sub-lexical unit in visual word recognition. The present study investigated whether sub-lexical unit in Korean visual word recognition is phonological syllable or orthographic syllable. In Experiment 1, we orthogonally manipulated the number of phonological syllable neighbors and the number of orthographic syllable neighbors in Korean disyllable words and conducted a lexical decision task In Experiment 2, new words were used in a computational simulation. The results of two experiments showed the inhibitory effect as increasing the number of phonological syllable neighbor. We can account that the inhibitory effect is due to the lexical competition between phonological syllable-based candidates in the lexical level. The present study expands the notion that the syllable is an important sub-lexical unit in visual word recognition and also phonological information is a main function of lexical process.	lexicon;syllable;visual word	YouAn Kwon;Kinam Park;Heuiseok Lim;Kichun Nam	2007		10.1109/ICNC.2007.731	natural language processing;speech recognition;linguistics	Vision	-11.912431763947028	-81.10682488185661	55517
0851827201c75aba1369060b29f0b76b8cfc4b54	lecture subtopic retrieval by retrieval keyword expansion using subordinate concept		We developed a supporting system for creation of educational video contents. The system automatically segments a lecture video material into subtopics based on speech signals by a statistical model for text segmentation. In this paper, we reports on the result of retrieving the lecture subtopics by keyword expansion using the knowledge of the dictionary, and so on. The keyword expansion using the subordinate concept improved the average reciprocal order (MRR:Mean Reciprocal Rank) from 0.51 to 0.55 when subtopics are retrieved by a set of three search keywords for the lecture voice text recognized by automatic speech recognition.	dictionary;speech recognition;statistical model;text segmentation	Noboru Kanedera;Tetsuo Funada;Seiichi Nakagawa	2010			natural language processing;speech recognition;computer science;information retrieval	NLP	-22.88019469469479	-82.734168455085	55599
8ec2118d02c699f1cad8511265b65489f90c6bb4	dominant audio descriptors for audio classification and retrieval	k nearest neighbors;signal classification audio signal processing;audio signal processing;mel frequency cepstral coefficient mpeg 7 standard music information retrieval hidden markov models humans filters feature extraction time frequency analysis machine learning support vector machines;support vector machines;audio retrieval;hidden markov model;audio classification;low level feature representation;transform coding;data mining;mel frequency cepstral coefficient;training data;dominant audio descriptors;hidden markov models;local features;feature extraction;signal classification;k nearest neighbors dominant audio descriptors audio classification audio retrieval low level feature representation audio signals mpeg 7 dominant color descriptor mel frequency cepstral coefficients hidden markov models support vector machines;mel frequency cepstral coefficients;audio signals;k nearest neighbor;support vector machine;dominant color descriptor;mpeg 7 dominant color descriptor	In this paper, we propose a new general low-level feature representation for audio signals. Our approach, called Dominant Audio Descriptor is inspired by the MPEG-7 Dominant Color Descriptor. It is based on clustering timelocal features and identifying dominant components. The features used to illustrate this approach are the well-known Mel Frequency Cepstral Coefficients. The performance of the proposed framework is evaluated on audio classification and retrieval tasks. In particular, the experiments are performed on a benchmark music data set. The results are compared to those previously obtained on the same data base. We show that our approach improved classification and retrieval results by more then 3%, and for the case of retrieval reached almost perfect retrieval rate of 99:36%. In addition, the paper presents comparative results against several state of the art classifiers, such as Hidden Markov Models, Support Vector Machines and k-Nearest Neighbors.	audio signal processing;benchmark (computing);cluster analysis;coefficient;database;experiment;feature vector;hidden markov model;high- and low-level;mpeg-7;markov chain;mel-frequency cepstrum;statistical classification;support vector machine;whole earth 'lectronic link	Aleksey Fadeev;Oualid Missaoui;Hichem Frigui	2009	2009 International Conference on Machine Learning and Applications	10.1109/ICMLA.2009.120	support vector machine;speech recognition;computer science;machine learning;pattern recognition;mel-frequency cepstrum;k-nearest neighbors algorithm;hidden markov model	Vision	-9.35575919546459	-92.32402228823112	55762
0c2ace3f30946fab9e8065f66935031475adb1c3	unsupervised speaker change detection for mobile device recorded speech	audio segmentation;speech processing bayes methods decision making mobile radio speaker recognition;change detection;mobile device;metadata;bayes methods;speech processing;metadata speaker segmentation speaker change detection mobile audio segmentation multimedia database;mobile phone;speaker recognition;speaker change detection;mobile radio;speaker segmentation;system development;mobile phone mobile device speech recording unsupervised speaker change detection bayesian information criterion decision making false alarm compensation;mobile audio segmentation;multimedia database;bayesian information criterion;audio recording change detection algorithms detectors image segmentation noise robustness multimedia databases video recording indexing speech analysis bayesian methods	In this paper we propose an unsupervised speaker change detection (SCD) system developed for mobile device applications. We use Bayesian information criterion (BIC) to find initial speaker changes, which are then verified or discarded in the second phase by utilizing modified BIC and silence detector information. Silence information usage after initial BIC in decision making is useful to separate real changes from noise peaks. Enhanced peak detector adjusts BIC penalty parameter automatically, which improve the robustness and feasibility. Improved BIC based false alarm compensation (FAC) merges effectively consecutive segments belonging to same speaker. Our experiments have shown the robustness of the algorithm and it produces very satisfactory results for difficult mobile phone recorded speech data.	algorithm;bayesian information criterion;experiment;fly-by-wire;mobile device;mobile phone;pc speaker;penalty method;precision rectifier	Olli Vuorinen;Johannes Peltola;Satu-Marja Mäkelä	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.366346	speaker recognition;computer vision;speech recognition;computer science;pattern recognition;mobile device;speech processing;metadata;bayesian information criterion;change detection;statistics	Robotics	-14.997483894034684	-94.40762012061647	55775
1663bdd6193a203afb034fa9083c57ec38c4174e	utd-crss submission for mgb-3 arabic dialect identification: front-end and back-end advancements on broadcast speech		This study presents systems submitted by the University of Texas at Dallas, Center for Robust Speech Systems (UTD-CRSS) to the MGB-3 Arabic Dialect Identification (ADI) subtask. This task is defined to discriminate between five dialects of Arabic, including Egyptian, Gulf, Levantine, North African, and Modern Standard Arabic. We develop multiple single systems with different front-end representations and back-end classifiers. At the front-end level, feature extraction methods such as Mel-frequency cepstral coefficients (MFCCs) and two types of bottleneck features (BNF) are studied for an i-Vector framework. As for the back-end level, Gaussian back-end (GB), and Generative Adversarial Networks (GANs) classifiers are applied alternately. The best submission (contrastive) is achieved for the ADI subtask with an accuracy of 76.94% by augmenting the randomly chosen part of the development dataset. Further, with a post evaluation correction in the submitted system, final accuracy is increased to 79.76%, which represents the best performance achieved so far for the challenge on the test dataset.	beta normal form;coefficient;feature extraction;generative adversarial networks;gulf of execution;mel-frequency cepstrum;randomness;uniform theory of diffraction	Ahmet Emin Bulut;Qian Zhang;Chunlei Zhang;Fahimeh Bahmaninezhad;John H. L. Hansen	2017	2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)	10.1109/ASRU.2017.8268958	modern standard arabic;arabic;generative grammar;feature extraction;speech recognition;mel-frequency cepstrum;broadcasting;gaussian;front and back ends;computer science	NLP	-15.884709965274885	-88.75051912880937	55787
2f637682e2ebe2eeb489591e12e8b63d302c3830	applying the harmonic plus noise model in concatenative speech synthesis	analisis fase;discontinuity;time varying;discontinuite;speech intelligibility;text;decomposition;noise factor;speech synthesis;pitch acoustics;etude experimentale;harmonics;speech pleasantness harmonic plus noise model concatenative text to speech synthesis speech signals representation time varying harmonic component modulated noise component speech signal decomposition natural sounding signal modifications adapted schemes parametric speech representation discontinuities smoothing acoustic units formal listening tests high quality speech synthesis speech intelligibility speech naturalness;armonica;facteur bruit;acoustic signal processing;harmonic;concatenacion;tonie;texte;indexing terms;time varying system;concatenation;analyse phase;smoothing methods;acoustic signal processing speech synthesis speech intelligibility harmonics noise signal representation smoothing methods;harmonique;factor ruido;harmonic plus noise model;phase analysis;smoothing;phase estimation;systeme parametre variable;altura sonida;signal representation;alisamiento;text to speech;speech synthesis signal synthesis acoustic noise speech processing phase estimation context modeling linear predictive coding filters degradation transaction databases;sintesis palabra;discontinuidad;sistema parametro variable;descomposicion;texto;estudio experimental;lissage;synthese parole;noise	This paper describes the application of the harmonic plus noise model (HNM) for concatenative text-to-speech (TTS) synthesis. In the context of HNM, speech signals are represented as a time-varying harmonic component plus a modulated noise component. The decomposition of a speech signal into these two components allows for more natural-sounding modifications of the signal (e.g., by using different and better adapted schemes to modify each component). The parametric representation of speech using HNM provides a straightforward way of smoothing discontinuities of acoustic units around concatenation points. Formal listening tests have shown that HNM provides high-quality speech synthesis while outperforming other models for synthesis (e.g., TD-PSOLA) in intelligibility, naturalness, and pleasantness.	acoustic cryptanalysis;automatic sounding;concatenation;intelligibility (philosophy);modulation;psola;smoothing;speech synthesis	Yannis Stylianou	2001	IEEE Trans. Speech and Audio Processing	10.1109/89.890068	concatenation;speech recognition;index term;acoustics;computer science;noise;discontinuity;harmonic;noise figure;decomposition;speech synthesis;intelligibility;smoothing;harmonics	ML	-9.142181298467575	-86.91542960795546	55793
7daab238d1735b72ae7de137ca89448004cccadf	object-oriented access to the estonian phonetic database		The paper introduces the Estonian Phonetic Database developed at the Laboratory of Phonetics and Speech Technology of the Institute of Cybernetics at the Tallinn Technical University, and its integration into QuickSig – an object-oriented speech processing environment developed at the Acoustics Laboratory of the Helsinki University of Technology. Methods of database access are discussed, relations between different speech units – sentences, words, phonemes – are defined, examples of predicate functions are given to perform searches for different contexts, and the advantage of an object-oriented paradigm is demonstrated. The introduced approach has been proven to be a flexible research environment allowing studies to be performed in a more efficient way.	cybernetics;programming paradigm;signal processing;speech processing;speech technology	Einar Meister;Arvo Eek;Toomas Altosaar;Martti Vainio	2000			artificial intelligence;speech recognition;natural language processing;predicate (grammar);speech technology;computer science;database;cybernetics;phonetics;estonian;object-oriented programming;speech processing	DB	-23.76713373991031	-84.20367423418148	55815
6cabdc7db493b0bcb363935d1b4e766271ea03d6	mathematical morphology in the process of musical notation recognition		  Mathematical Morphology is a tool for extracting image components that are useful for representation and description. This  article shows part of process of automatic Optical Music Recognition.. It suggest effective methods used to remove staff line  and preparing image to symbol identification. This method based on mathematical morphology. The experimental results are showing.    		Arkadiusz Rajs	2010		10.1007/978-3-642-16295-4_38	natural language processing;speech recognition;computer science;communication	Vision	-26.070713297834075	-81.96740827210293	55869
44e9943b7543789ff16d298c55c3aa6e9f866e34	inclusion of video information for detection of acoustic events using the fuzzy integral	support vector machines;hidden markov model;video 3d tracking;fuzzy integral;multimodality;hidden markov models;support vector machine;acoustic event detection;3d video;article	When applied to interactive seminars, the detection of acoustic events from only audio information shows a large amount of errors, which are mostly due to the temporal overlaps of sounds. Video signals may be a useful additional source of information to cope with that problem for particular events. In this work, we aim at improving the detection of steps by using two audiobased Acoustic Event Detection (AED) systems, with SVM and HMM, and a video-based AED system, which employs the output of a 3D video tracking algorithm. The fuzzy integral is used to fuse the outputs of the three detection systems. Experimental results using the CLEAR 2007 evaluation data show that video information can be successfully used to improve the results of audiobased AED.	acoustic cryptanalysis;algorithm;hidden markov model;information source;support vector machine;video tracking	Taras Butko;Andrey Temko;Climent Nadeu;Cristian Canton-Ferrer	2008		10.1007/978-3-540-85853-9_7	support vector machine;computer vision;speech recognition;computer science;machine learning;video tracking;pattern recognition;hidden markov model	ML	-6.304182237267098	-89.25317793443695	55998
5cec4ae9d5dff547aa5a807d71e1917d9f58eb0d	a new hybrid approach to large vocabulary cursive handwriting recognition	hybrid modeling technique;modeling technique;probability;handwriting recognition;neural networks;hidden markov model;hidden markov model based handwriting recognition;probability density function;prototypes;vocabulary;large vocabulary cursive handwriting recognition;hybrid model;hybrid approach;markov model;discrete markov models;hidden markov models;continuous markov models;pattern recognition;probability handwriting recognition hidden markov models;computer science;vocabulary handwriting recognition hidden markov models power system modeling probability density function neural networks prototypes pattern recognition gaussian distribution computer science;power system modeling;discrete markov models large vocabulary cursive handwriting recognition hybrid modeling technique hidden markov model based handwriting recognition continuous markov models;gaussian distribution	This paper presents a novel hybrid modeling technique that is used for the first time in Hidden Markov Modelbased handwriting recognition. This new approach combines the advantages of discrete and continuous Markov models and it is shown that this is especially suitable for modeling the features typically used in handwriting recognition. The performance of this hybrid technique is demonstrated by an extensive comparison with traditional modeling techniques for a difficult large vocabulary handwriting recognition task.	discrete mathematics;handwriting recognition;hidden markov model;markov chain;performance;vocabulary	Gerhard Rigoll;Andreas Kosmala;Daniel Willett	1998		10.1109/ICPR.1998.711994	normal distribution;probability density function;speech recognition;intelligent character recognition;computer science;machine learning;pattern recognition;probability;prototype;markov model;hidden markov model;statistics	Vision	-19.956538778708325	-91.83896670120492	56013
9a63c009b2dce1a656db1c0f9b05af182ad4c7d1	gaussian mixture optimization based on efficient cross-validation	sufficient statistic;gaussian mixture;large vocabulary speech recognition gaussian mixture optimization cross validation likelihood objective function conventional training set likelihood;hidden markov model;cross validation likelihood;optimal method;sufficient statistics;gaussian mixture optimization;model complexity;objective function;training data;conventional training set likelihood;statistical distributions;hidden markov models;sufficient statistics cross validation gaussian mixture hidden markov model hmm speech recognition;hidden markov models speech recognition parameter estimation optimization methods merging training data computational efficiency statistical distributions gaussian distribution signal processing algorithms;large vocabulary speech recognition;merging;hidden markov model hmm;speech recognition;cross validation;speech recognition gaussian distribution;parameter estimation;signal processing algorithms;computational efficiency;gaussian distribution;optimization methods	A Gaussian mixture optimization method is developed by using the cross-validation (CV) likelihood as an objective function instead of the conventional training set likelihood. The optimization is based on reducing the number of mixture components by selecting and merging pairs of Gaussians step by step according to the objective function so as to remove redundant components and improve the generality of the model. The CV likelihood is more effective for avoiding over-fitting than is the conventional likelihood, and it provides a termination criterion that does not rely on empirical thresholds. While the idea is simple, one problem is its infeasible computational cost. To make such optimization practical, an efficient evaluation algorithm using sufficient statistics is proposed. In addition, aggregated CV (AgCV) is developed to further improve the generalization performance of CV. Large-vocabulary speech recognition experiments on oral presentations show that the proposed methods improve speech recognition performance with automatically determined model complexity. The AgCV-based optimization is computationally more expensive than the CV-based method but gives better recognition performance.	algorithmic efficiency;bayesian information criterion;cross-validation (statistics);discriminative model;experiment;genetic algorithm;loss function;mdl (programming language);mathematical optimization;model selection;monte carlo method;optimization problem;overfitting;speech recognition;test set;variational principle;vocabulary	Takahiro Shinozaki;Sadaoki Furui;Tatsuya Kawahara	2010	IEEE Journal of Selected Topics in Signal Processing	10.1109/JSTSP.2010.2048235	sufficient statistic;computer science;machine learning;pattern recognition;mathematics;hidden markov model;statistics	ML	-18.507143090778087	-92.32724081342668	56040
c2c704ecb2f29d4ac969ec30d1c9fb6aecd227a8	estimating a user's internal state before the first input utterance	detailed information;dialog-based system;user modeling;dialog system;input utterance;typical system;discrimination experiment;internal state;trained user model;linguistic information	This paper describes a method for estimating the internal state of a user of a spoken dialog system before his/her first input utterance. When actually using a dialog-based system, the user is often perplexed by the prompt. A typical system provides more detailed information to a user who is taking time to make an input utterance, but such assistance is nuisance if the user is merely considering how to answer the prompt. To respond appropriately, the spoken dialog system should be able to consider the user’s internal state before the user’s input. Conventional studies on user modeling have focused on the linguistic information of the utterance for estimating the user’s internal state, but this approach cannot estimate the user’s state until the end of the user’s first utterance. Therefore, we focused on the user’s nonverbal output such as fillers, silence, or head-moving until the beginning of the input utterance. The experimental data was collected on a Wizard of Oz basis, and the labels were decided by five evaluators. Finally, we conducted a discrimination experiment with the trained user model using combined features. As a three-class discrimination result, we obtained about 85% accuracy in an open test.		Yuya Chiba;Akinori Ito	2012	Adv. Human-Computer Interaction	10.1155/2012/865362	natural language processing;user;user modeling;speech recognition;computer science;communication	NLP	-26.111547502230806	-87.47569623473407	56410
b667ed7815a3252af6ee25da06a56bdc59814401	bilinguals activate words from both languages when listening to spoken sentences: evidence from an erp-study		The current study examines whether bilingual word recognition in spoken sentences is influenced by cross-lingual phonological similarity. ERPs were measured while GermanEnglish bilinguals listened to German sentences. Target words in the sentences were either German-English homophones (e.g., eagle – Igel ‘hedgehog’), German words that were phonologically closely related to English words (e.g., kitten – Kittel ‘smock’), or German words that had no phonological relation to English words (e.g., Ziegel ‘brick’). ERPs to target words showed an N400-like facilitation effect for words with cross-lingual phonological overlap (homophones and German-English related words) compared to words with no cross-lingual overlap. However, these results were restricted to bilinguals who learned both languages before age 6, but not for those bilinguals who learned English after age 6. This suggests that early bilinguals activate words from both languages when processing spoken sentences in their dominant language-context.	erp	Nicole Altvater-Mackensen;Nivedita Mani	2011			natural language processing;linguistics	NLP	-11.396639643267486	-80.92221706529156	56486
8341fcca41c780fd3d8d37c3e2871f74704c42b5	a comparative study on methods of weighted language model training for reranking lvcsr n-best hypotheses	lattices;impact factor;weighted gclm;nickel;error correction weighted gclm reranking boost mert discriminative lm;training;vocabulary;discriminative n gram language model;natural languages;testing;discriminative lm;reranking boost;minimum error rate training;error analysis;speech recognition error correction error analysis parameter estimation natural languages boosting laboratories vocabulary testing lattices;boosting;hidden markov models;n best hypotheses reranking;vocabulary speech recognition task;loss function;error correction;n best hypotheses reranking weighted language model training discriminative n gram language model vocabulary speech recognition task reranking boosting minimum error rate training weighted global log linear model loss function;comparative study;mathematical model;speech recognition;parameter estimation;weighted global log linear model;reranking boosting;mert;weighted language model training;language model;log linear model	This paper focuses on discriminative n-gram language models for a large vocabulary speech recognition task. Specifically we compare three training methods, Reranking Boosting (ReBst), Minimum Error Rate Training (MERT) and the Weighted Global Log-Linear Model (W-GCLM). They have a mechanism for handling sample weights, which are useful for providing an accurate model and work as impact factors of hypotheses for training. W-GCLM is proposed in this paper. We discuss the relationship between the three methods by comparing their loss functions. We also compare them experimentally by reranking N-best hypotheses under several conditions. We show that MERT and W-GCLM are different types of expansion of ReBst and have different respective advantages. Our experimental results reveal that W-GCLM outperforms ReBst and whether MERT or W-GCLM is superior depends on the training and test conditions.	experiment;gradient boosting;language model;linear model;log-linear model;loss function;multi-environment real-time;n-gram;speech recognition;vocabulary	Takanobu Oba;Takaaki Hori;Atsushi Nakamura	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495028	nickel;error detection and correction;speech recognition;computer science;machine learning;comparative research;pattern recognition;lattice;mathematical model;software testing;natural language;estimation theory;boosting;statistics;log-linear model;language model;loss function	NLP	-21.328784100943857	-89.81656249189025	56505
7802edcd681d01e44de8245fca8091b6c01e0c48	monotrans2: a new human computation system to support monolingual translation	wisdom of crowds;user interface;monolingual;distributed human computation;translation interface;crowd sourcing;translation;target language;human computation;machine translation	In this paper, we present MonoTrans2, a new user interface to support monolingual translation; that is, translation by people who speak only the source or target language, but not both. Compared to previous systems, MonoTrans2 supports multiple edits in parallel, and shorter tasks with less translation context. In an experiment translating children's books, we show that MonoTrans2 is able to substantially close the gap between machine translation and human bilingual translations. The percentage of sentences rated 5 out of 5 for fluency and adequacy by both bilingual evaluators in our study increased from 10% for Google Translate output to 68% for MonoTrans2.	book;compiler;google translate;human-based computation;machine translation;user interface	Chang Hu;Benjamin B. Bederson;Philip Resnik;Yakov Kronrod	2011		10.1145/1978942.1979111	computer-assisted translation;natural language processing;translation;synchronous context-free grammar;speech recognition;transfer-based machine translation;example-based machine translation;computer science;machine translation;rule-based machine translation;user interface;machine translation software usability	NLP	-26.55017027652043	-84.1208687799768	56538
69f828cf8d9ee675c7ac2b2ef88aed84e1280b46	machine translation among languages with transitivity divergences using the causal relation in the interlingual lexicon	traitement automatique des langues naturelles;entree lexicale;japonais;relation semantique;verbe;transitivite;traduction automatique;anglais;causalite;target language;mechanical translation;source language;evenement;semantic relation;event;computational linguistics;causal relation;transitivity;interlangue;linguistique informatique;natural language processing;verb word;machine translation;causality;verb	This paper proposes a design of verb entries in Interlingua to facilitate the machine translation (MT) of two languages with transitivity divergence as derived from their shared and individual linguistic characteristics. It suggests that the transitivity difference is best treated with verb entries containing information of the causal relation of the expressed events. It also demonstrates how the proposed design of verb entries gives a principled treatment of aspect divergence in semantically corresponding verbs of a source language (SL) and a target language (TL). Although the current paper focuses on English and Japanese, the proposed treatment should be applicable to the MT of similarly divergent languages, since the proposed lexicon in language-independent Interlingua contains information on causal relations of events as necessary to bridge the transitivity difference.	causality;lexicon;machine translation;vertex-transitive graph	Yukiko Sasaki Alam	1998		10.1007/3-540-49478-2_41	natural language processing;causality;event;computer science;computational linguistics;machine translation;transitive relation	NLP	-32.91771487829274	-81.27904989094569	56563
5a76a5f382ad5dd6c8f5b6257b176ca41286d2b8	incorporating language level information into acoustic models		This paper proposed a class of novel Deep Recurrent Neural Networks which can incorporate language-level information into acoustic models. For simplicity, we named these networks Recurrent Deep Language Networks (RDLNs). Multiple variants of RDLNs were considered, including two kinds of context information, two methods to process the context, and two methods to incorporate the language-level information. RDLNs provided possible methods to fine-tune the whole Automatic Speech Recognition (ASR) system in the acoustic modeling process.	acoustic cryptanalysis;acoustic model;neural networks;recurrent neural network;speech recognition	Peidong Wang;Deliang Wang	2016	CoRR		natural language processing;speech recognition;computer science	ML	-17.630757922737924	-88.00064698195207	56636
5bdc8b9ee4b410f12303399ebd54393b08e1d277	mixture of hmm experts with applications to landmine detection	wemi mixture of experts hidden markov models me hmm landmine detection metal detector;landmine detection;me;mixture of experts;wemi;hmm;hidden markov models;hidden markov models logic gates context landmine detection data models context modeling metals;landmine detection hidden markov models;metal detector;me based models landmine detection hidden markov model experts mhmme context based classification variable length sequences single probabilistic model landmine dataset	This paper introduces a novel mixture of experts model, the Mixture of Hidden Markov Model Experts (MHMME). This model is designed to perform context-based classification of samples that are variable length sequences. The contexts are determined by the gates and the classifiers are determined by the experts. The gates and the experts are learned simultaneously using a single probabilistic model. Experimental results on landmine dataset show that MHMME significantly outperforms the HMM-based and ME-based models.	hidden markov model;markov chain;statistical model	Seniha Esen Yuksel;Paul D. Gader	2012	2012 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2012.6352589	speech recognition;computer science;machine learning;pattern recognition;hidden markov model	Vision	-20.316746972207525	-93.98759291042477	56642
b5715e1de7c85777c17b8fd098832a272c350aff	oriental-script text detection and extraction in videos		Urdu text detection and extraction from videos especially from news feed is a challenging task. The main challenge is to design a robust algorithm that can accurately detect and localize Urdu text regions in videos. Urdu language is originated from oriental languages like Arabic, Persian and Turkish, so alphabets of these languages are very similar. Due to the similarities of basic alphabets and writing styles, this research is equally applicable to Arabic or Persian text. The development of an OCR that gives acceptable results on Urdu is still an ongoing research area. Thus need of a good text detection and extraction system is needed that can reduce the recognition errors as much as possible. It is because if the input image contains only text candidate regions, then possibilities of errors will be reduced for the same OCR. The proposed system is evaluated on a set of videos of news channels that vary along the dimensions of intensity variations, scale and orientation. Accuracy in terms of precision and recall rates is analyzed to determine the success and limitations of the approach. Problems, possible errors and suggestions for improvements are also discussed in the paper.	algorithm;connected component (graph theory);edge detection;internet;optical character recognition;precision and recall;social media;web feed	Usman Shahzad;Khurram Khurshid	2017	2017 1st International Workshop on Arabic Script Analysis and Recognition (ASAR)	10.1109/ASAR.2017.8067752	writing style;turkish;arabic;precision and recall;algorithm design;persian;natural language processing;urdu;artificial intelligence;computer science	NLP	-24.460312547918477	-80.59374846895774	56828
c0badc853ad0cd1f2abe653f1fabdf55356eb9f7	physical and perceptual configurations of japanese fricatives from multidimensional scaling analyses.	multidimensional scaling	This study investigates the correlations between physical and perceptual spaces of voiceless Japanese fricatives /f s S C h/, using Multidimensional Scaling technique. The spatial configurations were constructed from spectral distance measures and perceptual similarity judgements. The results show that 2-dimensional solutions adequately account for the data and the correlations between the two spaces are high. The dimensions also corresponded to ‘sibilance’ and ‘place’ properties. The spectral analyses on fricative sections, excluding the transitions, seem to contain sufficient information for correct perceptual judgements. These results are highly comparable to those of English fricative study (Choo, 1999), and support a universal prototype theory, according to which the correct identification of speech segments depends on the perceived distance between speech stimuli and a prototype in perceptual spaces.	2.5d;image scaling;multidimensional scaling;prototype	Won Tokuma	2003			multidimensional scaling;computer science	HCI	-9.873654745183126	-83.19971803436657	56873
c49f4779fef1b8bd5ab93456b3a7fd080c135153	a real-time mri investigation of the role of lingual and pharyngeal articulation in the production of the nasal vowel system of french	pharynx;french;articulation;rt mri;nasalization	It is well known that, for nasal vowels, traditional estimation of the shape of the vocal tract via inference from acoustic characteristics is complicated by the acoustic effects of velopharyngeal coupling (i.e. nasalization). Given this complexity, measuring the shape of the vocal tract directly is, perhaps, a more desirable method of assessing oro-pharyngeal configuration. Real-time MRI (rt-MRI) allows us to explore the shape of the entire vocal tract during the production of nasal vowels. This permits us to better assess the contribution of the oro-pharyngeal acoustic transfer function to the acoustic signal, which is otherwise obscured by the conflation of the independent oro-pharyngeal and nasal acoustic transfer functions. The oro-pharyngeal shape associated with nasal vowels has implications for both synchronic and diachronic phonology, particularly in French, where descriptions of nasal vowels have long suggested that differences in oral articulation, in addition to velopharyngeal coupling, serve to distinguish oral and nasal vowels. In this study, we use single-slice rt-MRI (midsagittal slice) and multi-slice rt-MRI (oral, velopharyngeal, mediopharyngeal, and lower pharyngeal slices) to examine three nasal vowels / ~ ɛ, ~ ɑ, ~ ɔ/ and their traditional oral counterparts /ɛ, a, o/ as produced by three female speakers of Northern Metropolitan French (NMF). We find evidence of lingual and pharyngeal articulatory configurations which may, in some cases, enhance formant-frequency-related acoustic effects associated with nasalization, viz., modulation of F1 and F2. Given these findings, we speculate that the synchronic oral articulation of NMF nasal vowels may have arisen—at least in part—due to misperception of the articulatory source of changes in F1 and F2, rather than to mere chance, as has been argued. & 2015 Elsevier Ltd. All rights reserved.	acoustic cryptanalysis;biconnected component;modulation;non-negative matrix factorization;real-time clock;real-time transcription;synchronicity;tract (literature);transfer function;viz: the computer game	Christopher Carignan;Ryan Shosted;Maojing Fu;Zhi-Pei Liang;Bradley P. Sutton	2015	J. Phonetics	10.1016/j.wocn.2015.01.001	speech recognition;nasalization;acoustics;philosophy;french;linguistics;sociology;communication	HCI	-8.84942063598291	-83.45075106274818	56893
6c26676979bcb988a17cc67140701b83589140f0	閩南語語句基週軌跡產生: 兩種模型之混合與比較 (min-nan sentence pitch-contour generation: mixing and comparison of two kinds of models) [in chinese]			nan	Hung-Yan Gu;Wei Huang	2005			pitch contour;artificial intelligence;pattern recognition;mathematics;sentence	NLP	-14.598102153441385	-85.58157266923575	56982
1f85b9a820763981ade4e55fcfc2caa8f6984e29	classifying unprompted speech by retraining lstm nets	modelo markov oculto;modele markov cache;hidden markov model;reconocimiento voz;computer sciences;reseau memoire long court terme;speech recognition;verbmobil data;reseau neuronal recurrent;recurrent neural nets;reconnaissance parole;reseau neuronal;red neuronal;long short term memory;neural network	We apply Long Short-Term Memory (LSTM) recurrent neural networks to a large corpus of unprompted speechthe German part of the VERBMOBIL corpus. By training first on a fraction of the data, then retraining on another fraction, we both reduce time costs and significantly improve recognition rates. For comparison we show recognition rates of Hidden Markov Models (HMMs) on the same corpus, and provide a promising extrapolation for HMM-LSTM hybrids.	artificial neural network;extrapolation;hidden markov model;long short-term memory;markov chain;microsoft outlook for mac;recurrent neural network;skolem normal form;test set;verbmobil	Nicole Beringer;Alex Graves;Florian Schiel;Jürgen Schmidhuber	2005		10.1007/11550822_90	speech recognition;computer science;artificial intelligence;machine learning;artificial neural network;hidden markov model;long short term memory	NLP	-19.806762528984358	-87.90007390466934	57001
d4bb8ee66e1cd3b3f4158792de236ad28ef9b885	a robust howling detection algorithm based on a statistical approach	support vector machine robust howling detection algorithm statistical methods audio signal energy based feature frequency stability howling component;support vector machines;howling detection acoustic signal detection;acoustics;support vector machines acoustic signal detection audio signal processing statistical analysis;training;speech;support vector machines speech acoustics time frequency analysis multiple signal classification hidden markov models training;multiple signal classification;hidden markov models;time frequency analysis	This paper presents an algorithm for the detection of howlings that arise in audio signals. Our method is based on the combination of two energy-based features and one new feature related to the frequency stability of a howling component. The decision stage, which implies a Support Vector Machine (SVM) model, outputs a decision every 20 ms. The evaluation, carried out on a large database, showed that the algorithm is able to detect both pure and multiple tones howling in a wide range of energy. Furthermore, even on complex signals such as music, the detection is still efficient with very few false alarms.	algorithm;database;frequency drift;support vector machine	Joachim Flocon-Cholet;Julien Faure;Alexandre Guérin;Pascal Scalart	2014	2014 14th International Workshop on Acoustic Signal Enhancement (IWAENC)	10.1109/IWAENC.2014.6953339	speech recognition;computer science;machine learning;pattern recognition	ML	-10.813812261498608	-91.91914919896656	57019
4ad86d0e78d733549ce95445ea9d83be29c8e42c	attention, word position, and perceptual learning		This paper presents the results of an experiment which tested the roles of directed attention and lexical bias on perceptual learning. Attention was manipulated by directing a group of listeners to be aware the speaker had an ambiguous pronunciation of /s/. Lexical bias was manifested by where in the word the ambiguous /s/ was positioned – the first or final syllable. In all conditions listeners were exposed to an ambiguous sound halfway between an /s/ and a /ʃ/ in word contexts for /s/. Listeners who were only exposed to the ambiguous sound in final syllable adapted their category boundary more than listeners who were exposed to the ambiguous sound at the beginnings of words, but only when they were not explicitly instructed to pay attention to the speaker’s ambiguous /s/ sounds. These results indicate that perceptual adaptation is at least a partially controlled adjustment.	lexicon;syllable	Michael McAuliffe;Molly Babel	2015			cognitive psychology;perceptual learning;psychology	AI	-10.197745073480233	-80.82775903346945	57073
2fea1a0690018fa9f0ecb6559f4d48b88c09ccdd	asr system modeling for automatic evaluation and optimization of dialogue systems	databases;dialogue system;confidence level;computational modeling optimization markov processes speech recognition databases learning;learning;system modeling;reinforcement learning;automatic evaluation;spoken dialogue system;conference paper;computational modeling;error rate;speech recognition;optimization;markov processes;learning strategies;simulation environment	Though the field of spoken dialogue systems has developed quickly in the last decade, rapid design of dialogue strategies remains uneasy. Several approaches to the problem of automatic strategy learning have been proposed and aie use of Reinforcement Learning introduced by Levin and Pieraccini is becoming part of the state of the art in this area. However, the quality of the strategy learned by the system depends on the definition of the optimization criterion and on the accuracy of aie environment model. In this paper, we propose to bring a model of an ASR system in the simulated environment in order to enhance the learned strategy. To do so, we introduced recognition error rates and confidence levels produced by ASR systems in the optimization criterion.	automated system recovery;dialog system;mathematical optimization;reinforcement learning;systems modeling;virtual reality	Olivier Pietquin;Steve Renals	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5743650	natural language processing;computer vision;speech recognition;systems modeling;confidence interval;word error rate;computer science;artificial intelligence;machine learning;markov process;computational model;reinforcement learning;statistics	Robotics	-25.38798669487007	-88.31772762083355	57147
c827974dc480794e957c444d75240bb4e779d142	an efficient clustering algorithm and its use in phoneme synthesis.	cluster algorithm			Jin Zijian;Ke Youan	1987			correlation clustering;speech recognition;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;fsa-red algorithm;cluster analysis;linde–buzo–gray algorithm;k-medoids	EDA	-17.642020910456708	-95.85107232832587	57230
6501955bf314aa570c89b6073b4a93de24aeb78f	text search for medieval manuscript images	word spotting;medieval manuscripts;algoritmo busqueda;algorithme recherche;speech processing;search algorithm;alfabeto;tratamiento palabra;traitement parole;accuracy;precision;reconocimiento voz;word recognition;speech recognition;reconnaissance parole;alphabet;elastic matching	In this article we introduce a text search algorithm designed for ancient manuscripts. Word-spotting is the best alternative to word recognition on this type of document. Our method is based on differential features that are compared using a cohesive elastic matching method, based on zones of interest in order to match only the informative parts of the words. Thus we improved both the accuracy and the runtime of the word-spotting process. The proposed method is tested on medieval manuscripts of Latin and Semitic alphabets as well as on more recent manuscripts.		Yann Leydier;Frank Lebourgeois;Hubert Emptoz	2007	Pattern Recognition	10.1016/j.patcog.2007.04.024	speech recognition;computer science;artificial intelligence;speech processing;accuracy and precision;algorithm	Vision	-24.202878021886242	-81.77524437913064	57244
2273814a2ce44a330a177bd13a2cf8215885a3f3	dynamic minimum subband spectral subtraction and its application in robust speech recognition	noise estimation;estimation theory;robust speech recognition;nonlinear feature process algorithm;speech recognition acoustic noise working environment noise speech enhancement noise robustness automatic speech recognition feature extraction noise level hidden markov models additive noise;spectral subtraction;automatic speech recognition;automatic speech recognition dynamic minimum subband spectral subtraction nonlinear feature process algorithm noise estimation;speech recognition;speech recognition estimation theory signal denoising spectral analysis;spectral analysis;dynamic minimum subband spectral subtraction;signal denoising	A nonlinear feature process algorithm called dynamic minimum subband spectral subtraction (DMSS) is described, inspired by amplitude spectra properties of noisy speech. This method does not require noise estimation and it is effective in dealing with both stationary and non-stationary noise. Its application for minimizing mismatch between clean and noisy speech features is also present. Experimental results show the proposed method can effectively improve the robustness of automatic speech recognition (ASR) and when combined with peak isolation method properly, it can improve the recognition performance greatly	algorithm;nonlinear system;speech recognition;stationary process	Xin Ma;Yuhua Peng	2006	First International Conference on Innovative Computing, Information and Control - Volume I (ICICIC'06)	10.1109/ICICIC.2006.442	speech recognition;computer science;pattern recognition;speech processing;mathematics;estimation theory	Robotics	-12.999082540214426	-91.9524040749849	57336
bb1faf77092a401a5fd046eb6dc611c27166b78b	long-term snr estimation using noise residuals and a two-stage deep-learning framework		Knowing the signal-to-noise ratio of a noisy speech signal is important since it can help improve speech applications. This paper presents a two-stage approach for estimating the long-term signal-tonoise ratio (SNR) of speech signals that are corrupted by background noise. The first stage produces noise residuals from a speech separation module. The second stage then uses the residuals and a deep neural network (DNN) to predict long-term SNR. Traditional SNR estimation approaches use signal processing, unsupervised learning, or computational auditory scene analysis (CASA) techniques. We propose a deep-learning based approach, since DNNs have outperformed other techniques in several speech processing tasks. We evaluate our approach across a variety of noise types and input SNR levels, using the TIMIT speech corpus and NOISEX-92 noise database. The results show that our approach generalizes well in unseen noisy environments, and it outperforms several existing methods.	algorithm;artificial neural network;computational auditory scene analysis;deep learning;signal processing;signal-to-noise ratio;speech corpus;speech processing;timit;unsupervised learning	Xuan Dong;Donald S. Williamson	2018		10.1007/978-3-319-93764-9_33	unsupervised learning;deep learning;artificial neural network;signal processing;speech processing;computational auditory scene analysis;background noise;speech corpus;pattern recognition;computer science;artificial intelligence	ML	-14.973523262252929	-90.83662516289533	57360
df46eb579c9ec7cb3294c1260777f89cb3cde1db	tones as gestures: the case of italian and german		In this paper we investigate tonal alignment in peak accents in Italian and German. We show that timing differences across the two languages are systematic and result from crucial differences in phonological structure. The F0 rise in Italian is represented as a tonal high gesture, the onset of which is synchronised with the vocalic gesture. The German rise is the result of a sequence of tonal gestures, low and high, which compete for alignment with the vocalic gesture, resulting in a delay in the rise. A comparison of accented and deaccented syllables in German shows that the presence of a non-lexical tone does not affect the timing of consonantal and vocalic gestures.	onset (audio)	Henrik Niemann;Doris Mücke;Hosung Nam;Louis Goldstein;Martine Grice	2011			speech recognition;engineering;linguistics;communication	NLP	-10.711827041168558	-81.77762550016098	57412
c25a5742b9ef703683da13d529b0189d5fbd46cc	robust spectro-temporal features based on autoregressive models of hilbert envelopes	hilbert envelopes;hilbert transforms;ar model;phoneme recognition frequency domain linear prediction fdlp hilbert envelopes robust spectro temporal features;frequency modulation;frequency domain linear prediction;speech processing;speech processing autoregressive processes feature extraction hilbert transforms;speech;frequency domain linear prediction fdlp;robust spectro temporal features;autoregressive model;telephone speech;performance improvement;autoregressive models;autoregressive processes;feature extraction;noise robustness feature extraction predictive models frequency domain analysis frequency modulation frequency conversion speech recognition telephony spatial databases error analysis;error rate;speech recognition;phoneme recognition;robustness;spectro temporal feature extraction;noise;telephone speech autoregressive models hilbert envelopes spectro temporal feature extraction frequency domain linear prediction phoneme recognition	In this paper, we present a robust spectro-temporal feature extraction technique using autoregressive models (AR) of sub-band Hilbert envelopes. AR models of Hilbert envelopes are derived using frequency domain linear prediction (FDLP). From the sub-band Hilbert envelopes, spectral features are derived by integrating these envelopes in short-term frames and the temporal features are formed by converting these envelopes into modulation frequency components. The spectral and temporal feature streams are then combined at the phoneme posterior level and are used as the input features for a recognition system. For the proposed features, robustness is achieved by using novel techniques of noise compensation and gain normalization. Phoneme recognition experiments on telephone speech in the HTIMIT database show significant performance improvements for the proposed features when compared to other robust feature techniques (average relative reduction of 10.6 % in phoneme error rate). In addition to the overall phoneme recognition rates, the performance with broad phonetic classes is also reported.	autoregressive model;experiment;feature extraction;modulation	Sriram Ganapathy;Samuel Thomas;Hynek Hermansky	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495668	speech recognition;computer science;pattern recognition;speech processing;autoregressive model;statistics	Vision	-11.71284652161573	-91.19183430924288	57420
1d77f30a93a7b0ae8a4190c95be63f16364cb301	semi-supervised feature selection for audio classification based on constraint compensated laplacian score	signal image and speech processing;constraint information;acoustics;audio classification;semi supervised feature selection;mathematics in music;locality preserving;engineering acoustics	Audio classification, classifying audio segments into broad categories such as speech, non-speech, and silence, is an important front-end problem in speech signal processing. Dozens of features have been proposed for audio classification. Unfortunately, these features are not directly complementary and combining them does not improve classification performance. Feature selection provides an effective mechanism for choosing the most relevant and least redundant features for classification. In this paper, we present a semi-supervised feature selection algorithm named Constraint Compensated Laplacian score (CCLS), which takes advantage of the local geometrical structure of unlabeled data as well as constraint information from labeled data. We apply this method to the audio classification task and compare it with other known feature selection methods. Experimental results demonstrate that CCLS gives substantial improvement.	feature selection;selection algorithm;semi-supervised learning;semiconductor industry;signal processing;speech processing	Xu-Kui Yang;Liang He;Dan Qu;Wei-Qiang Zhang;Michael T. Johnson	2016	EURASIP J. Audio, Speech and Music Processing	10.1186/s13636-016-0086-9	audio mining;speech recognition;acoustics;computer science;machine learning;speech coding;pattern recognition;physics	AI	-15.842525458310746	-92.05467641729194	57440
ac64e3c0f5db0666b0499cee56fb91beae2de13a	video handling with music and speech detection	timed systems;video signal processing;multiple signal classification music cameras image segmentation streaming media indexing speech recognition hidden markov models speech analysis telegraphy;speech processing;video segmentation;multimedia systems;video indexing;director intentions video handling music detection speech detection audio based approach video indexing indexed video segments video sound browser random video access video in time system video condensation levels video structuring;indexing;indexation;music;indexing video signal processing music multimedia systems speech processing;speech detection	The audio-based approach to video indexing described by the authors detects music and speech independently even when they occur simultaneously. The indexed video segments, when presented on the Video Sound Browser, let users randomly access the video. The Video in Time system provides different video condensation levels based on video structuring that can link the video segments and the director's intentions.		Kenichi Minami;Akihito Akutsu;Hiroshi Hamada;Yoshinobu Tonomura	1998	IEEE MultiMedia	10.1109/93.713301	video compression picture types;microsoft video 1;computer vision;speech recognition;video;uncompressed video;computer science;video capture;video tracking;music;speech processing;multimedia;video processing;smacker video;video post-processing;video denoising;multiview video coding;non-linear editing system	Vision	-6.206341337610569	-94.00708706666845	57473
cb4c68f73bcb6c06d218ec9f20a8ba6d8b26a37c	wavelet autoencoder for radar hrrp target recognition with recurrent neural network		A Wavelet Autoencoder model with Recurrent Neural Network (WaveletAE with RNN) is developed for radar automatic target recognition (RATR), with an encoder-decoder layer, in which the weights of decoder are fixed as a set of overcomplete bases derived from mother wavelet. Imposing an sparsity constraint on the hidden units of encoder-decoder layer, interesting structure in the data is discovered, and superior recognition performance is achieved on the measured High Resolution Range Profiles (HRRP) data, showing the effectiveness of the proposed model. Specific results are represented in our experiments.	autoencoder;radar;recurrent neural network;wavelet	Mengjiao Zhang;Bo Chen	2018		10.1007/978-3-030-02698-1_23	wavelet;autoencoder;radar;recurrent neural network;artificial intelligence;pattern recognition;automatic target recognition;computer science	AI	-16.33831234624531	-89.20428026614528	57475
eca3bc77a557c1d409aae21da29edc8a252ea5b5	rejection of out-of-vocabulary words using phoneme confidence likelihood	vocabulary speech recognition acoustic measurements viterbi algorithm humans laboratories acoustic testing error analysis acoustic noise current measurement;maximum likelihood out of vocabulary words rejection phoneme confidence likelihood performance speech recognition anti keyword model method small vocabulary task independent rejection unknown words phoneme confidence measure partial utterances verification phonetic typewriter verification accuracy tests recognition rate error rates acoustic model training mce algorithm;confidence measure;out of vocabulary;acoustic modeling;maximum likelihood estimation;maximum likelihood estimation speech recognition error statistics;equal error rate;speech recognition;error statistics	The rejection of unknown words is important in improving the performance of speech recognition. The anti-keyword model method can reject unknown words with high accuracy in a small vocabulary and specified task. Unfortunately, it is either inconvenient or impossible to apply if words in the vocabulary change frequently. We propose a new method for task independent rejection of unknown words, where a new phoneme confidence measure is used to verify partial utterances. It is used to verify each phoneme while locating candidates. Furthermore, the whole utterance is verified by a phonetic typewriter. This method can improve the accuracy of verification in each phoneme, and improve the speed of candidate search. Tests show that the proposed method improves the recognition rate by 4% compared to the conventional algorithm at equal error rates. Furthermore, a 3% improvement is obtained by training acoustic models with the MCE algorithm.	rejection sampling;vocabulary	Takatoshi Jitsuhiro;Satoshi Takahashi;Kiyoaki Aikawa	1998		10.1109/ICASSP.1998.674406	natural language processing;speech recognition;word error rate;computer science;pattern recognition;mathematics;maximum likelihood;statistics	NLP	-15.305622408587755	-92.55998786253157	57487
d44ae01efb351bb15f107d7420e20ed361dd4ccd	a new approach to speech-input statistical translation	dynamic programming;graph theory;probability;speech recognition natural languages pattern recognition decoding speech processing tiles transducers stochastic systems acoustic distortion;language translation;text input;natural extension;traveller task speech input statistical translation statistical pattern recognition word graph;iterative methods;statistical pattern recognition;speech recognition;search problems;probability speech recognition language translation graph theory dynamic programming search problems iterative methods	"""The statistical pattern recognition is a promising framework for text-to-text translation. However, a natural extension to speech-input translation is not straightforward. In this paper, we present a method to deal with the speech input statistical translation problem that could be considered as a step towards a fully integrated recognition-translation procedure. In this version a word graph was used in the input as a representation of the acoustic of a given utterance. As a case study, experimental results with the so-called """" Traveller task """" are presented by using a text-input statistical translator."""	acoustic cryptanalysis;pattern recognition;statistical machine translation	Ismael García-Varea;Alberto Sanchís;Francisco Casacuberta	2000		10.1109/ICPR.2000.903492	natural language processing;cache language model;speech recognition;feature;word error rate;computer science;graph theory;machine learning;dynamic programming;pattern recognition;probability;iterative method;statistics;language model	NLP	-21.974161151848527	-91.61831621435476	57535
d2a68d307f6c0487631db0b1df80bf5c7f533d54	a hybrid approach for discourse segment detection in the automatic subtitle generation of computer science lecture videos	subtitles discourse segment detection;video signal processing computer aided instruction computer science education speech recognition speech synthesis;videos speech acoustics pragmatics automatic speech recognition computer science;speech engine hybrid discourse segment detection approach automatic subtitle generation computer science lecture videos cmu sphinx speech api speech recognition translated text dsd	The aim of this paper is to develop an automatic subtitle generation system for computer science lecture videos. CMU Sphinx Speech API is used to accomplish speech recognition. The main challenge of this work, is to align the translated text with the video. Discourse Segment Detection (DSD) is the process of analyzing and identifying discourse boundaries in human speech. Discourse Segment Detection (DSD) is carried out that classifies word boundaries and groups words until a discourse break occurs. The approach that has been devised in this paper for DSD to identify word boundary is a hybrid approach combining acoustic and linguistic features from the speech. This helps to segment the text obtained from Speech Engine, group words that can be written to the subtitles file without violating the subtitle standards. The devised approach has shown an improved performance than the existing approach as the error has reduced from 30% to 18 %.	acoustic cryptanalysis;align (company);computer science;document structure description;microsoft speech api;speech recognition;speech synthesis;sphinx4	Rajeswari Sridhar;S. Aravind;Hamid Muneerulhudhakalvathi;M. Sibi Senthur	2014	2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2014.6968422	voxforge;natural language processing;speech technology;speech recognition;speech corpus;computer science;speech synthesis;speech analytics	NLP	-22.22420489535763	-82.97309539726683	57635
7395a117b647ac4637df6b0ececf7c7bbe1fa886	a multiple bandwidth objective speech intelligibility estimator based on articulation index band correlations and attention	speech intelligibility;abc mrt16;articulation index;abc mrt;objective estimator;modified rhyme test mrt	We present ABC-MRT16—a new algorithm for objective estimation of speech intelligibility following the Modified Rhyme Test (MRT) paradigm. ABC-MRT16 is simple, effective and robust. When compared to subjective MRT data from 367 diverse conditions that include coding, noise, frame erasures, and much more, ABC-MRT16 (containing just one optimized parameter) yields a very high Pearson correlation (above 0.95) and a remarkably low RMS estimation error (below 7% of full scale.) We attribute these successes to concise modeling of core human processes in audition and forced-choice word selection. On each trial, ABC-MRT16 gathers word selection evidence in the form of articulation index band correlations and then uses a simple attention model to perform word selection using the best available evidence. Attending to best evidence allows ABC-MRT16 to work well for narrowband, wideband, superwideband, and fullband speech and noise without any bandwidth detection algorithm or side information.		Stephen D. Voran	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7953128	psychology;speech recognition;acoustics;communication	DB	-9.274009135970807	-88.1156918799251	57750
a3a3dc69174aa1aeac324bda58c7b62d053bd67f	features for the identification of mixed excitation in speech analysis	citation analysis;speech synthesis;pitch determination algorithm;speech analysis;speech segmentation;filters;frequency measurement;frequency spectrum;statistical analysis;acoustic noise;pattern classification;humans;synthesizers;speech analysis speech synthesis pattern classification acoustic noise frequency measurement filters citation analysis humans synthesizers statistical analysis	Features for use in a pattern classification scheme to identify simultaneous periodic and noiselike excitation of a segment of speech are examined. Pattern classification techniques have been applied with considerable success to the problem of classifying a speech segment as voiced or unvoiced. The features that have proven adequate for the voiced/unvoiced decision have not sufficed for the three-way voiced/unvoiced/mixed excitation classification. The incorporation of periodicity measures (e.g. from pitch determination algorithms) into such a pattern classification framework are examined. A variety of features which compare periodicity in different bands of the frequency spectrum are presented.	voice analysis	Leah J. Siegel	1979		10.1109/ICASSP.1979.1170783	frequency spectrum;speech recognition;computer science;noise;pattern recognition;speech processing;speech segmentation;speech synthesis;citation analysis;statistics	NLP	-8.657315506556113	-90.83812787879853	57761
073a6a341af1740f570f9829535f5089886d8253	noise robust exemplar-based connected digit recognition	background noise;decoding;kullback leibler divergence;speech recognition character recognition noise abatement signal denoising;speech processing;speech;noise robust exemplar;speech enhancement;noise abatement;noise robustness;noise measurement;automatic speech recognition;accuracy;exemplar based;hidden markov models;sparsity;speech recognition system;signal processing;non negative matrix factorization;dictionaries;speech recognition;sparsity speech recognition exemplar based noise robustness non negative matrix factorization;connected digit recognition;connected digit recognition noise robust exemplar speech recognition system kullback leibler divergence signal to noise ratio;signal to noise ratio;article in monograph or in proceedings;character recognition;noise robustness speech enhancement speech recognition automatic speech recognition hidden markov models decoding signal to noise ratio background noise speech processing signal processing;signal denoising	This paper proposes a noise robust exemplar-based speech recognition system where noisy speech is modeled as a linear combination of a set of speech and noise exemplars. The method works by finding a small number of labeled exemplars in a very large collection of speech and noise exemplars that jointly approximate the observed speech signal. We represent the exemplars using melenergies, which allows modeling the summation of speech and noise, and estimate the activations of the exemplars by minimizing the generalized Kullback-Leibler divergence between the observations and the model. The activations of the speech exemplars are directly being used for recognition. This approach proves to be promising, achieving up to 55.8% accuracy at signal-to-noise ratio −5 dB on the AURORA-2 connected digit recognition task.	approximation algorithm;kullback–leibler divergence;signal-to-noise ratio;speech recognition	Jort F. Gemmeke;Tuomas Virtanen	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495580	speech recognition;computer science;noise measurement;speech;signal processing;pattern recognition;speech processing;background noise;accuracy and precision;kullback–leibler divergence;noise control;signal-to-noise ratio;sparsity-of-effects principle;non-negative matrix factorization;statistics	Robotics	-16.35235710611098	-93.13085228116036	57770
08e83f93f8d75101c9b5c5dc78d73329c7f53253	continuous speech recognition under non-stationary musical environments based on speech state transition model	kalman filters;noise speech recognition state estimation music kalman filters filtering theory prediction theory;state estimation;prediction theory;speech recognition state estimation working environment noise noise reduction speech enhancement background noise kalman filters filtering taylor series predictive models;speech recognition;word accuracy rate continuous speech recognition nonstationary musical environments speech state transition model nonstationary noise reduction method speech signal estimation nonstationary noisy environments kalman filtering estimation nonstationary noisy speech taylor expansion linear predictive estimation large vocabulary continuous speech recognition parallel model combination method;music;filtering theory;noise	We propose a non-stationary noise reduction method based on the speech state transition model. Our proposed method estimates the speech signal under non-stationary noisy environments such as musical background by applying the speech state transition model to Kalman filtering estimation. The speech state transition model represents the state transition of the speech component in non-stationary noisy speech and is modeled by using Taylor expansion. In this model, the state transition of the noise component is estimated by using linear predictive estimation. In order to evaluate the proposed method, we carried out large vocabulary continuous speech recognition experiments under 3 types of music and compared the results with the conventional parallel model combination (PMC) method in word accuracy rate. As a result, the proposed method obtained a word accuracy rate that was superior to PMC.	speech recognition;state transition table;stationary process	Masakiyo Fujimoto;Yasuo Ariki	2001		10.1109/ICASSP.2001.940826	linear predictive coding;speech recognition;computer science;noise;speech coding;pattern recognition;music	NLP	-13.849861098492404	-93.4995830593072	57781
f3654759578d8a1c32c5534637b5ea2f6ed11aeb	motion-driven concatenative synthesis of cloth sounds	sound synthesis;cloth simulation;data driven methods	We present a practical data-driven method for automatically synthesizing plausible soundtracks for physics-based cloth animations running at graphics rates. Given a cloth animation, we analyze the deformations and use motion events to drive crumpling and friction sound models estimated from cloth measurements. We synthesize a low-quality sound signal, which is then used as a target signal for a concatenative sound synthesis (CSS) process. CSS selects a sequence of microsound units, very short segments, from a database of recorded cloth sounds, which best match the synthesized target sound in a low-dimensional feature-space after applying a hand-tuned warping function. The selected microsound units are concatenated together to produce the final cloth sound with minimal filtering. Our approach avoids expensive physics-based synthesis of cloth sound, instead relying on cloth recordings and our motion-driven CSS approach for realism. We demonstrate its effectiveness on a variety of cloth animations involving various materials and character motions, including first-person virtual clothing with binaural sound.	binaural beats;cascading style sheets;concatenation;concatenative synthesis;graphics	Steven S. An;Doug L. James;Steve Marschner	2012	ACM Trans. Graph.	10.1145/2185520.2185598	speech recognition;computer graphics (images)	Graphics	-15.074798246021395	-82.57078277211797	57804
43861dae5094a2d59bc9f39f190104322836c183	toward an acoustic-articulatory model of inter-speaker variability			acoustic cryptanalysis;spatial variability	Parham Mokhtari;Frantz Clermont;Kazuyo Tanaka	2000			speech recognition;artificial intelligence;pattern recognition;computer science	NLP	-14.19248061468118	-86.95672552881912	57954
156d2fd7cfdc5dab363e698be30566b3c2bdc135	a multi-smartwatch system for assessing speech characteristics of people with dysarthria in group settings	loudness;knowledge based speech processing;shimmer dysarthria jitter knowledge based speech processing loudness multi smartwatch system perceptual speech quality pitch semitone standard deviation sharpness;shimmer;sharpness;dysarthria;speech processing blind source separation loudness medical disorders medical signal processing;pitch;perceptual speech quality;jitter;sharpness computation multismartwatch system speech characteristics dysarthria speech language pathologists vocal exercises patient treatment speech disorders mixed speech signal blind separation loudness computation pitch computation shimmer computation jitter computation semitone standard deviation;semitone standard deviation;speech speech processing monitoring random variables blind source separation acoustics estimation;multi smartwatch system	Speech-language pathologists (SLPs) frequently use vocal exercises in the treatment of patients with speech disorders. Patients receive treatment in a clinical setting and need to practice outside of the clinical setting to generalize speech goals to functional communication. In this paper, we describe the development of technology that captures mixed speech signals in a group setting and allows the SLP to analyze the speech signals relative to treatment goals. The mixed speech signals are blindly separated into individual signals that are preprocessed before computation of loudness, pitch, shimmer, jitter, semitone standard deviation and sharpness. The proposed method has been previously validated on data obtained from clinical trials of people with Parkinson disease and healthy controls.	acoustic cryptanalysis;blind signal separation;computation;ecology;pitch (music);smartwatch;source separation;superword level parallelism	Harishchandra Dubey;J. Cody Goldberg;Kunal Mankodiya;Leslie Mahler	2015	2015 17th International Conference on E-health Networking, Application & Services (HealthCom)	10.1109/HealthCom.2015.7454559	loudness;speech recognition;jitter;telecommunications;computer science;pitch	HCI	-5.809451210500505	-84.83597109154042	57970
155abaf778c2d70a394558d9502a2cacb5c31406	subjective speech quality and speech intelligibility evaluation of single-channel dereverberation algorithms	reverberation;acoustic distortion;perceptual validation subjective speech quality speech intelligibility evaluation single channel dereverberation algorithm speech reception threshold itu t p 835 recommendations regularized spectral inverse approach preecho removal;perceptual validation dereverberation speech intelligibility speech quality;speech;speech enhancement;speech acoustic distortion reverberation speech enhancement algorithm design and analysis;speech processing reverberation speech intelligibility;algorithm design and analysis	In this contribution, six different single-channel dereverberation algorithms are evaluated subjectively in terms of speech intelligibility and speech quality. In order to study the influence of the dereverberation algorithms on speech intelligibility, speech reception thresholds in noise were measured for different reverberation times. The quality ratings were obtained following the ITU-T P.835 recommendations (with slight changes for adaptation to the problem of dere-verberation) and included assessment of the attributes: reverberant, colored, distorted, and overall quality. Most of the algorithms improved speech intelligibility for short as well as long reverberation times compared to the reverberant condition. The best performance in terms of speech intelligibility and quality was observed for the regularized spectral inverse approach with pre-echo removal. The overall quality of the processed signals was highly correlated with the attribute reverberant or/and distorted. To generalize the present outcomes, further studies are needed to account for the influence of the estimation errors.	algorithm;echo removal;estimation theory;intelligibility (philosophy)	Anna Warzybok;Ina Kodrasi;Jan Ole Jungmann;Emanuel A. P. Habets;Timo Gerkmann;Alfred Mertins;Simon Doclo;Birger Kollmeier;Stefan Goetze	2014	2014 14th International Workshop on Acoustic Signal Enhancement (IWAENC)	10.1109/IWAENC.2014.6954313	linear predictive coding;speech recognition;acoustics;psqm;communication;intelligibility	Web+IR	-9.485202065017392	-88.23021182425673	57995
0a91663e66d2f92a55d80eb2b53742dd9b5da2ac	simultaneous estimation of vocal tract and voice source parameters based on an arx model	voicing source model;kalman filter;arx model;simulated annealing method		arx;tract (literature)	Wen Ding;Hideki Kasuya;Shuichi Adachi	1995	IEICE Transactions		kalman filter;speech recognition;computer science;machine learning	Vision	-13.962431432936002	-87.49911733080916	58003
06a283c8b6a6e50f119b9dd432105d50e164be31	an algorithm of high resolution and efficient multiple string hypothesization for continuous speech recognition using inter-word models	high resolution;context modeling speech recognition acoustic beams decoding humans laboratories natural languages viterbi algorithm protection;search algorithm;natural languages;continuous speech recognition;model matching;speech recognition;context dependent;search problems;tree searching;high performance;language model;natural languages speech recognition tree searching search problems;string model matching continuous speech recognition inter word models string hypothesization algorithm multiple string hypotheses long term language model scores context dependent subword models context dependent triphone models backward search forward search tree trellis n best search algorithm search efficiency high resolution acoustic models high resolution language models search heuristics;frame synchronization	In this paper, we propose a new accurate string hypothesization algorithm to find the N-best multiple string hypotheses in continuous speech recognition. The algorithm differs from the conventional N-best search algorithms in that it allows the use of the same set of long term language model scores and the detailed context-dependent subword models such as inter-word context dependent triphone models in both forward and backward search for high performance speech recognition. I t is an extension of the tree-trellis N best search algorithm[l]. The inter-word context dependency is exactly preserved in both forward partial path map preparation and the proposed backward N-best multiple string hypothesis tree search. The search efficiency is maximized by applying the same high resolution acoustic and language models in both search directions. When search heuristics are used, the proposed approach provides a more accurate string model matching than that of the conventional frame-synchronous Viterbi beam search decoder.	acoustic cryptanalysis;backward induction;beam search;context-sensitive language;heuristic (computer science);image resolution;language model;search algorithm;speech recognition;substring;tree traversal;trellis quantization;triphone	Wu Chou;Tatsuo Matsuoka;Biing-Hwang Juang;Chin-Hui Lee	1994		10.1109/ICASSP.1994.389696	beam search;speech recognition;image resolution;commentz-walter algorithm;frame synchronization;computer science;machine learning;context-dependent memory;boyer–moore string search algorithm;pattern recognition;best-first search;natural language;language model;string searching algorithm;search algorithm	NLP	-21.57093372960797	-87.3479279841966	58076
49f9afa4d0405019d01b55529ce4167380acc103	articulatory controllable speech modification based on gaussian mixture models with direct waveform modification using spectrum differential		In our previous work, we have developed a speech modification system capable of manipulating unobserved articulatory movements by sequentially performing speech-to-articulatory inversion mapping and articulatory-to-speech production mapping based on a Gaussian mixture model (GMM)-based statistical feature mapping technique. One of the biggest issues to be addressed in this system is quality degradation of the synthetic speech caused by modeling and conversion errors in a vocoderbased waveform generation framework. To address this issue, we propose several implementation methods of direct waveform modification. The proposed methods directly filter an input speech waveform with a time sequence of spectral differential parameters calculated between unmodified and modified spectral envelop parameters in order to avoid using vocoderbased excitation signal generation. The experimental results show that the proposed direct waveform modification methods yield significantly larger quality improvements in the synthetic speech while also keeping a capability of intuitively modifying phoneme sounds by manipulating the unobserved articulatory movements.	elegant degradation;mixture model;synthetic intelligence;time series;waveform	Patrick Lumban Tobing;Kazuhiro Kobayashi;Tomoki Toda;Graham Neubig;Sakriani Sakti;Satoshi Nakamura	2015			pattern recognition;artificial intelligence;mixture model;speech recognition;waveform;computer science	ML	-10.713764173121485	-86.93033206744103	58106
a6ef5ef23aaa1edbd42d66453d2ba53e8b024f08	prominence prediction for supersentential prosodic modeling based on a new database		Most current prosodic modeling techniques are concerned with variation within the sentence. With the improvement of local prosodic variation modeling in techniques like unit selection, we would like to address issues of wider context in producing appropriate synthetic output. A common experience found in unit selection synthesis is that a sentence that sounds natural in isolation does not sound so natural when embedded in a wider context, because it has inappropriate prosody. This work presents the careful design and creation of a speech database designed to capture significant super-sentential prosodic variation. It was designed specifically to allow our own investigations into a notion of “prominence” which we define as a hidden variable that can contribute to surface level prosodic realisation (duration, F0 and power). The background that led up to the construction of this database and our previous attempts to capture prominence are also described.	database;embedded system;hidden variable theory;semantic prosody;synthetic intelligence	Jason Y. Zhang;Arthur R. Toth;Kevyn Collins-Thompson;Alan W. Black	2004			realisation;prosody;database;database design;computer science;sentence;pattern recognition;artificial intelligence	Vision	-13.606814633828805	-83.23514222868414	58112
ebf7a3493d23d09b4506ad786dbc2567498e3bc5	dnn-driven mixture of plda for robust speaker verification		The mismatch between enrollment and test utterances due to different types of variabilities is a great challenge in speaker verification. Based on the observation that the SNR-level variability or channel-type variability causes heterogeneous clusters in i-vector space, this paper proposes to apply supervised learning to drive or guide the learning of probabilistic linear discriminant analysis PLDA mixture models. Specifically, a deep neural network DNN is trained to produce the posterior probabilities of different SNR levels or channel types given i-vectors as input. These posteriors then replace the posterior probabilities of indicator variables in the mixture of PLDA. The discriminative training causes the mixture model to perform more reasonable soft divisions of the i-vector space as compared to the conventional mixture of PLDA. During verification, given a test i-vector and a target-speaker's i-vector, the marginal likelihood for the same-speaker hypothesis is obtained by summing the component likelihoods weighted by the component posteriors produced by the DNN, and likewise for the different-speaker hypothesis. Results based on NIST 2012 SRE demonstrate that the proposed scheme leads to better performance under more realistic situations where both training and test utterances cover a wide range of SNRs and different channel types. Unlike the previous SNR-dependent mixture of PLDA which only focuses on SNR mismatch, the proposed model is more general and is potentially applicable to addressing different types of variability in speech.	artificial neural network;deep learning;discriminative model;heart rate variability;linear discriminant analysis;logistic regression;marginal model;mixture model;signal-to-noise ratio;spatial variability;speaker recognition;statistical classification;supervised learning;support vector machine	Na Li;Man-Wai Mak;Jen-Tzung Chien	2017	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2017.2692304	discriminative model;artificial intelligence;marginal likelihood;mixture model;speech recognition;pattern recognition;supervised learning;artificial neural network;probabilistic logic;computer science;linear discriminant analysis;machine learning;posterior probability	ML	-16.871582127554383	-91.21631540737327	58276
5997047672f826424b09582ba598dd12747034d5	perceptual interference between regional accent and voice/speech disorders		We present a study where we examined the influence of a regional accent in the perception of voice and/or speech disorders. These aspects are most of the time overshadowed in clinical context. This protocol, involving multiple sources of speech variations, is also interesting for perception theories. For the experiment, speakers with or without a Southern French accent and with or without speech/voice disorders were recorded on reading a text. The samples were then randomly played back to two groups of listeners (familiar vs unfamiliar with the regional accent), specialists in speech therapy. The task was the perceptual evaluation of voice quality, articulation disorders and dysprosody. We focused in this paper on the voice dimension. The main results on this part concern the weak influence of regional accent on the perception of moderate or severe dysphonia, where the speech signal is strongly disturbed by the disorder. By contrast, the effect of regional accent is important on normal voices perception: listeners unfamiliar with the regional accent judge speakers with accent without voice disorder as slightly dysphonic. This last result can be interpreted as a form of perceptual interference between different dimensions of speech variations around a central position.	biconnected component;interference (communication);randomness;theory	Alain Ghio;Médéric Gasquet-Cyrus;Juliette Roquel;Antoine Giovanni	2013			speech recognition;perception;sociolinguistics;voice disorder;speech perception;articulation disorders;computer science;voice analysis;dysprosody	HCI	-9.820116607235052	-83.14194335972779	58327
549a2aa355405911bef3d533cc6a63d5dae8fd3f	improving phone recognition performance via phonetically-motivated units		This paper examines how phonetically-motivated units affect the performance of phone recognition systems. Focusing on the realization of /h/, which is one of the most frequently error-making phones in Korean phone recognition, three different phone sets are designed by considering optional phonetic constraints which show complementary distributions. Experimental results show that one of the proposed sets, the hdeletion set improves phone recognition performance compared to the baseline phone recognizer. It is noteworthy that this set needs no additional phonetic unit, which means that no more HMM is necessary to be modeled, accordingly it has the advantage in terms of model size. Besides, it obtains competent performance compared to the baseline system in terms of word recognition as well. Thus, this phoneticallymotivated approach dealing with improvement of phone recognition performance is expected to be used in embedded solutions which require fast and light recognition process.	baseline (configuration management);embedded system;finite-state machine;hidden markov model	Hyejin Hong;Minhwa Chung	2009			speech recognition;phone;computer science	Vision	-19.12753585796474	-86.587613812967	58328
322c071b2c35c23d3f6b34576e498bfb19b378c1	a wavelet-based approach to pattern discovery in melodies	paradigmatic analysis;haar wavelet;pattern discovery;melody	We present a computational method for pattern discovery based on the application of the wavelet transform to symbolic representations of melodies or monophonic voices. We model the importance of a discovered pattern in terms of the compression ratio that can be achieved by using it to describe that part of the melody covered by its occurrences. The proposed method resembles that of paradigmatic analysis developed by Ruwet (1966) and Nattiez (1975). In our approach, melodies are represented either as ‘raw’ 1-dimensional pitch signals or as these signals filtered with the continuous wavelet transform (CWT) at a single scale using the Haar wavelet. These representations are segmented using various approaches and the segments are then concatenated based on their similarity. The concatenated segments are compared, clustered and ranked. The method was evaluated on two musicological tasks: discovering themes and sections in the JKU Patterns Development Database and determining the parent compositions of excerpts from J. S. Bach’s Two-Part Inventions (BWV 772–786). The results indicate that the new approach performs well at finding noticeable and/or important patterns in melodies and that filtering makes the method robust to melodic variation.	algorithm;cognition;complex wavelet transform;computation;concatenation;continuous wavelet;create project;data compression ratio;experiment;haar wavelet;performance;statistical classification;text corpus;theme (computing)	Gissel Velarde;David Meredith;Tillman Weyde	2016		10.1007/978-3-319-25931-4_12	melody;speech recognition;paradigmatic analysis;machine learning;pattern recognition;mathematics;linguistics	ML	-8.048103639307008	-91.59553523504064	58416
06baa5d536d11a52baccbcfef771b542abf6ee44	tone sandhi and tonal coarticulation in fuzhou min		This study examines the completeness of Fuzhou tonal neutralisation and its interaction with tonal coarticulation. In Fuzhou, Tone 44, 242 and 53 are allegedly neutralised into Tone 44 preceding Tone 53. Acoustic data, analysed with Linear Mixed Modelling, show no significant difference in pitch height between neutralised tones, suggesting complete neutralisation in production. A forcedchoice identification test reveals that Fuzhou speakers are unable to distinguish these neutralised tones perceptually. Further acoustic data show how the output of categorical tone sandhi may be modified by tonal coarticulation, with dissimilatory effects superimposed on the sandhi tone.	acoustic cryptanalysis	Yang Li	2015			tone sandhi;coarticulation;speech recognition;linguistics;psychology	HCI	-11.077134143237162	-81.75955032672367	58429
af65cfefc808d63224779ef2e466b1436b2fc4f2	detecting laughter and filler events by time series smoothing with genetic algorithms		Social signal detection, where the aim is to identify vocalizations like laughter and filler events (sounds like “eh”, “er”, etc.) is a popular task in the area of computational paralinguistics, a subfield of speech technology. Recent studies have shown that besides applying state-of-the-art machine learning methods, it is worth making use of the contextual information and adjusting the frame-level scores based on the local neighbourhood. In this study we apply a weighted average time series smoothing filter for laughter and filler event identification, and set the weights using genetic algorithms. Our results indicate that this is a viable way of improving the Area Under the Curve (AUC) scores: our resulting scores are much better than the accuracy of the raw likelihoods produced by both AdaBoost.MH and DNN, and we also significantly outperform standard time series filters as well.	adaboost;computational complexity theory;detection theory;genetic algorithm;machine learning;modified huffman coding;sensor;smoothing;speech technology;star filler;test set;time series	Gábor Gosztolya	2016		10.1007/978-3-319-43958-7_27	econometrics;speech recognition;artificial intelligence	ML	-4.723825859666469	-88.55437160749719	58474
4230249f796faf1fd1c3bbb1d38e22bbbadc3b91	improvements to the pruning behavior of dnn acoustic models		This paper examines two strategies that improve the beam pruning behavior of DNN acoustic models with only a negligible increase in model complexity. By augmenting the boosted MMI loss function used in sequence training with the weighted cross-entropy error, we achieve a real time factor (RTF) reduction of more than 13%. By directly incorporating a transition model into the DNN, which leads to a parameter size increase of less than 0.017%, we achieve a RTF reduction of 16%. Combining both techniques results in a RTF reduction of more than 23%. Both strategies, and their combination, also lead to small but statistically significant word error rate reductions.	acoustic cryptanalysis;acoustic model;cross entropy;error detection and correction;loss function;word error rate	Matthias Paulik	2015			speech recognition;machine learning;pattern recognition	NLP	-18.987125626680594	-89.01851437158778	58551
868245fbb336939b8e786c7c6f3d7c3948062dff	a speaker line-up for the likelihood ratio	likelihood ratio	We propose an analogy to eye witness line-up in order to compute calibrated likelihood ratios for speaker recognition, by including the target model in an identification trial with a cohort of foils. Expressions for the likelihood ratio as a function of cohort size, identification rank and system ROC performance are derived, and some properties of the likelihood ratio function are discussed. The line-up procedure is used as a method to calibrate recognition scores. Using NIST SRE 2010, we find calibration loss comparable to linear calibration (FoCal), while the proposed method gives improved discrimination.	speaker recognition	David A. van Leeuwen;Niko Brümmer	2011			likelihood-ratio test;sociology;maximum likelihood sequence estimation	Vision	-16.437276183066373	-95.37313887390665	58560
d4ac6feda913a260ea4f32f5cfda0f93adc93202	tonal and morphophonological effects on the location of perceptual centers (p-centers): evidence from a bantu language		Abstract Perceptual centers (or ‘p-centers’) correspond to the perceptual moment of occurrence of a syllable or word, and are crucial in the perception of speech rhythm. A metronome alignment task was used to investigate how tone and prenasalization—two elements which affect speech timing and which also interact acoustically—influenced p-center location in Medʉmba, a Grassfields Bantu language. Plain CV words bearing low tones were found to have p-centers which were later (farther from consonant releases and closer to vowel onsets) than those bearing high tones, but the observed effect was not present in prenasalized words. We attribute this difference to the effects of tone depression and slope leveling in prenasalized forms. While prenasalization generally led to earlier p-centers (mirroring effects found for onset clusters in other languages), forms with morphologically-derived prenasal onsets behaved more like plain CV forms, suggesting that nasal prefixes do not contribute to p-center timing. Our findings for derived prenasal sequences parallel similar articulatory findings for languages with simplex onset coordination, where consonant ‘clusters’ actually behave as separate timing units.		Kathryn Franich	2018	J. Phonetics	10.1016/j.wocn.2017.11.001	speech recognition;psychology;metronome;perception;syllable;consonant;vowel;mirroring;bantu languages;prefix	NLP	-10.510923043130854	-81.68629088736375	58600
a4d8a4fe8d644beeb4c10e8c35e71286fcd41600	speaker verification in noise using temporal constraints		This paper addresses the problem of state duration modeling in combination with spectral subtraction and Rasta filtering to cancel both additive and convolutional noise in a text-dependent speaker verification task. The results presented in this paper suggest that temporal constraints can lead to reductions of 30 and 14% in the error rates at SNR equal to 0 and 6dB, respectively, without noise canceling techniques. However, with noise canceling methods, temporal restrictions give a lower improvement. The results here shown propose that state duration modeling can be useful in those cases when the noise reduction is low.	noise reduction;rasta filtering;signal-to-noise ratio;speaker recognition;utility functions on indivisible goods	Néstor Becerra Yoma;Tarciano Facco Pegoraro	2000			speech recognition;artificial intelligence;pattern recognition;computer science	EDA	-13.309229496941102	-91.40483580794529	58614
5703cdb5611e7db3a1dbe5c7e517b002e68f5b0a	double double, morphology and trouble: looking into reduplication in indonesian	conference paper	This paper investigates reduplication in Indonesian. In particular, we focus on verb reduplication that has the agentive voice affix meN, exhibiting a homorganic nasal. We outline the recent changes we have made to the implementation of our Indonesian grammar, and the motivation for such changes. There are two main issues that we deal with in our implementation: how we account for the morphophonemic facts relating to sound changes in the morpheme; and how we construct word formation (i.e. sublexical) rules in creating these derived words exhibiting reduplication.	applicative programming language;lexicon;mathematical morphology;morphological parsing;period-doubling bifurcation;realization (linguistics);wiki	Meladel Mistica;I Wayan Arka;Timothy Baldwin;Avery D. Andrews	2009			humanities;history;linguistics;literature	NLP	-31.60238139791746	-80.467734660113	58658
5b850f1b55eb3c1915aed26b052bd1f7a6ecadfd	neural substrates of phonological selection for japanese character kanji based on fmri investigations	fmri;chinese character;superior temporal gyrus;prefrontal cortex;anterior cingulate cortex;event related functional magnetic resonance imaging;information selection;japanese kanji;language production;phonological processing;homophone judgment	Japanese and Chinese both share the same ideographic/logographic character system. How these characters are processed, however, is inherently different for each language. We harnessed the unique property of homophone judgment in Japanese kanji to provide an analogous Chinese condition using event-related functional magnetic resonance imaging (fMRI) in 33 native Japanese speakers. We compared two types of kanji: (1) kanji that usually evokes only one pronunciation to Japanese speakers, which is representative of most Chinese characters (monophonic character); (2) kanji that evoked multiple pronunciation candidates, which is typical in Japanese kanji (heterophonic character). Results showed that character pairs with multiple sound possibilities increased activation in posterior regions of the left, middle and inferior frontal gyri (MFG and IFG), the bilateral anterior insulae, and the left anterior cingulate cortex as compared with those of kanji with only one sound. The activity seen in the MFG, dorsal IFG, and ventral IFG in the left posterior lateral prefrontal cortex, which was thought to correspond with language components of orthography, phonology, and semantics, respectively, was discussed in regards to their potentially important roles in information selection among competing sources of the components. A comparison with previous studies suggested that detailed analyses of activation in these language areas could explain differences between Japanese and Chinese, such as a greater involvement of the prefrontal language production regions for Japanese, whereas, for Chinese there is more phonological processing of inputs in the superior temporal gyrus.	acclimatization;bilateral filter;cns disorder;cingulate cortex;experience;ifng wt allele;ideogram (document);insula of reil;language disorders;lateral thinking;linguistics;personality character;prefrontal cortex;resonance;simulation;superior temporal gyrus;fmri;phonology	Kayako Matsuo;Shen-Hsing Annabel Chen;Chih-Wei Hue;Chiao-Yi Wu;Epifanio Bagarinao;W Y I Tseng;Toshiharu Nakai	2010	NeuroImage	10.1016/j.neuroimage.2009.12.099	psychology;cognitive psychology;kanji;communication	NLP	-9.029523806097949	-80.23290932192747	58764
ff060d359be8d5aa0722c37f0116f6939ec76034	the efficiency of cross-dialectal word recognition	casual speech;r insertion;word recognition;british english;article in monograph or in proceedings;american english	Dialects of the same language can differ in the casual speech processes they allow; e.g., British English allows the insertion of [r] at word boundaries in sequences such as saw ice, while American English does not. In two speeded word recognition experiments, American listeners heard such British English sequences; in contrast to non-native listeners, they accurately perceived intended vowel-initial words even with intrusive [r]. Thus despite input mismatches, cross-dialectal word recognition benefits from the full power of native-language processing.	experiment	Annelie Tuinman;Holger Mitterer;Anne Cutler	2011			natural language processing;speech recognition;word recognition;computer science;linguistics	AI	-11.327801837404571	-81.62271610303142	58775
22e0a342eb81bd435fbdd2e3aefa7fd6f8689629	new pruning criteria for efficient decoding	search space;speech recognition	In large vocabulary continuous speech recognizers the search space needs to be constrained efficiently to make the recognition task feasible. Beam pruning and restricting the number of active paths are the most widely applied techniques for this. In this paper, we present three additional pruning criteria, which can be used to further limit the search space. These new criteria take into account the state of the search space, which enables tighter pruning. In the speech recognition experiments, the new pruning criteria were shown to reduce the search space up to 50% without affecting the search accuracy. We also present a method for optimizing the threshold parameters of the pruning criteria for the selected level of recognition accuracy. With this method even a large number of different pruning thresholds can be determined with little effort.	experiment;finite-state machine;speech recognition;vocabulary	Janne Pylkkönen	2005			speech recognition;artificial intelligence;pruning;voice activity detection;pattern recognition;decoding methods;computer science	AI	-20.965879171244193	-88.73686449791013	58803
9c7f83de8beb68b7a20a904184f79a4a2fa81fb9	itc-irst at clef 2002: using n-best query translations for clir	hidden markov model	This paper reports on the participation of ITC-irst in the Italian monolingual retrieval track and in the bilingual English-Italian track of the Cross Language Evaluation Forum (CLEF) 2002. A crosslanguage information retrieval systems is proposed which integrates retrieval and translation scores over the set of N-best translations of the source query. Translations are computed by a statistical translation model, based on an hidden Markov model, and trained over a bilingual dictionary and the target document collection. Retrieval scores result as a combination of a statistical language model and a standard Okapi model.	archive;bilingual dictionary;cross-language information retrieval;hidden markov model;language model;markov chain;statistical machine translation	Nicola Bertoldi;Marcello Federico	2002				Web+IR	-22.884975678815568	-82.87207714864877	58810
03ebea67b51d7cb7662a3f66a27ae65d5b213bfe	listening to two simultaneous speeches	analisis escena;analyse scene;error reduction;segregation;modelo markov;reconocimiento palabra;taux erreur;hidden markov model;fuente sonora;segregacion;speech enhancement;simultaneous speakers;automatic speech recognition;automatic recognition;markov model;transfer function;word recognition;speech stream segregation;source sonore;error rate;speech recognition;analyse scene auditive;reconnaissance parole;modele markov;indice error;sound source;multiple;auditory scene analysis;reconocimiento automatico;reconnaissance automatique;scene analysis	Speech stream segregation is presented as a new speech enhancement for automatic speech recognition. Two issues are addressed: speech stream segregation from a mixture of sounds, and interfacing speech stream segregation with automatic speech recognition. Speech stream segregation is modeled as a process of extracting harmonic fragments, grouping these extracted harmonic fragments, and substituting non-harmonic residue for non-harmonic parts of a group. The main problem in interfacing speech stream segregation with hidden Markov model (HMM)-based speech recognition is how to improve the degradation of recognition performance due to spectral distortion of segregated sounds, which is caused mainly by transfer function of a binaural input. Our solution is to re-train the parameters of HMM with training data binauralized for four directions. Experiments with 500 mixtures of two women's utterances of an isolated word showed that the error reduction rate of the 1-best/10-best word recognition of each woman's utterance is, on average, 64% and 75%, respectively. Ó 1999 Elsevier Science B.V. All rights reserved.	binaural beats;computational auditory scene analysis;distortion;elegant degradation;experiment;hidden markov model;hiroshi ishii (computer scientist);markov chain;renegade;speech enhancement;speech recognition;transfer function	Hiroshi G. Okuno;Tomohiro Nakatani;Takeshi Kawabata	1999	Speech Communication	10.1016/S0167-6393(98)00080-6	voice activity detection;audio mining;speech recognition;word recognition;word error rate;computer science;pattern recognition;speech processing;acoustic model;transfer function;auditory scene analysis;markov model;hidden markov model;multiple	NLP	-13.755652324025743	-86.88293738122087	58980
aa9c6d02ee178f35933a05217e42a16e5f8c7bf1	a robust non-parametric and filtering based approach for glottal closure instant detection		In this paper, a novel non-parametric based glottal closure instant (GCI) detection method after filtering the speech signal through a pulse shaping filter is proposed. The pulse shaping filter essentially de-emphasises the vocal tract resonances by emphasising the frequency components containing the pitch information. The filtered signal is subjected to non-linear processing to emphasise the GCI locations. The GCI locations are finally obtained by a non-parametric histograms based approach in the detected voiced regions from the filtered speech signal. The proposed method is compared with the two state-of-theart epoch extraction methods : Zero frequency filtering (ZFF) and SEDREAMS (both of which requires upfront knowledge of average pitch period). The performance of the method is evaluated on the complete CMU-ARCTIC dataset consisting of both speech and Electroglottograph (EGG) signals. The robustness of the proposed method to the additive white noise is evaluated with several degradation levels. The experimental results showed that the proposed method is indeed immune to noise and the obtained results are comparably better than the two state-ofthe-art methods.	electroglottograph;elegant degradation;google code-in;noise shaping;nonlinear system;pulse shaping;tract (literature);utility functions on indivisible goods;white noise	Pradeep Rengaswamy;M. Reddy GurunathReddy;K. Sreenivasa Rao;Pallab Dasgupta	2016		10.21437/Interspeech.2016-369	mathematical optimization	AI	-11.39924406125351	-90.97162773019022	58997
c161d6926b8e8781e72d8018ebcfc1622b39619e	speech recognition in a home environment using parallel decoding with gmm-based noise modeling	decoding;gaussian mixture model home environment parallel decoding gmm based noise modeling noise robust speech recognition multiple signal noise reduction mean vectors confidence score;speech;noise speech recognition speech hidden markov models vectors noise reduction decoding;hidden markov models;vectors;noise reduction;speech recognition;confidence measure speech recognition in noise noise modeling fbank gaussian mixture model;speech recognition decoding gaussian processes mixture models signal denoising;noise	In this paper, we propose a method for noise-robust speech recognition in a home environment based on noise modeling and parallel decoding. There are three basic ideas of the proposed method. First, we model the noise signals observed in the environment using a GMM. Second, we generate multiple noise-reduced signals using the mean vectors of the GMM and decode the signals in parallel. Third, we choose the best recognition result from the multiple recognition results based on the confidence score. The proposed method is very simple and straightforward, yet effective compared with simple noise reduction. The experiments proved that the proposed method is effective for not only noise signals in the database but also for those in the real home environment.	experiment;google map maker;noise reduction;speech recognition;vii	Kohei Machida;Takashi Nose;Akinori Ito	2014	Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific	10.1109/APSIPA.2014.7041622	gaussian noise;speech recognition;computer science;noise measurement;machine learning;pattern recognition	Robotics	-15.933013894757046	-92.65612520551085	59010
31d66e1ef9019ce26475129043a41313477f0215	predictive adaptation and compensation for robust speech recognition		Earlier work in parametric modeling of distortions for robust speech recognition has focussed on estimating the distortion parameter using maximum likelihood and other techniques as a point in the parameter space, and treating this estimate as if it is thetruevalue in a plug-in maximuma posteriori(MAP) decoder. This approach is deficient in most real environments where, due to many reasons, the value of the distortion parameter varies significantly. In this paper we introduce an approach which combines the power of parametric transformation and Bayesian prediction to solve this problem. Instead of approximating the distortion parameter with a point estimate, we average over its variation, thus taking into consideration the distribution of the parameter as well. This approach provides more robust performance than the conventional maximum-likelihood approach. It also provides the solution that minimizes the overall error given the distribution of the parameter. We present results to demonstrate the robustness and effectiveness of the predictive approach.	approximation algorithm;baseline (configuration management);bayesian network;cylinder-head-sector;distortion;mike lesser;plug-in (computing);programming paradigm;race condition;speech recognition;the european library	Arun C. Surendran;Chin-Hui Lee	1998			speech recognition;artificial intelligence;pattern recognition;speaker recognition;computer science	ML	-14.349624935733052	-93.40376239601034	59086
10bda016c82d6fadeba0250e97bed869cce1f5fe	variability compensation in small data: oversampled extraction of i-vectors for the classification of depressed speech	speech recognition emotion recognition sampling methods signal classification;speech accuracy acoustics training standards speech recognition vectors;t distributed stochastic neighbour embedding technique small data oversampled extraction i vectors extraction depressed speech classification acoustic space due speaker mental state speaker identity phonetic content audio visual emotion challenge depression dataset speaker depression supervised variability compensation method linear discriminant analysis within class covariance normalisation speaker variability phonetic variability;t distributed stochastic neighbour embedding depression acoustic variability i vectors linear discriminant analysis within class covariance normalisation	Variations in the acoustic space due to changes in speaker mental state are potentially overshadowed by variability due to speaker identity and phonetic content. Using the Audio/Visual Emotion Challenge and Workshop 2013 Depression Dataset we explore the suitability of i-vectors for reducing these latter sources of variability for distinguishing between low or high levels of speaker depression. In addition we investigate whether supervised variability compensation methods such as Linear Discriminant Analysis (LDA), and Within Class Covariance Normalisation (WCCN), applied in the i-vector domain, could be used to compensate for speaker and phonetic variability. Classification results show that i-vectors formed using an over-sampling methodology outperform a baseline set by KL-means supervectors. However the effect of these two compensation methods does not appear to improve system accuracy. Visualisations afforded by the t-Distributed Stochastic Neighbour Embedding (t-SNE) technique suggest that despite the application of these techniques, speaker variability is still a strong confounding effect.	acoustic cryptanalysis;baseline (configuration management);heart rate variability;linear discriminant analysis;mental state;oversampling;sampling (signal processing);spatial variability;t-distributed stochastic neighbor embedding	Nicholas Cummins;Julien Epps;Vidhyasaharan Sethu;Jarek Krajewski	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6853741	speaker recognition;speaker diarisation;speech recognition;computer science;pattern recognition	Robotics	-14.130957278236085	-89.36688131688116	59100
5b4d84e59b3b61912980337c61185a65988f23e6	vector taylor series based model adaptation using noisy speech trained hidden markov models		Conventionally, in vector Taylor series (VTS) based compensation for noise-robust speech recognition, hidden Markov models (HMMs) are usually trained with clean speech. However, it is known that better performance is generally obtained by training the HMM with noisy speech rather than clean speech. From this viewpoint, we propose a novel VTS-based HMM adaptation method for the noisy speech trained HMM. We derive a mathematical relation between the training and test noisy speech in the cepstrum-domain using VTS and the mean and covariance of the noisy speech trained HMM are adapted to the test noisy speech in an iterative expectation-maximization (EM) algorithm. In the experimental results on the Aurora 2 database, we could obtain about 10–25% relative improvements in word error rates (WERs) over multi-condition training (MTR) method depending on speech front-ends and the HMM complexity.		Yongjoo Chung	2016	Pattern Recognition Letters	10.1016/j.patrec.2016.02.013	speech recognition;computer science;machine learning;pattern recognition	Vision	-18.174001826546643	-91.60814491155016	59186
9c8a16c6351adcc2bc6ef4a113549b00f169e799	morphological random forests for language modeling of inflectional languages	interpolation;decision trees radio frequency natural languages history training data computer science greedy algorithms interpolation speech recognition;decision tree;random forest;speech recognition morphological random forests language modeling inflectional languages decision trees czech lvcsr lecture recognition task trigram language models interpolation;speech recognition;decision trees;speech recognition decision trees interpolation natural language processing;natural language processing;language model	In this paper, we are concerned with using decision trees (DT) and random forests (RF) in language modeling for Czech LVCSR. We show that the RF approach can be successfully implemented for language modeling of an inflectional language. Performance of word-based and morphological DTs and RFs was evaluated on lecture recognition task. We show that while DTs perform worse than conventional trigram language models (LM), RFs of both kind outperform the latter. WER (up to 3.4% relative) and perplexity (10%) reduction over the trigram model can be gained with morphological RFs. Further improvement is obtained after interpolation of DT and RF LMs with the trigram one (up to 15.6% perplexity and 4.8% WER relative reduction). In this paper we also investigate distribution of morphological feature types chosen for splitting data at different levels of DTs.	decision tree;interpolation;language model;perplexity;radio frequency;random forest;speech analytics;trigram;word error rate	Ilya Oparin;Ondrej Glembek;Lukás Burget;Jan Cernocký	2008	2008 IEEE Spoken Language Technology Workshop	10.1109/SLT.2008.4777872	trigram;natural language processing;speech recognition;computer science;machine learning;decision tree;linguistics;language model	NLP	-20.342821920258558	-85.21333971014958	59216
93cda87b12e4869d06792db3cd466e4c48bc9e4a	further intelligibility results from human listening tests using the short-time phase spectrum	analyse parole;nivel ruido;traitement signal;group delay;evaluation performance;speech intelligibility;reconstruccion senal;magnitude spectrum;performance evaluation;retardo grupo;retard groupe;spectrum analysis;analyse spectre;analisis palabra;analisis espectro;institute for integrated and intelligent systems;human speech recognition;evaluacion prestacion;signal analysis;speech processing;speech analysis;faculty of science environment engineering and technology;tratamiento palabra;frequence instantanee;niveau bruit;traitement parole;instantaneous frequency;analisis de senal;280206;spectrum;speech perception;short time fourier transform;verbal perception;intelligibilite parole;etat actuel;overlap add procedure;automatic speech recognition;phase spectrum;automatic recognition;percepcion verbal;noise level;reconocimiento voz;frecuencia instantanea;group delay function;pre2009 speech recognition;fourier transformation;feature extraction;signal processing;state of the art;transformation fourier;instantaneous frequency distribution;speech recognition;estado actual;signal reconstruction;reconnaissance parole;reconstruction signal;extraction caracteristique;spectral analysis;procesamiento senal;analyse signal;reconocimiento automatico;reconnaissance automatique;transformacion fourier;perception verbale	State-of-the-art automatic speech recognition systems (ASRs) use only the short-time magnitude spectrum for feature extraction; the short-time phase spectrum is generally ignored in these systems. Results from our recent human listening tests indicate that the short-time phase spectrum can significantly contribute to speech intelligibility over small window durations (i.e., 20–40 ms). This is an interesting result, indicating the possible usefulness of the short-time phase spectrum for ASR, which commonly employs small window durations of 20–40 ms for spectral analysis. In this paper, we continue our investigation of the short-time phase spectrum. We explore the use of partial short-time phase spectrum information, in the absence of all the short-time magnitude spectrum information, for intelligible signal reconstruction. We create two types of stimuli; one in which its frequency-derivative (i.e., group delay function, GDF) is preserved and another in which its time-derivative (i.e., instantaneous frequency distribution, IFD) is preserved. We do this to determine the contribution that each of these derivatives provides toward intelligibility. Reconstructing stimuli from knowledge of only the GDF or only the IFD results in poor intelligibility. However, when we create stimuli using knowledge of both the GDF and the IFD, reasonable intelligibility is obtained. In light of these results, we conclude that both the GDF and IFD components of the short-time phase spectrum are needed to reconstruct an intelligible signal. In addition, we also perform some experiments to quantify the intelligibility of stimuli reconstructed from the short-time phase and magnitude spectra of noisy speech. The intelligibility of stimuli constructed from either the short-time magnitude spectrum or the short-time phase spectrum degrades at a similar rate under increasing noise levels. The intelligibility of the original signals under noisy conditions also degrades with increased noise, but in all cases the intelligibility is superior to that provided by the stimuli constructed from the separate short-time components. Therefore, we argue that knowledge of both short-time magnitude and phase spectrum information results in superior human speech recognition performance. 2005 Elsevier B.V. All rights reserved.	experiment;feature extraction;geographic data files;group delay and phase delay;i-frame delay;information flow diagram;instantaneous phase;intelligibility (philosophy);signal reconstruction;spectral density;speech recognition	Leigh D. Alsteris;Kuldip K. Paliwal	2006	Speech Communication	10.1016/j.specom.2005.10.005	signal reconstruction;instantaneous phase;fourier transform;spectrum;spectrum analyzer;speech recognition;speech perception;short-time fourier transform;telecommunications;feature extraction;computer science;group delay and phase delay;signal processing;intelligibility	AI	-8.40242549935668	-89.51312393977217	59249
f0068cc593462343ea2abd4c93aa1820244e571e	dependency analysis of read japanese sentences using pause and f0 information: a speaker independent case		This paper deals with the problem of exploiting prosodic information in syntactic analysis of sentences. Duration of pauses at phrase boundaries and relative F0 contour features have been found to be effective for parsing in speaker-dependent case. In this paper, effectiveness of pause and F0 information was examined in a speakerindependent manner by using prosodic features extracted from the spoken version of sentences of 44 speakers. By simplifying the estimation of both pause and F0 contour models, better performance was obtained. Linear combination of pause and F0 information gave significant improvements in parsing accuracy. It was also shown that the effectiveness of pause information was larger when pause models were estimated separately for zero-duration and non-zero-duration pauses.	dependence analysis;parsing	Kazuyuki Takagi;Kazuhiko Ozeki	2004			rule-based system;speech recognition;speaker diarisation;guitar;natural language processing;electric guitar;computer science;speaker recognition;artificial intelligence	NLP	-18.100642696643863	-84.59677579431215	59279
82acb568718fe1d950b2e15eff4bc2482ed4fd1f	a mandarin to taiwanese min nan machine translation system with speech synthesis of taiwanese min nan	chuan jie;判解;speech synthesis;lin;mandarin;tone sandhi;法律詞典;taiwanese;論文;大陸法學;法規;月旦法學;法律題庫;裁判時報;月旦知識庫;法學資料庫;hsin hsi;tssci;min nan;教學;machine translation;chen	This paper presents a design of a Mandarin to Taiwanese Min Nan (abbreviated as Taiwanese hereafter) machine translation system. It is the first machine translation system which focuses on these two languages. An input Mandarin sentence is segmented, tagged and translated word by word according to the part of speech of each word. The candidates come from a Mandarin-Taiwanese dictionary. If more than one candidate exists, an example base is consulted. When a Mandarin word is not found in the Mandarin-Taiwanese dictionary, it is translated according to a SingleCharacter dictionary. The output can be in terms of either speech or text. For speech output, we also deal with the tone sandhi problem in changing the tone of each Taiwanese syllable. Because the mapping between Taiwanese syllables and Chinese characters is still a subject of disagreement, and the phonetic spelli ng coding systems are not familiar to everybody, speech output is useful but is also a challenge.	dictionary;http 404;machine translation;nan;speech synthesis;super robot monkey team hyperforce go!;syllable	Chuan-Jie Lin;Hsin-Hsi Chen	1999	IJCLCLP		speech recognition;computer science;linguistics;communication	NLP	-21.239183075983288	-82.11842354819805	59286
b425b166686e5ffa2926b49c4283852532a3db42	a three-stage approach to the automated scoring of spontaneous spoken responses	model combination;automated scoring;language testing;english speaking proficiency;speech recognition;constructed response scoring;non native speaker	This paper presents a description and evaluation of SpeechRater^S^M, a system for automated scoring of non-native speakers' spoken English proficiency, based on tasks which elicit spontaneous monologues on particular topics. This system builds on much previous work in the automated scoring of test responses, but differs from previous work in that the highly unpredictable nature of the responses to this task type makes the challenge of accurate scoring much more difficult. SpeechRater uses a three-stage architecture. Responses are first processed by a filtering model to ensure that no exceptional conditions exist which might prevent them from being scored by SpeechRater. Responses not filtered out at this stage are then processed by the scoring model to estimate the proficiency rating which a human might assign to them, on the basis of features related to fluency, pronunciation, vocabulary diversity, and grammar. Finally, an aggregation model combines an examinee's scores for multiple items to calculate a total score, as well as an interval in which the examinee's score is predicted to reside with high confidence. SpeechRater's current level of accuracy and construct representation have been deemed sufficient for low-stakes practice exercises, and it has been used in a practice exam for the TOEFL since late 2006. In such a practice environment, it offers a number of advantages compared to human raters, including system load management, and the facilitation of immediate feedback to students. However, it must be acknowledged that SpeechRater presently fails to measure many important aspects of speaking proficiency (such as intonation and appropriateness of topic development), and its agreement with human ratings of proficiency does not yet approach the level of agreement between two human raters.	spontaneous order	Derrick Higgins;Xiaoming Xi;Klaus Zechner;David M. Williamson	2011	Computer Speech & Language	10.1016/j.csl.2010.06.001	natural language processing;speech recognition;language assessment;computer science	NLP	-17.177372539158316	-82.20756966875608	59369
127373deae858ea43f5f621a333f5813f0a883d9	text normalization for the pronunciation of non-standard words in an inflected language	coupling;logro;guidage;speech synthesis;achievement;language generator;numerical method;taux erreur;generateur langage;speech processing;tratamiento palabra;traitement parole;hombre;intelligence artificielle;guiado;couplage;generador lenguaje;greek;griego;texto hacia palabra;metodo numerico;acoplamiento;human;palabra;text to speech;error rate;guidance;artificial intelligence;audition;word;sintesis palabra;audicion;texte a parole;reussite;inteligencia artificial;grec;indice error;hearing;methode numerique;synthese parole;mot;homme	In this paper we present a novel approach, called “Text to Pronunciation (TtP)”, for the proper normalization of Non-Standard Words (NSWs) in unrestricted texts. The methodology deals with inflection issues for the consistency of the NSWs with the syntactic structure of the utterances they belong to. Moreover, for the achievement of an augmented auditory representation of NSWs in Text-to-Speech (TtS) systems, we introduce the coupling of the standard normalizer with: i) a language generator that compiles pronunciation formats and ii) VoiceXML attributes for the guidance of the underlying TtS to imitate the human speaking style in the case of numbers. For the evaluation of the above model in the Greek language we have used a 158K word corpus with 4499 numerical expressions. We achieved an internal error rate of 7,67% however, only 1,02% were perceivable errors due to the nature of the language.	bit error rate;composer;emoticon;microsoft word for mac;numerical analysis;speech synthesis;text normalization;voicexml	Gerasimos Xydas;Georgios Karberis;Georgios Kouroupetroglou	2004		10.1007/978-3-540-24674-9_41	natural language processing;speech recognition;computer science;greek;speech synthesis	NLP	-28.40296230414291	-80.91735544535807	59455
a8ef9a37bb1fbdf8fb2af87c14abb5f8f98f7d0e	linear dynamic segmental hmms: variability representation and training procedure	extra segmental variability;hidden markov model;speech processing;speech analysis;hidden markov models vectors speech analysis cepstral analysis cepstrum trajectory gaussian distribution covariance matrix state estimation data analysis;hmm;acoustic signal processing;training procedure;state estimation;feature vector;speech models;data analysis;linear trajectory;speech feature vector trajectories;cepstral analysis;hidden markov models;trajectory;vectors;mel cepstrum features;cepstrum;feature extraction;intra segmental variability;speech recognition;speech recognition hmm linear dynamic segmental hidden markov models variability representation training procedure speech feature vector trajectories speech models extra segmental variability intra segmental variability mel cepstrum features linear trajectory;linear dynamic segmental hidden markov models;variability representation;gaussian distribution;acoustic signal processing speech recognition speech processing cepstral analysis hidden markov models feature extraction;covariance matrix	This paper describes investigations into the use of linear dynamic segmental hidden Markov models (SHMMs) for modelling speech feature-vector trajectories and their associated variability. These models use linear trajectories to describe how features change over time, and distinguish between extrasegmental variability of different trajectories and intrasegmental variability of individual observations around any one trajectory. Analyses of mel cepstrum features have indicated that a linear trajectory is a reasonable approximation when using models with three states per phone. Good recognition performance has been demonstrated with linear SHMMs. This performance is, however, dependent on the model initialisation and training strategy, and on representing the distributions accurately according to the model assumptions.	approximation;cepstrum;feature vector;hidden markov model;inter-rater reliability;markov chain;spatial variability	Wendy J. Holmes;Martin J. Russell	1997		10.1109/ICASSP.1997.596209	normal distribution;covariance matrix;speech recognition;feature vector;feature extraction;computer science;trajectory;machine learning;cepstrum;pattern recognition;data analysis;hidden markov model	ML	-18.262419793833633	-92.08206268372561	59477
c7ed89b9df54e485ac870f2e690d473fa2f8abe4	speaker adaptation of various components in deep neural network based speech synthesis		In this paper, we investigate the effectiveness of speaker adaptation for various essential components in deep neural network based speech synthesis, including acoustic models, acoustic feature extraction, and post-filters. In general, a speaker adaptation technique, e.g., maximum likelihood linear regression (MLLR) for HMMs or learning hidden unit contributions (LHUC) for DNNs, is applied to an acoustic modeling part to change voice characteristics or speaking styles. However, since we have proposed a multiple DNN-based speech synthesis system, in which several components are represented based on feed-forward DNNs, a speaker adaptation technique can be applied not only to the acoustic modeling part but also to other components represented by DNNs. In experiments using a small amount of adaptation data, we performed adaptation based on LHUC and simple additional fine tuning for DNNbased acoustic models, deep auto-encoder based feature extraction, and DNN-based post-filter models and compared them with HMM-based speech synthesis systems using MLLR.	acoustic cryptanalysis;acoustic model;artificial neural network;autoencoder;deep learning;encoder;experiment;feature extraction;hidden markov model;speech synthesis	Shinji Takaki;SangJin Kim;Junichi Yamagishi	2016		10.21437/SSW.2016-25	speech recognition;acoustics;time delay neural network;communication	AI	-17.46884720187831	-88.3174988261353	59481
e22eaa740553ee3cfb13106ef3bd59e8f449b23e	a system for high quality crowdsourced indigenous language transcription	cultural heritage;transcription;crowdsourcing	In this article, a crowdsourcing method is proposed to transcribe manuscripts from the Bleek and Lloyd Collection, where non-expert volunteers transcribe pages of the handwritten text using an online tool. The digital Bleek and Lloyd Collection is a rare collection that contains artwork, notebooks and dictionaries of the indigenous people of Southern Africa. The notebooks, in particular, contain stories that encode the language, culture and beliefs of these people, handwritten in now-extinct languages with a specialized notation system. Previous attempts have been made to convert the approximately 20,000 pages of text to a machine-readable form using machine learning algorithms but, due to the complexity of the text, the recognition accuracy was low. This article presents details of the system used to enable transcription by volunteers as well as results from experiments that were conducted to determine the quality and consistency of transcriptions. The results show that volunteers are able to produce reliable transcriptions of high quality. The inter-transcriber agreement is 80 % for |Xam text and 95 % for English text. When the |Xam text transcriptions produced by the volunteers are compared with a gold standard, the volunteers achieve an average accuracy of 64.75 %, which exceeded that in previous work. Finally, the degree of transcription agreement correlates with the degree of transcription accuracy. This suggests that the quality of unseen data can be assessed based on the degree of agreement among transcribers.	algorithm;crowdsourcing;dictionary;display resolution;encode;emoticon;experiment;human-readable medium;language model;machine learning;transcriber;transcription (software);wisdom of the crowd	Ngoni Munyaradzi;Hussein Suleman	2014	International Journal on Digital Libraries	10.1007/s00799-014-0112-4	speech recognition;computer science;cultural heritage;transcription;world wide web;crowdsourcing	Web+IR	-24.410795888999715	-80.78337529454677	59535
68b8741fa8cca67528cf086d338a26c9988a0279	full covariance modelling and adaptation in sub-bands	sjcamo continuous speech database full covariance modelling sub bands asr recognition performance complexity reduction speaker adaptation covariance matrix gaussian distributions maximum likelihood linear regression sub band cepstra localised discriminative cues;adaptation model covariance matrix maximum likelihood linear regression discrete cosine transforms gaussian distribution speech recognition hidden markov models filter bank decorrelation cepstral analysis;maximum likelihood estimation;maximum likelihood linear regression;complexity reduction;computational complexity;covariance matrices;speech recognition;experimental evaluation;maximum likelihood estimation covariance matrices computational complexity speech recognition;speaker adaptation;gaussian distribution;covariance matrix	With regard to the current interest in sub-band based modelling in the ASR community, this paper explores the gains in recognition performance and complexity reduction achieved by sub-band based full covariance modelling and speaker adaptation. With sub-band features, instead of a single large covariance matrix, it is now possible to have a set of smaller matrices making it practical to use Gaussian distributions employing full covariance matrices. This benefit is further demonstrated to give a significant complexity reduction in the implementation of speaker adaptation by MLLR. The use of sub-band cepstra moreover presents the opportunity of capturing localised discriminative cues which contribute to increased recognition. In light of these gains, this paper explores the advantages of sub-band full covariance modelling and presents experimental evaluation on the WSJCAMO continuous speech database.	discriminative model;reduction (complexity)	Bernard Doherty;Saeed Vaseghi;Paul M. McCourt	2000		10.1109/ICASSP.2000.859123	normal distribution;matérn covariance function;estimation of covariance matrices;covariance intersection;covariance matrix;speech recognition;cma-es;computer science;pattern recognition;mathematics;maximum likelihood;computational complexity theory;reduction;rational quadratic covariance function;statistics;covariance function	AI	-16.19051281912286	-92.90753766574436	59634
eb974ef9dbea7b40288e387c34b9615b932faab2	acoustics and perception of velar softening for unaspirated stops		This paper provides articulatory, acoustic and perceptual data in support of the hypothesis that the velar softening process through which /k/ becomes /tP/ is based on articulation rather than on acoustic equivalence if operating on unaspirated stops. Production data for unaspirated /k/ are analyzed for five speakers of Majorcan Catalan, where the velar stop phoneme exhibits (alveolo)palatal or velar allophones depending on vowel context and position. Data on several parameters, i.e., contact anteriority and dorsopalatal contact degree, burst spectral peak frequency, energy and duration, and F2 and F3 vowel transition endpoints and ranges, suggest that /tP/ may have originated from (alveolo)palatal stop realizations not only before front vocalic segments but also before low and central vowels and word finally. Perception results are consistent with the production data in indicating that the most significant /tP/ perception cues are burst energy before /], u/ and burst duration word/utterance finally. They also suggest that velar softening for unaspirated /k/ before front vocalic segments is triggered by an increase in burst frication energy and duration resulting from the narrowing of an (alveolo)palatal central channel occurring at stop closure release. These findings are in agreement with the existence of contextual and positional (alveolo)palatal stop allophones of /k/, and with evidence from sound change, in the Romance languages. r 2009 Elsevier Ltd. All rights reserved.	acoustic cryptanalysis;biconnected component;burst error;softening;turing completeness	Daniel Recasens;Aina Espinosa	2009	J. Phonetics	10.1016/j.wocn.2009.01.001	speech recognition;acoustics;communication	AI	-10.408671243836372	-81.8347255358795	59674
b7ca3eec17138c7ece8b3e75af9153ddfd7eb95f	on the application of associative method to chinese connected digit recognition				Wang Jiu-Long;Yuan Xue-Gong;Zhao Guo-Tian	1987			speech recognition;artificial intelligence;associative property;pattern recognition;computer science;numerical digit	AI	-15.41156834251034	-86.74586471983389	59745
fc4e43c718c7e3be8a993341d553c56d9b449cd3	environmental sound classification based on feature collaboration	chirp;fourier transform;support vector machines;discrete curvelet transform;acoustics;acoustic signal processing;discrete hilbert transform;wavelet transforms;hilbert transform;accuracy;wavelet transforms acoustic signal processing feature extraction fourier transforms;sound classification;feature collaboration;collaboration discrete transforms signal analysis chirp frequency domain analysis speech recognition feature extraction hidden markov models principal component analysis independent component analysis;wavelet transform;feature extraction;environmental sound recognition;fourier transforms;transforms;feature extraction environmental sound recognition discrete chirplet transform discrete curvelet transform discrete hilbert transform;environmental sound classification;acoustic signal analysis environmental sound classification feature collaboration mpeg 7 fourier transform wavelet transform discrete chirplet transform discrete curvelet transform feature extraction;acoustic signal analysis;time frequency analysis;discrete chirplet transform;mpeg 7	To date, common acoustic features such as MPEG-7 and Fourier/wavelet transform-based features have been frequently used for environmental sound classification. However, these transforms have difficulty dealing with specific properties of environmental sounds, due to their limited scopes. In this paper, we investigate three types of transforms as yet untried for this purpose, and show that they are more effective than traditional features. This result is mainly due to the fact that they have functionalities that were not easily treatable with traditional transforms. Experimental results show that the combination of these features with traditional features can achieve 86.09% of the maximum accuracy in environmental sound classification, compared to 74.35% of the maximum accuracy when confined to traditional features.	acoustic cryptanalysis;mpeg-7;texture mapping;variable rules analysis;wavelet transform	Byeong-jun Han;Eenjun Hwang	2009	2009 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2009.5202553	fourier transform;computer vision;speech recognition;pattern recognition;mathematics;wavelet transform	Robotics	-8.371073319266861	-91.55809411165764	59816
4ae2662f287b69636e3bd626d30edcb3401979ab	boosting multi-modal camera selection with semantic features	teleconferencing;human computer interaction;group action;image segmentation;video signal processing;hidden markov model;video conferencing multimodal video camera selection boosting semantic feature extraction group action person action person speaking acoustic feature extraction visual feature extraction image feature fusion image segmentation image classification hidden markov model frame error rate machine learning human machine interaction meeting analysis automatic speech recognition;image fusion;image classification;acoustic signal processing;video signal processing acoustic signal processing error statistics feature extraction hidden markov models human computer interaction image classification image fusion image segmentation learning artificial intelligence speech recognition teleconferencing video cameras;indexing terms;multi cameras;person speaking;mel frequency cepstral coefficient;error analysis;automatic speech recognition;visualization;hidden markov models;multi modal low level features;machine learning;visual feature extraction;video conferencing;semantic feature extraction;video cameras;feature extraction;meeting analysis;visual features;multi modal low level features machine learning human machine interaction multi cameras meeting analysis;speech recognition;error statistics;feature fusion;boosting hidden markov models videoconference smart cameras error analysis streaming media minutes microphone arrays mel frequency cepstral coefficient image sequences;learning artificial intelligence;acoustic feature extraction;multimodal video camera selection boosting;frame error rate;image feature fusion;person action;cameras;human machine interaction	In this work semantic features are used to improve the results of the camera selection. These semantic features are group action, person action and person speaking. For this purpose low level acoustic and visual features are combined with high level semantic ones. After the feature fusion, a segmentation and classification are performed by Hidden Markov Models. The evaluation shows that an absolute improvement of 6.5% can be achieved. The frame error rate is reduced to 38.1% by using acoustic and all semantic features. The best model using only low level features achieves a frame error rate of 44.6%, which is the best one reported on this data set.	acoustic cryptanalysis;boosting (machine learning);hidden markov model;high-level programming language;markov chain;modal logic	Benedikt Hörnler;Dejan Arsic;Björn W. Schuller;Gerhard Rigoll	2009	2009 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2009.5202740	computer vision;contextual image classification;speech recognition;teleconference;visualization;index term;feature extraction;computer science;machine learning;group action;pattern recognition;image segmentation;videoconferencing;image fusion;hidden markov model	Vision	-15.19287864840025	-93.89442971260075	59878
152ec18e9ed460900ea166deb601b1bd2ccdd1f0	signing avatars: making education more inclusive		In Brazil, there are approximately 9.7 million inhabitants who are deaf or hard of hearing. Moreover, about 30% of the Brazilian deaf community is illiterate in Brazilian Portuguese due to difficulties to offer deaf children an inclusive environment based on bilingual education. Currently, the prevailing teaching practice depends heavily on verbal language and on written material, making the inclusion of the deaf a challenging task. This paper presents the author’s approach for tackling this problem and improving deaf students’ accessibility to written material in order to help them master Brazilian Portuguese as a second language. We describe an ongoing project aimed at developing an automatic Brazilian Portuguese-to-Libras translation system that presents the translated content via an animated virtual human, or avatar. The paper describes the methodology adopted to compile a source language corpus having the deaf student needs in central focus. It also describes the construction of a parallel Brazilian Portuguese/Brazilian Sign Language (Libras) corpus based on motion capture technology. The envisioned translation architecture includes the definition of an Intermediate Language to drive the signing avatar. The results of a preliminary assessment of signs intelligibility highlight the application potential.	accessibility;avatar (computing);code signing;compiler;intelligibility (philosophy);machine translation;motion capture;text corpus;virtual actor	José Mario De Martino;Ivani Rodrigues Silva;Carmen Zink Bolognini;Paula Dornhofer Paro Costa;Kate Mamhy Oliveira Kumada;Luis Cláudius Coradine;Patrick Henrique da S. Brito;Wanessa Machado do Amaral;Ângelo Brandão Benetti;Enzo Telles Poeta;Leandro Martin Guertzenstein Angare	2016	Universal Access in the Information Society	10.1007/s10209-016-0504-x	language interpretation	NLP	-28.324044172935217	-83.86055350170938	59958
9ce3805f661bf84476837f9a5e0d293ccbe1c437	a fuzzy logic approach to improve phone segmentation - a case study of the dutch language		Phone segmentation is an essential task for Automatic Speech Recognition (ASR) systems, which still lack in performance when compared to the ability of humans’ speech recognition. In this paper, we propose novel Fuzzy Logic (FL) based approaches for the prediction of phone durations using linguistic features. To the best of our knowledge, this is the first development and deployment of FL based approaches in the area of phone segmentation. In this study, we perform a case study on the Dutch IFA corpus, which consists of 50000 words. Different experiments are conducted on tuned FL Systems (FLSs) and Neural Networks (NNs). The experimental results show that FLSs are more efficient in phone duration prediction in comparison to their Neural Network counterparts. Furthermore, we observe that differentiating between the vowels and the consonants improves the performance of predictions, which can facilitate enhanced ASR systems. The FLS with the differentiation between vowels and consonants had an average Mean Average Precision Error of 43.3396% on a k=3 fold. We believe that this first attempt of the employment of FL based approaches will be an important step for a wider deployment of FL in the area of ASR systems.	automated system recovery;automatic system recovery;baseline (configuration management);experiment;free library of springfield township;fuzzy logic;information retrieval;mean squared error;neural networks;software deployment;speech recognition;timit	Victor Milewski;Aysenur Bilgin;Tufan Kumbasar	2017		10.5220/0006499800640072	phone;fuzzy logic;natural language processing;machine learning;segmentation;artificial intelligence;computer science	NLP	-18.339349385289587	-86.11497041966011	60003
3caa49d93d30805e099db39f283f6238e244df80	use of recurrent network for unknown language rejection in language identification system	language identification	In the past, we attempted to use a multilayer perceptron neural network as a means to prevent those unknown language inputs from being misidentified as one of the target languages in language identification system. However, the use of multilayer perceptron neural network could not utilize the temporal information from the utterances. Results show that with the use of phonemic unigram as input features to a recurrent neural network of Jordan architecture, a 3 target language identification rate of 98.1% can be achieved. By setting the output thresholds to 0.6 to reject 2 more unknown languages, a lower overall rate of 85.9% is obtained.	artificial neural network;compiler;language identification;multilayer perceptron;n-gram;recurrent neural network;rejection sampling	HingKeung Kwan;Keikichi Hirose	1997			artificial intelligence;speech recognition;architecture;language identification;pattern recognition;computer science;artificial neural network;recurrent neural network;multilayer perceptron	NLP	-18.60551277848554	-87.09556329881335	60104
4a6fb01448cd6ea46414661a35c23044ad87f715	a 75-year-old hungarian spontaneous speech database		The first attempt to develop a large collection of recorded speech material in Hungarian was made by the phonetician Lajos Hegedűs in the 1940s. He wanted to preserve the sounding of the various Hungarian dialects in the country and even outside Hungary with the purpose of analyzing, among other things, the intonation, pauses and rhythm of speech in those dialects. This paper introduces the properties of the Hegedűs Archives on the one hand, and discusses the results of investigations that aimed to compare the speaking strategies of people that could be the great-grandparents of the speakers living now, on the other hand.	automatic sounding;spontaneous order	Mária Gósy	2015			speech recognition;computer science	ML	-24.284572218026106	-84.35593562997401	60157
98fa4809d0c9a7147a19f4b9fdc60fcdab7043f8	fast i-vector denoising using map estimation and a noise distributions database for robust speaker recognition		We use a normal distribution model for both clean and noisy i-vectors.We use an additive model of the noise in the i-vector space.We use a MAP estimator to clean-up noisy i-vectors based on both clean i-vectors and noise distributions in the i-vector space. Once the i-vector paradigm has been introduced in the field of speaker recognition, many techniques have been proposed to deal with additive noise within this framework. Due to the complexity of its effect in the i-vector space, a lot of effort has been put into dealing with noise in other domains (speech enhancement, feature compensation, robust i-vector extraction and robust scoring). As far as we know, there was no serious attempt to handle the noise problem directly in the i-vector space without relying on data distributions computed on a prior domain. The aim of this paper is twofold. First, it proposes a full-covariance Gaussian modeling of the clean i-vectors and noise distribution in the i-vector space and introduces a technique to estimate a clean i-vector given the noisy version and the noise density function using the MAP approach. Based on NIST data, we show that it is possible to improve by up to 60% the baseline system performance. Second, in order to make this algorithm usable in a real application and reduce the computational time needed by i-MAP, we propose an extension that requires building a noise distribution database in the i-vector space in an off-line step and using it later in the test phase. We show that it is possible to achieve comparable results using this approach (up to 57% of relative EER improvement) with a sufficiently large noise distribution database.	noise reduction;speaker recognition	Waad Ben Kheder;Driss Matrouf;Pierre-Michel Bousquet;Jean-François Bonastre;Moez Ajili	2017	Computer Speech & Language	10.1016/j.csl.2016.12.007	computer science;value noise;estimator;machine learning;speech recognition;database;noise reduction;speaker recognition;gaussian noise;noise measurement;gradient noise;noise spectral density;pattern recognition;artificial intelligence	ML	-15.680400712744982	-92.30489885903151	60211
8943f36be3e2665cb6fa7a0ee676a1eb853137f6	phonotactic and lexical constraints in speech recognition	speech recognition	We demonstrate a method for partitioning a large lexicon into small equivalence classes, based on sequential phonetic and prosodic constraints. The representation is attractive for speech recognition systems because it allows all but a small number of word candidates to be excluded, using only gross phonetic and prosodic information. The approach is a robust one in that the representation is relatively insensitive to phonetic variability and recognition error.	lexicon;spatial variability;speech recognition;turing completeness	Daniel P. Huttenlocher;Victor W. Sue	1983			natural language processing;phonetic form;speech recognition;computer science;phonetic search technology	NLP	-19.588584079346305	-84.76431461227406	60290
ab17712fbfe501b36bd62979c87104dffbaee72a	learning deep wavelet networks for recognition system of arabic words		In this paper, we propose a new method of learning for speech signal. This technique is based on the deep learning and the wavelet network theories. The goal of our approach is to construct a deep wavelet network (DWN) using a series of Stacked Wavelet Auto-Encoders. The DWN is devoted to the classification of one class compared to other classes of the dataset. The Mel-Frequency Cepstral Coefficients (MFCC) is chosen to select speech features. Finally, the experimental test is performed on a prepared corpus of Arabic words.	wavelet	Amira Bouallégue;Salima Hassairi;Ridha Ejbali;Mourad Zaied	2016		10.1007/978-3-319-47364-2_48	natural language processing;speech recognition;pattern recognition	Vision	-15.516871881365674	-87.53648937806233	60352
2f412125b71c6b0929e45024def0567b2abf6009	a new framework for supervised speech enhancement in the time domain		This work proposes a new learning framework that uses a loss function in the frequency domain to train a convolutional neural network (CNN) in the time domain. At the training time, an extra operation is added after the speech enhancement network to convert the estimated signal in the time domain to the frequency domain. This operation is differentiable and is used to train the system with a loss in the frequency domain. This proposed approach replaces learning in the frequency domain, i.e., short-time Fourier transform (STFT) magnitude estimation, with learning in the original time domain. The proposed method is a spectral mapping approach in which the CNN first generates a time domain signal then computes its STFT that is used for spectral mapping. This way the CNN can exploit the additional domain knowledge about calculating the STFT magnitude from the time domain signal. Experimental results demonstrate that the proposed method substantially outperforms the other methods of speech enhancement. The proposed approach is easy to implement and applicable to related speech processing tasks that require spectral mapping or time-frequency (T-F) masking.	artificial neural network;convolutional neural network;loss function;short-time fourier transform;speech enhancement;speech processing	Ashutosh Pandey;DeLiang Wang	2018		10.21437/Interspeech.2018-1223	speech recognition;speech enhancement;artificial intelligence;time domain;pattern recognition;computer science	Vision	-15.790049483367618	-90.33061499499436	60362
a705938a8c167aebb446c7751f7026c671fbd4ed	"""erratum: """"an optical music recognition system for skew or inverted musical scores"""""""				Yung-Sheng Chen;Feng-Sheng Chen;Chin-Hung Teng	2014	IJPRAI	10.1142/S0218001414920025	speech recognition	Vision	-13.64374723993431	-85.87243419833656	60367
2ec4f84ff791d173756a061f9251632757621a0c	syllabification by phone categorization		Syllables play an important role in speech synthesis, speech recognition, and spoken document retrieval. A novel, low cost, and language agnostic approach to dividing words into their corresponding syllables is presented. A hybrid genetic algorithm constructs a categorization of phones optimized for syllabification. This categorization is used on top of a hidden Markov model sequence classifier to find syllable boundaries. The technique shows promising preliminary results when trained and tested on English words.	categorization;document retrieval;genetic algorithm;hidden markov model;language-independent specification;markov chain;memetic algorithm;speech recognition;speech synthesis;syllable	Jacob Krantz;Maxwell Dulin;Paul De Palma;Mark VanDam	2018		10.1145/3205651.3208781	machine learning;computer science;artificial intelligence;hidden markov model;phone;syllable;computational linguistics;categorization;document retrieval;pattern recognition;syllabification;speech synthesis	AI	-18.91980283210572	-85.61122047131377	60368
d6d73ec243c0361df92af40bdd1e033dfb416e4a	discriminative bernoulli mixture models for handwritten digit recognition	log linear models bernoulli mixture discriminative training mmi mixture of multi class logistic regression;mixture of multi class logistic regression;handwriting recognition;training;text analysis;joints;maximum likelihood estimation;hidden markov models training maximum likelihood estimation data models logistics joints handwriting recognition;mmi;log linear models;hidden markov models;logistics;maximum likelihood decoding;discriminative training;bernoulli mixture;regression analysis;text analysis entropy handwritten character recognition learning artificial intelligence maximum likelihood decoding maximum likelihood estimation natural language processing regression analysis;entropy;learning artificial intelligence;capitulo de libro;natural language processing;handwritten character recognition;indian digit recognition task discriminative bernoulli mixture models handwritten digit recognition bernoulli hmms handwritten text recognition character recognition continuous handwritten words isolated handwritten words maximum likelihood estimation maximum mutual information log linear model maximum entropy model bernoulli mixture classifier binary data multiclass logistic regression discriminative training framework;data models	Bernoulli-based models such as Bernoulli mixtures or Bernoulli HMMs (BHMMs), have been successfully applied to several handwritten text recognition (HTR) tasks which range from character recognition to continuous and isolated handwritten words. All these models belong to the generative model family and, hence, are usually trained by (joint) maximum likelihood estimation (MLE). Despite the good properties of the MLE criterion, there are better training criteria such as maximum mutual information (MMI). The MMI is a widespread criterion that is mainly employed to train discriminative models such as log-linear (or maximum entropy) models. Inspired by the Bernoulli mixture classifier, in this work a log-linear model for binary data is proposed, the so-called mixture of multi-class logistic regression. The proposed model is proved to be equivalent to the Bernoulli mixture classifier. In this way, we give a discriminative training framework for Bernoulli mixture models. The proposed discriminative training framework is applied to a well-known Indian digit recognition task.	bernoulli polynomials;bernoulli scheme;binary data;discriminative model;euler–bernoulli beam theory;generative model;linear model;log-linear model;logistic regression;mixture model;mutual information;optical character recognition;principle of maximum entropy;turing completeness	Adrià Giménez;Jesús Andrés-Ferrer;Alfons Juan-Císcar;Nicolás Serrano	2011	2011 International Conference on Document Analysis and Recognition	10.1109/ICDAR.2011.118	logistics;data modeling;entropy;speech recognition;computer science;machine learning;pattern recognition;maximum likelihood;handwriting recognition;regression analysis;log-linear model	Vision	-19.851889320317284	-91.65215454819526	60388
8c253d419ebba1c7ff2b86890d7f874763a55600	context-driven automatic bilingual movie subtitle alignment		Movie subtitle alignment is a potentially useful approach for deriving automatically parallel bilingual/multilingual spoken language data for automatic speech translation. In this paper, we consider the movie subtitle alignment task. We propose a distance metric between utterances of different languages based on lexical features derived from bilingual dictionaries. We use the dynamic time warping algorithm to obtain the best alignment. The best F-score of ∼0.713 is obtained using the proposed approach.	algorithm;bilingual dictionary;dynamic time warping;f1 score	Andreas Tsiartas;Prasanta Kumar Ghosh;Panayiotis G. Georgiou;Shrikanth (Shri) Narayanan	2009			natural language processing;speech recognition;computer science	NLP	-21.870419851755983	-83.00428390487416	60390
4d9b72050aee74bbe41a1c94f86b402f3ead97bf	on the modeling of natural vocal emotion expressions through binary key	acoustic signal processing;emotion recognition;regression analysis;speech recognition;support vector machines;german tv talk-show;vam corpus;acoustic descriptors;binary acoustic modeling;binary key modeling;binary value representation;correlation coefficient;mean absolute error;natural vocal emotion expression modelling;speaker emotion characteristics;spectral parameters;speech features;spontaneous dialogues;standard acoustic feature mapping;support vector regression model;three-continuous emotional dimensions;emotion modeling;vam corpus;binary fingerprint;dimensional emotions	This work presents a novel method to estimate natural expressed emotions in speech through binary acoustic modeling. Standard acoustic features are mapped to a binary value representation and a support vector regression model is used to correlate them with the three-continuous emotional dimensions. Three different sets of speech features, two based on spectral parameters and one on prosody are compared on the VAM corpus, a set of spontaneous dialogues from a German TV talk-show. The regression analysis, in terms of correlation coefficient and mean absolute error, show that the binary key modeling is able to successfully capture speaker emotion characteristics. The proposed algorithm obtains comparable results to those reported on the literature while it relies on a much smaller set of acoustic descriptors. Furthermore, we also report on preliminary results based on the combination of the binary models, which brings further performance improvements.	acoustic coupler;acoustic cryptanalysis;acoustic model;algorithm;approximation error;binary number;experiment;fingerprint;ground truth;matthews correlation coefficient;regular expression;semantic prosody;software regression;speech corpus;spontaneous order;support vector machine	Jordi Luque;Xavier Anguera Miró	2014	2014 22nd European Signal Processing Conference (EUSIPCO)		psychology;natural language processing;speech recognition;communication	NLP	-12.51009631649907	-87.05726676623506	60429
37776ad18deea7ec5a51b6e33cd11212cf25dd39	grammatical and ungrammatical structures in user-adviser dialogues= evidence for sufficiency of restricted languages in natural language interfaces to advisory systems		User-adviser dialogues were collected in a typed Wizardof-Oz study (=man-behind-the-curtain study*). Thirty-two users had to solve simple statistics problems using an unfamiliar statistical package. Users received help on how to use the statistical package by typing utterances to what they believed was a computerized adviser. The observed limited set of users' grammatical and ungrammatical forms demonstrates the sufficiency of a very restricted grammar of English for a natural language interface to an advisory sys* tem. The users' language shares many features of spoken face-to-face language or of language generated under realtime production constraints (i.e., very simple forms of utterances). Yet, users also appeared to believe that the natural language interface could not handle fragmentary or informal language and users planned or edited their language to be more like formal written language (i.e., very infrequent fragments and phatics). Finally, users also appeared to believe in poor shared context between users and computerized advisers and referred to objects and events using complex nominals instead of faster-to-type pronouns. I N T R O D U C T I O N It has been azgued that natural language interfaces with very rich functionality are crucial to the effective use of advisory systems and that interfaces using formal languages, menus, or direct manipulation will not suffice (Finin, Joshi, and Webber, 1986). Designing, developing, and debugging a rich natural language interface (its parser, grammar, recovery strategies from unparsable input, etc.) are timeconsuming and labor-intensive. Nevertheless, natural language interfaces can be quite brittle in the face of uncon* strained input from the user, as can be found in applications such as user-advising. One step toward a solution to these problems would be the identification of a subset of gram* matical and ungrammatical structures that correspond to the language generated by users in any user-advising situations, irrespective of the domain. This subset could be used to design a core grammar, strategies to handle ungrammatical input, and some parsing heuristics portable to any natural language interface to advisory systems. This strategy would increase the habitability of the natural language interface (Watt, 1968; Trawick, 1983) and reduce its development	context-free grammar;debugging;direct manipulation interface;formal language;heuristic (computer science);list of statistical packages;natural language user interface;parsing	Raymonde Guindon;Kelly Shuldberg;Joyce Conner	1987			natural language processing;language identification;limit set;natural language programming;universal networking language;object language;natural language user interface;computer science;linguistics;natural language;programming language specification	NLP	-26.39657246055794	-86.12387569330305	60485
2eec53105850ad5e58e1bdcd97c439bd78036557	resource production of written forms of sign languages by a user-centered editor, swift (signwriting improved fast transcriber)		The SignWriting improved fast transcriber (SWift), presented in this paper, is an advanced editor for computer-aided writing and transcribing of any Sign Language (SL) using the SignWriting (SW). The application is an editor which allows composing and saving desired signs using the SW elementary components, called “glyphs”. These make up a sort of alphabet, which does not depend on the national Sign Language and which codes the basic components of any sign. The user is guided through a fully automated procedure making the composition process fast and intuitive. SWift pursues the goal of helping to break down the “electronic” barriers that keep deaf people away from the web, and at the same time to support linguistic research about Sign Languages features. For this reason it has been designed with a special attention to deaf user needs, and to general usability issues. The editor has been developed in a modular way, so it can be integrated everywhere the use of the SW as an alternative to written “verbal” language may be advisable. Keyword: Sign Languages, SignWriting, Accessible editor	code;glyph;sl (complexity);shattered world;transcriber;usability;user-centered design	Fabrizio Borgia;Claudia S. Bianchini;Patrice Dalle;Maria De Marsico	2012			speech recognition;computer science;artificial intelligence;algorithm	HCI	-28.62370958029321	-83.90690317644587	60561
150626c981641515cd97fbe80eb6a4f696dc8c7f	polish rhythmic database ― new resources for speech timing and rhythm analysis		This paper reports on a new database – Polish rhythmic database and tools developed with the aim of investigating timing phenomena and rhythmic structure in Polish including topics such as, inter alia, the effect of speaking style and tempo on timing patterns, phonotactic and phrasal properties of speech rhythm and stability of rhythm metrics. So far, 19 native and 12 non-native speakers with different first languages have been recorded. The collected speech data (5 h 14 min.) represents five different speaking styles and five different tempi. For the needs of speech corpus management, annotation and analysis, a database was developed and integrated with Annotation Pro (Klessa et al., 2013, Klessa, 2016). Currently, the database is the only resource for Polish which allows for a systematic study of a broad range of phenomena related to speech timing and rhythm. The paper also introduces new tools and methods developed to facilitate the database annotation and analysis with respect to various timing and rhythm measures. In the end, the results of an ongoing research and first experimental results using the new resources are reported and future work is sketched.	maxima and minima;speech corpus	Agnieszka Wagner;Katarzyna Klessa;Jolanta Bachan	2016			natural language processing;artificial intelligence;speech recognition;rhythm;computer science	NLP	-12.650739029575252	-81.08582423492034	60580
0be892e959540d4aaf464f3059cd2a649d9213d7	speaker verification on the polycost database using frequency filtered spectral energies	conference report	The spectral parameters that result from filtering the frequency sequence of log mel-scaled filter-bank energies with a first or second order FIR filter have proved to be competitive for speech recognition. Recently, the authors have shown that this frequency filtering can approximately equalize the cepstrum variance enhancing the oscillations of the spectral envelope curve that are most effective for discrimination between speakers. Even better speaker identification results than using mel-cepstrum were observed on the TIMIT database, especially when white noise was added. In this paper, the hybridization of both linear prediction and filter-bank spectral analysis using either cepstral transformation or the alternative frequency filtering is explored for speaker verification. This combination, that had shown to be able to outperform the conventional techniques in clean and noisy word recognition, has yield good text-dependent speaker verification results on the new speakeroriented telephone-line POLYCOST database.	cepstrum;filter bank;finite impulse response;speaker recognition;speech recognition;timit;telephone line;white noise	Javier Hernando;Climent Nadeu	1998			speech recognition;computer science;pattern recognition;statistics	DB	-12.513060336840827	-91.43382796377108	60708
876a8720bfb824a118e781e9317b9848fb291145	automatic detection of breathy voiced vowels in gujarati speech	loudness measure;vowel onset point;breathy voiced vowel;epoch;modal voiced vowel	This paper proposes a method for automatic detection of breathy voiced vowels in continuous Gujarati speech. As breathy voice is a specific phonetic feature predominantly present in Gujarati among Indian languages, it can be used for identifying Gujarati language. The objective of this paper is to differentiate breathy voiced vowels from modal voiced vowels based on loudness measure. Excitation source characteristics represented by loudness measure are used for differentiating the voice quality. In the proposed method, initially vowel regions in continuous speech are determined by using the knowledge of vowel onset points and epochs. Later, hypothesized vowel segments are classified by using loudness measure. Performance of the proposed method is evaluated on Gujarati speech utterances containing around 47 breathy and 192 modal vowels spoken by 5 male and 5 female speakers. Classification of vowels into breathy or modal voice is achieved with an accuracy of around 94 %.		Anil Kumar Vuppala;Peri Bhaskararao	2014	I. J. Speech Technology	10.1007/s10772-013-9207-3	speech recognition;mid vowel;series	NLP	-10.67253096149211	-82.64298783032655	60750
3884386ad02f949c1b632a01713e130e16cf3f69	a discriminative recognizer for isolated and continuous speech using statistical separability measures			discriminative model;finite-state machine;linear separability	Klaus Zünkler	1991			artificial intelligence;speech recognition;discriminative model;pattern recognition;computer science	NLP	-14.819863896725428	-87.95805884805665	60754
3e92485dbb208b4b14c246c4c70d6d23bf6744a7	automatic spoken customer query identification for arabic language	automatic call routing;automated dialogue system;speech recognition;palestinian arabic dialect;arabic asr	In this paper we propose an approach that aims to build an automated task-oriented Arabic dialogue system which is capable to determine the topic of spoken question asked by telecom provider customers. The system is based on an Arabic adapted CMU sphinx ASR. In addition to formal Arabic speech, our implemented Arabic ASR is capable to recognize some Palestinian Arabic dialectal words. The recognized text is used to determine the question category using supervised machine learning techniques in order to take desired action such as routing customer call to the appropriate destination. The best performance of proposed overall system is 76.4% accuracy with random forest classifier provided by Weka toolkit tested on 750 questions recorded by 30 speakers with Palestinian dialect.	dialog system;machine learning;random forest;routing;sphinx4;supervised learning;weka	Aziz Qaroush;Abualsoud Hanani;Bassam Jaber;Mohammed Karmi;Bashar Qamhiyeh	2016		10.1145/3012258.3012261	natural language processing;speech recognition;engineering;linguistics	NLP	-22.519240677978075	-85.37552773563066	60792
480dcddbb85a5aeb109290b52df8de0edfc2f2ac	hybrid hmm/ann systems for training independent tasks: experiments on phonebook and related improvements	databases;topology;isolated words hybrid hmm ann systems training independent tasks phonebook multi gaussian hmm systems task independent training medium size vocabularies small size vocabularies telephone database test vocabulary hmm topologies state tying recognition performance lexica cmu lexicon telephone speech lexicon words;hmm topologies;cmu;medium size vocabularies;neural nets;gaussian processes;recognition performance;state tying;training;vocabulary;hidden markov models databases artificial neural networks automatic speech recognition vocabulary telephony system testing training data context modeling topology;speech;telephony;isolated words;training data;automatic speech recognition;artificial neural networks;telephone speech;hidden markov models;lexica;multi gaussian hmm systems;cmu lexicon;speech recognition learning artificial intelligence neural nets hidden markov models gaussian processes;speech recognition;system testing;small size vocabularies;task independent training;phonebook;test vocabulary;learning artificial intelligence;context modeling;hybrid hmm ann systems;lexicon words;independent tasks;telephone database	In this paper we evaluate multi Gaussian HMM sys tems and hybrid HMM ANN systems in the framework of task independent training for small size words and medium size words vocabularies To do this we use the Phonebook database which is particu larly well suited to this kind of experiments since it is a very large telephone database and the size and content of the test vocabulary is very exible For each system di erent HMM topologies are compared to test the in uence of state tying with a number of parame ters approximately kept constant on the recognition performance Two lexica Phonebook and CMU are also compared and it is shown that the CMU lexicon is leading to signi cantly better performance Finally it is shown that with a quite simple system and a few adaptations to the basic HMM ANN scheme recog nition performance of and can easily be achieved respectively on a lexicon of and words isolated words telephone speech and lexicon words not present in the training data	experiment;hidden markov model;lexicon;vocabulary	Stéphane Dupont;Hervé Bourlard;Olivier Deroo;Vincent Fontaine;Jean-Marc Boite	1997		10.1109/ICASSP.1997.598872	natural language processing;training set;speech recognition;computer science;speech;machine learning;pattern recognition;gaussian process;context model;telephony;system testing;artificial neural network	NLP	-20.311553787117532	-87.17623720595888	60810
7156b1f010581b72600d2c720bf9990613440ed3	pronunciation variants description using recognition error modeling with phonetic derivation hypotheses	corpus based approach;multiple pronunciation generation;speech variants;phonological knowledge	This paper proposes a new method of pronunciation variant generation for reducing word error rate in conversational speech recognition. In particular, this paper focuses on the generation of alternative pronunciations from canonical forms by using the phonological knowledge derived from the analysis of a phonetic transcription corpus. The experimental results show that the pronunciation variation generated by the proposed method provides slightly better performance than a method based on manually written pronunciation. These results also demonstrate the applicability of phonological knowledge-based generation of pronunciation variation.	speech recognition;transcription (software);word error rate	Hideharu Nakajima;Yoshinori Sagisaka;Hirofumi Yamamoto	2000			natural language processing;speech recognition;linguistics	NLP	-19.3077305035554	-84.46297754560341	60910
036e98f199f453595770d41f9c2bf4965143a0d1	prosodic characteristics of czech contrastive topic.		It is the main motivation of our present study to identify some further criterion for the oppositions of topic, contrastive topic and, as the case may be, of focus in the prosodic characteristics of utterances. For this purpose, we have examined three sets of naturally occurring speech and analyzed them in order to find whether the character of intonation contours distinguishes between the three notions.		Katerina Vesela;Nino Peterek;Eva Hajicová	2004			speech recognition;czech;computer science	NLP	-12.654119727773542	-80.33573445415423	60943
f08174017b040de2f968ffb773bcb0bb9738bc69	monolingual experiments with far-east languages in ntcir-6	clir;evaluation.;language model;probabilistic ir model;japanese and korean languages;chinese;evaluation	This paper describes our third participation in an evaluation campaign involving the Chinese, Japanese and Korean languages (NTCIR-6). Our participation is motivated by three objectives: 1) study the retrieval performances of various probabilistic and language models for these languages; 2) compare the relative retrieval effectiveness of a combined “unigram & bigram” indexing scheme combined with an automatic wordsegmenting approach for Chinese and Japanese languages; and 3) evaluate the relative performance of the various data fusion strategies used to combine separate result lists in order to enhance retrieval effectiveness.	bigram;language model;n-gram;performance	Samir Abdou;Jacques Savoy	2007			far east;natural language processing;search engine indexing;language model;probabilistic logic;bigram;sensor fusion;speech recognition;computer science;artificial intelligence	NLP	-23.073939811482813	-82.85119392848276	61119
dc20a758b0c303cecc8fbd5aa02cb77eee55ca5c	error visualization for tandem acoustic modeling on the aurora task	analytical models;acoustic modeling;posterior probability;analytical models robustness visualization;error analysis;visualization;gaussian mixture model;distributed models;error rate;robustness;electrical engineering;applied mathematics;neural network	Tandem acoustic modeling consists of taking the outputs of a neural network discriminantly trained to estimate the phone-class posterior probabilities of speech, and using them as the input features of a conventional distribution-modeling Gaussian mixture model (GMM) speech recognizer, thereby employing two acoustic models in tandem. This structure reduces the error rate on the Aurora 2 noisy English digits task in more than 50% compared to the HTK baseline. Even though there are some reasonable hypothesis to explain this improvement, the origins are still unclear. This paper introduces the use of visualization tools for error analysis of some variations of the tandem system. The error behavior is first analyzed using word-level confusion matrices. Posteriorgrams (displays of the variation in time of per-phone posterior probabilities) provide for further analysis. The results of corroborate our previous hypothesis that the gains from tandem modeling arise from the very different training and modeling schemes of the two acoustic models.	acoustic cryptanalysis;acoustic model;artificial neural network;aurora;baseline (configuration management);confusion matrix;error analysis (mathematics);finite-state machine;htk (software);mixture model;speech recognition;tandem computers	Manuel Reyes-Gomez;Daniel P. W. Ellis	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5745630	speech recognition;visualization;word error rate;computer science;machine learning;mixture model;posterior probability;artificial neural network;statistics;robustness	Robotics	-19.104385081369724	-88.79049531574103	61141
d853b9464574d454c1d17a548281bb545b907f03	using a high-speed video camera for robust audio-visual speech recognition in acoustically noisy conditions		The purpose of this study is to develop a robust audio-visual speech recognition system and to investigate the influence of a high-speed video data on the recognition accuracy of continuous Russian speech under different noisy conditions. Developed experimental setup and collected multimodal database allow us to explore the impact brought by the high-speed video recordings with various frames per second (fps) starting from standard 25 fps up to high-speed 200 fps. At the moment there is no research objectively reflecting the dependence of the speech recognition accuracy from the video frame rate. Also there are no relevant audio-visual databases for model training. In this paper, we try to fill in this gap for continuous Russian speech. Our evaluation experiments show the increase of absolute recognition accuracy up to 3% and prove that the use of the high-speed camera JAI Pulnix with 200 fps allows achieving better recognition results under different acoustically noisy conditions.	audio-visual speech recognition	Denis Ivanko;Alexey Karpov;Dmitry Ryumin;Irina S. Kipyatkova;Anton I. Saveliev;V. Yu. Budkov;Dmitriy Ivanko;Milos Zelezný	2017		10.1007/978-3-319-66429-3_76	audio-visual speech recognition;frame rate;viseme;computer vision;speech recognition;artificial intelligence;computer science;video camera	Vision	-12.206627162236217	-89.02908291326057	61233
6c0a40158d50b8b4fe652d2695ecf5cf12f87001	a statistical phonemic segment model for speech recognition based on automatic phonemic segmentation	speech recognition	This paper presents a method of constructing a statistical phonemic segment model (SPSM) for a speech recogn iti n system based on speaker-independent context-independent automatic phonemic segmentation. In our recent research, we proposed the phoneme recogn ition system using the template matching method with the same segmentation, and confirmed that 5-frame-fixed time sequence of feature vectors used as a template represents features of phoneme effectively. This time, to improve a mass of these templates to a smarter model, we introduced a statistical method into modeling. The structure of SPSM connects 5 distributions of Gaussian N-mixture density in series. By the experiment of closed Japanese spoken word recognition, using VCV balanced 4920 words spoken by 10 male adults including 34430 phonemes in total, the rate of phoneme recogn ition using SPSM was up to 90.23 % compared with the rate using phoneme templates, 80.39 %.	series and parallel circuits;speech recognition;template matching;time series	Katsura Aizawa;Chieko Furuichi	1998			speech recognition;artificial intelligence;pattern recognition;template;logogen model;template matching;computer science;feature vector;segmentation	NLP	-14.85180513396006	-87.91548125650657	61248
2608c82580ade1c2053113ad608e76bb60b6ae17	unified decoding and feature representation for improved speech recognition.	speech recognition	In this paper we propose a unified framework for decoding and feature representation based on the Maximum A Posterior (MAP) principle. The search space is augmented with an additional feature stream dimension such that different feature representations can be utilized for different phonetic context under the HMM decoding framework. We also provide a theoretic explanation for the unified framework. It gives us “supervised” signal processing and feature extraction for the recognition system, which has reduced the word recognition error rate by 15% on a large-vocabulary continuous sp eech recognition task when multiple feature streams are used simultaneously.	experiment;feature extraction;hidden markov model;rover (the prisoner);signal processing;speech recognition;supervised learning;theory;unified framework;vocabulary	Li Jiang;Xuedong Huang	1999			speech recognition;feature;machine learning;pattern recognition	ML	-16.958665226784603	-88.2591230927351	61261
36655391aa8069a1b2dca575b67acfddbf68dca9	dominance spectrum based v/uv classification and f_0 estimation	spectrum;instantaneous frequency;fixed point	Abstract This paper presents a new,method,for robust voiced/unvoiced segment,(V/UV) classification and accurate fundamental,fre- quency,( ) estimation in a noisy environment. For this pur- pose, we introduce the degree of dominance and dominance spectrum that are defined by instantaneous frequency. The de- gree of dominance,allows us to evaluate the magnitude,of in- dividual harmonic,components,of speech signals relative to the background,noise. The V/UV segments,are robustly classified based on the capability of the dominance,spectrum,to extract the regularity in the harmonic,structure.,is accurately de- termined based on fixed points corresponding,to dominant,har- monic,components,easily selected from the dominance,spec- trum. Experimental results show,that the present method,is better than the existing methods,in terms of gross and fine errors, and V/UV correct rates in the presence of background white and babble noise.		Tomohiro Nakatani;Toshio Irino;Parham Zolfaghari	2003			artificial intelligence;pattern recognition;induction coil;acoustics;auxiliary electrode;gas-discharge lamp;computer science;mathematical optimization;fixed point;instantaneous phase	EDA	-9.897918111768911	-89.7037201902999	61324
b7c5195619a1b565a3382c3408999df3397865c5	introducing a fm based feature to hierarchical language identification	system integration;language identification;indexing terms	Although relatively neglected in auditory analysis, phase information plays an important role in human auditory intelligibility. This paper investigates a Frequency Modulation (FM) based feature and its contribution to a Language Identification (LID) system, using a Hierarchical LID framework. FM components represent the phase information of a given signal in an AM-FM model. In this paper, we extract a FM-based feature using a technique which produces consistent and continuous FM components, and build a LID system on this feature with GMM based modeling. The performance is improved by combining this system with existing MFCC, Prosody based systems and a PRLM system. When compared to the baseline system without integrating a FM-based system, the proposed Hierarchical LID system shows improvements. Additionally, the proposed system outperforms the GMM fusion-based system integrating the same four primary systems, showing that the Hierarchical LID framework is more effective in integrating additional features.	baseline (configuration management);fm broadcasting;google map maker;intelligibility (philosophy);language identification;modulation;semantic prosody	Bo Yin;Tharmarajah Thiruvaran;Eliathamby Ambikairajah;Fang Chen	2008			artificial intelligence;language identification;diaphragm (structural system);pattern recognition;diaphragm valve;fuel tank;control valves;computer science	SE	-15.323744084277543	-88.33749432026099	61354
11ffb317e2a109488b79be0db45dec6302f6f781	the pytorch-kaldi speech recognition toolkit		The availability of open-source software is playing a remarkable role in the popularization of speech recognition and deep learning. Kaldi, for instance, is nowadays an established framework used to develop state-of-the-art speech recognizers. PyTorch is used to build neural networks with the Python language and has recently spawn tremendous interest within the machine learning community thanks to its simplicity and flexibility. The PyTorch-Kaldi project aims to bridge the gap between these popular toolkits, trying to inherit the efficiency of Kaldi and the flexibility of PyTorch. PyTorch-Kaldi is not only a simple interface between these software, but it embeds several useful features for developing modern speech recognizers. For instance, the code is specifically designed to naturally plug-in user-defined acoustic models. As an alternative, users can exploit several pre-implemented neural networks that can be customized using intuitive configuration files. PyTorch-Kaldi supports multiple feature and label streams as well as combinations of neural networks, enabling the use of complex neural architectures. The toolkit is publicly-released along with a rich documentation and is designed to properly work locally or on HPC clusters. Experiments, that are conducted on several datasets and tasks, show that PyTorch-Kaldi can effectively be used to develop modern state-of-the-art speech recognizers.	acoustic cryptanalysis;acoustic model;artificial neural network;computer cluster;deep learning;documentation;experiment;finite-state machine;kaldi;list of toolkits;machine learning;open-source software;plug-in (computing);python;spawn (computing);speech recognition;speech synthesis	Mirco Ravanelli;Titouan Parcollet;Yoshua Bengio	2018	CoRR		documentation;artificial neural network;deep learning;computer science;software;python (programming language);speech recognition;exploit;artificial intelligence	NLP	-23.98598751595242	-86.30704247646653	61580
5677d2b565c8265fef1693a9be861739cb01bf2f	evolution-strategy-based automation of system development for high-performance speech recognition		The state-of-the-art large vocabulary speech recognition systems consist of several components including hidden Markov model and deep neural network. To realize the highest recognition performance, numerous meta-parameters specifying the designs and training setups of these components must be optimized. A prominent obstacle in system development is the laborious effort required by human experts in tuning these meta-parameters. To automate the process, we propose to tune the meta-parameters of a whole large vocabulary speech recognition system using the evolution strategy with a multi-objective Pareto optimization. As the result of the evolution, the system is optimized for both low word error rate and compact model size. Since the approach requires repeated training and evaluation of the recognition systems that require large computation, we make use of parallel computation on cloud computers. Experimental results show the effectiveness of the proposed approach by discovering appropriate configuration for large vocabulary speech recognition systems automatically.	artificial neural network;automation;biological evolution;cloud computing;computation;computer;computers;deep learning;evolution strategy;hidden markov model;markov chain;mathematical optimization;neural network simulation;numerous;parallel computing;pareto efficiency;speech recognition;vocabulary;word error rate	Takafumi Moriya;Tomohiro Tanaka;Takahiro Shinozaki;Shinji Watanabe;Kevin Duh	2019	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2018.2871755	speech recognition;artificial neural network;computation;computer science;multi-objective optimization;word error rate;speech processing;hidden markov model;vocabulary;evolution strategy	ML	-20.093142280195874	-89.30687995317099	61592
229bdd14af2d97619e6f30819f3f687ecbd7935f	comparison between two different approaches in speaker - independent isolated digit recognition				Javier Ferreiros;A. Castro;José Manuel Pardo	1991			speech recognition;artificial intelligence;pattern recognition;speaker diarisation;speaker recognition;computer science;numerical digit	Vision	-14.724693590314669	-87.99798675624847	61844
a5370ccc6df6c29c45d991c6d56287fb5b799003	learning a lexicon and translation model from phoneme lattices		Language documentation begins by gathering speech. Manual or automatic transcription at the word level is typically not possible because of the absence of an orthography or prior lexicon, and though manual phonemic transcription is possible, it is prohibitively slow. On the other hand, translations of the minority language into a major language are more easily acquired. We propose a method to harness such translations to improve automatic phoneme recognition. The method assumes no prior lexicon or translation model, instead learning them from phoneme lattices and translations of the speech being transcribed. Experiments demonstrate phoneme error rate improvements against two baselines and the model’s ability to learn useful bilingual lexical entries.	acoustic cryptanalysis;acoustic model;automatic control;baseline (configuration management);documentation;finite-state machine;language model;lexicon;syllable;transcription (software);word error rate	Oliver Adams;Graham Neubig;Trevor Cohn;Steven Bird;Quoc Truong Do;Satoshi Nakamura	2016			computer science;natural language processing;artificial intelligence;lattice (order);speech recognition;lexicon	NLP	-20.646054156582345	-84.93432166244077	61859
de0e5ef03dc8ebe526e96e4db70de34b16efedaa	improved parcel sorting by combining automatic speech and character recognition	databases;sorting;hidden markov model;confidence measure;optical character recognition;data collection;edit distance;speech;optical character recognition software speech error analysis sorting hidden markov models databases noise measurement;system performance;noise measurement;error analysis;optical character recognition software;automatic speech recognition;parcel sorting;postal services;hidden markov models;address recognition automatic speech recognition optical character recognition parcel sorting;address recognition;speech recognition;character recognition;address recognition parcel sorting automatic speech recognition automatic postal sorting system optical character recognition ocr technology ocr system flat mail item asr technology ocr output confidence asr system levenshtein edit distance ocr confidence measure dynamic fusion strategy asr output multimodal solution zip code recognition rate;speech recognition optical character recognition postal services sorting	Automatic postal sorting systems have traditionally relied on optical character recognition (OCR) technology. While OCR systems perform well for flat mail items such as envelopes, the performance deteriorates for parcels. In this study, we propose a new multimodal solution for parcel sorting which combines automatic speech recognition (ASR) technology with OCR in order to deliver better performance. Our multimodal approach is based on estimating OCR output confidence, and then optionally using ASR system output when OCR results show low confidence. Particularly, we proposed a Levenshtein edit distance (LED) based measure to compute OCR confidence. Based on the OCR confidence measure, a dynamic fusion strategy is developed that forms its final decision based on (i) OCR output alone, (ii) ASR output alone, and (iii) combination of ASR and OCR outputs. The proposed system is evaluated on speech and image data collected in real-world conditions. Our experiments show that the proposed multimodal solution achieves an overall zip code recognition rate of 90.2%, which is a substantial improvement over ASR alone (81%) and OCR alone (80.6%) systems. This advancement represents an important contribution that leverages OCR and ASR technologies to improve address recognition in parcels.	automated system recovery;experiment;graph edit distance;levenshtein distance;multimodal interaction;optical character recognition;postal;sorting;speech recognition;speech synthesis	Amriteshwar Singh;Abhijeet Sangwan;John H. L. Hansen	2012	2012 IEEE International Conference on Emerging Signal Processing Applications	10.1109/ESPA.2012.6152444	natural language processing;speech recognition;computer science;pattern recognition	Robotics	-21.41686992398289	-85.65570034117724	61921
b76c3f3f2ea9fef7bc8d0108f274f095e8e90e37	the correlation between signal distance and consonant pronunciation in mandarin words	prediction algorithms;speech;speech enhancement;mathematical model;correlation;biomedical measurement	In Mandarin language speaking, some consonant and vowel pairs are hard to be distinguished and pronounced clearly even for some native speakers. This study investigates the signal distance between consonants compared in pairs from the signal processing point of view to reveal the correlation of signal distance and consonant pronunciation. Some popular speech quality objective measures are innovatively applied to obtain the signal distance. The experimental results show that the confusing pair, /l/-/n/, does have the shortest signal distance compared with other consonants followed by the same vowel and lexical tone. The finding suggests that the signal distance is able to evaluate the confusion degree of certain consonant/vowel pair in a numerical and quantitative way. The signal distance of the other consonant/vowel pairs will be explored in the future work.	numerical analysis;signal processing;super robot monkey team hyperforce go!	Huijun Ding;Chenxi Xie;Lei Zeng;Guo Dan	2016	2016 10th International Symposium on Chinese Spoken Language Processing (ISCSLP)	10.1109/ISCSLP.2016.7918482	speech recognition;mid vowel;prediction;computer science;speech;mathematical model;linguistics;correlation;statistics	NLP	-10.608785990845332	-83.88709097142414	61928
fbff02bc60327f9478e0107bde5f6d8c4534cc46	a generalized composition algorithm for weighted finite-state transducers	weighted finite state transducer	This paper describes a weighted finite-state transducer composition algorithm that generalizes the concept of the composition filter and presents filters that remove useless epsilon paths and push forward labels and weights along epsilon paths. This filtering permits the compostion of large speech recognition contextdependent lexicons and language models much more efficiently in time and space than previously possible. We present experiments on Broadcast News and a spoken query task that demonstrate an∼5% to 10% overhead for dynamic, runtime composition compared to a static, offline composition of the recognition transducer. To our knowledge, this is the first such system with so little overhead.	algorithm;experiment;finite-state transducer;language model;lexicon;online and offline;overhead (computing);speech recognition	Cyril Allauzen;Michael Riley;Johan Schalkwyk	2009			mathematical optimization;computer science	NLP	-22.67776860080625	-87.50427559188746	62082
26dfee4fc889fcbfc719884aec8c540641f518ef	speaker-independent hmm-based voice conversion using quantized fundamental frequency	fundamental frequency	This paper proposes a segment-based voice conversion technique between arbitrary speakers with a small amount of training data. In the proposed technique, an input speech utterance of source speaker is decoded into phonetic and prosodic symbol sequences, and then the converted speech is generated from the pre-trained target speaker’s HMM using the decoded information. To reduce the required amount of training data, we use speaker-independent model in the decoding of the input speech, and model adaptation for the training of the target speaker’s model. Experimental results show that there is no need to prepare the source speaker’s training data, and the proposed technique with only ten sentences of the target speaker’s adaptation data outperforms the conventional GMM-based one using parallel data of 200 sentences.	google map maker;hidden markov model;quantization (signal processing)	Takashi Nose;Takao Kobayashi	2010			artificial intelligence;speech recognition;hidden markov model;pattern recognition;computer science;fundamental frequency;quantization (physics)	NLP	-18.62911595700418	-87.90795476582173	62158
ff4c62a4e513410c5e3c1754e8e179ec3b1c3b39	enriching text-to-speech synthesis using automatic dialog act tags		We present an approach for enriching dialog based textto-speech (TTS) synthesis systems by explicitly controlling the expressiveness through the use of dialog act tags. The dialog act tags in our framework are automatically obtained by training a maximum entropy classifier on the Switchboard-DAMSL data set, unrelated to the TTS database. We compare the voice quality produced by exploiting automatic dialog act tags with that using human annotations of dialog acts, and with two forms of reference databases. Even though the inventory of tags is different for the automatic tagger and human annotation, exploiting either form of dialog markup generates better voice quality in comparison with the reference voices in subjective evaluation.	brill tagger;database;dialog system;markup language;multinomial logistic regression;netware file system;speech synthesis;telephone switchboard	Vivek Kumar Rangarajan Sridhar;Ann K. Syrdal;Alistair Conkie;Srinivas Bangalore	2011			dialog act;speech recognition;computer science;speech synthesis	NLP	-21.786283196751498	-83.43554278445791	62192
367aee7d1bc90106fdc4eded18902e2aaa1f3bb9	a syntax-controlled segmentation of speech signal on the basis of dynamic spectra	cybernetics;speech processing;filters;similarity relation;spectrum;pattern recognition signal processing algorithms speech recognition filters production laboratories cybernetics speech processing signal processing biomedical signal processing;signal processing;biomedical signal processing;pattern recognition;production;speech recognition;signal processing algorithms	"""In this paper an effective algorithm of segmentation of the dynamic spectra of speech signal is presented. The algorithm realizes segmentation into simple elements, very similar to the classic """"phonetic-acoustic segments"""", which contain the full linguistic information carried by the speech signal. The described algorithm is based on the syntactic theory of pattern recognition and makes use of the similarity relations among the momentary spectra composing the dynamic spectrum of an utterance. The proposed method has been experimentally verified, the final correctness of segmentation exceeding 90%."""		Leszek Kot	1982		10.1109/ICASSP.1982.1171845	voice activity detection;multidimensional signal processing;natural language processing;spectrum;linear predictive coding;speech recognition;cybernetics;computer science;digital signal processing;speech coding;signal processing;speech processing;signal	Vision	-9.305112234501435	-91.07672686998148	62341
3cc46bf79fb9225cf308815c7d41c8dd5625cc29	age interval and gender prediction using parafac2 applied to speech utterances	tensile stress;trinity college dublin speaker ageing database parafac2 parallel factor analysis 2 speech utterances speech soft biometrics speaker age prediction speaker gender prediction speech utterance feature matrix auditory cortical representations speaker age matrix speaker gender matrix dimensionality reduction semantic space ranking vector decision making;semantics;speech;aging;vectors age issues biometrics access control decision making gender issues matrix algebra speaker recognition;matrix decomposition;parafac2 speaker biometrics speaker ageing;feature extraction;distributed databases;aging speech feature extraction tensile stress matrix decomposition semantics distributed databases	Important problems in speech soft biometrics include the prediction of speaker's age or gender. Here, the aforementioned problems are addressed in the context of utterances collected during a long time period. A unified framework for age and gender prediction is proposed based on Parallel Factor Analysis 2 (PARAFAC2). PARAFAC2 is applied to a collection of three matrices, namely the speech utterance-feature matrix whose columns are the auditory cortical representations, the speaker age matrix whose columns are indicator vectors of suitable dimension, and the speaker gender matrix whose columns are proper indicator vectors associated to speaker's gender. PARAFAC2 is able to reduce the dimensionality of the auditory cortical representations by projecting these representations onto a semantic space dominated by the age and the gender concepts, yielding a sketch (i.e., a feature vector of reduced dimensions). To predict speaker's age interval associated to a test utterance, the speech utterance sketch is pre-multiplied by the left singular vectors of the speaker age matrix. To predict the gender of the speaker who uttered any test utterance, the speech utterance sketch is pre-multiplied by the left singular vectors of the speaker gender matrix. In both cases, a ranking vector is obtained that is exploited for decision making. Promising results are demonstrated, when the aforementioned framework is applied to the Trinity College Dublin Speaker Ageing Database.	column (database);factor analysis;feature vector;soft biometrics;speaker recognition;trinity;unified framework	Evangelia Pantraki;Constantine Kotropoulos;Andreas Lanitis	2016	2016 4th International Conference on Biometrics and Forensics (IWBF)	10.1109/IWBF.2016.7449694	natural language processing;speaker recognition;speaker diarisation;speech recognition;feature extraction;computer science;speech;semantics;stress;matrix decomposition	ML	-5.023656292283643	-86.22975482232816	62532
ad219c2bc411ffa097c8201e2f6d57aeafda0685	fast discriminative speaker verification in the i-vector space	speaker models;kernel;nist 2010 speaker recognition evaluation;nist;cost function;generic model;support vector machines;training;vector space;speech;speaker recognition evaluation;decision cost function;speaker verification;speaker recognition;equal error rate;support vector machines speaker recognition;equal error rate fast discriminative speaker verification i vector space speaker models support vector machine kernel discriminative models tel tel extended core condition nist 2010 speaker recognition evaluation decision cost function;support vector machine kernel;discriminative training;support vector machines training nist speaker recognition kernel covariance matrix speech;support vector machine;i vectors;i vector space;tel tel extended core condition;two covariance kernel;fast discriminative speaker verification;discriminative model;i vectors discriminative training two covariance kernel support vector machines;discriminative models;covariance matrix	This work presents a new approach to discriminative speaker verification. Rather than estimating speaker models, or a model that discriminates between a speaker class and the class of all the other speakers, we directly solve the problem of classifying pairs of utterances as belonging to the same speaker or not.	consistency model;discriminative model;pc speaker;speaker recognition	Sandro Cumani;Niko Brümmer;Lukás Burget;Pietro Laface	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947442	speaker recognition;support vector machine;speaker diarisation;speech recognition;computer science;machine learning;pattern recognition;discriminative model	Robotics	-16.534358644466856	-91.70056749863843	62679
105b5af8ada874bb623b0d4e7e9e148dd6143d92	measurement and classification of the affective value of drum rhythm patterns	rhythm;signal classification acoustic signal processing;affective evaluation;atmospheric measurements;multivariate analysis of variance;particle measurements;variance multivariate analysis drum rhythm patterns avsm single factor analysis snare drum beats bass drum beats;acoustic signal processing;psychology;music and affect;drum rhythm pattern;signal classification;analysis of variance;rhythm analysis of variance correlation psychology particle measurements atmospheric measurements educational institutions;correlation;music and affect drum rhythm pattern affective evaluation;correlation analysis	We have measured the affective value of 14 different varieties of drum-rhythm patterns by applying the affective value scale of music (AVSM). The results that we obtained, from a single-factor analysis of variance, a multivariate analysis of variance, and a correlation analysis, suggest that the number of snare-drum beats (high pitch) and bass-drum beats (low pitch) in one measure influences the affective value in respect of elation, lightness, and solemnity. An increased number of snare-drum beats in one measure leads to an increase in the affective value of elation (described as cheerful and bright), as well as the affective value of lightness (described as light and restless). In addition, the results revealed that an increased number of bass-drum beats in one measure leads to an increase in the affective value of solemnity (described as solemn and dignified).	beneath a steel sky;cluster analysis;drum memory;factor analysis;group cohesiveness;snare (software)	Yuta Kurotaki;Hisao Shiizuka	2011	2011 Third International Conference on Intelligent Networking and Collaborative Systems	10.1109/INCoS.2011.116	multivariate analysis of variance;speech recognition;analysis of variance;rhythm;correlation;statistics	Robotics	-9.514411439209436	-83.18380542187002	62710
7dcd68de73737a1f0539e228094c9ecc57daeae2	low cost speaker dependent isolated word speech preselection system using static phoneme pattern recognition.	pattern recognition;speaker dependent			Manuel A. Leandro;José Manuel Pardo	1993			speaker recognition;speaker diarisation;speech recognition;word error rate;pattern recognition;communication	Vision	-14.546347681521272	-87.72920367891531	62731
021bc52188f81cbaa892e05fbe524e76d20a7ca1	autobi - a tool for automatic tobi annotation		This paper describes the AuToBI tool for automatic generation of hypothesized ToBI labels. While research on automa ic prosodic annotation has been conducted for many years, AuToBI represents the first publicly available tool to automat ically detect and classify the breaks and tones that make up the ToBI annotation standard. This paper describes the feature extr action routines as well as the classifiers used to detect and classif y the prosodic events of the ToBI standard. Additionally, we repo rt performance evaluating AuToBI models trained on the Boston Directions Corpus on the Columbia Games Corpus. By evaluating on distinct speakers domains and recording conditions, thi evaluation represents an accurate representation of the pe rformance of the system when applied to novel spoken material.		Andrew Rosenberg	2010			artificial intelligence;speech recognition;pattern recognition;feature extraction;computer science;annotation	NLP	-18.554090211573392	-82.46436835582458	62789
472017797511fefd0d7aec34dfdd1aec634243ff	a study of speaker verification performance with expressive speech		Expressive speech introduces variations in the acoustic features affecting the performance of speech technology such as speaker verification systems. It is important to identify the range of emotions for which we can reliably estimate speaker verification tasks. This paper studies the performance of a speaker verification system as a function of emotions. Instead of categorical classes such as happiness or anger, which have important intra-class variability, we use the continuous attributes arousal, valence, and dominance which facilitate the analysis. We evaluate an speaker verification system trained with the i-vector framework with a probabilistic linear discriminant analysis (PLDA) back-end. The study relies on a subset of the MSP-PODCAST corpus, which has naturalistic recordings from 40 speakers. We train the system with neutral speech, creating mismatches on the testing set. The results show that speaker verification errors increase when the values of the emotional attributes increase. For neutral/moderate values of arousal, valence and dominance, the speaker verification performance are reliable. These results are also observed when we artificially force the sentences to have the same duration.	acoustic cryptanalysis;linear discriminant analysis;max;podcast;spatial variability;speaker recognition;speech synthesis;speech technology	Srinivas Parthasarathy;Chunlei Zhang;John H. L. Hansen;Carlos Busso	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7953216	artificial intelligence;computer science;speaker recognition;probabilistic logic;speaker diarisation;categorical variable;pattern recognition;speech technology;speech recognition;speech processing;linear discriminant analysis	SE	-12.197404718198959	-85.1220371494713	63082
e975eb0a77624ea18b2f9f006d4185d380e3d6d7	detection of vowel onset points in voiced aspirated sounds of indian languages.		Vowel onset point (VOP) is defined as the instant at which onset of vowel takes place. Accurate detection of VOP is useful in many applications like syllable unit recognition, end-point detection, speaker verification etc. Manually and automatically locating VOPs accurately in case of voiced aspirated (VA) sounds is found to be difficult and ambiguous. This is due to the complex nature of the speech signal waveform around the VOP. This work addresses this issue and a manual marking approach using electroglottograph (EGG) signal is described which accurately marks the VOPs without any ambiguity. The knowledge derived from this manual analysis is transformed into an automatic method for the detection of VOPs in VA sounds. An automatic method is proposed using both source and vocal tract information. VOP detection accuracy of the proposed method is found to be significantly higher than some of the state of the art techniques.	electroglottograph;item unique identification;onset (audio);speaker recognition;syllable;tract (literature);waveform	Biswajit Dev Sarma;S. R. Mahadeva Prasanna	2014			speech recognition;mid vowel	SE	-10.618026620114433	-85.56679753530703	63083
4f3261da8e36fb5e63c2e4e710550d0a269abea7	pre-nuclear intonation in questions of japanese students in english				Margaret Maeda	1994			speech recognition;computer science	NLP	-15.453857045283625	-85.0502901178575	63153
9b1b1c68cc931f2c8be5dfdcb0d8e71f24504d9c	signals and speech.		Our ancestors were a social species before they had language, and like other apes likely used a system of signals (dominance, interest, warning, etc) to coordinate their activities. Speech appears to have evolved as an elaboration of this earlier communication framework, with signaling remaining as a major functional structure in both dyadic and group interactions.	dyadic transformation;interaction	Alex Pentland	2011			voice activity detection;speech coding;speech processing	ML	-8.353328679515418	-82.49591419550198	63295
7cd93876bde524a5ac6f69c6fc39b35e79a86395	i-vector transformation using a novel discriminative denoising autoencoder for noise-robust speaker recognition		This paper proposes i-vector transformations using neural networks for achieving noise-robust speaker recognition. A novel discriminative denoising autoencoder (DDAE) is employed on i-vectors to remove additive noise effects. The DDAE is trained to denoise and classify noisy i-vectors simultaneously, making it possible to add discriminability to the denoised i-vectors. Speaker recognition experiments on the NIST SRE 2012 task shows 32% better error performance as compared to a baseline system. Also, our proposed method outperforms such conventional methods as multi-condition training and a basic denoising autoencoder.	additive white gaussian noise;artificial neural network;autoencoder;baseline (configuration management);experiment;noise reduction;speaker recognition;utility functions on indivisible goods	Shivangi Mahto;Hitoshi Yamamoto;Takafumi Koshinaka	2017			autoencoder;speech recognition;discriminative model;coordinate vector;artificial intelligence;speaker recognition;noise reduction;pattern recognition;computer science	NLP	-14.95973291762246	-91.09354166078366	63392
7cb4a725e80b340b13fd25eb8adf80e1e798020c	simultaneous multispeaker segmentation for automatic meeting recognition	silicon;word error rate multispeaker segmentation automatic meeting recognition vocal activity detection automatic speech recognition automatic speech understanding automatic multichannel segmentation system;microphones;manuals;crosstalk;acoustics;training;speech;manuals silicon speech training microphones acoustics crosstalk;speaker recognition	Vocal activity detection is an important technology for both automatic speech recognition and automatic speech understanding. In meetings, participants typically vocalize for only a fraction of the recorded time, and standard vocal activity detection algorithms for close-talk microphones have shown to be ineffective. This is primarily due to the problem of crosstalk, in which a participant's speech appears on other participants' microphones, making it hard to attribute detected speech to its correct speaker. We describe an automatic multichannel segmentation system for meeting recognition, which accounts for both the observed acoustics and the inferred vocal activity states of all participants using joint multi-participant models. Our experiments show that this approach almost completely eliminates the crosstalk problem. Recent improvements to the baseline reduce the development set word error rate, achieved by a state-of-the-art multi-pass speech recognition system, by 62% relative to manual segmentation. We also observe significant performance improvements on unseen data.	algorithm;baseline (configuration management);crosstalk;experiment;microphone;speech recognition;word error rate	Kornel Laskowski;Christian Fügen;Tanja Schultz	2007	2007 15th European Signal Processing Conference		voice activity detection;speaker recognition;speech recognition;acoustics;computer science;speech processing;communication	NLP	-13.222236329136045	-89.58778280551628	63548
17f574595013cedff1ca25e384d75d0ca24d751c	a short-time objective intelligibility measure for time-frequency weighted noisy speech	weight measurement time frequency analysis speech processing speech enhancement degradation artificial intelligence noise reduction signal processing noise measurement testing;speech intelligibility speech enhancement;degradation;speech intelligibility;speech separation;speech processing;time frequency weighted noisy speech;time frequency;signalbehandling;speech;testing;speech enhancement;system on a chip;noise measurement;weight measurement;intelligibility prediction;signal processing;noise reduction;noisy speech;objective speech intelligibility measurement;artificial intelligence;time frequency weighting;correlation;time frequency analysis;objective intelligibility measurement;noisy speech intelligibility prediction speech enhancement;noise;objective intelligibility measurement time frequency weighted noisy speech objective speech intelligibility measurement time frequency weighting noise reduction speech separation	Existing objective speech-intelligibility measures are suitable for several types of degradation, however, it turns out that they are less appropriate for methods where noisy speech is processed by a time-frequency (TF) weighting, e.g., noise reduction and speech separation. In this paper, we present an objective intelligibility measure, which shows high correlation (rho=0.95) with the intelligibility of both noisy, and TF-weighted noisy speech. The proposed method shows significantly better performance than three other, more sophisticated, objective measures. Furthermore, it is based on an intermediate intelligibility measure for short-time (approximately 400 ms) TF-regions, and uses a simple DFT-based TF-decomposition. In addition, a free Matlab implementation is provided.	discrete fourier transform;elegant degradation;intelligibility (philosophy);matlab;noise reduction	Cees H. Taal;Richard Christian Hendriks;Richard Heusdens;Jesper Jensen	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495701	computer vision;speech recognition;time–frequency analysis;computer science;signal processing;speech processing	Robotics	-9.592792485064662	-88.41343356576036	63550
364549b19c37a6bf26249ebbc1126c5ead9e463b	angie: a new framework for speech analysis based on morpho-phonological modelling	natural languages;bottom up;speech processing;computer science;morphology;acoustic waves;grammars;speech recognition;error rate	This paper describes a new system for speech analysis, ANGIE, which characterizes word substructure in terms of a trainable grammar. ANGIE capture morpho-phonemic and phonological phenomena through a hierarchical framework. The terminal categories can be alternately letters or phone units, yielding a reversible letter-tosound/sound-to-letter system. In conjunction with a segment network and acoustic phone models, the system can produce phonemicto-phonetic alignments for speech waveforms. For speech recognition, ANGIE uses a one-pass bottom-up best-first search strategy. Evaluated in the ATIS domain, ANGIE achieveda phone error rate of 36%, as compared with 40% achieved with a baseline phone-bigram based recognizer under similar conditions. ANGIE potentially offers many attractive features, including dynamic vocabulary adaptation, as well as a framework for handling unknown words.	acoustic cryptanalysis;automatic transmitter identification system (television);baseline (configuration management);best-first search;bigram;finite-state machine;speech recognition;vocabulary;voice analysis	Stephanie Seneff;Raymond Lau;Helen M. Meng	1996			natural language processing;speech recognition;computer science;linguistics	NLP	-19.048209111882787	-85.2521361837066	63582
dc583eaec60b76149752e2969b1e3020b1543ff0	optimal residual frame based source modeling for hmm-based speech synthesis	databases;speech synthesis;speech;speech synthesis decision trees hidden markov models;hidden markov models speech decision trees speech synthesis high temperature superconductors feature extraction databases;hidden markov models;feature extraction;high temperature superconductors;hidden markov model optimal residual frame source modeling hmm based speech synthesis system hts residual signal speech corpus contextual feature cluster decision tree intensity contour pulse excitation;decision trees;optimal residual frame hmm based speech synthesis excitation modeling hybrid synthesis	This paper proposes a method for modeling the excitation signal to improve the quality of HMM-based speech synthesis system (HTS). Single optimal residual frame which closely relates to all frames of phone is chosen to represent the entire residual signal of the phone. Optimal residual frames of all phones present in the speech corpus are efficiently grouped based on positional and contextual features to form a decision tree of clusters. During synthesis, suitable residual frames are picked up from the leaf of decision tree based on certain selection criteria. From each optimal residual frame, the entire residual signal corresponding to the phone is generated based on pitch and intensity contours obtained from HMMs. Subjective evaluation results show that the proposed excitation model can significantly improve the quality of HTS compared to traditional pulse excitation.	cmos;concatenation;decision tree;hidden markov model;high-throughput satellite;jumbo frame;residual frame;speech corpus;speech synthesis	N. P. Narendra;K. Sreenivasa Rao	2015	2015 Eighth International Conference on Advances in Pattern Recognition (ICAPR)	10.1109/ICAPR.2015.7050668	speech recognition;computer science;pattern recognition;communication	Robotics	-12.033585087128623	-89.3349646775666	63784
40092261e88e6bbef5dbaac5e11d6abd5181a301	automatic speech translation based on the semantic structure	semantic decoding;inflectional model;syntactic model;se- mantic model;language production;semantic structure;speech understanding;automatic speech translation;stochastic processes;tin;natural languages;decoding;graphics;knowledge base;speech recognition;semantic model;logic;language translation;speech synthesis;linguistics	This paper describes a system for the semantic-based translation of spoken or written limited-domain utterances. The semantic structure as output of a semantic decoder serves as the interlingua-level. A word chain generator combined with a linguistic post-processor produces the according word chain in the target language. Both the semantic decoder and the word chain generator work with pure stochastic and trainable knowledge bases. The grammatical features of certain words can be easily extracted by the help of both the word chain and the semantic structure.	compiler;knowledge base	Johannes Müller;Holger Stahl;Manfred K. Lang	1996			natural language processing;speech production;semantic computing;speech recognition;computer science;semantic compression;linguistics;rule-based machine translation	NLP	-29.036126517052807	-80.22212573417633	63797
668cac970026bc3405be220357a838fc328fca76	keyword-specific normalization based keyword spotting for spontaneous speech	bayesian information criterion keyword spotting dynamic time warping gaussian mixture model sliding window;gaussian processes;bayes methods;speech processing;bayesian information criterion keyword specific normalization based keyword spotting spontaneous speech keyword model word spotting architecture scoring patch feature vector sequence extraction sliding windows threshold setting dynamic time warping based template matching dtw gaussian mixture models gmm log likelihood ratio based method speech utterance phonetic hidden markov model;hidden markov models;pattern matching;feature extraction;speech recognition bayes methods feature extraction gaussian processes hidden markov models pattern matching speech processing;speech recognition;hidden markov models speech acoustics training vectors data models training data	This paper presents a novel architecture for keyword spotting in spontaneous speech, in which keyword model is trained from a small number of acoustic examples provided by a user. The word-spotting architecture relies on scoring patch feature vector sequences extracted by using sliding windows, and performing keyword-specific normalization and threshold setting. Dynamic time warping (DTW) based template matching and Gaussian Mixture Models (GMM) are proposed to model the keyword, and another GMM is proposed to model the non-keywords. Our keyword spotting experiments demonstrate the effectiveness of the proposed methods. More specifically, the proposed GMM log-likelihood ratio based method achieves about 17% absolute improvement in terms of recall rates compared to the baseline system.	acoustic cryptanalysis;baseline (configuration management);dynamic time warping;experiment;feature vector;google map maker;microsoft windows;mixture model;spontaneous order;template matching	Weifeng Li;Qingmin Liao	2012	2012 8th International Symposium on Chinese Spoken Language Processing	10.1109/ISCSLP.2012.6423490	speech recognition;feature extraction;computer science;machine learning;pattern matching;pattern recognition;gaussian process;speech processing;hidden markov model	NLP	-16.540504840126843	-90.65565672954415	63803
34b8adea74aac778f9de5d4466f7e807e92430f6	automatic punctuation generation for speech	manuals;acoustic model;multilayer perceptron automatic punctuation generation speech to text transcription maximum a posteriori automatic speech recognition acoustic model language model mlp based trigger word model trigram punctuation predictor;mlp based trigger word model;speech synthesis;automatic punctuation generation;acoustics;multilayer perceptrons;training;speech;speech to text transcription;multilayer perceptron;maximum likelihood estimation;automatic speech recognition;trigram punctuation predictor;digital audio broadcasting;speech synthesis maximum likelihood estimation multilayer perceptrons speech recognition;speech recognition;predictive models;maximum a posteriori;automatic speech recognition speech recognition voice mail predictive models asia maximum a posteriori estimation delay laboratories information science acoustical engineering;language model	Automatic generation of punctuation is an essential feature for many speech-to-text transcription tasks. This paper describes a Maximum A-Posteriori (MAP) approach for inserting punctuation marks into raw word sequences obtained from Automatic Speech Recognition (ASR). The system consists of an “acoustic model” (AM) for prosodic features (actually pause duration) and a “language model” (LM) for text-only features. The LM combines three components: an MLP-based trigger-word model and a forward and a backward trigram punctuation predictor. The separation into acoustic and language model allows to learn these models on different corpora, especially allowing the LM to be trained on large amounts of data (text) for which no acoustic information is available. We find that the trigger-word LM is very useful, and further improvement can be achieved when combining both prosodic and lexical information. We achieve an F-measure of 81.0% and 56.5% for voicemails and podcasts, respectively, on reference transcripts, and 69.6% for voicemails on ASR transcripts.	acoustic cryptanalysis;acoustic model;database trigger;insertion sort;kerrison predictor;language model;memory-level parallelism;podcast;quad flat no-leads package;semantic prosody;speech recognition;text corpus;text-based user interface;transcription (software);trigram	Wenzhu Shen;Roger Peng Yu;Frank Seide;Ji Wu	2009	2009 IEEE Workshop on Automatic Speech Recognition & Understanding	10.1109/ASRU.2009.5373365	natural language processing;speech recognition;computer science;speech;maximum a posteriori estimation;pattern recognition;acoustic model;predictive modelling;maximum likelihood;multilayer perceptron;speech synthesis;language model	NLP	-20.524129972955954	-86.33645455289823	63961
7a1c571effdd1b8c5e33b8ff644fbd16d6d879e7	cross-lingual acoustic modeling for indian languages based on subspace gaussian mixture models	sgmm cross lingual acoustic model;hidden markov models acoustics speech speech recognition data models vectors gaussian mixture model;sgmm;cross lingual acoustic model	Cross-lingual acoustic modeling using Subspace Gaussian Mixture Model for low-resource languages of Indian origin is investigated. Building acoustic model for a low-resource language with limited vocabulary by leveraging resources from another language with comparatively larger resources was focused upon. Experiments were done on Bengali and Tamil corpus from MANDI database, with Tamil having greater resources than Bengali. We observed that the word accuracy of cross-lingual acoustic model of Bengali was approximately 2.5% above it's CDHMM model and gave equivalent performance as it's monolingual SGMM model.	acoustic cryptanalysis;acoustic model;experiment;heart rate variability;subspace gaussian mixture model;vocabulary	Neethu Mariam Joy;Basil Abraham;K. Navneeth;S. Umesh	2014	2014 Twentieth National Conference on Communications (NCC)	10.1109/NCC.2014.6811282	natural language processing;speech recognition;computer science;pattern recognition	NLP	-18.173627429920717	-90.10883467023636	63967
3771f29f81ad2378bdb6b2b4e02712a549126e56	derivation of underlying valency frames from a learner's dictionary	underlying valency frame;derivation procedure design;bilingual research project;target lexicon;main focus;lexical data;english syntactic analysis;primary source;computer usable version;complex relation;target complementation paradigm;syntactic analysis	"""syntactic information. The lexicon and grammars, enriched by feedback from The authors collect lexical data for the parsed texts, can later be used a module of English syntactic analysis within t}~e machine translation system in the context of a bilingual research proper. project. The computer usable version of OA/JD (Hornby, 1974) is used as the At present, the pril~ry source of primary source. The main focus is on lexical data for the English analysis the structure and derivation of is a m~chine readable dictionary, valency frames for verbal entries in preprocessed to contain only relevant the target lexicon. Illustration of information in a transparent format. the complex relation between OALD's This paper foeusses on how valency verb subc~tegorization codes and the frames for verbal entries are target complementation paradigms is extra~ted from subcategorization codes provided, and an approach to the in the ,~chine readable dictionary. derivation procedure design suggested. 2. ~ CI{OICES 1. INTROD[b'~ION Even though the correspondences The present p a p e r describes a part between parallel text units can be of a larger project, which should reestablished at an arbitrary level sult in the extraction of lexical and starting from word forms up to an structural correspondences between elaborate logical representation, the grammatical units in large parallel practical solution seems to lie English and CVzech texts. The cortessomewhere in between. The approach we pondenees will then be used to build have chosen is based on the a transfer module for an Englishrepresentation of linguistic analysis -to-Czech (and possibly Czech-toin terms of underlyin~ (tectogram-English) machine translation system, metical) structures, which are Final as well as partial results determined by the given laaqgu~ge, but should also be useful as source data void of various irregularities of the for text-oriented lir~uistic research, surface strings, including the both hiand monolingual I . ~unbiguity of n~rp~mic and surface syntactic units. 2 A """"deeper"""" analysis This task entails the need for would increase the. risk of errors and tools to analyse unrestricted Czech introduce more theoretical bias while and English texts. In the first stage a very shallow level would require of the project the goal is to produce larger amounts of data to arrive at Czech and English lexicons of adequate simple facts when parallel text units coverage and implemented analysis are compared. g r a m m a r s , which will l a t e r be augmented with tools for preliminary The (underlying) syntactic disambiguation. The parser will build description is dependency-based (with annotated dependency structures, coordination and apposition as usable for tagging word forms, clauses relations of a different type) and the and sentences with morphological and project described here makes it ACRES DE COLING-92. NANTES, 23-28 AOOT 1992 5 5 3 PROC. OF COLING-92, NANTES, AUG. 23-28, 1992 possible (i) to test the basic assumptions of the theory on a large data collection, and (ii) to formulate an implementable relation between the surface string and the underlying representation. A constrained-based (unification) formalism was selected due to its declarativeness, conciseness and formal rigour, but its other interesting properties were a]so appreciated: i.a., the important role of the lexicon and the need I~ treat surface facts within the same rigoro~ f r a m e w o r k a.q deeper concep~. 3"""	code;dictionary;formal system;human-readable medium;jd - java decompiler;lexicon;machine translation;parallel text;parsing;primary source;rp (complexity);source data;unification (computer science);word-sense disambiguation	Alexandr Rosen;Eva Hajicová;Jan Hajic	1992			natural language processing;speech recognition;computer science;parsing;linguistics	NLP	-30.532419107491265	-81.59159632195973	64034
5fc27526d9137be221d8647b7b75f10d335df95b	comparison of adaptation methods for gmm-svm based speech emotion recognition	support vector machines;gaussian processes;mllr gmm svm based speech emotion recognition automatic emotion recognition emotion distinction short utterances model adaptation methods maximum a posteriori map maximum likelihood linear regression;emotion recognition;maximum likelihood estimation;support vector machines emotion recognition gaussian processes maximum likelihood estimation regression analysis speech recognition;speech emotion recognition support vector machines speech recognition databases adaptation models hidden markov models;mllr adaptation emotion recognition gmm supervector based svm map adaptation;speech recognition;regression analysis	The required length of the utterance is one of the key factors affecting the performance of automatic emotion recognition. To gain the accuracy rate of emotion distinction, adaptation algorithms that can be manipulated on short utterances are highly essential. Regarding this, this paper compares two classical model adaptation methods, maximum a posteriori (MAP) and maximum likelihood linear regression (MLLR), in GMM-SVM based emotion recognition, and tries to find which method can perform better on different length of the enrollment of the utterances. Experiment results show that MLLR adaptation performs better for very short enrollment utterances (with the length shorter than 2s) while MAP adaptation is more effective for longer utterances.	algorithm;emotion recognition;google map maker	Jianbo Jiang;Zhiyong Wu;Mingxing Xu;Jia Jia;Lianhong Cai	2012	2012 IEEE Spoken Language Technology Workshop (SLT)	10.1109/SLT.2012.6424234	support vector machine;speech recognition;computer science;machine learning;pattern recognition;gaussian process;maximum likelihood;regression analysis;statistics	NLP	-18.340302071064777	-90.93044039873915	64040
495ecb92903edbda3740e8bda346ef99876dd730	session 3: machine translation	statistical approach;statistical analysis;machine translation	In 1965 the United States Academy of Science cemmissoned a study of he state of the art in Machine Translation, whose findings were published the following year and become popularly known as the ALPAC report. In essence, ALPAC argued that there was insufficient scientific basis in natural langauge processing to perform reliable machine t~anslatinn, and the large expensive computers of the time would make NIT eeonornically infeasible. Both situations have since changed drastically, invaLidating the ALPAC conciusions. In fact, DARPA has played a major role in fostering the development of the NLP scientific infrsstmcture in the post-ALPAC years. IBM, in which the direct-transfer paradigm is still king and translation is viewed as transduction between two character (or word) s t reams--essent ia l ly two encodings of the same message. However, the direct transfer rules are totally learned by statistical analysis of large bi-lingual corpora, rather than laboriously and incompletely hand-coded. A drawback of the statistical approach, of course, is that it carmot guarantee the accuracy of any textual passage being translated, but rather strives to minimize the total number of errors over time.	alpac;academy;computer;machine translation;natural language processing;programming paradigm;text corpus;transduction (machine learning)	Jaime G. Carbonell	1991			natural language processing;example-based machine translation;computer science;machine translation;statistics	NLP	-25.50814509693244	-81.37998905737194	64053
b17de22d32ab99bd2c8e6a1cd45d0ff48b50b128	iterative deep neural networks for speaker-independent binaural blind speech separation		In this paper, we propose an iterative deep neural network (DNN)-based binaural source separation scheme, for recovering two concurrent speech signals in a room environment. Besides the commonly-used spectral features, the DNN also takes non-linearly wrapped binaural spatial features as input, which are refined iteratively using parameters estimated from the DNN output via a feedback loop. Different DNN structures have been tested, including a classic multilayer perception regression architecture as well as a new hybrid network with both convolutional and densely-connected layers. Objective evaluations in terms of PESQ and STOI showed consistent improvement over baseline methods using traditional binaural features, especially when the hybrid DNN architecture was employed. In addition, our proposed scheme is robust to mismatches between the training and testing data.		Qingju Liu;Yong Xu;Philip J. B. Jackson;Wenwu Wang;Philip Coleman	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462603	artificial neural network;artificial intelligence;architecture;feature extraction;computer science;test data;source separation;binaural recording;pesq;pattern recognition	Robotics	-15.574230077579882	-90.62979877448073	64066
92a0433418756e9c243a8789a36a1e129a5a9c46	an approach to common acoustical pole and zero modeling of consecutive periods of voiced speech.		In this paper the open and closed phases within a speech period are separately modeled as acoustical pole-zero filters. We approach the estimation of the coefficients associated to the poles and zeros by minimizing a cost function based on the reconstruction error. The cost function leads to a matrix formulation of the error for two time intervals where the error must be defined. This defines a framework that facilitates to model the phases associated to consecutive periods. We give a matrix formulation of the estimation process that let us to attain two main objectives. Firstly, estimate the common-pole structure of several consecutive periods and their particular zero structure. And secondly, estimate their common-pole-zero structure. The experiments are carried out over a speech database of five men and five women. The experiments are done in terms of the reconstruction error and its dependence on the period length and the order of the analysis.	coefficient;experiment;loss function;matrix mechanics;non-functional requirement;speech corpus	Pedro J. Quintana-Morales;Juan L. Navarro-Mesa	2003			artificial intelligence;speech recognition;pattern recognition;computer science	Vision	-8.225813119747519	-88.8198071713253	64093
5163a44c47682be31b2a6b138413569a1c233f49	pitch determination and voice quality analysis using subharmonic-to-harmonic ratio	pitch determination algorithm;voice quality;frequency estimation;spectrum;personal digital assistants variable speed drives frequency estimation;personal digital assistants;variable speed drives	This paper presents an improvement of a previously proposed pitch determination algorithm (PDA). Particularly aiming at handling alternate cycles in speech signal, the algorithm estimates pitch through spectrum shifting on logarithmic frequency scale and calculating the Subharmonic-to-Harmonic Ratio (SHR). The evaluation results on two databases show that this algorithm performs considerably better than other PDAs compared. Application of SHR to voice quality analysis task is also presented. The implementation and evaluation routines are available from &#60;http://mel.speech.nwu.edu/sunxj/pda.htm>.	algorithm;database;personal digital assistant;shr	Xuejing Sun	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5743722	spectrum;speech recognition;phonation;computer science	Robotics	-9.080989658495218	-87.3384916711405	64098
cf36b77796b25e240cc3f5d1a91d4f0bea8a3224	speech recognition under musical environments using kalman filter and iterative mllr adaptation.	kalman filter;speech recognition	In this paper, we propose a speech recognition method under non-stationary musical environments using Kalman ltering speech signal estimation method and iterative unsupervised MLLR(Maximum Likelihood Linear Regression) adaptation. Our proposing method estimates the speech signal under non-stationary noisy environments such a s m usical background by applying speech state transition model to Kalman ltering estimation. The speech state transition model represents the state transition of speech component in non-stationary noisy speech and is modeled by using Taylor expansion. In this model, the state transition of noise component is estimated by using linear predictive estimation. Furthermore , to obtain higher recognition accuracy, w e consider to adapt the acoustic models by using iterative unsuper-vised MLLR adaptation to speech spectra distorted by Kalman ltering residual noise. In order to evaluate the proposed method, we carried out large vocabulary continuous speech recognition experiments under 3 types of music. As a result, the proposed method obtained the signiicant improvement in word accuracy, from 20.04% to 64.43% at 0dB SNR.	acoustic cryptanalysis;acoustic model;distortion;experiment;iteration;iterative method;kalman filter;signal-to-noise ratio;speech recognition;state transition table;stationary process;vocabulary	Masakiyo Fujimoto;Yasuo Ariki	2001			kalman filter;speech recognition;computer science;pattern recognition	ML	-13.807590779443004	-93.44888365140794	64396
c7e973bec61afde89ec64b1a61746447a4b2575b	robust recognition of noisy speech through partial imputation of missing data		Two main categories of speech recognition robustness through missing data are spectral imputation and classifier modification. In this paper, we introduce a novel technique that could combine methods from these two categories while improving the accuracy of the combined methods. Methods in these two categories are rarely employed together due to their incompatible structures. Based on our previous work, we propose a technique to solve the problem of incompatibility. The technique is based on the idea of partial restoration of the log-spectrum. We decide to whether restore or estimate a possible range for the missing component. We also propose a method to more effectively employ dynamic features. The combined techniques are a classic spectral imputation method and our previously proposed classifier modification technique, namely spectral variance learning. The experiments show that the proposed technique is able to improve the accuracies of both combined techniques significantly, leading to improvements in recognition accuracy as high as nearly four percent on Aurora 2.0 data and more than two percent on a noisy version of TIMIT data.	geo-imputation;missing data	Kian Ebrahim Kafoori;Seyed Mohammad Ahadi	2018	CSSP	10.1007/s00034-017-0616-4	timit;missing data;robustness (computer science);imputation (statistics);machine learning;artificial intelligence;pattern recognition;computer science	ML	-14.384949784488667	-92.10488682330379	64402
9790553f29e25aec0566ba1eae74202ff7c7ec45	a percussive sound synthesizer based on physical and perceptual attributes	perceptual attributes;percussive sound synthesizer	32 Computer Music Journal Synthesis of impact sounds is far from a trivial task owing to the high density of modes generally contained in such signals. Several authors have addressed this problem and proposed different approaches to model such sounds. The majority of these models are based on the physics of vibrating structures, as with for instance modal synthesis (Adrien 1991; Pai et al. 2001; van den Doel, Kry, and Pai 2001; Cook 2002; Rocchesso, Bresin, and Fernström 2003). Nevertheless, modal synthesis is not always suitable for complex sounds, such as those with a high density of mixed modes. Other approaches have also been proposed using algorithmic techniques based on digital signal processing. Cook (2002), for example, proposed a granularsynthesis approach based on a wavelet decomposition of sounds. The sound-synthesis model proposed in this article takes into account both physical and perceptual aspects related to sounds. Many subjective tests have shown the existence of perceptual clues allowing the source of the impact sound (its material, size, etc.) to be identified merely by listening (Klatzky, Pai, and Krotkov 2000; Tucker and Brown 2002). Moreover, these tests have brought to the fore some correlations between physical attributes (the nature of the material and dimensions of the structure) and perceptual attributes (perceived material and perceived dimensions). Hence, it has been shown that the perception of the material mainly correlates with the damping coefficient of the spectral components contained in the sound. This damping is frequency-dependent, and highfrequency modes are generally more heavily damped than low-frequency modes. Actually, the dissipation of vibrating energy owing to the coupling between the structure and the air increases with frequency (see, for example, Caracciolo and Valette 1995). To take into account this fundamental sound behavior from a synthesis point of view, a timevarying filtering technique has been chosen. It is well known that the size and shape of an object’s attributes are mainly perceived by the pitch of the generated sound and its spectral richness. The perception of the pitch primarily correlates with the vibrating modes (Carello, Anderson, and KunklerPeck 1998). For complex structures, the modal density generally increases with the frequency, so that high frequency modes overlap and become indiscernible. This phenomenon is well known and is described for example in previous works on room acoustics (Kuttruff 1991). Under such a condition, the human ear determines the pitch of the sound from emergent spectral components with consistent frequency ratios. When a complex percussive sound contains several harmonic or inharmonic series (i.e., spectral components that are not exact multiples of the fundamental frequency), different pitches can generally be heard. The dominant pitch then mainly depends on the frequencies and the amplitudes of the spectral components belonging to a so-called dominant frequency region (Terhardt, Stoll, and Seewann 1982) in which the ear is pitch sensitive. (We will discuss this further in the Tuning section of this article.) With all these aspects in mind, and wishing to propose an easy and intuitive control of the model, we have divided it into three parts represented by an excitation element, a material element, and an object element. The large number of parameters available through such a model necessitates a control strategy. This strategy (generally called a mapping) is of great importance for the expressive capabilities of the instrument, and it inevitably influences the way it can be used in a musical context (Gobin et al. 2004). A Percussive Sound Synthesizer Based on Physical and Perceptual Attributes	algorithm;computer music journal;content-control software;control theory;digital signal processing;emergence;html element;midi;matthews correlation coefficient;mind;modal logic;morphing;nonlinear system;overlap–add method;pitch (music);real-time clock;real-time computing;relevance;semiotics;tucker decomposition;velocity (software development);wavelet	Mitsuko Aramaki;Richard Kronland-Martinet;Thierry Voinier;Sølvi Ystad	2006	Computer Music Journal	10.1162/comj.2006.30.2.32	speech recognition	Graphics	-7.849249874485955	-86.23170379619053	64429
b864826b6ecde96c0d118d40fbbad683629ba7c4	a stochastic speech coder with multi-band long-term prediction			speech coding	Carmen García-Mateo;José Luis Alba-Castro;Luis A. Hernández Gómez	1993			artificial intelligence;speech recognition;long-term prediction;pattern recognition;computer science	NLP	-13.877608567641053	-87.26560267389965	64490
1af774f04af13cfb51fa638764af89f9536d4f69	three algorithms for word-to-phrase machine translation	databases;lexical gap;magnetic heads;english vietnamese machine translation;language translation;presses;target language phrases;data mining;word to phrase machine translation;natural language processing knowledge based systems language translation;computational modeling;word to phrase translation;pruning algorithm;target language;dictionaries;source language;cities and towns;natural languages dictionaries computer science frequency training data databases helium books;pruning algorithm word to phrase machine translation lexical gap solving model source language target language phrases english vietnamese machine translation rule based machine translation;rule based machine translation;word to phrase translation english vietnamese machine translation lexical gap;lexical gap solving model;natural language processing;knowledge based systems;machine translation	Word-to–phrase machine translation (WPMT) is a model for lexical gap solving. Generally, lexical gap is an absent of word in a particular language. In a word-to-phrase model, words in source language could be replaced by target language phrases. This approach is very helpful for English- Vietnamese rule-based machine translation. The phrase replacement changes target sentence to unpredictable structure. This work develops three algorithms for WPMT: The insert algorithm applies phrase replacement, pruning algorithm simplifies target structure and moving algorithm keeps correct order of components in Vietnamese sentence.	algorithm;compiler;logic programming;microsoft word for mac;rule-based machine translation	Le Manh Hai;Phan Thi Tuoi	2009	2009 International Conference on Asian Language Processing	10.1109/IALP.2009.77	natural language processing;speech recognition;transfer-based machine translation;example-based machine translation;computer science;linguistics;machine translation;rule-based machine translation	NLP	-22.950473450029282	-81.8401275841943	64568
e2c41f4e5b3c5feea612671351901bffc3ad7bf4	spoken document retrieval: sub-sequence dtw framework and variants	phonetic audio search;audio search;spoken document retrieval;quantized search;sub sequence dtw	We address the problem of spoken document retrieval alternately termed content-based audio-search and retrieval, which involves searching a large spoken document or database for a specific spoken query. We formulate the search within the sub-sequence DTW SS-DTW framework proposed earlier in literature, adapted here to work on acoustic feature representation of the database and spoken query term. Further, we propose several variants within this framework, such as i path-length based score normalization, ii clustered quantization of acoustic feature vectors for fast search and retrieval with invariant performances and, iii phonetic representation of the database and spoken query term, derived from ground-truth annotation as well as HMM based continuous phoneme recognition. We characterize the performance of the proposed framework, algorithms and variants in terms of ROC curves, EER and time-complexity and present results using the TIMIT database with annotated spoken sentences from 400 speakers.	document retrieval	Akshay Khatwani;Komala Pawar;Sushma Hegde;Sudha Rao;Adithya Seshasayee;V. Ramasubramanian	2015		10.1007/978-3-319-26832-3_29	natural language processing;speech recognition;computer science;pattern recognition	Web+IR	-8.835186353958925	-93.26335065441727	64615
7c5c5566ea9167c3d9dd93a6d6f41c7b39c0d6e6	on the adaptation of foreign language speech recognition engines for lithuanian speech recognition	human resource;model development;speech recognition;foreign language	This paper presents some of our activities trying to adapt the foreign language based speech recognition engines for the recognition  of the Lithuanian speech commands. In recent years several quiet successful speech recognition engines became available for  the most popular languages (such as English, French, Spanish, German, etc.). The speakers of a less widely used languages  (such as Lithuanian) have several choices: to develop own speech recognition engines or to try adapting speech recognition  models developed and trained for the foreign languages to the task of recognition of their native spoken language. First approach  is expensive in time, financial and human resources sense. Second approach can lead to faster implementation of Lithuanian  speech recognition modules into some practical tasks but proper adaptation and optimization procedures should be found and  investigated.  	speech recognition	Vytautas Rudzionis;Rytis Maskeliūnas;Algimantas Rudzionis;Kastytis Ratkevicius	2009		10.1007/978-3-642-03424-4_13	foreign language;natural language processing;speech technology;cued speech;speech recognition;speech corpus;human resources;computer science	NLP	-22.176295624633102	-85.47816086965385	64648
cf38adaa0136a5f37484a67e1315c056626d9802	eigenvoice modelling for cross likelihood ratio based speaker clustering: a bayesian approach	speaker clustering;cross likelihood ratio;speaker diarization;joint factor analysis;eigenvoice modelling	This paper proposes the use of Bayesian approaches with the cross likelihood ratio (CLR) as a criterion for speaker clustering ithin a speaker diarization system, using eigenvoice modelling techniques. The CLR has previously been shown to be an effective ecision criterion for speaker clustering using Gaussian mixture models. Recently, eigenvoice modelling has become an increasingly opular technique, due to its ability to adequately represent a speaker based on sparse training data, as well as to provide an improved apture of differences in speaker characteristics. The integration of eigenvoice modelling into the CLR framework to capitalize on he advantage of both techniques has also been shown to be beneficial for the speaker clustering task. Building on that success, his paper proposes the use of Bayesian methods to compute the conditional probabilities in computing the CLR, thus effectively ombining the eigenvoice-CLR framework with the advantages of a Bayesian approach to the diarization problem. Results obtained n the 2002 Rich Transcription (RT-02) Evaluation dataset show an improved clustering performance, resulting in a 33.5% relative mprovement in the overall diarization error rate (DER) compared to the baseline system. 2012 Elsevier Ltd. All rights reserved.	akaike information criterion;baseline (configuration management);cluster analysis;medical transcription;mixture model;sparse matrix;speaker diarisation	David Wang;Robbie Vogt;Sridha Sridharan	2013	Computer Speech & Language	10.1016/j.csl.2012.12.001	speaker diarisation;speech recognition;computer science;machine learning;pattern recognition	AI	-17.337340331934097	-91.60739581700054	64711
686f8c58c627f1a310b84d75a9c1b2b519be489b	acoustic packaging: maternal speech and action synchrony	infant directed speech;maternal speech;biology computing;utterance classification;pediatrics;bioacoustics;multimodal communication;acoustics;speech processing;maternal action;semantics;biomechanics;attention getting utterances;speech;speech coding;low pass filter;action descriptions;packaging;nonaction descriptions;speech acoustics packaging semantics speech recognition humans robots;speech processing action processing multimodal communication pediatrics;paediatrics;acoustic packaging;speech coding bioacoustics biology computing biomechanics low pass filters paediatrics speech;global semantic messages;robots;age 0 5 yr to 1 08 yr acoustic packaging maternal speech maternal action action descriptions nonaction descriptions attention getting utterances global semantic messages infant directed speech low pass filter utterance classification;action unit;speech recognition;age 0 5 yr to 1 08 yr;low pass filters;humans;action processing	The current study addressed the degree to which maternal speech and action are synchronous in interactions with infants. English-speaking mothers demonstrated the function of two toys, stacking rings and nesting cups to younger infants (6-9.5 months) and older infants (9.5-13 months). Action and speech units were identified, and speech units were coded as being ongoing action descriptions or nonaction descriptions (examples of nonaction descriptions include attention-getting utterances such as “Look!” or statements of action completion such as “Yay, we did it!”). Descriptions of ongoing actions were found to be more synchronous with the actions themselves in comparison to other types of utterances, suggesting that: 1) mothers align speech and action to provide synchronous “acoustic packaging” during action demonstrations; and 2) mothers selectively pair utterances directly related to actions with the action units themselves rather than simply aligning speech in general with actions. Our results complement past studies of acoustic packaging in two ways. First, we provide a quantitative temporal measure of the degree to which speech and action onsets and offsets are aligned. Second, we offer a semantically based analysis of the phenomenon, which we argue may be meaningful to infants known to process global semantic messages in infant-directed speech. In support of this possibility, we determined that adults were capable of classifying low-pass filtered action- and nonaction-describing utterances at rates above chance.	acoustic cryptanalysis;action potential;align (company);cups;interaction;low-pass filter;offset (computer science);stacking;toys	Meredith Meyer;Bridgette Hard;Rebecca J. Brand;Molly McGarvey;Dare A. Baldwin	2011	IEEE Transactions on Autonomous Mental Development	10.1109/TAMD.2010.2103941	speech recognition;low-pass filter;computer science;biomechanics;speech processing;semantics;communication	NLP	-8.716783826405553	-80.90223861007543	64718
d92a4b80de746c589645573ede1e226fd03e1476	on the use of data-driven clustering technique for identification of poly- and mono-phonemes for four european languages	natural languages speech recognition speech processing;speech processing;natural languages;limit set;natural languages speech acoustic testing databases labeling acoustic measurements telephony performance evaluation linear predictive coding cepstral analysis;language identification;speech recognition;language discriminators data driven clustering technique mono phonemes poly phonemes european languages functionality language identification acoustically similar speech sounds danish german italian british english language independent poly phonemes multi lingual training base labelling recognition systems similarity conditions	The research reported in this paper presents a method to identify polyand mono-phonemes for four European languages. The functionality of the poly-phonemes is tested in two experiments, and a limited set of mono-phonemes is identified for a languageidentification experiment. Ten acoustically-similar speech sounds were identified across the four languages British-English, Danish, German, and Italian. These sounds. which constitute a substantial proportion of the phonemes of each language. are designated as (language independent) poly-phonemes, and may serve as a multi-lingual training base for labelling and recognition systems. The remaining sounds of each language, which do not fulfill the similarity conditions, are dubbed mono-phonemes. Two application experiments were conducted. In the first the poly-phonemes are applied in a label alignment task. In the second a small selected of mono-phonemes for each of the four languages IS used in a preliminary test of the abihty of these sets to serve as language discriminators.	cluster analysis;experiment	Ove Andersen;Paul Dalsgaard;William J. Barry	1994		10.1109/ICASSP.1994.389340	natural language processing;language identification;limit set;cued speech;speech recognition;computer science;speech;speech processing;acoustic model;natural language	NLP	-21.339717632477534	-85.12109359142427	64802
22ed5944940ca68af59c4dd8b5ff6aa04f47922c	speech in noisy environments: robust automatic segmentation, feature extraction, and hypothesis combination	hypothesis combination;front end;design decisions;filtering;noise compensation algorithms;automatic segmentation;working environment noise;speech analysis;testing;speech enhancement;digital filters speech recognition compensation cepstral analysis acoustic noise feature extraction;noise robustness;military settings;spinel;robust automatic segmentation;cepstral analysis;compensation;noise level;robust mel scale filtering;system design;feature extraction;acoustic noise;digital filters;parallel front end features;speech recognition;design decisions robust automatic segmentation feature extraction hypothesis combination speech in noisy environments spinel speech recognition military settings session adaptive segmentation robust mel scale filtering cepstra parallel front end features noise compensation algorithms parallel hypotheses combination word graphs;parallel hypotheses combination;word graphs;working environment noise noise robustness speech analysis military computing testing speech recognition speech enhancement noise level algorithm design and analysis filtering;algorithm design and analysis;military computing;speech in noisy environments;cepstra;session adaptive segmentation	The purpose of the evaluation was to test existing core speech recognition technologies for speech in the presence of varying types and levels of noise. In this case the noises were taken from military settings. Among the strategies used by Carnegie Mellon University's successful systems designed for this task were session-adaptive segmentation, robust mel-scale filtering for the computation of cepstra, the use of parallel front-end features and noise-compensation algorithms, and parallel hypotheses combination through word-graphs. This paper describes the motivations behind the design decisions taken for these components, supported by observations and experiments.	algorithm;computation;experiment;feature extraction;mel scale;speech recognition	Rita Singh;Michael L. Seltzer;Bhiksha Raj;Richard M. Stern	2001		10.1109/ICASSP.2001.940820	filter;algorithm design;speech recognition;digital filter;feature extraction;computer science;electrical engineering;front and back ends;machine learning;noise;pattern recognition;software testing;systems design	AI	-13.380624158839257	-90.48338149126657	64833
31bd2e0e74083ff74894f93ebd8f9c0cb6a2aa06	integrating additional chord information into hmm-based lyrics-to-audio alignment	audio user interfaces;audio signal processing;instruments;user interface;music performance;hidden markov model;information retrieval;speech processing;inference mechanisms;statistical significance;indexing terms;materials;automatic generation;speech processing audio signal processing hidden markov models inference mechanisms information retrieval music;hidden markov models feature extraction mel frequency cepstral coefficient internet instruments materials;mel frequency cepstral coefficient;hidden markov models;internet;speech processing audio user interfaces hidden markov models hmms music music information retrieval;song prompter chord information hmm lyrics to audio alignment karaoke scores song browsing audio thumbnails generation phoneme feature mel frequency cepstral coefficient inference procedure hidden markov model markov chain emission vector;feature extraction;music information retrieval;hidden markov models hmms;music;markov chain	Aligning lyrics to audio has a wide range of applications such as the automatic generation of karaoke scores, song-browsing by lyrics, and the generation of audio thumbnails. Existing methods are restricted to using only lyrics and match them to phoneme features extracted from the audio (usually mel-frequency cepstral coefficients). Our novel idea is to integrate the textual chord information provided in the paired chords-lyrics format known from song books and Internet sites into the inference procedure. We propose two novel methods that implement this idea: First, assuming that all chords of a song are known, we extend a hidden Markov model (HMM) framework by including chord changes in the Markov chain and an additional audio feature (chroma) in the emission vector; second, for the more realistic case in which some chord information is missing, we present a method that recovers the missing chord information by exploiting repetition in the song. We conducted experiments with five changing parameters and show that with accuracies of 87.5% and 76.7%, respectively, both methods perform better than the baseline with statistical significance. We introduce the new accompaniment interface Song Prompter, which uses the automatically aligned lyrics to guide musicians through a song. It demonstrates that the automatic alignment is accurate enough to be used in a musical performance.	audio signal processing;baseline (configuration management);book;chroma subsampling;coefficient;color gradient;database;experiment;feature extraction;hidden markov model;informatics;internet;keyboard shortcut;markov chain;mel-frequency cepstrum;multimodal interaction;phrase structure rules;preprocessor;thumbnail;viterbi decoder	Matthias Mauch;Hiromasa Fujihara;Masataka Goto	2012	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2011.2159595	markov chain;speech recognition;audio signal processing;feature extraction;computer science;machine learning;pattern recognition;music;statistical significance;user interface;hidden markov model;statistics	ML	-21.269653771982767	-83.32883977282684	64908
3c1419f591851257db577493daa95b959094d2da	on the state definition for a trainable excitation model in hmm-based speech synthesis	analysis by synthesis;state dependent filters;speech synthesis hidden markov models speech coding;trainable excitation model;bottom up;decision tree;speech synthesis;hidden markov model;speech processing;speech coding;state dependent filters trainable excitation model speech synthesis hidden markov models analysis by synthesis speech coders;state dependence;indexing terms;digital filter;filter design;hmm based speech synthesis;analysis by synthesis speech coders;hidden markov models;digital filters;hidden markov models speech synthesis synthesizers speech analysis filters decision trees databases signal synthesis merging natural languages;digital filters speech processing speech synthesis hidden markov models	One of the issues of speech synthesizers based on hidden Markov models concerns the vocoded quality of the synthesized speech. From the principle of analysis-by-synthesis speech coders a trainable excitation model has been proposed to improve naturalness, where the method consists in the design of a set of state-dependent filters in a way to minimize the distortion between residual and synthetic excitation. Although this approach seems successful, state definition still represents an open issue. This paper describes a method for state definition wherein bottom-up clustering is performed on full context decision trees, using the likelihood of the residual database as merging criterion. Experiments have shown that improvement on residual modeling through better filter design can be achieved.	bottom-up proteomics;cluster analysis;decision tree;distortion;filter design;hidden markov model;markov chain;speech coding;speech synthesis;synthetic intelligence;vocoder	Ranniery Maia;Tomoki Toda;Keiichi Tokuda;Shinichi Sakai;Shun Nakamura	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518522	natural language processing;speech recognition;digital filter;computer science;speech coding;pattern recognition;hidden markov model	Robotics	-15.37371296548882	-93.41290259152586	65069
3a9fa4d6010b3ce67f38241923278206203aac5a	out-of-domain detection method based on sentence distance for dialogue systems		For dialogue systems, it is critical to detect the out-of-domain (OOD) utterances in a conversation. We detect OOD sentences occurring in a dialogue based on sentence distances. The sentence distances are measured by sentence embedding vectors using RNN(Recurrent Neural Network) encoders with the attention mechanism. Our approach improves the accuracy of the out-of-domain detection(OOD) method, and we apply this method to develop a chatbot system for customer services.	dialog system;encoder;recurrent neural network;the sentence	KyoJoong Oh;Dongkun Lee;Chan Yong Park;Young-Seob Jeong;Sawook Hong;Sungtae Kwon;Ho-Jin Choi	2018	2018 IEEE International Conference on Big Data and Smart Computing (BigComp)	10.1109/BigComp.2018.00123	conversation;encoder;chatbot;semantics;feature extraction;recurrent neural network;computer science;pattern recognition;artificial intelligence;vocabulary;sentence	Robotics	-17.84587683840141	-84.20273637291054	65110
964b37c9c7055830017b61929af843eafbd5d171	a pitch determination algorithm based on subharmonic-to-harmonic ratio	spectrum	In the present paper, a pitch determination algorithm (PDA) based on Subharmonic-to-Harmonic Ratio (SHR) is proposed. The algorithm is motivated by the results of a recent study on the perceived pitch of alternate pulse cycles in speech [1]. The algorithm employs a logarithmic frequency scale and a spectrum shifting technique to obtain the amplitude summation of harmonics and subharmonics, respectively. Through comparing the amplitude ratio of subharmonics and harmonics with the pitch perception results, the pitch of normal speech as well as speech with alternate pulse cycles (APC) can be determined. . Evaluation of the algorithm is performed on CSTR’s database and on synthesized speech with APC. The results show that this algorithm is one of the most reliable PDAs. Furthermore, superior to most other algorithms, it handles subharmonics reasonably well.	algorithm;personal digital assistant;pitch (music);shr;speech synthesis	Xuejing Sun	2000			speech recognition;control theory;subharmonic;pitch detection algorithm;audio time-scale/pitch modification;computer science;harmonic	NLP	-9.567361576829018	-86.89136552616175	65227
fd58d4108b4bc2899f60a4efa1c00d5deeb60d8d	dialing for documents: an experiment in information theory	text entry;hearing impaired;information theoretic;information theory	Abstract   Standard telephone keypads are labeled with letters of the alphabet, enabling users to enter textual data for a variety of possible applications. However, the overloading of three letters on a single key creates a potential ambiguity as to which character was intended, which must be resolved for unambiguous text entry. Existing systems all use pairs of keypresses to spell out single letters, but are extremely cumbersome and frustrating to use.  Instead, we propose single-stroke text entry on telephone keypads, with the ambiguity resolved by exploiting information-theoretic constraints. We develop algorithms capable of correctly identifying up to 99% of the characters in typical English text, sufficient for such applications as telephones for the hearing-impaired, E-mail without a terminal and advanced voice-response systems.	information theory	Harald Rau;Steven Skiena	1996	J. Vis. Lang. Comput.	10.1006/jvlc.1996.0005	natural language processing;speech recognition;information theory;computer science;database	NLP	-25.278889378174874	-82.42577067264075	65299
0eec417d1d3e9aa3dd60d17d1a03999e9f7e027f	pronominalization in generated discourse and dialogue	various domain;simple method;complex theoretical mechanism;multi-page generation system;actual generation;satisfying explanatory power;previous approach;anaphoric pronoun;centering theory;noun	Previous approaches to pronominalization have largely been theoretical rather than applied in nature. Frequently, such methods are based on Centering Theory, which deals with the resolution of anaphoric pronouns. But it is not clear that complex theoretical mechanisms, while having satisfying explanatory power, are necessary for the actual generation of pronouns. We first illustrate examples of pronouns from various domains, describe a simple method for generating pronouns in an implemented multi-page generation system, and present an evaluation of its performance.	algorithm;anaphora (linguistics);computational linguistics;definition;dialog system;natural language generation;requirement;single-instance storage;text corpus;the new york times	Charles B. Callaway;James C. Lester	2002			natural language processing;noun;personal pronoun;computer science;linguistics	NLP	-32.68407744859923	-81.70811866162077	65405
7628b6699a9eeb9e8336b79dae9897fc28a8f956	speech inversion and re-synthesis		Inverse problems with respect to parameters of the articulatory model are solved for all types of sounds: vowels, semi-vowels, nasals, stops and fricatives in various contexts. Acoustical parameters of the speech signal and trajectories of some reference points inside the vocal tract serve as input data. 3.7%, 3.8% and 2.6% average approximation error for the first three formants, 8.5% for the specific frequencies of fricative spectra, 2.8% for the coordinates of reference points for all kinds of phonemes are obtained when both – acoustic and articulatory data are used. 1.8%, 1.6%, and 1.1% error for the first three formant frequencies, and 6% for the coordinates of reference points are obtained when only acoustic data are used. Original and re-synthesized utterances are found to be very similar in appearance, according to subjective assessment.	acoustic cryptanalysis;approximation error;semiconductor industry;tract (literature);voice inversion	Victor N. Sorokin;Alexander S. Leonov;I. S. Makarov;A. I. Tsyplikhin	2005			artificial intelligence;speech recognition;pattern recognition;computer science;inversion (meteorology)	Visualization	-9.281976093933356	-86.2461175667671	65462
76e7b6a15ab8754c071bae71826a2b5bdc2c10ad	effects of talker variability on speech perception: implications for current research and theory	speech perception		code talker;heart rate variability	David B. Pisoni	1990			motor theory of speech perception;speech recognition;speech perception;computer science	HCI	-8.984137613544359	-83.32976774239992	65562
cefd6c9ace22a5204c4c5041fa4ecdc86696dca3	a symbolic descriptive system for facial expression conveying linguistic information in signing		"""The kinds of change in facial expression which can be produced by a sender's neural control of facial muscles, and, at the same time, can be visually perceived by a listener are described in discrete symbols. The linguistic information supplemented by the facial expressions in signing is categorized into syllabic, lexical and syntactic components. In order to control the facial expressions for this linguistic information, the timing and duration of the symbols are specified, and, by applying a set of concatenation rules, the control commands are generated. The adequacy of the symbolic descriptive system and its control rules have been examined through a computer synthesis of artificial facial expression, taking the example of transmitting syntactic information in traditional Japanese sign language. These facial expressions are combined with the symbolic descriptive system for hand shapes and arm actions and their control rules for the pictorial display of signing gestures. 1. SYMBOLIC DESCRIPTION 1. 1 . Elements of Facial Expression The kinds of change in facial expression which can be produced by a sender's neural control of facial muscles [References 1 and 2], and, at the same time, can be perceived visually by a listener, are described in discrete symbols. Parameters used for the element of facial expression are as follows: 1. Opening and closing of the eyelids 2. the eye direction 3. raising, knitting and lowering of the eyebrows 4. opening of the nostrils 5. opening or closing of the mouth 6. protrusion or corner pull of the lips 7. popping up or drawing of the chin and 8. nod or shake of the head The correspondence between the parameters of the elements and their symbols in a symbolic ELEMENTS SYMBOLS PARAMETERS EYELIDS: NEUTRAL EL0 WIDELY OPENED EL+ NARROWED, OR ELCLOSED EL-EYE DIRECTION: NEUTRAL EY0 DIRECTLY UP EYu+, u++ DOWN EYd+, d++ LEFT, OR EYl+, l++ RIGHT EYr+, r++ EACH IN TWO STEPS DIRECT TO A PARTICULAR POINT EYxy EYEBROWS: NEUTRAL EB0 THE INSIDES RAISED, OR EBi+, i++ KNITTED EBi-, i-THE LATERAL SIDES RAISED, OR EBl+, l++ LOWERED EBl-, l-EACH IN TWO STEPS. NOSTRILS: NEUTRAL NR0 WIDELY OPENED NR+ MOUTH: NEUTRAL MO0 WIDELY OPENED MO+ NARROWED, OR MOCLOSED MO-THE LIPS PROTRUDED, OR LP+, ++ THE CORNERS PULLED LATERALLY LP-, -EACH IN TWO STEPS CHIN: NEUTRAL CN0 POPPING UP, OR CN+, ++ DRAWING IN THE CHIN CN+, ++ EACH IN TWO STEPS HEAD: NEUTRAL HD0 NOD, OR HDN SHAKE. HDS TILT FORWARD, OR HD+, ++ BACKWARD HD-, -EACH IN TWO STEPS Tabl e 1 : Correspondence between the parameters of elements and their symbols in a symbolic descriptive system of the facial expressions. ! """" # $ ISCA Archive %&&'''( ( & )"""	archive;categorization;closing (morphology);combinatorial chemistry;concatenation;eset nod32 antivirus;holographic data storage;image;international symposium on computer architecture;lateral computing;neutral monism;opening (morphology);r++;shake;transmitter	Kazuya Imaizumi;Shizuo Hiki;Yumiko Fukuda	1998			communication;facial expression;rule-based machine translation;computer science	AI	-5.040475033609701	-80.26477613098257	65659
1b397b4bf076b608800855a2564d25b98ef3e076	a tool for generating and explaining expressive music performances of monophonic jazz melodies	music performance;machine learning;expressive performance	In this paper we present a machine learning approach to modeling the knowledge applied by a musician when performing a score in order to produce an expressive performance of a piece. We describe a tool for both generating and explaining expressive music performances of monophonic Jazz melodies. The tool consists of three components: (a) a melodic transcription component which extracts a set of acoustic features from monophonic recordings, (b) a machine learning component which induce both an expressive transformation model and a set of expressive performance rules from the extracted acoustic features, and (c) a melody synthesis component which generates expressive monophonic output (MIDI or audio) from inexpressive melody descriptions using the induced expressive transformation model. We compare several machine learning techniques we have explored for inducing the expressive transformation model.	acoustic cryptanalysis;emoticon;jazz (computer);logic programming;midi;machine learning;nl (complexity);natural language;numerical aperture;performance;programming style;sensor;transcription (software)	Rafael Antonio Márquez Ramírez;Amaury Hazan	2006	International Journal on Artificial Intelligence Tools	10.1142/S0218213006002862	natural language processing;speech recognition;computer science;machine learning	AI	-15.673892567975843	-83.83841520324921	65682
8390822ae5ab4c1497852580086ba8e555ae5d92	improved hidden markov models speech recognition using radial basis function networks	hidden markov model;radial basis function network;speech recognition	A high performance speaker-independent isolated-word hybrid speech recognizer was developed which combines Hidden Markov Models (HMMs) and Radial Basis Function (RBF) neural networks. In recognition experiments using a speaker-independent E-set database, the hybrid recognizer had an error rate of 11.5% compared to 15.7% for the robust unimodal Gaussian HMM recognizer upon which the hybrid system was based. These results and additional experiments demonstrate that RBF networks can be successfully incorporated in hybrid recognizers and suggest that they may be capable of good performance with fewer parameters than required by Gaussian mixture classifiers. A global parameter optimization method designed to minimize the overall word error rather than the frame recognition error failed to reduce the error rate. 1 HMM/RBF HYBRID RECOGNIZER A hybrid isolated-word speech recognizer was developed which combines neural network and Hidden Markov Model (HMM) approaches. The hybrid approach is an attempt to capitalize on the superior static pattern classification performance of neural network classifiers [6] while preserving the temporal alignment properties of HMM Viterbi decoding. Our approach is unique when compared to other studies [2, 5] in that we use Radial Basis Function (RBF) rather than multilayer sigmoidal networks. RBF networks were chosen because their static pattern classification performance is comparable to that of other networks and they can be trained rapidly using a one-pass matrix inversion technique [8] . The hybrid HMM/RBF isolated-word recognizer is shown in Figure 1. For each 159 160 Singer and Lippmann	artificial neural network;experiment;finite-state machine;global optimization;hidden markov model;hybrid system;markov chain;mathematical optimization;radial (radio);radial basis function network;sigmoid function;speech recognition;viterbi algorithm	Elliot Singer;Richard Lippmann	1991				ML	-17.6062541207759	-88.46355827500156	65879
f7ae99484bc455f83c5cce19639e56424fde7127	a socio-phonetic analysis of taiwan mandarin interview speech		This paper presents results of a socio-phonetic analysis of Taiwan Mandarin by using a corpus of questionnaire-based interview speech. Questions were asked to collect data of the interviewee’s background of language use, socio-economic status, and internet access in different regions of Taiwan. Two typical dialect-influenced pronunciation errors, the deletion of /w/ before /o/ and the delabilialization of /y/ were analyzed with the associated socio-economic factors and the degree of dialect exposure. The degree of dialect exposure (Southern Min) and the studied pronunciation variants are statistically correlated with the accuracy rate. But no direct correlation was found between the pronunciation variation and the socioeconomic factors.	internet access;super robot monkey team hyperforce go!;text corpus	Shu-Chuan Tseng;Yun-Ru Huang	2010			mandarin chinese;speech recognition;political science	HCI	-12.185425269371637	-81.94314194426546	65885
0d0d300c6069ab54419436f6c2cd556af6479b77	discriminative multiple canonical correlation analysis for multi-feature information fusion	speech based emotion recognition discriminative multiple canonical correlation analysis multifeature information fusion dmcca multifeature information representation frobenius norm speaker recognition;emotion recognition dmcca information fusion speaker recognition;emotion recognition;correlation methods;speaker recognition;correlation feature extraction mel frequency cepstral coefficient speaker recognition emotion recognition databases vectors;dmcca;information fusion;sensor fusion;speaker recognition correlation methods emotion recognition sensor fusion	This paper presents a novel approach for multi-feature information fusion. The proposed method is based on the Discriminative Multiple Canonical Correlation Analysis (DMCCA), which can extract more discriminative characteristics for recognition from multi-feature information representation. It represents the different patterns among multiple subsets of features identified by minimizing the Frobenius norm. We will demonstrate that the Canonical Correlation Analysis (CCA), the Multiple Canonical Correlation Analysis (MCCA), and the Discriminative Canonical Correlation Analysis (DCCA) are special cases of the DMCCA. The effectiveness of the DMCCA is demonstrated through experimentation in speaker recognition and speech-based emotion recognition. Experimental results show that the proposed approach outperforms the traditional methods of serial fusion, CCA, MCCA and DCCA.	effective method;emotion recognition;feature vector;kernel method;multimodal interaction;nonlinear system;pattern recognition;speaker recognition	Lei Gao;Lin Qi;Enqing Chen;Ling Guan	2012	2012 IEEE International Symposium on Multimedia	10.1109/ISM.2012.15	speaker recognition;speech recognition;computer science;pattern recognition;sensor fusion	Vision	-5.741988131248227	-90.22788350104989	66037
fa24aaad48466792976b95fd736791f97e792aad	a word-level token-passing decoder for subword n-gram lvcsr	hidden markov models abstracts;word processing natural language processing protocols speech coding speech recognition vocabulary;decoding;large vocabulary continuous speech recognition;subword n grams;finnish large vocabulary continuous speech recognition task word level token passing decoder subword n gram lvcsr speech recognizer out of vocabulary rates oov rates subword n gram model recognition vocabulary oov words;token passing;subword n grams large vocabulary continuous speech recognition decoding token passing	The decoder is a key component of any modern speech recognizer. Morphologically rich languages pose special challenges for the decoder design, as a very large recognition vocabulary is required to avoid high out-of-vocabulary (OOV) rates. To alleviate these issues, the n-gram models are often trained over subwords instead of words. A subword n-gram model is able to assign probabilities to unseen word forms. We review token-passing decoding and suggest a novel way of creating the decoding graph for subword n-grams on word-level. This approach has the advantage of a better control over the recognition vocabulary, including removal of nonsense words and the possibility to include important OOV-words to the graph. The different decoders are evaluated in a Finnish large vocabulary continuous speech recognition (LVCSR) task.	finite-state machine;grams;n-gram;speech analytics;speech recognition;substring;vocabulary	Matti Varjokallio;Mikko Kurimo	2014	2014 IEEE Spoken Language Technology Workshop (SLT)	10.1109/SLT.2014.7078624	natural language processing;speech recognition;token passing;computer science	NLP	-20.743001775108883	-85.10731120186858	66055
14a4769c0ca567955e5ff629557ef31876aeaeb9	frame-synchronous and local confidence measures for on-the-fly keyword spotting	broadcast news;speech recognition engines decoding deafness automatic speech recognition management training delay speech processing radio broadcasting vocabulary;frame synchronous;word processing decoding speech recognition;probability;decoding;on the fly keyword spotting;confidence measure;acoustics;frame synchronous measure;keyword spotting;speech;local;eer frame synchronous measure local confidence on the fly keyword spotting speech recognition automatic speech transcription;engines;current measurement;automatic speech transcription;on the fly;local confidence;eer;speech recognition;acoustic measurements;false accept rate;word processing;frame synchronization	This paper presents several new confidence measures for speech recognition applications. The major advantage of these measures is that they can be evaluated with only a part of the whole sentence. Two of these measures can be computed directly within the first step of the recognition process, synchronously with the decoding engine. Such measures are useful to drive the recognition process by modifying the likelihood score or to validate recognized words in on-the-fly applications as keyword spotting task and on-line automatic speech transcription for deaf people. Two kinds of results are given. Firstly, an EER evaluation on a French broadcast news corpus shows performance close to the batch version of these measures (23.9% against 23.8% of EER). Secondly, for the keyword spotting application, our best measure provides a decrease of the false-acceptation rate by 50% with only a decrease of the correct words by 5%.	enhanced entity–relationship model;frame language;online and offline;speech recognition;synchronization (computer science);transcription (software)	Joseph Razik;Odile Mella;Dominique Fohr;Jean Paul Haton	2007	2007 9th International Symposium on Signal Processing and Its Applications	10.1109/ISSPA.2007.4555572	natural language processing;speech recognition;frame synchronization;computer science;speech;probability;mathematics;statistics	Arch	-20.30495801489812	-85.98400851768037	66064
1a0c539714fca0ff92dc57a6f4119522e8b62ba9	example-based head tracking	nonlinear mapping;human heads pose estimation;output parametric description;head tracking;training set;vector quantization;indexing;indexation;tree structure;example based head tracking;nearest example neighbor;quantized input code;vector quantizer;real time prototype example based head tracking human heads pose estimation nonlinear mapping output parametric description training set nearest example neighbor vector quantization quantized input code indexing tree structured vector quantizer;vector quantisation;head monitoring humans prototypes workstations road safety vector quantization neural networks computer science indexing;real time prototype;tree structured vector quantizer	We want to estimate the pose of human heads. This estimation involves a nonlinear mapping from the input image to an output parametric description. We characterize the mapping through examples from a training set, outputting the pose of the nearest example neighbor of the input. This is vector quantization, with the modi cation that we store an output parameter code with each quantized input code. For e cient indexing, we use a tree-structured vector quantizer (TSVQ). We make design choices based on the example application of monitoring an automobile driver's face. The reliance on stored data over computation power allows the system to be simple; e cient organization of the data allows it to be fast. We incorporate tracking in position and scale within the same vector quantization framework with virtually no cost in added computation. We show reasonable experimental results for a real-time prototype running on an inexpensive workstation.	computation;motion capture;nonlinear system;parameter (computer programming);pose (computer vision);prototype;quantization (signal processing);real-time clock;real-time computing;technical support;test set;vector quantization;workstation	Sourabh Niyogi;William T. Freeman	1996		10.1109/AFGR.1996.557294	computer vision;computer science;theoretical computer science;machine learning	Robotics	-18.54192720546737	-95.03927014567768	66088
8c3b60150af6eac8d154986bcfdfa1a01f844b27	not all created equal: individual-technology fit of brain-computer interfaces	computers;technology fit;rhythm;information systems;brain computer interface;selected works;performance;transducers;individual characteristics;motor cortex;brain modeling;mu rhythm;computer information systems;task technology fit;performance technology fit brain computer interface mu rhythm individual characteristics;bepress;brain computer interfaces;electroencephalography;instrument playing brain computer interfaces task technology technology fit process control signal motor cortex region eegs;transducers brain modeling rhythm electroencephalography brain computer interfaces computers information systems	This work presents a model stemming from literature on task-technology fit that seeks to match individual user characteristics and features of brain-computer interface technologies with performance to expedite the technology-fit process. The individual-technology fit model is tested with a brain-computer interface based on a control signal called the mu rhythm that is recorded from the motor cortex region. Characteristics from eighty total participants are tested across two different sessions. Performance is measured as a person's ability to modulate his/her mu rhythm. It appears that the version of software used in recording and interpreting EEGs, instrument playing, being on affective drugs, a person's sex, and age all play key roles in predicting mu rhythm modulation.	brain–computer interface;electroencephalography;modulation;stemming	Adriane B. Randolph	2012	2012 45th Hawaii International Conference on System Sciences	10.1109/HICSS.2012.451	brain–computer interface;simulation;human–computer interaction;artificial intelligence;information system	HCI	-6.047299955731904	-83.90118839183657	66166
a3d179503ca0dbfaea0558a66f7c63ace8d55ba5	direct estimation of musical pitch contour from audio data	music retrieval;audio databases music audio signal processing parameter estimation information retrieval signal representation correlation methods;audio signal processing;image processing;information retrieval;musical pitch contour estimation;constant q transform;testing;audio recording;correlation methods;data mining;symbolic format;multiple signal classification;signal processing;midi;signal representation;music information retrieval;crosscorrelation;web sites;digital formats;raw audio data;pitch transcription;crosscorrelation musical pitch contour estimation raw audio data digital formats music information retrieval symbolic format midi pitch transcription musical audio constant q transform;audio databases;parameter estimation;musical audio;music;music information retrieval multiple signal classification data mining audio databases signal processing image processing image retrieval testing audio recording web sites;image retrieval	The increasing amount of music being stored in digital formats calls for increasingly more creative methods for music information retrieval. One subset of music retrieval methods relies on storing a melody in a pitch contour representation. Most often, this contour information is generated either from symbolic format (MIDI) or from raw audio after a pitch transcription step. We propose a method of extracting pitch contour information from musical audio without an intermediate transcription step by combining a musically-tuned constant Q transform with crosscorrelation. When tested on a database of 520 monophonic music recordings, our method generates pitch contours from raw audio data with up to 98% accuracy.	pitch (music)	Adriane Swaim Durey;Mark A. Clements	2003		10.1109/ICASSP.2003.1200031	midi;computer vision;constant q transform;speech recognition;image processing;audio signal processing;image retrieval;pitch correction;computer science;multiple signal classification;signal processing;music	ML	-7.573650798937323	-92.80638461092244	66194
91880ea49629215a339d99534b82fcd90c53674b	estimation of unknown speaker's height from speech	regression algorithms;speech processing;human height estimation from speech	In the present study, we propose a regression-based scheme for the direct estimation of the height of unknown speakers from their speech. In this scheme every speech input is decomposed via the openSMILE audio parameterization to a single feature vector that is fed to a regression model, which provides a direct estimation of the persons’ height. The focus in this study is on the evaluation of the appropriateness of several linear and non-linear regression algorithms on the task of automatic height estimation from speech. The performance of the proposed scheme is evaluated on the TIMIT database, and the experimental results show an accuracy of 0.053 meters, in terms of mean absolute error, for the best performing Bagging regression algorithm. This accuracy corresponds to an averaged relative error of approximately 3%. We deem that the direct estimation of the height of unknown people from speech provides an important additional feature for improving the performance of various surveillance, profiling and access authorization applications.	algorithm;approximation error;authorization;feature vector;nonlinear system;norm (social);profiling (computer programming);timit	Iosif Mporas;Todor Ganchev	2009	I. J. Speech Technology	10.1007/s10772-010-9064-2	speech recognition;computer science;machine learning;pattern recognition;speech processing	NLP	-14.243864903285994	-93.86193813601275	66353
8961ae998b372e54a11ebdc4b0f88383a4d8a8c1	unina system for the evalita 2011 forced alignment task		This report presents the system proposed and the results obtained by our group for the EVALITA 2011 Forced Alignment on Spontaneous Speech task. The system is composed of a module for acoustic modelling, which uses the SPRAAK toolkit, and a second one for the processing of textual information. Several tests were performed to determine the impact of frame shift size and speaker adaptation on the accuracy of the alignment. Good segmentation results were obtained, the proposed system outperforming the other teams’ systems, but performance improvements can be achieved by using a pronunciation dictionary.	acoustic cryptanalysis;dictionary;lexicon;spontaneous order;text corpus	Bogdan Ludusan	2011		10.1007/978-3-642-35828-9_36	simulation;speech recognition;engineering;communication	Vision	-20.850497839315192	-84.53487472410845	66394
6266002509da42bb5455fffd2c51ed26eabd929b	prosodic features of mandarin repair in classroom lecture speech	maintenance engineering speech speech processing acoustics;speech prosody;speech processing linguistics natural language processing;information weighting;discourse structure;discourse prosodic boundary;mandarin repair;continuous speech production prosodic features mandarin repair classroom lecture speech acoustic features reparandum reparatum university classroom lectures nonrepaired phrase boundaries acoustic cues prosodic phrases repair yield significant distinctions;information weighting mandarin repair speech prosody discourse prosodic boundary discourse structure	The current study examines some acoustic features of the reparandum (R1) and reparatum (R2) of Mandarin repair in university classroom lectures with respect to discourse structure and information weighting. By the comparison of contrast degree in acoustic features between R1/R2 of the repair and at boundaries of the prosodic phrase without repair, it is found that almost 60% of repair instances present a significant difference from non-repaired phrase boundaries in terms of acoustic cues including F0, duration and intensity. Further pairing of contrast degree in acoustic features with pause duration reveals that prosodically repair instances and boundaries of prosodic phrases without repair yield significant distinctions. The findings thus demonstrate that the speaker would orient to smaller acoustic degree of contrast while repairing, which in turn may suggest alternative planning and deploying for information highlights at higher level of discourse structure during repair in the continuous speech production.	acoustic cryptanalysis;super robot monkey team hyperforce go!	Helen Kai-Yun Chen;Wei-te Fang;Chiu-yu Tseng	2014	2014 17th Oriental Chapter of the International Committee for the Co-ordination and Standardization of Speech Databases and Assessment Techniques (COCOSDA)	10.1109/ICSDA.2014.7051431	natural language processing;speech recognition;computer science;linguistics	NLP	-11.714523672638967	-81.40712465399591	66443
f03d0b57410bbc565591314a4cd92e5671b7489e	composite background models and score standardization for language identification systems	standardization natural languages speech recognition iron testing laboratories electronic mail training data error analysis robustness;background modeling;score function;perceptrons;natural languages;speech recognition composite background models score standardization language identification systems background language standardized confidence scoring function single layer perceptron robust modeling score distributions;system performance;target language;equal error rate;language identification;speech recognition;gaussian distribution;natural languages speech recognition perceptrons gaussian distribution	Thispaperdescribestwo enhancements to our languageidentificationsystem.Compositebackground(CBG) modelingallows usto identify target languagespeechin anenvironmentwherelabeledbackgroundtrainingdatais unavailableor limited. Instead of separatemodelsfor eachof the backgroundlanguages, a singlecompositebackgroundmodelis createdfrom all thenon-target training speech.Generally, the CBG systemperformedaboutas well asa baselinesystemcontaininga separatemodelper backgroundlanguage.The averageequalerror ratefor 12 CBG tests was13.6%versus13.4%for thebaseline.Wehavealsodeveloped andtesteda standardizedconfidencescoringfunctionbasedon a single-layerperceptronwhich hasproven to becapableof robust modelingof scoredistributions.	language identification	Terry P. Gleason;Marc A. Zissman	2001		10.1109/ICASSP.2001.940884	normal distribution;natural language processing;language identification;speech recognition;computer science;perceptron;machine learning;pattern recognition;computer performance;score;natural language;statistics	Vision	-19.765969065237417	-90.75279063494959	66539
f74bbf1720d854c7d3fbc8bbb08c7b749735bd22	a proposal system for historic arabic manuscript transcription and retrieval		In this paper, we propose a computer-assisted transcription system of old registers, handwritten in Arabic from the 19th century onwards, held in the National Archives of Tunisia (NAT). The proposed system assists the human supervisor to complete the transcription task as efficiently as possible. This assistance is given at all different recognition levels. Our system addresses different approaches for transcription of document images. It also implements an alignment method to find mappings between word images of a handwritten document and their respective words in its given transcription. © (2013) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	transcription (software)	Abdelaziz Labben;Afef Kacem;Abdel Belaïd	2013		10.1117/12.2008472	speech recognition;telecommunications;computer science;transcription;multimedia;world wide web	Vision	-25.40897715313044	-83.86713037341885	66755
3bffc950d40fa9844e0079b12d2df8af08f501eb	linear and non linear kernel gmm supervector machines for speaker verification	kernel function;indexing terms;nuisance attribute projection;support vector;speaker recognition evaluation;speaker verification;speaker recognition;gaussian mixture model;equal error rate;support vector machine	This paper presents a comparison between Support Vector Machines (SVM) speaker verification systems based on linear and non linear kernels defined in GMM supervector space. We describe how these kernel functions are related and we show how the nuisance attribute projection (NAP) technique can be used with both of these kernels to deal with the session variability problem. We demonstrate the importance of GMM model normalization (M-Norm) especially for the non linear kernel. All our experiments were performed on the core condition of NIST 2006 speaker recognition evaluation (all trials). Our best results (an equal error rate of 6.3%) were obtained using NAP and GMM model normalization with the non linear kernel.	experiment;google map maker;heart rate variability;speaker recognition;support vector machine	Réda Dehak;Najim Dehak;Patrick Kenny;Pierre Dumouchel	2007			speaker recognition;support vector machine;kernel method;speech recognition;kernel embedding of distributions;radial basis function kernel;computer science;machine learning;pattern recognition;variable kernel density estimation;polynomial kernel	AI	-15.801205089732326	-92.02619359261135	66776
c2b76bb652c498d91a823fff4678ffbad925f9a4	hmm adaptation and voice conversion for the synthesis of child speech: a comparison	indexing terms	This study compares two different methodologies for producing data-driven synthesis of child speech from existing systems that have been trained on the speech of adults. On one hand, an existing statistical parametric synthesiser is transformed using model adaptation techniques, informed by linguistic and prosodic knowledge, to the speaker characteristics of a child speaker. This is compared with the application of voice conversion techniques to convert the output of an existing waveform concatenation synthesiser with no explicit linguistic or prosodic knowledge. In a subjective evaluation of the similarity of synthetic speech to natural speech from the target speaker, the HMM-based systems evaluated are generally preferred, although this is at least in part due to the higher dimensional acoustic features supported by these techniques.	acoustic cryptanalysis;concatenation;hidden markov model;natural language;parametric polymorphism;synthetic intelligence;waveform	Oliver Watts;Junichi Yamagishi;Simon King;Kay M. Berkling	2009			voice activity detection;speech recognition;index term;speech corpus;computer science;voice analysis;speech processing;world wide web	NLP	-15.119013260343339	-84.45020432083184	66784
621f5fd6b0a7d09bdbe75a16b1ab202b7e337133	source separation for biomedical signals: blind or not blind?			source separation	Christian Jutten	2013			computer vision;computer science;data mining;source separation;artificial intelligence	ML	-13.555857841612173	-87.19940214347889	66893
20d0ca0c2cec086b77d2677ed9948709939aee1c	effect of spectral degradation to the intelligibility of vowel sentences		Based on the noise-replacement paradigm, recent studies showed that vowels carried more perceptional information for sentence intelligibility than consonants. Considering that vowels contain many important acoustic cues for speech perception, this study further assessed the effect of spectral degradation to the intelligibility of Mandarin vowel sentences. Mandarin sentences were processed to generate three types of spectrally degraded [i.e., fundamental frequency (F0) flattened, sinewave synthesized, and noise-vocoded] stimuli. Noisereplacement paradigm was implemented to preserve different amounts of vowel centers and replace the rest with noise. Listening experiments showed that flattening F0 had a minimal effect on the intelligibility of Mandarin vowel sentences, and the harmonic structure within vowels accounted more for the intelligibility of Mandarin vowel sentences. While deleting vowel edges had little influence on the intelligibility of the unprocessed vowel sentences, it had a significantly negative effect on the intelligibility of vowel sentences with spectral degradation.	acoustic cryptanalysis;elegant degradation;experiment;intelligibility (philosophy);programming paradigm;super robot monkey team hyperforce go!;vocoder	Fei Chen;Sharon W. K. Wong;Lena L. N. Wong	2014			artificial intelligence;speech recognition;pattern recognition;vowel;computer science;intelligibility (communication)	NLP	-10.475366392103581	-83.93460934249737	66906
5a92653944d04037d99b185f1495eb2c10e0b262	two-stage phone duration modelling with feature construction and feature vector extension for the needs of speech synthesis	phone duration modelling;text to speech synthesis;statistical modelling;feature construction	We propose a two-stage phone duration modelling scheme, which can be applied for the improvement of prosody modelling in speech synthesis systems. This scheme builds on a number of independent feature constructors (FCs) employed in the first stage, and a phone duration model (PDM) which operates on an extended feature vector in the second stage. The feature vector, which acts as input to the first stage, consists of numerical and non-numerical linguistic features extracted from text. The extended feature vector is obtained by appending the phone duration predictions estimated by the FCs to the initial feature vector. Experiments on the American-English KED TIMIT and on the Modern Greek WCL-1 databases validated the advantage of the proposed two-stage scheme, improving prediction accuracy over the best individual predictor, and over a two-stage scheme which just fuses the first-stage outputs. Specifically, when compared to the best individual predictor, a relative reduction in the mean absolute error and the root mean square error of 3.9% and 3.9% on the KED TIMIT, and of 4.8% and 4.6% on the WCL-1 database, respectively, is observed.	approximation error;database;feature vector;kerrison predictor;mean squared error;numerical analysis;semantic prosody;speech synthesis;timit	Alexandros Lazaridis;Todor Ganchev;Iosif Mporas;Evangelos Dermatas;Nikos Fakotakis	2012	Computer Speech & Language	10.1016/j.csl.2012.01.009	statistical model;speech recognition;feature vector;computer science;machine learning;pattern recognition;statistics	ML	-18.711224558380962	-88.06006535432226	66930
7e34a46936ab27d3407f379f0c69167fd93cd70f	speech recognition using stochastic phonemic segment model based on phoneme segmentation	speech recognition	This paper discusses speech recognition based on a new statistical phoneme segment model which is trained by phoneme parameters derived from automatically extracted phoneme segments. The proposed system operates as follows. In preprocessing before recognition, the phoneme boundaries are detected by segmentation. The phonemes are discriminated using a stochastic phoneme segment model, and a phoneme segment lattice with scores is constructed. Next the speech recognition is performed by matching of symbol sequences to dictionary items. The segmentation system that is employed can infer phoneme boundaries with high accuracy. This helps to eliminate unnecessary parameters, leaving the feature parameters which are effective in separating phonemes. In other words, the phoneme recognition problem in continuous speech can be reduced to a discrimination problem and thus a speaker-independent model can be constructed from a relatively small number of training data. The stochastic phoneme segment model is trained with training samples extracted from a phoneme-balanced word set of 4920 words uttered by 10 speakers. In a recognition experiment with 6709 words uttered by 63 nontraining speakers, a recognition rate of 92.6% was obtained as the average for all speakers, using a word dictionary of 212 words. © 2000 Scripta Technica, Syst Comp Jpn, 31(10): 89–98, 2000	speech recognition	Chieko Furuichi;Katsura Aizawa;Kazuhiko Inoue	2000	Systems and Computers in Japan	10.1002/1520-684X(200009)31:10%3C89::AID-SCJ9%3E3.0.CO;2-7	natural language processing;audio mining;speech recognition;word error rate;computer science	Robotics	-18.769978346045423	-86.54847689493322	67039
33b861a9bceb18b65496e2fa5fb2d1ab2de41e4f	mrasta and plp in automatic speech recognition	automatic speech recognition	This work explores different methods for combining estimated posterior probabilities from Multi-RASTA (MRASTA) and Perceptual Linear Prediction (PLP) features for Automatic Speech Recognition (ASR). The improved performance by the ASR system indicates the complementary nature of information present in MRASTA and PLP. Among the different combining methods explored, product gives best performance.	automated system recovery;elegant degradation;pl/p;speech recognition	S. R. Mahadeva Prasanna;Hynek Hermansky	2007			speech recognition;artificial intelligence;pattern recognition;computer science	NLP	-13.894726368013865	-90.46082565622396	67149
74163d5e2ee6a3fb9275e9610e0a7f0581e31a4b	study on the consistency analysis between the prosody and the spectrum for mandarin speech	warping process;warping curve;hidden markov model algorithm;pronunciation process;linde buzo gray algorithm;hmm state sequences;mandarin speech quality;text to speech system;prosodic vector;vq codebooks;prosody;vector quantisation;consistency analysis text to speech system warping process prosodic vector vq codebooks linde buzo gray algorithm vector quantisation hmm state sequences hidden markov model algorithm warping curve pronunciation process mandarin speech quality prosody;vector quantisation hidden markov models speech processing;consistency analysis	In this work, a consistency analysis between the prosody and the spectrum for Mandarin speech is presented. Found by an inspection on the pronunciation process of human beings, the consistency can be interpreted as a close correlated relation of a warping curve between the spectrum and the prosody intra a syllable. Through three steps in the procedure of the consistency analysis, the hidden Markov model (HMM) algorithm is used firstly to decode HMM-state sequences within a syllable at the same time as to divide them into three segments. Secondly, based on a designated syllable, the vector quantisation (VQ) with the Linde–Buzo–Gray algorithm is used to train the VQ codebooks of each segment. Thirdly, the prosodic vector of each segment is encoded as an index by VQ codebooks, and then the probability of each possible path is evaluated as a prerequisite to analyse the consistency. It is demonstrated experimentally that a consistency is definitely acquired in case the syllable is located exactly in the same word. These results offer a research direction that the warping process between the spectrum and the prosody intra a syllable must be considered in a text-to-speech  system to improve the speech quality.	semantic prosody;super robot monkey team hyperforce go!	Cheng-Yu Yeh;Kuan-Lin Chen;Shaw-Hwa Hwang;Long-Jhe Yan	2013	IET Signal Processing	10.1049/iet-spr.2012.0099	natural language processing;speech recognition;computer science;pattern recognition;prosody;linde–buzo–gray algorithm	ML	-14.256842681983168	-82.27504999056711	67184
43a75e3d5b87310bce60ba341448c421f4e40d93	a low-complexity voice activity detector for smart hearing protection of hyperacusic persons		In this paper, a Voice Activity Detector (VAD) is proposed for smart hearing protection applications where speech is to get through the hearing protector while ambient noise is to be blocked out. The VAD calculates a short-term statistical assessment of the temporal envelopes within different frequency bands. This assessment uses the Inter-Quartile Range (IQR) and reflects the dispersion of the envelopes’ magnitudes. The VAD’s decision is made using two threshold comparison rules and a hangover scheme triggered after a given number of observations. These four parameters have been optimized off-line using a genetic algorithm approach. The performance of the proposed VAD is compared to Sohn’s VAD using a database of 90 speech signals corrupted by five real-world noise environments at Signal-to-Noise ratios (SNR) varying from 0 to +10 dB. Results show that the proposed VAD performs better than Sohn’s VAD with an 85.9% (compared to 77.5%) F1 score averaged across all SNRs and also minimizes by a factor of three the mid-speech clipping rate. In addition, the evaluation of the proposed VAD’s computational cost shows that its implementation on-board a low-power low-consumption DSP is very feasible and would enable smart hearing protection for hypersensitive persons.	algorithmic efficiency;computation;f1 score;frequency band;genetic algorithm;low-power broadcasting;on-board data handling;online and offline;signal-to-noise ratio;voice activity detection	Narimene Lezzoum;Ghyslain Gagnon;Jérémie Voix	2013			speech recognition;detector;computer science	Mobile	-10.770573889982401	-93.30076182440772	67254
0d7ba3c3a92667a66373874a5303df7669bbb56f	automatic dialog acts recognition based on sentence structure	sentence structure;bayesian network;dialog act;hidden markov models natural languages animation classification tree analysis bayesian methods informatics computer science application software system testing speech;application software;sentence structure information automatic dialog acts recognition sentence structure czech multimodal reservation system animated talking head hearing impaired people prosody lexical only information;bayesian methods;speech;natural languages;czech;hearing impaired;handicapped aids;hidden markov models;speech recognition handicapped aids;hearing impaired people;animation;speech recognition;system testing;animated talking head;talking head;informatics;multimodal reservation system;classification tree analysis;computer science;sentence structure information;lexical only information;classification accuracy;prosody;automatic dialog acts recognition;language model	This paper deals with automatic dialog acts (DAs) recognition in Czech. Our work focuses on two applications: a multimodal reservation system and an animated talking head for hearing-impaired people. In that context, we consider the following DAs: statements, orders, investigation questions and other questions. The main goal of this paper is to propose, implement and evaluate new approaches to automatic DAs recognition based on sentence structure and prosody. Our system is tested on a Czech corpus that simulates a task of train tickets reservation. With lexical-only information, the classification accuracy is 91%. We proposed two methods to include sentence structure information, which respectively give 94% and 95%. When prosodic information is further considered, the recognition accuracy reaches 96%	dialog system;multimodal interaction;semantic prosody;text corpus	Pavel Král;Christophe Cerisara;Jana Klecková	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1659957	natural language processing;anime;application software;speech recognition;bayesian probability;computer science;speech;bayesian network;prosody;natural language;informatics;system testing;language model	Robotics	-21.675485278727585	-82.48379065686929	67313
f716aef357887d3142df0345cf36f18618240fec	robust arabic speaker verification system using lsf extracted from the g.729 bitstream	robust arabic speaker verification system decoded speech mfcc mel frequency cepstral coefficients feature extraction line spectral frequency svm support vector machine aradigit database voip internet protocol g 729 bitstream;support vector machines;speech coding;support vector machines audio databases feature extraction speaker recognition speech coding;speaker recognition;feature extraction;audio databases;speech support vector machines feature extraction databases vectors noise measurement speech coding;voip g729 svm speaker verification lsf mfcc	This paper deals with an Arabic text-independent speaker verification system over the Internet Protocol (VoIP). The system, using the ARADIGIT database and based on Support Vector Machine (SVM), was designed to use the information extracted directly from the coded parameters embedded in the ITU-T G.729 bitstream. Experiments evaluated the robustness of the system at different noisy conditions. The results showed that the use of Line Spectral Frequency (LSF) features extracted directly from G.729 encoded bitstream improved significantly the recognition performance compared with the Mel Frequency Cepstral Coefficients (MFCC) features extracted from decoded speech.	bitstream;coefficient;embedded system;g.729;lsf;mel-frequency cepstrum;robustness (computer science);speaker recognition;support vector machine	Kawthar Yasmine Zergat;Abderrahmane Amrouche;Meriem Fedila;Mohamed Debyeche	2013	2013 IEEE International Workshop on Machine Learning for Signal Processing (MLSP)	10.1109/MLSP.2013.6661939	natural language processing;speaker recognition;speaker diarisation;speech recognition;computer science;pattern recognition	Robotics	-14.079065483291705	-89.68893468876944	67362
237eba4822744a9eabb121fe7b50fd2057bf744c	facial expression synthesis using pad emotional parameters for a chinese expressive avatar	partial expression parameter;facial animation;facial expression;talking avatar;human perception;face to face;pad emotional model;emotional expression	Facial expression plays an important role in face to face communication in that it conveys nonverbal information and emotional intent beyond speech. In this paper, an approach for facial expression synthesis with an expressive Chinese talking avatar is proposed, where a layered parametric framework is designed to synthesize intermediate facial expressions using PAD emotional parameters [5], which describe the human emotional state with three nearly orthogonal dimensions. Partial Expression Parameter (PEP) is proposed to depict the facial expression movements in specific face regions, which act as the mid-level expression parameters between the low-level Facial Animation Parameters (FAPs) [11] and the high-level PAD emotional parameters. A pseudo facial expression database is established by cloning the real human expression to avatar and the corresponding emotion states for each expression is annotated using PAD score. An emotionexpression mapping model is trained on the database to map the emotion state (PAD) into facial expression configuration (PEP). Perceptual evaluation shows the input PAD value is consistent with that of human perception on synthetic expression, which supports the effectiveness of our approach.	direct manipulation interface;face animation parameter;high- and low-level;skeletal animation;speech synthesis;synthetic intelligence	Shen Zhang;Zhiyong Wu;Helen M. Meng;Lianhong Cai	2007		10.1007/978-3-540-74889-2_3	psychology;computer vision;speech recognition;computer facial animation;emotional expression;communication;perception;facial expression	Graphics	-14.766804253119577	-83.21038297432086	67450
61c43b4eb152bae69f878ec74b8f60bbf788ece7	use of simulated data for robust telephone speech recognition	dialogue model;hmm;simulated data;speech recognition	The collection of telephone databases, for training speech recognisers, is a time consuming and costly work. In the paper we propose a method for producing simulated telephone data starting from clean wide band databases. The result of the simulation is the generation of a noisy database that can be used, in addition to other techniques, for compensating or adapting speech recogniser parameters with respect to different test environments. For the first of the two adopted test sets, performance improvements ranging from about 30% to about 9% have been measured, as a function of the quantity of real telephone data used, in addition to the simulated ones, for system training. For the second test set no significant improvements were obtained.	database;simulation;speech recognition;speech synthesis;test set	Tarcisio Coianiz;Daniele Falavigna;Roberto Gretter;Marco Orlandi	1999			speech recognition;voice activity detection;audio mining;artificial intelligence;speaker recognition;acoustic model;pattern recognition;speech processing;computer science	ML	-20.33745347432776	-86.02947213361254	67517
12a8708f7df3c3bc152f909a9e04b5ad78f7aae6	analysis of correlation between audio and visual speech features for clean audio feature prediction in noise	multiple linear regression;active appearance model;speech enhancement;indexing terms;visual features	The aim of this work is to examine the correlation between audio and visual speech features. The motivation is to find visual features that can provide clean audio feature estimates which can be used for speech enhancement when the original audio signal is corrupted by noise. Two audio features (MFCCs and formants) and three visual features (active appearance model, 2-D DCT and cross-DCT) are considered with correlation measured using multiple linear regression. The correlation is then exploited through the development of a maximum a posteriori (MAP) prediction of audio features solely from the visual features. Experiments reveal that features representing broad spectral information have higher correlation to visual features than those representing finer spectral detail. The accuracy of prediction follows the results found in the correlation measurements.	active appearance model;discrete cosine transform;speech enhancement	Ibrahim Almajai;Ben P. Milner;Jonathan Darch	2006			computer vision;active appearance model;speech recognition;index term;computer science;linear regression;speech coding;pattern recognition	HCI	-10.9455665085723	-89.22606465098009	67519
d122ad04b952328e32ba2c90a04b148736e6520c	discriminating speakers by their voices - a fusion based approach				Halim Sayoud;Siham Ouamour;Zohra Hamadache	2017		10.1007/978-3-319-66429-3_31		Robotics	-13.906165418941809	-87.55372697431558	67601
4be84c38852440510e496764266f5ad366ab64ed	toward a theory of distributed word expert natural language parsing	history;parsing algorithms;prototypes;words language;filters;natural language computers;natural languages;natural language parsing;morphology;moon;natural languages humans prototypes moon computer science morphology history filters;computerized simulation;lisp programming language;on line programming;humans;computer science;sentences;symbols;linguistics	An approach to natural language meaning-based parsing in which the unit of linguistic knowledge is the word rather than the rewrite rule is described. In the word expert parser, knowledge about language is distributed across a population of procedural experts, each representing a word of the language, and each an expert at diagnosing that word's intended usage in context. The parser is structured around a coroutine control environment in which the generator-like word experts ask questions and exchange information in coming to collective agreement on sentence meaning. The word expert theory is advanced as a better cognitive model of human language expertise than the traditional rule-based approach. The technical discussion is organized around examples taken from the prototype LISP system which implements parts of the theory.	cognitive model;coroutine;lisp;logic programming;parsing;prototype;rewrite (programming);rewriting	Chuck Rieger;Steve Small	1981	IEEE Transactions on Systems, Man, and Cybernetics	10.1109/TSMC.1981.4308577	natural language processing;semeval;morphology;computer science;natural satellite;lisp;word lists by frequency;prototype;symbol;natural language;programming language	NLP	-31.171271738754054	-81.03351073013246	67643
14fa503418e5a4916aad1b5746afabb2e6091722	acoustic and perceptual characteristic of italian stop consonants		We report in this paper the results of a study carried out to analyse the acoustic and perceptual characteristics of Italian stop consonants. The aim of this study is twofold: give an acoustical description of Italian stops and investigate which are the perceptual cues relative to their place of articulation. From the acoustic point of view we report: the measurements relative to the length of the whole consonant and of its release burst; the F1 and F2 of the following vowel measured at the beginning of it. Moreover we counted the presence of the release burst and we tried to describe its acoustical characteristics in terms of the spectral structure as suggested by Blumstein [1] [2]. From the perceptual point of view we report the results of three perceptual tests that we run with the aim of evaluating whether the release burst or the formant transitions are more relevant for the perception of Italian stop consonants’ place of articulation.	acoustic cryptanalysis;biconnected component	Loredana Cerrato;Mauro Falcone	1998			artificial intelligence;speech recognition;pattern recognition;perception;computer science	AI	-10.50083456570419	-82.10957210532841	67655
5e522acb66c207f6e9d70dd605c67ef94ef3f384	a cross-version approach for stabilizing tempo-based novelty detection		The task of novelty detection with the objective of detecting changes regarding musical properties such as harmony, dynamics, timbre, or tempo is of fundamental importance when analyzing structural properties of music recordings. But for a specific audio version of a given piece of music, the novelty detection result may also crucially depend on the individual performance style of the musician. This particularly holds true for tempo-related properties, which may vary significantly across different performances of the same piece of music. In this paper, we show that tempobased novelty detection can be stabilized and improved by simultaneously analyzing a set of different performances. We first warp the version-dependent novelty curves onto a common musical time axis, and then combine the individual curves to produce a single fusion curve. Our hypothesis is that musically relevant points of novelty tend to be consistent across different performances. This hypothesis is supported by our experiments in the context of music structure analysis, where the cross-version fusion curves yield, on average, better results than the novelty curves obtained from individual recordings.	apache axis;experiment;jaquet-droz automata;novelty detection;performance;sensor	Meinard Müller;Thomas Prätzlich;Jonathan Driedger	2012			simulation	ML	-9.48453003766448	-82.66551852659956	67922
888ab218d2e7cc134402b5aacaeab2025f7e5d49	multi-band and adaptation approaches to robust speech recognition	automatic speech recognition	In this paper we present two approaches to deal with degradation of automatic speech recognizers due to acoustic mismatch in training and testing environments. The rst approach is based on the multi-band approach to automatic speech recognition (ASR). This approach is shown to be inherently robust to frequency selective degradation. In the second approach, we present a conceptually simple unsupervised feature adaptation technique, based on recursive estimation of means and variances of the cepstral parameters to compensate for the noise e ects. Both techniques yield signi cant reduction in error rates.	acoustic cryptanalysis;cepstrum;elegant degradation;finite-state machine;recursion;speech recognition;speech synthesis;unsupervised learning	Sangita Tibrewala;Hynek Hermansky	1997			voice activity detection;speech recognition;artificial intelligence;acoustic model;speech technology;pattern recognition;speaker recognition;computer science;speech processing	NLP	-14.259863536537232	-91.35908167196251	67983
0d5a0fef1c09260be6a43167a76ecc22134d1a84	comparison of independent component and independent subspace analysis algorithms	audio signal processing;convolution;independent component analysis;source separation;time-domain analysis;ica;isa;audio signal convolutive mixture separation;independent subspace analysis algorithm;pseudoconvolutive mixture separation	Recent advances in separation of convolutive mixtures of audio signals have shown that the problem can be successfully solved in time-domain in a multistep procedure including an application of some method of instantaneous independent component analysis (ICA) or independent subspace analysis (ISA), as one of the steps. In this paper we propose a test that allows a comparison of different ICA and ISA algorithms from this perspective. The test consists in evaluating separation of a pseudo-convolutive mixture of given independent signals. The mixture has features of real-world convolutive mixtures and of instantaneous mixtures simultaneously. We apply the proposed test to compare performance of several ICA and ISA algorithms in four different scenarios, taking in mind that suitability of the algorithms depends on properties of the separated signals.	algorithm;independent computing architecture;independent component analysis	Zbynek Koldovský;Petr Tichavský	2009	2009 17th European Signal Processing Conference		speech recognition;computer science;machine learning;pattern recognition	ML	-11.499071453343857	-93.6974955591947	68002
fadf04692c37005e555cdedb2edcf6506e5f88ef	is there a link between media-multitasking and the executive functions of filtering and response inhibition?	filtering;distractor congruency;response inhibition;executive functions;inhibitory control;media multitasking	Media-multitasking refers to utilising at least two forms of media simultaneously. This study examined the link between media-multitasking and the inhibitory control executive function. Performance on measures of filtering (flanker task) and inhibitory response control (Go/NoGo task) were compared across heavy media multitaskers (HMM), average media multitaskers (AMM) and light media multitaskers (LMM). For both tasks performance was better under low than high perceptual load conditions. For the flanker task, there was an effect of distractor congruency only for the low perceptual load and no performance differences between the media groups. For the Go/No-Go task, there was a distractor congruency effect for the LMM and HMM for the Go-trials. For the No-Go trials there was no difference between the groups’ performance in the low perceptual load condition. The AMM were more adversely affected by the higher perceptual load than the LMM or HMM particularly when the distractors were incongruent and neutral relative to the targets. These results suggest a link between average levels of media-multitasking and the inhibitory control executive function.	computer multitasking;hidden markov model;phase congruency;smart meter;streaming media	Karen Murphy;Stephanie McLauchlan;Mindy Lee	2017	Computers in Human Behavior	10.1016/j.chb.2017.06.001	psychology;filter;speech recognition;inhibition theory;communication;social psychology	HCI	-6.253647685351628	-81.05472490874774	68012
b64f74d7e4ee340f41f43bd8e007b4f5b36afd89	a study of a direct speech transform method on laryngectomee speech		This paper proposes and evaluates a new direct speech transform method with waveforms from laryngectomee speech to normal speech. Most conventional speech recognition systems and speech processing systems are not able to treat laryngectomee speech with satisfactory results. One of the major causes is difficulty preparing corpora. It is very hard to record a large amount of clear and intelligible utterance data because the acoustical quality depends strongly on the individual status of such people. Our proposed method focuses on the acoustic characteristics of speech waveform of laryngectomee people and transforms such characteristics directly into normal speech. The proposed method is able to deal with esophageal and alaryngeal speech in the same algorithm. The method is realized by learning transform rules that have acoustic correspondences between laryngectomee and normal speech. Results of several fundamental experiments indicate a promising performance for real transform. Speech is a perfect medium and the most common for human-to-human information exchange because it is able to be used without hands or other tools, being a fundamental contributor to ergonomic multi-modality. Much research have been developed to realize such advantages for human-machine interaction. Many applications are produced and they are recently contributing to human life. On the other hand, many people who are unable to use their larynxes are not able to benefit from such advances in technology although such assistance is expected. Both esophageal and alaryngeal speech, which laryngectomee people practice to enable conversation, are understandable and enable adequate communication. However, conventional speech processing systems are not able to accept them as inputs because almost all current systems deal with only normal speech. Many intelligible utterances spoken by normal people have to be prepared as learning data to construct useful acoustic models for the systems. It is easy to find a lot of corpora valuable in both quality and quantity in many languages. However, there are not many resources of laryngectomee or other disordered speech because it is very difficult to sample a number of intelligible and clear utterances. One of the major causes is dependence on individual status of speech Thus it is not easy to obtain a high acoustic quality of corpora. Fig.1 Processing of the proposed method We focus on laryngectomee speech waveforms themselves to transform them into normal speech. Many studies have attempted to transform laryngectomee speech to normal speech, for example: re-synthesizing the fundamental frequency or formant of normal speech[1], or …	acoustic cryptanalysis;acoustic model;algorithm;experiment;human factors and ergonomics;human–computer interaction;information exchange;modality (human–computer interaction);speech processing;speech recognition;speech synthesis;text corpus;waveform	Koji Murakami;Kenji Araki;Makoto Hiroshige;Koji Tochinai	2003			speech recognition;direct speech;laryngectomee;computer science	HCI	-10.231519973542323	-85.78149488392239	68148
760bbdee497f07e044cd67022daaa2bcb2b77ea0	kernel pca for speech enhancement		In this paper, we apply kernel principal component analysis (kPCA), which has been successfully used for image denoising, to speech enhancement. In contrast to other enhancement methods which are based on the magnitude spectrum, we rather apply kPCA to complex spectral data. This is facilitated by Gaussian kernels. In the experiments, we show good noise reduction with few artifacts for noise corrupted speech at different SNR levels using additive white Gaussian noise. We compared kPCA with linear PCA and spectral subtraction and evaluated all algorithms with perceptually motivated quality measures.	additive white gaussian noise;algorithm;experiment;kernel principal component analysis;noise reduction;signal-to-noise ratio;speech enhancement;utility functions on indivisible goods	Christina Leitner;Franz Pernkopf;Gernot Kubin	2011			artificial intelligence;pattern recognition;kernel principal component analysis;speech enhancement;noise reduction;magnitude (mathematics);additive white gaussian noise;subtraction;computer science;gaussian	ML	-13.643892692185762	-92.72878533910954	68198
877c89d11580c8ab35ff4fb8f1af219fd9224fb5	adaptation techniques for ambience and microphone compensation in the ibm tangora speech recognition system	microphones;real time;speech coding;adaptive signal processing;speech recognition;microphones speech recognition background noise histograms testing databases training data noise robustness speech enhancement noise level;running energy histogram microphone compensation ibm tangora speech recognition system ambience background noise microphone characteristics adaptation techniques vq codebooks training data speech domain frame energy;vector quantisation;microphones speech recognition adaptive signal processing vector quantisation speech coding	Conventional speech recognition systems such as the IBM Tangora tend to be adversely influenced by external factors such as background noise and microphone characteristics. This paper discusses some adaptation techniques to counteract such influences. We also consider ways to reduce computation so that the methods may be implemented for real time Tangora operation. The adaptation strategy utilized two sets of VQ codebooks derived from the training data, one representing the ambience and the other characterizing the speech domain. A speech versus ambience decision was made by examining several factors, such as a comparison of the overall energy in a frame with a percentile point of a running energy histogram. We include results to demonstrate the effectiveness of our approach. >	microphone;speech recognition	Subrata K. Das;Arthur Nádas;David Nahamoo;Michael Picheny	1994		10.1109/ICASSP.1994.389365	voice activity detection;adaptive filter;speech recognition;computer science;speech coding;speech processing	Crypto	-14.093512073999905	-91.64838316824302	68204
0b2ba95386db8103a39d5de60ba0ea9c7a4c3d79	automatic recognition of cantonese-english code-mixing speech	判解;tan lee;code mixing;acoustic modeling;language modeling;法律詞典;論文;automatic speech recognition;大陸法學;法規;月旦法學;p c ching;joyce y c chan;houwei cao;法律題庫;裁判時報;月旦知識庫;法學資料庫;tssci;教學	Code-mixing is a common phenomenon in bilingual societies. It refers to the intra-sentential switching of two different languages in a spoken utterance. This paper presents the first study on automatic recognition of Cantonese-English code-mixing speech, which is common in Hong Kong. This study starts with the design and compilation of code-mixing speech and text corpora. The problems of acoustic modeling, language modeling, and language boundary detection are investigated. Subsequently, a large-vocabulary code-mixing speech recognition system is developed based on a two-pass decoding algorithm. For acoustic modeling, it is shown that cross-lingual acoustic models are more appropriate than language-dependent models. The language models being used are character tri-grams, in which the embedded English words are grouped into a small number of classes. Language boundary detection is done either by exploiting the phonological and lexical differences between the two languages or is done based on the result of cross-lingual speech recognition. The language boundary information is used to re-score the hypothesized syllables or words in the decoding process. The proposed code-mixing speech recognition system attains the accuracies of 56.4% and 53.0% for the Cantonese syllables and English words in code-mixing utterances.	acoustic cryptanalysis;acoustic model;algorithm;bigram;compiler;embedded system;grams;information;language model;lexicon;neural coding;search algorithm;speech recognition;syllable;text corpus;triangular function;vocabulary	Joyce Y. C. Chan;Houwei Cao;Pak-Chung Ching;Tan Lee	2009	IJCLCLP		natural language processing;cache language model;speech production;cued speech;speech recognition;speech corpus;computer science;speech;speech shadowing;acoustic model;linguistics;speech segmentation;language model;speech analytics	NLP	-19.750245759217634	-84.81024315380309	68223
5e247c3822de73017c3d91c6d87201243feefd69	integrating multiple observations for model-based single-microphone speech separation with conditional random fields	speech separation;speaker likelihood multiple observation model based single microphone speech separation conditional random fields speech mixture feature function;speech processing random processes source separation;speech processing;training;speech;hidden markov models speech signal to noise ratio spectral analysis training gaussian distribution speech recognition;conditional random fields;hidden markov models;conditional random fields speech separation;random processes;speech recognition;spectral analysis;signal to noise ratio;source separation;gaussian distribution	A single-microphone speech separation framework based on conditional random fields (CRFs) is proposed in this paper. Unlike factorial HMM, CRF does not have the conditional independence assumption on observations, thus different types of observations from the speech mixture can be integrated into the models through feature functions. Similar to factorial HMM, there is the statistical independence assumption on sources. Under this assumption, the two-source single-microphone speech separation problem can be expressed by two independent linear-chain CRFs. The separation problem becomes two pattern recognition problems, with respect to CRF models of the two sources. Experimental results show that by integrating initial separation outputs from factorial HMM with log power spectrum, fundamental frequency and speaker likelihoods of the mixture, CRF separation framework consistently improves the results from factorial HMM in terms of SNR, segmental SNR and PESQ.	conditional random field;experiment;hidden markov model;microphone;pesq;pattern recognition;signal-to-noise ratio;spectral density	Yu Ting Yeung;Tan Lee;Cheung-Chi Leung	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6287866	normal distribution;speech recognition;computer science;speech;machine learning;pattern recognition;speech processing;signal-to-noise ratio;conditional random field;statistics	ML	-17.338836503352	-91.806720602165	68304
dbf2cfd6e540d3a65ba695735d2e5f5c59a9ee5a	sorting by sound : arbitrary lexical ordering for transcribed thai text	conference paper	When either Thai or transcribed (Romanized) Thai is sorted alphabetically, words that sound very much alike usually end up far apart. maay and may are thrown to opposite ends of the letter m entries, even though mistaking one for the other causes problems for both foreign students who cannot speak clearly, and Thais who can't spell. This paper explains how and why the difficulty occurs, and shows why both Thai and transcription are inherently difficult to sort by sound. It introduces a method of preprocessing — deriving phonemic signatures — that lets us define improved lexical or dictionary orders, yet does not require anything but standard sorting code. The method can be applied to other languages — Lao, Khmer, and Burmese — that, like Thai, distinguish words on the basis of vowel length and/or tone.	dictionary;preprocessor;sorting;transcription (software);type signature	Doug Cooper	1995			speech recognition;engineering;linguistics;communication	PL	-25.722198262235345	-81.84521792055713	68332
c3ad55acbcd0a57ccef31306d19fa9629597fd77	accurate automatic visible speech synthesis of arbitrary 3d models based on concatenation of diviseme motion capture data	speech synthesis;visible speech;coarticulation modelling;motion capture data;visual speech synthesis;3d model;face animation;animated speech;speech animation	Abstract#R##N##R##N#We present a technique for accurate automatic visible speech synthesis from textual input. When provided with a speech waveform and the text of a spoken sentence, the system produces accurate visible speech synchronized with the audio signal. To develop the system, we collected motion capture data from a speaker's face during production of a set of words containing all diviseme sequences in English. The motion capture points from the speaker's face are retargeted to the vertices of the polygons of a 3D face model. When synthesizing a new utterance, the system locates the required sequence of divisemes, shrinks or expands each diviseme based on the desired phoneme segment durations in the target utterance, then moves the polygons in the regions of the lips and lower face to correspond to the spatial coordinates of the motion capture data. The motion mapping is realized by a key-shape mapping function learned by a set of viseme examples in the source and target faces. A well-posed numerical algorithm estimates the shape blending coefficients. Time warping and motion vector blending at the juncture of two divisemes and the algorithm to search the optimal concatenated visible speech are also developed to provide the final concatenative motion sequence. Copyright © 2004 John Wiley & Sons, Ltd.	3d modeling;concatenation;motion capture;speech synthesis	Jiyong Ma;Ronald A. Cole;Bryan L. Pellom;Wayne H. Ward;Barbara Wise	2004	Journal of Visualization and Computer Animation	10.1002/cav.11	natural language processing;computer vision;speech recognition;computer science;viseme;speech synthesis	Visualization	-15.382257632018753	-82.00457287481693	68362
82e75b3ae030904c50818291700648b82c3e1185	blstm-ctc combination strategies for off-line handwriting recognition		In this paper we present several combination strategies using multiple BLSTM-CTC systems. Given several feature sets our aim is to determine which strategies are the most relevant to improve on an isolated word recognition task (the WR2 task of the ICDAR 2009 competition), using a BLSTM-CTC architecture. We explore different combination levels: early integration (feature combination), mid level combination and late fusion (output combinations). Our results show that several combinations outperform single feature BLSTM-	baseline (configuration management);feature extraction;handwriting recognition;high-level programming language;international conference on document analysis and recognition;randomness extractor	Luc Mioulet;Gautier Bideault;Clément Chatelain;Thierry Paquet;Stephan Brunessaux	2015			speech recognition;intelligent character recognition;computer science;recurrent neural network;machine learning;pattern recognition;handwriting recognition;artificial neural network	NLP	-4.837591394733544	-87.77640204090066	68379
6c62729da2a2b732268eeb40504adb2f1a2dee37	confirmation detection in human-agent interaction using non-lexical speech cues		Even if only the acoustic channel is considered, human communication is highly multi-modal. Non-lexical cues provide a variety of information such as emotion or agreement. The ability to process such cues is highly relevant for spoken dialog systems, especially in assistance systems. In this paper, we focus on the recognition of non-lexical confirmations such as ”mhm”, as they enhance the system’s ability to accurately interpret human intent in natural communication. We implemented and evaluated a system for online detection of nonlexical confirmations. The architecture uses a Support Vector Machine to detect confirmations based on acoustic features. In a systematic comparison, several feature sets were evaluated for their performance on a corpus of human-agent interaction in a setting with naive users including elderly and cognitively impaired people. Our results show that using stacked formants as features yield an accuracy of 84% outperforming regular formants and MFCC or pitch based features for online classification.	acoustic cryptanalysis;dialog system;human–computer interaction;modal logic;spoken dialog systems;support vector machine	Mara Brandt;Britta Wrede;Franz Kummert;Lars Schillingmann	2017	CoRR		architecture;human–computer interaction;support vector machine;formant;speech recognition;computer science;spoken dialog systems;human communication;mel-frequency cepstrum;communication channel	NLP	-12.862163671840289	-84.9032291036237	68391
93749e09cbe1b7d03face8567ce30a7219b4dfd5	estonian emotional speech corpus: culture and age in selecting corpus testers	selection principle;corpus testers;corpus tester;test result;estonian language technology;estonian culture;theoretical model;estonian synthesizer;national program;certain culture;estonian emotional speech corpus	The Estonian Emotional Speech Corpus serves as the acoustic basis for emotional text-to-speech synthesis. Because the Estonian synthesizer is a TTSsynthesizer, we started off by focusing on read texts and the emotions contained in them. The corpus is built on a theoretical model and we are currently at the stage of verifying the components of the model. In the present article we give an overview of the corpus and the principles used in selecting its testers. Some studies show that people who have lived longer in a certain culture can more easily recognize vocal expressions of emotion that are characteristic of the culture without seeing the speaker’s facial expressions. We therefore decided not to use people under 30 years of age as testers of emotions in our theoretical model. We used two tests to verify the selection principles for the testers. In the first test, 27 young adults aged under 30 were asked to listen to and identify the emotion (joy, anger, sadness, neutral) of 35 sentences. We then compared the results with those of adults aged over 30. In the second test we asked 32 Latvians listen to the same sentences, and then compared the results with those of Estonians. Our analysis showed that younger and older testers, Estonians and Latvians perceive emotions quite differently. From these test results we can say that the selection principle of corpus testers, using people who are more familiar with Estonian culture, is acceptable.	acoustic cryptanalysis;identifier;sadness;speech corpus;speech synthesis;text corpus;theory;verification and validation	Rene Altrov;Hille Pajupuu	2010		10.3233/978-1-60750-641-6-25	psychology;developmental psychology;communication;literature	NLP	-11.359186805718883	-82.77202153428378	68394
6dbeed99bcaf65f0bccd33827c0c2b31d561e5d4	the effects of tone categories on the perception of mandarin vowels	keyboards;acoustics;auditory system;speech;analysis of variance;production;context	In order to explore the effects of tone categories on the perception of Mandarin vowels, the present study investigated the perceptual performance of vowel continua, which contained three Mandarin vowels /a, ɤ, u/ under four different tone conditions (i.e. high-level, mid-rising, falling-rising, and high-falling tones). The results showed that there was a shift in the categorical boundaries of /a/-/ɤ/ among the four different tone conditions. More specifically, participants generally tended to label the stimuli less as /a/ under the high-falling tone compared with the other three tone conditions. Moreover, the maximum identification rate of /ɤ/ was much lower under the falling-rising tone in contrast with other tone conditions. These findings suggest that vowel perceptions may be strongly influenced by the pitch properties of different tone categories.	high- and low-level;super robot monkey team hyperforce go!	Hao Zhang;Fei Chen;Nan Yan;Lan Wang;Yu Chen;Feng Shi	2016	2016 10th International Symposium on Chinese Spoken Language Processing (ISCSLP)	10.1109/ISCSLP.2016.7918413	speech recognition;analysis of variance;acoustics;speech;linguistics;statistics	HCI	-10.538169827804577	-81.84157707648681	68556
dbd2338fd9d30bc563224e1ff6cb2391024b1505	efficient language identification using anchor models and support vector machines	databases;speaker identification;support vector machines;speech processing;nist 2003 language identification lid anchor model support vector machine speaker identification speaker indexing speech utterance;speech;natural languages;testing;data mining;support vector machines natural languages testing indexing databases computer science speech support vector machine classification error analysis data mining;nist 2003;speaker recognition;error analysis;anchor model;speaker indexing;indexing;support vector machines natural languages speaker recognition speech processing;indexation;equal error rate;language identification;support vector machine classification;speech utterance;computer science;support vector machine;lid	Anchor models have been recently shown to be useful for speaker identification and speaker indexing. The advantage of the anchor model representation of a speech utterance is its compactness (relative to the original size of the utterance) which is achieved with only a small loss of speaker-relevant information. This paper shows that speaker-specific anchor model representation can be used for language identification as well, when combined with support vector machines for doing the classification, and achieve state-of-the-art identification performance. On the NIST-2003 language identification task, it has reached an equal error rate of 4.8% for 30 second test utterances	language identification;speaker recognition;support vector machine	Elad Noor;Hagai Aronowitz	2006	2006 IEEE Odyssey - The Speaker and Language Recognition Workshop	10.1109/ODYSSEY.2006.248101	natural language processing;speech recognition;computer science;pattern recognition	NLP	-16.097748931932024	-91.32615677553022	68715
fa93ebdd7c80312df4611b7af8090bce726b59e4	early mfcc and hpcp fusion for robust cover song identification		While most schemes for automatic cover song identification have focused on note-based features such as HPCP and chord profiles, a few recent papers surprisingly showed that local self-similarities of MFCCbased features also have classification power for this task. Since MFCC and HPCP capture complementary information, we design an unsupervised algorithm that combines normalized, beat-synchronous blocks of these features using cross-similarity fusion before attempting to locally align a pair of songs. As an added bonus, our scheme naturally incorporates structural information in each song to fill in alignment gaps where both feature sets fail. We show a striking jump in performance over MFCC and HPCP alone, achieving a state of the art mean reciprocal rank of 0.87 on the Covers80 dataset. We also introduce a new medium-sized hand designed benchmark dataset called “Covers 1000,” which consists of 395 cliques of cover songs for a total of 1000 songs, and we show that our algorithm achieves an MRR of 0.9 on this dataset for the first correctly identified song in a clique. We provide the precomputed HPCP and MFCC features, as well as beat intervals, for all songs in the Covers 1000 dataset for use in further research.	algorithm;align (company);benchmark (computing);precomputation	Christopher J. Tralie	2017			fusion;speech recognition;artificial intelligence;clique;machine learning;normalization (statistics);computer science;mel-frequency cepstrum;mean reciprocal rank;chord (music)	ML	-13.017334493699977	-89.6441813421063	68716
3efd6b2ab1d96342d48ebda78833420108f25189	active learning: theory and applications to automatic speech recognition	modelizacion;evaluation performance;fiabilidad;reliability;learning algorithm;performance evaluation;transcription automatique;cost function;learning;large vocabulary continuous speech recognition;signal sampling;random sampling;evaluacion prestacion;active learning;acoustic modeling;speech processing;language modeling;tratamiento palabra;procede discontinu;traitement parole;selective sampling;tratamiento lenguaje;methode acoustique;algorithme apprentissage;funcion coste;transcripcion automatica;machine learning acoustic modeling active learning language modeling large vocabulary continuous speech recognition;aprendizaje;modelisation;automatic speech recognition humans speech recognition sampling methods machine learning algorithms machine learning vocabulary stochastic processes statistics cost function;automatic speech recognition;accuracy;apprentissage;automatic recognition;acoustic method;precision;reconocimiento voz;machine learning;language processing;fiabilite;muestreo aleatorio;metodo acustico;adaptive learning;traitement langage;batch process;learning artificial intelligence speech recognition signal sampling;speech recognition;fonction cout;vocabulary continuous speech recognition automatic speech recognition adaptive learning active learning algorithm lattice output language modeling;procedimiento discontinuo;reconnaissance parole;learning artificial intelligence;automatic transcription;echantillonnage aleatoire;algoritmo aprendizaje;modeling;on line algorithm;supervision;language model;reconocimiento automatico;reconnaissance automatique;on line learning	We are interested in the problem of adaptive learning in the context of automatic speech recognition (ASR). In this paper, we propose an active learning algorithm for ASR. Automatic speech recognition systems are trained using human supervision to provide transcriptions of speech utterances. The goal of Active Learning is to minimize the human supervision for training acoustic and language models and to maximize the performance given the transcribed and untranscribed data. Active learning aims at reducing the number of training examples to be labeled by automatically processing the unlabeled examples, and then selecting the most informative ones with respect to a given cost function for a human to label. In this paper we describe how to estimate the confidence score for each utterance through an on-line algorithm using the lattice output of a speech recognizer. The utterance scores are filtered through the informativeness function and an optimal subset of training samples is selected. The active learning algorithm has been applied to both batch and on-line learning scheme and we have experimented with different selective sampling algorithms. Our experiments show that by using active learning the amount of labeled data needed for a given word accuracy can be reduced by more than 60% with respect to random sampling.	acoustic cryptanalysis;active learning (machine learning);automated system recovery;canonical account;experiment;feedback;finite-state machine;information;language model;loss function;mathematical optimization;monte carlo method;online algorithm;online and offline;online machine learning;sampling (signal processing);speech recognition;test set	Giuseppe Riccardi;Dilek Z. Hakkani-Tür	2005	IEEE Transactions on Speech and Audio Processing	10.1109/TSA.2005.848882	semi-supervised learning;natural language processing;unsupervised learning;speech recognition;computer science;machine learning;accuracy and precision;stability;active learning;statistics;language model;generalization error	ML	-20.909876348529785	-90.90281554879057	68795
b6617e9e6e69bd941e594c7292b507d757fabefb	speech recognition chip for monosyllables	low power system design;integrated circuit layout;intellectual property;hidden markov model;real time;system level modeling;speech coding;system on a chip;chip;linear predictive coding;speech recognition hidden markov models linear predictive coding speech analysis vectors circuits algorithm design and analysis hardware feature extraction speech coding;hidden markov models;speaker independent;cmos digital integrated circuits;integrated circuit layout real time systems speech recognition digital signal processing chips hidden markov models learning artificial intelligence cmos digital integrated circuits speech recognition equipment linear predictive coding feature extraction speech coding;feature extraction;speech recognition equipment;speech recognition;0 35 micron monosyllables real time speech recognition chip hidden markov model hmm parameters updating speaker independent recognition method short speech frame symbol code learning circuit interface vdec rohm submicron process chip layout lpc analysis feature vector extraction dsp chip 0 6 micron;digital signal processing chips;cores;learning artificial intelligence;parameterized architectures;real time systems	In the paper, we present a real-time speech recognition chip for monosyllables such as A, B, ..., etc. The chip recognizes up to 64 monosyllables based on the Hidden Markov Model (HMM), which is a well known speaker-independent recognition method. The chip accepts a short-speech frame including 256 16-bit digitized samples corresponding to 11.6 msec period, and outputs the 6-bit symbol code of monosyllables for 16 short-frames (corresponding to 185.6 msec). A learning circuit to update HMM parameters for the recognition chip has also been designed, and the recognition chip includes an interface to the learning circuit. We have fabricated the recognition chip by VDEC Rohm 0.6 um process on a 4.5 mm x 4.5 mm chip. We have also made a layout of the entire circuit including the learning circuit by VDEC Rohm 0.35 um process on a 4.9 mm x 4.9 mm chip.	16-bit;algorithm;field-programmable gate array;hidden markov model;markov chain;real-time clock;real-time computing;real-time transcription;speech recognition;syllable;unified model	Kazuhiro Nakamura;Qiang Zhu;Shinji Maruoka;Takashi Horiyama;Shinji Kimura;Katsumasa Watanabe	2001		10.1145/370155.370422	chip;system on a chip;multi-core processor;electronic engineering;linear predictive coding;speech recognition;feature extraction;computer science;machine learning;speech coding;integrated circuit layout;intellectual property;hidden markov model	ML	-8.359415965420695	-96.79353833464265	68961
9447ac6a76111a981e9ba175b393f554d26a4d84	text-dependent speaker recognition by compressed feature-dynamics derived from sinusoidal representation of speech	spectral analysis computational complexity feature extraction signal classification signal representation speaker recognition;speaker recognition;speaker recognition speech mel frequency cepstral coefficient spectrogram speech recognition hidden markov models complexity theory;nearest neighbor classifier;sinogram csd msri method text dependent speaker recognition compressed feature spectral dynamic speech sinusoidal representation spectral envelope based feature temporal spectral dynamics	Prevalent speaker recognition methods use only spectral-envelope based features such as MFCC, ignoring the rich speaker identity information contained in the temporal-spectral dynamics of the entire speech signal. We propose a new feature for speaker recognition based on sinusoidal representation of speech called compressed spectral dynamics (Sinogram-CSD), which effectively captures such spectral dynamics and the inherent speaker identity. The discriminative power of CSD allows classification to remain simple. The proposed CSD-MSRI method uses a simple nearest neighbor classifier to deliver performance competitive to conventional MFCC+DTW based text-dependent speaker recognition methods at significantly lower complexity.	cambridge structural database;nearest neighbour algorithm;speaker recognition	Amitava Das;Gokul Chittaranjan;V. Srinivasan	2008	2008 16th European Signal Processing Conference		natural language processing;speaker recognition;speaker diarisation;speech recognition;computer science;dynamic time warping;pattern recognition	Vision	-11.259625361203732	-90.69657444634055	68974
1d34ebe7dc96fc6c54b6bbfb6fbcc487db00ae7d	star: articulation training for young children	speech production;hidden markov model	The Speech Training, Assessment, and Remediation (STAR) system is intended to assist Speech and Language Pathologists in treating children with articulation problems. The system is embedded in an interactive video game that is set in a spaceship and involves teaching aliens to “understand” selected words by spoken example. The sequence of events leads children through a series of successively more diff icult speech production tasks, beginning with CV syllables and progressing to words/phrases. Word selection is further tailored to emphasize the contrastive nature of phonemes by the use of minimal pairs (e.g., run/won) in production sets. To assess children’s speech, a discrete hidden Markov model recognition engine is used[1]. Phone models were trained on the CMU Kids database[2]. Performance of the HMM recognizer was compared to perceptual ratings of speech recorded from children who substitute /w/ for /r/. The difference in log likelihood between /r/ and /w/ models correlates well with perceptual ratings of utterances containing substitution errors, but very poorly for correctly articulated examples. The poor correlation between perceptual and machine ratings for correctly articulated utterances may be due to very restricted variance in the perceptual data for those utterances.	aliens;biconnected component;diff utility;embedded system;finite-state machine;hidden markov model;markov chain;spaceship (cellular automaton)	H. Timothy Bunnell;Debra Yarrington;James B. Polikoff	2000			phone;speech recognition;perception;hidden markov model;natural language processing;speech training;correlation;artificial intelligence;interactive video;computer science;speech production	ML	-13.780186942471506	-83.3996987804935	69030
7db409f00fadcef899a6e84db2ed6f50003af3f3	a new hearing aid fitting strategy for severe to profound hearing loss			curve fitting	Tobias Herzke;Sabine Haumann;Volker Hohmann	2012			audiology	Vision	-6.554150543525254	-85.24686858604514	69093
fc3dee0033507de8fb1f9123ee0f9eeba21659cd	a non-intrusive signal-based model for speech quality evaluation using automatic classification of background noises.	quality evaluation;automatic classification	This paper describes an original method for speech quality evaluation in the presence of different types of background noises for a range of communications (mobile, VoIP, RTC). The model is obtained from subjective experiments described in [1]. These experiments show that background noise can be more or less tolerated by listeners, depending on the sources of noise that can be identified. Using a classification method, the background noises can be classified into four groups. For each one of the four groups, a relation between loudness of the noise and speech quality is proposed.	experiment;real-time clock;statistical classification	Adrien Leman;Julien Faure;Etienne Parizet	2009			computer vision;speech recognition;computer science;pattern recognition	NLP	-10.543557861156716	-89.39175087737895	69156
2400a28462ced3dee04d7d7742b2180164af767e	driftin' down the scale: dynamic time warping in the presence of pitch drift and transpositions			dynamic time warping	Simon Waloschek;Aristotelis Hadjakos	2018				Theory	-13.45571844835147	-86.21075441906925	69177
3c60635910689c2f6c2a9b49953e2dec3935d9a7	language modeling approach to retrieval for sms and faq matching		"""Short Messaging service popularly known as \SMS"""" has seen growth due to the growth in Mobile phone users. A mobile phone is con- sidered as a cheap and easy device for communication. It is also used as a source to acquire and spread information. SMS based FAQ Retrieval task proposed in FIRE 2011 aims to provide the required information from frequently asked questions (FAQs). Challenge is to nd a question from corpora of FAQs that best answers/matches with the SMS query. But, SMS queries are noisy as users tend to compress text by omitting letters, using slang, etc. This is observed due to a cap on the length of messages (160 characters constitute one SMS), lack of screen space (which makes reading large amounts of text dicult). In this paper, we propose a method using language modeling approach to match noisy SMS text with right FAQ. We extended this framework to match SMS queries with Cross-language FAQs. Results are promising for monolin- gual retrieval applied on English, Hindi and Malayalam languages."""	language model	Aditya Mogadala;Rambhoopal Kothwal;Vasudeva Varma	2011		10.1007/978-3-642-40087-2_12	concatenated sms;computer science;data mining;world wide web;information retrieval	Vision	-24.067376329955362	-81.20499422585509	69339
7924e2ec199225527ea9505555676361f1e4374c	a proposal for incremental dialogue evaluation	natural language;speech;processing	"""The SIS community has made progress recently toward evaluating SLS systems that deal with dialogue, but there is still considerable work that needs to be done in this area. Our goal is to develop incremental ways to evaluate dialogue processing, not just going from Class D1 (dialogue pairs) to Class D2 (dialogue triples), but measuring aspects of dialogue processing other than length. We present two suggestions; one for extending the common evaluation procedures for dialogues, and one for modifying the scoring metric. I N T R O D U C T I O N There is no single dialogue problem. By its nature, dialogue processing is composed of many different capabilities matched to many different aspects of the problem. It is reasonable to expect that dialogue evaluation methodologies should be multffaceted to reflect this richness of structure. Ideally, each new addit ion to the set of evaluat ion methodologies should test a different aspect of dialogue processing, and should be harder than the methodologies that came before iL We present two suggestions: one which extends the common evaluation procedure in order to test one new aspect of dialogues, and one which modifies the scoring metric. Difference: 1. Conversation is cooperative, but a game is competitive. 2. In chess, the goal is clear (checkmate), but in a conversational dialogue, the goal is less dear. 3. In a chess game, any state can be completely and concisely represented by a single board position; in a dialogue it is not known what comprises a state, nor how to represent it. Like the game tree for chess, the human/computer dialogue tree is enormous, as indicated in figure 1. There are usually hundreds or thousands of alternatives the human may produce. The number of responses the system can make is much smaller; some responses may be clearly wrong, but seldom is there a single """"right"""" or """"best"""" response (just as there is seldom a single such move in chess). Even when striving for the same goal, two different people are very likely to choose very different paths. An Analogy with Chess We as a community have been thinking about dialogue evaluation in terms of whether the systems we are building give the """"right"""" answer (the one the wizard gave, or the one agreed upon by the Principles of Interpretation) at every step. We have been trying to come up with a methodology to measure whether our systems can reproduce the wizard's answers at each step of a lengthy dialogue. But is this a reasonable approach? Participating in a dialogue, whether between two humans or between a human and a machine, bears a striking resemblance to playing a complex game such as chess."""	dialog system;dialog tree;standard sea level;state (computer science);wizard (software)	Madeleine Bates;Damaris M. Ayuso	1991			natural language processing;speech recognition;computer science;speech;processing;linguistics;natural language	NLP	-25.55300235984712	-81.7199322624947	69363
a71f3a15bb3725b9ba1ac4bace044e2e0661fdf0	making searchable melodies: human versus machine		Systems that find music recordings based on hummed or sung, melodic input are called Query-By-Humming (QBH) systems. Such systems employ search keys that are more similar to a cappella singing than the original recordings. Successful deployed systems use human computation to create these search keys: hand-entered MIDI melodies or recordings of a cappella singing. Tunebot is one such system. In this paper, we compare search results using keys built from two automated melody extraction system to those gathered using two populations of humans: local paid singers and Amazon Turk workers.	amazon mechanical turk;human-based computation;midi;population;query by humming;the turk;tunebot	Mark Brozier Cartwright;Zafar Rafii;Jinyu Han;Bryan Pardo	2011			speech recognition;artificial intelligence	HCI	-16.660256559400853	-80.70598736502467	69375
0203ec9fba012b59f66ab2571da21cdc09337e62	is that your final answer	decision making process;evaluation;language learning;machine translation;native language	The purpose of this research is to test the efficacy of applying automated evaluation techniques, originally devised for the evaluation of human language learners, to the output of machine translation (MT) systems. We believe that these evaluation techniques will provide information about both the human language learning process, the translation process and the development of machine translation systems. This, the first experiment in a series of experiments, looks at the intelligibility of MT output. A language learning experiment showed that assessors can differentiate native from non-native language essays in less than 100 words. Even more illuminating was the factors on which the assessors made their decisions. We tested this to see if similar criteria could be elicited from duplicating the experiment using machine translation output. Subjects were given a set of up to six extracts of translated newswire text. Some of the extracts were expert human translations, others were machine translation outputs. The subjects were given three minutes per extract to determine whether they believed the sample output to be an expert human translation or a machine translation. Additionally, they were asked to mark the word at which they made this decision. The results of this experiment, along with a preliminary analysis of the factors involved in the decision making process will be presented here.	experiment;intelligibility (philosophy);machine translation;natural language processing	Florence Reeder	2001		10.3115/1072133.1072194	natural language processing;speech recognition;example-based machine translation;computer science;evaluation of machine translation;rule-based machine translation;communication;machine translation software usability	NLP	-26.963033878991094	-82.80143689396392	69445
db98921fb0669cf32d4bc8f5bbdadbbf799409f6	quality dependent multimodal fusion of face and iris biometrics	databases;standards;measurement;iris recognition;face	Although iris is known as the most accurate and face as the most accepted in biometrics, these distinct modalities encounter variability in data in real-world applications. Such limitation can be overcome by a multimodal system based on both traits. Additionally, by conditioning the multimodal fusion on quality, useful information can be extracted from lower quality measures rather than rejecting them out of hand. This paper suggests a dynamic weighted sum fusion that exploits an iris occlusion-based quality metric while combining unimodal scores. Instead of incorporating the quality of the gallery and probe images separately, a single quality metric for each gallery-probe comparison was used. Two strategies for integrating this metric into score-level fusion were explored. Experiments on the IV2 multimodal database including multiple variabilities proved that the proposed method improves some best current non quality-based fusion schemes by more than 30% in terms of Equal Error Rates.	artificial intelligence;authentication;biometrics;modality (human–computer interaction);multimodal interaction;multiple biometric grand challenge;oracle fusion architecture;spatial variability;uncontrolled format string;weight function	Nefissa Khiari Hili;Christophe Montagne;Sylvie Lelandais;Kamel Hamrouni	2016	2016 Sixth International Conference on Image Processing Theory, Tools and Applications (IPTA)	10.1109/IPTA.2016.7820954	face;computer vision;speech recognition;computer science;iris recognition;mathematics;measurement	Robotics	-10.568171502156652	-95.82127439269244	69477
0908c453e4e5802ca76d6da1c254f08ad3b749ce	unlimited vocabulary grapheme to phoneme conversion for korean tts	morpheme phonetic pattern dictionary;grapheme-to-phoneme conversion method;phonetic morpheme;morpheme-to-phoneme conversion module;grapheme-to-phoneme conversion performance;morpheme normalization;ccv conversion rule;unlimited vocabulary grapheme;korean tts;phoneme connectivity check;conversion method;phoneme connectivity	This paper describes a grapheme-to-phoneme conversion method using phoneme connectivity and CCV conversion rules. The method consists of mainly four modules including morpheme normalization, phrase-break detection, morpheme to phoneme conversion and phoneme connectivity check. The morpheme normalization is to replace non-Korean symbols into standard Korean graphemes. The phrase-break detector assigns phrase breaks using part-of-speech (POS) information. In the morpheme-to-phoneme conversion module, each morpheme in the phrase is converted into phonetic patterns by looking up the morpheme phonetic pattern dictionary which contains candidate phonological changes in boundaries of the morphemes. Graphemes within a morpheme are grouped into CCV patterns and converted into phonemes by the CCV conversion rules. The phoneme connectivity table supports grammaticality checking of the adjacent two phonetic morphemes. In the experiments with a corpus of 4,973 sentences, we achieved 99.9% of the graphemeto-phoneme conversion performance and 97.5% of the sentence conversion performance. The full Korean TTS system is now being implemented using this conversion method.	card security code;database normalization;dictionary;experiment;netware file system;part-of-speech tagging;phoneme;text corpus;vocabulary	Byeongchang Kim;Wonil Lee;Gary Geunbae Lee;Jong-Hyeok Lee	1998			natural language processing;speech recognition;part of speech;computer science;linguistics	NLP	-21.819043608153677	-81.1773902708621	69545
0534472c4b02eb7fc86eeff0af6775e72cd23d5f	an evaluation of non-standard features for grapheme-to-phoneme conversion		Machine learning methods for grapheme-to-phoneme (G2P) conversion are popular, but the features used in the literature are most often simply a window of context letters, despite the availability of other features. In this paper, a set of features beyond the seven-letter window, termed non-standard features, are systematically evaluated for American English, using decision trees. The results show that adding nonstandard features to a seven-letter window gives clear improvements for English, with the most important features being the previous three phone sequences predicted, an initial prediction of lexical stress location, and a window of vowel letters around the current letter.	decision tree;machine learning;window function	Gabriel Webster;Norbert Braunschweiler	2008			speech recognition;artificial intelligence;american english;phone;pattern recognition;decision tree;vowel;computer science;grapheme	NLP	-20.91587846991973	-81.320888594966	69550
2e185e47fb76a28377ea9ef5a46c45280dcb8928	predicting and compensating for lexicon access errors	adaptive testing;intelligent interfaces;tip of the tongue;dictionary search;character similarity;multiple choice;foreign language;adaptive vocabulary testing	Learning a foreign language is a long, error-prone process, and much of a learner's time is effectively spent studying vocabulary. Many errors occur because words are only partly known, and this makes their mental storage and retrieval problematic. This paper describes how an intelligent interface may take advantage of the access structure of the mental lexicon to help predict the types of mistakes that learners make, and thus compensate for them. We give two examples, firstly a dictionary interface which circumvents the tip-of-the-tongue problem through search-by-similarity, and secondly an adaptive test generator which leverages user errors to generate plausible multiple-choice distractors.	access structure;cognitive dimensions of notations;dictionary;lexicon;vocabulary	Lars Yencken;Timothy Baldwin	2011		10.1145/1943403.1943432	multiple choice;foreign language;natural language processing;speech recognition;computer science;artificial intelligence;machine learning;linguistics;computerized adaptive testing;world wide web	AI	-27.12361674927558	-83.36484780709131	69626
adfaa67154a1ab8a56792328feb061f4b469c50a	composite conversation gesture synthesis using layered planning	linkage	Automatic generation of gestures linked to conversation is required in CG animation and game creation. When multiple gestures are combined in the conventional method, continuity or matching at the motion level is not considered. As a result, a machinelike impression is produced, such as a return to the upright position each time a gesture is performed. To deal with this problem, this paper proposes a method of composite gesture generation which considers continuity of motion and linkage of gestures and generates more natural conversational motions. As the first step in the proposed method, two different networks that represent the continuity and linkage of gestures are constructed. Then the combinations of gestures which are likely for a given conversational sentence are expanded as a tree, and a plan for the series of conversation motions is generated by evaluating the conversational content and gesture continuity. In the proposed method, composite gestures can be generated with allowance for continuity of motion. Consequently, more humanlike conversational motions can be generated while preserving gestures which are important in transmitting the semantics of the conversation, such as nodding while pointing, or scratching the head with one hand while placing the other hand on the hip. © 2007 Wiley Periodicals, Inc. Syst Comp Jpn, 38(10): 58–68, 2007; Published online in Wiley InterScience (). DOI 10.1002&sol;scj.20532	speech synthesis	Atsushi Nakano;Junichi Hoshino	2007	Systems and Computers in Japan	10.1002/scj.20532	speech recognition;computer science;artificial intelligence;linkage	Robotics	-15.332222861717007	-81.87983387004402	69650
565794db65c742e9273ca7d422c9bdee218e95c0	noise robust speech recognition using recent developments in neural networks for computer vision	parametric rectified linear unit automatic speech recognition noise robustness convolutional neural network;convolution neural networks speech recognition acoustics hidden markov models noise measurement training;noise robustness;automatic speech recognition;parametric rectified linear unit;prelu noise robust speech recognition convolutional neural networks computer vision cnn image classification learning dynamic features modulation frequency analysis convolution filters parametric rectified linear unit;convolutional neural network;speech recognition computer vision image classification neural nets	Convolutional Neural Networks (CNNs) are superior to fully connected neural networks in various speech recognition tasks and the advantage is pronounced in noisy environments. In recent years, many techniques have been proposed in the computer vision community to improve CNN's classification performance. This paper considers two approaches recently developed for image classification and examines their impacts on noisy speech recognition performance. The first approach is to increase the depth of convolution layers. Different approaches to deepening the CNNs are compared. In particular, the usefulness of learning dynamic features with small convolution layers that perform convolution in time is shown along with a modulation frequency analysis of the learned convolution filters. The second approach is to use trainable activation functions. Specifically, the use of a Parametric Rectified Linear Unit (PReLU) is investigated. Experimental results show that both approaches yield significant improvements in performance. Combining the two approaches further reduces recognition errors, producing a word error rate of 11.1% in the Aurora4 task, the best published result for this corpus, with a standard one-pass bi-gram decoding set-up.	activation function;artificial neural network;computer vision;convolution;convolutional neural network;frequency analysis;modulation;rectifier (neural networks);speech recognition;word error rate	Takuya Yoshioka;Katsunori Ohnishi;Fuming Fang;Tomohiro Nakatani	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472775	computer vision;speech recognition;computer science;machine learning;pattern recognition;time delay neural network;deep learning;convolutional neural network	Vision	-15.938228677403167	-89.608968499269	69845
0286c7a02180e2cd76d396371f1d6652fa60a63a	multigrained model adaptation with map and reference speaker weighting for text independent speaker verification	map;background modeling;speaker recognition maximum likelihood estimation;model adaptation;maximum likelihood estimation;speaker recognition evaluation;model complexity;speaker verification;speaker recognition;text independent speaker verification;adaptation model speaker recognition statistics nist loudspeakers interpolation telecommunications research and development data engineering speech;speaker adaptation method;universal background model;maximum a posteriori;speaker recognition evaluation multigrained model adaptation map reference speaker weighting text independent speaker verification maximum a posteriori universal background model speaker adaptation method;speaker adaptation;reference speaker weighting;multigrained model adaptation	When traditional maximum a posteriori (MAP) adaptation is used to adapt a universal background model (UBM), some model components with little or no enrollment data would remain unchanged in the derived speaker model. These model components would have weak discriminative capability over the background model, and would impair subsequent verification performance. In this paper, we present a new speaker adaptation method which combines MAP and reference speaker weighting (RSW) adaptation in a hierarchical, multigrained mode. It enables all model components to be updated in a way that strikes a good balance between model complexity and available data. The experimental results of NIST speaker recognition evaluation confirmed the effective performance increase with this new method compared with using MAP or RSW adaptation techniques alone	speaker recognition	Xianyu Zhao;Yuan Dong;Jun Luo;Hao Yang;Haila Wang	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1660170	natural language processing;speaker recognition;speech recognition;computer science;maximum a posteriori estimation;map;pattern recognition;maximum likelihood;statistics	Robotics	-18.179777360267504	-91.96853923256089	70017
bee265d0aca44d7f70624a85daf22006f47eb913	syllable lengths in russian, bulgarian, old church slavonic and slovene			syllable	Otto A. Rottmann	2002	Glottometrics		syllable;bulgarian;linguistics;history;old church slavonic	Theory	-29.163748942224263	-82.6379338137451	70064
5647308aed4fb6903f04ab29891b8fa30773acae	speech event detection using svm and nmd	support vector machines deconvolution markov processes speech recognition;detectors;front end;support vector machines;speech event detection;hidden markov model;acoustics;training;speech segmentation;negative matrix deconvolution;hidden markov models speech event detection negative matrix deconvolution support vector machines speech segmentation;speech;event detection;event detection support vector machines support vector machine classification acoustic signal detection detectors automatic speech recognition deconvolution switches hidden markov models information resources;hidden markov models;deconvolution;speech recognition;markov processes;support vector machine	In this paper we propose a speech event detector that segments speech signals in terms of four broad acoustic-phonetic classes of events. Frame-based detection was carried out using support vector machines (SVM). Non-negative matrix deconvolution (NMD) was used in order to switch from a frame-based detection to a segment-based detection. Results obtained using the TIMIT corpus are reported and compared to a broad class detector based on hidden Markov models (HMM) with a MFCC front-end. It was found that the proposed SVM/NMD system outperforms the HMM system in what concerns to accuracy and also to the quality of he detected boundaries.	acoustic cryptanalysis;deconvolution;hidden markov model;markov chain;non-negative matrix factorization;sensor;support vector machine;timit	Carla Lopes;Fernando Perdigão	2007	2007 9th International Symposium on Signal Processing and Its Applications	10.1109/ISSPA.2007.4555570	support vector machine;speech recognition;computer science;machine learning;pattern recognition;hidden markov model	Arch	-15.610463750191881	-89.7462698522222	70080
d0ae18a0443f9ad38670349978610cf03595ec60	acoustic-articulatory evaluation of the upper vowel-formant region and its presumed speaker-specific potency		We present some evidence indicating that phonetic distinctiveness and speaker individuality, are indeed manifested in vowels' vocal-tract shapes estimated from the lower and the upper formant-frequencies, respectively. The methodology developed to demonstrate this dichotomy, rst implicates Schroeder's [8] acoustic-articulatory model which can be coerced to yield, on a per-vowel and a per-speaker basis, area-function approximations to vocal-tract shapes of di ering formant components. Using ten steady-state vowels recorded in /hVd/-context, ve times at random, by four adult-male speakers of Australian English, the variability of resulting shapes aligned at mid-length was then measured on an intraand an inter-speaker basis. Gross shapes estimated from the lower formants, were indeed found to cause the largest spread amongst the vowels of individual speakers. By contrast, the more detailed shapes obtained by recruiting certain higher formants of the front and the back vowels, accounted for the largest spread amongst the speakers. Collectively, these results contribute a quasi-articulatory substantiation of a long-standing view on the speaker-speci c potency of the upper formant region of spoken vowels, together with some useful implications for automatic speech and speaker recognition.	acoustic cryptanalysis;approximation;needham–schroeder protocol;spatial variability;speaker recognition;steady state;tract (literature)	Frantz Clermont;Parham Mokhtari	1998			speech recognition;artificial intelligence;formant;pattern recognition;computer science;potency;vowel	HCI	-10.376362984713033	-81.86262300949961	70096
dc6cf839bb2a0a700c2de463ef038d2d671f6ee2	a survey of audio-based music classification and annotation	signal classification audio signal processing music pattern classification query processing;classification algorithm;audio signal processing;instruments;timbre;query processing;semantics;acoustic signal processing;audio based music classification;music information retrieval acoustic signal processing classification algorithms feature extraction;music querying;feature extraction;music information retrieval;signal classification;classification algorithms;taxonomy;music industry;pattern classification;music querying audio based music classification music annotation music information retrieval;book reviews;genre classification;music;feature extraction timbre book reviews taxonomy instruments semantics;music annotation	Music information retrieval (MIR) is an emerging research area that receives growing attention from both the research community and music industry. It addresses the problem of querying and retrieving certain types of music from large music data set. Classification is a fundamental problem in MIR. Many tasks in MIR can be naturally cast in a classification setting, such as genre classification, mood classification, artist recognition, instrument recognition, etc. Music annotation, a new research area in MIR that has attracted much attention in recent years, is also a classification problem in the general sense. Due to the importance of music classification in MIR research, rapid development of new methods, and lack of review papers on recent progress of the field, we provide a comprehensive review on audio-based classification in this paper and systematically summarize the state-of-the-art techniques for music classification. Specifically, we have stressed the difference in the features and the types of classifiers used for different classification tasks. This survey emphasizes on recent development of the techniques and discusses several open issues for future research.	graph coloring;information retrieval;sound card;speech recognition;statistical classification	Zhouyu Fu;Guojun Lu;Kai Ming Ting;Dengsheng Zhang	2011	IEEE Transactions on Multimedia	10.1109/TMM.2010.2098858	speech recognition;audio signal processing;feature extraction;computer science;machine learning;music industry;music;semantics;information retrieval;taxonomy	Web+IR	-6.281499732159933	-91.6548754872016	70147
4166800d7d049ff7b30e74091117c00b6fbf2726	co-ordinative ellipsis in russian texts: problem of description and restoration		ABST]~CTo Russian elliptic const~-uctions are exami.ned from the point of view of syn~ tactic analysiso Reciprocal elements in a co-ordi~Ltive elliptic sentence are exposed and possJ.ble types Of the: i / ' similarJ.ty are explored. Linear formulae of el].ipsis for most textual cases az.e constl~cted ~md statistics oJ~ their use is discussed, As a re~ sult the main steps of ellipsis restoration algorit[bm are outlined,	circuit restoration;image restoration;point of view (computer hardware company)	Igor A. Bolshakov	1988				Vision	-28.43154724698165	-81.3897506508776	70148
9242011aac9fd7a691ec9bf3513210e594c6afa6	evaluating unsupervised language model adaptation methods for speaking assessment		In automated speech assessment, adaptation of language models (LMs) to test questions is important to achieve high recognition accuracy However, for large-scale language tests, the ordinary supervised training, which uses an expensive and time-consuming manual transcription process, is hard to utilize for LM adaptation. In this paper, several LM adaptation methods that require either no manual transcription process or just a small amount of transcriptions have been evaluated. Our experiments suggest that these LM adaptation methods can allow us to obtain considerable recognition accuracy gain with no or low human transcription cost.	experiment;language model;transcription (software)	Shasha Xie;Lei Chen	2013			natural language processing;speech recognition;computer science;machine learning	NLP	-20.296800361706616	-84.247759497726	70176
adb6f3a822377cdc71c8eb0b5c3cf647946355c2	development of under-resourced bahasa indonesia speech corpus		Although Bahasa Indonesia is used by about 263 milion people in the world, it is calssified into an under- resourced language. In this paper we outlined the development of casual sentences of Bahasa Indonesia speech corpus in which contains a speech database and its transcription. Firstly, we selected casual Bahasa Indonesia sentences from movie and drama trasncript and formed 1029 declarative sentences and 500 question sentences, respectively. We hired six professional radio news readers to utter the sentences to avoid local dialect in sound-proof booth. Then segmentation and labeling was performed to make create transcription including the time label of each invidual phoneme. To ensure the quality of the database, we manually inspected the waveform and the frequency of the individual sentences using spectrogram. The results suggest that the speech corpus may be used for speech processing project like speech recognition and speech synthesis. In the on-going research, we are developing high quality of speech synthesis, namely speaker adaptation and speaker averaging.	display resolution;spectrogram;speech corpus;speech processing;speech recognition;speech synthesis;transcription (software);waveform	Elok Cahyaningtyas;Dhany Arifianto	2017	2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)	10.1109/APSIPA.2017.8282191	speech recognition;speech corpus;speech processing;spectrogram;speech synthesis;computer science;casual	NLP	-21.153824254102823	-83.48169927275049	70188
cfa7cde6aba3654ab1b35b00b8dfb07485684f19	an investigation of spectral feature partitioning for replay attacks detection		Replay attacks from unseen utterances poses a significant challenge in Anti-Spoofing Detection. In this paper, we propose a statistical measure based on the Rayleigh Quotient in order to investigate a feature partition capable of discerning genuine and playback speech under unseen conditions. The Log- Magnitude Spectrum (LMS) of the utterances is used in this study. Using the proposed measure, we analyze the frequency bands of the LMS based on the amount of discriminative information between the scatter matrices of the genuine and spoof utterances. This allows us to determine the optimal frequency bands required for replay attacks detection. In addition, we further investigate the effects of training our models using voiced and unvoiced portions of the utterances. We conducted our experiments based on the ASVspoof 2017 database. On the development set, our partitioned LMS feature based on the whole utterance yields a 3.8% EER. After utilizing just the unvoiced portions of the utterances, the EER is further decreased to 3.27% while our baseline using the Constant Q Cepstral Coefficients (CQCC) as a feature is at 10.21%. The evaluation results also confirms the effectiveness of our approach.	baseline (configuration management);cepstrum;coefficient;enhanced entity–relationship model;experiment;frequency band;linear discriminant analysis;loss function;rayleigh–ritz method;replay attack	Zhi Hao Lim;Xiaohai Tian;Wei Rao;Chng Eng Siong	2017	2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)	10.1109/APSIPA.2017.8282273	replay attack;discriminative model;feature extraction;magnitude (mathematics);rayleigh quotient;matrix (mathematics);mel-frequency cepstrum;utterance;mathematics;pattern recognition;artificial intelligence	NLP	-11.714923850546601	-91.3063428866782	70234
661519cf8d9b78b2cb9e755f6861c8b6653e2f76	significance of automatic detection of vowel regions for automatic shout detection in continuous speech	information technology;speech;resonant frequency;feature extraction;production;security;context	Automatic detection of shout prosody in continuous speech signal involves examining changes in its production characteristics. Our recent study of electroglottograph signals highlighted that significant changes occur in the glottal excitation source characteristics during production of shouted speech, especially in the vowel contexts. But the differences between normal and shouted speech, in the production features derived over utterances or word segments, may be masked sometimes by pauses or unvoiced regions related variations. Also, for such a real-time system, these vowel regions need to be found automatically. In this paper, changes in the shout production features are examined in the automatically detected vowel regions. Production of a vowel involves periodic impulse-like excitation and relatively high signal energy. Hence, the knowledge of epochs using zero-frequency filtering, and accurate vowel onset points can be used for detecting these regions. Changes in two excitation source features, the instantaneous fundamental frequency and strength of excitation, and in a vocal tract filter feature the dominant frequency, are examined for five steady vowel regions. Larger changes in these distinguishing features are observed in the automatically found vowel regions, than in word segments. This approach can help improving the systems for automatic detection of shout regions in continuous speech, and in paralinguistic applications that involve detection of prosody or emotions.	electroglottograph;epoch (reference date);onset (audio);real-time computing;real-time locating system;semantic prosody;sensor;tract (literature)	Vinay Kumar Mittal;Anil Kumar Vuppala	2016	2016 10th International Symposium on Chinese Spoken Language Processing (ISCSLP)	10.1109/ISCSLP.2016.7918393	speech recognition;resonance;feature extraction;computer science;speech;linguistics;information technology	NLP	-10.458563948274092	-85.11660115772408	70388
fc6fa888f64c9d150914039b08772f2b62a4cdd8	independent component analysis applied to voice activity detection	conjunto independiente;evaluation performance;observation likelihood;optimisation;570 biowissenschaften biologie;experimental analysis;mobile radiocommunication;informatique mobile;optimum;performance evaluation;protocole transmission;optimizacion;independent set;cellular radio;evaluacion prestacion;database;base dato;false alarm rate;independent component analysis;voice;radiocommunication service mobile;voz;570 biowissenschaften;protocolo transmision;ensemble independant;reconocimiento voz;optimo;likelihood ratio test;distributed speech recognition;base de donnees;analyse composante independante;speech recognition;optimization;reconnaissance parole;voice activity detection;ddc 570;analisis componente independiente;radiotelephonie cellulaire;radiocomunicacion servicio movil;mobile computing;test razon verosimilitud;taux fausse alarme;discriminacion;test rapport vraisemblance;porcentaje falsa alarma;independent component;discrimination;biologie;voix;transmission protocol	In this paper we present the first application of Independent Component Analysis (ICA) to Voice Activity Detection (VAD). The accuracy of a multiple observation-likelihood ratio test (MO-LRT) VAD is improved by transforming the set of observations to a new set of independent components. Clear improvements in speech/non-speech discrimination accuracy for low false alarm rate demonstrate the effectiveness of the proposed VAD. It is shown that the use of this new set leads to a better separation of the speech and noise distributions, thus allowing a more effective discrimination and a tradeoff between complexity and performance. The algorithm is optimum in those scenarios where the loss of speech frames could be unacceptable, causing a system failure. The experimental analysis carried out on the AURORA 3 databases and tasks provides an extensive performance evaluation together with an exhaustive comparison to the standard VADs such as ITU G.729, GSM AMR and ETSI AFE for distributed speech recognition (DSR), and other recently reported VADs.	ansi escape code;adaptive multi-rate audio codec;algorithm;analog front-end;automated system recovery;blind signal separation;database;g.729;independent computing architecture;independent component analysis;jumbo frame;long-running transaction;performance evaluation;preprocessor;source separation;speech recognition;voice activity detection	Juan Manuel Górriz;Javier Ramírez;Carlos García Puntonet;Elmar Wolfgang Lang;Kurt Stadlthanner	2006		10.1007/11758501_35	voice activity detection;independent component analysis;discrimination;speech recognition;independent set;telecommunications;likelihood-ratio test;computer science;constant false alarm rate;mobile computing;voice;experimental analysis of behavior	ML	-13.545543302753524	-95.96008330776476	70398
83331f1aa7f80fc6d57a77f712b5dc35094083d4	multi-band long-term signal variability features for robust voice activity detection		In this paper, we propose robust features for the problem of voice activity detection (VAD). In particular, we extend the long term signal variability (LTSV) feature to accommodate multiple spectral bands. The motivation of the multi-band approach stems from the non-uniform frequency scale of speech phonemes and noise characteristics. Our analysis shows that the multi-band approach offers advantages over the single band LTSV for voice activity detection. In terms of classification accuracy, we show 0.3%-61.2% relative improvement over the best accuracy of the baselines considered for 7 out 8 different noisy channels. Experimental results, and error analysis, are reported on the DARPA RATS corpora of noisy speech.	baseline (configuration management);error analysis (mathematics);heart rate variability;spatial variability;text corpus;voice activity detection	Andreas Tsiartas;Theodora Chaspari;Athanasios Katsamanis;Prasanta Kumar Ghosh;Ming Li;Maarten Van Segbroeck;Alexandros Potamianos;Shrikanth (Shri) Narayanan	2013			voice activity detection;speech recognition;computer science;pattern recognition	NLP	-13.675774736027627	-90.72972486272914	70465
f240f5a49ba47ff5e6443a2774d52d14bb0d03b4	improved i-vector speaker verification based on wccn and zt-norm		For the purpose of improving system performance in high channel variability, an improved i-vector speaker verification algorithm is proposed in this paper. Firstly, i-vectors are obtained from GMM-UBM of registered speakers. And then, the weighted linear discriminant analysis is utilized to play the role of channel compensation and dimensionality reduction in i-vectors. By doing this, more discriminant vectors could be extracted. Immediately following, WCCN and ZT-norm are combined to normalize the scores from cosine distance score classifier for the sake of removing channel disturbance. Finally, cosine distance score classifier of high robustness is generated to find target speaker. Experiment results demonstrate that our proposed i-vector system has better performance.	speaker recognition;t-norm	Yujuan Xing;Ping Tan;Chengwen Zhang	2016		10.1007/978-3-319-46654-5_47	robustness (computer science);cosine distance;dimensionality reduction;normalization (statistics);linear discriminant analysis;classifier (linguistics);discriminant;artificial intelligence;communication channel;pattern recognition;computer science	Logic	-14.31433083313336	-91.2341870615684	70551
80e5a6a3c1924cee3c2714ad77caba8854a20ae3	pitch accent versus lexical stress: quantifying acoustic measures related to the voice source	time course;voice quality;open quotient;indexing terms;american english	In this paper, we explore acoustic correlates of pitch accent and main lexical stress in American English, and the interaction of these cues with other factors that affect prosody. In a controlled study, we varied presence or absence and type of pitch accent (L∗ vs H∗), boundary-related tone sequence (L-L% vs. HH%) and gender of the talker, for the sentence “Dagada gave Bobby doodads”. The measures were duration, F0 (fundamental frequency), H∗ 1−H 2 (related to open quotient), and H∗ 1−A3 (related to spectral tilt). Contour approximations were used to analyze time-course movements of these measures. For “Dagada” we found that, consistent with earlier literature, a) H∗ and L∗ pitch accents showed different F0 contours, b) pitchaccented syllables were longer than unaccented ones, c) stressed “ga” syllables had lower H∗ 1 −H∗ 2 values than surrounding unstressed syllables, and for male talkers, lower H∗ 1 − A3 values, indicating lesser spectral tilt. Unexpectedly, F0 maxima associated with an H∗ accent occurred most of the time later in the accented syllable than F0 minima associated with L∗. The cues to lexical stress were consistent with or without pitch accent (e.g. lower H∗ 1 −H∗ 2 ), but they sometimes interacted with gender and/or boundary tones: for example, lower H∗ 1 − A3 in stressed “ga” syllables was only found for female talkers in unaccented cases, and some cues of both accent and stress were less pronounced in the final word “doodads”, which also carried boundary-related tones.	acoustic cryptanalysis;approximation;bobby (software);contour line;emoticon;maxima and minima;mike lesser;semantic prosody;stress ball;syllable	Yen-Liang Shue;Markus Iseli;Nanette Veilleux;Abeer Alwan	2007			speech recognition;index term;phonation;computer science;linguistics;world wide web	NLP	-10.614303265740725	-81.91152112622095	70629
9ae53d0972f8d469d527f6ef8c300efb200b1ff9	a preliminary acoustic analysis of laryngectomised speech in adult new zealanders		In this paper, we used the voice samples recorded from laryngectomised patients to analyse temporal acoustic characteristics of distorted speech in New Zealand English. Jitter, shimmer, and voice onset time are the features which have been analysed in three post-laryngectomised individuals and the results have been compared with three normal speakers as control group. This acoustic analysis, although limited in size, shows significant differences between normal and distorted speech in all three features; such disparity should be carefully taken into account in improving the quality of recently developed computational speech reconstruction systems for aphonic and dysphonic individuals.	acoustic cryptanalysis;binocular disparity;onset (audio)	M. E. Sabaee;Hamid R. Sharifzadeh;Iman Tabatabaei Ardekani;Jacqueline E. Allen	2018	2018 16th International Workshop on Acoustic Signal Enhancement (IWAENC)	10.1109/IWAENC.2018.8521374	speech recognition;new zealand english;jitter;speech enhancement;voice-onset time;computer science	NLP	-8.946450508205455	-84.4290201298772	70653
0972180fe904f2781e41331224081460c87c883d	predicting utterance pitch targets in yorùbá for tone realisation in speech synthesis	speech synthesis;tone language;yoruba;article;fundamental frequency	Pitch is a fundamental acoustic feature of speech and as such needs to be determined during the process of speech synthesis. While a range of communicative functions are attributed to pitch variation in speech of all languages, it plays a vital role in distinguishing meaning of lexical items in tone languages. As a number of factors are assumed to affect the realisation of pitch, it is important to know which mechanisms are systematically responsible for pitch realisation in order to be able to model these effectively and thus develop robust speech synthesis systems in underresourced environments. To this end, features influencing syllable pitch targets in continuous utterances in Yorùbá are investigated in a small speech corpus of 4 speakers. It is found that the previous syllable pitch level is strongly correlated with pitch changes between syllables and a number of approaches and features are evaluated in this context. The resulting models can be used to predict utterance pitch targets for speech synthesisers (whether it be concatenative or statistical parametric systems), and may also prove useful in speech-recognition systems.	acoustic cryptanalysis;speech corpus;speech recognition;speech synthesis;syllable	Daniel R. van Niekerk;Etienne Barnard	2014	Speech Communication	10.1016/j.specom.2013.01.009	speech recognition;computer science;fundamental frequency;speech synthesis	NLP	-10.694183063295513	-83.19822855885847	70857
ec0d0b5f7d0ad19af5baa7c4f81136f3ae66e47f	a spoken dialog system based on automatic grammar generation and template-based weighting for autonomous mobile robots		We have been developing a spoken dialog system. Conventional spoken dialog systems need grammar descriptions and scripts of a dialog, that are difficult to develop. The system proposed in this paper is based on semantic frames, and the system generates the recognition grammar from the frames automatically. As the system requires only a frame-based description for a task of dialog, the system can be easily applied to different kinds of tasks. Moreover, the recognition accuracy is improved by sentence weighting based on phrase class template. We evaluated the system by experiments. The system reached the goal with 2.44 user’s utterances in average.	autonomous robot;dialog system;experiment;frame language;mobile robot;spoken dialog systems;template (c++)	Takashi Konashi;Motoyuki Suzuki;Akinori Ito;Shozo Makino	2004			speech recognition;natural language processing;spoken dialog systems;scripting language;computer science;mobile robot;phrase;dialog box;sentence;dialog system;artificial intelligence;grammar	NLP	-26.401491314187005	-85.12445028878867	70881
c2c578872f2e012e76d9d2f7df2c2af938f6389f	meaningful head movements driven by emotional synthetic speech		Speech-driven head movement methods are motivated by the strong coupling that exists between head movements and speech, providing an appealing solution to create behaviors that are timely synchronized with speech. This paper offers solutions for two of the problems associated with these methods. First, speech-driven methods require all the potential utterances of the conversational agent (CA) to be recorded, which limits their applications. Using existing text to speech (TTS) systems scales the applications of these methods by providing the flexibility of using text instead of pre-recorded speech. However, simply training speechdriven models with natural speech, and testing them with synthetic speech creates a mismatch affecting the performance of the system. This paper proposes a novel strategy to solve this mismatch. The proposed approach starts by creating a parallel corpus either with neutral or emotional synthetic speech timely aligned with the original speech for which we have the motion capture recordings. This parallel corpus is used to retrain the models from scratch, or adapt the models originally built with natural speech. Both subjective and objective evaluations show the effectiveness of this solution in reducing the mismatch. Second, creating head movement with speech-driven methods can disregard the meaning of the message, even when the movements are perfectly synchronized with speech. The trajectory of head movements in conversations also has a role in conveying meaning (e.g. head nods for acknowledgment). In fact, our analysis reveals that head movements under different discourse functions have distinguishable patterns. Building on the best models driven by synthetic speech, we propose to extract dialog acts directly from the text and use this information to directly constrain our models. Compared to the unconstrained model, the model generates head motion sequences that not only are closer to the statistical patterns of the original head movements, but also are perceived as more natural and appropriate. c © 2011 Published by Elsevier Ltd.	acknowledgment index;dialog system;motion capture;natural language;netware file system;parallel text;speech synthesis;synthetic intelligence	Najmeh Sadoughi;Yang Liu;Carlos Busso	2017	Speech Communication	10.1016/j.specom.2017.07.004	computer science;speech analytics;speech recognition;motor theory of speech perception;natural language processing;dialog box;head movements;trajectory;motion capture;dialog system;speech synthesis;artificial intelligence	NLP	-25.694902851682542	-86.13580527473421	70947
ae9e381319a48883daf0f683f3c5835d21c57c71	a 54-mw 3×-real-time 60-kword continuous speech recognition processor vlsi	low power;vlsi;speech recognition	This paper describes a low-power VLSI chip for speakerindependent 60-kWord continuous speech recognition. We implement parallel and pipelined architecture for GMM computation and Viterbi processing. It includes a 8-path Viterbi transition architecture to maximize the processing speed and adopts tri-gram language model to improve the recognition accuracy. A two-level cache architecture is implemented for the demo system. Measured results show that our implementation achieves 25% required frequency reduction (62.5MHz) and 26% power consumption reduction (54.8mW) for 60 k-Word real-time continuous speech recognition compared to the previous work. This chip can maximally process 3.02× and 2.25× times faster than real-time at 200MHz using the bigram and trigram language models, respectively.	bigram;computation;google map maker;language model;low-power broadcasting;real-time clock;speech recognition;triangular function;trigram;very-large-scale integration	Guangji He;Yuki Miyamoto;Kumpei Matsuda;Shintaro Izumi;Hiroshi Kawaguchi;Masahiko Yoshimoto	2014	IEICE Electronic Express	10.1587/elex.10.20130787	speech recognition;computer science;very-large-scale integration	NLP	-24.064472034063364	-91.28248975463131	71025
af3a725ea36756d39c4958ecafd234bc05637ff8	a connectionist approach to automatic transcription of polyphonic piano music	adaptive oscillators;oscillations;audio signal processing;multiple signal classification music oscillators neural networks instruments pattern recognition machine learning algorithms time frequency analysis adaptive systems signal processing;neural networks;neural nets;indexing terms;audio frequency oscillators neural nets music audio signal processing tracking;time frequency representation;audio frequency oscillators;neural network model;real piano recordings connectionist approach automatic transcription polyphonic piano musical signal neural network models time frequency representation partial tracking technique auditory model adaptive oscillator networks synchronization;music;music transcription;tracking;neural network	In this paper, we present a connectionist approach to automatic transcription of polyphonic piano music. We first compare the performance of several neural network models on the task of recognizing tones from time-frequency representation of a musical signal. We then propose a new partial tracking technique, based on a combination of an auditory model and adaptive oscillator networks. We show how synchronization of adaptive oscillators can be exploited to track partials in a musical signal. We also present an extension of our technique for tracking individual partials to a method for tracking groups of partials by joining adaptive oscillators into networks. We show that oscillator networks improve the accuracy of transcription with neural networks. We also provide a short overview of our entire transcription system and present its performance on transcriptions of several synthesized and real piano recordings. Results show that our approach represents a viable alternative to existing transcription systems.	artificial neural network;connectionism;feedforward neural network;modulation;rough set;time–frequency representation;transcription (software)	Matija Marolt	2004	IEEE Transactions on Multimedia	10.1109/TMM.2004.827507	speech recognition;audio signal processing;computer science;machine learning;music;tracking;artificial neural network	Visualization	-8.160211486193743	-92.01477480727696	71222
c2a3cd4fbd6a7369d1c8cf7793e9153eadae7742	a performance investigation of noisy voice recognition over ip telephony networks.	voice recognition;ip telephony	A performance analysis of noisy automatic speech recognition (ASR) based on Internet protocol (IP) telephony networks is presented in this paper. The present public telephone voice communication networks, which utilize digital technology via circuit switching, provide satisfactory quality service. In contrast, the Internet telephony switching network, which is a packetswitched network, does not provide a guarantee of quality service at all, since such networks have packet losses. Moreover, the performance of ASR systems deteriorates when such ASR systems are used in adverse environments. For measuring the influence of missing speech packets on the ASR system performance, we use a Soekris net 4501 IP simulator in order to control packet loss rate. Also, to examine how additive acoustic noise influences the speech recognition performance, eleven different types of noise sources are chosen for use in our experiments. In these experiments, the results show that the level of packet loss has a detrimental impact on the speech recognition performance. Furthermore, the speech recognition rate also degrades as the level of additive noise increases under the different kinds of noise conditions.	acoustic cryptanalysis;additive white gaussian noise;automated system recovery;circuit switching;digital electronics;experiment;network packet;quality of service;speech recognition;telecommunications network;utility functions on indivisible goods	Gang Chen;Douglas D. O'Shaughnessy;Hesham Tolba	2005			speech recognition;computer science;voice over ip;telephony	Metrics	-12.708114391005001	-95.66883834549748	71351
bc16e3db41db74244ea3584195984abbcae4f595	cepstral noise subtraction for robust automatic speech recognition	low signal to noise ratios cepstral noise subtraction robust automatic speech recognition statistical moments mel frequency cepstral coefficients mfcc cepstral mean normalization cmn cepstral mean and variance normalization cmvn complete utterance background noise single channel speech enhancement feature normalization methods noise power spectral density;speech;speech enhancement automatic speech recognition cepstral analysis feature normalization noise robustness;noise measurement;noise measurement speech speech recognition mel frequency cepstral coefficient signal to noise ratio accuracy;mel frequency cepstral coefficient;accuracy;speech recognition cepstral analysis signal denoising speech enhancement;speech recognition;signal to noise ratio	The robustness of speech recognizers towards noise can be increased by normalizing the statistical moments of the Mel-frequency cepstral coefficients (MFCCs), e. g. by using cepstral mean normalization (CMN) or cepstral mean and variance normalization (CMVN). The necessary statistics are estimated over a long time window and often, a complete utterance is chosen. Consequently, changes in the background noise can only be tracked to a limited extent which poses a restriction to the performance gain that can be achieved by these techniques. In contrast, algorithms recently developed for single-channel speech enhancement allow to track the background noise quickly. In this paper, we aim at combining speech enhancement techniques and feature normalization methods. For this, we propose to transform an estimate of the noise power spectral density to the MFCC domain, where we subtract it from the noisy MFCCs. This is followed by a conventional CMVN. For background noises that are too instationary for CMVN but can be tracked by the noise estimator, we show that this processing leads to an improvement in comparison to the sole application of CMVN. The observed performance gain emerges especially in low signal-to-noise-ratios.	algorithm;cepstral mean and variance normalization;coefficient;database normalization;finite-state machine;mel-frequency cepstrum;noise power;signal-to-noise ratio;spectral density;speech enhancement;speech recognition;speech synthesis	Robert Rehr;Timo Gerkmann	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7177994	speech recognition;computer science;noise measurement;speech;pattern recognition;accuracy and precision;signal-to-noise ratio	Vision	-13.00435495009925	-91.46335560008488	71355
ac78330fb26b04574f1d522cc0e3075dc5d71854	inter-language vowel perception and production by korean and japanese listeners	natural languages;speaker recognition;speech recognition;australian english monophthongal front vowels;japanese listeners;korean listeners;seoul dialect;inter-language vowel perception;phonological learning	This paper investigates the influence of phonological learning upon the perception of non-native vowels. Four groups of Korean and Japanese English learners, at two levels of English experience, and a group of lder monolingual Korean listeners were assessed on the perception and production of Australian English monophthongal front vowels: /i: w e æ a:/. Korean is of interest, because of a recent phonological merger of two front vowels (/e/ and / e ), which has produced a generation split among speakers of Seoul dialect above and below 45-50 years of age (Hong, 1991). The present study is the first reported case of how a phonemic merger, resulting in cross-generation differences within a speech community, can influence speakers' perception and production of non-native vowels. The effects of phonological learning on vowel perception were also observed in The first experiment tested non-native listeners recognition of the the tendency of the Japanese, but not the Korean listeners, to 5 Australian English front-mid vowels. A forced choice normalise tokens of non-native vowels for speaker-dependent durational variation, consistent with the respective phonological roles of vowel length in Japanese and Korean.		John C. L. Ingram;See-Gyoon Park	1996			psychology;speech recognition;linguistics;communication	NLP	-11.514575285743144	-81.7263284328063	71412
f4d915f0c3a2dc6a8ee1d74fe84db593a1eae46e	improving visual noise insensitivity in small vocabulary audio visual speech recognition applications	vocabulary speech recognition noise measurement degradation dispersion testing gain measurement laboratories systems engineering and theory australia;audio visual systems;degradation;audio signal processing;high dimensionality;video signal processing;confidence measure;vocabulary;audio modality;video modality;testing;audio visual speech recognition;noise measurement;systems engineering and theory;small vocabulary;high dimensional secondary classifier;random noise;catastrophic fusion boundary visual noise insensitivity improvement small vocabulary audio visual speech recognition high dimensional secondary classifier word likelihood scores audio modality video modality adaptive fusion;adaptive signal processing;adaptive fusion;visual noise insensitivity improvement;word likelihood scores;audio visual systems random noise speech recognition adaptive signal processing audio signal processing video signal processing;speech recognition;catastrophic fusion boundary;dispersion;gain measurement;australia	Visual noise insensitivity is important to audio visual speech recognition (AVSR). Visual noise can take on a number of forms such as varying frame rate, occlusion, lighting or speaker variabilities. In this paper the use of a high dimensional secondary classifier on the word likelihood scores from both the audio and video modalities is investigated for the purposes of adaptive fusion. Preliminary results are presented demonstrating performance above the catastrophic fusion boundary for our confidence measure irrespective of the type of visual noise presented to it. Our experiments were restricted to small vocabulary applications.	audio-visual speech recognition;experiment;image noise;oracle fusion middleware;vocabulary	Simon Lucey;Sridha Sridharan;Vinod Chandran	2001		10.1109/ISSPA.2001.950173	adaptive filter;computer vision;dispersion;audio mining;speech recognition;degradation;audio signal processing;computer science;noise measurement;software testing	Vision	-13.405364033984826	-91.65168185491683	71469
4ea0b5db79aaf414c849f73dd8b2b52ff9f59001	cross-lingual vocal emotion recognition in five native languages of assam using eigenvalue decomposition	sampling frequency;emotion recognition;eigenvalue decomposition;gaussian mixture model;eigenvalues of autocorrelation matrix;cross lingual vocal emotion recognition;full blown basic emotion;signal to noise ratio;white noise;human machine interaction;native language;emotional expression	This work investigates whether vocal emotion expressions of full-blown discrete emotions can be recognized cross-lingually. This study will enable us to get more information regarding nature and function of emotion. Furthermore, this work will help in developing a generalized vocal emotion recognition system, which will increase the efficiency required for human-machine interaction systems. An emotional speech database was created with 140 simulated utterances (20 per emotion) per speaker, consisting of short sentences of six full-blown discrete basic emotions and one 'no-emotion' (i.e. neutral) in five native languages (not dialects) of Assam. A new feature set is proposed based on Eigenvalues of Autocorrelation Matrix (EVAM) of each frame of utterance. The Gaussian Mixture Model is used as classifier. The performance of EVAM feature set is compared at two sampling frequencies (44.1 kHz and 8.1 kHz) and with additive white noise with signal-to-noise ratios of 0 db, 5 db, 10 db and 20 db.	emotion recognition	Aditya Bihar Kandali;Aurobinda Routray;Tapan Kumar Basu	2009		10.1007/978-3-642-11164-8_84	natural language processing;eigendecomposition of a matrix;speech recognition;emotional expression;first language;mixture model;white noise;signal-to-noise ratio;sampling;statistics	NLP	-13.262992036744281	-87.72722817374604	71474
3154397bcd7056d86119f5b4ff5899e179e4f507	improving the ensemble speaker and speaking environment modeling approach by enhancing the precision of the online estimation process.		In this paper, we study methods to enhance the precision of the online estimation process of a recently proposed approach, ensemble speaker and speaking environment modeling (ESSEM), and therefore improve its overall performance. The ESSEM approach consists of two integral phases, offline and online. In the offline phase, an ensemble environment configuration is prepared by a large collection of acoustic models. Each set of acoustic models represents a particular environment. In the online phase, with speech data from the testing condition, we estimate a mapping function and use it to generate a new set of acoustic models for that particular testing condition. In our previous study, we have discussed the issues of the offline process and proposed algorithms to refine the environment configuration. In this paper, we first study different online mapping structures and compare their performances on a same environment configuration. Next, we propose a multiple clustering matching algorithm to further improve the overall performance of ESSEM. We tested ESSEM and its extensions on the full evaluation set of the Aurora2 connected digit recognition task. When using our best offline environment configuration along with a properly specified online estimation method, the ESSEM approach can achieve an average word error rate (WER) of 4.77%, corresponding to a WER reduction of 13.43% (from 5.51% WER to 4.77% WER) over the baseline result.	acoustic cryptanalysis;acoustic model;algorithm;baseline (configuration management);cluster analysis;online and offline;pc speaker;performance;web mapping;word error rate	Yu Tsao;Chin-Hui Lee	2008			machine learning;pattern recognition;statistics	SE	-17.584370467747934	-90.42730838030653	71492
23707035d5034711edf7d54f45f971ccbc00cc21	unsupervised speaker adaptation for telephone call transcription	unsupervised learning;acoustic devices;word error rate;unsupervised speaker adaptation;acoustic model adaptation;acoustic modeling;gain;speech;acoustic signal processing;natural languages;indexing terms;speech recognition speaker adaptation acoustic model adaptation language model adaptation unsupervised adaptation;internet telephony;speaker dependent speech recognition;telephone call transcription;speaker recognition;error analysis;automatic speech recognition;accuracy;adaptation model;hidden markov models;internet;unsupervised adaptation;unsupervised learning acoustic signal processing internet telephony natural languages speaker recognition;loudspeakers;language model adaptation;automatic speech transcription;loudspeakers speech recognition adaptation model natural languages internet telephony acoustic devices automatic speech recognition laboratories australia error analysis;speaker dependent;speech recognition;unsupervised training;unsupervised training unsupervised speaker adaptation telephone call transcription internet speaker dependent speech recognition acoustic model adaptation language model adaptation automatic speech transcription;speaker adaptation;australia;data models	The use of the PC and Internet for placing telephone calls will present new opportunities to capture vast amounts of un-transcribed speech for a particular speaker. This paper investigates how to best exploit this data for speaker-dependent speech recognition. Supervised and unsupervised experiments in acoustic model and language model adaptation are presented. Using one hour of automatically transcribed speech per speaker with a word error rate of 36.0%, unsupervised adaptation resulted in an absolute gain of 6.3%, equivalent to 70% of the gain from the supervised case, with additional adaptation data likely to yield further improvements. LM adaptation experiments suggested that although there seems to be a small degree of speaker idiolect, adaptation to the speaker alone, without considering the topic of the conversation, is in itself unlikely to improve transcription accuracy.	acoustic cryptanalysis;acoustic model;experiment;internet;language model;pc speaker;personal computer;speech recognition;transcription (software);word error rate	R. Wallace;Kishan Thambiratnam;Frank Seide	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4960603	loudspeaker;natural language processing;unsupervised learning;speaker recognition;data modeling;speaker diarisation;the internet;speech recognition;index term;gain;word error rate;computer science;speech;pattern recognition;accuracy and precision;natural language	NLP	-20.489416785888285	-86.97301850270976	71605
bbba4160b854ee246b6603d3437d9d92f050106c	compensation of utterance length for speaker verification		The effect of utterancelength on the estimationof the likelihoodof a speaker haspreviously seena brief treatmentin past works. In many speaker recognitionevaluations,theutterances weretypically configuredto have a relatively consistentlength. Thispaperinvestigatestheeffectof varyingenrollmentandtest utterancelengthson the scoredistributions and consequently their effect on performance.In addition, this study examines themixing of anumberof variabletraininglengthutterancetrialsin asingleevaluation.To addressthisproblem,aparametric solutionusingsplinesis proposedandis shown to significantly reducetheerrorin theconductedexperiments.	speaker recognition	Jason W. Pelecanos;Upendra V. Chaudhari;Ganesh N. Ramaswamy	2004			mathematics;speaker diarisation;parametric statistics;speaker recognition;artificial intelligence;utterance;pattern recognition	NLP	-13.24382186192624	-89.12135413557748	71642
c9581cbbaf2ca86a1ec465a62f2f21125380b578	robust multipitch analyzer against initialization based on latent harmonic allocation using overtone corpus	musical instrument sounds;特集 音楽情報処理の新展開 音楽情報科学研究会20周年記念特集 multipitch estimation;harmonic clustering;multipitch estimation;overtone estimation	We present a Bayesian analysis method that estimates the harmonic structure of musical instruments in music signals on the basis of psychoacoustic evidence. Since the main objective of multipitch analysis is joint estimation of the fundamental frequencies and their harmonic structures, the performance of harmonic structure estimation significantly affects fundamental frequency estimation accuracy. Many methods have been proposed for estimating the harmonic structure accurately, but no method has been proposed that satisfies all these requirements: robust against initialization, optimization-free, and psychoacoustically appropriate and thus easy to develop further. Our method satisfies these requirements by explicitly incorporating Terhardt’s virtual pitch theory within a Bayesian framework. It does this by automatically learning the valid weight range of the harmonic components using a MIDI synthesizer. The bounds are termed “overtone corpus.” Modeling demonstrated that the proposed overtone corpus method can stably estimate the harmonic structure of 40 musical pieces for a wide variety of initial settings.	bayesian network;midi;mathematical optimization;psychoacoustics;requirement;spectral density estimation;text corpus	Daichi Sakaue;Katsutoshi Itoyama;Tetsuya Ogata;Hiroshi G. Okuno	2013	JIP	10.2197/ipsjjip.21.246	speech recognition	ML	-11.488081984468199	-93.17368850902452	71643
a4e7935fa895ead6c54dacfac8bb531c28a5a54b	automatic classification of question turns in spontaneous speech using lexical and prosodic evidence	health research;uk clinical guidelines;lexical evidence;biological patents;spontaneous speech question turn speech act dialog prosody;word error rate;speech act;rule based pattern learning;europe pubmed central;speech acts;question turn;speech processing;rule based;automatic speech recognition speech analysis humans robustness error analysis natural languages system testing data mining databases acoustic testing;citation search;spoken language systems;dialog;nonlinguistic vocalizations;spontaneous multiparty speech;uk phd theses thesis;word error rate spoken language system question bearing turns nonlinguistic vocalizations rule based pattern learning spontaneous multiparty speech prosodic evidence lexical evidence;spoken language system;speech recognition knowledge based systems learning artificial intelligence linguistics pattern classification speech processing;life sciences;pattern classification;speech recognition;spontaneous speech;learning artificial intelligence;classification accuracy;prosody;automatic classification;uk research reports;medical journals;prosodic evidence;knowledge based systems;europe pmc;biomedical research;question bearing turns;bioinformatics;linguistics	The ability to identify speech acts reliably is desirable in any spoken language system that interacts with humans. Minimally, such a system should be capable of distinguishing between question-bearing turns and other types of utterances. However, this is a non-trivial task, since spontaneous speech tends to have incomplete syntactic, and even ungrammatical, structure and is characterized by disfluencies, repairs and other non-linguistic vocalizations that make simple rule based pattern learning difficult. In this paper, we present a system for identifying question-bearing turns in spontaneous multi-party speech (ICSI Meeting Corpus) using lexical and prosodic evidence. On a balanced test set, our system achieves an accuracy of 71.9% for the binary question vs. non-question classification task. Further, we investigate the robustness of our proposed technique to uncertainty in the lexical feature stream (e.g. caused by speech recognition errors). Our experiments indicate that classification accuracy of the proposed method is robust to errors in the text stream, dropping only about 0.8% for every 10% increase in word error rate (WER).	experiment;linguistics;numerous;partial;speech recognition;sperm injections, intracytoplasmic;spontaneous order;test set;word error rate;wound healing	Sankaranarayanan Ananthakrishnan;Prasanta Kumar Ghosh;Shrikanth (Shri) Narayanan	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518782	natural language processing;speech recognition;word error rate;computer science;speech processing;prosody	NLP	-18.639317837436796	-82.05206675423281	71832
6ac8acb8a588cdb63a3970a3fe22dc3199e3a950	stimulus duration and type in perception of female and male speaker age	jamforande sprakvetenskap och lingvistik;perception studies;stimulus duration;difference set;spontaneous speech;judgements	In a small perception study, our ability to estimate speaker age from speech samples was investigated with respect to stimulus duration, stimulus type and speaker gender. Four separate listening tests were carried out with four different sets of stimuli: 10 and 3 seconds of spontaneous speech, one isolated word, and 6 concatenated isolated words, all produced by the same 24 speakers. The results showed that the listeners’ judgements were about twice as accurate compared to a baseline estimator, and that both stimulus duration and type affected the judgements. It was also found that stimulus duration influenced the listeners judgements of female speakers somewhat more, while stimulus type affected the age judgments of male speakers more, indicating that listeners may use different strategies when judging female and male speaker age.	baseline (configuration management);concatenation;spontaneous order	Susanne Schötz	2005			speech recognition;difference set;statistics	NLP	-10.504432583264768	-82.23410111561552	71927
f04321aac4f111ce5f7c1bfc4b35816b4b27d4bc	a water hammer analysis of pressure and flow in the voice production system	oscillations;speech synthesis;43 28 ra;production system;43 70 h;physical sciences;voice production;water hammer;43 70 bk	The sudden pressure rise produced by glottal closure in the subglottal tract during vocal fold oscillation causes a flow transient which can be computed as a water hammer effect in engineering. In this article, we present a basic water hammer analysis for the trachea and the supralaryngeal tract under conditions which are analogue to those operating during voice production. This approach allows predicting both, the intra-oral and intra-tracheal pressure fluctuations induced by vocal fold motion, as well as the airflow evolution throughout the phonatory system. 2008 Elsevier B.V. All rights reserved. PACS: 43.70. h; 43.70.Bk; 43.28.Ra	acoustic cryptanalysis;approximation;complexity;covox speech thing;picture archiving and communication system;pipeline (computing);production system (computer science);row hammer;tract (literature)	D. Sciamarella;Guillermo Artana	2009	Speech Communication	10.1016/j.specom.2008.10.004	speech recognition;place of articulation;computer science;physical science;linguistics;production system;oscillation;speech synthesis	AI	-8.254879338858245	-84.40412049742008	72053
7af33c424278eb8a0414a30dede91a41f6b360ed	tessa, a system to aid communication with deaf people	aids for the deaf;translation systems;conference paper;interactive system;avatars;speech recognition;interactive systems	TESSA is an experimental system that aims to aid transactions between a deaf person and a clerk in a Post Office by translating the clerk's speech to sign language. A speech recogniser recognises speech from the clerk and the system then synthesizes the appropriate sequence of signs in British Sign language (BSL) using a specially-developed avatar. By using a phrase lookup approach to language translation, which is appropriate for the highly constrained discourse in a Post Office, we were able to build a working system that we could evaluate. We summarise the results of this evaluation (undertaken by deaf users and Post office clerks), and discuss how the findings from the evaluation are being used in the development of an improved system.	boost;experimental system;lookup table;speech synthesis	Stephen Cox;Michael Lincoln;Judy Tryggvason;Melanie Nakisa;Mark Wells;Marcus Tutt;Sanja Abbott	2002		10.1145/638249.638287	cued speech;speech recognition;human–computer interaction;language interpretation;computer science;multimedia;world wide web	NLP	-25.91837078315423	-85.83113785508473	72078
0957189f98fef283b05a672177465867600dc5a6	spoken term detection using visual spectrogram matching	spoken term detection;spectrograms;document handling;query processing;spectrogram;template matching spoken document retrieval spoken term detection spectrograms;speech;spoken document retrieval;query processing document handling;computer vision;distance measurement;visualization;hidden markov models;newscast recording spoken term detection visual spectrogram matching spoken document visual images computer vision;deformable template;template matching;time frequency analysis;spectrogram image retrieval vocabulary music information retrieval automatic speech recognition frequency computer vision audio recording indexing nist	This work proposes a novel spoken term detection technique, where the query is in audio format. Detection and retrieval are performed by matching the spectrograms of the spoken document and query as visual images, using ideas from computer vision. Local descriptors are computed on a dense grid over each spectrogram, and the query term is detected using deformable template matching of grids. Detection experiments are performed on an hour-long newscast recording, involving 10 query terms of length 2-3 words. When the query term comes from the document, nearly all other instances of the term in the document are detected; performance degrades when the query is recorded by the user.	computer vision;experiment;spectrogram;template matching	Nevena Lazic;Parham Aarabi	2008	2008 Tenth IEEE International Symposium on Multimedia	10.1109/ISM.2008.28	computer vision;query expansion;speech recognition;computer science;spectrogram;pattern recognition;information retrieval;hidden markov model	Vision	-7.812883117347686	-93.92079586277518	72100
ffbbb9ebc03de12eae9ecd869a807b686d49d573	sufficient statistics for re-optimizing repetitive queries			optimizing compiler	Feng Yu;Wen-Chi Hou;Michael Wainer;Cheng Luo	2013	I. J. Comput. Appl.		sufficient statistic;pattern recognition;artificial intelligence;computer science	Theory	-7.118162350419133	-95.61126431173008	72115
be59601e0dac2aaaf1b03adfa5c309e2f6309dad	temporal factors in the perception of consonants for different age and hearing impairment groups				Elzbieta B. Slawinski	1992			cognitive psychology;artificial intelligence;pattern recognition;perception;computer science	HCI	-8.16442686627837	-83.21234710959683	72261
bf7911b6f50af748ce5a649075a2d9a3a195ce65	learning parametric dictionaries for graph signals			dictionary;parametric polymorphism	Dorina Thanou;David I. Shuman;Pascal Frossard	2014	CoRR			NLP	-13.813301722296075	-86.54236632657083	72514
211427ef22ce4099d8108c5be15e2be73582ab00	feature selection for log-linear acoustic models	log linear acoustic model;optimisation;wall street journal;complexity theory;log linear models feature selection l 1 regularization relieff acoustic modeling;gaussian processes;sparse feature;sparse feature feature selection log linear acoustic model gaussian mixture model speech recognition simple univariate feature selection algorithm multivariate algorithm relieff algorithm rprop algorithm l 1 regularized function wall street journal corpus;hidden markov model;acoustics;acoustic modeling;training;l 1 regularization;speech recognition gaussian processes optimisation;polynomial optimization;polynomials;relieff algorithm;training hidden markov models acoustics speech recognition polynomials optimization complexity theory;gaussian mixture model;log linear models;hidden markov models;relieff;wall street journal corpus;simple univariate feature selection algorithm;rprop algorithm;speech recognition;feature selection;optimization;l 1 regularized function;multivariate algorithm;log linear model	Log-linear acoustic models have been shown to be competitive with Gaussian mixture models in speech recognition. Their high training time can be reduced by feature selection. We compare a simple univariate feature selection algorithm with ReliefF - an efficient multivariate algorithm. An alternative to feature selection is ℓ1-regularized training, which leads to sparse models. We observe that this gives no speedup when sparse features are used, hence feature selection methods are preferable. For dense features, ℓ1-regularization can reduce training and recognition time. We generalize the well known Rprop algorithm for the optimization of ℓ1-regularized functions. Experiments on the Wall Street Journal corpus showed that a large number of sparse features could be discarded without loss of performance. A strong regularization led to slight performance degradations, but can be useful on large tasks, where training the full model is not tractable.	acoustic cryptanalysis;acoustic model;cobham's thesis;feature selection;genetic algorithm;log-linear model;mathematical optimization;mixture model;rprop;selection algorithm;sparse matrix;speech recognition;speedup;the wall street journal;whole earth 'lectronic link	Simon Wiesler;Alexander Richard;Yotaro Kubo;Ralf Schlüter;Hermann Ney	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947560	speech recognition;computer science;machine learning;pattern recognition;mathematics;hidden markov model;statistics;log-linear model	Vision	-17.45049072136253	-92.20306111087525	72541
5a0064e72d1aa9fbe1175f321f2293485bfc23ba	pc-based 台灣手語轉語音溝通輔助系統 (pc-based taiwanese sign language to speech communication aided system)[in chinese]				Yu-Hsien Chiu;Chung-Hsien Wu;Chi-Shiang Guo;Kao-Chi Chung	2000			sign language;speech recognition;computer science	NLP	-15.364718992801329	-85.45411888609956	72554
d41d408457c38c59ccba816adffacf576cf9026f	modification of the glottal voice characteristics based on changing the maximum-phase speech component	maximum-phase component;vocal tract;minimum-phase component;synthetic emotional speech;maximum-phase speech signal component;emotional speech;vocal cords behavior;glottal flow characteristic;maximum-phase speech component;vocal cord;glottal voice characteristic;glottal flow signal	Voice characteristics are influenced especially by the vocal cords and by the vocal tract. Characteristics known as voice type (normal, breathy, tense, falsetto etc.) are attributed to vocal cords. Emotion influences among others the tonus of muscles and thus influences also the vocal cords behavior. Previous research confirms a large dependence of emotional speech on the glottal flow characteristics. There are several possible ways for obtaining the glottal flow signal from speech. One of them is the decomposition of speech using the complex cepstrum into the maximum- and minimum-phase components. In this approach the maximum-phase component is considered as the open phase of the glottal flow signal. In this contribution we present experiments with the modification of the maximum-phase speech signal component with the aim to obtain synthetic emotional speech.		Martin Vondra;Robert Vích	2010		10.1007/978-3-642-25775-9_24	speech recognition;human voice;phonation;acoustics;voice analysis;communication	Robotics	-9.588374563385965	-84.8131094749913	72612
aaaecf976d5af7306c21bfac8c9326de327b6819	development of an annotated multimodal dataset for the investigation of classification and summarisation of presentations using high-level paralinguistic features		Expanding online archives of presentation recordings provide potentially valuable resources for learning and research. However, the huge volume of data that is becoming available means that users have difficulty locating material which will be of most value to them. Conventional summarisation methods making use of text-based features derived from transcripts of spoken material can provide mechanisms to rapidly locate topically interesting material by reducing the amount of material that must be auditioned. However, these text-based methods take no account of the multimodal high-level paralinguistic features which form part of an audio-visual presentation, and can provide valuable indicators of the most interesting material within a presentation. We describe the development of a multimodal video dataset, recorded at an international conference, designed to support the exploration of automatic extraction of paralinguistic features and summarisation based on these features. The dataset is comprised of parallel recordings of the presenter and the audience for 31 conference presentations. We describe the process of performing manual annotation of high-level paralinguistic features for speaker ratings, audience engagement, speaker emphasis, and audience comprehension of these recordings. Used in combination these annotations enable research into the automatic classification of high-level paralinguistic features and their use in video summarisation.	archive;high- and low-level;multimodal interaction;text-based (computing)	Keith Curtis;Nick Campbell;Gareth J. F. Jones	2018			artificial intelligence;natural language processing;speech recognition;paralanguage;computer science	Visualization	-17.44492893568344	-80.51618950551368	72662
cf1c2597e48840d49c2303c9119110eaccf6030c	acoustic model building based on non-uniform segments and bidirectional recurrent neural networks	duration model;feature extraction acoustic model building nonuniform segments bidirectional recurrent neural networks speech recognition systems segment to phoneme probabilities duration model long term dependencies parameter efficient context dependent models phoneme classifiers frame classifiers discriminative training phoneme language model classification tests timit phoneme database test data set speech recognition;acoustic modeling;speech processing;acoustic signal processing;maximum likelihood estimation;recurrent neural networks neural networks speech recognition acoustic testing probability merging databases pattern recognition statistical analysis error analysis;learning artificial intelligence acoustic signal processing speech processing speech recognition recurrent neural nets maximum likelihood estimation pattern classification feature extraction;feature extraction;pattern classification;speech recognition;context dependent;discriminative training;recurrent neural nets;recurrent neural network;learning artificial intelligence;language model;neural network	"""In this paper a new framework for acoustic model building is presented. It is based on non-uniform segment models, which are learned and scored with a time bidirectional recurrent neural network. While usually neural networks in speech recognition systems are used to estimate posterior """"frame to phoneme"""" probabilities, they are used here to estimate directly """"segment to phoneme"""" probabilities, which results in an improved duration model. The special MAP approach allows not only incorporation of long term dependencies on the acoustic side, but also on the phone (output) side, which results automatically in parameter e cient context dependent models. While the use of neural networks as frame or phoneme classi ers always results in discriminative training for the acoustic information, the MAP approach presented here also incorporates discriminative training for the internally learned phoneme language model. Classi cation tests for the TIMIT phoneme database gave promising results of 77.75 (82.38)% for the full test data set with all 61 (39) symbols."""	acoustic cryptanalysis;acoustic model;artificial neural network;bidirectional recurrent neural networks;discriminative model;language model;recurrent neural network;speech recognition;timit;test data	Mike Schuster	1997		10.1109/ICASSP.1997.595486	speech recognition;feature extraction;computer science;recurrent neural network;machine learning;context-dependent memory;pattern recognition;speech processing;time delay neural network;maximum likelihood;language model	ML	-18.67413767986032	-90.46642963343176	72821
f45eebb6fab9a6896bbdd4b9414d9f930c220520	fld-based detection of re-compressed speech signals	non uniform quantization;re quantization detection;generation time;automatic detection;detection algorithm;fisher linear discriminant;time domain;speech forensics	Prior works on re-quantization detection were mainly focused on still images and videos, implying that the involved quantization is uniform. In this paper, we examine non-uniform re-quantization, and then investigate the automatic detection of re-compressed speech signals. Based on Fisher Linear Discriminant (FLD), two detection algorithms are described in the time-domain and in the DFT-domain respectively. Comparative experiments indicate that both detection algorithms produce reliable results with AUC values higher than 0.9744 for a set of different experimental setups. In general, time-domain detection performs slightly better than DFT-domain detection. However, the latter is superior in the less dimensionality of input vectors.	algorithm;experiment;linear discriminant analysis;speech coding;time-compressed speech;video	Xiaoying Feng;Gwenaël J. Doërr	2010		10.1145/1854229.1854239	speech recognition;computer science;machine learning;pattern recognition	ML	-11.18050328717974	-91.47635446137983	72831
a43bfcbf9377f8b178b55df6cb3408dee423d4f5	anything to clarify? report your parsing ambiguities!	004 informatik;ddc 004;dialogue manager	An important factor for the acceptance of spoken dialogue systems is their ability to react flexibly on misunderstandings between user and system. This paper addresses the issue of grounding utterances in task-oriented human-computer dialogues. It focuses on the aspect of handling ambiguities while parsing word lattices from a speech recognizer: The parser detects the origin and the type of ambiguities which are reported to the dialogue manager as comments to the list of readings for the user’s utterance. This way, disambiguation is delegated to the dialogue manager and accomplished either by exploiting the application situation or by initiating clarification dialogues that are suitable for the dialogue situation.	dialog system;exploit (computer security);finite-state machine;parsing;speech recognition;speech synthesis;word-sense disambiguation	Kerstin Bücher;Michael Knorr;Bernd Ludwig	2002			natural language processing;speech recognition;computer science;artificial intelligence	NLP	-26.31310179604796	-85.3196114041373	72833
0b9e415f0e131ca6bc1cf40a975a4944b6b56fce	automatic phonetic base form generation based on maximum context tree		To improve the performance and the usability of the speech recognition devices, it is necessary for most applications to allow users to enter new words or personalize words in the system vocabulary. The voicetagging technique is a simple example of using speaker dependent spoken samples to generate baseform transcriptions of the spoken words. More sophisticated techniques can use both spoken samples and text versions of the new words to generate baseform transcriptions. In this paper, we propose a maximum context tree (MCT) based approach to the problem. Comparison is made to the common decision tree based method and Pronunciation by Analogy (PbA) approach. The new approach gives exact baseform transcription for in-vocabulary words and it shows better performance than decision tree. It performs significantly better than PbA approach with less memory usages. MCT uses the word segment probability rather than frequency count used in PbA. MCT uses the full context for the focus letter to overcome the some deficiencies in the PbA approach.		Changxue Ma	2004			speech recognition;usability;natural language processing;decision tree;transcription (linguistics);pattern recognition;computer science;analogy;vocabulary;pronunciation;artificial intelligence	AI	-20.72685097387702	-83.57673998543198	72836
41151077a08fbe208eaad12091ac6f4a79927c2a	natural language dialogue service for appointment scheduling agents	natural language dialogue service;message extraction technique;nl coverage;dialogue behaviour;corpus-based grammar development;cooperating agent system;appointment scheduling;natural language;machine agent;appointment scheduling agent system;german language server	Appointment scheduling is a problem faced daily by many individuals and organizations Cooperating agent systems have been developed to par tially automate this task In order to extend the circle of participants as far as possible we advocate the use of natural language transmitted by e mail We describeCosma a fully implemented German language server for exist ing appointment scheduling agent systems Cosma can cope with multiple dialogues in parallel and accounts for di erences in dialogue behaviour between human and machine agents NL coverage of the sublanguage is achieved through both corpus based grammar development and the use of message extraction techniques	email;nl (complexity);natural language;open-shop scheduling;schedule (project management);scheduling (computing);server (computing);sublanguage;the circle (file system)	Stephan Busemann;Thierry Declerck;Abdel Kader Diagne;Luca Dini;Judith Klein;Sven Schmeier	1997			natural language processing;computer science;artificial intelligence;linguistics;natural language	AI	-28.538987484039527	-82.57306549871375	72874
a137602fa1513b74252316240a759926f525909a	speaker-independent machine lip-reading with speaker-dependent viseme classifiers		In machine lip-reading, which is identification of speech from visual-only information, there is evidence to show that visual speech is highly dependent upon the speaker [1]. Here, we use a phoneme-clustering method to form new phoneme-to-viseme maps for both individual and multiple speakers. We use these maps to examine how similarly speakers talk visually. We conclude that broadly speaking, speakers have the same repertoire of mouth gestures, where they differ is in the use of the gestures.	cluster analysis;linear classifier;map;talk box	Helen L. Bear;Stephen J. Cox;Richard Harvey	2015			natural language processing;speech recognition;computer science;communication	NLP	-12.671275313088	-83.4190538309897	73093
e2f3ad9822cda5d1112e07038d6e1112de07ff75	semi-automatic phonemic labelling of speech data using a self-organising neural network	neural network	In the perspective of assessing existing and new speech input and output devices, the development of methods for computerised semi-automatic labelling of speech data is becoming increasingly important. This paper describes preliminary work to achieving this goal by utilising Neural Network technique to perform phonemic classification. The speech data used for Neural Network learning are taken from the SAM-EUROM.O database, which has been manually labelled by expert phoneticians as a reference. Separate data from 1 male speaker are used to test the performance of the semiautomatic phonemic labelling system. The preliminary results show an average classification rate of 65 % an the vocalic and 72% an the consonantal Danish archiphonemes.	artificial neural network;input/output;output device;self-organization;semiconductor industry	Paul Dalsgaard	1989			speech recognition;artificial intelligence;pattern recognition;artificial neural network;labelling;time delay neural network;computer science	ML	-17.277963738551136	-86.01708934624952	73103
b0fdcd7287ce65055b2502a2d7334dfcfe14e512	domain and speaker adaptation for cortana speech recognition		Voice assistant represents one of the most popular and important scenarios for speech recognition. In this paper, we propose two adaptation approaches to customize a multi-style well-trained acoustic model towards its subsidiary domain of Cortana assistant. First, we present anchor-based speaker adaptation by extracting the speaker information, i-vector or d-vector embeddings, from the anchor segments of ‘Hey Cortana’. The anchor embeddings are mapped to layer-wise parameters to control the transformations of both weight matrices and biases of multiple layers. Second, we directly update the existing model parameters for domain adaptation. We demonstrate that prior distribution should be updated along with the network adaptation to compensate the label bias from the development data. Updating the priors may have a significant impact when the target domain features high occurrence of anchor words. Experiments on Hey Cortana desktop test set show that both approaches improve the recognition accuracy significantly. The anchor-based adaptation using the anchor d-vector and the prior interpolation achieves 32% relative reduction in WER over the generic model. Index Terms: deep neural network, domain adaptation, speaker adaptation, anchor embedding	acoustic cryptanalysis;acoustic model;anchor modeling;artificial neural network;cortana (halo);deep learning;desktop computer;domain adaptation;interpolation;speech recognition;test set;word error rate	Yong Zhao;Jinyu Li;Shi-Xiong Zhang;Liping Chen;Yifan Gong	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461553	interpolation;pattern recognition;prior probability;artificial intelligence;artificial neural network;acoustic model;computer science;matrix (mathematics);embedding;speech recognition;test set;data modeling	Vision	-18.005754120763243	-89.5236259925769	73209
8da010ca6a2677c05b3a182f521b4c1ef3f90138	automatic segmentation of speech recorded in unknown noisy channel characteristics	mejoramiento procedimiento;canal con ruido;robust speech recognition;base donnee;colored noise;modelo markov;bruit colore;reconocimiento palabra;automatic segmentation;speech processing;speech segmentation;noisy channel;additive noise;database;tratamiento palabra;ruido aditivo;traitement parole;base dato;bruit additif;segmentation;canal avec bruit;speech enhancement;noise robustness;spectral subtraction;automatic recognition;markov model;amelioration procede;soustraction spectrale generalisee;ruido coloreado;noise source;color noise;robustesse;source bruit;rehaussement mmse d ephraim malah;speech recognition;robustness;process improvement;reconnaissance parole;fuente ruido;modele markov;soustraction spectrale non lineaire;segmentacion;speech corpus development;reconocimiento automatico;reconnaissance automatique;robustez	This paper investigates the problem of automatic segmentation of speech recorded in noisy channel corrupted environments. Using an HMM-based speech segmentation algorithm, speech enhancement and parameter compensation techniques previously proposed for robust speech recognition are evaluated and compared for improved segmentation in colored noise. Speech enhancement algorithms considered include: Generalized Spectral Subtraction , Nonlinear Spectral Subtraction, Ephraim-Malah MMSE enhancement, and Auto-LSP Constrained Iterative Wiener ltering. In addition, the Parallel Model Combination (PMC) technique is also compared for additive noise compensation. In telephone environments, we compare channel normalization techniques including Cepstral Mean Normalization (CMN) and Signal Bias Removal (SBR) and consider the coupling of channel compensation with front-end speech enhancement for improved automatic segmentation. Compensation performance is assessed for each method by automatically segmenting TIMIT degraded by additive colored noise (i.e., aircraft cockpit, automobile highway, etc.), telephone transmitted NTIMIT, and cellular telephone transmitted CTIMIT databases.	additive white gaussian noise;algorithm;cepstrum;colors of noise;database normalization;hidden markov model;iterative method;mobile phone;noisy channel model;noisy-channel coding theorem;spectral method;speech enhancement;speech recognition;speech segmentation;standard business reporting;timit;utility functions on indivisible goods	Bryan L. Pellom;John H. L. Hansen	1998	Speech Communication	10.1016/S0167-6393(98)00031-4	speech recognition;colors of noise;computer science;speech processing	ML	-14.015315057091016	-95.43906908148328	73210
4b31e6be5409eee1864bf1a914d9a8f02e6dd27b	noise robust automatic speech recognition with adaptive quantile based noise estimation and speech band emphasizing filter bank	etalon atomique;background noise;noise estimation;metodo adaptativo;cuantila;voice activity detector;standards;filter bank;detection signal;automovil;banc filtre;etude experimentale;speech processing;signal detection;cepstre;database;tratamiento palabra;traitement parole;base dato;condicion estacionaria;methode adaptative;condition stationnaire;noise robustness;senal vocal;mel frequency cepstral coefficient;signal vocal;automatic speech recognition;automatic recognition;deteccion senal;reconocimiento voz;senal voceada;bande frequence;automobile;cepstrum;frequency band;banco filtro;motor car;adaptive method;horloge atomique;norma;analyse spectrale;base de donnees;stationary condition;pattern recognition;signal voise;speech recognition;ruido fondo;voiced signal;analisis espectral;patron atomico;quantile;reconnaissance forme;reconnaissance parole;voice activity detection;reconocimiento patron;spectral analysis;atomic standard;vocal signal;bruit fond;estimacion adaptativa;atomic clock;banda frecuencia;estudio experimental;norme;adaptive estimation;reconocimiento automatico;reconnaissance automatique;estimation adaptative;reloj atomico	An important topic in Automatic Speech Recognition (ASR) is to reduce the effect of noise, in particular when mismatch exists between the training and application conditions. Many noise robutness schemes within the feature processing domain use as a prerequisite a noise estimate prior to the appearance of the speech signal which require noise robust voice activity detection and assumptions of stationary noise. However, both of these requirements are often not met and it is therefore of particular interest to investigate methods like the Quantile Based Noise Estimation (QBNE) mehtod which estimates the noise during speech and non-speech sections without the use of a voice activity detector. While the standard QBNE-method uses a fixed pre-defined quantile accross all frequency bands, this paper suggests adaptive QBNE (AQBNE) which adapts the quantile individually to each frequency band. Furthermore the paper investigates an alternative to the standard mel frequency cepstral coefficient filter bank (MFCC), an empirically chosen Speech Band Emphasizing filter bank (SBE), which improves the resolution in the speech band. The combinations of AQBNE and SBE are tested on the Danish SpeechDat-Car database and compared to the performance achieved by the standards presented by the Aurora consortium (Aurora Baseline and Aurora Advanced Fronted). For the High Mismatch (HM) condition, the AQBNE achieves significantly better performance compared to the Aurora Baseline, both when combined with SBE and standard MFCC. AQBNE also outperforms the Aurora Baseline for the Medium Mismatch (MM) and Well Matched (WM) conditions. Though for all three conditions, the Aurora Advanced Frontend achieves superior performance, the AQBNE is still a relevant method to consider for small foot print applications.	aurora;baseline (configuration management);coefficient;experiment;filter bank;frequency band;mel-frequency cepstrum;requirement;signal-to-noise ratio;speech recognition;stationary process;voice activity detection	Casper Stork Bonde;Carina Graversen;Andreas Gregers Gregersen;Kim Hoang Ngo;Kim Nørmark;Mikkel Purup;Thomas Thorsen;Børge Lindberg	2005		10.1007/11613107_26	voice activity detection;atomic clock;quantile;speech recognition;telecommunications;computer science;cepstrum;filter bank;speech processing;background noise;detection theory	ML	-13.512602502822567	-95.82860270876861	73469
85a87c1923f58fda1dd7cf16a669be784a15608a	"""comprehension of kth text-to-speech with """"listening speed"""" paradigm"""	text to speech	The comprehension of natural and synthetic speech in Swedish and American English was investigated using a sentence-by-sentence listening paradigm. The synthesized speech was generated by the KTH text-to-speech systems. Results indicated that sentence listening times were signzficantly longer only for American English synthetic speech as compared to natural speech. Text dijjjculty was found to be a signijkant variable in both Swedish and American English for sentence listening times and word recognition, and only in American English for proposition recognition. The results are discussed in terms of the quality of the synthesizers and factors involved in comprehension.	american and british english spelling differences;list comprehension;natural language;programming paradigm;speech synthesis;synthetic intelligence	Lennart Neovius;Parimala Raghavendra	1993			speech recognition;natural language processing;active listening;comprehension;computer science;artificial intelligence;speech synthesis	NLP	-13.78774508283986	-83.54137135483124	73489
8a9fa58d4d6caae57dca5569832ea2a435504d30	data-driven pmc and bayesian learning integration for fast model adaptation in noisy conditions	bayesian learning	In this paper, we present an integration of Data Driven Parallel Model Combination (DPMC) and Bayesian Learning into a fast and accurate framework which can be easily integrated in standard training and recognition systems. The original DPMC technique has been enhanced to avoid any modi cation of the acoustic models, as required by the original method. The Bayesian Learning estimation has been used in order to specialize a general noisy speech model (the a priori model) to the target acoustic environment, where the DPMC-generated observations are used as adaptation data. Thanks to these innovations, the proposed method can achieve better performance than the original DPMC, while consuming far less computational resources.	acoustic cryptanalysis;acoustic model;computation;computational resource;finite-state machine;k-means clustering;performance;pubmed central;speech recognition	Stefano Crafa;Luciano Fissore;Claudio Vair	1998			speech recognition;artificial intelligence;pattern recognition;variable-order bayesian network;wake-sleep algorithm;machine learning;computer science;data-driven;bayesian inference	ML	-17.850731379292544	-92.27889993683806	73515
207ed1568fd06ac2bbcfc36128936313f3aafae9	correction of formal prosodic structures in czech corpora using legendre polynomials		Naturalness is a very important aspect of speech synthesis that is necessary for a pleasant and undemanding listening and understanding of synthesized speech. However, in a unit selection, unexpected changes in (F_0) caused by units transitions can lead to an inconsistent prosody. This paper proposes a two-phased classification-based method that improves the overall prosody by correcting a formal prosodic description of speech corpora. For speech data representation, the authors decided to use Legendre polynomials.	legendre polynomials;polynomial ring;text corpus	Martin Matura;Markéta Juzová	2018		10.1007/978-3-319-99579-3_41	anomaly detection;legendre polynomials;czech;external data representation;active listening;prosody;naturalness;pattern recognition;artificial intelligence;mathematics;speech synthesis	NLP	-19.487279233968447	-82.67559529456068	73577
053b4f29e9201e52e7d8a0293ac6d7a30549e74f	integrating articulatory data in deep neural network-based acoustic modeling	acoustic modeling;acoustic to articulatory mapping;deep neural networks;dnn hmm;electromagnetic articulography;autoencoders	HighlightsWe test strategies to exploit articulatory data in DNN-HMM phone recognition.Autoencoder-transformed articulatory features produce the best results.Pre-training of phone classifier DNNs driven by acoustic-to-articulatory mapping.Utility of articulatory information in noisy conditions and in cross-speaker settings. Hybrid deep neural network-hidden Markov model (DNN-HMM) systems have become the state-of-the-art in automatic speech recognition. In this paper we experiment with DNN-HMM phone recognition systems that use measured articulatory information. Deep neural networks are both used to compute phone posterior probabilities and to perform acoustic-to-articulatory mapping (AAM). The AAM processes we propose are based on deep representations of the acoustic and the articulatory domains. Such representations allow to: (i) create different pre-training configurations of the DNNs that perform AAM; (ii) perform AAM on a transformed (through DNN autoencoders) articulatory feature (AF) space that captures strong statistical dependencies between articulators. Traditionally, neural networks that approximate the AAM are used to generate AFs that are appended to the observation vector of the speech recognition system. Here we also study a novel approach (AAM-based pretraining) where a DNN performing the AAM is instead used to pretrain the DNN that computes the phone posteriors. Evaluations on both the MOCHA-TIMIT msak0 and the mngu0 datasets show that: (i) the recovered AFs reduce phone error rate (PER) in both clean and noisy speech conditions, with a maximum 10.1% relative phone error reduction in clean speech conditions obtained when autoencoder-transformed AFs are used; (ii) AAM-based pretraining could be a viable strategy to exploit the available small articulatory datasets to improve acoustic models trained on large acoustic-only datasets.	acoustic cryptanalysis;acoustic model;artificial neural network;deep learning	Leonardo Badino;Claudia Canevari;Luciano Fadiga;Giorgio Metta	2016	Computer Speech & Language	10.1016/j.csl.2015.05.005	speech recognition;computer science;pattern recognition	NLP	-17.692726632953843	-89.12867035295406	73647
390a5b74c43cc6b6e0ac66b022c8c233c2b3ad5a	assessing the acceptability of the smartkom speech synthesis voices		The acceptability of the synthetic voices used by the multimodal SmartKom dialog system was tested in a series of experiments. Early in the project a first set of evaluation tasks was carried out to verify the intelligibility of the diphone voice which serves as the default voice for external opendomain applications. The tests confirmed that the diphone voice produced satisfactory intelligibility. The speech corpus for the unit selection voice recorded by the same speaker is tailored to the typical, more restricted, SmartKom domains. Evaluation tasks focusing on typical SmartKom scenarios demonstrated the superiority of the unit selection voice. In tasks involving open-domain material, however, intelligibility of the unit selection voice appears to be less consistent than that of the diphone voice. In an audio-visual assessment task involving SmartKom specific contexts, the unit selection voice was found to be very well accepted and judged to be satisfactorily intelligible.	dialog system;experiment;intelligibility (philosophy);multimodal interaction;speech corpus;speech synthesis;synthetic intelligence	Antje Schweitzer;Norbert Braunschweiler;Grzegorz Dogil;Bernd Möbius	2004			acoustics;speech synthesis;computer science	NLP	-16.771381616953587	-83.45755875104248	73659
fd79966ebfb399d7cc8cd0469dbb1999c5756e9b	maximum margin training of gaussian hmms for handwriting recognition	databases;image recognition;off line handwriting recognition hidden markov model maximum margin training on line handwriting recognition;handwriting recognition;gaussian processes;nonconvex optimization;hidden markov model;on line handwriting recognition;training;learning artificial intelligence gaussian processes handwriting recognition hidden markov models image recognition;speech;partially labeled training set;handwriting recognition hidden markov models speech recognition text analysis algorithm design and analysis standards development automatic speech recognition maximum likelihood estimation testing performance evaluation;indexing terms;maximum likelihood estimation;partially labeled training set maximum margin training gaussian hmm hidden markov model handwriting recognition nonconvex optimization;hidden markov models;speech recognition;maximum margin training;gaussian hmm;learning artificial intelligence;off line handwriting recognition	Recent works for learning Hidden Markov Models in a discriminant way have focused on maximum margin training, which remains an open problem due to the lack of efficient optimization algorithms. We developed a new algorithm that is based on non convex optimization ideas and that may solve maximum margin learning of GHMMs within the standard setting of partially labeled training sets. We provide experimental results on both on-line handwriting and off-line handwriting recognition.	algorithm;convex optimization;discriminant;handwriting recognition;hidden markov model;markov chain;mathematical optimization;online and offline	Trinh Minh Tri Do;Thierry Artières	2009	2009 10th International Conference on Document Analysis and Recognition	10.1109/ICDAR.2009.221	speech recognition;index term;computer science;speech;machine learning;pattern recognition;gaussian process;maximum likelihood;hidden markov model	ML	-19.375092926447408	-91.87378453229988	73795
6f307c17a6fc89fcc659273695ac7326e5e39659	stable and unstable intervals as a basic segmentation procedure of the speech signal		The concept of acoustically stable and unstable intervals to structure continuous speech is introduced. We present a method to compute stable intervals efficiently and reliably as a bottom-up approach at an early processing stage. We argue that such intervals stand in close relation to the rhythm of speech as they contribute to the overall temporal organization of the speech production process and the acoustic signal (stable intervals = intervals of reduced movement of certain articulators; unstable intervals = intervals of enhanced movement of certain articulators). To test the relationship of stability intervals with speech rhythm we investigated the between-speaker variability of stable and unstable intervals in the TEVOID corpus. Results revealed that significant between-speaker variability exists. We hypothesize from our findings that the basic segmentation of speech into stable and unstable intervals is a process that might play a role in human perception and processing of speech.	acoustic cryptanalysis;bottom-up proteomics;control theory;heart rate variability;top-down and bottom-up design	Ulrike Glavitsch;Lei He;Volker Dellwo	2015			rhythm;artificial intelligence;speech rhythm;speech recognition;pattern recognition;perception;computer science;segmentation;speech production	NLP	-9.993155315822326	-82.60699988929713	73903
1561eb45907611a9a6954eb8e850b1525659bd06	context management with topics for spoken dialogue systems	probabilistic topic type tree;recognition accuracy;discourse model;domain model;different topic type;dialogue act;topic model;context management;topic tree;dialogue system;dialogue topic	In this paper wc discuss the use of discourse context in spoken dialogue systems and argue that the knowledge of the domain, modelled with the help of dialogue topics is important in maintaining robusthess of the system and improving recognition accuracy of spoken utterances. We propose a topic model which consists of a domain model, structured into a topic tree, and the Predict-Support algorithm which assigns topics to utterances on the basis of the topic transitions described in the topic tree and the words recognized in the input utterance. The algorithm uses a probabilistic topic type tree and mutual infor~ mation between the words and different topic types, and gives recognition accuracy of 78.68c~ and precision of 74.64%. This makes our topic model highly comparable to discourse models which are based on recognizing dialogue acts. 1 I n t r o d u c t i o n One of the fragile points in integrated spoken language systems is the erroneous analyses of the initial speech input, t The output of a speech recognizer has direct influence on the performance of other mod~ ules of the system (dealing with dialogue management, translation, database search, response planning, etc.), and the initial inaccuracy usually gets accumulated in the later stages of processing. Performance of speech recognizers can be improved by tuning their language model and lexicon, but prob~ lems still remain with the erroneous ranking of the best paths: information content of the selected utterances may be wrong. It is thus essential to use contextual information to compensate various errors in the output, to provide expectations of what will be said next and to help to determine the appropriate dialogue state. tlowever, negative effects of an inaccurate context have also been noted: cumulative error in discourse context drags performance of the system below the rates it would achieve were contextual information 1Alexandersson (1996) remarks tha t with a 3000 word lexicon, a 75 % word accuracy means that in practice the word lattice does not contain the actually spoken sentence. not used (Qu et ah, 1996; Church and Gale, 1991). Successful use of context thus presupposes appropriate context management: (1) features that define the context are relevant for the processing task, and (2) construction of the context is accurate. In this paper we argue in favour of using one type of contextual information, topic information, to maintain robustness of a spoken language systen,. Our model deals with the information content of utterances, and defines the context it, terms of topic types, related to the current domain knowledge and represented in the form of a topic tree. To update the context with topics we introduce the Predict-Support algorithm which selects utterance topics on the basis of topic transitions described in the topic tree and words recognized in the current utterance. At present, the algorithm is designed as a filter which re-orders the candidates produced by the speech recognizer, but future work encompasses integration of the algorithm into a language model and actual speech recognition process. The paper is organised as follows. Section 2 reviews the related previous research and sets out our starting point. Section 3 presents the topic model and the Predict-Support algorithm, and section 4 gives results of tile experiments conducted with the modeh Finally, section 5 summarises the properties of the topic model, and points to future research.	algorithm;bottom-up proteomics;cluster analysis;commonsense knowledge (artificial intelligence);dialog system;domain model;experiment;finite-state machine;language model;lexicon;mutual information;outline (list);parse tree;parsing;propagation of uncertainty;self-information;sparse matrix;speech recognition;speech synthesis;statistical model;top-down and bottom-up design;topic model	Kristiina Jokinen;Hideki Tanaka	1998			natural language processing;speech recognition;computer science;domain model;mutual information;statistics	NLP	-20.109463026161755	-81.79263177800438	73952
2a8ee42e5116012d820b8d2aa52db4516cae6fc1	structured svms for automatic speech recognition	hidden markov models mathematical model joints training equations support vector machines vectors;support vector machines;gaussian processes;log linear models structured support vector machines large margin;vocabulary;vocabulary feature extraction gaussian processes hidden markov models signal classification speech recognition support vector machines;hidden markov models;feature extraction;signal classification;aurora 4 structured svm automatic speech recognition structured discriminative model flexible sequence classification approach structured support vector machines vocabulary speech recognition task context dependent generative model hidden markov model combined generative model feature extraction utterance segmentation viterbi like scheme optimal segmentation large margin log linear model zero mean gaussian prior training algorithm 1 slack algorithm caching competing hypothesis parallelization strategy aurora 2;speech recognition	Structured discriminative models are a flexible sequence classification approach that enable a wide variety of features to be used. This paper describes a particular model in this framework, structured support vector machines (SSVM), and how it can be applied to medium to large vocabulary speech recognition tasks. An important aspect of SSVMs is the form of the joint feature spaces. Here, context-dependent generative models, hidden Markov models, are used to obtain the features. To apply this form of combined generative and discriminative model to medium and larger vocabulary tasks, a number of issues need to be addressed. First, the features extracted are a function of the segmentation of the utterance. A Viterbi-like scheme for obtaining the “optimal” segmentation is described. Second, SSVMs can be viewed as large margin log linear models using a zero mean Gaussian prior of the discriminative parameter. However this form of prior is not appropriate for all features. A modified training algorithm is proposed that allows general Gaussian priors to be incorporated into the large margin criterion. Finally to speed up the training process, a 1-slack algorithm, caching competing hypotheses and parallelization strategies are also described. The performance of SSVMs is evaluated on small and medium to large speech recognition tasks: AURORA 2 and 4.	algorithm;context-sensitive language;cutting-plane method;discriminative model;feature vector;generative model;global serializability;hidden markov model;integer programming;kernelization;linear model;markov chain;parallel computing;robustification;speech recognition;structured support vector machine;triphone;vehicle tracking system;vocabulary	Shi-Xiong Zhang;Mark J. F. Gales	2013	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2012.2227734	support vector machine;speech recognition;feature extraction;computer science;machine learning;pattern recognition;gaussian process;hidden markov model	ML	-18.780251523206395	-91.14941722161004	74026
4e03b49d2bd9668cd9bb1d4c671f0fb761876f11	say cheese vs. smile: reducing speech-related variability for facial emotion recognition	phoneme;speech;emotion recognition;segmentation;dynamics;viseme;facial expression;dynamic time warping	Facial movement is modulated both by emotion and speech articulation. Facial emotion recognition systems aim to discriminate between emotions, while reducing the speech-related variability in facial cues. This aim is often achieved using two key features: (1) phoneme segmentation: facial cues are temporally divided into units with a single phoneme and (2) phoneme-specific classification: systems learn patterns associated with groups of visually similar phonemes (visemes), e.g. P, B, and M. In this work, we empirically compare the effects of different temporal segmentation and classification schemes for facial emotion recognition. We propose an unsupervised segmentation method that does not necessitate costly phonetic transcripts. We show that the proposed method bridges the accuracy gap between a traditional sliding window method and phoneme segmentation, achieving a statistically significant performance gain. We also demonstrate that the segments derived from the proposed unsupervised and phoneme segmentation strategies are similar to each other. This paper provides new insight into unsupervised facial motion segmentation and the impact of speech variability on emotion classification.	biconnected component;emotion recognition;facial recognition system;heart rate variability;modulation;spatial variability;unsupervised learning	Yelin Kim;Emily Mower Provost	2014		10.1145/2647868.2654934	computer vision;dynamics;speech recognition;computer science;speech;dynamic time warping;viseme;segmentation;facial expression	Vision	-12.079438045488232	-88.15421477498502	74060
1396225d3aa5ceef2ba80ff0aa6328d9218114de	smart multifunctional digital content ecosystem using emotion analysis of voice		In an attempt to establish an improved service-oriented architecture (SOA) for interoperable and customizable access of digital cultural resources an automatic deterministic technique can potentially lead to the improvement of searching, recommending and personalizing of content. Such technique can be developed in many ways using different means for data search and analysis. This paper focuses on the use of voice and emotion recognition in speech as a main vehicle for delivering an alternative way to develop novel solutions for integrating the loosely connected components that exchange information based on a common data model. The parameters used to construct the feature vectors for analysis carried pitch, temporal and duration information. They were compared to the glottal symmetry extracted from the speech source using inverse filtering. A comparison to their first derivatives was also a subject of investigation in this paper. The speech source was a 100-minute long theatrical play containing four male speakers and was recorder at 8kHz with 16-bit sample resolution. Four emotional states were targeted namely: happy, angry, fear, and neutral. Classification was performed using k-Nearest Neighbor method. Training and testing experiments were performed in three scenarios: 60/40, 70/30 and 80/20 minutes respectively. A close comparison of each feature and its rate of change show that the time-domain features perform better while using lesser computational strain than their first derivative counterparts. Furthermore, a correct recognition rate was achieved of up 95% using the chosen features.	16-bit;computation;connected component (graph theory);data model;ecosystem;emotion recognition;experiment;feature vector;global variable;interoperability;inverse filter;k-nearest neighbors algorithm;mike lesser;multi-function printer;sampling (signal processing);service-oriented architecture;service-oriented device architecture	Alexander I. Iliev;Peter L. Stanchev	2017		10.1145/3134302.3134342	data mining;architecture;machine learning;filter (signal processing);computer science;data model;digital content;feature vector;speech recognition;emotion recognition;artificial intelligence;smart system	NLP	-8.275475719213354	-91.66543542810543	74285
67e621d31d94add911b106221f5874b83366b043	application of time-frequency analysis to the classification of bjork-shiley convexo-concave heart valve condition	cardiology;acoustic signal processing;prosthetics;short time fourier transform;time frequency analysis laser sintering resonant frequency heart valves leg flanges signal generators prosthetics acoustic signal detection acoustic testing;resonant frequency;fourier transforms;prosthetics time frequency analysis cardiology fourier transforms acoustic signal processing medical signal processing;data acquisition system time frequency analysis bjork shiley convexo concave heart valve acoustic signal analysis prosthetic heart valve single leg separated bscc valve outlet strut intact outlet strut short time fourier transform based approach resonant frequency acoustic signature acoustic recordings;prosthetic heart valve;heart valve;time frequency analysis;medical signal processing	This paper presmt.s the resirlis ofn iiine-fi.eyiiency ono1.v.vi.s of the ncoirstic .signnl.s generoted during ihe closing qf o ~ ~ o r k S h i l e y (’onvero-(bncove prosthetic hcnrt vnlve The ohlective of this ana1.vsi.s i s to iilentifL f~nt i rres in ihe ticoirstic signnitire io clossIfi the heart valve condition os Intaci or Single-Leg Sepornietl (XLS). The SLS contlition occirr.~ when one vf the legs vf the BSCC valve vuilei .sirzit seporotes from ihe volve ,flange [l] The SLS condition is believed io occur prior lo on oirilet .strtit frrrciiire. in which both legs qf the stritt separate ,from ihe flange The presence o f t h e tonnl coinponeni ossocintetl wiih the resonont ,frequency of on Intoci Outlei Sfrirt (IOS) 11o.s heen inihcntive of on Inioct volve condition. while the obsence (Jf the IOS ,frequency i,v on inihcoior of poteniiol SLY [2] . This poper describes the iinplernentniion of o ShoriTime Fourier Tron.sfiiriii-bn.rerl npprooch fvr the deteciivn of the IOS resonnnt frequency. including the reyiriretl preprocessing steps ond a post-processing technique for rtlenti/jin<y the tonal cvinpvnenis in the ncoirstic signature which are consistent ocross o given recording session Ann1.v.vi.s resiills ore presented .for acoustic recortlings .froin 22 volves jiir which the siote (Intact or SLS) 1,s known Initial tests indicate ihot the .sinhrlity of ihe IOS resonnnf freyiiency fioin heoi to heat nntl ocross recording ,sessions inoy he iised io posi iive~-v iikwti[v the I O s in ihe presence (Jf interfiring valve tonols Keynwrrls. Class!ficoiion Time-freyirencv nnol.vsis. SlioriTime Fourier Trnnsforin. EX’(’ Heart Vnlve,	acoustic cryptanalysis;closing (morphology);concave function;frequency analysis;preprocessor;standard sea level;time–frequency analysis;tron;video post-processing	Kent Scarbrough;Rebecca S. Inderbitzen	1994		10.1109/CBMS.1994.316000	fourier transform;time–frequency analysis;resonance;short-time fourier transform	DB	-7.0779818029336194	-89.00283310813568	74403
0da851ccc445e6d60835b82237ce3b66873b97a5	persistent information state in a data-centric architecture	adamach data;dialog system;persistent information state;off-line mining;speech recognition result;dialog context;domain knowledge;fat pipeline;university helpdesk domain;language processing;empirical data;data-centric architecture	We present the ADAMACH data centric dialog system, that allows to perform onand offline mining of dialog context, speech recognition results and other system-generated representations, both within and across dialogs. The architecture implements a “fat pipeline” for speech and language processing. We detail how the approach integrates domain knowledge and evolving empirical data, based on a user study in the University Helpdesk domain.	database-centric architecture;dialog manager;dialog system;file allocation table;online and offline;speech recognition;spoken dialog systems;usability testing	Sebastian Varges;Giuseppe Riccardi;Silvia Quarteroni	2008			natural language processing;speech recognition;computer science;dialog system;communication	NLP	-28.06878328379829	-85.36819681302975	74486
0f7363d39b0484e51cd01855180327d5d1fc77b1	compensating acoustic mismatch using class-based histogram equalization for robust speech recognition	mismatching;signal image and speech processing;traitement signal;evaluation performance;phonetique;robust speech recognition;amelioration parole;base donnee;egalisation;performance evaluation;ligne de base;learning;fonction repartition;correction erreur;evaluacion prestacion;error relativo;speech processing;erreur quadratique moyenne;additive noise;database;tratamiento palabra;ruido aditivo;traitement parole;base dato;equalization;bruit additif;speech enhancement;algorithme;aprendizaje;algorithm;funcion distribucion;histogram;distribution function;apprentissage;relative error;cepstral analysis;desadaptacion;quantum information technology spintronics;reconocimiento voz;histogramme;analyse cepstrale;igualacion;mean square error;error correction;feature extraction;signal processing;erreur relative;baseline;fonetica;speech recognition;phonetics;desadaptation;reconnaissance parole;correccion error;extraction caracteristique;error medio cuadratico;histograma;procesamiento senal;histogram equalization;algoritmo	A new class-based histogram equalization method is proposed for robust speech recognition. The proposed method aims at not only compensating for an acoustic mismatch between training and test environments but also reducing the two fundamental limitations of the conventional histogram equalization method, the discrepancy between the phonetic distributions of training and test speech data, and the nonmonotonic transformation caused by the acoustic mismatch. The algorithm employs multiple class-specific reference and test cumulative distribution functions, classifies noisy test features into their corresponding classes, and equalizes the features by using their corresponding class reference and test distributions. The minimum mean-square error log-spectral amplitude (MMSE-LSA)-based speech enhancement is added just prior to the baseline feature extraction to reduce the corruption by additive noise. The experiments on the Aurora2 database proved the effectiveness of the proposed method by reducing relative errors by 62% over the mel-cepstral-based features and by 23% over the conventional histogram equalization method, respectively.	acoustic cryptanalysis;acoustic fingerprint;additive white gaussian noise;algorithm;baseline (configuration management);cepstrum;data logger;discrepancy function;experiment;feature extraction;feature vector;histogram equalization;motion estimation;regular expression;speech enhancement;speech recognition;test data;utility functions on indivisible goods	Youngjoo Suh;Sungtak Kim;Hoirin Kim	2007	EURASIP J. Adv. Sig. Proc.	10.1155/2007/67870	phonetics;computer vision;approximation error;error detection and correction;speech recognition;equalization;feature extraction;computer science;histogram matching;artificial intelligence;distribution function;signal processing;speech processing;histogram;mean squared error;adaptive histogram equalization;baseline;histogram equalization;statistics	ML	-13.699424714000793	-95.70793902259328	74503
040dffa304a112197f3918b219ab4b03ffe9fd79	deep syntactic analysis and rule based accentuation in text-to-speech synthesis	text to speech synthesis;speech synthesis;rule based;finnish;prosodic prominence;discourse structure;syntactic analysis;text to speech;prediction model	With the emergence of the HMM-synthesis paradigm, producing natural, expressive prosody has become viable in speech synthesis. This paper describes the development of rule-based prominence prediction model for Finnish Text-to-Speech system, based on deep syntactic analysis and discourse structure.	speech synthesis	Antti Suni;Martti Vainio	2008		10.1007/978-3-540-87391-4_68	natural language processing;speech recognition;computer science;parsing;predictive modelling;linguistics;speech synthesis	Logic	-16.806099074947312	-84.95565186021379	74638
28a511f693db49ad06fa0c34cc6ad50a808ea67b	analysis of the effect of speech-laugh on speaker recognition system		A robust speaker recognition system should be able to recognize a speaker despite all the possible variations in speaker’s speech. A common variation of the neutral speech is speechlaugh, which occurs when a person is speaking and laughing, simultaneously. In this paper, we show that speech-laugh significantly degrades the performance of an i-vector based speaker recognition system. Further, we show that laughter and neutral speech contain complementary speaker information, which can be combined to improve the performance of the speaker recognition system for speech-laugh scenarios. Using AMI meeting corpus database, we show that by including neutral speech and laughter in enrollment phase, the performance of the system in the speech-laugh scenarios can be relatively improved by 36% in EER.	enhanced entity–relationship model;speaker recognition;speech synthesis	Sri Harsha Dumpala;Ashish Panda;Sunil Kumar Kopparapu	2018		10.21437/Interspeech.2018-2090	speech recognition;speaker recognition;artificial intelligence;pattern recognition;computer science	NLP	-13.541298563379053	-89.52005088523958	74717
f3334cf27d8f29f4526f8c9270bd9b5c8c5b9816	a tool for automatic simplification of swedish texts		We present a rule based automatic text simplification tool for Swedish. The tool is designed to facilitate experimentation with various simplification techniques. The architecture of the tool is inspired by and partly built on a previous text simplification tool for Swedish, CogFLUX. New functionality, new operation types, and new simplification operations were added.	experiment;gene expression programming;text simplification	Evelina Rennes;Arne Jönsson	2015			natural language processing;text simplification;computer science;engineering drawing	HCI	-29.673436637233	-81.00289004514366	74726
827fdecf6a292cefb21837b9d11533a0e40f9e08	the conversation: deep audio-visual speech enhancement		Our goal is to isolate individual speakers from multi-talker simultaneous speech in videos. Existing works in this area have focussed on trying to separate utterances from known speakers in controlled environments. In this paper, we propose a deep audio-visual speech enhancement network that is able to separate a speakeru0027s voice given lip regions in the corresponding video, by predicting both the magnitude and the phase of the target signal. The method is applicable to speakers unheard and unseen during training, and for unconstrained environments. We demonstrate strong quantitative and qualitative results, isolating extremely challenging real-world examples.	convolution;flow network;linear scale;short-time fourier transform;spectrogram;speech enhancement;subnetwork	Triantafyllos Afouras;Joon Son Chung;Andrew Zisserman	2018		10.21437/Interspeech.2018-1400	speech recognition;conversation;computer science;speech enhancement	Vision	-11.208896897184108	-89.59804074199998	74736
bc6fa174e92bb9dd761f56a77e2383e6f35562d2	a new computer-based analytical speech perception test for prelingually deaf children and children with speech disorders	speech perception			Anne-Marie Öster	2002			cued speech;speech recognition;speech perception;speech disorder;computer science;intelligibility (communication)	HCI	-7.597277754230513	-84.08624539477475	74848
1d3327b1d36335ecef3418306ca5cae77add8b47	speaker verification with the mixture of gaussian factor analysis based representation	databases;analytical models;nist;probability;会议论文;text analysis gaussian processes speaker recognition;mel frequency cepstral coefficient;nist noise probability analytical models databases mel frequency cepstral coefficient;text independent speaker verification generalized i vector representation framework mixture of gaussian factor analysis mog factor analysis single standard factor analysis single multivariate gaussian frame level posterior probability gaussian distributed text dependent speaker verification;i vector speaker verification factor analysis mixture of gaussian;noise	This paper presents a generalized i-vector representation framework using the mixture of Gaussian (MoG) factor analysis for speaker verification. Conventionally, a single standard factor analysis is adopted to generate a low rank total variability subspace where the mean supervector is assumed to be Gaussian distributed. The energy that can't be represented by the low rank space is modeled by a single multivariate Gaussian. However, due to the sparsity of the frame level posterior probability and the short duration characteristics, some dimensions of the first-order statistics may not be Gaussian distributed. Therefore, we replace the single Gaussian with a mixture of Gaussians to better represent the residual energy. Experimental results on the NIST SRE 2010 condition 5 female task and the RSR 2015 part 1 female task show that the MoG i-vector outperforms the i-vector baseline by more than 10% relatively for both text independent and text dependent speaker verification tasks, respectively.	baseline (configuration management);factor analysis;first-order predicate;mixture model;sparse matrix;spatial variability;speaker recognition	Ming Li	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178858	gaussian noise;speech recognition;nist;computer science;noise;pattern recognition;probability;mathematics;gaussian filter;gaussian function;statistics	Robotics	-17.418167325633263	-92.10060400978998	74908
a0112b00eea12649659b13e02b03ebcbf13cbb9b	l1/l2 difference in phonological sensitivity and information planning — evidence from f0 patterns	stress;acoustics;linear regression;noise measurement;decision support systems;robustness;root mean square	Assuming that linguistic specifications and information planning contribute to different levels of prosodic organization that cumulatively constitute output prosody, quantitative analysis of respective contributions can be derived through normalization procedures that remove levels of interactions involved. The current study attempts to account for how Τ2 prosody departs from the Τ1 norm in the two levels mentioned and whether an account can be offered. F0 patterns of word English stress categories (primary, secondary and tertiary) and emphases in controlled conditions (narrow-, broad- and non-focus) are compared using speech data from English Τ1 and Mandarin Τ2 speakers. Τ1 speech exhibits similar F0 patterns of binary high-low contrasts in both stress/non-stress as well as focus/non-focus categories, suggesting comparable planning are used to express phonological and information planning. However, Li's primary stress and emphasis exhibited less degree of F0 high-low contrast, coupled with reversed F0 patterns in both the secondary and tertiary categories as well as non-emphases conditions. The results demonstrate that being less sensitive to phonological categories may also affect information planning in similar ways. We believe the results explain how stress and focus interact to cause L2 accent and unintelligibility, help understand stress and focus composition of L1-and-L2 speech, and are readily applicable to CALL.	database normalization;interaction;semantic prosody;super robot monkey team hyperforce go!	Chao-yu Su;Chiu-yu Tseng	2016	2016 10th International Symposium on Chinese Spoken Language Processing (ISCSLP)	10.1109/ISCSLP.2016.7918390	natural language processing;speech recognition;root mean square;decision support system;acoustics;computer science;noise measurement;linear regression;linguistics;stress;robustness	NLP	-11.150855342167361	-82.7573944772451	75093
3e49a87726bd8aa15dbe91e2074d4f380e6ac5fc	toward a parallel corpus of spoken cantonese and written chinese		We introduce a parallel corpus of spoken Cantonese and written Chinese. This sentencealigned corpus consists of transcriptions of Cantonese spoken in television programs in Hong Kong, and their corresponding Chinese (Mandarin) subtitles. Preliminary evaluation shows that the corpus reflects known syntactic differences between Cantonese and Mandarin, facilitates quantitative analyses on these differences, and already reveals some phenomena not yet discussed in the literature.	parallel text;super robot monkey team hyperforce go!	John Lee	2011			natural language processing;speech recognition;linguistics	NLP	-16.991238818112006	-81.50153999089564	75136
3b94ceb739519bfd4584a0cb0b452fcf8ca30383	segmentation of touching and fused devanagari characters	character fragmentation;prototype construction;devanagari script;character segmentation;character fusion;character text recognition;character segmentation decomposition;structural properties	Devanagari script is a two dimensional composition of symbols. It is highly cumbersome to treat each composite character as a separate atomic symbol because such combinations are very large in number. This paper presents a two pass algorithm for the segmentation and decomposition of Devanagari composite characters/symbols into their constituent symbols. The proposed algorithm extensively uses structural properties of the script. In the first pass, words are segmented into easily separable characters/composite characters. Statistical information about the height and width of each separated box is used to hypothesize whether a character box is composite. In the second pass, the hypothesized composite characters are further segmented. A recognition rate of 85 percent has been achieved on the segmented conjuncts. The algorithm is designed to segment a pair of touching characters.	algorithm;composite video;modifier key;printing;vertical bar	Veena Bansal;R. Mahesh K. Sinha	2002	Pattern Recognition	10.1016/S0031-3203(01)00081-4	arithmetic;speech recognition;computer science	Vision	-26.250949250597973	-81.71769235573859	75217
8523ed4ecd32c37716afde9da5e5c99705c73fd9	moody music generator: characterising control parameters using crowdsourcing		We characterise the expressive effects of a music generator capable of varying its moods through two control parameters. The two control parameters were constructed on the basis of existing work on valence and arousal in music, and intended to provide control over those two mood factors. In this paper we conduct a listener study to determine how people actually perceive the various moods the generator can produce. Rather than directly attempting to validate that our two control parameters represent arousal and valence, instead we conduct an open-ended study to crowd-source labels characterising different parts of this twodimensional control space. Our aim is to characterise perception of the generator’s expressive space, without constraining listeners’ responses to labels specifically aimed at validating the original arousal/valence motivation. Subjects were asked to listen to clips of generated music over the Internet, and to describe the moods with free-text labels. We find that the arousal parameter does roughly map to perceived arousal, but that the nominal “valence” parameter has strong interaction with the arousal parameter, and produces different effects in different parts of the control space. We believe that the characterisation methodology described here is general and could be used to map the expressive range of other parameterisable generators.	crowdsourcing;internet;nonlinear gameplay	Marco Scirea;Mark J. Nelson;Julian Togelius	2015		10.1007/978-3-319-16498-4_18	simulation;speech recognition;engineering;data science	HCI	-9.058981922364687	-85.36608774235056	75330
d5d88ed003c705f47915f5bbd132c8a216c94e13	general and nimble ocr open system and the flexible segmentation recognition algorithms			algorithm	Jiangchuan Du;Hongjian Liu	1988			pattern recognition;computer vision;artificial intelligence;mathematics;open system (systems theory);speech recognition;segmentation	Vision	-8.740415896759178	-98.09706007477062	75334
2123a4e6a81cc340f30b538c3ad609ee525b948e	the effects of syntactic and discourse variables on the segmental intelligibility of speech			intelligibility (philosophy)	Judith C. Goodman;Howard C. Nusbaum;Lisa Lee;Kevin Broihier	1990			speech recognition;syntax;natural language processing;intelligibility (communication);computer science;artificial intelligence	NLP	-14.289026404756967	-84.671679322634	75350
7d780a86ab775ef2d271881b2089084c9a55bf5b	automatic labelling of voice-quality in speech databases for synthesis	voice quality	A series of experiments was performed to determine the extent to which voice-quality di erences could be labelled automatically in a speech database. Using speech corpora of three di erent speaking styles from the same speaker as test material, hidden-Markov models were trained to distinguish the prosodic and acoustic characteristics of each style, and were used to re-label the voiced-segments in order to provide a single, merged, labelled corpus. Perceptual tests of speech synthesised by concatenation using CHATR showed that both prosodic and voice-quality cues to stylistic variation (in this case emotion) can be detected and labelled by the trained models. However, speech synthesised from the original separate databases was perceived as being more expressive.	acoustic cryptanalysis;concatenation;database;experiment;hidden markov model;markov chain;speech corpus;speech synthesis;text corpus	Nick Campbell;Toru Marumoto	2000			speech recognition;natural language processing;labelling;artificial intelligence;computer science	NLP	-14.231565539925347	-83.30763553607918	75518
428895d9a443dbeb3eb2599766b3534d45fa698b	on the portability of complex constraint-based grammars	complex constraint-based grammar	"""]{.ec('.nt years have seen tile appearance of' a number of g r a m m a r f 'ormalisms 1 shar ing a s t rong family resemblance, which we have character ised elsewhere [R,upp et al., 199d] as tim proper ty of being constraint-based. As well as having in common many formal properties, these formal isms also suppor t , of ten by explicit de.sign, descr ipt ions from a similarly convergent range o[' l inguistic theories, which we might reasonably label """" [ lPSG-l ike"""" ."""	arnold;cj's elephant antics;computation;computational linguistics;course (navigation);experiment;internet key exchange;radio frequency;semantics (computer science);software portability;team foundation server;word lists by frequency	C. J. Rupp;Rod L. Johnson	1994			natural language processing;artificial intelligence;family resemblance;shar;programming language;computer science;software portability;rule-based machine translation	AI	-30.518128594551726	-80.71897739969359	75548
503cb25505c08a1cf41dc266d07d2c97d1ca47f8	an hmm-mlp hybrid model for cursive script recognition	variable etat;base donnee;senal discreta;modelo markov;methode mesure;layer model;caracter manuscrito;proceso markov;hidden markov model;modele hidden markov;modelo capa;state variable;multilayer perceptrons;manuscript character;extraction forme;modelo hibrido;database;base dato;metodo medida;heterojunctions;geometria variable;geometrie variable;segmentation;cheque bancaire;modele hybride;hybrid model;identificacion sistema;perceptron multicouche;word segmentation;markov model;reconnaissance caractere;red multinivel;system identification;extraccion forma;mesure probabilite;processus markov;modele couche;variable geometry;markov process;estimacion parametro;discrete signal;word recognition;variable estado;signal discret;cheque bancario;bank check;multi layer perceptron;multilayer network;parameter estimation;estimation parametre;modele markov;measurement method;reseau multicouche;reseau neuronal;heterojonction;caractere manuscrit;probability measure;character recognition;pattern extraction;red neuronal;medida probabilidad;segmentacion;identification systeme;state transition;reconocimiento caracter;neural network	This paper presents an HMM (Hidden Markov Model)-MLP (Multi-Layer Perceptron) hybrid model for recognising cursive script words. We adopt an explicit segmentation-based word level architecture to implement an HMM classifier. An efficient state transition model and a parameter re-estimation scheme are introduced to use non-scaled and non-normalised symbol vectors without having to label primitive vectors. This approach brings well-formed discrete signals for the variable state duration of the HMM. We also introduce a new probability measure as well as conventional schemes to combine the proposed HMMs and a general MLP. The main contributions of this model are a novel design of the segmentation-based variable length HMMs, and an efficient method of combining two distinct classifiers. Experiments have been conducted using the legal word database of CENPARMI with encouraging results.	experiment;hidden markov model;markov chain;memory-level parallelism;perceptron;quad flat no-leads package;rejection sampling;well-formed petri net	Jinho Kim;Kye Kyung Kim;Ching Y. Suen	2000	Pattern Analysis & Applications	10.1007/s100440070003	text segmentation;speech recognition;heterojunction;probability measure;system identification;discrete-time signal;cashier's check;word recognition;computer science;artificial intelligence;machine learning;mathematics;markov process;markov model;estimation theory;multilayer perceptron;segmentation;hidden markov model;statistics;state variable	AI	-22.430248824757918	-90.98823233254889	75581
e56b52287929b4ed9a0874380ff32498385072bd	a perceptually motivated visualisation paradigm for musical timbre	timbre;generative computer graphics;audio analysis;timbre visualisation	Background is given on existing research into the perception and description of timbre. This is used to contextualise the development of visual mappings for acoustic timbre features. Two example systems are explained and discussed, which demonstrate the use of perceptually motivated mappings from acoustic features to visual properties, in specific contexts. Possible applications and extensions of such systems are proposed.	acoustic cryptanalysis;machine perception;paradigm	Sean Soraghan;Alain Renaud;Ben Supper	2016		10.14236/ewic/EVA2016.11	speech recognition;acoustics;order;communication	Robotics	-7.495819341673407	-86.66157379900896	75636
bf9caa40d8ae6c21402a5cecd6032533674f7ad3	non-sequential automatic classification of anuran sounds for the estimation of climate-change indicators		Abstract Several biological research studies have shown that the number of individuals of certain species of anurans in a specific geographical region, and the evolution of this number over time, can be used as an indicator of climate change. To detect the presence of anurans, Wireless Sensor Networks (WSNs) are usually deployed with the aim of obtaining bio-acoustic information in a set covering numerous locations. However, the identification of the anuran species from a huge number of recordings usually involves an overwhelming task that has to be undertaken by expert and intelligent systems. Previous studies into this issue have proposed several classification techniques with a common approach: they all take into account the sequential characteristic of sounds by considering syllables or other kinds of vocal segments. In noisy sounds, as it is usually the case in recordings made in natural habitats, segmentation of the signal is no straightforward task and may cause low classification accuracy. To override this problem, a new non-sequential approach is proposed in this paper. It is based on considering very small pieces of sounds (frames) each of which is then classified without considering preceding or subsequent information. Up to nine frame-based classifiers are explored in this paper and their performances are compared to the most commonly used sequential classifier: the Hidden Markov Model (HMM). Additionally, for featuring the frames, many choices have been described, although the application of the Mel Frequency Cepstral Coefficients (MFCCs) has probably become the most common method. In this work, an alternative methodology is suggested: the use of a set of MPEG-7 parameters, which offers a normalized solution with a much greater semantic content. The experimental results have shown that the proposed method clearly outperforms the HMM, thereby showing the non-sequential classification of anuran sounds to be feasible. From among the algorithms tested, the decision-tree classifier has shown the best performance with an overall classification success rate of 87.30%, which is an especially striking result considering that the analyzed sounds were affected by a decidedly noisy background.		Amalia Luque;Javier Romero-Lemos;Alejandro Carrasco;Julio Barbancho	2018	Expert Syst. Appl.	10.1016/j.eswa.2017.11.016	wireless sensor network;machine learning;normalization (statistics);intelligent decision support system;hidden markov model;mel-frequency cepstrum;classifier (linguistics);pattern recognition;computer science;artificial intelligence	Vision	-6.011521480814979	-89.90770278097152	75674
5f0815d465c18f66634425d306e78c8dfee77910	a time-length constrained level building algorithm for large vocabulary handwritten word recognition	experimental design;vocabulaire;ajustamiento modelo;analisis estadistico;funcion no lineal;caracter manuscrito;search space;manuscript character;vocabulary;plan experiencia;non linear function;regression model;vocabulario;linear functionals;ajustement modele;modelo regresion;reconnaissance caractere;statistical analysis;plan experience;modele regression;model matching;analyse statistique;palabra;fonction non lineaire;handwritten word recognition;word;caractere manuscrit;character recognition;reconocimiento caracter;mot	In this paper we introduce a constrained Level Building Algorithm (LBA) in order to reduce the search space of a Large Vocabulary Handwritten Word Recognition (LVHWR) system. A time and a length constraint are introduced to limit the number of frames and the number of levels of the LBA respectively. A regression model that fits the response variables, namely, accuracy and speed, to a non–linear function of the constraints is proposed and a statistical experimental design technique is employed to analyse the effects of the two constraints on the responses. Experimental results prove that the inclusion of these constraints improve the recognition speed of the LVHWR system without changing the recognition rate significantly.	algorithm;design of experiments;fits;linear function;vocabulary	Alessandro L. Koerich;Robert Sabourin;Ching Y. Suen	2001		10.1007/3-540-44732-6_13	speech recognition;computer science;artificial intelligence;word;mathematics;design of experiments;algorithm;regression analysis;statistics	Vision	-21.170745274719312	-90.09933244388978	75719
449f04b6d10d6a5f06f2727fbb6cf48010edcfef	minimum classification error training for online handwritten word recognition	word error rate;optimisation;probability;hidden markov modeling minimum classification error training online handwritten word recognition online unconstrained style word recognition allograph hmms writer variability baseline maximum likelihood system;maximum likelihood;bayes methods;parameter estimation probability hidden markov models bayes methods decision theory optimisation handwritten character recognition;hidden markov models;handwriting recognition hidden markov models error analysis writing vocabulary personal digital assistants handheld computers shape signal processing speech recognition;decision theory;word recognition;handwritten word recognition;parameter estimation;minimum classification error;handwritten character recognition	We describe an application of the minimum classification error (MCE) training criterion to online unconstrained-style word recognition. The described system uses allograph-HMMs to handle writer variability. The result, on vocabularies of 5k to 10k, shows that MCE training achieves around 17% word error rate reduction when compared to the baseline maximum likelihood system.		Alain Biem	2002		10.1109/IWFHR.2002.1030885	speech recognition;decision theory;word recognition;word error rate;computer science;intelligent word recognition;machine learning;pattern recognition;probability;maximum likelihood;estimation theory;hidden markov model;statistics	Vision	-19.322893033157765	-91.90767860307247	75782
7b5422f4da6705cc89b6a0d3061da94c8930adb3	context-free 2d tree structure model of musical notes for bayesian modeling of polyphonic spectrograms		[1] N. Bertin, R. Badeau, and G. Richard, “Blind signal decompositions for automatic transcription of polyphonic music: NMF and K-SVD on the benchmark,” in Proc. ICASSP, Vol. 1, pp. 65-68, 2007. [2] K. Ochiai, H. Kameoka, and S. Sagayama, “Explicit beat structure modeling for non-negative matrix factorization-based multipitch analysis,” in Proc. ICASSP, pp. 133-136, 2012. [3] A. T. Cemgil, “Bayesian inference in non-negative matrix factorisation models,” Technical Report CUED/FINFENG/TR.609, University of Cambridge, 2008. [4] M. D. Hoffman, D. M. Blei, and P. R. Cook, “Bayesian nonparametric matrix factorization for recorded music,” in Proc. ICML, pp. 439-446, 2010. [5] K. Yoshii, and M. Goto, “A nonparametric Bayesian multipitch analyzer based on infinite latent harmonic allocation,” IEEE Trans. Audio, Speech, Language Process., Vol. 20, No. 3, pp. 717-730, 2012. [6] M. Nakano, Y. Ohishi, H. Kameoka, R. Mukai, and K. Kashino, “Bayesian nonparametric music parser,” in Proc. ICASSP, pp. 461-464, 2012. [7] P. Liang, S. Petrov, M. I. Jordan, and D. Klein, “The infinite PCFG using hierarchical Dirichlet processes,” in Proc. EMNLP, pp. 688-697, 2007. [8] H. Kameoka, T. Nishimoto, and S. Sagayama, “A multipitch analyzer based on harmonic temporal structured clustering,” IEEE Trans. on Audio, Speech, Language Process., Vol. 15, No. 3, pp. 982-994, 2007. References 2. Motivation 3. Generative Model	benchmark (computing);cluster analysis;empirical methods in natural language processing;generative model;goto;international conference on acoustics, speech, and signal processing;international conference on machine learning;k-svd;monoid factorisation;non-negative matrix factorization;singular value decomposition;spectrogram;stochastic context-free grammar;transcription (software);tree structure	Hirokazu Kameoka;Kazuki Ochiai;Masahiro Nakano;Masato Tsuchiya;Shigeki Sagayama	2012			generative grammar;speech recognition;machine learning;artificial intelligence;tree structure;musical notation;generative model;fundamental frequency;spectrogram;mathematics;rhythm;bayesian inference	ML	-19.39960195593614	-95.39852777088227	75805
029e683fcf4d6d574bcfe81646d18c468ba3c55a	linguistic features and sociolinguistic variability in everyday spoken russian		The paper reviews the results of the project aimed at describing everyday Russian language and analyzing the special characteristics of its usage by different social groups. The presented study was made on the material of 125,000 words annotated subcorpus of the ORD corpus, which contains speech fragments of 256 people representing different gender, age, professional and status groups. The linguistic features from different linguistic levels, which could be considered as diagnostic for different social groups, have been analyzed. It turned out that in terms of sociolinguistic variability all features under investigation may be divided into three categories: (1) the diagnostic features, which display statistically significant differences between certain social groups; (2) the linguistic features, which could be considered as common for all sociolects and referring to some permanent, universal properties of everyday language; and (3) the potentially diagnostic features, which have shown some quantitative difference between the considered groups, but the extent of this difference does not allow to regard them as statistically significant at the moment. The last group of features is the most extensive and requires additional studies on a larger amount of speech data.	heart rate variability	Natalia Bogdanova-Beglarian;Tatiana Y. Sherstinova;Olga Blinova;Gregory Y. Martynenko	2017		10.1007/978-3-319-66429-3_50	morphology (linguistics);syntax;social group;sociolinguistics;phonetics;linguistics;computer science;vocabulary	NLP	-13.677977245081648	-80.90065806828204	75923
f2f3c5c916de9c678e51cc4ca2a391d5a8592df0	an overview of decoding techniques for large vocabulary continuous speech recognition	vocabulaire;traitement automatique de la parole;decodage;decoding;large vocabulary continuous speech recognition;etude experimentale;search space;speech processing;heuristic method;vocabulary;search algorithm;linguistique appliquee;time synchronization;methode;algorithme;probabilistic model;algorithm;modele de langage;weighted finite state transducer;reconnaissance de la parole;modele probabiliste;speech recognition;computational linguistics;chaine de markov;linguistique informatique;method;theorie de bayes;language model;applied linguistics;algoritmo	A number of decoding strategies for large vocabulary continuous speech recognition (LVCSR) are examined from the viewpoint of their search space representation. Different design solutions are compared with respect to the integration of linguistic and acoustic constraints, as implied by m-gram language models (LM) and cross-word (CW) phonetic contexts. This study is structured along two main axes: the network expansion and the search algorithm itself. The network can be expanded statically or dynamically while the search can proceed either time-synchronously or asynchronously which leads to distinct architectures. Three broad classes of decoding methods are briefly reviewed: the use of weighted finite state transducers (WFST) for static network expansion, the time-synchronous dynamic-expansion search and the asynchronous stack decoding. Heuristic methods for further reducing the search space are also considered. The main approaches are compared and some prospective views are formulated regarding possible future avenues. c © 2002 Academic Press	acoustic cryptanalysis;decoding methods;finite-state transducer;heuristic;language model;prospective search;search algorithm;speech analytics;speech recognition;vocabulary	Xavier L. Aubert	2002	Computer Speech & Language	10.1006/csla.2001.0185	natural language processing;statistical model;method;speech recognition;computer science;computational linguistics;applied linguistics;speech processing;linguistics;language model;search algorithm	NLP	-24.312611013711944	-81.79857042699176	76002
bdf484d79bffba05cfa78e6d296139c31587b8e7	the munich biovoice corpus: effects of physical exercising, heart rate, and skin conductance on human speech production		We introduce a spoken language resource for the analysis of impact that physical exercising has on human speech production. In particular, the database provides heart rate and skin conductance measurement information alongside the audio recordings. It contains recordings from 19 subjects in a relaxed state and after exercising. The audio material includes breathing, sustained vowels, and read text. Further, we describe pre-extracted audio-features from our openSMILE feature extractor together with baseline performances for the recognition of high and low heart rate using these features. The baseline results clearly show the feasibility of automatic estimation of heart rate from the human voice, in particular from sustained vowels. Both regression in order to predict the exact heart rate value and a binary classification setting for high and low heart rate classes are investigated. Finally, we give tendencies on feature group relevance in the named contexts of heart rate estimation and skin conductivity estimation.	baseline (configuration management);binary classification;conductance (graph);performance;randomness extractor;relevance;skin (computing)	Björn W. Schuller;Felix Friedmann;Florian Eyben	2014			speech recognition	HCI	-11.953303810755507	-85.02735232453982	76133
3ac3d709475239a25837fee46657a2a8f7a53b6b	automatic emphasis labeling for emotional speech by measuring prosody generation error	classification and regression tree;generalization error;emotion emphasis;speech synthesis;prosody generation error;maximum entropy	Emotion helps human to express their feelings and intentions clearly. And the emphasis labels of speeches are the key of speech emotion analysis and synthesis. In order to label the emotion emphasis of speech samples from a corpus with only phonetic and prosodic information, this paper introduces an automatic labeling algorithm by measuring the prosody generation error (PGE) of the result from a statistical synthesizer. Classification and Regression Tree (CART) and Maximum Entropy (ME) modeling are adopted for automatically labeling. Experiment shows that both models are helpful for labeling.	semantic prosody	Jun Xu;Lianhong Cai	2009		10.1007/978-3-642-04070-2_20	natural language processing;speech recognition;computer science;principle of maximum entropy;machine learning;speech synthesis;statistics;generalization error	NLP	-16.95460962752014	-84.19314048934115	76311
45099a4db6d8d71590ae37f6b629e40c5ac2c833	the 'neural' phonetic typewriter	biological models;engineering;speaker adaptive system;processing 990210 supercomputers 1987 1989;general and miscellaneous mathematics computing and information science;neural networks;speech synthesis;speech recognition vocabulary telephony optical films automatic speech recognition speech synthesis neural networks pattern recognition text recognition natural languages;neural nets;hidden markov model;vocabulary;data processing;speech;natural languages;telephony;automatic speech recognition;vector quantization;neural network processor;neural computers;shortcut learning algorithm;pattern recognition;speech analysis and processing;acoustic preprocessing;speech recognition;artificial intelligence;text recognition;dynamic time warping;phonotopic maps;symbolic forms;speech recognition artificial intelligence neural nets speech analysis and processing;optical films;artificial neural network;neural network;symbolic forms artificial intelligence speech recognition neural computers speaker adaptive system neural network processor acoustic preprocessing vector quantization shortcut learning algorithm phonotopic maps;human factors engineering	The factors that make speech recognition difficult are examined, and the potential of neural computers for this purpose is discussed. A speaker-adaptive system that transcribes dictation using an unlimited vocabulary is presented that is based on a neural network processor for the recognition of phonetic units of speech. The acoustic preprocessing, vector quantization, neural network model, and shortcut learning algorithm used are described. The utilization of phonotopic maps and of postprocessing in symbolic forms are discussed. Hardware implementations and performance of the neural networks are considered.<<ETX>>	acoustic cryptanalysis;adaptive system;algorithm;artificial neural network;keyboard shortcut;map;network model;network processor;preprocessor;speech recognition;vector quantization;vocabulary;wetware computer	Teuvo Kohonen	1988	Computer	10.1109/2.28	neural gas;natural language processing;speech recognition;computer science;speech;machine learning;dynamic time warping;time delay neural network;natural language;telephony;vector quantization;artificial neural network	ML	-17.838233024900227	-86.50305932786611	76312
205018047e28a7f0d3145062277623425e2f5cb5	atts2s-vc: sequence-to-sequence voice conversion with attention and context preservation mechanisms		This paper describes a method based on a sequenceto-sequence learning (Seq2Seq) with attention and context preservation mechanism for voice conversion (VC) tasks. Seq2Seq has been outstanding at numerous tasks involving sequence modeling such as speech synthesis and recognition, machine translation, and image captioning. In contrast to current VC techniques, our method 1) stabilizes and accelerates the training procedure by considering guided attention and proposed context preservation losses, 2) allows not only spectral envelopes but also fundamental frequency contours and durations of speech to be converted, 3) requires no context information such as phoneme labels, and 4) requires no time-aligned source and target speech data in advance. In our experiment, the proposed VC framework can be trained in only one day, using only one GPU of an NVIDIA Tesla K80, while the quality of the synthesized speech is higher than that of speech converted by Gaussian mixture model-based VC and is comparable to that of speech generated by recurrent neural network-based text-to-speech synthesis, which can be regarded as an upper limit on VC performance.	artificial neural network;graphics processing unit;machine translation;mixture model;nvidia tesla;recurrent neural network;speech synthesis	Kou Tanaka;Hirokazu Kameoka;Takuhiro Kaneko;Nobukatsu Hojo	2018	CoRR		mixture model;machine translation;artificial intelligence;fundamental frequency;recurrent neural network;computer science;closed captioning;pattern recognition;speech synthesis	ML	-17.6438255004226	-87.8177697513788	76371
65c92c30f63053f0c83c90d6c33e3dd543c0e9a3	a multimodal density function estimation approach to formant tracking.	density functional	We address the problem of robust formant tracking in continuous speech. We propose the robust statistical model of t-distribution mixture density (tMM) operating on the “pyknogram” obtained through a multiband AM-FM demodulation technique. The statistical model of the pyknogram is shown to be more-effective to handle the variability in the signal processing stage. The t-mixture density estimation is shown to be more effective than Gaussian mixture density because of outlier data in the pyknogram. For formant tracking, we show that the tMM is better in terms of parameter selection, accuracy, and smoothness of the estimate. We present experimental results on simulated data, real speech sentences, and test the robustness of the proposed MDA-tMM method to additive noise. Comparisons with PRAAT software and a recentlydeveloped adaptive filterbank technique show that the proposed MDAtMM method is superior in several aspects.	additive white gaussian noise;fm broadcasting;filter bank;kernel density estimation;multimodal interaction;praat;signal processing;spatial variability;statistical model;utility functions on indivisible goods	Sundar Harshavardhan;Chandra Sekhar Seelamantula;Thippur V. Sreenivas	2010			computer vision;speech recognition;computer science	ML	-13.54871521288211	-93.33561203324435	76385
458ad8222f215e2625cfd5222ded76db79bc5e08	multipitch tracking using a factorial hidden markov model		In this paper, we present an approach to track the pitch of two simultaneous speakers. Using a well-known feature extraction method based on the correlogram, we track the resulting data using a factorial hidden Markov model (FHMM). In contrast to the recently developed multipitch determination algorithm [1], which is based on a HMM, we can accurately associate estimated pitch points with their corresponding source speakers. We evalute our approach on the “Mocha-TIMIT” database [2] of speech utterances mixed at 0dB, and compare the results to the multipitch determination algorithm [1] used as a baseline. Experiments show that our FHMM tracker yields good performance for both pitch estimation and correct speaker assignment.	baseline (configuration management);bittorrent tracker;experiment;feature extraction;hidden markov model;markov chain;mocha;pitch detection algorithm;timit	Michael Wohlmayr;Franz Pernkopf	2008			speech recognition;artificial intelligence;factorial;pattern recognition;correlogram;hidden markov model;feature extraction;computer science	NLP	-12.108528441242235	-92.46306931247148	76410
d0a5a8f9c14144c897b7c9caa53021d6936eef3e	sparse kernel cepstral coefficients (skcc): inner-product based features for noise-robust speech recognition	kernel;pre computed matrix;skcc;hilbert spaces;speech recognition cepstral analysis feature extraction hilbert spaces regression analysis sparse matrices speech coding;regression techniques;inner product;speech;feature extraction speech speech recognition mel frequency cepstral coefficient robustness noise kernel;speech coding;noise robustness;mel frequency cepstral coefficient;cepstral analysis;normalized gamma tone basis functions;feature extraction;reproducing kernel hilbert space;speech feature extraction;sparse kernel cepstral coefficients;speech recognition;robustness;regression analysis;sparse matrices;noise;pre computed matrix sparse kernel cepstral coefficients speech recognition speech feature extraction sparse auditory coding regression techniques reproducing kernel hilbert space normalized gamma tone basis functions noise robustness skcc;sparse auditory coding	In this paper we present a novel speech feature extraction algorithm based on sparse auditory coding and regression techniques in a reproducing kernel Hilbert space (RKHS). The features known as sparse kernel cepstral coefficients (SKCC) are extracted under the hypothesis that the noise-robust information in speech signal is embedded in a subspace spanned by overcomplete, regularized and normalized gamma-tone basis functions. After identifying the information bearing subspace, noise-robustness is achieved by sparsifying the SKCC features using simple thresholding. We show that computing the SKCC features involves correlating the speech signal with a pre-computed matrix, thus making the algorithm amenable to DSP based implementation. Speech recognition experiments using AURORA 2 dataset demonstrate that the SKCC features delivers consistent improvements in recognition performance over the state-of-the-art features under different noisy recording conditions.	additive white gaussian noise;algorithm;baseline (configuration management);basis function;cepstrum;coefficient;computation;embedded system;experiment;feature extraction;hilbert space;kernel (operating system);precomputation;real-time clock;sparse matrix;speech corpus;speech recognition;thresholding (image processing);utility functions on indivisible goods	Amin Fazel;Shantanu Chakrabartty	2011	2011 IEEE International Symposium of Circuits and Systems (ISCAS)	10.1109/ISCAS.2011.5938087	kernel;speech recognition;sparse matrix;dot product;feature extraction;computer science;noise;speech;machine learning;speech coding;pattern recognition;reproducing kernel hilbert space;mathematics;regression analysis;robustness;hilbert space	Vision	-13.909402508795653	-92.59615374853972	76441
5d47d43d6b6ef7e7ad9c6d290c496792964b943f	locating boundaries for prosodic constituents in unrestricted mandarin texts		This paper proposes a three-tier prosodic hierarchy, including prosodic word, intermediate phrase and intonational phrase tiers, for Mandarin that emphasizes the use of the prosodic word instead of the lexical word as the basic prosodic unit. Both the surface difference and perceptual difference show that this is helpful for achieving high naturalness in text-to-speech conversion. Three approaches, the basic CART approach, the bottom-up hierarchical approach and the modified hierarchical approach, are presented for locating the boundaries of three prosodic constituents in unrestricted Mandarin texts. Two sets of features are used in the basic CART method: one contains syntactic phrasal information and the other does not. The one with syntactic phrasal information results in about a 1% increase in accuracy and an 11% decrease in error-cost. The performance of the modified hierarchical method produces the highest accuracy, 83%, and lowest error cost when no syntactic phrasal information is provided. It shows advantages in detecting the boundaries of intonational phrases at locations without breaking punctuation. 71.1% precision and 52.4% recall are achieved. Experiments on acceptability reveal that only 26% of the mis-assigned break indices are real infelicitous errors, and that the perceptual difference between the automatically assigned break indices and the manually annotated break indices are small.	algorithm;bottom-up parsing;experiment;goto;inp (database);limewire;multitier architecture;rom cartridge;semantic prosody;sensor;speech synthesis;super robot monkey team hyperforce go!	Min Chu;Yao Qian	2001	IJCLCLP		syntax;mandarin chinese;prosodic unit;naturalness;hierarchy;punctuation;speech recognition;recall;art;phrase	NLP	-20.51740031146775	-81.7631787602884	76480
7d71f79bc16146b6492ba3cf2da1438f2c68fdf5	typetalker: a speech synthesis-based multi-modal commenting system	transcription error;automatic speech recognition;multi modal comment;speech comments;transcript based speech editing;self consciousness	Speech commenting systems have been shown to facilitate asynchronous online communication from educational discussion to writing feedback. However, the production of speech comments introduces several challenges to users, including overcoming self-consciousness and time consuming editing. In this paper, we introduce TypeTalker, a speech commenting interface that presents speech as a synthesized generic voice to reduce speaker self-consciousness, while retaining the expressivity of the original speech with natural breaks and co-expressive gestures. TypeTalker streamlines speech editing through a simple textbox that respects temporal alignment across edits. A comparative evaluation shows that TypeTalker reduces speech anxiety during live-recording, and offers easier and more effective speech editing facilities than the previous state-of-the-art interface technique. A follow-up study on recipient perceptions of the produced comments suggests that while TypeTalker's generic voice may be traded-off with a loss of personal touch, it can also enhance the clarity of speech by refining the original speech's speed and accent.	computer-mediated communication;self-consciousness;speech synthesis	Ian Arawjo;Dongwook Yoon;François Guimbretière	2017		10.1145/2998181.2998260	voice activity detection;natural language processing;audio mining;speech recognition;speech corpus;computer science;motor theory of speech perception;self-consciousness;communication;speech synthesis;speech analytics	HCI	-25.764327635535903	-86.10731860203258	76501
0611f79daf625d90c907414628e80ba7061074dd	a system for natural language sentence generation	lenguaje natural;search result;linguistica aplicada;implementation;generacion lenguaje;sistema informatico;langage naturel;text analysis;higher education;produit recherche;computer system;linguistique appliquee;educational theories;language research;sentence structure grammar;attribute grammar;resultado busqueda;second language acquisition;ejecucion;grammaire a structure phrase;gramatica a estructura frase;computer assisted instruction;operating system;gramatica por atributo;natural language;grammaire par attribut;natural language generation;systeme informatique;language learning;generation langage;second language learning;sentences;models;applied linguistics;language generation;linguistics	This paper describes a natural language generation system known as VINCI, which accepts as input a formal description of some subset of a natural language, and generates strings in the language. With the help of an attribute grammar formalism, the system can be used to simulate on a computer components of several current linguistic theories. The program, implemented in C, runs under a variety of operating systems, including UNIX, MS-DOS and VM/CMS. In this paper we consider not only the design of the system, but also some of its applications in linguistic modelling and second language acquisition research.		Michael Levison;Gregory Lessard	1992	Computers and the Humanities	10.1007/BF00114887	natural language processing;language identification;natural language programming;speech recognition;picture language;universal networking language;object language;specification language;natural language user interface;second-language acquisition;computer science;applied linguistics;linguistics;education theory;natural language;attribute grammar;higher education;language technology;implementation;context-sensitive language	NLP	-28.789734002750563	-81.45910710282729	76504
16248227d7a7eac9209e2b6b89003bbb2f384af2	a study of trumpet envelopes		Abstract: Most synthesis techniques provide for some amount of parametric control. Generating suitable controls is a difficult problem, especially for instruments that admit continuous control by the performer. The traditional approach to control generation in computer music has been note-based, but note-by-note synthesis tends to overlook the interaction between notes in a phrase. This study considers factors, including melodic contour, articulation, and dynamics, that affect the shape of amplitude envelopes in trumpet performance. After showing statistically significant variation due to these factors, a model for trumpet envelopes is described. This model is used with Spectral Interpolation Synthesis to synthesize realistic trumpet performances.	biconnected component;computer simulation;interpolation;numerical weather prediction;performance;simple features;speech synthesis	Roger B. Dannenberg;Hank Pellerin;Istvan Derenyi	1998			melody;interpolation;acoustics;amplitude;phrase;computer music;parametric statistics;mathematics	Graphics	-9.36623874815501	-85.8384029160165	76522
6d7333befe65942a1691333fb8f72a98da85b1cf	harmonic-based robust voice activity detection for enhanced low snr noisy speech recognition system			signal-to-noise ratio;speech recognition;voice activity detection	Po-Yi Shih;Po-Chuan Lin;Jhing-Fa Wang	2016	IEICE Transactions		voice activity detection;speech recognition;pattern recognition	Vision	-13.693354300978731	-90.68704410524374	76565
869dcf9937f7266ff49e6efcfd632bf218991050	resampling auxiliary data for language model adaptation in machine translation for speech	domain adaptation;interpolation;support vector machines;language modeling community;domain adaptation language model adaptation machine translation;auxiliary data resampling;speech processing;training;biological system modeling;language translation;language modeling community auxiliary data resampling language model adaptation machine translation n gram language models speech to speech translation system;speech;speech coding;materials testing;natural languages;indexing terms;materials;speech to speech translation system;adaptation model;natural languages adaptation model entropy speech coding materials testing system testing performance gain text categorization support vector machines support vector machine classification;language model adaptation;n gram language models;performance gain;system testing;support vector machine classification;entropy;speech processing language translation;text categorization;language model;machine translation;data models	Performance of n-gram language models depends to a large extent on the amount of training text material available for building the models and the degree to which this text matches the domain of interest. The language modeling community is showing a growing interest in using large collections of auxiliary textual material to supplement sparse in-domain resources. One of the problems in using such auxiliary corpora is that they may differ significantly from the specific nature of the domain of interest. In this paper, we propose three different methods for adapting language models for a Speech to Speech (S2S) translation system when auxiliary corpora are of different genre and domain. The proposed methods are based on centroid similarity, n-gram ratios and resampled language models. We show how these methods can be used to select out of domain textual data such as newswire text to improve a S2S system. We were able to achieve an overall relative improvement of 3.8% in BLEU score over a baseline system that uses only in-domain conversational data.	bleu;baseline (configuration management);language model;machine translation;n-gram;sparse matrix;text corpus	Sameer Maskey;Abhinav Sethy	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4960709	natural language processing;data modeling;support vector machine;entropy;speech recognition;index term;interpolation;computer science;speech;machine learning;speech coding;pattern recognition;speech processing;machine translation;natural language;system testing;language model	NLP	-20.579218671312006	-86.92280251230775	76569
7aa33c994b8213ab43b09e52c9f9551712292283	intensity- and location-normalized training for hmm-based visual speech recognition		This paper describes an approach to estimating the parameters of continuous density HMMs for visual speech recognition. One of the key issues of image-based visual speech recognition is normalization of lip location and lighting condition prior to estimating the parameters of HMMs. We present an average-intensity and location normalized training method, in which the normalization process is integrated in the model training. The proposed method provides a theoretically-well-defined algorithm based on a maximum likelihood formulation, hence the likelihood for the training data is guaranteed to increase at each iteration of the normalized training. Experimental results show that the recognition performance can be significantly improved by the normalized training.	algorithm;estimation theory;hidden markov model;image scaling;iteration;speech recognition;teaching method;word error rate	Yoshihiko Nankaku;Keiichi Tokuda;Tadashi Kitamura	1999			artificial intelligence;speech recognition;maximum likelihood;speaker recognition;normalization (statistics);pattern recognition;hidden markov model;training set;computer science	ML	-18.585031669972327	-92.51114286744968	76601
bd8528e95d5a273f700d7c9c221194ac5bc39949	a hmm-based system for automatic segmentation and labeling of speech			hidden markov model	Fabio Brugnara;Daniele Falavigna;Maurizio Omologo	1992			artificial intelligence;scale-space segmentation;pattern recognition;hidden markov model;segmentation-based object categorization;computer science;segmentation	NLP	-14.939539471049649	-87.03781417643859	76646
dd8a3b42d0b0785a4f30e096c8c3959ba1520d3d	a big data approach to acoustic model training corpus selection		Deep neural networks (DNNs) have recently become the state of the art technology in speech recognition systems. In this paper we propose a new approach to constructing large high quality unsupervised sets to train DNN models for large vocabulary speech recognition. The core of our technique consists of two steps. We first redecode speech logged by our production recognizer with a very accurate (and hence too slow for real-time usage) set of speech models to improve the quality of ground truth transcripts used for training alignments. Using confidence scores, transcript length and transcript flattening heuristics designed to cull salient utterances from three decades of speech per language, we then carefully select training data sets consisting of up to 15K hours of speech to be used to train acoustic models without any reliance on manual transcription. We show that this approach yields models with approximately 18K context dependent states that achieve 10% relative improvement in large vocabulary dictation and voice-search systems for Brazilian Portuguese, French, Italian and Russian languages.	acoustic cryptanalysis;acoustic model;artificial neural network;big data;display resolution;finite-state machine;ground truth;heuristic (computer science);real-time clock;speech recognition;transcription (software);vocabulary	Olga Kapralova;John Alex;Eugene Weinstein;Pedro J. Moreno;Olivier Siohan	2014			natural language processing;speech recognition;pattern recognition	NLP	-20.46151637559135	-84.54098944474481	76670
e7c0fbd32190e7dd5346987bd314672dffb26a5a	production of face and goat by slovak and czech immigrants in edinburgh	bilingualism;variation;vowels;scotland;linguistics	This study examines the stylistic constraints on the pronunciation of the FACE and GOAT lexical sets as spoken by Slovak and Czech female immigrants who permanently reside in Edinburgh, Scotland. We undertake an acoustic analysis of monosyllablic words taken from a structured interview, a reading passage, and a wordlist to compare these speakers to fluent learners of RP English living in Slovakia, specifically investigating immigrants’ acquisition of the Scottish English monophthongal variant. The results suggest that long-term immigration has a significant impact on pronunciation patterns, although more formal speech styles may trigger a reversion to instructed L2 norms.	acoustic cryptanalysis;rp (complexity);reversion (software development)	Zuzana Elliott;Lauren Hall-Lew	2015			humanities;classics;art;linguistics	NLP	-11.378585545653324	-81.37422205092604	76676
463d3fb236191329434615a6ef5b1b051afab594	automatic lip synchronization by speech signal analysis	signal analysis	In this paper a system for the automatic lip synchronization of virtual 3D human based only on the speech input is described. The speech signal is classified into viseme classes using neural networks. Visual representation of phonemes, visemes, defined in MPEG-4 FA, is used for face synthesis.	artificial neural network;signal processing	Goranka Zoric;Aleksandra Cerekovic;Igor S. Pandzic	2008			speech recognition;artificial intelligence;pattern recognition;synchronization;signal processing;computer science;speech processing	ML	-13.870934784757926	-87.06345742275052	77091
7f7d1cd08509f44c5a8bd77c316e445595cd8753	distant speech recognition in reverberant noisy conditions employing a microphone array	array signal processing;belief networks;convex programming;microphone arrays;signal detection;speaker recognition;speech enhancement;convex optimized beamforming;deep belief networks;distant speech recognition;home environment;microphone array;position-pitch plane;reverberant noisy conditions;robust asr;speaker localization;speech enhancement;vector taylor series compensation;voice activity detector;german database;popi speaker localization;convexoptimized beamforming;deep belief network voice activity detection;distant speech recognition;natural mixing;reverberant and noisy environment;vector taylor series compensation	This paper addresses the problem of distant speech recognition in reverberant noisy conditions employing a microphone array. We present a prototype system that can segment the utterances in real-time and generate robust ASR results off-line. The segmentation is carried out by a voice activity detector based on deep belief networks, the speaker localization by a position-pitch plane, and the enhancement by a novel combination of convex optimized beamforming and vector Taylor series compensation. All of the components are compared with other similar ones and justified in terms of word accuracy on a proposed database which simulates distant speech recognition in a home environment.	bayesian network;beamforming;deep belief network;microphone;online and offline;prototype;real-time clock;speech recognition	Juan Andres Morales-Cordovilla;Martin Hagmüller;Hannes Pessentheiner;Gernot Kubin	2014	2014 22nd European Signal Processing Conference (EUSIPCO)		voice activity detection;speech recognition;acoustics;engineering;speech processing;communication	Robotics	-14.095995210853804	-92.90410811028359	77177
02f14dd2406379e04994e5f0e902c97bb574ef12	incremental adaptation of speech-to-speech translation	speech recognition;speech system;speech-to-speech translation system;speech-to-speech system;darpa transtac program;original training data;incremental adaptation;machine translation;overall system performance;cmu iraqi-english portable two-way;practical two-way speech-to-speech translation	In building practical two-way speech-to-speech translation systems the end user will always wish to use the system in an environment different from the original training data. As with all speech systems, it is important to allow the system to adapt to the actual usage situations. This paper investigates how a speech-to-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two. The platform is the CMU Iraqi-English portable two-way speechto-speech system as developed under the DARPA TransTac program. We show how machine translation, speech recognition and overall system performance can be improved on day 2 after adapting from day 1 in both a supervised and unsupervised way.	day one;inferring horizontal gene transfer;machine translation;speech recognition	Nguyen Bach;Roger Hsiao;Matthias Eck;Paisarn Charoenpornsawat;Stephan Vogel;Tanja Schultz;Ian R. Lane;Alexander H. Waibel;Alan W. Black	2009			natural language processing;speech recognition;computer science	NLP	-21.597951983549738	-85.51089535086284	77210
8aed75929bf2fa49287aa4dacdd9e49442678e96	nonparametric hidden markov models: principles and applications to speech recognition	modelo markov oculto;densite probabilite;gaussian mixture;algoritmo busqueda;analisis estadistico;probability density;melange loi probabilite;maximum likelihood;modele markov cache;proceso markov;activation function;hidden markov model;algorithme recherche;probability density function;modele markov variable cachee;search algorithm;maximum vraisemblance;bioinformatique;mixed distribution;methode indirecte;probabilistic approach;densidad probabilidad;statistical properties;automatic speech recognition;automatic recognition;reconnaissance caractere;hidden markov models;neural net;reconocimiento voz;statistical analysis;enfoque probabilista;approche probabiliste;processus markov;robustesse;metodo indirecto;analyse statistique;markov process;speech recognition;mezcla ley probabilidad;robustness;reconnaissance parole;bioinformatica;indirect method;continuous density hidden markov model;character recognition;maxima verosimilitud;training algorithm;reconocimiento caracter;reconocimiento automatico;reconnaissance automatique;robustez;bioinformatics	An Algorithm to Model Paradigm Shifting in Fuzzy Clustering p. 70 ANFIS Synthesis by Hyperplane Clustering for Time Series Prediction p. 77 Generalized Splitting 2D Flexible Activation Function p. 85 Face Localization with Recursive Neural Networks p. 99 Multi-class Image Coding via EM-KLT Algorithm p. 106 A Face Detection System Based on Color and Support Vector Machines p. 113 A Neural Architecture for 3D Segmentation p. 121 Automatic Polyphonic Piano Music Transcription by a Multi-classification Discriminative-Learning p. 129	activation function;adaptive neuro fuzzy inference system;algorithm;artificial neural network;face detection;fuzzy clustering;hidden markov model;markov chain;medical transcription;recursion (computer science);speech recognition;support vector machine;time series	Edmondo Trentin	2003		10.1007/978-3-540-45216-4_1	probability density function;speech recognition;computer science;machine learning;pattern recognition;mathematics;hidden markov model;statistics	ML	-13.087976987144721	-97.19130523578015	77253
dcd06d90187da3140f7f0ce2d086df76c9976e33	automated metrics for speech translation	automated translation metrics;arabic language;speech translation;speech translation evaluation;machine translation	In this paper, we describe automated measures used to evaluate machine translation quality in the Defense Advanced Research Projects Agency's Spoken Language Communication and Translation System for Tactical Use program, which is developing speech translation systems for dialogue between English and Iraqi Arabic speakers in military contexts. Limitations of the automated measures are illustrated along with variants of the measures that seek to overcome those limitations. Both the dialogue structure of the data and the Iraqi Arabic language challenge these measures, and the paper presents some solutions adopted by MITRE and NIST to improve confidence in the scores.	machine translation	Sherri L. Condon;Mark Arehart;Christy Doran;Dan Parvaz;John S. Aberdeen;Karine Megerdoomian;Beatrice T. Oshika	2009		10.1145/1865909.1865959	rouge;computer-assisted translation;natural language processing;speech recognition;example-based machine translation;computer science;evaluation of machine translation;linguistics;machine translation;rule-based machine translation;machine translation software usability	NLP	-24.183030939268612	-83.0782044470839	77332
3d6ec8c3b8d23dbca09684637fde4bd2b81b1cba	automatic stylization of intonation: application to speech synthesis			speech synthesis	Christophe d'Alessandro;Piet Mertens;Frédéric Beaugendre	1994			speech recognition;prosody;speech synthesis;computer science	NLP	-15.055894824401197	-85.46808573038383	77342
ed8acc811a41e0018f6ec9d1432e77a8881ac343	an automatic timing detection method for superimposing closed captions of tv programs.		This paper describes a way of automatically detecting the timing for the superimposition of closed captions on TV programs that use electronic manuscripts. Speech is discriminated from music to decrease the number of false alarms and the amount of speech data within which a search must be carried out. Timing is then detected serially from the first sentence to the last sentence of a program by using a phonetically HMM-based word spotter. After all timing has been determined, it is checked for errors and revised. Detection rates of 96.9% and 98.5% were obtained for allowable timing errors of one and three seconds on the aired documentary programs.	hidden markov model;sensor	Ichiro Maruyama;Yoshiharu Abe;Terumasa Ehara;Katsuhiko Shirai	2000			computer vision;real-time computing;computer hardware	OS	-26.334477646566114	-82.30961491237403	77361
9c15128f1a41ad8ff61a63863347069aa07f9cf8	autoregressive time-frequency interpolation in the context of missing data theory for impulsive noise compensation	ar model;operant conditioning;time frequency;missing data;prediction error;impulse noise;word recognition;automatic speech recognition;spectrum;time series;auto regressive;filter bank	The present paper reports on a novel technique for the identification and replacement of spectral coefficients degraded by impulsive noise. The problem is viewed from the perspective of Missing Feature Theory (MFT). The analysis is carried out in the linear spectrum prior to, or after applying the mel-scale filter-bank depending on whether one aims at improving the quality of perception of speech recordings or at Automatic Speech Recognition (ASR). Each filter-bank output is considered to be a time series drawn from an AutoRegressive process (AR). A validation corpus of undistorted recordings is used to derive a-priori bounds on the expected prediction error of each AR model. In operational conditions, the prediction procedure is monitored and the violation of the statistical bounds indicates band corruption and entails the substitution of the degraded spectral coefficients by the prediction of the corresponding AR model. ASR experiments and informal listening tests demonstrate large improvement in terms of word recognition performance and Itakura-Saito divergence at very low Signal to Impulsive Noise Ratios (SINRs). Data, and implementation code can be found at: ftp://wcl.ee.upatras.gr/	autoregressive model;coefficient;experiment;filter bank;interpolation;itakura–saito distance;media foundation;mel scale;missing data;speech recognition;time series	Ilyas Potamitis;Nikos Fakotakis	2001			speech recognition;interpolation;knuckle;artificial intelligence;missing data;pattern recognition;mean squared prediction error;autoregressive model;shock absorber;computer science;suspension (vehicle);time–frequency analysis	ML	-13.149368671966759	-93.38666188550258	77365
ca987e7bc1e6d3d8c284640beffe8f780ba36268	comparison between automatic methods and human listeners in speaker recognition tasks	speaker recognition		speaker recognition	Antonella Federico;G. Ibba;Andrea Paoloni;N. De Sario;B. Saverione	1989			artificial intelligence;speech recognition;speaker diarisation;pattern recognition;speaker recognition;computer science	Vision	-14.216506190556808	-87.53167212792097	77495
703e3500400010430940d0fa895e5937aa623752	effective and efficient sports highlights extraction using the minimum description length criterion in selecting gmm structures	audio classification;gaussian mixture model;minimum description length;pattern recognition	In fitting the training data with GMMs of appropriate structures using the MDL criterion, we are able to improve audio classification accuracy with a large margin. With the MDLGMMs, we are also able to greatly improve the accuracy in extracting sports highlights. Since we have focused on audio domain processing, it enables us to extract highlights very fast. In this paper, we have demonstrated the importance of a better understanding of model structures in such a pattern recognition task.	google map maker;mdl (programming language);minimum description length;pattern recognition	Ziyou Xiong;Regunathan Radhakrishnan;Ajay Divakaran;Thomas S. Huang	2004			speech recognition;minimum description length;computer science;machine learning;pattern recognition;mixture model;statistics	Vision	-17.40220564901713	-90.81756778732515	77539
deb785be55ae6a42fbc1739c2da5c784620f45d8	using denoising autoencoder for emotion recognition		In this paper, we propose to use the denoising autoencoder to generate robust feature representations for emotion recognition. In our method, the input of the denoising autoencoder is the normalized static feature set (state-of-the-art features for emotion recognition). This input is mapped to two hidden representations: one is to capture the neutral information from the input, and the other one is used to extract emotional information. Model parameters are learned by minimizing the squared error between the original and the reconstructed input. After pre-training and fine-tuning, we use the hidden representation as features in the SVM model for emotion classification. Our experimental results show significant performance improvement compared to using the static features.	autoencoder;emotion recognition;noise reduction	Rui Xia;Yang Liu	2013			speech recognition;pattern recognition;artificial intelligence;autoencoder;noise reduction;emotion recognition;computer science	NLP	-16.39948265707615	-89.09447861533879	77600
9fc420baf988ff52bf557aeb2a9c6147484bfbc9	loudness measurement of human utterance to a robot in noisy environment	level 2;sound localization;mobile robot;mobile robots;human robot interaction;sound pressure level;speaker recognition;distance measurement;speaker recognition distance measurement human robot interaction microphone arrays mobile robots;voice recognition;microphone array;voice recognition system loudness measurement human utterance noisy environment human robot interation noise level robot distance synchronized sound source auditory scene recorded noiseless utterance sound pressure level background noise language pronounce online spoken command recognition system mobile robot low side lobe microphone array omini directional telescopic microphone dsbf fbs method sound source localization sound source segmentation caller location sound stream segmentation;sound source localization;humans robots microphones arrays noise level noise measurement noise;microphone arrays;human utterance	In order to understand utterance based human-robot interation, and to develop such a system, this paper initially analyzes how loud humans speak in a noisy environment. Experiments were conducted to measure  how loud humans speak with 1) different noise levels, 2) different number of sound sources, 3) different sound sources, and 4) different distances to a robot. Synchronized sound sources add noise to the auditory scene, and resultant utterances are recorded and compared to a previously recorded noiseless utterance. From experiments, we understand that humans generate basically the same level of sound pressure level at his/her location irrespective of distance and background noise. More precisely, there is a band according to a distance, and also according to sound sources that is including  language pronounce.  According to this understanding, we developed an online spoken command recognition system for a mobile robot. System consists of two key componenets: 1) Low side-lobe microphone array that works as omini-directional telescopic microphone, and 2) DSBF combined with FBS  method for sound source localization and segmentation. Caller location and segmented sound stream are calculated, and then the segmented sound stream is sent to voice recognition system. The system works with at most five sound sources at the same time with about at most  18[dB] sound pressure differences. Experimental results with the modile robot are also shown.	acoustic lobing;covox speech thing;experiment;function-behaviour-structure ontology;image noise;microphone;mobile robot;resultant;speech recognition	Satoshi Kagami;Yoko Sasaki;Simon Thompson;Tomoaki Fujihara;Tadashi Enomoto;Hiroshi Mizoguchi	2008	2008 3rd ACM/IEEE International Conference on Human-Robot Interaction (HRI)	10.1145/1349822.1349851	precedence effect;human–robot interaction;mobile robot;speaker recognition;speech recognition;critical distance;computer science;artificial intelligence	Robotics	-7.114337353820453	-87.6527825889832	77738
d70bf7aca189d8b8b5850c8f63dc551dc19881cc	speech emotion recognition using cross-correlation and acoustic features		Speech emotion recognition is a trending research topic these days, with its main motive to improve the human-machine interaction. At present, most of the work in this area utilizes extraction of discriminatory features for the purpose of classification of emotions into various categories. Most of the present work involves use of only Mel Frequency Cepstral Coefficients (MFCCs) as an integral feature for emotion recognition. In some other works, the utterance of words is used for lexical analysis for emotion recognition, which is language dependent. In this paper, two different techniques are utilized for classifying emotions into Angry, Happy or Neutral categories. In the first technique, the maximum cross correlation between audio files is computed for labeling the speech data into one of the three emotion categories. Accordingly, a function is developed in MATLAB for Identification of an emotion for any audio file passed as an argument. The second technique makes use of six discriminatory features, namely, Energy, Volume, MFCC, Zero Crossing Rate, Formants and Spectral Centroid. These features are used as predictors for the purpose of classification of emotions. A variety of classifiers are used through the MATLAB classification learner toolbox, and an accuracy of 91.3% is achieved using the Cubic SVM Classifier. The proposed techniques pave way for a real-time prototype for speech emotion recognition in the near future.	acoustic cryptanalysis;algorithm;cross-correlation;cubic hermite spline;cubic function;emotion recognition;feature selection;high-level programming language;human–computer interaction;lexical analysis;matlab;mel-frequency cepstrum;prototype;real-time clock;real-time computing;real-time transcription;spectral centroid;speech recognition;statistical classification;zero crossing;zero-crossing rate	Joyjit Chatterjee;Vajja Mukesh;Hui-Huang Hsu;Garima Vyas;Zhen Liu	2018	2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)	10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00050	support vector machine;zero-crossing rate;formant;emotion classification;feature extraction;spectral centroid;mel-frequency cepstrum;computer science;utterance;artificial intelligence;pattern recognition	HCI	-12.368199000584443	-87.82663398115321	77774
a1108819c2c578dcbf3464d985b072f2cd29eddc	representing a system of lexical types using default unification	concise representation;default feature structure;useful tool;default unification operation;verbal subcategorisation;linguistic sub-regularities;encodes linguistic information;default inheritance;lexical type;linguistic generalisations	Default inheritance is a useful tool for encoding linguistic generalisations that have exceptions. In this paper we show how the use of an order independent typed default unification operation can provide non-redundant highly structured and concise representation to specify a network of lexical types, that encodes linguistic information about verbal subcategorisation. The system of lexical types is based on the one proposed by Pollard and Sag (1987), but uses the more expressive typed default feature structures, is more succinct, and able to express linguistic sub-regularities more elegantly. 1 I n t r o d u c t i o n Several authors have highlighted the importance of using defaults in the representation of linguistic knowledge, in order to get linguistically adequate descriptions for some natural language phenomena ((Gazdar, 1987), (Bouma, 1992), (Daelemans et al, 1992), (Briscoe, 1993)). Defaults have been used in the definition of inflectional morphology, specification of lexical semantics, analysis of gapping constructions and ellipsis among others. In this paper we use defaults to structure the lexicon, concentrating on the description of verbal subcategorisation information. The issue of how to organise lexical information is especially important when a lexicalised formalism like Categorial Grammar (CG) or HeadDriven Phrase Structure Grammar (HPSG) is employed, since the burden of linguistic description is concentrated in the lexicon and if lexical entries are organised as unrelated lists, there is a significant loss of generalisation and an increase in redundancy. Alternatively, it is possible to use inheritance networks, which provide representations that are able to capture linguistic regularities about classes of items that behave similarly. This idea is employed in Pollard and Sag's (1987) sketch of an HPSG lexicon as a monotonic multiple orthogonal inheritance type hierarchy. However, this work fail to make use of defaults, which would significantly reduce redundancy in lexical specifications and would enable them to elegantly express sub-regularities (Krieger and Nerbonne, 1993). In this paper we demonstrate that using default unification, namely the order-independent and persistent version of default unification described in (Lascarides et al, 1996b) and (Lascarides and Copestake, 1999), to implement a default inheritance network results in a fully declarative specification of a lexical fragment based on Pollard and Sag's (1987), but that is both more succinct and able to express elegantly linguistic sub-regularities, such as the marked status of subject control of transitive subject-control verbs. In section 2, a brief description of the use of defaults and YADU is given. In section 3, we present the results of representing the proposed lexical fragment in terms of default multiple inheritance networks. Finally, we discuss the results achieved and future work. 2 D e f a u l t I n h e r i t a n c e a n d YADU In this work, a default multiple orthogonal inheritance network is used to represent lexical information. Thus, with different subnetworks used to encode different kinds of linguistic knowledge, the idea is that linguistic regularities are encoded near the top of the network, while nodes further down the network are used to represent sub-regularities or exceptions. Such an approach to representing the lexicon has some advantages, like its ability to capture linguistic generalisations, conciseness, uniformity, ease of maintenance and modification, and modularity (Daelemans et al, 1992). This default multiple inheritance network is im-	categorial grammar;circuit complexity;class hierarchy;default logic;encode;exception handling;formal system;head-driven phrase structure grammar;lexicon;maschinen krieger zbv 3000;mathematical morphology;multiple inheritance;natural language;unification (computer science);word lists by frequency	Aline Villavicencio	1999		10.3115/977035.977076	natural language processing;computer science;linguistics	NLP	-32.59483166115649	-81.7059780552998	77915
ca62e4bdea942da954df4fc7e7a6503b2d82d9b7	adaptive determination of audio and visual weights for automatic speech recognition	automatic speech recognition		speech recognition	Alexandrina Rogozan;Paul Deléglise;Mamoun Alissali	1997			audio mining;voice activity detection;acoustic model;speaker recognition;speech coding;speech processing;speech recognition;computer science;pattern recognition;artificial intelligence	NLP	-14.301179795999218	-87.46648902247065	77927
63f1b62ff3cb57e974f5dfa464a4d197f3dab74c	on the equivalence between objective intelligibility and mean-squared error for deep neural network based speech enhancement		Although speech enhancement algorithms based on deep neural networks (DNNs) have shown impressive results, it is unclear, if they are anywhere near optimal in terms of aspects related to human auditory perception, e.g. speech intelligibility. The reason is that the vast majority of DNN based speech enhancement algorithms rely on the mean squared error (MSE) criterion of short-time spectral amplitudes (STSA). State-of-theart speech intelligibility estimators, on the other hand, rely on linear correlation of speech temporal envelopes. This raises the question if a DNN training criterion based on envelope linear correlation (ELC) can lead to improved intelligibility performance of DNN based speech enhancement algorithms compared to algorithms based on the STSA-MSE criterion. In this paper we derive that, under certain general conditions, the STSA-MSE and ELC criteria are practically equivalent, and we provide empirical data to support our theoretical results. The important implication of our findings is that the standard STSA minimumMSE estimator is optimal, if the objective is to perform optimally with respect to a state-of-the-art speech intelligibility estimator.	algorithm;artificial neural network;deep learning;intelligibility (philosophy);mean squared error;speech enhancement;turing completeness	Morten Kolbæk;Zheng-Hua Tan;Jesper Jensen	2018	CoRR		artificial intelligence;estimator;computer science;machine learning;equivalence (measure theory);artificial neural network;intelligibility (communication);speech enhancement;mean squared error	ML	-10.008044331320217	-88.00962448661218	78023
3474972a056f5e235e4bd95f06ac3fd35e4b68f8	multi-voxel pattern analysis applied to the language switch in the bilingual brain-an fmri study			voxel	Hiroyuki Akama;MiaoMei Lei;Li Na;Brian Murphy	2012				ML	-7.697839457443729	-85.05868116888442	78043
01c327a7c9a6d75527fbf0292b5fd8ae526237eb	online model adaptation for voice conversion using model-based speech synthesis techniques.	speech synthesis;model adaptation;voice conversion	In this paper, we present a novel voice conversion method using model-based speech synthesis that can be used for some applications where prior knowledge or training data is not available from the source speaker. In the proposed method, training data from a target speaker is used to build a GMM-based speech model and voice conversion is then performed for each utterance from the source speaker according to the pre-trained target speaker model. To reduce the mismatch between source and target speakers, online model adaptation is proposed to improve model selection accuracy, based on maximum likelihood linear regression (MLLR). Objective and subjective evaluations suggest that the proposed methods are quite effective in generating acceptable voice quality for voice conversion even without training data from source speakers.	google map maker;model selection;speech synthesis	Dalei Wu;Baojie Li;Hui Jiang;Qian-Jie Fu	2009			voice activity detection;speech recognition;computer science;speech synthesis	NLP	-17.375669055938843	-91.81929067273946	78107
38b8cef2378987b594b8b648014a300cd9aa2b7e	syntax over focus		This paper compares three factors affecting prosodic phrasing in Korean: syntax, focus and speech rate. The Syntax and Focus constraints have been claimed to be stronger than the Speech Rate constraint, but it is not known which of these -the syntactic constraint (SYNTAX) or the focus constraint (FOCUS) – is stronger. Based on production and perception data, it is found that SYNTAX is stronger than FOCUS, both at normal and fast speech rates, though less so at fast rate.	focus;speech synthesis;syntax (logic)	Sun-Ah Jun	2002			speech recognition;syntax (programming languages);syntax;perception;abstract syntax;computer science	NLP	-14.055804463481328	-83.58925697447529	78201
355402a48ee8435ae22e1ed5522b32f3098496a5	a comparison of signal processing front ends for automatic word recognition	automatic;front end;word error rate;errors;filters speech recognition signal processing speech intelligibility cepstral analysis error statistics linear predictive coding;speech intelligibility;filter bank;output;real time;gain;filters;signals;speech;human auditory system;linear discriminate analysis;time;data bases;waveforms;linear filtering;data reduction signal processing front ends automatic word recognition word error rate speech recognizer auditory properties control mel filter bank based cepstral front end clean speech degraded speech noise spectral variability ti 105 isolated word database mfb recognition error rates auditory models linear filtering linear predictive coding;linear prediction coding;automatic speech recognition;linear predictive coding;cepstral analysis;algebra;factor analysis;signal processing;principal component analysis;coding;word recognition;rates;error rate;speech recognition;error statistics;control;signal processing error analysis linear predictive coding speech enhancement degradation speech processing speech recognition automatic speech recognition automatic control filter bank;humans;data reduction;data reduction techniques;signal to noise ratio;filtration;models;ratios;noise	This paper compares the word error rate of a speech recognizer using several signal processing front ends based on auditory properties. Front ends were compared with a control mel filter bank (MFB) based cepstral front end in clean speech and with speech degraded by noise and spectral variability, using the TI-105 isolated word database. MFB recognition error rates ranged from 0.5 to 26.9% in noise, depending on the SNR, and auditory models provided error rates as much as four percentage points lower. With speech degraded by linear filtering, MFB error rates ranged from 0.5 to 3.1%, and the reduction in error rates provided by auditory models was less than 0.5 percentage points. Some earlier studies that demonstrated considerably more improvement with auditory models used linear predictive coding (LPC) based control front ends. This paper shows that MFB cepstra significantly outperform LPC cepstra under noisy conditions. Techniques using an optimal linear combination of features for data reduction were also evaluated. >	signal processing	Charles R. Jankowski;Hoang-Doan H. Vo;Richard Lippmann	1995	IEEE Trans. Speech and Audio Processing	10.1109/89.397093	speech recognition;acoustics;word error rate;computer science;signal processing;pattern recognition;statistics	Vision	-13.200373619417674	-91.04671825672527	78255
237b4d92de3f43d2e6f4907efaa0da24aa7a1fdf	a sample and feature selection scheme for gmm-svm based language recognition	nist;support vector machines;gaussian processes;training;gmm svm;speech;system performance;language recognition;training data;gaussian mixture model;adaptation model;testing nist support vector machines kernel degradation training data system performance natural languages speech mutual information;support vector machines feature extraction gaussian processes speech recognition;feature extraction;speech recognition;feature selection;discriminative training;gaussian mixture model feature selection gmm svm language recognition discriminative training	Discriminative training for language recognition has been a key tool for improving system performance. SVM-based algorithms (i.e. GMM-SVM, GLDS-SVM etc.) are important ones for language recognition. The core of these algorithms is to construct the kernel for comparing the similarity of two sequences. It is known that the mismatch between training and test condition will degrade the performance. In this paper, we proposed a novel sample and feature selection scheme under the GMM-SVM framework, which aims at alleviating the duration mismatch problem. The proposed method is evaluated on NIST 03 and 07 language recognition evaluation tasks with improvement over prior techniques.	algorithm;feature selection;google map maker;language identification	Yan Song;Li-Rong Dai	2008	2008 6th International Symposium on Chinese Spoken Language Processing	10.1109/CHINSL.2008.ECP.93	cache language model;support vector machine;training set;speech recognition;nist;feature extraction;computer science;speech;machine learning;pattern recognition;mixture model;gaussian process;feature selection	Arch	-16.688001251793334	-91.24425822609871	78571
f85c3e942a8114a1938cbf071a116c97bb3a12c0	the impact of lexical simplification by verbal paraphrases for people with and without dyslexia	dyslexia;readability;understandability;lexical simplification;eye tracking;verbal paraphrases	Text simplification is the process of transforming a text into an equivalent which is easier to read and to understand, preserving its meaning for a target population. One such population who could benefit from text simplification are people with dyslexia. One of the alternatives for text simplification is the use of verbal paraphrases. One of the more common verbal paraphrase pairs are the one composed by a lexical verb (to hug) and by a support verb plus a noun collocation (to give a hug). This paper explores how Spanish verbal paraphrases impact the readability and the comprehension of people with and without dyslexia dyslexia. For the selection of pairs of verbal paraphrases we have used the Badele.3000 database, a linguistic resource composed of more than 3,600 verbal paraphrases. To measure the impact in reading performance and understandability, we performed an eye-tracking study including comprehension questionnaires. The study is based on a group of 46 participants, 23 with confirmed dyslexia and 23 control group. We did not find significant effects, thus tools that can perform this kind of paraphrases automatically might not have a large effect on people with dyslexia. Therefore, other kinds of text simplification might be needed to benefit readability and understandability of people with dyslexia.		Luz Rello;Ricardo A. Baeza-Yates;Horacio Saggion	2013		10.1007/978-3-642-37256-8_41	natural language processing;surface dyslexia;dyslexia;eye tracking;computer science;linguistics	HCI	-28.734878493463924	-83.28814650776218	78760
2ffffd5be6baefac0b654122612a98494144e38f	uu database: a spoken dialogue corpus for studies on paralinguistic information in expressive conversation	paralinguistic information studies;key property;expressive japanese conversational speech;utsunomiya university;spoken dialogue database;expressive conversation;speech scientist;paralinguistic information;uu database;spoken dialogue corpus;expressive dialogue speech	The Utsunomiya University (UU) Spoken Dialogue Database for Paralinguistic Information Studies, now available to the public, is introduced. The UU database is intended mainly for use in understanding the usage, structure and effect of paralinguistic information in expressive Japanese conversational speech. This paper describes the outline, design, building, and key properties of the UU database, to show how the corpus meets the demands of speech scientists and developers who are interested in the nature of expressive dialogue speech.	unique user	Hiroki Mori;Tomoyuki Satake;Makoto Nakamura;Hideki Kasuya	2008		10.1007/978-3-540-87391-4_55	natural language processing;speech recognition;computer science;linguistics	NLP	-24.405893094369443	-84.27576700133977	78766
9f39a2896655c9f02cad18f37b1f1c0c81455745	a speech recognition technique using mfcc with dwt in isolated hindi words		Human speech recognition is indeed a challenging task through which several words spoken by different individuals could be analyzed and synthesized for man and machine interaction. It allows us to build up a framework for understanding the variability of different phonemes, which can be applied to identify an individual’s word. We create a Hindi speech repository of 5 persons where five words were spoken by 10 times for five different persons. An attempt had been taken to investigate the correctness of speech recognition technique of isolated 5 different Hindi words. We apply a discrete wavelet transforms for extracting the frequency coefficients of spatiotemporal speech signal with respect to time. The mel-second frequency cepstral coefficients (MFCC) considered as feature vector are obtained by applying DWT decomposition on accumulating speech signals. A feature extraction technique, principal component analysis (PCA), is applied on MFCC feature space for reducing the dimensionality of feature vector. We apply a minimum distance-based classifier for comparing an unknown test speech signal among all the training set of Hindi words. The recognition of isolated Hindi words is exploited in two conditions such as applying DWT and without DWT decomposition technique. In this paper, comparative analysis of various methods is carried out. Among all the methods, we found that MFCC with DWT decomposition provides higher recognition accuracy as compared to other techniques.	discrete wavelet transform;speech recognition	Neha Baranwal;Ganesh Jaiswal;Gora Chand Nandi	2013		10.1007/978-81-322-1665-0_70	wavelet transform;principal component analysis;curse of dimensionality;feature extraction;feature vector;speech recognition;mel-frequency cepstrum;classifier (linguistics);discrete wavelet transform;computer science	Vision	-9.102459664789857	-91.29578312432413	78867
26ef3c4f12e5bb1535912075b60a2cbf1a532e36	an adaptive fuzzy multimodal biometric system for identification and verification	adaptive systems biometrics access control face speech fuzzy systems fuzzy logic databases;normal adaptive method adaptive fuzzy multimodal biometric system biometric data sensitive personal information intraclass variability environment conditions semisupervised learning decision threshold mobio face database speech database;fuzzy system mulimodal biometric adaptive rate semisupervised learning;mulimodal biometric;speech processing decision making face recognition fuzzy set theory learning artificial intelligence;adaptive rate;fuzzy system;semisupervised learning	Biometric data are the sensitive personal information and the large intra-class variability due to changes of the environment conditions is an issue in these type of data. Adaptive biometric is the solution that has been introduced and can make the systems more accurate and reliable. For this purpose, semi-supervised learning has been shown to be a possible strategy. On the other hand, one problem in semi-supervised learning is selecting the decision threshold for adaption which can make the strategy unstable. In particular, a strong classifier, in a multimodal system, is better if adapted threshold is replaced with an inflexible one. This paper presents a fuzzy system to find the better threshold for adaptation. Experiments on MOBIO face and speech database show that the proposed strategy is a better approach in comparison to normal adaptive method.	adaptive system;biometrics;control theory;experiment;fuzzy control system;heart rate variability;multimodal interaction;personally identifiable information;semi-supervised learning;semiconductor industry;supervised learning	Mehdi Ghayoumi;Kambiz Ghazinour	2015	2015 IEEE/ACIS 14th International Conference on Computer and Information Science (ICIS)	10.1109/ICIS.2015.7166583	computer vision;adaptive neuro fuzzy inference system;computer science;artificial intelligence;machine learning;pattern recognition;fuzzy control system	Vision	-17.66250404936933	-96.15114378567158	78873
2bce2994caf020c7cd82e7bd329392081df4f8d9	a singing style modeling system for singing voice synthesizers		This paper describes a method of modeling singing styles by a statistical method. In this system, singing expression parameters consisting of melody and dynamics which are derived from F0 and power are modeled by context-dependent Hidden Markov Models (HMMs.) A modeling method of the parameters are optimized for dealing with them. Since parameters we focus on are essential but general ones for singing synthesizers, generated parameters from the trained models may be possible to be applied to many of them. In the experiment, we trained singing style models by using singing recording with much expressive style, then parameters were generated for songs not included in training data and actually applied to our singing synthesizer VOCALOID. As a result, the style was well perceived in the synthesized sound with good synthetic quality.	context-sensitive language;hidden markov model;markov chain;software quality;speech synthesis;synthetic intelligence	Keijiro Saino;Makoto Tachibana;Hideki Kenmochi	2010			speech recognition;computer science;singing	ML	-15.907878521753155	-84.02803497532014	78902
67f97bfb33d7b359bf16551a03442d0b89d3aa1d	timing of experimentally elicited minimal responses as quantitative evidence for the use of intonation in projecting trps		In an RT experiment, subjects were asked to respond with minimal responses to prerecorded dialogs and a manipulated version of these dialogs that contained only intonation and pause information. Response delays and, especially, variances were higher to the impoverished, intonation only, stimuli than to the original recordings. It was also found that in onation onlyutterances ending in a mid-frequency pitch induced significantly longer response delays than utterances ending in a low pitch. These results are interpreted as evidence that just the intonation and pauses of a conversation already contain sufficient information to project end-of-utterance TRPs. However this information is measurably impoverished with respect to full speech to an extent that increases the “processing” time by 10%. Our subjects seemed to fall back to reacting to pauses when presented with intonation onlyutterances ending in a mid-frequency tone. This suggests that, in contrast to low or high end-tones, intonation contours that end in a mid-frequency tone might not contain any useful information for predicting end-of-utterance TRPs.	experiment	Wieneke Wesseling;R. J. J. H. van Son	2005			speech recognition	ML	-8.499993185311148	-81.55389878868212	78922
5951ebe5b229ea247adb2db4dc513181421c23a8	"""comments on """"distinctive regions and modes: a new theory of speech production"""" by m. mrayati, r. carré and b. guérin"""	speech production	regions associated to the eight possible combinations of the sensitivity of the first three resonances. Bringing out acoustic properties of symmetry and compensation, they claim that the production of vowels and consonants is based on these geometric and acoustic properties, since the eight regions can be linked to morphological and articulatory properties of the vocal tract. The authors formulate a new vowel production theory and they propose a universal phonological system for consonants. We are critical of the New Theory on several counts: the limitations of sensitivity functions have been overlooked; the generalizations are anthropomorphically weak; the predictions fail to match known acoustic facts; the universal classification is in contradiction with basic phonetic knowledge. More generally this kind of approach seems intrinsically very limited: the vocal tract is not a series of tubes whose dimensions can be manipulated independently, without referring to an underlying articulatory model integrating articulatory constraints. The New Theory will retain all of its value once it has been returned to its natural context i.e. the simple acoustic description of the vocal tract around the neutral position and as a tool for speech synthesis. Z u s m m m e a f ~ t m g . Mrayati et al. sttitzen sich auf die Sensibilit~itsfunktionen eines gleichm~iBigen Ansatzrohres um den Vokaltrakt in acht Regionen, welche mit den m6glichen Veranderungsmustern der drei ersten Traktusresonanzen in Verbindung stehen, zu unterteilen. Dadurch zeigen sie akustische Symmetrieund Kompensationseffekte. Die Autoren behaupten, dab die Produktion der Vokale und Konsonanten auf diesen akustischen Effekten beruht, da die acht Regionen mit morphologischen und artikulatorischen Eigenschaften des Vokaltrakts in Verbindung gebracht werden kOnnen. Die Autoren formulieren so eine neue Theorie der Vokalproduktion und sic schlagen ein universales phonologisches System fiir Kon-	acoustic cryptanalysis;eine and zwei;sie (file format);series of tubes;speech synthesis;tract (literature);unified model	Louis-Jean Boë;Pascal Perrier	1990	Speech Communication	10.1016/0167-6393(90)90058-H	natural language processing;speech production;computer science;linguistics	NLP	-14.046361948972656	-84.55552150757963	78987
1e2434464c820cb711062687f5325574ab5e0f0d	dnn-based source enhancement self-optimized by reinforcement learning using sound quality measurements		We investigated whether a deep neural network (DNN)-based source enhancement function can be self-optimized by reinforcement learning (RL). The use of a DNN is a powerful approach to describing the relationship between two sets of variables and can be useful for source enhancement function design. By training the DNN using a huge amount of training data, sound quality of output signals are improved. However, collecting a huge amount of training data is often difficult in practice. To use limited training data efficiently, we focus on the “self-optimization” of DNN-based source enhancement function in which RL is commonly utilized in the development of game playing computers. As a reward for RL, quantitative metrics that reflect a human's perceptual score (perceptual score), e.g., perceptual evaluation methods for audio source separation (PEASS), are utilized. To investigate whether the sound quality is improved by RL-based source enhancement, subjective tests were conducted. It was confirmed that the output sound quality of the RL-based source enhancement function improved as the number of iterations was increased and finally outperformed the conventional method.	artificial neural network;computer;deep learning;iteration;mathematical optimization;reinforcement learning;sound quality;source separation	Yuma Koizumi;Yusuke Hioka;Hisashi Uematsu;Kazunori Kobayashi;Youichi Haneda	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952122	artificial neural network;reinforcement learning;pattern recognition;machine learning;perception;sound quality;computer science;source separation;training set;artificial intelligence	Vision	-14.885490917168477	-90.67987558089075	79036
451081291eea193cd4ea915ebcb5ceb3b814d68f	aradaisy: a system for automatic generation of arabic daisy books		A good library is one that has all of its resources accessible to all kinds of people, e.g. people with print disabilities. For this purpose, librarians try to provide books in several types of formats to accommodate different users. For example, e-books and digital talking books (DTB) are now available and can be used by a wider spectrum of users. Several systems can be found for transforming a book into DTB for people where the mother language is English. Such systems transform a book into DTB by encoding the document using DAISY format. However, Arabic DTBs are sparse since the work done so far in this domain is not well understood. In fact, Arabic language is very hard to process and such characteristics provide another complex dimension to a system that is able to transform an Arabic book into a DTB. In this paper, we propose a framework for an Arabic DTB that uses DAISY format. The proposed system includes an image-to-text converter, context-injector, text-to-audio-generator, and finally a DAISY gen...	daisy digital talking book	Iyad Abu Doush;Faisal Alkhateeb;Abdelraouf Albsoul	2017	IJCAT	10.1504/IJCAT.2017.10006836	engineering;arabic;multimedia;optical character recognition;talking books;systems engineering;encoding (memory);complex dimension;first language	NLP	-28.505770599527256	-83.89881338539001	79084
ba5c72a088d9c869f49d96de6ec541f047ebb931	instantaneous environment adaptation techniques based on fast pmc and map-cms methods	background noise;collision mitigation;multi pass search;channel distortion;fpmc;fast parallel model combination instantaneous environment adaptation techniques fast pmc method map cms method additive noise channel distortion fpmc speech recognition sentence real time adaptation cepstral mean subtraction method maximum a posteriori estimation multi pass search combination method experiment results;cepstral mean subtraction;real time;working environment noise;fast parallel model combination;real time adaptation;additive noise;combination method;map cms method;experiment results;maximum likelihood estimation;maximum a posteriori estimation;telephony;parallel model combination;sentence;cepstral analysis;adaptive systems;fast pmc method;cepstral mean subtraction method;speech recognition;real time implementation;search problems;parallel algorithms speech recognition maximum likelihood estimation adaptive systems cepstral analysis search problems noise;collision mitigation additive noise speech recognition working environment noise telephony cepstral analysis real time systems background noise calibration character recognition;character recognition;calibration;instantaneous environment adaptation techniques;noise;new combination;real time systems;parallel algorithms	This paper proposes instantaneous environment adaptation techniques for both additive noise and channel distortion based on the fast PMC (FPMC) and the MAP-CMS methods. The instantaneous adaptation techniques enable a recognizer to improve recognition on a single sentence that is used for the adaptation in real-time. The key innovations enabling the system to achieve the instantaneous adaptation are: 1) a cepstral mean subtraction method based on maximum a posteriori estimation (MAP-CMS), 2) real-time implementation of the fast PMC [5] that we proposed previously, 3) utilization of multi-pass search, and 4) a new combination method of MAP-CMS and FPMC to solve the problem of both channel distortion and additive noise. Experiment results showed that the proposed methods enabled the system to perform recognition and adaptation simultaneously nearly in real-time and obtained good improvements in performance.	2.5d;additive white gaussian noise;cepstrum;distortion;elegant degradation;experiment;fast fourier transform;finite-state machine;real-time clock;real-time transcription;utility functions on indivisible goods	Tetsuo Kosaka;Hiroki Yamamoto;Masayuki Yamada;Yasuhiro Komori	1998		10.1109/ICASSP.1998.675383	calibration;speech recognition;computer science;noise;adaptive system;maximum a posteriori estimation;machine learning;pattern recognition;background noise;parallel algorithm;maximum likelihood;telephony;statistics	Robotics	-13.242053261972398	-94.31181983107376	79173
2fc9ec771e7697321f67df11d82ce523331125fe	perceptual and analytical analysis of the effect of the hammer impact on the piano tones		This paper presents a model of the excitation corresponding to the hammer impact in a hybrid piano model. An experimental setup permits the measurement of the signal at the bridge location. The signal is separated into an excitation and resonant contribution. This work focuses on the modeling of the excitation part using both physical and perceptually relevant parameters. The resulting model can be used both in analysis/synthesis and transformation. Important piano performance parameters such as position and velocity of the hammer are inherent in the model.	velocity (software development)	Julien Bensa;Kristoffer Jensen;Richard Kronland-Martinet;Sølvi Ystad	2000			speech recognition;acoustics;audiology	HCI	-8.67742646734726	-85.23769778052814	79185
98d3a501d888bb1326257a7d71a2f0b4a50ab561	aspiration and the gradient structure of english prefixed words		Building on work examining the phonetic properties of prefixed and pseudoprefixed English words (mistimes vs. mistakes), we investigate aspiration in 110 English words beginning with misand dis-, produced by 16 native speakers of American English. We find that some items show considerable cross-speaker variation, but most are stable. Aspiration can occur even before an unstressed syllable (dis-[p]ossessed), suggesting that not only word-initial but also some stem-initial voiceless stops are aspirated in English, either because of their prosodic position (prosodic-word-initial) or because of influence of the stem’s freestanding pronunciation. Frequency factors correlate with an item’s propensity to aspirate, supporting the view that whole-word and decomposed representations compete.	gradient;syllable	Kie Zuraw;Sharon Peperkamp	2015			mathematics	NLP	-10.713679803646349	-81.0822982558965	79234
1a4ae7d67da6e74348edaa66f80ffbd7f2a5a173	attributing modelling errors in hmm synthesis by stepping gradually from natural to modelled speech	hidden markov models speech lead smoothing methods;speech;smoothing methods;hidden markov models;mel line spectral pairs modelling errors hmm synthesis hidden markov model natural speech modelled speech speech synthesis systems speech naturalness vocoded speech mel cepstra pairs;lead;vocoding speech synthesis hidden markov modelling;voice equipment hidden markov models speech synthesis vocoders	Even the best statistical parametric speech synthesis systems do not achieve the naturalness of good unit selection. We investigated possible causes of this. By constructing speech signals that lie in between natural speech and the output from a complete HMM synthesis system, we investigated various effects of modelling. We manipulated the temporal smoothness and the variance of the spectral parameters to create stimuli, then presented these to listeners alongside natural and vocoded speech, as well as output from a full HMM-based text-to-speech system and from an idealised `pseudo-HMM'. All speech signals, except the natural waveform, were created using vocoders employing one of two popular spectral parameterisations: Mel-Cepstra or Mel-Line Spectral Pairs. Listeners made `same or different' pairwise judgements, from which we generated a perceptual map using Multidimensional Scaling. We draw conclusions about which aspects of HMM synthesis are limiting the naturalness of the synthetic speech.	hidden markov model;line spectral pairs;mel-frequency cepstrum;multidimensional scaling;natural language;speech synthesis;stepping level;synthetic intelligence;vocoder;waveform	Thomas Merritt;Javier Latorre;Simon King	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178766	lead;linear predictive coding;speech recognition;computer science;speech;hidden markov model	Robotics	-11.030552050535874	-86.23105857798896	79250
b77d1714593f1ff52c3420b9b136592e203a52c5	training deep bidirectional lstm acoustic model for lvcsr by a context-sensitive-chunk bptt approach	dblstm;bptt;parallel training long short term memory dblstm dblstm dnn lvcsr context sensitive chunk bptt model averaging;model averaging;training hidden markov models speech acoustics speech processing context modeling graphics processing units;speech recognition backpropagation hidden markov models recurrent neural nets;accuracy degradation dblstm acoustic model lvcsr deep bidirectional long short term memory recurrent neural network large vocabulary continuous speech recognition context sensitive chunk back propagation through time approach dblstm training csc based decoding method gpu epoch wise bptt training frame level cross entropy criterion ce trained dblstm hmm system relative word error rate csc bptt;lvcsr;context sensitive chunk;dnn;parallel training;long short term memory	This paper presents a study of using deep bidirectional long short-term memory (DBLSTM) recurrent neural network as acoustic model for DBLSTM-HMM based large vocabulary continuous speech recognition (LVCSR), where a context-sensitive-chunk (CSC) back-propagation through time (BPTT) approach is used to train DBLSTM by splitting each training sequence into chunks with appended contextual observations, and a CSC-based decoding method with possibly overlapped CSCs is used for recognition. Our approach makes mini-batch based training on GPU more efficient and reduces the latency of DBLSTM-based LVCSR from a whole utterance to a short chunk. Evaluations have been made on Switchboard-I benchmark task. In comparison with epoch-wise BPTT training, our method can achieve more than three times speedup on a single GPU card without degrading recognition accuracy. In comparison with a highly optimized DNN-HMM system trained by a frame-level cross entropy (CE) criterion, our CE-trained DBLSTM-HMM system achieves relative word error rate reductions of 9% and 5% on Eval2000 and RT03S testing sets, respectively. Furthermore, by running model averaging based parallel training of DBLSTM on a cluster of GPUs, CSC-BPTT incurs less accuracy degradation than epoch-wise BPTT while achieves a linear speedup.	acoustic cryptanalysis;acoustic model;artificial neural network;backpropagation through time;benchmark (computing);computational complexity theory;context-sensitive grammar;cross entropy;discriminative model;elegant degradation;experiment;feature vector;gpu cluster;graphics processing unit;hidden markov model;long short-term memory;parallel computing;recurrent neural network;software propagation;speech analytics;speech recognition;speedup;telephone switchboard;vocabulary;word error rate	Kai Chen;Zhi-Jie Yan;Qiang Huo	2015	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2016.2539499	speech recognition;computer science;machine learning;pattern recognition;long short term memory	NLP	-18.56666001936787	-88.74076312965205	79339
770dd96eb6bcb7b1bce5061ef367e76dfff64c56	a novel arabic text-independent speaker verification system based on fuzzy hidden markov model		Abstract The most important shortcoming of the current speaker verification methods (based on knowledge or possession) is that this process is not sure whether the holder of the given ID is the entitled one or an imposter. Using biometrics in the verification system is designated for minimizing this problem and decreasing the necessity of carrying the tokens. In this paper, a novel Arabic text-independent speaker verification system is presented. First of all, new speech features are proposed for speaker characterization, which denoted as Wavelet Packet Four-Directional Features (WPFDF). With the objective of speaker verification, the paper proposes a Fuzzy Hidden Markov Model, termed FHMM, where the kernel fuzzy c-means (KFCM) is extended to calculate fuzzy memberships of HMMs training samples. Thus, information loss is reduced as well as recognition rate is increased. The proposed approach reached 98.38% of recognition rate.	hidden markov model;markov chain;speaker recognition;verification and validation	Rania M. Ghoniem;Khaled Shaalan	2017		10.1016/j.procs.2017.10.119	fuzzy logic;kernel (linear algebra);machine learning;wavelet;artificial intelligence;hidden markov model;speaker diarisation;speaker recognition;computer science;biometrics;network packet;pattern recognition	NLP	-12.952789276501145	-96.27158663274268	79469
eb362c8c2db2b5076c6a75d9734b69baf65d6b7b	learning consistent semantics from training data	databases;notice of violation;classification tree;learning artificial intelligence speech recognition natural languages grammars;speech analysis;semantics;natural language understanding system;natural languages;chanel;semantic classification trees;training data;grammars;semantically important word islands;natural language understanding;context dependent utterances;land transportation;context dependent utterances semantics training data chanel chart based parser semantically important word islands semantic classification trees natural language understanding system;speech recognition;cities and towns;speech understanding;context dependent;humans;classification tree analysis;learning artificial intelligence;information analysis;chart based parser;training data cities and towns classification tree analysis speech analysis humans educational institutions information analysis databases land transportation	In [1,2], we described a speech understanding system called “CHANEL” with two components: A chart-based parser that analyzes semantically important word islands within an utterance; A component based on “Semantic Classification TTees” (SCTs) that builds the representation for the complete utterance. The construction of a natural-language understanding (NLU) system is a task that has traditionally required lavish expenditure of programmer-hours. By dividing the task in this way, we enabled many of the system’s rules (those contained in the SCT component) to be learned automatically from training data, freeing human expertise t o be applied where it is most effective. This paper describes recent improvements to both components of CHANEL, along with a new module that handles contextdependent utterances. The new version of CHANEL has a new use for SCTs: a special SCT decides whether a sentence is context-dependent or not.	context-sensitive language;natural language understanding;programmer;speech recognition	Roland Kuhn;Renato De Mori;Evelyne Millien	1994		10.1109/ICASSP.1994.389724	natural language processing;training set;speech recognition;decision tree learning;computer science;machine learning;context-dependent memory;semantics;natural language;data analysis	NLP	-22.70472298553757	-81.93503492960474	79500
c7d987a91382284fde5242beb97df0c25ce1b126	articulatory consequences of vocal effort elicitation method		Articulatory features from two datasets, Slovak and Swedish, were compared to see whether different methods of eliciting loud speech (ambient noise vs. visually presented loudness target) result in different articulatory behavior. The features studied were temporal and kinematic characteristics of lip separation within the closing and opening gestures of bilabial consonants, and of the tongue body movement from /i/ to /a/ through a bilabial consonant. The results indicate larger hyperarticulation in the speech elicited with visually presented target. While individual articulatory strategies are evident, the speaker groups agree on increasing the kinematic features consistently within each gesture in response to the increased vocal effort. Another concerted strategy is keeping the tongue response considerably smaller than that of the lips, presumably to preserve acoustic prerequisites necessary for the adequate vowel identity. While the method of visually presented loudness target elicits larger span of vocal effort, the two elicitation methods achieve comparable consistency per loudness conditions.	acoustic cryptanalysis;closing (morphology)	Elisabet Eir Cortes;Marcin Wlodarczak;Juraj Simko	2018		10.21437/Interspeech.2018-1038	loudness;speech recognition;ambient noise level;vocal effort;phonetics;computer science	HCI	-9.2413510997007	-81.88727198168121	79665
c6f885bb664bf894e7d75a1155e61266d7012c92	correntropy function for fundamental frequency determination of musical instrument samples	autocorrelation function;musical instruments;functional dependency;correntropy;kernel method;pitch;music transcription;full width at half maximum;fundamental frequency;autocorrelation	Fundamental frequency or pitch determination is one of the main issues in the transcription of music. In this paper, we determined the fundamental frequencies of isolated musical instrument samples by computing the correntropy functions. As the correntropy function depends on kernel methods, we demonstrated its performance using various kernel sizes. We presented the better resolution of the correntropy function than the conventional and the summary autocorrelation functions by calculating the full-width-at-half-maximum of the peaks of the functions. The superiority was confirmed for the samples of 20 different instruments based on the average width of the peaks.		M. Erdal Özbek;F. Acar Savaci	2011	Expert Syst. Appl.	10.1016/j.eswa.2011.02.015	speech recognition;autocorrelation;computer science;mathematics;statistics	ML	-8.430618844339108	-91.1149043492677	79677
3a39d316825846a6cb98d35dc2c08015c43f0ec4	the effect of timbre in clarinet interpretation		The perceptual importance of timbre variations was investigated in clarinet expressive music performance. Three basic transformations acting on timbre, rhythm and dynamics and four combinations of them were applied to solo clarinet recordings in order to remove or flatten some of the expressive variations of the performer. Twenty skill ed musicians were asked to choose the interpretations they preferred in a pair comparison task. The rankings of the performances were strongly consistent and this, for two different musical excerpts coming from Bach and Mozart pieces. Multidimensional scaling showed that the most prominent factor used by listeners was linked to the timbre of the tones.	image scaling;multidimensional scaling;performance;spectral centroid;strong consistency;structure of observed learning outcome	Mathieu Barthet;Philippe Depalle;Richard Kronland-Martinet;Sølvi Ystad	2007			acoustics	ML	-9.702555315541426	-83.084864558512	79770
b61b7368e1182186cac8e2de2caf5f3a6bd142fb	performance analysis of neural networks in combination with n-gram language models	analytical models;speech synthesis feedforward neural nets recurrent neural nets;artificial neural networks history analytical models training data vocabulary interpolation;interpolation;history;speech synthesis;vocabulary;training data;stt neural network language model;artificial neural networks;stt;data homogeneity performance analysis neural network language models nnlm n gram language models speech to text systems feedforward neural network recurrent neural network;feedforward neural nets;recurrent neural nets;language model;neural network	Neural Network language models (NNLMs) have recently become an important complement to conventional n-gram language models (LMs) in speech-to-text systems. However, little is known about the behavior of NNLMs. The analysis presented in this paper aims to understand which types of events are better modeled by NNLMs as compared to n-gram LMs, in what cases improvements are most substantial and why this is the case. Such an analysis is important to take further benefit from NNLMs used in combination with conventional n-gram models. The analysis is carried out for different types of neural network (feed-forward and recurrent) LMs. The results showing for which type of events NNLMs provide better probability estimates are validated on two setups that are different in their size and the degree of data homogeneity.	language model;n-gram;neural networks;profiling (computer programming);recurrent neural network;speech recognition	Ilya Oparin;Martin Sundermeyer;Hermann Ney;Jean-Luc Gauvain	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6289044	training set;speech recognition;interpolation;computer science;artificial intelligence;machine learning;artificial neural network	SE	-19.6782531034315	-87.0232429028968	79844
08935019faf48c763ba2b514ab15bebd2381a773	investigation of glottal features and annotation procedures for speech emotion recognition	speech;emotion recognition;cepstrum;feature extraction;speech recognition;context;labeling	Speech emotion recognition is a still challenging problem despite having been investigated over the last couple of decades. Conventional speech emotion recognition performance is low, but this may be improved by considering new features and an annotation method. In this paper, firstly we use glottal features for speech emotion recognition to improve its performance because the emotions are related to glottal characteristics. Secondly emotional labels used as “correct” emotions are usually annotated by multiple annotators in the conventional approach, but this may be ambiguous due to the confusion created by using each annotator's criterion. In this paper, a single annotator gives emotional labels in an aim to obtain a unified criterion and we use these labels for speech emotion recognition to improve its performance. Finally, context information which is included in a speech segment before the utterance being annotated, will be considered.	baseline (configuration management);emotion recognition;federal enterprise architecture;focus group;sensitivity and specificity	Masaaki Takebe;Kazumasa Yamamoto;Seiichi Nakagawa	2016	2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)	10.1109/APSIPA.2016.7820791	psychology;natural language processing;speech recognition;communication;speech analytics	NLP	-12.654193393763018	-85.49350218347303	79879
8c4151a28e2179b008e425b3b7e14916d2ed14b2	exploring human speech production mechanisms by mri	tecnologia electronica telecomunicaciones;magnetic resonance imaging mri;vocal tract acoustics;speech physiology;tecnologias;grupo a;speech production			Kiyoshi Honda;Hironori Takemoto;Tatsuya Kitamura;Satoru Fujita;Sayoko Takano	2004	IEICE Transactions		speech production;speech recognition	OS	-7.720136305128508	-84.34478304708324	79888
7c3b1cf479296f94ae1ff21172ba3581c5c33f9d	recognition of persian online handwriting using elastic fuzzy pattern recognition	online handwriting recognition;fuzzy modeling;elastic pattern matching;pattern recognition	Persian is a fully cursive handwriting in which each character may take different forms in different parts of the word, characters overlap and there is a wide range of possible styles. These complexities make automatic recognition of Persian a very hard task. This paper presents a novel approach on recognition of such writings systems which is based on the description of input stream by a sequence of fuzzy linguistic terms; representation of character patterns with the same descriptive language; and comparison of inputs with character patterns using a novel elastic pattern matching approach. As there is no general benchmark for recognition of Persian handwriting, the approach has been tested on the set of words in first primary Iranian school books including 1250 words resulting in 78% correct recognition without dictionary and 96% with dictionary.	pattern recognition	Ramin Halavati;Saeed Bagheri Shouraki	2007	IJPRAI	10.1142/S0218001407005533	natural language processing;speech recognition;feature;intelligent character recognition;computer science;intelligent word recognition;pattern recognition	Vision	-27.064680641598986	-80.81678614492002	80062
caba61387d05fa29bb805a49ebf145283ad5a7ee	bayesian separation of audio-visual speech sources	mouth;bayesian methods microphone arrays mouth working environment noise face detection independent component analysis speech processing speech enhancement acoustic noise feature extraction;time dependent;video signal processing;acoustic noise speech processing video signal processing bayes methods feature extraction source separation;bayes methods;speech processing;working environment noise;bayesian methods;independent component analysis;speech enhancement;speech source time dependency;online demixing process visual feature extraction bayesian audio visual speech source separation acoustically noisy environments ica bayesian mixing process model speech source bimodality speech source time dependency;mixing process;bayesian mixing process model;visual feature extraction;feature extraction;acoustic noise;online demixing process;microphone arrays;face detection;source separation;acoustically noisy environments;ica;speech source bimodality;bayesian model;bayesian audio visual speech source separation	In this paper, we investigate the use of audio and visual rather than only audio features for the task of speech separation in acoustically noisy environments. The success of existing independent component analysis (ICA) systems for the separation of a large variety of signals, including speech, is often limited by the ability of this technique to handle noise. In this paper, we introduce a Bayesian model for the mixing process that describes both the bimodality and the time dependency of speech sources. Our experimental results show that the online demixing process presented here outperforms both the ICA and the audio-only Bayesian model at all levels of noise.	bayesian network;independent computing architecture;independent component analysis	Shyamsundar Rajaram;Ara V. Nefian;Thomas S. Huang	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1327196	independent component analysis;face detection;speech recognition;feature extraction;bayesian probability;computer science;machine learning;speech coding;noise;pattern recognition;speech processing;bayesian inference;statistics	Robotics	-12.399445230804993	-92.216938425844	80108
bcdcc474db6869a6d12323ea70aee2fef553ac3f	a search method for on-line handwritten text employing writing-box-free handwriting recognition	handwriting recognition;on line handwritten text;information retrieval;information retrieval handwritten character recognition;search method;search methods handwriting recognition character recognition text recognition lattices keyboards writing agriculture working environment noise hardware;candidate characters search method writing box free handwriting recognition writing box free online handwritten text search target keyword candidate segmentations;text recognition;text recognition search method on line handwritten text character recognition;character recognition;handwritten character recognition	This paper presents a method for writing-box-free on-line handwritten text search. It searches for a target keyword in the lattice composed of candidate segmentations and candidate characters. By considering the accuracy of the recognition method and the length of the keyword, the method decreases noises to be output from the lattice effectively. When the keyword consists of three characters, we have achieved the recall rate 89.4%, the precision rate 93.2% and F measure 0.912.	computation;experiment;f1 score;handwriting recognition;online and offline;sensitivity and specificity	Hideto Oda;Akihito Kitadai;Motoki Onuma;Masaki Nakagawa	2004	Ninth International Workshop on Frontiers in Handwriting Recognition	10.1109/IWFHR.2004.9	natural language processing;speech recognition;document processing;intelligent character recognition;computer science;intelligent word recognition;pattern recognition;handwriting recognition;optical character recognition	Vision	-23.58693773697917	-81.1991832755202	80187
182bfdbf02077e2bed30ced98860dac7398292bc	limited domain synthesis	errors;reliability;unit selection;speech synthesis;data compression;efficiency;semantics;speech representation;data bases;templates	This work presents a reliable and efficient method for building limited domain speech synthesis voices. By constructing databases close to the targeted domain of the speech application, unit selection synthesis techniques can be used to reliably give very high quality synthesis within domain. In addition to a high quality result we include the techniques and processes required to build such voices often allowing new voices in limited but quite complex domains such as dialog systems to be created in under a week. The full tools, documentation examples etc are available for free at http://festvox.org .	database;dialog system;display resolution;documentation;speech synthesis	Alan W. Black;Kevin A. Lenzo	2000			data compression;natural language processing;speech recognition;computer science;reliability;semantics;linguistics;efficiency;speech synthesis;statistics	Logic	-20.876417872510256	-83.90927368448669	80203
c0d387cfc583f97fc16b122d10394a1dd2abd88d	exploring speech enhancement with generative adversarial networks for robust speech recognition		We investigate the effectiveness of generative adversarial networks (GANs) for speech enhancement, in the context of improving noise robustness of automatic speech recognition (ASR) systems. Prior work [1] demonstrates that GANs can effectively suppress additive noise in raw waveform speech signals, improving perceptual quality metrics; however this technique was not justified in the context of ASR. In this work, we conduct a detailed study to measure the effectiveness of GANs in enhancing speech contaminated by both additive and reverberant noise. Motivated by recent advances in image processing [2], we propose operating GANs on log-Mel filterbank spectra instead of waveforms, which requires less computation and is more robust to reverberant noise. While GAN enhancement improves the performance of a clean-trained ASR system on noisy speech, it falls short of the performance achieved by conventional multi-style training (MTR). By appending the GAN-enhanced features to the noisy inputs and retraining, we achieve a 7% WER improvement relative to the MTR system.	additive white gaussian noise;automated system recovery;computation;filter bank;generative adversarial networks;image processing;mtr;speech enhancement;speech recognition;utility functions on indivisible goods;waveform;word error rate	Chris Donahue;Bo Li;Rohit Prabhavalkar	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462581	speech recognition;task analysis;generative grammar;robustness (computer science);image processing;noise measurement;adversarial system;filter bank;artificial intelligence;pattern recognition;speech enhancement;computer science	Vision	-14.044552828130469	-91.15361186413472	80211
9998d894777ef59b8c42af40c15e763e0c7c28e1	novel discriminative vector quantization approach for speaker identification	speaker identification;discriminative weight;vector quantization;feature space segmentation;vector quantizer	A novel Discriminative Vector Quantization method for Speaker Identification (DVQSI) is proposed, and its parameters selection is discussed. In the training mode of this approach, the vector space of speech features is divided into a number of regions. Then, a Vector Quantization (VQ) codebook for each speaker in each region is constructed. For every possible combination of speaker pairs, a discriminative weight is assigned for each region, based on the region's ability to discriminate between the speaker pair. Consequently, the region, which contains a larger distribution difference between the speech feature vector sets of the two speakers in the speaker pair, plays a more important role by assigning it a larger discriminative weight, in identifying the better speaker match from the two speakers. In the testing mode, to identify an unknown speaker, discriminative weighted average VQ distortion pairs are computed for the unknown speaker input waveform. Then, a technique is described that figures out the best match between the unknown waveform and speakers' templates. The proposed DVQSI approach can be considered a generalization of the existing VQ technique for Speaker Identification (VQSI). The method presented here yields better Speaker Identification (SI) accuracy by employing the discriminative weights and space segmentation as design parameters. This is confirmed experimentally. In addition, a computationally efficient implementation of the DVQSI technique is given which uses a tree-structured-like approach to obtain the codebooks.	vector quantization	Guangyu Zhou;Wasfy B. Mikhael;Brent Myers	2005	Journal of Circuits, Systems, and Computers	10.1142/S0218126605002404	speaker recognition;speaker diarisation;speech recognition;computer science;machine learning;pattern recognition;mathematics;vector quantization	ML	-12.381221361250441	-90.22870006012161	80249
3e57809744bd1e6031be2c69c41ee24e26fe8754	generative modeling of speech f0 contours		Fundamental frequency (F0) contour  time course of frequency of vocal fold vibration (Manifestation of physical movement of thyroid cartilage)  contains various types of non-linguistic information • Speaker’s identity, intention, attitude, mood, etc.  modeling and analyzing F0 contours can be potentially useful for many speech applications in which prosodic information plays a significant role. (In speech synthesis, one challenge is to create a natural -sounding pitch contour for the utterance as a whole.) Generative modeling of speech F0 contours H. Kameoka1,2, K. Yoshizato1, T. Ishihara1, Y. Ohishi2, K. Kashino2, S. Sagayama1*	equal-loudness contour;generative modelling language;speech synthesis	Hirokazu Kameoka;Kota Yoshizato;Tatsuma Ishihara;Yasunori Ohishi;Kunio Kashino;Shigeki Sagayama	2013				ML	-8.540034618501954	-82.92756957842779	80314
f229a55dd1f2cf250096d7bfd26357edb2fbae31	direct modelling of output context dependence in discriminative hidden markov model	dynamic back off modelling algorithm;hidden markov model;mutual information independence assumption;discriminative hidden markov model;part of speech tagging;mutual information;context dependent;output context dependence;constraint relaxation principle	This paper proposes a discriminative HMM to directly model output context dependence. The discriminative HMM assumes mutual information independence in its output model that a ''hidden'' state is only dependent on the outputs and independent on other ''hidden'' states. As a result, it overcomes the output context independent assumption in the traditional generative HMM. In addition, a dynamic back-off modelling algorithm using constraint relaxation principle is proposed to resolve the data sparseness problem in the discriminative HMM due to the direct modelling of the output context dependence in its output model. The evaluations on part-of-speech tagging and phrase chunking show that the discriminative HMM can effectively capture the output context dependence through its output context dependent output model and the dynamic back-off modelling algorithm.	hidden markov model;markov chain	Guodong Zhou	2005	Pattern Recognition Letters	10.1016/j.patrec.2004.09.009	speech recognition;computer science;machine learning;context-dependent memory;pattern recognition;mathematics;mutual information;hidden markov model;statistics	Vision	-18.776882438301193	-90.09113784908057	80403
4a92a3e3264b2d1f81fd528d5145938f20c1682e	a multiobjective learning and ensembling approach to high-performance speech enhancement with compact neural network architectures		In this study, we propose a novel deep neural network (DNN) architecture for speech enhancement (SE) via a multiobjective learning and ensembling (MOLE) framework to achieve a compact and lowlatency design, while maintaining good performance in quality evaluations. MOLE follows the boosting concept when combining weak models into a strong classifier and consists of two compact DNNs. The first, called the multiobjective learning DNN (MOL-DNN), takes multiple features, such as log-power spectra (LPS), mel-frequency cepstral coefficients (MFCCs) and Gammatone frequency cepstral coefficients (GFCCs) to predict a multiobjective set that includes clean speech feature, dynamic noise feature, and ideal ratio mask (IRM). The second, called the multiobjective ensembling DNN (MOE-DNN), takes the learned features from MOL-DNN as inputs and separately predicts clean LPS and IRM, clean MFCC and IRM, and clean GFCC and IRM using three sets of weak regression functions. Finally, a postprocessing operation can be applied to the estimated clean features by leveraging the multiple targets learned from both the MOL-DNN and the MOE-DNN. On speech corrupted by 15 noise types not seen in model training the SE results show that the MOLE approach, which features a small model size and low run-time latency, can achieve consistent improvements over both DNN- and long short-term memory (LSTM)-based techniques in terms of all the objective metrics evaluated in this study for all three cases (the input contexts contain 1-frame, 4-frame and 7-frame instances). The 1-frame MOLE-based SE system outperforms the DNN-based SE system with a 7-frame input expansion at a 3-frame delay and also achieves better performance than the LSTM-based SE system with 4-frame, no delay expansion by including only 3 previous frames, and with 170 times less processing latency.	artificial neural network;coefficient;deep learning;information rights management;lightweight portable security;long short-term memory;moe;mel-frequency cepstrum;speech enhancement	Qing Wang;Jun Du;Li-Rong Dai;Chin-Hui Lee	2018	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2018.2817798	boosting (machine learning);latency (engineering);artificial intelligence;architecture;artificial neural network;pattern recognition;computer science;noise measurement;mel-frequency cepstrum;speech enhancement	Web+IR	-16.018320949117157	-90.07187628881725	80497
75ce36250821ce67a4f34f17d0b72d717e4a8952	dynamically configurable acoustic models for speech recognition	baum welch trained system;parameter size;histograms;error reduction;model combination;covariance analysis;decision tree;continuous density hmm;parameter size dynamically configurable acoustic models speech recognition hidden markov model hmm parameters sharing sub phonetic level senones phonetic contexts prediction senonic decision tree speech recognition system downsizing gaussian covariances continuous density hmm chmm recognition accuracy system size baum welch trained system shared covariance model performance unshared full model gaussian means model accuracy covariance sharing algorithm downsizing algorithm error reduction;gaussian processes;system size;hidden markov model;acoustic modeling;speech processing;performance;resource management;dynamically configurable acoustic models;recognition accuracy;acoustic signal processing;model accuracy;trees mathematics;senones;baum welch;downsizing algorithm;acoustic testing;density functional theory;training data;error analysis;gaussian covariances;acoustic signal processing speech recognition speech processing hidden markov models gaussian processes covariance analysis error statistics trees mathematics decision theory;hidden markov models;speech recognition hidden markov models decision trees error analysis resource management training data histograms acoustic testing statistics density functional theory;decision theory;statistics;speech recognition;error statistics;hmm parameters sharing;senonic decision tree;gaussian means;sub phonetic level;decision trees;shared covariance model;covariance sharing algorithm;chmm;unshared full model;speech recognition system downsizing;phonetic contexts prediction;dynamic configuration	Senones were introduced to share Hidden Markov model (HMM) parameters at a sub-phonetic level in [3] and decision trees were incorporated to predict unseen phonetic contexts in [4]. In this paper, we will describe two applications of the senonic deci sion tree in (1) dynamically downsizing a speech recognition sys tem for small platforms and in (2) sharing the Gaussian covarian ces of continuous density HMMs (CHMMs). We experimented how to balance different parameters that can offer the best trad e off between recognition accuracy and system size. The dynamica lly downsized system, without retraining, performed even bett er than the regular Baum-Welch [1] trained system. The shared covar iance model provided as good a performance as the unshared full mod el and thus gave us the freedom to increase the number of Gaussia n means to increase the accuracy of the model. Combining the downsizing and covariance sharing algorithms, a total of 8% error reduction was achieved over the Baum-Welch trained system w ith approximately the same parameter size.	acoustic cryptanalysis;acoustic model;baum–welch algorithm;bett;cluster analysis;decision tree;finite-state machine;hidden markov model;markov chain;requirement;sion's minimax theorem;speech recognition;standard ml;welch's method	Mei-Yuh Hwang;Xuedong Huang	1998		10.1109/ICASSP.1998.675353	speech recognition;computer science;machine learning;decision tree;pattern recognition;hidden markov model;statistics	ML	-19.019406715635096	-92.35766767944675	80613
2c07ef91180989d990d16562ea37de4d737621e1	a sequential repetition model for improved disfluency detection		This paper proposes a new method for automatically detecting disfluencies in spontaneous speech – specifically, selfcorrections – that explicitly models repetitions vs. other disfluencies. We show that, in a corpus of Supreme Court oral arguments, repetition disfluencies can be longer and more stutterlike than the short repetitions observed in the Switchboard corpus and suggest that they can be better represented with a flat structure that covers the full sequence. Since these disfluencies are relatively easy to detect, weakly supervised training is an effective way to minimize labeling costs. By explicitly modeling these, we improve general disfluency detection within and across domains, and we provide a richer transcript.	sensor;spontaneous order;telephone switchboard;text corpus	Mari Ostendorf;Sangyun Hahn	2013			speech recognition;artificial intelligence;pattern recognition;computer science	NLP	-19.354003373253413	-81.61554285986857	80703
36e57a3e5d7a7366e51ac3e1e056a224f4a3ee8c	filled pauses as markers of discourse structure	loudspeakers;natural languages;psychology;speech processing	This study aims to test quantitatively whether lled pauses (FPs) may highlight discourse structure. More speci cally, it is rst investigated whether FPs are more typical in the vicinity of major discourse boundaries. Secondly, the FPs are analyzed acoustically, to check whether those occurring at major discourse boundaries are segmentally and prosodically di erent from those at shallower breaks. Analyses of twelve spontaneous monologues (Dutch) show that phrases following major discourse boundaries more often contain FPs. Additionally, FPs after stronger breaks tend to occur phraseinitially, whereas the majority of the FPs after weak boundaries are in phrase-internal position. Also, acoustic observations reveal that FPs at major discourse boundaries are both segmentally and prosodically distinct. They also di er with respect to the distribution of neighbouring silent pauses.	acoustic cryptanalysis;spontaneous order	Marc Swerts;Anne Wichmann;Robbert-Jan Beun	1996			psychology;speech recognition;acoustics;communication	NLP	-10.993907453905834	-81.4803735296441	80774
10a6326606d1c695fc2e8fcf1c737bc9f8c677d3	fast, cheap, and creative: evaluating translation quality using amazon's mechanical turk	mechanical turk;translation quality;wmt08 translation task;comprehension experiment;machine translation quality;non-expert annotators;machine translation;high quality reference translation;human-mediated translation;combined non-expert judgment;gold standard	Manual evaluation of translation quality is generally thought to be excessively time consuming and expensive. We explore a fast and inexpensive way of doing it using Amazon’s Mechanical Turk to pay small sums to a large number of non-expert annotators. For $10 we redundantly recreate judgments from a WMT08 translation task. We find that when combined non-expert judgments have a high-level of agreement with the existing gold-standard judgments of machine translation quality, and correlate more strongly with expert judgments than Bleu does. We go on to show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), to conduct reading comprehension experiments with machine translation, and to create high quality reference translations.	amazon mechanical turk;bleu;display resolution;experiment;high- and low-level;machine translation;the turk	Chris Callison-Burch	2009			natural language processing;speech recognition;gold standard;computer science;machine translation;algorithm	NLP	-24.675453750330977	-80.89692225040524	80804
679fad803478a0dfa80016efdfd8a409905446f9	efficient implementation of the room simulator for training deep neural network acoustic models		In this paper, we describe how to efficiently implement an acoustic room simulator to generate large-scale simulated data for training deep neural networks. Even though Google Room Simulator in [1] was shown to be quite effective in reducing the Word Error Rates (WERs) for far-field applications by generating simulated far-field training sets, it requires a very large number of Fast Fourier Transforms (FFTs) of large size. Room Simulator in [1] used approximately 80 percent of Central Processing Unit (CPU) usage in our CPU + Graphics Processing Unit (GPU) training architecture [2]. In this work, we implement an efficient OverLap Addition (OLA) based filtering using the open-source FFTW3 library. Further, we investigate the effects of the Room Impulse Response (RIR) lengths. Experimentally, we conclude that we can cut the tail portions of RIRs whose power is less than 20 dB below the maximum power without sacrificing the speech recognition accuracy. However, we observe that cutting RIR tail more than this threshold harms the speech recognition accuracy for rerecorded test sets. Using these approaches, we were able to reduce CPU usage for the room simulator portion down to 9.69 percent in CPU/GPU training architecture. Profiling result shows that we obtain 22.4 times speed-up on a single machine and 37.3 times speed up on Googleu0027s distributed training infrastructure.	acoustic cryptanalysis;artificial neural network;central processing unit;deep learning;experiment;graphics processing unit;maximum power transfer theorem;microsoft word for mac;open-source software;profiling (computer programming);simulation;speech recognition;websphere optimized local adapters	Chanwoo Kim;Ehsan Variani;Arun Narayanan;Michiel Bacchiani	2018		10.21437/Interspeech.2018-2566	cpu time;simulation;graphics processing unit;architecture;artificial neural network;filter (signal processing);profiling (computer programming);speedup;central processing unit;computer science	NLP	-22.27490235930476	-88.34206698872282	80813
b97638e1acff5fe1b5937e042b882db856952fb7	generative modeling of voice fundamental frequency contours	voice fundamental frequency contour expectation maximization algorithm fujisaki model prosody;ieee transactions;speech processing;speech;computational modeling;parameter extractor voice fundamental frequency contour generative modeling prosodic feature extraction raw speech data fujisaki model mathematical model vocal fold vibration probabilistic model discrete time stochastic process parameter estimation framework statistical model based text to speech synthesizer emotion recognition speaker identification speech conversion dialogue system;hidden markov models;expectation maximization algorithm;voice fundamental frequency contour;mathematical model;speech hidden markov models mathematical model speech processing ieee transactions computational modeling data models;prosody;fujisaki model;stochastic processes feature extraction mathematical analysis probability speech processing speech synthesis statistical analysis;data models	This paper introduces a generative model of voice fundamental frequency (F0) contours that allows us to extract prosodic features from raw speech data. The present contour model is formulated by translating the Fujisaki model, a well-founded mathematical model representing the control mechanism of vocal fold vibration, into a probabilistic model described as a discrete-time stochastic process. There are two motivations behind this formulation. One is to derive a general parameter estimation framework for the Fujisaki model that allows the introduction of powerful statistical methods. The other is to construct an automatically trainable version of the Fujisaki model that we can incorporate into statistical-model-based text-to-speech synthesizers in such a way that the Fujisaki-model parameters can be learned from a speech corpus in a unified manner. It could also be useful for other speech applications such as emotion recognition, speaker identification, speech conversion and dialogue systems, in which prosodic information plays a significant role. We quantitatively evaluated the performance of the proposed Fujisaki model parameter extractor using real speech data. Experimental results revealed that our method was superior to a state-of-the-art Fujisaki model parameter extractor.	dialog system;emotion recognition;estimation theory;expectation–maximization algorithm;forward–backward algorithm;fujisaki model;generative modelling language;generative model;mathematical model;randomness extractor;semantic prosody;speaker recognition;speech corpus;speech synthesis;statistical model;stochastic process;viterbi algorithm	Hirokazu Kameoka;Kota Yoshizato;Tatsuma Ishihara;Kento Kadowaki;Yasunori Ohishi;Kunio Kashino	2015	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2015.2418576	natural language processing;data modeling;speech recognition;expectation–maximization algorithm;computer science;speech;pattern recognition;mathematical model;speech processing;linguistics;prosody;computational model	NLP	-19.063737441621058	-93.31499194358203	80883
ca644b8bdbea16ce1b12381dbe0895e0fb4f00f2	nonlinear predictive models: overview and possibilities in speaker recognition	special emphasis;nonlinear predictive model;brief overview;speaker recognition;nonlinear feature extraction;main challenge;non-linear model;neural net	In this paper we give a brief overview of speaker recognition with special emphasis on nonlinear predictive models, based on neural nets. Main challenges and possibilities for nonlinear feature extraction are described, and experimental results of several strategies are provided. This paper is presented as a starting point for the non-linear model for speaker recognition.	speaker recognition	Marcos Faúndez-Zanuy;Mohamed Chetouani	2005		10.1007/978-3-540-71505-4_10	speech recognition;engineering;machine learning;pattern recognition	AI	-16.359626248320218	-97.80400355982471	80889
b988c6917a8b1dfcab7c5f2cb29d8666fc610c68	discriminative boosting regression backend for phonotactic language recognition	discriminative boosting regression dbr language recognition;language recognition;discriminative boosting regression dbr;test utterance discriminative boosting regression backend phonotactic language recognition spoken language recognition slr discriminative vector space model vsm training boosting variation dbr national institute of standards and technology language recognition evaluation task nist lre task equal error rate eer;support vector machines learning artificial intelligence regression analysis speech recognition;training distributed bragg reflectors boosting nist databases error analysis support vector machines	In spoken language recognition (SLR), discriminatively trained models always outperform non-discriminative models but computationally expensive and complex to implement. In this paper, we explore a novel approach to discriminative vector space model (VSM) training by using a boosting regression framework, in which an ensemble of VSMs is trained sequentially. The effectiveness of our boosting variation comes from the emphasis on working with the high confidence test data to achieve discriminatively trained models. Our variant of boosting also includes utilizing original training data in VSM training. The discriminative boosting regression (DBR) is applied to the National Institute of Standards and Technology (NIST) language recognition evaluation (LRE) 2009 task and show performance improvements. The experimental results demonstrate that the proposed DBR shows 4.13%, 14.38% and 14.22% relative reduction for 30s, 10s and 3s test utterances in equal error rate (EER) than baseline system.	analysis of algorithms;baseline (configuration management);boosting (machine learning);discriminative model;distributed bragg reflector;enhanced entity–relationship model;language identification;test data;viable system model	Wei-Wei Liu;Wei-Qiang Zhang;Jia Liu	2014	The 9th International Symposium on Chinese Spoken Language Processing	10.1109/ISCSLP.2014.6936600	speech recognition;computer science;machine learning;pattern recognition	NLP	-17.414634450405863	-90.38619316745005	80892
bfabf7d332220cb0424cee5bcfdf6360dc29ecff	intégration de la reconnaissance des entités nommées au processus de reconnaissance de la parole		We are interested in the recognition of named entities for the speech modality. Some difficulties may arise for this task due to speech processing. In this work, we propose to study the tight pairing between the speech recognition task and the named entity recognition task. For that purpose, we take away the basic functionnalities of a speech recognition system to turn it into a named entity recognition system. Therefore, by mobilising the inherent knowledge of the speech processing to the named entity recognition task, we ensure a better synergy between the two tasks. This leads to a significative rise in the quality of the named entity recognition task. MOTS-CLÉS : reconnaissance des entités nommées, parole, modèle de langage.	linear algebra;modality (human–computer interaction);named entity;speech processing;speech recognition;synergy	Mohamed Hatmi;Christine Jacquin;Sylvain Meignier;Emmanuel Morin;Solen Quiniou	2013	TAL		psychology;speech recognition;artificial intelligence;communication	NLP	-29.80729750759809	-86.36981657120309	80978
d33cd1984cd96f7da81f2a415c4fa01d11c3adac	spoken language understanding method using confidence measure and dialogue history	speech recognition	In the real environment, it is hard for a speech recognizer to avoid misrecognitions completely. However, if misrecognitions occur, user’s intentions are usually misunderstood by a conventional language understanding technique, which simply gives priority to the higher rank hypothesis of a speech recognition result (N-best). The utterances in a dialogue are coherent and correct user’s intentions might appear in the lower rank hypothesis of N-best. To understand user’s speech intentions in the real environment, we propose the language understanding technique that utilizes the dialogue context and confidence measure, which is the word posterior probability. The experimental results show that proposed technique is more efficient (about 15%) than the conventional technique. © 2007 Wiley Periodicals, Inc. Syst Comp Jpn, 38(9): 21–31, 2007; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/scj.20758	automotive navigation system;coherence (physics);computer user satisfaction;digi-comp i;finite-state machine;john d. wiley;natural language understanding;simulation;speech recognition;speech synthesis	Noriki Fujiwara;Toshihiko Itoh;Kenji Araki;Atsuhiko Kai;Tatsuhiro Konishi;Yukihiro Itoh	2007	Systems and Computers in Japan	10.1002/scj.20758	natural language processing;speech recognition;computer science;artificial intelligence	NLP	-25.786826261117724	-86.83421513334365	81138
409d237eeefd71d13dcb345b6ac318f4225486ef	the sovideo mandarin chinese broadcast news retrieval system	broadcast news;large vocabulary continuous speech recognition;information retrieval;spoken document retrieval;mandarin chinese;speech recognition	This paper describes the SoVideo broadcast news retrieval system for Mandarin Chinese. The system is based on technologies such as large-vocabulary continuous speech recognition for Mandarin Chinese, automatic story segmentation, and information retrieval. Currently, the database consists of 177 hours of broadcast news, which yields 3264 stories by automatic story segmentation. We discuss the development of the retrieval system, and the evaluation of each component and the retrieval system.	information retrieval;speech recognition;super robot monkey team hyperforce go!;vocabulary	Hsin-Min Wang;Shih-Sian Cheng;Yong-cheng Chen	2004	I. J. Speech Technology	10.1023/B:IJST.0000017017.57574.42	natural language processing;speech recognition;mandarin chinese;computer science;linguistics	Web+IR	-22.22332876533105	-84.15876427442316	81228
d91b307f52ebb0da271e86e5121122067c347728	detection of recognition errors based on classifiers trained on artificially created data	publications detection of recognition errors based on classifiers trained on artificially created data;katedra kybernetiky;kybernetika;informacni a řidici systemy;automaticke řizeni;uměla inteligence;publikace detection of recognition errors based on classifiers trained on artificially created data	This paper wishes to contribute to the solution of the problem occurring when an automatic speech recognition system does not recognize an input utterance correctly. The solution is usually based on a utilization of a confidence measure (CM) which is assigned to each recognized word and which informs a user or a higher level module on the belief that the recognized word has been really said. The task becomes more difficult if the vocabulary contains acoustically similar words which differ for example only in one phoneme. To cope with this problem, we introduced a new confidence measure based on our previous experiments. The basic elementary unit for which the presented CM is investigated is a phone. The first part of the article shortly describes the used speech recognition system and the previously used confidence measures. The main part of this article deals with description of creation of the new CM based on utilization of artificially created training data and also with description of the used classification features and the classifiers based on this CM. A rejection technique based on the described CM was evaluated on the Czech yellow-pages database. Experimental results show that the proposed rejection technique achieves approximately 5% equal error rate (ERR) for phone rejection and about 6-16% EER for word rejection.	artificial neural network;bit error rate;confusion matrix;enhanced entity–relationship model;estimation theory;experiment;institute for operations research and the management sciences;n-gram;rejection sampling;speech recognition;triphone;vocabulary	Tomás Bartos;Ludek Müller	2005			speech recognition;machine learning;pattern recognition	NLP	-19.301097092233693	-85.96069027727805	81289
29de9b196dbabe5779d88635a5a2d417225918b8	transfer learning for plda-based speaker verification		Abstract Currently, the majority of the state-of-the-art speaker verification systems are based on i-vector and PLDA; however, PLDA requires a huge volume of development data from multiple different speakers. This makes it difficult to learn PLDA parameters for a domain with scarce data. In this paper, we study and extend an effective transfer learning method based on Bayesian joint probability, in which the Kullback–Leibler (KL) divergence between the source domain and the target domain is added as a regularization factor. This method utilizes the development data from the source domain to help find the optimal PLDA parameters for the target domain. Specifically, speaker verification of short utterances can be viewed as a task in the domain with a limited amount of long utterances. Therefore, transfer learning for PLDA can also be adopted to learn discriminative information from other domains with a great deal of long utterances. Experimental results based on the NIST SRE and Switchboard corpus demonstrate that the proposed method offers a significant performance gain when compared with the traditional PLDA.	speaker recognition	Qingyang Hong;Lin Li;Jun Zhang;Lihong Wan;Huiyang Guo	2017	Speech Communication	10.1016/j.specom.2017.05.004	discriminative model;artificial intelligence;transfer of learning;speech recognition;computer science;joint probability distribution;pattern recognition;nist;bayesian probability	NLP	-16.995259594397687	-91.3823980364275	81490
f6dad3486617c4ef3b08b41dd54b99a36cd781e6	3-d cnn models for far-field multi-channel speech recognition		Automatic speech recognition (ASR) in far-field reverberant environments, especially when involving natural conversational multiparty speech conditions, is challenging even with the state-of-the-art recognition methodologies. The two main issues are artifacts in the signal due to reverberation and the presence of multiple speakers. In this paper, we propose a three dimensional (3-D) convolutional neural network (CNN) architecture for multi-channel far-field ASR. This architecture processes time, frequency & channel dimensions of the input spectrogram to learn representations using convolutional layers. Experiments are performed on the REVERB challenge LVCSR task and the augmented multi-party (AMI) LVCSR task using the array microphone recordings. The proposed method shows improvements over the baseline system that uses beamforming of the multi-channel audio along with a 2-D conventional CNN framework (absolute improvements of 1.1 % over the beamformed baseline system on AMI dataset).	artifact (error);artificial neural network;baseline (configuration management);beamforming;convolutional neural network;microphone;spectrogram;speech analytics;speech recognition	Sriram Ganapathy;Vijayaditya Peddinti	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461580	convolutional neural network;architecture;microphone;artificial neural network;computer science;spectrogram;beamforming;hidden markov model;speech recognition;communication channel	Robotics	-14.815153220343271	-90.03318324146883	81512
1b0bb3a18ec47808a4d2c60fb645b1468e75eb61	high-toned [il] in korean: phonetics, intonational phonology, and sound change	voice quality;seoul korean;high toned il;intonational phonology;sound change;accentual phrase	This study investigates recent changes in Korean intonation where an Accentual Phraseinitial [il] syllable is produced with a High tone by some speakers, introducing an exception to the model of intonational phonology of Seoul Korean (Jun 1993, 1996, 2006a). Data from eighty speakers of Seoul Korean born between 1952 and 1990 show that this phenomenon, found most often when [il] means ‘Number 1’, is employed by Seoul speakers born since 1970, and is not triggered by glottalization at vowel onset. It is proposed that enhancing a perceptual distinction between two similar-sounding morphemes is one of the major motivations for this phenomenon, and various factors affecting this High-on-[il] phenomenon are examined. In the discussion, the tonal change on [il] is compared with the changes in the VOT values in Korean stops which have been claimed to have been initiated by the same age groups in the same dialect. Finally, individual variation and the implications of this phenomenon for the model of intonational phonology of Seoul Korean are discussed.	automatic sounding;cjk characters;onset (audio);syllable;wiki	Sun-Ah Jun;Jihyeon Cha	2015	J. Phonetics	10.1016/j.wocn.2015.05.002	psychology;speech recognition;phonation;philosophy;linguistics;sociology;communication	HCI	-11.075937865005125	-81.70306263414489	81660
1beccd6fda7d1ad0a89d2b5e65900d0e1eee4c69	hmm-based system for recognizing words in historical arabic manuscript	hmms;fourier transform;arabic character recognition;off line;cursive text	This paper presents an omni-font Arabic word recognition system. The system is based on multiple Hidden Markov Models (HMMs). Each word in the lexicon is represented with a distinct HMM. The proposed system first extracts a set of spectral features from word images, then uses those features to tune HMM parameters. The performance of the proposed system is assessed using a corpus that includes both handwritten and computer-generated scripts. The likelihood probability of the input pattern is calculated against each word model and the pattern is assigned to the model with the highest probability.		Mohammad S. Khorsheed	2007	I. J. Robotics and Automation	10.2316/Journal.206.2007.4.206-3000	natural language processing;fourier transform;speech recognition;computer science;pattern recognition	Robotics	-18.317996546690956	-82.97566293874628	81684
b992efe46ed9a8f328f25fcb5b879d3295be2375	hierarchical prosody conversion using regression-based clustering for emotional speech synthesis	modelizacion;classification and regression tree;processus gauss;evaluation performance;pattern clustering;pitch modeling;prosody conversion classification;regression tree;performance evaluation;speech synthesis;learning;decoding;gaussian processes;pitch acoustics;regression based clustering method;hierarchized structure;linguistic prosody features;frase;evaluacion prestacion;speech analysis;discrete legendre polynomial coefficients;symbolic prosody features;database;base dato;objective evaluations;analisis objetivos;hierarchical prosody conversion;structure hierarchisee;speech coding;tonie;polinomio legendre;source pitch contour;legendre polynomial;polynomials;aprendizaje;modelisation;feature vector;pitch contour decoding;sentence;apprentissage;gaussian mixture model;evaluation subjective;analisis regresion;speech synthesis gaussian processes pattern clustering regression analysis speech coding;regression based clustering;gaussian mixture models;clustering method;altura sonida;prosodie;spatial databases;prosody conversion;signal classification;small sized emotional parallel speech databases;base de donnees;classification signal;signal acoustique;councils;analyse regression;emotional speech synthesis;hierarchical prosodic structure;sentence word level conversion;regression analysis;teoria mezcla;sintesis palabra;phrase;acoustic signal;classification tree analysis;gaussian process;computer science;classification automatique;regression based clustering emotional speech synthesis hierarchical prosodic structure prosody conversion;polynome legendre;prosody;proceso gauss;clustering methods;automatic classification;speech synthesis clustering methods polynomials spatial databases decoding classification tree analysis regression tree analysis speech analysis councils computer science;subjective evaluation;mixture theory;subsyllable level;modeling;clasificacion automatica;subjective evaluations;theorie melange;analyse objective;pitch feature vectors;estructura jerarquizada	This paper presents an approach to hierarchical prosody conversion for emotional speech synthesis. The pitch contour of the source speech is decomposed into a hierarchical prosodic structure consisting of sentence, prosodic word, and subsyllable levels. The pitch contour in the higher level is encoded by the discrete Legendre polynomial coefficients. The residual, the difference between the source pitch contour and the pitch contour decoded from the discrete Legendre polynomial coefficients, is then used for pitch modeling at the lower level. For prosody conversion, Gaussian mixture models (GMMs) are used for sentence- and prosodic word-level conversion. At subsyllable level, the pitch feature vectors are clustered via a proposed regression-based clustering method to generate the prosody conversion functions for selection. Linguistic and symbolic prosody features of the source speech are adopted to select the most suitable function using the classification and regression tree for prosody conversion. Three small-sized emotional parallel speech databases with happy, angry, and sad emotions, respectively, were designed and collected for training and evaluation. Objective and subjective evaluations were conducted and the comparison results to the GMM-based method for prosody conversion achieved an improved performance using the hierarchical prosodic structure and the proposed regression-based clustering method.	cluster analysis;coefficient;contour line;database;decision tree learning;google map maker;holomatix rendition;legendre polynomials;mixture model;pitch (music);polynomial;semantic prosody;speech processing;speech synthesis;statistical classification;top-down and bottom-up design	Chung-Hsien Wu;Chi-Chun Hsia;Chung-Han Lee;Mai-Chun Lin	2010	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2009.2034771	natural language processing;speech recognition;computer science;machine learning;pattern recognition;mixture model;gaussian process;speech synthesis;statistics	NLP	-16.972071475185704	-84.90295192117843	81694
ed28e247d242dfb50dfc1747271f73e5d2e980ca	improving speech recognition of two simultaneous speech signals by integrating ica bss and automatic missing feature mask generation	missing feature theory;missing feature mask;robot audition;independent component analysis;indexing terms;feature vector;automatic speech recognition;speech recognition;single input multiple output;frequency domain;ica	Robot audition systems require capabilities for sound source separation and the recognition of separated sounds, since we hear a mixture of sounds in our daily lives, especially mixed of speech. We report a robot audition system with a pair of omni-directional microphones embedded in a humanoid that recognizes two simultaneous talkers. It first separates the sound sources by Independent Component Analysis (ICA) with the single-input multiple-output (SIMO) model. Then, spectral distortion in the separated sounds is then estimated to generate missing feature masks. Finally, the separated sounds are recognized by missing-feature theory (MFT) for Automatic Speech Recognition (ASR). The novel aspects of our system involve estimates of spectral distortion in the temporalfrequency domain in terms of feature vectors and based on estimates error in SIMO-ICA signals. The resulting system outperformed the baseline robot audition system by 7 %.	baseline (configuration management);covox speech thing;distortion;embedded system;independent computing architecture;independent component analysis;mask data preparation;media foundation;microphone;robot;source separation;speech recognition	Ryu Takeda;Shun'ichi Yamamoto;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno	2006			voice activity detection;independent component analysis;speech recognition;index term;feature vector;feature;computer science;machine learning;pattern recognition;speech processing;frequency domain	ML	-12.061717942071464	-90.80279210458338	81724
1e61c61cdb89080d3d10f22a58fa797ed9c3ef85	query by tapping system based on alignment algorithm	databases;dynamic programming;microphones;elementary operator;keyboards;rhythm;music information retrieval signal analysis content based retrieval databases rhythm microphones dynamic programming keyboards mobile handsets information analysis;query by tapping systems;rhythmic pattern;query processing;information retrieval;mirex 2008 database;probability density function;signal analysis;acoustic signal processing;dynamic program;event detection;data mining;noise measurement;transient analysis;estimation;content based music retrieval systems;music information retrieval;mobile handsets;audio signals;audio samples;alignment algorithm;kurtosis based analysis method alignment algorithm query by tapping systems content based music retrieval systems microphone rhythmic pattern melodic similarity estimation audio signals audio samples mirex 2008 database;kurtosis based analysis method;dynamic programming music information retrieval acoustic signal analysis;acoustic signal analysis;query processing acoustic signal processing content based retrieval;information analysis;content based retrieval;music;melodic similarity estimation;content based music retrieval;microphone	Query-by-tapping systems are content-based music retrieval systems that allow users to tap or clap in a microphone the rhythmic pattern of the melody requested. In this paper, a new query-by-tapping system is described. This system is based on adaptations of alignment algorithms successfully applied for melodic similarity estimation. A similarity score is computed according to the cost of the elementary operations necessary to transform the query into the musical piece tested. A new method for extracting the rhythmic pattern from audio signals is also presented. This method is based on the analysis of the variations of the kurtosis computed from the audio samples. Experiments performed on the MIREX 2008 database show that the alignment technique proposed performs better than the participating algorithms. They also confirm the interest of the kurtosis-based analysis method in the case of noisy percussive queries.	algorithm;microphone	Pierre Hanna;Matthias Robine	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4959975	estimation;probability density function;speech recognition;computer science;noise measurement;rhythm;dynamic programming;signal processing;music;mathematics;multimedia;information retrieval;statistics	Visualization	-7.824533324269264	-92.76785603263328	81753
03d47b494c90554108785f3c562f49219449fe5f	cepstral analysis of speech signals in the process of automatic pathological voice assessment		The paper describes the problem of cepstral speech analysis in the process of automated voice disorder probability estimation. The author proposes to derive two of the most diagnostically significant voice features: quality of harmonic structure and degree of subharmonic from cepstrum of speech signal. Traditionally, these attributes are estimated auricularly or by spectrum (or spectrogram) observation, hence this analysis often lacks accuracy and objectivity. The introduced parameters were calculated for the recordings from Disordered Voice Database (Kay, model 4337 version 2.7.0) which consists of 710 voice samples (657 pathological, 53 healthy) recorded in the laboratory environment and described with diagnosis and a number of additional attributes (such as age, sex, nationality). The proposed cepstral voice features were compared to similar voice parameters derived from Multidimensional Voice Program (Kay, model 5105 version 2.7.0) in respect to their diagnostic significance and presented graphically. The results show that cepstral features are more correlated with decision and better discriminate clusters of healthy and disordered voices. Additionally, both parameters are obtained by single cepstral transform and do not require to perform F0 tracking earlier as it is derived simultaneously.	cepstrum;objectivity/db;spectrogram;voice analysis	Anna Samborska-Owczarek	2007	Annales UMCS, Informatica		computer science;subharmonic;pathological;voice analysis;harmonic;speech recognition;cepstrum	Vision	-6.4081284905133735	-85.9619351419494	81785
37fa78b03f8d0a2814ff3a2cfd944809ce7195e8	reversed speech comprehension depends on the auditory efferent system functionality	time reversal	In the present study we explore the implication of high and low level mechanisms in degraded (time-reversed) speech comprehension in normal hearing subjects. In experiment 1 we compared the loss of intelligibility due to the increasing size of reversion windows in both words and pseudowords. Results showed that words are generally reconstructed better than pseudowords, suggesting the existence of a lexical benefit in degraded speech restoration. Moreover, there was greater variability between individuals when reconstructing pseudowords than words. In experiment 2, we demonstrated that this interindividual variability correlated with the subjects’ medial olivocochlear bundle functionality, as measured by contralateral suppression of otoacoustic emissions (OAEs). Together these experiments highlight the importance of low-level auditory mechanisms in degraded speech restoration. Moreover they put forward the existence of major interindividual variability in the capacity to reconstruct degraded speech, which correlates with the physiological properties of the auditory system (low-level property). In addition, our results also suggest the existence of multiple higher-level strategies that can compensate on-line for the lack of information caused by speech degradation.	circuit restoration;elegant degradation;experiment;high- and low-level;intelligibility (philosophy);inter-rater reliability;medial graph;microsoft windows;online and offline;reversion (software development);spatial variability;speech synthesis;zero suppression	Claire-Léonie Grataloup;Michel Hoen;François Pellegrino;E. Veuillet;Lionel Collet;Fanny Meunier	2005			communication;speech recognition;auditory masking;efferent;otoacoustic emission;comprehension;computer science;auditory system	ML	-9.076043071432506	-82.70230102253929	81951
1a4f9fac75dcc8d5ed580c39c670debbe108fad6	using sub-word-level information for confidence estimation with conditional random field models		The task of word-level confidence estimation (CE) for automatic speech recognition (ASR) systems stands to benefit from the combination of suitably defined input features from multiple information sources. However, the information sources of interest may not necessarily operate at the same level of granularity as the underlying ASR system. The research described here builds on previous work on confidence estimation for ASR systems using features extracted from word-level recognition lattices, by incorporating information at the sub-word level. Furthermore, the use of Conditional Random Fields (CRFs) with hidden states is investigated as a technique to combine information for word-level CE. Performance improvements are shown using the sub-word-level information in linear-chain CRFs with appropriately engineered feature functions, as well as when applying the hidden-state CRF model at the word level.	automated system recovery;conditional random field;norm (social);speech recognition	Matthew Stephen Seigel;Philip C. Woodland	2012			speech recognition;crfs;granularity;machine learning;pattern recognition;artificial intelligence;conditional random field;computer science	NLP	-19.627047051527004	-81.62005114552353	81989
7273797b7490adc99cc03f556ded3b695d1933bd	f0 contour generation and synthesis using bengali hmm-based speech synthesis system	prosodic word prediction;speech synthesis;hbox f _ 0 f0 contour modification;prosodic phrase prediction;bengali hts;fujisaki model	HMM based Bengali speech synthesis system (Bengali-HTS) generates highly intelligible synthesized speech but its naturalness is not adequate even though it is trained with a very good amount of speech corpus. In case of interrogative, imperative and exclamatory sentences, naturalness of the synthesized speech falls drastically. This paper proposes a method to overcome this problem by modifying the $$\hbox {F}_{0}$$F0 contour of synthetic speech based on Fujisaki model. The Fujisaki model features for different types of Bengali sentences are analyzed for the generation of $$\hbox {F}_{0}$$F0 contour. These features depend on prosodic word/phrase boundary of the sentence. So a two layer supervised classification and regression tree is trained to predict the prosodic word/phrase boundary. Fujisaki model then generates $$\hbox {F}_{0}$$F0 contour from input text using the prosodic word/phrase boundary and segmental duration information from HMM-based speech synthesis system. Moreover, for HMM training purpose, prosodic structure of sentence has been employed rather than lexical structure. From MOS and preference test it is found that proposed method significantly improved the overall quality of synthesized speech than that of Bengali-HTS.	speech synthesis	Sankar Mukherjee;Shyamal Kumar Das Mandal	2015	I. J. Speech Technology	10.1007/s10772-014-9247-3	natural language processing;speech recognition;computer science;linguistics;speech synthesis	NLP	-19.440702439588996	-83.5544563684059	82062
101e81342697c714c44cc725216d35978d278cf0	a higher-order theory of presupposition	semantic theory;donkey anaphora;standard montague semantics;discourse semantics;higher-order theory;file change semantics;new higher-order theory;discourse representation theory;pronominal anaphora;crosssentential anaphora;montague semantics	So-called ‘dynamic’ semantic theories such as Kamp’s discourse representation theory and Heim’s file change semantics account for such phenomena as crosssentential anaphora, donkey anaphora, and the novelty condition on indefinites, but compare unfavorably with Montague semantics in some important respects (clarity and simplicity of mathematical foundations, compositionality, handling of quantification and coordination). Preliminary efforts have been made by Muskens and by de Groote to revise and extend Montague semantics to cover dynamic phenomena. We present a new higher-order theory of discourse semantics which improves on their accounts by incorporating a more articulated notion of context inspired by ideas due to David Lewis and to Craige Roberts. On our account, a context consists of a common ground of mutually accepted propositions together with a set of discourse referents preordered by relative salience. Employing a richer notion of contexts enables us to extend our coverage beyond pronominal anaphora to a wider range of presuppositional phenomena, such as the factivity of certain sententialcomplement verbs, resolution of anaphora associated with arbitrarily complex definite descriptions, presupposition ‘holes’ such as negation, and the independence condition on the antecedents of conditionals. Formally, our theory is expressed within a higher-order logic with natural number type, separation-style subtyping, and dependent coproducts parameterized by the natural numbers. The system of semantic types builds on proposals due to Thomason and to Pollard in which the type of propositions (static meanings of sentential utterances) is taken as basic and worlds are constructed from propositions (rather than the other way around as in standard Montague semantics).	anaphora (linguistics);montague grammar;theory;thomason collection of civil war tracts	Scott Martin;Carl Pollard	2012	Studia Logica	10.1007/s11225-012-9427-6	epistemology;computer science;artificial intelligence;mathematics;linguistics;programming language;algorithm	NLP	-33.194359726833255	-81.99747157320904	82161
a4fc9fb3933a5bffb5f8e803b41afcda2f575bb7	text readability and word distribution in japanese		This paper reports the relation between text readability and word distribution in the Japanese language. There was no similar study in the past due to three major obstacles: (1) unclear definition of Japanese “word”, (2) no balanced corpus, and (3) no readability measure. Compilation of the Balanced Corpus of Contemporary Written Japanese (BCCWJ) and development of a readability predictor remove these three obstacles and enable this study. First, we have counted the frequency of each word in each text in the corpus. Then we have calculated the frequency rank of words both in the whole corpus and in each of three readability bands. Three major findings are: (1) the proportion of high-frequent words to tokens in Japanese is lower than that in English; (2) the type-coverage curve of words in the difficult-band draws an unexpected shape; (3) the size of the intersection between high-frequent words in the easy-band and these in the difficult-band is unexpectedly small.	kerrison predictor	Satoshi Sato	2014			linguistics	NLP	-11.817416091075243	-80.46093072033342	82241
091126d38ed460cba3cc85b589d5474b65772741	phonetic-level mispronunciation detection in non-native swedish speech		This contribution presents part of the work initiated at the CTT for the development of speech tec hnology to assist non-native speakers learn Swedish. This study focuses mainly on the automatic location of mispronunciations at a phonetic level. We first describe the database we created for this work and then report on the reliability of several phonetic scores to automatically locate segmental problems in student utterances.		Philippe Langlais;Anne-Marie Öster;Björn Granström	1998			artificial intelligence;speech recognition;pattern recognition;computer science	NLP	-16.75091229461271	-82.92046465999891	82363
62c2c52782f58f15d0a681a22c3a68505c3aa9ea	string transformation-based bayesian classification or proteins	bayesian classification;classifier system;genetic algorithm;classifier systems;bioinformatics;markov chain	We describe a Markov chain Bayesian classification tool, SCS, that can perform data-driven classification of proteins and protein segments. Training data for interesting classification problems is often limited; thus, SCS uses string transformation functions to change the encoding of proteins to reduce problem perplexity and improve classification. A wrapper-based genetic algorithm is used to search the space of possible string transformation functions to find functions that improve classification.	bayesian network;genetic algorithm;markov chain;naive bayes classifier;perplexity;test set	Timothy Meekhof;Gary W. Daughdrill;Robert B. Heckendorn	2006		10.1145/1143997.1144050	markov chain;naive bayes classifier;genetic algorithm;computer science;machine learning;pattern recognition;data mining;mathematics;one-class classification	ML	-4.747664946981632	-90.05042060834882	82449
f833a201bcd4921e096949c240dff2219afd6d5b	an on-line adaptation technique for emotional speech recognition using style estimation with multiple-regression hmm	speech recognition;multiple regression	This paper describes a model adaptation technique for emotional speech recognition based on multiple-regression HMM (MR-HMM). We use a low-dimensional vector called style vector which corresponds the degree of expressivity of emotional speech as the explanatory variable of the regression. In the proposed technique, first, the value of the style vector for input speech is estimated. Then, using the estimated style vector, new mean vectors of the output distributions of HMM are adapted to the input style. The style vector is estimated every input utterance, and an on-line adaptation can be done in each utterance. We perform phoneme recognition experiments for professional narrators’ acted speech and evaluate the performance by comparing with style-dependent and style-independent HMMs. Experimental results show the proposed technique reduced the error rates by 11% of the style-independent model.	experiment;hidden markov model;online and offline;speech recognition	Yusuke Ijima;Makoto Tachibana;Takashi Nose;Takao Kobayashi	2008			artificial intelligence;speech recognition;hidden markov model;linear regression;speaker recognition;pattern recognition;computer science	ML	-16.945332893254857	-84.57516810246642	82531
bca1cf47d0729d824a7d9454189021f6077f7dce	modulation spectral features for predicting vocal emotion recognition by simulated cochlear implants		It has been reported that vocal emotion recognition is challenging for cochlear implant (CI) listeners due to the limited spectral cues with CI devices. As the mechanism of CI, modulation information is provided as a primarily cue. Previous studies have revealed that the modulation components of speech are important for speech intelligibility. However, it is unclear whether modulation information can contribute to vocal emotion recognition. We investigated the relationship between human perception of vocal emotion and the modulation spectral features of emotional speech. For human perception, we carried out a vocal-emotion recognition experiment using noisevocoder simulations with normal-hearing listeners to predict the response from CI listeners. For modulation spectral features, we used auditory-inspired processing (auditory filterbank, temporal envelope extraction, modulation filterbank) to obtain the modulation spectrogram of emotional speech signals. Ten types of modulation spectral feature were then extracted from the modulation spectrogram. As a result, modulation spectral centroid, modulation spectral kurtosis, and modulation spectral tilt exhibited similar trends with the results of human perception. This suggests that these modulation spectral features may be important cues for voice emotion recognition with noise-vocoded speech.	cochlear implant;emotion recognition;filter bank;intelligibility (philosophy);modulation;simulation;spectral centroid;spectrogram;vocoder	Zhi Zhu;Ryota Miyauchi;Yukiko Araki;Masashi Unoki	2016		10.21437/Interspeech.2016-737	speech recognition	ML	-9.532759690429383	-86.27336879490889	82556
be88c434868a4768295b5e5072cf8b6c7df29888	an evaluation of ensemble methods in handwritten word recognition based on feature selection	classifier ensemble;single classifier;difficult problem;feature selection;pattern recognition;handwritten word recognition;handwritten text recognition;recognition rate;ensemble methods;individual classifier;multiple classifier;basic classifier;hidden markov model;feature extraction;word recognition;hidden markov models	Handwritten text recognition is one of the most difficult problems in the field of pattern recognition. The combination of multiple classifiers has been proven to be able to increase the recognition rate in difficult problems when compared to single classifiers. In this paper, several novel methods for the creation of classifier ensembles are compared where the individual classifiers use different feature subsets. The methods are evaluated in the context of handwritten word recognition, using a hidden Markov model recognizer as basic classifier.	best, worst and average case;ensemble learning;experiment;feature selection;finite-state machine;heuristic;hidden markov model;markov chain;neural ensemble;optical character recognition;pattern recognition;random subspace method	Simon Günter;Horst Bunke	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1334133	random subspace method;speech recognition;feature;feature extraction;word recognition;computer science;intelligent word recognition;machine learning;pattern recognition;hidden markov model	Vision	-4.774827301066101	-89.47655122084178	82767
8a0337db2caa625765ff17912e7f1994a47fc835	does lexical stress or metrical stress better predict word boundaries in dutch?	speech intelligibility;automatic speech recognition;psychology;languages;white spaces;natural languages;english language;speech processing;stress;speech recognition;word segmentation;human factors;search space;linguistics;profitability;concrete	For both human and automatic speech recognizers it is difficult to segment continuous speech into discrete units such as words. Word segmentation is so hard because there seem to be no self-evident cues for word boundaries in the speech stream. However, it has been suggested that English listeners can profit from the occurrence of full vowels (i.e. vowels with metrical stress) in the speech stream to make a first good guess about the location of word boundaries. The CELEX-database study described here investigates whether such a strategy is also feasible for Dutch, and whether the occurrence of full vowels or the occurrence of vowels with primary word stress (i.e. vowels with lexical stress) is a better cue for word boundaries. The CELEX-counts suggest that for Dutch metrical stress seems to be a better predictor of word boundaries than lexical stress.	eur-lex;finite-state machine;kerrison predictor;speech synthesis;stress ball;text segmentation	David van Kuijk	1996				NLP	-11.816496644861585	-80.98125308366492	82802
7b04607f79be8404cfab69cdca388c963b274a28	investigations into vowel and consonant structures in articulatory and auditory spaces using laplacian eigenmaps	auditory space;speech processing laplace equations speech intelligibility;consonants;vowel consonant acoustic space vowel structures consonant structures articulatory spaces auditory spaces laplacian eigenmaps speech sound speech vowels speech production speech perception japanese reading speech labial groups lingual groups;speech production;articulatory space;auditory space speech production consonants vowels articulatory space;vowels;speech laplace equations manifolds acoustics periodic structures tongue	Many studies have investigated the relationship between the articulatory and auditory features for isolated speech sound and vowels. For fully understanding the mechanisms of speech production and perception, it is necessary to investigate the consonants in the same way. For this reason, in this study, we investigate the manifolds of vowels and consonants out of Japanese reading speech using Laplacian eigenmaps. We constructed uniform articulatory and auditory spaces based on the vowels and consonants to investigate their manifolds. It is found that the distribution of consonants in articulatory space could be classified into labial and lingual groups which reflected their articulatory properties, while in auditory space their distribution was clustered according to voiced and unvoiced, plosive and fricative properties. In vowel-consonant acoustic space, the consonants distributed as a hoe-like shape, with voiced consonants located on the blade of the hoe and fused with vowels. We defined average correlation coefficients to measure the similarity of manifold between three speakers. The results indicated that the vowel/consonant structures had high consistency among the three speakers.	acoustic cryptanalysis;coefficient;james hoe;laplacian matrix;nonlinear dimensionality reduction	Jianwu Dang;Shengbei Wang;Masashi Unoki	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472700	speech production;speech recognition;speech;manner of articulation	Robotics	-9.64763302878337	-83.71238493298439	82885
7bd9efec59d7cafa8044dc226614e9397706e6d9	disambiguation by information structure in drt	semantic ambiguity;high quality machine translation;expensive strategy;semantic analysis;information structure;clear preference;ambigous structure;ambigous german adverb erst;cases deep semantic analysis;alternative reading;machine translation	~i%xt understanding and high quality machine translation often necessitate the disambiguation of ambigous structures or lexical elements. Drawing inferences from the context can be a means for resolving semantic ambiguities. However, often, this is an ex-. pensive s trategy that, in addition, not always comes up with a clear preference for one of the alternatives. In this paper, we argue that in a number of cases deep semantic analyses can be avoided by taking into account the constraints that the alternative readings impose onto the information structure. To this end, we present a study of the arnbigous German adverb erst and point out the particular circumstances under which the given information structure disambiguates the adverb without further semantic analysis. 1 I n t r o d u c t i o n German erst is ambigous. Consider the following examples: (1) Peter zeigte erst auf die vierte Gliickszahl. a) Und dann auf die zweite. (Peter first pointed to the fourth lucky number. And then to the second.) b) Nicht zuvor auf die erste, zweite oder dlqtte. (Peter only pointed to the fourth lucky number. Not to the first, second or third.) c) Noeh nieht aufd ie Nnfte. (Peter only pointed to the fourth lucky number so far. Not yet to the fifth.) The alternative contexts a) e) determine the meaning of the first sentence of (1) according to *This paper describes research done within the Sonderforschunsbereich 3~0 at IMS. I would like to thank Anette Frank, Hans Kamp, Michael Schiehlen and the other members of the IMS semantics group for helpfull discussion. the disambiguating translations presented. ' rhe example testifies the following three uses of crsl: • In the context (1.a), the recipient understands the introduced event as the first of a sequence of events that he expects to be completed by the following text. We call this reading: the First of a Sequence-reading (FS). • In the context (1.b), the recipient understands erst as a signal of the speaker/wri ter that the occurrence of the reported event is not preceded by the occurrence of similar (alternative) events. We call this reading: the Exclusion of Preceding Alternatives-	display resolution;emoticon;machine translation;word-sense disambiguation	Kurt Eberle	1996			natural language processing;computer science;linguistics;machine translation;information retrieval	NLP	-30.997349888199846	-83.04587168297117	82906
7eb990a2200676d712388e3a0f341be280937c81	ltag semantics of np-coordination		A central component of Kallmeyer and Joshi 2003 is the idea that the contribution of a quantifier is separated into a scope and a predicate argument part. Quantified NPs are analyzed as multi-component TAGs, where the scope part of the quantifier introduces the proposition containing the quantifier, and the predicate-argument part introduces the restrictive clause. This paper shows that this assumption presents difficulties for the compositional interpretation of NP coordination structures, and proposes an analysis which is based on LTAG semantics with semantic unification, developed in Kallmeyer and Romero 2004.	mark steedman;propositional variable;quantifier (logic);tree-adjoining grammar;unification (computer science);word-sense disambiguation	Olga Babko-Malaya	2004			programming language;semantics;mathematics	NLP	-33.21976614697234	-81.84179014575494	82983
6b4f26df13afe861f28a339de0811050a72e7bd5	towards unsupervised articulatory resynthesis of german utterances using ema data	speech synthesis;indexing terms;conference paper;text to speech	As part of ongoing research towards integrating an articulatory synthesizer into a text-to-speech (TTS) framework, a corpus of German utterances recorded with electromagnetic articulography (EMA) is resynthesized to provide training data for statistical models. The resynthesis is based on a measure of similarity between the original and resynthesized EMA trajectories, weighted by articulatory relevance. Preliminary results are discussed and future work outlined.	relevance;speech synthesis;statistical model;text corpus;unsupervised learning	Ingmar Steiner;Korin Richmond	2009			natural language processing;speech recognition;index term;computer science;speech synthesis	NLP	-16.81492438699426	-85.24256656490532	83207
595e07718943e9d670781167e7e43a0d603d5444	on second-order statistics and linear estimation of cepstral coefficients	second order;least mean squares methods cepstral analysis speech recognition statistical analysis parameter estimation noise gaussian processes random processes matrix algebra covariance analysis;linear estimation;covariance analysis;order statistic;least mean squares methods;modelo markov;gaussian random variable;gaussian processes;reconocimiento palabra;signal sampling;hidden markov model;statistical independence;speech processing;statistique ordre;estimacion lineal;erreur quadratique moyenne;additive noise;tratamiento palabra;ruido aditivo;traitement parole;orden 2;matrice diagonale;matrice covariance;bruit additif;matriz covariancia;cepstral statistic;matrix algebra;indexing terms;linear minimum mean square error;estimation lineaire;cepstral analysis;markov model;statistical analysis;matriz diagonal;cross covariance;mean square error;random processes;estadistica orden;statistics cepstral analysis additive noise covariance matrix vectors discrete fourier transforms gaussian noise signal processing speech recognition speech enhancement;speech recognition;covariance croisee;statistique cepstrale;english digits second order statistics linear estimation cepstral coefficients explicit expressions noisy signal waveform clean signal waveform additive noise statistically independent complex gaussian random variables cross covariance log periodograms covariance matrix signal samples signal independent matrix diagonal matrix linear minimum mean square error estimator noisy cepstral components recognition results;ordre 2;reconnaissance parole;parameter estimation;error medio cuadratico;modele markov;second order statistics;noise;covariance matrix;diagonal matrix	Explicit expressions for the second-order statistics of cepstral components representing clean and noisy signal waveforms are derived. The noise is assumed additive to the signal, and the spectral components of each process are assumed statistically independent complex Gaussian random variables. The key result developed here is an explicit expression for the cross-covariance between the log-periodograms of the clean and noisy signals. In the absence of noise, this expression is used to show that the covariance matrix of cepstral components representing N signal samples, is a fixed signal independent matrix, which approaches a diagonal matrix at a rate of 1/N. In addition, the cross-covariance expression is used to develop an explicit linear minimum mean square error estimator for the clean cepstral components given noisy cepstral components. Recognition results on the English digits using the fixed covariance and linear estimator are presented.	cepstrum;coefficient	Yariv Ephraim;Mazin G. Rahim	1999	IEEE Trans. Speech and Audio Processing	10.1109/89.748121	normal distribution;independence;covariance matrix;cross-covariance;order statistic;speech recognition;index term;analysis of covariance;noise;pattern recognition;gaussian process;speech processing;mathematics;mean squared error;markov model;estimation theory;diagonal matrix;second-order logic;hidden markov model;statistics	EDA	-20.104633622293527	-92.52569747224852	83230
23ace9295d004d91ed68d4b6fe0dd515402161c5	phonetic name matching for cross-lingual spoken sentence retrieval	audio segmentation;query processing;information retrieval;speech processing;information retrieval speech recognition;language translation;automatic speech recognition;speech recognition fuzzy systems language translation natural language processing pattern matching query processing speech processing;automatic speech recognition decoding information retrieval error analysis broadcasting natural languages speech recognition content based retrieval text recognition pipelines;entity translation phonetic name matching cross lingual spoken sentence retrieval oov words fuzzy matching query names candidate audio segments word decoding errors automatic speech recognition mandarin english clssr machine translation;pattern matching;speech recognition;natural language processing;fuzzy systems;machine translation	Cross-lingual spoken sentence retrieval (CLSSR) remains a challenge, especially for queries including OOV words such as person names. This paper proposes a simple method of fuzzy matching between query names and phones of candidate audio segments. This approach has the advantage of avoiding some word decoding errors in automatic speech recognition (ASR). Experiments on Mandarin-English CLSSR show that phone-based searching and conventional translation-based searching are complementary. Adding phone matching achieved 26.29% improvement on F-measure over searching on state-of-the-art machine translation (MT) output and 8.83% over entity translation (ET) output.	machine translation;speech recognition;super robot monkey team hyperforce go!	Heng Ji;Ralph Grishman;Wen Wang	2008	2008 IEEE Spoken Language Technology Workshop	10.1109/SLT.2008.4777895	natural language processing;cache language model;audio mining;speech recognition;speech corpus;word error rate;computer science;pattern matching;pattern recognition;speech processing;machine translation;language model;speech analytics	NLP	-22.116659065397506	-83.57302133280028	83333
1a43698ab3e36b93c1b05bc5937e765a56b267b5	variance and invariance in speech rate as a reflection of conceptual planning		This study investigates the variance and invariance in speech rate as a reflection of conceptual planning and cognitive rhythm. A four-frame comic strip was used to elicit speech and low-pass smoothing was done afterwards to filter out high frequency noise. Results showed that subjects invariably planned narration in terms of story plots. Variance lies in the way subjects synchronize planning and execution stages. Some tended to start the execution stage before the planning stage ends while others were inclined to speak only after macroplanning was done. Lexical retrieval failure is one of the main causes for disruptive story-plot-based temporal cycle.	low-pass filter;smoothing	Janice Fon	1998			invariant (physics);speech recognition;comics;smoothing;computer science;rhythm;narrative;cognition	AI	-8.995400192351754	-81.65444569295707	83348
72a6ad17fac5db042bede69b7e2001d4d621852e	investigating speaker features from very short speech records	performance measure;digital signal processing;time varying;prediction error;speech synthesis;speech processing;speech analysis;prediction theory speaker recognition feature extraction adaptive filters time varying filters;vocal tract;speaker recognition;adaptive filters;prediction theory;speaker identity verification;feature extraction;dissertation;speaker identity verification speaker features short speech records adaptive filtering convergence nonstationarities vocal tract dynamics vocal cord dynamics glottal signal estimates time varying filter coefficients prediction error signal glottal characteristics;speech analysis adaptive filters speech synthesis anatomy filtering detectors feature extraction speech coding tongue algorithm design and analysis;time varying filters;adaptive filter	"""A procedure is presented that is capable of extracting various speaker features, and is of particular value for analyzing records containing single words and shorter segments of speech. By taking advantage of the fast convergence properties of adaptive filtering, the approach is capable of modeling the nonstationarities due to both the vocal tract and vocal cord dynamics. Specifically, the procedure extracts the vocal tract estimate from within the closed glottis interval and uses it to obtain a time-domain glottal signal. This procedure is quite simple, requires minimal manual intervention (in cases of inadequate pitch detection), and is particularly unique because it derives both the vocal tract and glottal signal estimates directly from the time-varying filter coefficients rather than from the prediction error signal. Using this procedure, several glottal signals are derived from human and synthesized speech and are analyzed to demonstrate the glottal waveform modeling performance and kind of glottal characteristics obtained therewith. Finally, the procedure is evaluated using automatic speaker identity verification. Acknowledgments This work was largely inspired by summer internships with Bellcore and Bell Atlantic, under the mentorship of Dr. Timothy Feustel and Mr. Alex McAllister who provided an exciting introduction into the professional world of speech processing. Financial support was also provided through a fellowship from the Bradley Endowment, established by the late Mrs. Marion Bradley Via and continued by Mr. Edward Via. Their generosity enabled the pursuit of this very interesting research topic. Finally, employment with Agilent Technologies/Hewlett Packard allowed this work to continue until completion. Perhaps the biggest contributers, in terms of their valuable time spent reviewing this document and providing exceptional coaching thoughout the duration of this research, would be the Ph.D. committee members Dr. Ball, Dr. Jacobs, Dr. Reed, Dr. VanLandingham and, most of all, from committee chairman Dr. Beex. The suggestions and meticulous attention to detail provided by Dr. Beex have contributed immeasurably to the quality of this work. But even more valuable is the exceptional education provided by these professors and others at Virginia Tech. A special thanks to Dean (formerly Department Chair) and Mrs. Stephenson for their hospitality and for making the author's family feel at home at Virginia Tech. The author would like to thank his parents, Gerald and Linda, for their endless support, for instilling the importance of education from day one, and the confidence required to pursue it and """" never give up """". The author would …"""	adaptive filter;coefficient;david w. bradley;day one;dr. sbaitso;dr. web;identity verification service;linda (coordination language);pitch detection algorithm;speech processing;speech synthesis;tract (literature);waveform	B. LaRoy Berg;A. A. Beex	1999		10.1109/ISCAS.1999.778795	adaptive filter;speaker recognition;speech recognition;computer science;speech processing;speech synthesis	ML	-5.762064487088526	-95.79247329212498	83437
b07c4d70c8f7a0c1ee321c8130bb60cf87f76989	audio-replay attack detection countermeasures		This paper presents the Speech Technology Center (STC) replay attack detection systems proposed for Automatic Speaker Verification Spoofing and Countermeasures Challenge 2017. In this study we focused on comparison of different spoofing detection approaches. These were GMM based methods, high level features extraction with simple classifier and deep learning frameworks. Experiments performed on the development and evaluation parts of the challenge dataset demonstrated stable efficiency of deep learning approaches in case of changing acoustic conditions. At the same time SVM classifier with high level features provided a substantial input in the efficiency of the resulting STC systems according to the fusion systems results.	acoustic cryptanalysis;deep learning;experiment;google map maker;high-level programming language;replay attack;speaker recognition;speech technology;statistical classification	Galina Lavrentyeva;Sergey Novoselov;Egor Malykh;Alexander Kozlov;Oleg Kudashev;Vadim Shchemelinin	2017		10.1007/978-3-319-66429-3_16	artificial intelligence;computer science;machine learning;speech recognition;support vector machine;replay attack;deep learning;speaker recognition;classifier (linguistics);speech technology;spoofing attack	SE	-11.070325862317416	-91.8241826147726	83518
bfe8dff9334ace02d396e7941ac6922327c5459b	iterative grapheme-to-phoneme alignment for the training of wfst-based phonetic conversion	speech processing;training dictionaries training data speech recognition measurement vocabulary educational institutions;iterative methods;iterative grapheme to phoneme alignment phonetic transcription minimum edit distance algorithm czech vocabulary speech recognition system orthographic phonetic word pairs data driven training phonetic conversion weighted finite state transducers;speech recognition;wfst alignment conversion grapheme to phoneme phonetisaurus;speech recognition iterative methods speech processing	In this paper we propose an algorithm for grapheme-to-phoneme (G2P) alignment. Such alignment is needed mainly for the data-driven training of G2P conversion tools. Our approach utilizes a given phonetic alphabet and a set of given orthographic-phonetic word pairs as a source of prior knowledge. The development data are taken from a manually created pronunciation lexicon for a large vocabulary speech recognition system for Czech. The alignment method is based on extended Minimum Edit Distance algorithm. Moreover, we propose an approach to avoid the creation of reference alignments - we evaluate the improvements through a specially designed G2P converter, i.e. we compare the phonetic transcription directly to a set of test orthographic-phonetic word pairs. Results of our approach are comparable or even slightly better than the state-of-the-art.	algorithm;edit distance;lexicon;orthographic projection;speech recognition;transcription (software);vocabulary	Marek Bohac;Jirí Málek;Karel Blavka	2013	2013 36th International Conference on Telecommunications and Signal Processing (TSP)	10.1109/TSP.2013.6613977	natural language processing;speech recognition;speech corpus;word error rate;computer science;pattern recognition;phonetic search technology;speech processing;iterative method	NLP	-20.235523090170386	-85.66944242142625	83580
48d6ba1f1f359f44fe71180c4b28cb873e7aa49d	flush: a flexible lexicon design	hierarchical knowledge;phrasal entry;flexible lexicon design;phrasal lexicon;natural language processing;flexible construction;linguistic construction;knowledge-based lexicon design;broad phrasal coverage;flexible lexicon utilizing specialized;current phrasal lexicon;knowledge base	"""Approaches to natural language processing that use a phrasal lexicon have the advantage of easily handling linguistic constructions that might otherwise be extragrammatical. However, current phrasal lexicons are often too rigid: their phrasal entries fail to cover the more flexible constructions. FLUSH, for Flexible Lexicon Utilizing Specialized and Hierarchical knowledge, is a knowledge-based lexicon design that allows broad phrasal coverage. I. I n t r o d u c t i o n Natural language processing systems must use a broad range of lexical knowledge to account for the syntactic use and meaning of words and constructs. The problem of understanding is compounded by the fact that language is full of nonproductive constructs--expressions whose meaning is not fully determined by examining their parts. To handle these constructs, some systems use a phrasal lexicon [Becket, 1975, Wilensky and Arena, 1980b, Jacobs, .1985b, Steinacker and Buchberger, 1983, Dyer and Zernik, 1986], a dictionary designed to make the representation of these specialized constructs easier. The problem that phrasal lexicons have is that they are too rigid: the phrasal knowledge is entered in a way that makes it difficult to represent the many forms some expressions may take without treating each form as a distinct """"phrase"""". For example, expressions such as """"send a message"""", """"give a hug"""", """"working directory"""", and """"pick up"""" may be handled as specialized phrases, but this overlooks similar expressions such as """"give a message"""", """"get a kiss"""", """"working area"""", and """"take up"""". Specialized constructs must be recognized, but much of their meaning as well as their flexible linguistic behavior may come from a more general level. A solution to this problem of rigidity is to have a hierarchy of linguistic constructions, with the most specialized phrases grouped in categories with other phrases that behave similarly. The idea of a linguistic hierarchy is not novel, having roots in both linguistics [Lockwood, 1972, Halliday, 1978] and Artificial Intelligence [Sondheimer et al., 1984]. Incorporating phrasal knowledge into such a hierarchy was suggested in some AI work [Wilensky and Arena, 1980a], but the actual implementation of a hier186 archical phrasal lexicon requires substantial extensions to the phrasal representation of such work. The Flexible Lexicon Utilizing Specific and Hierarchical knowledge (FLUSH) is one component in a suite of natural language processing tools being developed at the GE Research and Development Center to facilitate rapid assimilation of natural language processing technology to a wide variety of domains. FLUSH has characteristics of both traditional and phrasal lexicons, and the phrasal portion is partitioned into four classes of phrasal entries:"""	artificial intelligence;buchberger's algorithm;construction grammar;data assimilation;dictionary;directory (computing);lexicon;natural language processing;roots;semantic interpretation;word lists by frequency;working directory	David J. Besemer;Paul S. Jacobs	1987			natural language processing;knowledge base;speech recognition;computer science;linguistics	NLP	-30.74151281178673	-82.08543906022456	83644
11c9dae4ceb1eca159dfa17d4012cded717800c2	mat - a project to collect mandarin speech data through telephone net works in taiwan		A cooperative project, called Polyphone, was initiated by the Coordinating Committee on Speech Databases and Speech I/O Systems Assessment (COCOSDA) in 1992. Accordingly, a project to collect Mandarin speech data across Taiwan (MAT) was conducted by a group of researchers from several universities and research organizations in Taiwan. The purpose was to generate a speech corpus for the development of Mandarin-based speech technology and products. The speech data were collected at eight recording stations through telephone networks. The speakers were chosen so as to reflect the population of the gender, the dialect, the educational level, and the residence in Taiwan. A preliminary Mandarin speech database of 800 speakers has been produced. The final goal is to generate a speech database of at least 5000 speakers.	database;input/output;speech corpus;speech recognition;speech synthesis;speech technology;super robot monkey team hyperforce go!;test data	Hsiao-Chuan Wang	1997	IJCLCLP			NLP	-24.370393173318416	-84.95682262896939	83652
33dbb2ee6f7e95b03bed21f50f825c5f3ac43be3	improving phoneme and accent estimation by leveraging a dictionary for a stochastic tts front-end	front end;speech synthesis interpolated lm japanese accent word clustering tts front end;training corpus phonemes accent estimation dictionary stochastic tts front end pitch accents natural japanese speech tts front end system vocabulary word n gram model;speech synthesis;training corpus;speech processing;vocabulary;pitch accents;n gram model;dictionaries stochastic processes speech synthesis tagging context modeling predictive models laboratories vocabulary natural languages scalability;stochastic processes;stochastic processes dictionaries natural language processing speech processing;tts front end system;interpolated lm;dictionaries;dictionary;japanese accent;word n gram model;natural japanese speech;word clustering;tts front end;accent estimation;natural language processing;stochastic tts front end;phonemes	Determining the correct phonemes and pitch accents is important for creating natural Japanese speech. We implemented a TTS front-end system based on an n-gram model. However, the vocabulary of the word n-gram model is limited to the list of the words found in the training corpus, and collecting a very large training corpus is not an easy task. In this paper, we propose using an additional class n-gram model to incorporate not only the words found in the training corpus, but the words found in the dictionary to further improve the accuracy. In our experiments, our proposed model relatively improves the accuracy for estimating accents by 16.9% and the accuracy for estimating phonemes by 21.6% compared to the word n-gram model.	dictionary;end system;experiment;n-gram;netware file system;speech synthesis;vocabulary	Tohru Nagano;Ryuki Tachibana;Nobuyasu Itoh;Masafumi Nishimura	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518703	natural language processing;speech recognition;computer science;front and back ends;speech processing;speech synthesis	NLP	-20.445229142788808	-86.54012434405907	83714
69bf02f0394800ae70a88eb5a363f501a2cd612c	applications of binary classification and adaptive boosting to the query-by-humming problem	arti- cial intelligence;sequence alignment;melodic retrieval;binary classification;hidden markov model	In the “query-by-humming” problem, we attempt to retrieve a specific song from a target set based on a sung query. Recent evaluations of query-by-humming systems show that the state-of-the-art algorithm is a simple dynamic programming-based interval matching technique. Other techniques based on hidden Markov models are far more expensive computationally and do not appear to offer significant increases in performance. Here, we borrow techniques from artificial intelligence to create an algorithm able to outperform the current state-of-the-art with only a negligible increase in running time.	algorithm;artificial intelligence;binary classification;dynamic programming;hidden markov model;markov chain;query by humming;time complexity	Charles L. Parker	2005			artificial intelligence;boosting (machine learning);machine learning;speech recognition;hidden markov model;binary classification;computer science;dynamic programming;sequence alignment;pattern recognition;query by humming	AI	-7.604570457139722	-95.00906968471693	83899
6ddb432e23b38bb205340016e389efa4754c692e	laryngeal voice quality changes in expression of prominence in continuous speech		In this study three di erent prominence and speech melody related e ects on voice quality were hypothesis that prominence as a function of sentence and word stress is signaled with more pressed voice in the parameterization results, and the original hy-		Matti Airas;Paavo Alku;Martti Vainio	2007			speech recognition;voice analysis;computer science	Web+IR	-10.831529041311649	-83.63784882618175	84085
0b84da9c26f4aa8d4a6feeb5b9640b8060784a2a	semi-supervised discriminative language modeling for turkish asr	confusion modeling discriminative training semi supervised learning language modeling;lattices;confusion modeling;signal sampling;acoustics;training;language modeling;speech;semi supervised learning;automatic speech recognition semisupervised discriminative language modeling turkish asr semisupervised learning perceptron algorithm morph based confusion models sample selection strategy supervised training;computational modeling;speech recognition;speech recognition learning artificial intelligence natural language processing signal sampling;discriminative training;learning artificial intelligence;training speech acoustics lattices speech recognition computational modeling semisupervised learning;natural language processing;semisupervised learning	We present our work on semi-supervised learning of discriminative language models where the negative examples for sentences in a text corpus are generated using confusion models for Turkish at various granularities, specifically, word, sub-word, syllable and phone levels. We experiment with different language models and various sampling strategies to select competing hypotheses for training with a variant of the perceptron algorithm. We find that morph-based confusion models with a sample selection strategy aiming to match the error distribution of the baseline ASR system gives the best performance. We also observe that substituting half of the supervised training examples with those obtained in a semi-supervised manner gives similar results.	algorithm;automated system recovery;baseline (configuration management);language model;perceptron;sampling (signal processing);semi-supervised learning;semiconductor industry;supervised learning;syllable;text corpus	Arda Çelebi;Hasim Sak;Erinç Dikici;Murat Saraclar;Maider Lehr;Emily Tucker Prud'hommeaux;Puyang Xu;Nathan Glenn;Damianos Karakos;Sanjeev Khudanpur;Brian Roark;Kenji Sagae;Izhak Shafran;Daniel M. Bikel;Chris Callison-Burch;Yuan Cao;Keith B. Hall;Eva Hasler;Philipp Koehn;Adam Lopez;Matt Post	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6289049	semi-supervised learning;speech recognition;computer science;speech;machine learning;pattern recognition;lattice;computational model;language model	Vision	-19.221325740410936	-89.77882377428871	84109
2bcb96d5e99c91f980659078d3b5b6814dda39e6	generating facial expressions for speech	affectivite;3d animation;speech synthesis;expressiveness;programming language;mimique;facial modeling;production de la parole;formal semantics;affectivity;anglais;facial action coding system;conversation;facial expression;mimic;discours oral;expressivite;functional group;mouvement phonatoire;programme informatique;coordination	This paper reports results from a program that produces high quality animation of facial expressions and head movements as automatically as possible in conjunction with meaning-based speech synthesis, including spoken intonation. The goal of the research is as much to test and define our theories of the formal semantics for such gestures, as to produce convincing animation. Towards this end we have produced a high level programming language for 3D animation of facial expressions. We have been concerned primarily with expressions conveying information correlated with the intonation of the voice: this includes the differences of timing, pitch, and emphasis that are related to such semantic distinctions of discourse as “focus”, “topic” and “comment”, “theme” and “rheme”, or “given” and “new” information. We are also interested in the relation of affect or emotion to facial expression. Until now, systems have not embodied such rule-governed translation from spoken utterance meaning to facial expressions. Our system embodies rules that describe and coordinate these relations: intonation/information, intonation/affect and facial expressions/affect. A meaning representation includes discourse information: what is contrastive/background information in the given context, and what is the “topic” or “theme” of the discourse. The system maps the meaning representation into how accents and their placement are chosen, how they are conveyed over facial expression and how speech and facial expressions are coordinated. This determines a sequence of functional groups: lip shapes, conversational signals, punctuators, regulators or manipulators. Our algorithms then impose synchrony, create coarticulation effects, and determine affectual signals, eye and head movements. The lowest level representation is the Facial Action Coding System (FACS), which makes the generation system portable to other facial models.	algorithm;bcs-facs;display resolution;high-level programming language;map;semantics (computer science);speech synthesis;theme (computing);theory	Catherine Pelachaud;Norman I. Badler;Mark Steedman	1996	Cognitive Science	10.1207/s15516709cog2001_1	psychology;cognitive psychology;speech recognition;facial action coding system;computer science;artificial intelligence;mimic;formal semantics;expressivity;mathematics;computer animation;linguistics;communication;speech synthesis;facial expression;cognitive science	NLP	-14.454518718150435	-81.27059121203119	84142
effecedcc30b31bdfcbc03f20e169bc2c4a13794	commonalities of glottal sources and vocal tract shapes among speakers in emotional speech		This paper explores the commonalities of the glottal source waves and vocal tract shapes among four speakers in emotional speech (vowel: /a/, neutral, joy, anger, and sadness) based on a source-filter model with the proposed precise estimation scheme. The results are as follows. When compared with the spectral tilts of glottal source waves of neutral, (1) those of anger and joy increased, and those of sadness decreased in the 200to 700-Hz frequency range; (2) those of anger increased, but those of joy decreased, and those of sadness were the same as those of neutral in the 700to 2000-Hz range; and (3) all spectral tilts had the same tendency over 2000 Hz. For front vocal tract shapes, the area function of anger was the largest, that of sadness was the smallest, and those of joy and neutral were in the middle.	frequency band;sadness;tract (literature)	Yongwei Li;Ken-Ichi Sakakibara;Daisuke Morikawa;Masato Akagi	2017		10.1007/978-3-030-00126-1_3	communication;anger;vocal tract;sadness;psychology;vowel	NLP	-9.451337288317376	-83.33584140636496	84244
9d7406a7f59d802ccfb3568ade37c37571647b6f	exploiting phone-class specific landmarks for refinement of segment boundaries in tts databases		High accuracy speech segmentation methods invariably depend on manually labelled data. However under-resourced languages do not have annotated speech corpora required for training these segmentors. In this paper we propose a boundary refinement technique which uses knowledge of phone-class specific subband energy events, in place of manual labels, to guide the refinement process. The use of this knowledge enables proper placement of boundaries in regions with multiple spectral discontinuities in close proximity. It also helps in the correction of large alignment errors. The proposed refinement technique provides boundaries with an accuracy of 82% within 20ms of actual boundary. Combining the proposed technique with iterative isolated HMM training technique boosts the accuracy to 89%, without the use of any manually labelled data.	hidden markov model;iterative method;refinement (computing);speech segmentation;speech synthesis;text corpus	Vijayaditya Peddinti;Kishore Prahallad	2011			artificial intelligence;phone;pattern recognition;hidden markov model;computer science;speech segmentation;classification of discontinuities	HCI	-20.854932282340215	-83.33142321195372	84318
ad76b192aa8f1ee9b3f2a26c0b4505ed3119c5d7	a preprocessor for speech recognition systems operating in noisy environments	analogue transmission line;noisy environment;nonlinear preprocessor;mit darpa funded speech;speech signal;speech recognition system;noise ratio;nonlinear transmission line preprocessor;nonlinear transmission line;preprocess speech	"""Objectives: The recognition of speech in noisy environments is critical to certain DoD systems now under development. Current preprocessors for speech recognition systems, such as those based on """"linear predictive coding,"""" are linear and therefore not effective in noisy environments. The objective of this project is to develop a nonlinear preprocessor for speech recognition systems that significantly improves the signal to noise ratio of the speech signal to be recognized. The nonlinear transmission line will be realized in software, although realization in hardware in FY91 should be possible. More specifically we will: 1) Develop a nonlinear transmission line preprocessor that accurately simulates the mechanics of the inner ear at all sound pressure levels. 2) Preprocess speech with the nonlinear transmission line and show that there is a substantial improvement in the signal to noise ratio. 3) Assess the desirability and feasibility of creating either a digital or analogue transmission line on a chip and using it as a preprocessor in the CMU, BBN, or MIT DARPA funded speech recognition systems."""	analog transmission;linear predictive coding;nonlinear system;preprocessor;signal-to-noise ratio;speech recognition;transmission line	George Zweig	1989			voice activity detection;speech recognition;computer science;speech processing	ML	-14.169623707870286	-89.55853855693415	84409
5f0e99d17bb34d591afe7de0674b4afd95bd33bb	mid-level audio features based on cascaded harmonic-residual-percussive separation				Patricio López-Serrano;Christian Dittmar;Meinard Müller	2017			residual;electronic engineering;harmonic;computer science	AI	-13.239083875299604	-86.99645055104958	84431
14cef141fac680f7c386c95d78c1412516fd2a54	a deep identity representation for noise robust spoofing detection		The issue of the spoofing attacks which may affect automatic speaker verification systems (ASVs) has recently received an increased attention, so that a number of countermeasures have been developed for detecting high technology attacks such as speech synthesis and voice conversion. However, the performance of anti-spoofing systems degrades significantly in noisy conditions. To address this issue, we propose a deep learning framework to extract spoofing identity vectors, as well as the use of soft missing-data masks. The proposed feature extraction employs a convolutional neural network (CNN) plus a recurrent neural network (RNN) in order to provide a single deep feature vector per utterance. Thus, the CNN is treated as a convolutional feature extractor that operates at the frame level. On top of the CNN outputs, the RNN is employed to obtain a single spoofing identity representation of the whole utterance. Experimental evaluation is carried out on both a clean and a noisy version of the ASVSpoof2015 corpus. The experimental results show that our proposals clearly outperforms other methods recently proposed such as the popular CQCC+GMM system or other similar deep feature systems for both seen and unseen noisy conditions.	artificial neural network;convolutional neural network;deep learning;feature extraction;feature vector;random neural network;randomness extractor;recurrent neural network;sensor;speaker recognition;speech synthesis;spoofing attack	Alejandro Gómez Alanís;Antonio M. Peinado;José A. González;Ángel M. Gómez	2018		10.21437/Interspeech.2018-1909	spoofing attack;artificial intelligence;pattern recognition;computer science	AI	-11.176887511475309	-91.99402294887872	84444
2ae4d95cd199b3f8e71f5024385a90923966596f	spectral integration of dynamic cues in the perception of syllable-initial stops	no 1 2;dynamic change;vol 65;phonetique;cues;auditory system;psychoacoustique;speech perception;perception de la parole;238712;sound spectrography;speech acoustics;center of gravity;humans;phonetics;user computer interface;psychoacoustics;phonetica 2008	The present experiments examine the potential role of auditory spectral integration and spectral center of gravity (COG) effects in the perception of initial formant transitions in the syllables [da]-[ga] and [t(h)a]-[k(h)a]. Of interest is whether the place distinction for stops in these syllables can be cued by a 'virtual F3 transition' in which the percept of a frequency transition is produced by a dynamically changing COG. Listeners perceived the virtual F3 transitions comparably with actual F3 transitions although the former were less salient a cue. However, in a separate experiment, static 'virtual F3 bursts' were not as effective as actual F3 bursts in cueing the alveolar-velar place distinction. These results indicate that virtual F3 transitions can provide phonetic information to the perceptual system and that auditory spectral integration (completed by the central auditory system) may play a significant role in speech perception.	auditory system;dna integration;experiment;speech disorders;syllable	Robert Allen Fox;Ewa Jacewicz;Lawrence L. Feth	2008	Phonetica	10.1159/000130014	psychology;phonetics;speech recognition;speech perception;acoustics;philosophy;psychoacoustics;linguistics;sociology;center of gravity;communication;physics	HCI	-9.91829666658925	-82.21573980477889	84511
a16cecbaf87d965e396e610f251f710a807b70ad	a hearing impairment simulation method using audiogram-based approximation of auditory charatecteristics		Hearing impairment simulation is an effective technique to educate normal-hearing people about auditory perception of the hearing-impaired. Because auditory characteristics of the hearing impaired vary greatly from person-to-person, personalization of the hearing impairment simulation systems is essential to accurately simulate these individual differences. However, measurement of auditory characteristics of individuals is time-consuming work. In this paper, we propose a hearing impairment simulation method that is easily applied to individual hearing-impaired persons. Auditory filter characteristics and gain characteristics are estimated from easily measurable audiograms of each individual. We also implement a method for manually adjusting the hearing impairment level to improve accuracy of the proposed hearing impairment simulation. An experimental evaluation is conducted to compare intelligibility between hearing-impaired and normal-hearing persons with the proposed hearing impairment simulation. The experimental results show that the proposed method effectively makes the word correct rate and phoneme confusion tendency of the normal hearing persons similar to those of the hearing impaired persons.	approximation;auditory processing disorder;critical band;nethack;simulation	Nozomi Jinbo;Shinnosuke Takamichi;Tomoki Toda;Graham Neubig;Sakriani Sakti;Satoshi Nakamura	2014			speech recognition;computer science	AI	-9.807247078787547	-86.07834308188538	84624
04e0fefb859f4b02b017818915a2645427bfbdb2	context dependent recurrent neural network language model	word error rate;human computer interaction;wall street journal;text analysis;contextual information;latent dirichlet allocation recurrent neural network language modeling topic models;latent dirichlet allocation;word error rate improvement context dependent recurrent neural network language model contextual real valued input vector sentence modelling latent dirichlet allocation text block topic conditioned rnnlm data fragmentation multiple topic models perplexity penn treebank data wall street journal speech recognition task;machine learning;word processing natural language processing recurrent neural nets speech recognition text analysis;vectors computational modeling context neurons context modeling recurrent neural networks data models;speech recognition;context dependent;recurrent neural nets;recurrent neural network;natural language processing;language model;word processing	Recurrent neural network language models (RNNLMs) have recently demonstrated state-of-the-art performance across a variety of tasks. In this paper, we improve their performance by providing a contextual real-valued input vector in association with each word. This vector is used to convey contextual information about the sentence being modeled. By performing Latent Dirichlet Allocation using a block of preceding text, we achieve a topic-conditioned RNNLM. This approach has the key advantage of avoiding the data fragmentation associated with building multiple topic models on different data subsets. We report perplexity results on the Penn Treebank data, where we achieve a new state-of-the-art. We further apply the model to the Wall Street Journal speech recognition task, where we observe improvements in word-error-rate.	artificial neural network;fragmentation (computing);language model;latent dirichlet allocation;perplexity;recurrent neural network;speech recognition;the wall street journal;topic model;treebank;word error rate	Tomas Mikolov;Geoffrey Zweig	2012	2012 IEEE Spoken Language Technology Workshop (SLT)	10.1109/SLT.2012.6424228	latent dirichlet allocation;natural language processing;speech recognition;word error rate;computer science;recurrent neural network;machine learning;context-dependent memory;pattern recognition;linguistics;language model	NLP	-19.08197763755188	-87.2849421508514	84727
ce3e69ca8ff25e513a5de04c031d029c276f3f84	syllable hmm based mandarin tts and comparison with concatenative tts.	hmm;mandarin;syllable;synthesis;tts	This paper introduces a Syllable HMM based Mandarin TTS system. 10-state left-to-right HMMs are used to model each syllable. We leverage the corpus and the front end of a concatenative TTS system to build the Syllable HMM based TTS system. Furthermore, we utilize the unique consonant/vowel structure of Mandarin syllable to improve the voiced/unvoiced decision of HMM states. Evaluation results show that the Syllable HMM based Mandarin TTS system with a 5.3MB’s model size can achieve an overall quality close to a concatenative TTS system with 1GB’ data size.	hidden markov model;netware file system;super robot monkey team hyperforce go!;syllable	Zhiwei Shuang;Shiyin Kang;Qin Shi;Yong Qin;Lianhong Cai	2009			syllable;speech recognition;artificial intelligence;mandarin chinese;hidden markov model;computer science;pattern recognition	Web+IR	-19.3572687110412	-85.49331197443871	84734
