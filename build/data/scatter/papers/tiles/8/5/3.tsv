id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
04ee9b332f1ecc12bc51441068a1c10dd11ee905	impact of unknown covariance structures in semiparametric models for longitudinal data: an application to wisconsin diabetes data	62 07;estimator efficiency;covariance structure model;donnee longitudinale;statistical simulation;covariancia;methode parametrique;modele structure covariance;analisis datos;metodo parametrico;parametric method;estimation non parametrique;metodo semiparametrico;longitudinal data analysis;matrice covariance;covariance;methode semiparametrique;matriz covariancia;urinary albumin excretion;estimation parametrique;non parametric estimation;data analysis;simulacion estadistica;parametric estimation;consistent estimator;simulation statistique;statistical computation;calculo estadistico;semiparametric method;systolic blood pressure;efficacite estimateur;semiparametric model;simulation study;analyse donnee;calcul statistique;estimacion no parametrica;efficiency loss;longitudinal data;perte efficacite;covariance structure;modele semi parametrique;eficacia estimador;estimateur convergent;covariance matrix;estimador convergente	Semiparametric models are becoming increasingly attractive for longitudinal data analysis. Often there is lack of knowledge of the covariance structure of the response variable. Although it is still possible to obtain consistent estimators for both parametric and nonparametric components of a semipatrametric model by assuming an identity structure for the covariance matrix, the resulting estimators may not be efficient. We conducted extensive simulation studies to investigate the impact of an unknown covariance structure on estimators in semiparametric models for longitudinal data. In some situations the loss of efficiency could be substantial. A two-step estimator is thus proposed to improve the efficiency. Our study was motivated by a population based data analysis to examine the temporal relationship between systolic blood pressure and urinary albumin excretion.	semiparametric model	Jialiang Li;Yingcun Xia;Mari Palta;Anoop Shankar	2009	Computational Statistics & Data Analysis	10.1016/j.csda.2009.05.008	efficient estimator;estimation of covariance matrices;econometrics;covariance matrix;mathematical optimization;deadweight loss;blood pressure;covariance;mathematics;data analysis;consistent estimator;semiparametric model;statistics;semiparametric regression	ML	33.07072014963791	-22.97399755376716	13138
1d446c8d6afef3147185bb69794fff86fb909d84	speech authentication and recovery scheme in encrypted domain		This paper proposes a self-embedding fragile watermarking scheme in encrypted speech based on hyper-chaotic system and reference sharing mechanism. Hyper-chaotic system is introduced to hide the feature and improve the confidentiality of original speech. Reference sharing mechanism is used to generate watermark for recovering tampered speech area. Combining the encryption and watermarking technologies, the confidentiality and integrity of original speech can be achieved simultaneously. Analysis and experimental results demonstrate that the proposed algorithm can detect and locate the tampered area. Meanwhile, the self-embedding watermark can be extracted to recover the content of tampered speech with high quality. Additionally, the key space is big enough to resist brute-force attack, while the secure keys are sensitive to slight change.	authentication	Qing Qian;Hongxia Wang;Sani M. Abdullahi;Huan Wang;Canghong Shi	2016		10.1007/978-3-319-53465-7_4	self-embedding;artificial intelligence;computer vision;watermark;encryption;computer science;digital watermarking;slight change;confidentiality;authentication;key space	Vision	38.88223214330048	-11.561540359674966	13150
1561f885569b69700df71d4698af66d89d529c6f	localization of acoustic marker in inside environments based on the level difference between a pair of microphones		The purpose of this paper Is to develop a simplified localization method for indoor navigation with a few system complexity. It is possible to localization more certain precision by a few sensors, and yet that is effective for application to indoor navigation immediately. The distance and the angle from an acoustic marker with a pair of microphones was estimated by measuring the sound-level (SL) and SL difference, respectively. Through the experiment, it was concerned that proposed method would perform enough to localize the marker, if it was near to the microphones.	acoustic cryptanalysis;internationalization and localization;microphone;sl (complexity);sensor	Rihito Muto;Koichi Mizutani;Naoto Wakatsuki;Keiichi Zempo	2015	2015 IEEE 4th Global Conference on Consumer Electronics (GCCE)	10.1109/GCCE.2015.7398701	acoustics;telecommunications;engineering;communication	Robotics	53.23895309051448	-18.48216313713063	13159
eb22a334a9a7e6c2f194312c4cf1d07707e24982	hiding electronic patient record (epr) in medical images: a high capacity and computationally efficient technique for e-healthcare applications	computational complexity;edge detection;pixel repetition method;reversibility	Display Omitted Pixel Repetition Method (PRM) has been proposed and utilized to implement a semi-reversible data hiding scheme.RC4 has been used for providing an additional layer of security to embedded EPR/data.Hybrid edge detection has been utilized to maximize the detected edges for better imperceptivity.ISB hiding has been used for reducing computational complexity. A high capacity and semi-reversible data hiding scheme based on Pixel Repetition Method (PRM) and hybrid edge detection for scalable medical images has been proposed in this paper. PRM has been used to scale up the small sized image (seed image) and hybrid edge detection ensures that no important edge information is missed. The scaled up version of seed image has been divided into 22 non overlapping blocks. In each block there is one seed pixel whose status decides the number of bits to be embedded in the remaining three pixels of that block. The Electronic Patient Record (EPR)/data have been embedded by using Least Significant and Intermediate Significant Bit Substitution (ISBS). The RC4 encryption has been used to add an additional security layer for embedded EPR/data. The proposed scheme has been tested for various medical and general images and compared with some state of art techniques in the field. The experimental results reveal that the proposed scheme besides being semi-reversible and computationally efficient is capable of handling high payload and as such can be used effectively for electronic healthcare applications.		Nazir A. Loan;Shabir A. Parah;Javaid A. Sheikh;Jahangir A. Akhoon;Ghulam Mohiuddin Bhat	2017	Journal of biomedical informatics	10.1016/j.jbi.2017.08.002	pixel;encryption;electron paramagnetic resonance;rc4;information hiding;computer science;scalability;edge detection;computer vision;artificial intelligence	Vision	38.7489349268247	-10.642554016415033	13167
0c2cf26b5095a0abea8f7bc324f051978c68ea5c	let's consider two objectives when estimating hand postures		Hand posture estimation is an important step in hand gesture detection. It refers to the process of modeling hand in computer to accurately represent the actual hand obtained from an acquisition device. In the literature, several objective functions (mostly based on silhouette or point cloud) have been used to formulate and solve the problem of hand posture estimation as a minimisation problem using stochastic or deterministic algorithms. The main challenge is that the objective function is computationally expensive. In the case of using point clouds, decreasing the number of points results in a better computational cost, but it decreases the accuracy of hand posture estimation. We argue in this paper that hand posture estimation is a bi-objective problem with two conflicting objectives: minimising the error versus minimising the number of points in the point cloud. As an early effort, this paper first formulates hand posture estimation as a bi-objective optimisation problem and then approximates its true Pareto optimal front with an improved Multi-Objective Particle Swarm Optimisation (MOPSO) algorithm. The proposed algorithm is used to determine the Pareto optimal front for 16 hand postures and compared with the original MOPSO. The results proved that the objectives are in conflict and the improved MOPSO outperforms the original algorithm when solving this problem.		Shahrzad Saremi;Seyed Mohammad Mirjalili;Andrew Lewis;Alan Wee-Chung Liew	2017		10.1007/978-3-319-63004-5_10	point cloud;gesture recognition;silhouette;data mining;distributed computing;pareto principle;mathematical optimization;computer science;particle swarm optimization	Robotics	28.73779180942198	-13.040924477847156	13192
4dc5717c55a4ad4c400dfe39e2c7eb46511c066d	blind recovery of perceptual models in distributed speech and audio coding		A central part of speech and audio codecs are their perceptual models, which describe the relative perceptual importance of errors in different elements of the signal representation. In practice, the perceptual models consists of signal-dependent weighting factors which are used in quantization of each element. For optimal performance, we would like to use the same perceptual model at the decoder. While the perceptual model is signal-dependent, however, it is not known in advance at the decoder, whereby audio codecs generally transmit this model explicitly, at the cost of increased bit-consumption. In this work we present an alternative method which recovers the perceptual model at the decoder from the transmitted signal without any side-information. The approach will be especially useful in distributed sensor-networks and the Internet of things, where the added cost on bit-consumption from transmitting a perceptual model increases with the number of sensors.	codec;internet of things;quantization (signal processing);sensor;transmitter	Tomas Bäckström;Florin Ghido;Johannes Fischer	2016		10.21437/Interspeech.2016-27	voice activity detection;speech recognition;speech coding	ML	48.85270146282266	-8.426538704666726	13250
c231bc47bd74568de169c19942bd9fad427b9fb9	ct-nor: representing and reasoning about events in continuous time	continuous time;generic model;distributed computing environment;em algorithm;change point detection;hypothesis test	We present a generative model for representing and reasoning about the relationships among events in continuous time. We apply the model to the domain of networked and distributed computing environments where we fit the parameters of the model from timestamp observations, and then use hypothesis testing to discover dependencies between the events and changes in behavior for monitoring and diagnosis. After introducing the model, we present an EM algorithm for fitting the parameters and then present the hypothesis testing approach for both dependence discovery and change-point detection. We validate the approach for both tasks using real data from a trace of network events at Microsoft Research Cambridge. Finally, we formalize the relationship between the proposed model and the noisy-or gate for cases when time can be discretized.	ct scan;discretization;distributed computing;expectation–maximization algorithm;generative model;microsoft research;or gate	Aleksandr Simma;Moisés Goldszmidt;John MacCormick;Paul Barham;Richard Black;Rebecca Isaacs;Richard Mortier	2008			statistical hypothesis testing;expectation–maximization algorithm;computer science;artificial intelligence;machine learning;data mining;mathematics;change detection;statistics;distributed computing environment	ML	25.962560077603236	-23.635282423752408	13277
e553fd4c4df216c57b8975cb66d7c92df075190a	automatically designing robot controllers and sensor morphology with genetic programming	automated design;genetic program;evolutionary robotics;technology and engineering;simulation environment	Genetic programming provides an automated design strategy to evolve complex controllers based on evolution in nature. In this contribution we use genetic programming to automatically evolve efficient robot controllers for a corridor following task. Based on tests executed in a simulation environment we show that very robust and efficient controllers can be obtained. Also, we stress that it is important to provide sufficiently diverse fitness cases, offering a sound basis for learning more complex behaviour. The evolved controller is successfully applied to real environments as well. Finally, controller and sensor morphology are co-evolved, clearly resulting in an improved sensor configuration.	computation;galaxy morphological classification;genetic programming;mathematical morphology;robot;sensor;simulation;time complexity	Bert Bonte;Bart Wyns	2010		10.1007/978-3-642-16239-8_14	simulation;computer science;artificial intelligence;machine learning;evolutionary robotics	Robotics	25.033704258464155	-12.609908974109697	13297
c22443c75c18f6af51036431546cea211d2fdd96	enhanced multiple transform for video coding	complexity theory;discrete cosine transforms;markov processes;encoding	The Discrete Cosine Transform (DCT), and in particular the DCT type II, has been widely used for image and video compression. Although DCT efficiently approximates the optimal Karhunen-Loève transform under first-order Markov conditions with low complexity, the energy packing efficiency is still limited since a fixed transform cannot always capture the highly dynamic statistics of natural video content. In this paper, to further improve the transform efficiency, an Enhanced Multiple Transform (EMT) scheme is proposed. In the proposed EMT, a few sinusoidal transforms, other than DCT, have also been utilized for coding both Intra and Inter prediction residuals. The best transform, as selected from a pre-defined transform subset specified by prediction mode, is explicitly signaled in a joint coding block level manner. Moreover, to accelerate encoding process, fast methods have also been proposed by skipping unnecessary transform rate-distortion evaluations using previously encoding statistics. The proposed method has been implemented on top of High-Efficiency Video Coding (HEVC) reference software, and significant coding gain has been verified.	coding gain;data compression;digital video;discrete cosine transform;distortion;first-order reduction;high efficiency video coding;markov chain;random access;set packing;vc dimension	Xin Zhao;Jianle Chen;Marta Karczewicz;Xiang Lin;Xiang Li;Wei-Jung Chien	2016	2016 Data Compression Conference (DCC)	10.1109/DCC.2016.9	sub-band coding;computer vision;transform coding;speech recognition;s transform;lapped transform;continuous wavelet transform;theoretical computer science;discrete cosine transform;mathematics;move-to-front transform;markov process;macroblock;encoding;statistics;sum of absolute transformed differences	Vision	46.844677755833885	-18.60810615287762	13350
d16a8e91fed5ff677eca7f5b22483b531fbd14b8	an ant colony optimizer for melody creation with baroque harmony	optimisation;harmonized melodies ant colony optimizer melody creation baroque harmony;ant colony optimization;indexing terms;optimisation music;genetic algorithm;music;ant colony optimization frequency bioinformatics computer science genetic algorithms evolutionary computation parallel processing multiple signal classification computer networks artificial neural networks	We propose an algorithm that is based on the Ant Colony Optimization (ACO) metaheuristic for producing harmonized melodies. The algorithm works in two stages. In the first stage it creates a melody. This melody is then harmonized according to the rules of Baroque harmony in the second stage. This is the first ACO algorithm to create music that uses domain knowledge and the first employed for harmonization of a melody.	algorithm;ant colony optimization algorithms;baroque;continuation;graph (discrete mathematics);loss function;mathematical optimization;metaheuristic;optimization problem;refind	Michael Geis;Martin Middendorf	2007	2007 IEEE Congress on Evolutionary Computation	10.1109/CEC.2007.4424507	mathematical optimization;ant colony optimization algorithms;speech recognition;genetic algorithm;computer science;artificial intelligence;machine learning;music	Vision	24.777526654630368	-1.0702426174705155	13385
c5bb2b751e301fb12b2a263b85cace90daaaf36c	a real-time wavelet vector quantization algorithm and its vlsi architecture	transformation ondelette;processing element;euclidean distance calculation real time wavelet vector quantization algorithm vlsi architecture image compression algorithm zerotree wavelet vector quantization wvq algorithm computation time wavelet images coding efficiency real time zero vector tree noniterative decision rule significant wavelet vectors coding performance wireless channel errors zero vector trees processing elements real time minimum distance calculation codebook size;organigramme;hierarchical system;wireless channels;distance minimale;architecture systeme;image coding;flowchart;algorithm performance;image processing;data compression;circuito multipiso;forme onde;telecommunication sans fil;structure arborescente;systolic arrays;real time;simulacion numerica;systeme hierarchise;circuit vlsi;procesamiento imagen;euclidean distance;transform coding;indexing terms;analyse multiresolution;traitement image;methode calcul;experimental result;multistage circuit;algorithme;minimal distance;metodo calculo;wavelet transforms;wireless communication;algorithm;temps calcul;sistema jerarquizado;codificacion;vlsi circuit;cuantificacion vectorial;scalar quantization;forma onda;vector quantization;image compression;minimum distance;estructura arborescente;resultado algoritmo;telecomunicacion sin hilo;vector quantization very large scale integration iterative algorithms image coding computer architecture tree data structures wavelet coefficients iterative methods prediction algorithms classification tree analysis;temps reel;tree structure;simulation numerique;coding;performance algorithme;resultado experimental;digital signal processing chips vector quantisation image coding vlsi real time systems wavelet transforms transform coding tree searching systolic arrays;vlsi;tiempo real;arquitectura sistema;digital signal processing chips;vector quantizer;waveform;compresion dato;transformacion ondita;circuito vlsi;tiempo computacion;circuit multietage;tree searching	In this paper, a real-time wavelet image compression algorithm using vector quantization and its VLSI architecture are proposed. The proposed zerotree wavelet vector quantization (WVQ) algorithm focuses on the problem of how to reduce the computation time to encode wavelet images with high coding efficiency. A conventional wavelet image-compression algorithm exploits the tree structure of wavelet coefficients coupled with scalar quantization. However, they can not provide the real-time computation because they use iterative methods to decide zerotrees. In contrast, the zerotree WVQ algorithm predicts in real-time zero-vector trees of insignificant wavelet vectors by a noniterative decision rule and then encodes significant wavelet vectors by the classified VQ. These cause the zerotree WVQ algorithm to provide the best compromise between the coding performance and the computation time. The noniterative decision rule was extracted by the simulation results, which are based on the statistical characteristics of wavelet images. Moreover, the zerotree WVQ exploits the multistage VQ to encode the lowest frequency subband, which is generally known to be robust to wireless channel errors. The proposed WVQ VLSI architecture has only one VQ module to execute in real-time the proposed zerotree WVQ algorithm by utilizing the vacant cycles for zero-vector trees which are not transmitted. And the VQ module has only + 1 processing elements (PE's) for the real-time minimum distance calculation, where the codebook size is . PE's are for Euclidean distance calculation and a PE is for parallel distance comparison. Compared with conventional architectures, the proposed VLSI architectures has very cost-effective hardware (H/W) to calculate zerotree WVQ algorithm in real time. Therefore, the zerotree WVQ algorithm and its VLSI architectures are very suitable to wireless image communication, because they provide high coding efficiency, real-time computation, and cost-effective H/W.	algorithm;algorithmic efficiency;codebook;coefficient;computation;encode;euclidean distance;image compression;iterative method;microprocessor;multistage interconnection networks;quantization (signal processing);real-time clock;simulation;time complexity;tree structure;vector quantization;very-large-scale integration;wavelet	Seung-Kwon Paek;Lee-Sup Kim	2000	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.836293	data compression;multiresolution analysis;transform coding;waveform;index term;flowchart;image processing;image compression;computer science;theoretical computer science;euclidean distance;decision rule;mathematics;iterative method;very-large-scale integration;hierarchical control system;tree structure;coding;vector quantization;algorithm;wireless;wavelet transform	EDA	45.98170277839674	-13.356636365428136	13456
e47a177aaf82e9765c9f4bcb4f66ac7c14f173c8	generating random variates for stable sub-gaussian processes with memory	random number generation;symmetric α stable distribution;α sub gaussian distribution;rejection sampling;student s t distribution	We present a computationally efficient method to generate random variables from a univariate conditional probability density function (PDF) derived from a multivariate α-sub-Gaussian (αSG) distribution. The approach may be used to sequentially generate variates for sliding-window models that constrain immediately adjacent samples to be αSG random vectors. We initially derive and establish various properties of the conditional PDF and show it to be equivalent to a Student's t-distribution in an asymptotic sense. As the αSG PDF does not exist in closed form, we use these insights to develop a method based on the rejection sampling (accept-reject) algorithm that allows generating random variates with computational ease. HighlightsAn efficient method to generate random variates for a-sub-Gaussian processes with memory is presented.Properties of the univariate conditional α-sub-Gaussian distribution are investigated.Convergence of the aforementioned distribution to a Student's t-distribution is proven in an asymptotic sense.Using the above properties and tabulation of a heavy-tailed function, rejection sampling is used to generate realizations.The method may be used in simulation-based performance analysis of systems operating in colored α-sub-Gaussian noise.	gaussian process	Ahmed Mahmood;Mandar Chitre	2017	Signal Processing	10.1016/j.sigpro.2016.08.016	random variate;marginal distribution;conditional probability distribution;econometrics;combinatorics;random number generation;mixture distribution;univariate distribution;slice sampling;student's t-distribution;inverse transform sampling;stochastic simulation;mathematics;rejection sampling;convolution random number generator;sampling distribution;joint probability distribution;statistics;variance reduction;multivariate t-distribution	ML	31.840853382712464	-17.598183462515326	13485
d6e2bf79352e2879f924f3e8741aeb16adceab77	efficient sampling of spreading processes on complex networks using a composition and rejection algorithm		Efficient stochastic simulation algorithms are of paramount importance to the study of spreading phenomena on complex networks. Using insights and analytical results from network science, we discuss how the structure of contacts affects the efficiency of current algorithms. We show that algorithms believed to require O(logN) or even O(1) operations per update—where N is the number of nodes—display instead a polynomial scaling for networks that are either dense or sparse and heterogeneous. This significantly affects the required computation time for simulations on large networks. To circumvent the issue, we propose a node-based method combined with a composition and rejection algorithm, a sampling scheme that has an average-case complexity of O[log(logN)] per update for general networks. This systematic approach is first set-up for Markovian dynamics, but can also be adapted to a number of non-Markovian processes and can enhance considerably the study of a wide range of dynamics on networks.	algorithm;average-case complexity;best, worst and average case;complex network;computation;image scaling;network science;polynomial;rejection sampling;sampling (signal processing);simulation;sparse matrix;time complexity	Guillaume St-Onge;Jean-Gabriel Young;Laurent Hébert-Dufresne;Louis J. Dubé	2018	CoRR		network science;sampling (statistics);mathematics;computation;complex network;polynomial;algorithm;markov process;stochastic simulation;binary logarithm	ML	33.69660055813983	-12.796217597595168	13557
c84692f324254d14df40c431116443f4c9b65a61	a 2-stage partial distortion search algorithm for block motion estimation	algorithme rapide;mesure deplacement;estimation mouvement;full search;algoritmo busqueda;algorithm performance;algorithm complexity;complexite calcul;algorithme recherche;complejidad algoritmo;estimacion movimiento;search algorithm;motion estimation;video coding;complejidad computacion;displacement measurement;complexite algorithme;codage video;computational complexity;resultado algoritmo;fast algorithm;performance algorithme;recherche distorsion partielle;medicion desplazamiento;algoritmo rapido	In this paper, we propose a novel 2-Stage Partial Distortion Search (2S-PDS) algorithm to reduce the computational complexity in block motion estimation algorithms. In this algorithm, an early-rejection stage is introduced where the partial distortion of a decimated pixel block is calculated and compared with its local minimum. A block is rejected without calculating the full distortion of the entire block if the partial distortion is larger. In order to reduce the probability of false rejection, the local minimum is amplified by a pre-defined threshold before the comparison. Experimental results show that the proposed algorithm can reduce the complexity of block motion estimation algorithm significantly with only marginal performance penalty. The proposed algorithm can be used in combination with full-search or other fast search algorithms.	distortion;motion estimation;search algorithm	Rongshan Yu;Keng Pang Lim;Dajun Wu;Feng Pan;Zhengguo Li;Genan Feng;Si Wu	2002		10.1007/3-540-36228-2_17	computer vision;computer science;calculus;motion estimation;mathematics;geometry;computational complexity theory;algorithm;search algorithm	Vision	49.2238761621819	-19.225592502433702	13582
03044c4ea3b38a7d633ffe355ae79de67a6fd4c2	h.264/avc motion vector concealment solutions using online and offline polynomial regression	error concealment;video compression;regression;machine learning	This paper introduces two polynomial regression solutions for error concealment by predicting the values of motion vectors of lost macroblocks. The two solutions are online and offline polynomial regression modeling. In the former solution, the regression model is built during the decoding process whilst in the latter solution; the model is built during the encoding or the transcoding process and then used at the decoder for concealment. Both solutions make use of the spatially and temporally neighboring motion vectors for building the regression models. The advantages and disadvantages of the proposed solutions are elaborated upon. In comparison to existing work, the experimental results show that the proposed solutions have clear advantages of computational time requirements and motion vector prediction accuracy.		Tamer Shanableh;Khaled Assaleh	2015	Signal, Image and Video Processing	10.1007/s11760-013-0489-3	data compression;computer vision;regression;computer science;theoretical computer science;machine learning	Vision	47.21449786152163	-20.337813302698805	13640
c3d16b01ce569287a6ca98570d454b10d733998b	a completely evolvable genotype-phenotype mapping for evolutionary robotics	robot sensing systems;genomics;genotype phenotype mapping;evolutionary computation;genotypic controller representation;robot controllers;mobile robot swarms;mobile robot;search space;self adaptation approach;self adjusting systems;mobile robots;data mining;evolutionary operators;mobile robot swarms genotype phenotype mapping evolutionary robotics self adaptation approach robot controllers genotypic controller representation finite state machines evolutionary operators;automata;finite state machines;evolutionary robotics;multi robot systems;aerospace electronics;self adjusting systems evolutionary computation finite state machines mobile robots multi robot systems robot programming;swarm robotics;evolvability;genetic mutations automata mobile robots erbium robot control humans orbital robotics performance evaluation genetic programming law;evolvability evolutionary robotics genotype phenotype mapping swarm robotics finite state machines;finite state machine;robot programming	To achieve a desired global behavior for a swarm of robots where each robot has a local view and operating range in the environment is a well-known and challenging problem. Evolutionary Robotics is a self-adaptation approach which has been shown to e effectively find robot controllers for behaviors which are hard to implement by hand. There, evolvability is highly dependent on controller representation during evolution. It is known that using a genotypic controller representation which also encodes parts of the genotype-phenotype mapping (GPM) can lead to a meta-adaptation of the evolutionary operators to the search space structure, thus improving evolvability. We enhance this idea using a fully flexible GPM which is represented in the same way as the behavioral controllers are, and, therefore, can be completely evolved along with the behavior. The approach is based on finite state machines and extends an existing framework for decentralized evolution of robot behavior in swarms of mobile robots. Experiments indicate that the evolvable GPM outperforms both the extensively improved operators of the existing framework and a standard operator for the new real-valued genotypes with fixed GPM.	evolution;evolutionary robotics;experiment;finite-state machine;futures studies;general-purpose macro processor;mobile robot;swarm;technical standard	Lukas König;Hartmut Schmeck	2009	2009 Third IEEE International Conference on Self-Adaptive and Self-Organizing Systems	10.1109/SASO.2009.20	mobile robot;simulation;computer science;artificial intelligence;finite-state machine	Robotics	25.194301579069094	-12.900100847249835	13684
6047b6810e96faa35609c1a52ff76946bd668f3f	memetic algorithm with double mutation for numerical optimization	numerical optimization;two meta learning systems;memetic algorithm;double mutation operator	A memetic algorithm with double mutation operators is proposed, termed as MADM. In this paper, the algorithm combines two meta-learning systems to improve the ability of global and local exploration. The double mutation operators in our algorithms guide the local learning operator to search the global optimum; meanwhile the main aim is to use the favorable information of each individual to reinforce the exploitation with the help of two meta-learning systems. Crossover operator and elitism selection operator are incorporated into MADM to further enhance the ability of global exploration. MADM is compared with the algorithms LCSA, DELG and CMA-ES on some benchmark problems and CEC2005's problems. For the most problems, the experimental results demonstrate that MADM are more effective and efficient than LCSA, DELG and CMA-ES in solving numerical optimization problems.	mathematical optimization;memetic algorithm;memetics	Yangyang Li;Bo Wu;Lc Jiao;Ruochen Liu	2011		10.1007/978-3-642-31919-8_9	mathematical optimization;artificial intelligence;machine learning;mathematics	EDA	26.40996543623918	-3.898829697474715	13718
56c5decd5739f2ad4049830bebf9e1ac450ceeb3	channel loss in contactless human body communication		Human body communication (HBC) utilizes human body as the transmission medium to facilitate data communications in a wireless body area network (WBAN). It normally uses a pair of transmitting (Tx) and receiving (Rx) electrodes clinging to the body surface to form a low-loss body channel, so a higher energy efficiency can be achieved in comparison to conventional wireless communications. In HBC, the Tx electrode can be shared with vital sign monitoring electrode, such as ECG electrode or EEG electrode, to inject the signal into body. As for the Rx electrode, it can be either in direct contact to body surface or placed in proximity to body surface. The late case forms a contactless HBC communication, which find more applications in the WBAN, e.g. a smart phone in one’s pocket to receive ECG signal from the chest electrode. In view of the adverse effect caused by the contactless case, this paper presents a study on the path loss of contactless HBC, which are investigated by finite element method (FEM) and verified by actual measurements. An empirical formula for path loss and contactless space is derived, showing that the path loss is increased by 18 dB when the distance between electrode and body increases from 1 mm to 10 mm. It also shows a 5 dB reduction on path loss with a 50% increase of the electrode size.	antibody to hepatitis b core antigen;body dysmorphic disorders;body surface;contactless payment;contactless smart card;decibel;electroencephalography;finite element method;glossary of computer graphics;human-based computation;sensor;smartphone;transmitter;vital signs;wearable computer;electrode;hearing impairment;prescription document	Jingna Mao;Huazhong Yang;Yong Lian;Bo Zhao	2018	2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2018.8513312	electronic engineering;transmission medium;path loss;wireless;electrode;body area network;capacitance;computer science;communication channel	Mobile	46.08312315098409	0.6832345066352977	13754
6d576f268326e18bf3afa65feeaa09a5915ab85c	a novel four-step search algorithm for fast block motion estimation	algorithme rapide;image numerique;estimation mouvement;motion estimation transform coding motion compensation redundancy image sequences computational modeling video compression discrete cosine transforms mpeg standards testing;average computational requirement four step search algorithm fast block motion estimation image sequences center biased motion vector distribution center biased checking point pattern halfway stop technique simulation performance three step search motion compensation errors worst case computational requirement;image processing;motion compensation;performance;simulation;search algorithm;video compression;procesamiento imagen;three step search;motion estimation;testing;transform coding;traitement image;experimental result;motion compensated;computational modeling;four step search algorithm;computational complexity motion estimation image sequences motion compensation;fast block motion estimation;redundancy;senal video;signal video;center biased checking point pattern;computational complexity;discrete cosine transforms;motion vector;motion compensation errors;fast algorithm;image sequence;worst case computational requirement;imagen numerica;resultado experimental;center biased motion vector distribution;video signal;secuencia imagen;mpeg standards;digital image;halfway stop technique;average computational requirement;resultat experimental;algoritmo rapido;sequence image;image sequences	Based on the real world image sequence’s characteristic of center-biased motion vector distribution, a new four-step search (4SS) algorithm with center-biased checking point pattern for fast block motion estimation is proposed in this paper. Halfway-stop technique is employed in the new algorithm with searching steps of 2 to 4 and the total number of checking points is varied from 17 to 27. Simulation results show that the proposed 4SS performs better than the well-known three-step search and has similar performance to the new three-step search (N3SS) in terms of motion compensation errors. In addition, the 4SS also reduces the worst-case computational requirement from 33 to 27 search points and the average computational requirement from 21 to 19 search points as compared with N3SS.	best, worst and average case;computation;maxima and minima;mean squared error;motion compensation;motion estimation;search algorithm;simulation;whole earth 'lectronic link;world file	Lai-Man Po;Wing-Chung Ma	1996	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.499840	data compression;interpolation search;beam search;computer vision;transform coding;simulation;performance;image processing;quarter-pixel motion;computer science;theoretical computer science;motion estimation;jump search;mathematics;software testing;redundancy;computational complexity theory;motion compensation;computational model;digital image;algorithm;binary search algorithm;search algorithm	Vision	48.48458536933355	-19.036137811327883	13768
ac58a5de9ff189852240a8a6ec5988eac89dd2ac	a fast vq codebook generation algorithm using codeword displacement	algorithme rapide;tabla codificacion;algoritmo busqueda;data compression;generic algorithm;implementation;algorithme recherche;search algorithm;generation time;generalized lloyd algorithm;temps calcul;codificacion;cuantificacion vectorial;vector quantization;clustering;codebook;table codage;fast algorithm;signal classification;coding;classification signal;vector quantizer;compresion dato;tiempo computacion;classification automatique;computation time;implementacion;automatic classification;clasificacion automatica;codebook generation;algoritmo rapido;temps generation;compression donnee;codage;tiempo generacion;quantification vectorielle	In this paper, we present a fast codebook generation algorithm called CGAUCD (Codebook Generation Algorithm Using Codeword Displacement) by making use of the codeword displacement between successive partition processes. By implementing a fast search algorithm named MFAUPI (Modified Fast Algorithm Using Projection and Inequality) for VQ encoding in the partition step of CGAUCD, the codebook generation time can be further reduced significantly. Using MFAUPI, the computing time of CGAUCD can be reduced by a factor of 4.7-7.6. Compared to Generalized Lloyd Algorithm (GLA), our proposed method can reduce the codebook generation time by a factor of 35.9-121.2. Compared to the best codebook generation algorithm to our knowledge, our approach can further reduce the corresponding computing time by 26.0-32.8%. It is noted that our proposed algorithm can generate the same codebook as that produced by the GLA. The superiority of our method is more remarkable when a larger codebook is generated.	algorithm;code word;codebook;displacement mapping;vector quantization	Jim Z. C. Lai;Yi-Ching Liaw;Julie Liu	2008	Pattern Recognition	10.1016/j.patcog.2007.04.015	data compression;genetic algorithm;computer science;artificial intelligence;theoretical computer science;machine learning;codebook;mathematics;coding;cluster analysis;linde–buzo–gray algorithm;generation time;implementation;vector quantization;algorithm;statistics;search algorithm	Vision	44.42903108240605	-12.720172208609544	13841
c2db199b753cee9a335a03abd3eb2e3b769858a8	speech quality assessment using 2d neurogram orthogonal moments	auditory nerve model;neurogram;polqa;speech quality assessment;orthogonal moments;discrete tchebichef krawtchouk transform dtkt;pesq	This study proposes a new objective speech quality measure using the responses of a physiologically-based computational model of auditory nerve (AN). The population response of the model AN fibers to a speech signal is represented by a 2D neurogram, and features of the neurogram are extracted by orthogonal moments. A special type of orthogonal moment, the orthogonal Tchebichef-Krawtchouk moment, is used in this study. The proposed measure is compared to the subjective scores from two standard databases, the NOIZEUS and the supplement 23 to the P series (P.Sup23) of ITU-T Recommendations. The NOIZEUS database is used in the assessment of 11 speech enhancement algorithms whereas the P.Sup23 database is used in the ITU-T 8źkbit/s codec (Recommendation G.729) characterization test. The performance of the proposed speech quality measure is also compared to the results from some traditional objective quality measures. In general, the proposed neural-response-based metric yielded better results than most of the traditional acoustic-property-based quality measures. The proposed metric can be applied to evaluate the performance of various speech-enhancement algorithms and compression systems.		Wissam A. Jassim;Muhammad S. A. Zilany	2016	Speech Communication	10.1016/j.specom.2016.03.004	speech recognition;polqa;pesq;computer science;machine learning	HCI	47.86701155959845	-7.693215865691004	13856
460b5c345e0dbe81076fb465fd3dd67c3879a7d9	the limit of the partial sums process of spatial least squares residuals	metodo cuadrado menor;somme partielle;62m30;espace hilbert;proyeccion;parametric model;metodo estadistico;kernels;methode moindre carre;analyse multivariable;medio ambiente;regression function;spatial process;proceso lineal;functional central limit theorem;covariance analysis;teorema limite;stochastic process;espacio hilbert;methode parametrique;multivariate analysis;fonctionnelle;fonction regression;least squares method;03cxx;bounded variation;spatial data;partial sums;processus spatial;62j05;methode noyau;funcion regresion;spatial least squares residuals partial sums process set indexed partial sums brownian sheet brownian pillow bounded variation riemann stieltjes integral;metodo parametrico;60g15;partial sums process;linear regression model;60j65;south african;theoreme central limite;parametric method;modele lineaire;60f05;variance analysis;62g20;linear regression;regression model;statistical method;non linear model;modele non lineaire;modelo lineal;modele parametrique;statistical regression;62jxx;processus lineaire;60f17;58b20;spatial least squares residuals;geometric approach;funcional;hilbert space;modelo regresion;modelo no lineal;theoreme limite;62h05;analyse covariance;central limit theorem;60b12;62g10;functional;linear process;methode statistique;brownian pillow;noyau mathematiques;analisis variancia;regresion estadistica;reproducing kernel hilbert space;projection;modele regression;environment;indexation;metodo nucleo;62j10;least square;linear model;regresion lineal;processus stochastique;suma parcial;point changement;teorema central limite;analisis multivariable;kernel method;60f99;environnement;analisis covariancia;estimation statistique;proceso estocastico;boundary detection;regression statistique;62p12;estimacion estadistica;statistical estimation;residuo regresion;punto cambio;limit theorem;regression lineaire;change point;riemann stieltjes integral;brownian sheet;analyse variance;set indexed partial sums;residu regression;variance;variancia	We establish a functional central limit theorem for a sequence of least squares residuals of spatial data from a linear regression model. Under mild assumptions on the model we explicitly determine the limit process in the case where the assumed linear model is true. Moreover, in the case where the assumed linear model is not true we explicitly establish the limit process for the localized true regression function under mild conditions. These results can be used to develop non-parametric model checks for linear regression. Our proofs generalize ideas of a univariate geometrical approach due to Bischoff [W. Bischoff, The structure of residual partial sums limit processes of linear regression models, Theory Stoch. Process. 8 (24) (2002) 23-28] which is different to that proposed by MacNeill and Jandhyala [I.B. MacNeill, V.K. Jandhyala, Change-point methods for spatial data, in: G.P. Patil, et al. (Eds.), Multivariate Environmental Statistics. Papers Presented at the 7th International Conference on Multivariate Analysis held at Pennsylvania State University, University Park, PA, USA, May 5-9 1992, in: Ser. Stat. Probab., vol. 6, North-Holland, Amsterdam, 1993, pp. 289-306 (in English)]. Moreover, Xie and MacNeill [L. Xie, I.B. MacNeill, Spatial residual processes and boundary detection, South African Statist. J. 40 (1) (2006) 33-53] established the limit process of set indexed partial sums of regression residuals. In our framework we get that result as an immediate consequence of a result of Alexander and Pyke [K.S. Alexander, R. Pyke, A uniform central limit theorem for set-indexed partial-sum processes with finite variance, Ann. Probab. 14 (1986) 582-597]. The reason for that is that by our geometrical approach we recognize the structure of the limit process: it is a projection of the Brownian sheet onto a certain subspace of the reproducing kernel Hilbert space of the Brownian sheet. Several examples are discussed.	least squares	Wolfgang Bischoff;Wayan Somayasa	2009	J. Multivariate Analysis	10.1016/j.jmva.2009.04.005	stochastic process;linear regression;calculus;mathematics;least squares;regression analysis;statistics	PL	32.38376285622277	-23.434385597390637	13891
273cf52d6a385e21708767e540edca840b167203	on effective dual use of auxiliary information in variability control charts	distributions;runs rules;estimators;auxiliary information;control chart;power curve	During the last decade, variance control charts based on different sampling schemes have attracted research interest in the field of statistical process control. These charts used extra (auxiliary) information either for ranking of units or estimation rather than using it for both. The effectiveness of a control chart can be increased by utilizing the auxiliary information for dual purposes. This article is focused on developing a generalized structure of variance control charts based on dual use of auxiliary information under different sampling strategies and runs rules. The generalized structure mainly depends on three auxiliary information-based estimators with dual use of auxiliary information, three bivariate process distributions, and variety of sampling schemes. The performance of the proposed control charts is investigated by assessing the power curve. We have observed that the proposals of the study perform better than its complement. An application example is also provided for practitionersu0027 concerns to monitor the stability of physicochemical parameter of groundwater. Copyright © 2015 John Wiley u0026 Sons, Ltd.	chart;heart rate variability	Muhammad Riaz;Rashid Mehmood;Nasir Abbas;Saddam Akber Abbasi	2016	Quality and Reliability Eng. Int.	10.1002/qre.1848	distribution;econometrics;power law;control chart;estimator;computer science;data mining;mathematics;statistics	HCI	28.385219308230006	-19.740330840626065	13950
e1124c3b7d24b55f542aa007b7b5abdb9ed9a13e	fast algorithm for intra prediction of hevc using adaptive decision trees	fast mode decisions;hevc;intra prediction;offline training;decision trees	High Efficiency Video Coding (HEVC) Standard, as the latest coding standard, introduces satisfying compression structures with respect to its predecessor Advanced Video Coding (H.264/AVC). The new coding standard can offer improved encoding performance compared with H.264/AVC. However, it also leads to enormous computational complexity that makes it considerably difficult to be implemented in real time application. In this paper, based on machine learning, a fast partitioning method is proposed, which can search for the best splitting structures for Intra-Prediction. In view of the video texture characteristics, we choose the entropy of Gray-Scale Difference Statistics (GDS) and the minimum of Sum of Absolute Transformed Difference (SATD) as two important features, which can make a balance between the computation complexity and classification performance. According to the selected features, adaptive decision trees can be built for the Coding Units (CU) with different size by offline training. Furthermore, by this way, the partition of CUs can be resolved as a binary classification problem. Experimental results have shown that the proposed algorithm can save over 34% encoding time on average, with a negligible Bjontegaard Delta (BD)-rate increase.	algorithm;decision tree;high efficiency video coding	Xing Zheng;Yao Zhao;Huihui Bai;Chunyu Lin	2016	TIIS	10.3837/tiis.2016.07.023	real-time computing;simulation;computer science;theoretical computer science;context-adaptive variable-length coding;machine learning;decision tree;context-adaptive binary arithmetic coding	ML	46.87776265169261	-19.675790137622343	13994
6b28e95115a474537f72c041638a890cf14b0fb8	new algorithm for searching minimum bit rate wavelet representations with application to multiresolution-based perceptual audio coding	periodized wavelet packet transform;cost function;orthonormal wavelets;prototypes;acoustic modeling;filters;wavelet tree;programmable control;entropy coding;psychology;adaptive codes;bit rate;huffman codes;wavelet packet;quantisation signal;adaptive huffman algorithm;wavelet transforms;audio coding;monophonic cd audio signals;bit packing;wavelet packet transform;perceptual cost function;psycho acoustic model;monophonic cd audio signals minimum bit rate wavelet representations multiresolution based perceptual audio coding wavelet tree perceptual cost function periodized wavelet packet transform scalar adaptive quantization psycho acoustic model entropy coding bit packing adaptive huffman algorithm laplacian distributions wavelet coefficients orthonormal wavelets;minimum bit rate wavelet representations;wavelet packets;wavelet domain;scalar adaptive quantization;huffman codes audio coding quantisation signal wavelet transforms filtering theory adaptive codes;laplacian distributions;wavelet coefficients;filtering theory;multiresolution based perceptual audio coding;bit rate wavelet packets wavelet domain wavelet transforms psychology entropy coding prototypes filters cost function programmable control	Outlines an algorithm for searching minimum bit rate wavelet packet representations in a high quality wavelet-based perceptual audio coder. The algorithm applies wavelet tree and prototype filter adaptation to a perceptual cost function. To achieve such adaptive structure, a periodized wavelet packet transform is performed for each audio frame. After the transform, the encoder employs scalar adaptive quantization, controlled by a psycho-acoustic model in the wavelet domain, followed by entropy coding and bit packing. An adaptive Huffman algorithm that assumes Laplacian distributions for wavelet coefficients is used for entropy coding. A relevant feature of our coder is the method to translate the psycho-acoustic information to the wavelet domain, which allows using orthonormal wavelets of any compact support. Experimental results indicate that our coder ensures 'transparent' coding of monophonic CD audio signals at bit rates lower than 64 kb/s for all the test signals.	algorithm;psychoacoustics;wavelet	N. Ruiz;D. Martínez;R. Mata;M. Rosa;F. López	2000		10.1109/ICPR.2000.903541	wavelet;speech recognition;second-generation wavelet transform;continuous wavelet transform;computer science;theoretical computer science;pattern recognition;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;fast wavelet transform;lifting scheme;statistics;wavelet transform	HCI	47.940981116292775	-9.302892384288228	14083
cfd1c9f79060c3c819358168decf497864fc5611	a rate-distortion cost estimation approach to fast intra prediction in video coding for ultra high definition tv applications	rate distortion;encoding copper prediction algorithms video coding estimation computational complexity transforms;data compression;video compression;low complexity;intra prediction;image quality rate distortion cost estimation approach video coding ultrahigh definition tv applications large resolution video materials high efficiency video compression techniques computational complexity new generation encoder;video coding;computational complexity;image quality;cost estimation;high efficiency;high definition;high definition television;video coding computational complexity data compression high definition television	For large resolution video materials used such as in Ultra High Definition TV applications, high efficiency video compression techniques with a low complexity is highly desirable. In this paper, we propose a fast intra prediction method based on the Rate Distortion cost estimation. The proposed method reduce the computational complexity of the new generation encoder with comparable image quality and bit-rate.	computational complexity theory;data compression;distortion;encoder;fast fourier transform;image quality;intra-frame coding	Younhee Kim;Jongho Kim;Dong-San Jun;Soon-Heung Jung;Jin Soo Choi	2012	2012 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2012.6161790	video compression picture types;data compression;scalable video coding;computer vision;computer science;theoretical computer science;video tracking;coding tree unit;block-matching algorithm;multimedia;rate–distortion optimization;context-adaptive binary arithmetic coding;motion compensation;algorithm;statistics;multiview video coding	Robotics	45.483197504183266	-18.936378650228573	14105
a9686f9109fcbf97ce4e7f2127b0dd00e6c92999	controlling correlated processes of poisson counts	inarma models;control charts;poisson inar 1 process;serially dependent poisson counts	The class of INARMA models is well suited to model the autocorrelation structure of processes with Poisson marginals arising in context of statistical quality control. After reviewing briefly the basic principles and important members of this broad family of models, we concentrate on the INAR(1) model, which is of particular relevance for quality control. We suggest four approaches to control such count processes, and compare their run length performance in a simulation study. Results show that only some of the out-of-control situations considered can be controlled effectively with the discussed control schemes. Copyright © 2007 John Wiley & Sons, Ltd.	autocorrelation;fractional poisson process;john d. wiley;relevance;run-length encoding;simulation	Christian H. Weiß	2007	Quality and Reliability Eng. Int.	10.1002/qre.875	econometrics;control chart;computer science;operations management;poisson regression;statistics	SE	31.77948160092677	-19.69349627001	14136
b93deff5b01099233bdc6f40fbeba201d6f6b6d1	steganalysis of multiple-base notational system steganography	image segmentation;steganalysis;image resolution;multiple base notational system;information science;image converters;steganalytic approach;gray scale;journal;data encapsulation;steganography multiple base notational system steganalysis;steganography;steganography detection steganalysis attacking multiple base notational system steganography image pixels steganalytic approach;cryptography;pixel;attacking multiple base notational system steganography;sun;humans;image pixels;image resolution cryptography data encapsulation;frequency;visual system;digital images;steganography pixel image converters digital images humans visual system frequency information science sun gray scale;steganography detection	This letter presents a method for attacking multiple-base notational system (MBNS) steganography . In the MBNS steganography, secret data are converted into symbols in a notational system with multiple bases. The pixels of a host image are then modified such that when the pixel values are divided by the bases, their remainders are equal to the symbols. Through analysis, we prove that the amount of small remainders increases due to the modification. Based on this observation, we propose a steganalytic approach which is effective in not only detecting MBNS steganography but also estimating its embedding rate.	pixel;sensor;steganalysis;steganography	Bin Li;Yanmei Fang;Jiwu Huang	2008	IEEE Signal Processing Letters	10.1109/LSP.2008.924000	computer vision;image resolution;visual system;steganalysis;steganography tools;information science;computer science;cryptography;frequency;mathematics;image segmentation;steganography;internet privacy;computer security;pixel;statistics	ML	41.807600812351446	-11.235468988528893	14142
d712a3563eff869613d5c8474071465f448c3cae	perceptual speech quality assessment in acoustic and binaural applications	perceptual speech quality assessment;quality assessment acoustic applications acoustic testing telephony narrowband system testing speech analysis acoustic measurements speech codecs telephone sets;telephone networks;binaural applications;speech processing;speech analysis;telephone sets;perceptual evaluation of speech quality;system under test;listening quality mos estimation;listening quality mean opinion score estimation;telephony;quality of service perceptual speech quality assessment acoustic applications binaural applications perceptual evaluation of speech quality pesq itu t p 862 listening quality mean opinion score estimation listening quality mos estimation telephone networks telephone equipment narrowband telephony mobile telephones hands free telephones acoustic interfaces acoustic assessment model;acoustic testing;quality assessment;speech codecs;narrowband telephony;hands free telephones;voice communication;itu t p 862;mean opinion score;mobile telephones;system testing;telephone equipment;parameter estimation;quality of service;acoustic measurements;acoustic interfaces;narrowband;hearing;pesq;acoustic applications;acoustic assessment model;quality of service speech processing voice communication telephone sets telephone networks hearing parameter estimation	Perceptual models such as perceptual evaluation of speech quality (PESQ, ITU-T P.862) are now in common use for estimation of listening quality mean opinion score (MOS) of telephone networks and equipment. PESQ was originally designed for evaluation of narrowband telephony, with electrical and/or digital connections to the systems under test. The paper discusses extending PESQ to measurements of terminals, such as mobile or hands-free telephones, using acoustic interfaces, under the working title of ITU-T P.AAM (acoustic assessment model). The changes to the model, and results comparing the extended model with subjective test data, are presented.	acoustic cryptanalysis;binaural beats;pesq;test data	Tom Goldstein;Antony W. Rix	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1326732	mean opinion score;speech recognition;polqa;quality of service;telephone network;telecommunications;pesq;computer science;speech processing;system under test;telephony;estimation theory;system testing	Robotics	48.94492502979264	-7.277490033923033	14152
0a6bfeecf1e7ed3a535e11dcd77f0d005c190ef2	building blocks for large annealed compact neural networks	building block;annealing neural networks cellular neural networks hardware integrated circuit interconnections multiaccess communication voltage decoding electronic circuits laboratories;cellular neural nets;integrated circuit design;neural chips;cmos digital integrated circuits;error compensation;error compensation cellular neural nets cmos digital integrated circuits neural chips integrated circuit design;0 5 micron building blocks large annealed compact neural networks design issues globally connected neural networks hardware annealing function offset compensation schemes offset currents cell voltage outputs digital cmos process cnn;neural network	In this paper the design issues of large globally connected compact neural networks are targeted. Building blocks of a cell that is capable of performing the hardware annealing function are designed. Different offset compensation schemes are used to eliminate the offset currents. The cell is designed to have voltage outputs to facilitate the interconnecting of cells. The blocks are processed with a 0.5 /spl mu/m standard digital CMOS process and measurement results of selected building blocks of the cell are included.	artificial neural network	Mika Laiho;Ari Paasio;Kari Halonen	2000		10.1109/ISCAS.2000.856085	embedded system;electronic engineering;computer science;engineering;electrical engineering;machine learning;artificial neural network;integrated circuit design	ML	38.87775178418766	-1.99848733991416	14166
795b073e189b95df9ba578060ad80689e09cb4cc	markov random field modelling of royal road genetic algorithms	markov random field;probabilistic model;genetic algorithm;fitness function	Markov Random Fields (MRFs) [5] are a class of probabalistic models that have been applied for many years to the analysis of visual patterns or textures. In this paper, our objective is to establish MRFs as an interesting approach to modelling genetic algorithms. Our approach bears strong similarities to recent work on the Bayesian Optimisation Algorithm [9], but there are also some signi cant di erences. We establish a theoretical result that every genetic algorithm problem can be characterised in terms of a MRF model. This allows us to construct an explicit probabilistic model of the GA tness function. The model can be used to generate chromosomes, and derive a MRF tness measure for the population. We then use a speci c MRF model to analyse two Royal Road problems, relating our analysis to that of Mitchell et al. [7].	approximation;character encoding;directed acyclic graph;estimation of distribution algorithm;genetic algorithm;greedy algorithm;interaction;ising model;least squares;markov chain;markov random field;mathematical optimization;mitchell corporation;neighbourhood (graph theory);sensor;software release life cycle;statistical model	Deryck Forsyth Brown;A. Beatriz Garmendia-Doval;John A. W. McCall	2001		10.1007/3-540-46033-0_6	artificial intelligence;machine learning;mathematics;markov model;statistics	ML	28.750688477174347	-11.290798001024303	14176
24e2b4dd7a934a3eaf2af3d2a44ddd2e0cbc3841	using generalized estimating equation to learn decision tree with multivariate responses	statistical approach;mahalanobis distance;tecnologia electronica telecomunicaciones;decision tree;multivariate decision tree;indexation;generalized estimating equation;tecnologias;grupo a;multiple responses;generalized estimating equations gee	Previous decision tree algorithms have used Mahalanobis distance for multiple continuous longitudinal response or generalized entropy index for multiple binary responses. However, these methods are limited to either continuous or binary responses. In this paper, we suggest a new tree-based method that can analyze any type of multiple responses by using a statistical approach, called GEE (generalized estimating equations). The value of this new technique is demonstrated with reference to an application using web-usage survey.	algorithm;autoregressive model;binary tree;decision tree learning;generalized entropy index	Seong Lee;Hyun-Cheol Kang;Sang-Tae Han;Kwang-Hwan Kim	2005	Data Mining and Knowledge Discovery	10.1007/s10618-005-0004-8	econometrics;mathematical optimization;computer science;mahalanobis distance;machine learning;decision tree;mathematics;statistics;generalized estimating equation	ML	31.525795449447596	-23.35740552087308	14233
b2d797a31268f7ebc8f7d91e5960aebfe5b2b445	doppler-aided gnss position estimation with weighted least squares	satellite navigation doppler measurement kalman filters least squares approximations;least squares approximations;doppler aided;cramer rao bound doppler aided gnss position estimation weighted least squares carrier phase measurements receiver generated doppler measurements urban canyons positioning accuracy kalman filter;weighted least square;kalman filters;tracking loops;global navigation satellite system gnss;weighted least square wls;receivers;doppler effect;estimation;weighted least square wls doppler aided global navigation satellite system gnss position estimation;satellites;satellite navigation;position estimation;mathematical model;doppler measurement;receivers satellites doppler measurements tracking loops doppler effect mathematical model estimation;doppler measurements;global navigation satellite system	In contrast to carrier-phase measurements, receiver-generated Doppler measurements have much better availability, even in severe urban canyons or high-dynamic movement applications, while providing centimeter-level precision. Thus, the Doppler-aided approach may improve the positioning accuracy of the receiver. Some Kalman-filter-based applications have been developed in recent years, but the theoretical error analysis is still missing. In this paper, we propose a model-free Doppler-aided position estimation approach. The Doppler measurements are modeled together with the code pseudoranges and then solved as a weighted least square (WLS) problem. In addition, we analyze the lower bound of errors of the approach based on the Cramér-Rao bound (CRB). Simulation results and the error analysis show that the proposed approach may improve accuracy in position estimation.	branch and bound;computer-aided design;error analysis (mathematics);kalman filter;least squares;satellite navigation;simulation	Liyan Li;Jie Zhong;Minjian Zhao	2011	IEEE Transactions on Vehicular Technology	10.1109/TVT.2011.2163738	kalman filter;estimation;electronic engineering;satellite navigation;doppler effect;mathematical model;control theory;mathematics;satellite;statistics	Visualization	49.90814595884601	1.4951301887138808	14282
d39ebb11bc4be134e256038907acb628e990548c	mixing-attack-proof randomized embedding audio watermarking system	spread spectrum watermarking;digital audio watermarking;mixing attack	Audio mixing greatly degrades the watermarking security. Consequently, it is of importance to introduce mixing-attack-proof audio watermarking algorithms. This article investigates audio mixing attack. A randomized embedding spread spectrum audio watermarking algorithm is accordingly proposed to improve the watermark immunity to mixing attacks. First, balanced modulation is introduced to get rid of the host signal interference and improve the correlation score stability. With more stable correlation scores, a randomized pseudo-noise embedding strategy is proposed to decrease the interference among different mixed components and bit error rate. To further improve the synchronization performances, a multiple synchronization strategy which simultaneously synchronizes different mixed components is also proposed. The experimental results indicate that the proposed algorithm shows a strong level immunity to mixing attacks and it can resist the mixing attack with up to 5 mixed components.	bit error rate;digital watermarking;interference (communication);modulation;performance;pseudorandom number generator;randomized algorithm;synchronization (computer science)	Yuhong Guo;Qingsheng Yuan;Xuemin Zhao;Jian Liu;Yonghong Yan	2013	JCP	10.4304/jcp.8.12.3243-3250	telecommunications;theoretical computer science;mathematics;computer security	ML	41.880831920677224	-8.57128337925416	14317
27bd759b9d62d2e9e4ba5670b13f7d194a09e885	an improved integrated optimization model for the structural topology problems	minimisation;stress;topology;electronic mail;paper technology;optimal method;topology optimization;load transfer integrated optimization model structural topology problems linear generalized ks function global stress minimization problem;design optimization;circuit topology;mechanical engineering;linear generalized ks function;topology minimisation structural engineering;stress the min max problem linear generalized ks function strain energy;sensitivity analysis;structural engineering;stress paper technology optimization methods circuit topology design optimization educational institutions mechanical engineering electronic mail capacitive sensors sensitivity analysis;the min max problem;strain energy;capacitive sensors;optimization model;optimization methods	This paper study the structural topology integrated optimization method based on the linear generalized KS function. A linear generalized KS function is presented which transforms the local stress to global stress minimization problem. The transformed topology optimization model greatly reduces the scale of stress of the structural elements in the global sense and it can help obtain optimal topology paths of transferring loads more easily.	global optimization;mathematical optimization;topology optimization	Jun Tie;Yun-kang Sui	2010	2010 International Conference on Machine Vision and Human-machine Interface	10.1109/MVHI.2010.102	topology;minimisation;mathematical optimization;topology optimization;multidisciplinary design optimization;computational topology;weak topology;capacitive sensing;strain energy;stress;sensitivity analysis;statistics	EDA	37.03962292725412	0.655522556444118	14354
9863b88e458115bf23a001ca482e3620d3a2f4c6	the improved sp frame coding technique for the jvt standard	rate distortion;protocols;optimisation;decoding;visual communication;code standards;packet switching;video coding;image reconstruction;protocols video coding code standards packet switching visual communication decoding optimisation image reconstruction;jvt standard sp frame h 26l standard drift free bitstream switching dct coefficient coding modes rate distortion optimization up switching bitstreams down switching bitstreams tcp friendly protocols;quantization parameter;streaming media bandwidth decoding switches bit rate web and internet services sun computer applications asia discrete cosine transforms	* This work has been done while the author is with Microsoft Research Asia. ABSTRACT An efficient and flexible coding technique is proposed in this paper inspired by the SP frame in the H.26L standard, which can achieve a drift-free bitstream switching at the predicted frame. The proposed scheme improves the coding efficiency of the SP frames in the H.26L standard by limiting the mismatch between the references for the prediction and reconstruction with two DCT coefficient coding modes and the rate-distortion optimization. Furthermore, the proposed scheme allows independent quantization parameters for up-switching and down-switching bitstreams. It further reduces the switching bitstream size while keeping the coding efficiency of the normal bitstreams. More rapid and frequent down-switching than up-switching and much smaller size of down-switching bitstream can be achieved with the proposed SP technique. These are very desirable features for any TCP-friendly protocols. Compared with the original SP method for H.26L, the proposed SP method improves the coding efficiency up to 1.0dB. This SP technique has been officially accepted by the JVT standard.	algorithmic efficiency;bitstream;coefficient;discrete cosine transform;distortion;frame language;h.264/mpeg-4 avc;mathematical optimization;microsoft research;quantization (signal processing);rate–distortion optimization	Xiaoyan Sun;Shipeng Li;Feng Wu;Guobin Shen;Wen Gao	2003		10.1109/ICIP.2003.1247240	iterative reconstruction;communications protocol;real-time computing;telecommunications;computer science;theoretical computer science;packet switching;visual communication	AI	47.15443919233223	-17.034540873941836	14431
1c3c6884c019e9082c23fe926e185b024729c816	performance of low-cost permanent magnet material in pm synchronous machines	eddy current losses;air gaps;synchronous machines air gaps current density eddy current losses ferrites permanent magnet machines rotors;rotors magnetic flux permanent magnet motors ferrites magnetomechanical effects windings torque;synchronous machines;permanent magnet machines;ferrites;rotors;permanent magnet machines fans ferrite magnets;induction machine low cost permanent magnet material pm synchronous machines permanent magnet synchronous machines low power industrial applications neodymium magnets samarium cobalt magnets linear current density ferrite permanent magnets eddy current losses low frequency applications motor drives rare earth magnets rotor surface ferrite magnets air gap flux density air gap torque tangential maxwell stress pole machines rotor yoke height air gap diameter;current density	Permanent magnet synchronous machines (PMSM) are considered a viable option in various types of applications. However, particularly in consumer and low-power industrial applications, the price may be a factor that limits the use of PMSMs. In addition to a different technology, the main reason for the high price of PMSMs is the use of expensive neodymium or samarium-cobalt magnets. Their use is necessary only if a high motor torque T to linear current density A ratio (T/A) is required. Ferrite permanent magnets are low cost, abundant, and have negligible eddy current losses in low-frequency applications such as motor drives. They have a much lower energy product (BHmax) than the most modern magnets. Because of the high prices of rare earth magnets, many parties are seeking for opportunities to use ferrites instead. In the case of rotor surface ferrite magnets, the air gap flux density remains low. The air gap torque producing tangential Maxwell stress is proportional to the product of the air gap flux density Bδ[ Vs/m2] and the linear current density A [A/m]. If the flux density is low and A cannot be increased, the rotor has to be made larger than in machines having a high air gap flux density. In the case of multiple pole machines, the outer rotor approach, with its low rotor yoke height, offers an interesting alternative. The air gap diameter of these machines can be made larger than in conventional inner rotor type motors without increasing the machine outer dimensions. In this paper, an outer rotor PMSM with ferrite magnets is analyzed and tested. The machine characteristics in a fan drive are compared with an induction machine of the same power.	air gap (networking);cobalt;ferrite (magnet);flux qubit;low-power broadcasting;mathematical induction;maxwell (microarchitecture);r.o.t.o.r.;the machine	Ilya Petrov;Juha J. Pyrhönen	2013	IEEE Transactions on Industrial Electronics	10.1109/TIE.2012.2191757	neodymium magnet;electronic engineering;permanent magnet synchronous generator;engineering;eddy current;electrical engineering;ferrite;nuclear magnetic resonance;air gap;physics;current density;quantum mechanics	Arch	52.98812939359227	-4.013650795193739	14437
9661750df6e2236625b84e720c68dca2e0436296	tree structure based data hiding for progressive transmission images	data hiding;progressive transmission image;steganography;progressive transmission;tree structure;spiht	Progressive image transmission (PIT) is supported by sever al encoders such as SPIHT, JPEG2000 and so on. However, few of data hiding scheme for pro gressive transmission images is designed. In this paper, tree-structure-based data hidi ng for set partitioning in hierarchical trees (SPIHT) images is proposed. The bit stream of SPIHT multi-st age encoded was structured of tree. The secret image can be progressively embedded into SPIHT bi t trees. Experimental results showed the progressive image data hid ing s achieved. The secret image is progressively embedded/extracted in SPIHT encoded images . Furthermore, a higher hiding capacity was provided in an earlier encoding which helped the secret i mage to be identified earlier. Also, an adaptive hiding capacity could be developed by using differ ent tree structures. In comparison with Tsai et al.’s scheme, the proposed scheme had a higher hiding capacity and a better secret image quality.	bitstream;distortion;embedded system;encoder;image quality;jpeg 2000;set partitioning in hierarchical trees;t-tree;tree (data structure);tree structure	Piyu Tsai	2010	Fundam. Inform.	10.3233/FI-2010-226	computer vision;computer science;theoretical computer science;pattern recognition;mathematics;steganography;tree structure;programming language;information hiding;set partitioning in hierarchical trees;statistics	Vision	41.48269047682993	-13.525999814791739	14450
0f81cc353ed7561c78d2ff93dc90e7eab930f269	determining the capacity parameters in pee-based reversible image watermarking	histograms;watermarking;optimisation;prediction error;reversible watermarking;laplace equation;local search algorithm;maximum likelihood estimation;materials;prediction error expansion;difference expansion;laplace equations;maximum likelihood estimate;capacity distortion optimization;local search algorithm pee based reversible image watermarking prediction error expansion capacity distortion optimization cdo problem optimal capacity parameters;watermarking payloads histograms optimization laplace equations materials maximum likelihood estimation;payloads;optimization;reversible watermarking capacity distortion optimization difference expansion prediction error expansion;search problems;image watermarking;search problems image watermarking optimisation	In the existing prediction-error expansion (PEE)-based reversible image watermarking schemes, the capacity parameters are determined in a recursive manner until the payload is just accommodated. This class of methods requires many rounds of embedding iterations, especially when the payload is high, and therefore, is computationally inefficient. Moreover, when multiple capacity parameters need to be determined, the previous methods cannot guarantee optimality in the capacity-distortion sense. In this work, a capacity-distortion optimization (CDO) framework is built to estimate the optimal capacity parameters. We prove that the CDO problem is convex for any embedding rates, permitting efficient solution. The estimated capacity parameters then serve as starting point to facilitate a local search algorithm to find the optimal capacity parameters with much less rounds of embedding iterations. Experimental results are provided to validate our findings.	digital watermarking;distortion;iteration;local search (optimization);loss function;mathematical optimization;optimization problem;payload (computing);recursion;search algorithm	Jiantao Zhou;Oscar C. Au	2012	IEEE Signal Processing Letters	10.1109/LSP.2012.2190508	mathematical optimization;discrete mathematics;mathematics;maximum likelihood;statistics	ML	49.29812764207393	-12.798507639451783	14469
5876d54b9586acb0949314ca42400c946f8976b5	bias from misspecification of the component variances in a normal mixture	methode jackknife;test hypothese;hypertension arterielle;62f40;estimacion sesgada;theorie echantillonnage;teoria muestreo;finite sample;bad specification;sample size;test statistique;covariance analysis;bootstrap;analisis datos;melange loi probabilite;maximum likelihood;fonction repartition;variance component;modele mal specifie;tamano muestra;metodo jackknife;test hipotesis;test estadistico;simulation;echantillon fini;maximum vraisemblance;variance analysis;error sistematico;statistical test;taille echantillon;mixed distribution;hypertension;simulacion;asymptotic bias bootstrap em algorithm normal mixture systolic blood pressure;ecuesta estadistica;componente variancia;statistical regression;misspecified model;estimation parametrique;funcion distribucion;sample survey;data analysis;distribution function;maximum likelihood estimate;composante variance;normal mixture;analyse covariance;consistent estimator;heteroscedasticidad;mixture model;bias;analisis variancia;biais asymptotique;regresion estadistica;hipertension arterial;heteroscedasticite;62j10;statistical computation;62f03;calculo estadistico;systolic blood pressure;borne inferieure;62d05;algorithme em;mezcla ley probabilidad;analyse donnee;mauvaise specification;calcul statistique;methode reechantillonnage;algoritmo em;parameter estimation;resampling method;heteroscedasticity;analisis covariancia;jackknife method;regression statistique;em algorithm;60e05;biased estimation;estimation biaisee;sondage statistique;maxima verosimilitud;estimateur convergent;lower bound;analyse variance;erreur systematique;estimador convergente;variance;cota inferior;variancia;asymptotic bias;sampling theory;hypothesis test	Bias in parameter estimates can be substantial when heteroscedastic normal mixtures are misspecified as homoscedastic normal mixtures, and vice versa. We show through simulations that the maximum likelihood estimators under the false assumption of equal variances are inconsistent and bias in parameter estimates is appreciable and even substantial when the mixture components are not well-separated. Finite sample bias in parameter estimates is close to the asymptotic bias even for a sample size of 200 or less. When homoscedastic normal mixtures are misspecified as heteroscedastic normal mixtures, the maximum likelihood estimators are consistent. However, the maximum likelihood estimators under a correctly specified homoscedasticmixturemodel converge to the true parameter values faster than those under a misspecified heteroscedastic mixture model. The bias of the maximum likelihood estimators is less dependent on the lower bound imposed on the component variances to ensure that the likelihood is bounded under the false assumption of unequal variances when the sample size is 500 or more and the component distributions are well-separated. An example is given to demonstrate the effects of a misspecification of the component variances on estimates of the prevalence of hypertension using normal mixtures. © 2011 Elsevier B.V. All rights reserved.		Yungtai Lo	2011	Computational Statistics & Data Analysis	10.1016/j.csda.2011.04.007	econometrics;statistical hypothesis testing;calculus;mathematics;maximum likelihood;quasi-maximum likelihood;statistics	ML	32.637369234327075	-22.67884290444909	14522
ee10012f582fa2024f001f82019dd49dc921ccce	motion drift modeling and correction for downscale video transcoding	image resolution video coding transcoding image motion analysis data compression;image motion analysis;image resolution;data compression;video coding;video transcoding;full resolution;spatial resolution motion drift modeling downscale video transcoding compressed video;transcoding;compressed video;spatial resolution;transcoding computer architecture signal resolution video compression motion compensation analytical models spatial resolution decoding motion analysis quality assessment	Inter frames in compressed video make transcoding a complicated task since motion information is involved. This is especially true for downscale (spatial resolution) video transcoding in which input motion information cannot be precisely mapped to the output, which causes motion drift. Often carried out in full resolution, the drift compensation loop is computing intensive. In this paper, we model the entire process of downscale transcoding and propose a transcoding architecture that performs all computing-intensive loops in reduced resolution. We show that the computing load is significantly reduced while the quality is preserved.	data compression;downscaling	Bo Shen	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1530483	computer vision;transcoding;image resolution;quarter-pixel motion;computer science;motion estimation;multimedia;motion compensation;video post-processing;statistics;computer graphics (images)	Robotics	46.005157263379914	-17.926654518095365	14533
41f2948f49f7ccafe15d3b93f296a4d32015c052	hybrid genetic-fuzzy systems for improved performance in residual-based fault detection	hybrid genetic fuzzy systems auc areas under the curve fuzzy models detection capability fd system roc curves receiver operating characteristic curves fdc fault detection curves converged solutions built in genetic operators nonglobal solutions deterministic optimization algorithm deterministic training evolutionary approach system identification model data driven fault detection system gfs residual based fault detection;genetic algorithms convergence deterministic algorithms fault diagnosis fuzzy set theory;receivers;stochastic processes;fault detection;black box modeling residual based fault detection fuzzy systems genetic fuzzy systems hybridization;statistics;fault detection receivers stochastic processes sociology statistics;sociology	We demonstrate how Residual-Based Fault Detection can be improved by means of Genetic-Fuzzy Systems (GFSs). Thus, the performance of a pure Data-Driven Fault Detection System, which relies on system identification models, is improved using models created by Genetic-Fuzzy Systems. The evolutionary approach is used in the cases where a deterministic training of the fuzzy systems is not able to produce good results. As such, when the deterministic optimization algorithm is trapped in local optima, GFSs are used in order to improve (fine tune) the non-global solutions using built-in genetic operators that are able to help converged solutions escape from their locality. The results are presented by means of Fault Detection Curves (FDC) -inspired by Receiver Operating Characteristic (ROC) curves- and show how, even when considering a Fault Detection (FD) system with good detection capabilities, the introduction of new, genetically evolved, fuzzy models still produces an important improvement, reflected by higher Areas Under the Curve (AUC).	algorithm;best, worst and average case;built-in self-test;fault detection and isolation;fuzzy control system;fuzzy logic;genetic fuzzy systems;genetic operator;iterative and incremental development;local optimum;locality of reference;mathematical optimization;microsoft outlook for mac;pure data;receiver operating characteristic;system identification;worst-case scenario	Francisco Serdio;Alexandru-Ciprian Zavoianu;Edwin Lughofer;Kurt Pichler;Thomas Buchegger;Hajrudin Efendic	2014	2014 Sixth World Congress on Nature and Biologically Inspired Computing (NaBIC 2014)	10.1109/NaBIC.2014.6921859	stochastic process;mathematical optimization;real-time computing;theoretical computer science;machine learning;fault detection and isolation;statistics	AI	27.106021508108157	-11.576679210592777	14558
9575108dc4f33ea3e42e17ca7d9a3e802fc015ca	a method for the comparative analysis of concentration of author productivity, giving consideration to the effect of sample size dependency of statistical measures	metodo estadistico;scientometrics;sample size;measurement techniques;comparative analysis;concentracion;authors;tamano muestra;estudio comparativo;taille echantillon;statistical method;etude comparative;statistical distributions;mathematical formulas;scientometria;methode statistique;productivite auteur;comparative study;scientometrie;productivity;productividad autor;concentration;author productivity	In this article, we propose a method for the comparative analysis of concentration in author productivity distributions. We define the notion of concentration on the basis of two viewpoints (absolute and relative concentration) and select G (Gini's index) and V (the number of authors) as suitable measures for these two viewpoints. We then discuss the statistical peculiarity of author productivity data (i.e., most of the statistical measures change systematically according to changes in the sample size) and we explain our method using developmental profiles, which takes into account the sample size dependency of statistical measures. Finally, by applying it to actual data, we demonstrate the usefulness of the proposed method.	qualitative comparative analysis	Fuyuki Yoshikane;Kyo Kageura;Keita Tsuji	2003	JASIST	10.1002/asi.10239	probability distribution;sample size determination;qualitative comparative analysis;formula;productivity;scientometrics;comparative research;concentration;statistics	ML	35.162760381042865	-20.771442326476304	14575
b91c734be62226b684751634279d1c9be7fbbdb2	massively parallel electromagnetic simulation for photolithographic applications	simulation ordinateur;methode domaine temps;modelizacion;microelectronic processing;computer aided design;computational modeling optical polarization optical scattering tellurium dispersion magnetic materials optical materials convergence lithography metrology;te polarization;convergence;massively parallel computer architecture;photolithographie;two dimensional electromagnetic simulation program;tellurium;diffusion onde;transverse electric;tm polarization;scattering;tempest;difusion onda;metodo dominio tiempo;metrology;finite difference method;cm 2;magnetic materials;edge currents;modelisation;methode difference finie;lithography;iteration two dimensional electromagnetic simulation program tempest photolithography metrology alignment tm polarization te polarization oblique incidence dispersive materials partially coherent optical images time domain finite difference method massively parallel computer architecture scattering convergence edge currents reflection cm 2 phase shifting mask;computational modeling;dispersive medium;optical imaging;onde electromagnetique;optical polarization;medio dispersivo;optical scattering;massively parallel computer;oblique incidence;dispersive materials;finite difference time domain analysis;wave scattering;conception assistee;iteration;semiconductor process modelling;time domain finite difference method;time domain;milieu dispersif;time domain method;parallel processing semiconductor process modelling photolithography finite difference time domain analysis;parallel implementation;transverse magnetic;photolithography;phase shift mask;optical materials;dispersion;modeling;phase shifting mask;computer simulation;integrated circuits;partially coherent optical images;reflection;alignment;parallel processing;circuit integre;fabrication microelectronique;electromagnetic waves	The two-dimensional massively parallel electromagnetic simulation program TEMPEST has been generalized to extend its applicability to many of the difficult problems in photolithography, metrology, and alignment. TEMPEST, which has been made available on the NCSA and other computing centers, combines together techniques for analysis of the transverse electric (TE) and the transverse magnetic (TM) polarizations, oblique incidence, highly dispersive materials, and a technique for synthesis of partially coherent optical images. The solution is based on the time-domain finite-difference method, but exploits the power of massively parallel computer architectures. Equations suitable for massively parallel implementation are given for oblique incidence, both polarizations and dispersive materials. Computer time per iteration cycle is constant irrespective of the polarization and angle of incidence. However, the total simulation time for convergence was found to be dominated by physical scattering phenomena. Convergence for the TM polarization is 1.5 times slower than the TE polarization because of edge currents, and oblique incidence is 2 times slower than normal incidence owing to artificial reflection from the domain boundaries. A typical simulation time is three to five minutes with 256 k (1 k = 1024) simulation nodes on a CM-2 with 8 k processors. The effectiveness of the program for photolithographic applications is demonstrated by considering the effects of subtle changes in phase-shifting mask topography on the optical images.	8k resolution;central processing unit;coherence (physics);computational electromagnetics;computer architecture;connection machine;dispersive partial differential equation;finite difference method;in-phase and quadrature components;incidence matrix;iteration;national center for supercomputing applications;oblique projection;parallel computing;polarization (waves);simulation;tempest;test engineer;topography;transverse wave;vergence	Alfred Kwok Kit Wong;Roberto Guerrieri;Andrew R. Neureuther	1995	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.466339	computer simulation;lithography;parallel processing;electromagnetic radiation;magnet;electronic engineering;dispersion;systems modeling;reflection;iteration;convergence;phase-shift mask;time domain;computer science;finite difference method;electrical engineering;computer aided design;optical imaging;tellurium;light scattering;scattering;computational model;metrology;photolithography	HPC	44.49374166219922	3.684944310935559	14591
7b254aa1831196a7e39cbdde8c9da46235813457	optimal design of multi-subject blocked fmri experiments	least squares analysis;block design;maastricht university;sample size;data interpretation statistical;cost function;research design;magnetic resonance imaging economics methods statistics numerical data;mixed effects model;digital archive;number of cycles;research economics;first order;open access;budgets;optimal design;algorithms;humans;publication;scientific;linear models;number of subjects;institutional repository;blocked design;hemodynamics physiology	The design of a multi-subject fMRI experiment needs specification of the number of subjects and scanning time per subject. For example, for a blocked design with conditions A or B, fixed block length and block order ABN, where N denotes a null block, the optimal number of cycles of ABN and the optimal number of subjects have to be determined. This paper presents a method to determine the optimal number of subjects and optimal number of cycles for a blocked design based on the A-optimality criterion and a linear cost function by which the number of cycles and the number of subjects are restricted. Estimation of individual stimulus effects and estimation of contrasts between stimulus effects are both considered. The mixed-effects model is applied and analytical results for the A-optimal number of subjects and A-optimal number of cycles are obtained under the assumption of uncorrelated errors. For correlated errors with a first-order autoregressive (AR1) error structure, numerical results are presented. Our results show how the optimal number of cycles and subjects depend on the within- to between-subject variance ratio. Our method is a new approach to determine the optimal scanning time and optimal number of subjects for a multi-subject fMRI experiment. In contrast to previous results based on power analyses, the optimal number of cycles and subjects can be described analytically and costs are considered.	australian bibliographic network;autoregressive model;block specimens;block code;experiment;first-order predicate;loss function;minimax;null value;numerical analysis;numerous;optimal design;optimality criterion;population parameter;sample variance;specification;fmri	Bärbel Maus;Gerard J. P. Van Breukelen;Rainer Goebel;Martijn P. F. Berger	2011	NeuroImage	10.1016/j.neuroimage.2011.03.019	econometrics;mathematical optimization;block design;computer science;mathematics;statistics	ML	32.61872454107145	-19.248830622569187	14620
d9caed7f2602d699b6bb5eda8a0d3db5a3f10ada	improved ant colony optimization algorithm by potential field concept for optimal path planning	optimal solution;path planning problems ant colony optimization potential field concept optimal path planning collision free path pheromone update;optimisation;path planning problems;ant colony optimization;optimal path planning;path planning;ant colony optimization path planning fuzzy logic humanoid robots optimal control mobile robots genetic algorithms artificial neural networks neural networks layout;mobile robots;potential field;pheromone update;optimal path;potential field concept;path planning mobile robots optimisation;collision free path;ant colony optimization algorithm	In this paper, an improved ant colony optimization (ACO) algorithm is proposed to solve path planning problems. These problems are to find a collision-free and optimal path from a start point to a goal point in environment of known obstacles. There are many ACO algorithm for path planning. However, it take a lot of time to get the solution and it is not to easy to obtain the optimal path every time. It is also difficult to apply to the complex and big size maps. Therefore, we study to solve these problems using the ACO algorithm improved by potential field scheme. We also propose that control parameters of the ACO algorithm are changed to converge into the optimal solution rapidly when a certain number of iterations have been reached. To improve the performance of ACO algorithm, we use a ranking selection method for pheromone update. In the simulation, we apply the proposed ACO algorithm to general path planning problems. At the last, we compare the performance with the conventional ACO algorithm.	algorithm;ant colony optimization algorithms;converge;iteration;map;mathematical optimization;motion planning;selection (genetic algorithm);simulation	Joon-Woo Lee;Jeong-Jung Kim;Byoung-Suk Choi;Ju-Jang Lee	2008	Humanoids 2008 - 8th IEEE-RAS International Conference on Humanoid Robots	10.1109/ICHR.2008.4756022	mobile robot;mathematical optimization;ant colony optimization algorithms;computer science;artificial intelligence;machine learning;motion planning	Robotics	30.250070747457194	-2.7988874657501084	14695
84e138366fde9a6d7481e1a34b5281386f838412	a robust blind watermarking method using quantization of distance between wavelet coefficients	wavelets;digital watermarking;quantization	In this paper, we propose a robust blind watermarking algorithm based on quantization of distance among wavelet coefficients for copyright protection. We divide wavelet coefficients into some blocks and obtain the first, second, and third maximum coefficients in each block. Then, we quantize the first and second maximum coefficients according to binary watermark bits. Using the block-based watermarking, we can extract the watermark without using the original image or watermark. The algorithm as a watermarking system has appropriate performance due to imperceptibility. In addition, experimental results also show that the proposed method is quite robust under either non-geometry or geometry attacks.	additive white gaussian noise;algorithm;coefficient;digital watermarking;histogram equalization;image processing;jpeg;median filter;quantization (signal processing);utility functions on indivisible goods;wavelet	M. J. Sahraee;S. Ghofrani	2013	Signal, Image and Video Processing	10.1007/s11760-011-0269-x	computer vision;speech recognition;theoretical computer science;mathematics	EDA	41.1996889745424	-10.722346228909888	14709
2ea23d44bdce009308502d631e97d43873872b2a	a new method for image segmentation based on bp neural network and gravitational search algorithm enhanced by cat chaotic mapping	optimization algorithm;image segmentation;cat chaotic mapping;gravitational search algorithm;back propagation algorithm;neural network	This paper proposes a novel image segmentation method based on BP neural network, which is optimized by an enhanced Gravitational Search Algorithm (GSA). GSA is a novel heuristic optimization algorithm based on the law of gravity and mass interactions. It has been proven that the GSA has good ability to search for the global optimum, but it suffers from the premature convergence due to the rapid reduction of diversity. This work introduces a cat chaotic mapping into the steps of population initialization and iterative stage of the original GSA, which forms a new algorithm called CCMGSA. Then the CCMGSA is employed to optimize BP neural networks, which forms a combination method called CCMGSA-BP and we use it for image segmentation. To verify the efficiency of this method, the visual and performance experiments are done. The visual results using our proposed method are compared with those using other segmentation methods including an improved k-means clustering algorithm (I-K-means), a hybrid region merging method (H-Region-merging), and manual segmentation. The comparison results show that the proposed method can get good segmentation results on grayscale images with specific characteristics. And we compare the performance of our proposed method with those of IGSA-BP, CLPSO-BP and RGA-BP for image segmentation. The results indicate that the CCMGSA-BP shows better performance in terms of the convergence rate and avoidance of local minima.	artificial neural network;backpropagation;cluster analysis;experiment;global storage architecture;global optimization;grayscale;heuristic;image segmentation;interaction;iterative method;k-means clustering;lazy evaluation;machine learning;mathematical optimization;maxima and minima;performance evaluation;premature convergence;rate of convergence;search algorithm;univac bp	XiaoHong Han;Xiaoyan Xiong;Fu Duan	2015	Applied Intelligence	10.1007/s10489-015-0679-5	computer vision;computer science;artificial intelligence;machine learning;segmentation-based object categorization;image segmentation;scale-space segmentation;artificial neural network	Robotics	29.214770251663104	-5.2690379072085145	14738
a7e9bd6f82ad047e947fab7e7d1febf7da8db48c	on parameter selection for reducing premature convergence of genetic algorithms	premature convergence;genetic algorithm		genetic algorithm;premature convergence	Hong-Kyu Lee;Dong Hwan Lee;Ran Zhao;Gordon K. Lee;Moonnoh Lee	2010			genetic algorithm;machine learning;truncation selection;mathematical optimization;artificial intelligence;computer science;premature convergence	AI	25.711195835972415	-5.092516919467943	14744
d71c3ee92a9716ac7a7b9e066f87987b05e641bb	memetic firefly algorithm for data fitting with rational curves	computer graphics;memetics;computational modeling;shape;manufacturing;optimization;local search method data fitting rational bernstein basis functions rational bezier fitting curve data point parameterization continuous multivariate nonlinear optimization problem mathematical optimization techniques memetic firefly algorithm metaheuristic technique global optimization;search problems data handling optimisation rational functions;optimization shape memetics computational modeling data models manufacturing computer graphics;data models	This paper concerns the problem of obtaining a smooth fitting curve to a given set of (noisy) data points. This problem arises frequently in several industrial fields, such as computer-aided design and manufacturing (construction of car bodies, ship hulls, airplane fuselage), computer graphics and animation, medicine, and many others. The classical approach relies on polynomial functions to solve this problem. It has been noticed, however, that some shapes cannot be properly approximated through this polynomial scheme. In this paper, we address this issue by using rational functions, particularly the rational Bernstein basis functions. This poses an additional challenge: we have not only to compute the poles of the resulting rational Bézier fitting curve but also to obtain their corresponding weights and a suitable parameterization of data points. Overall, this leads to a continuous multivariate nonlinear optimization problem that cannot be solved through traditional mathematical optimization techniques. Our approach to tackle this issue is based on a memetic firefly algorithm combining a powerful metaheuristic technique (the firefly algorithm) for global optimization with a local search method. The performance of our scheme is illustrated through its application to four illustrative examples of free-form synthetic shapes. Our experimental results show that our memetic approach performs very well, and allows us to reconstruct the underlying shape of data points automatically with high accuracy. A comparative analysis on our benchmark shows that our approach outperforms some alternative methods reported in the literature for this problem.	approximation algorithm;basis function;benchmark (computing);computer graphics;computer-aided design;curve fitting;data point;firefly (cache coherence protocol);firefly algorithm;global optimization;local search (optimization);mathematical optimization;memetics;metaheuristic;nonlinear programming;nonlinear system;optimization problem;phil bernstein;polynomial;qualitative comparative analysis;synthetic intelligence	Andrés Iglesias;Akemi Gálvez	2015	2015 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2015.7256932	data modeling;memetics;polynomial and rational function modeling;mathematical optimization;shape;computer science;machine learning;mathematics;manufacturing;computer graphics;computational model;algorithm	Robotics	32.348108488100586	-9.454138909610863	14829
96def329722b6574076dc4b05262595a29e7ca6e	emergence of cooperation in a multiple predator, single prey game	mobile robot;emergence of cooperation;cooperative behavior;artificial evolution;prey capture	This research concerns the comparison of three different artificial evolution approaches for the design of cooperative behavior in a group of simulated mobile robots. The first and second approaches, termed: single pools and plasticity, are characterized by robots that share a single genotype, though the plasticity approach includes a learning mechanism. The third approach, termed: multiple pools, is characterized by robots that use different genotypes. The application domain is a pursuit-evasion game in which a team of robots, termed: predators, collectively work to capture or slow a single robot, termed: the prey. Results indicate that the multiple pools approach is superior comparative to the other two approaches in deriving robust and consistently effective prey-capture strategies, given that this approach facilitates the evolution of specialized behavioral roles in the predator team.	application domain;effective method;emergence;evasion (network security);evolutionary algorithm;experiment;interference (communication);mobile robot;partial template specialization;pool (computer science);port knocking;prey;pursuit-evasion	Geoff S. Nitschke	2003			mobile robot;simulation;computer science;artificial intelligence;evolutionary algorithm	Robotics	25.20322800257336	-12.972457003096114	14848
cd66ea91917a01fadfbefcd866ac9b3a55a7c0d3	a surrogate accelerated multicanonical monte carlo method for uncertainty quantification	uncertainty quantification;gaussian processes;multicanonical monte carlo	In this work we consider a class of uncertainty quantification problems where the system performance or reliability is characterized by a scalar parameter y. The performance parameter y is random due to the presence of various sources of uncertainty in the system, and our goal is to estimate the probability density function (PDF) of y. We propose to use the multicanonical Monte Carlo (MMC) method, a special type of adaptive importance sampling algorithm, to compute the PDF of interest. Moreover, we develop an adaptive algorithm to construct local Gaussian process surrogates to further accelerate the MMC iterations. With numerical examples we demonstrate that the proposed method can achieve several orders of magnitudes of speedup over the standard Monte Carlo method.	adaptive algorithm;gaussian process;horner's method;importance sampling;iteration;memory management controller;monte carlo method;numerical analysis;portable document format;propagation of uncertainty;risk management;sampling (signal processing);simulation;software propagation;speedup;surrogates;uncertainty quantification	Keyi Wu;Jinglai Li	2016	J. Comput. Physics	10.1016/j.jcp.2016.06.020	quantum monte carlo;monte carlo method in statistical physics;quasi-monte carlo method;econometrics;mathematical optimization;uncertainty quantification;dynamic monte carlo method;hybrid monte carlo;computer science;wang and landau algorithm;monte carlo molecular modeling;gaussian process;mathematics;kinetic monte carlo;monte carlo integration;statistics;monte carlo method	ML	32.358294215946685	-16.215731806778834	14945
610690659b66267d5e6a50675caa0fea51fa1dad	selection of binary variables and classification by boosting	secondary 62p10;classification automatique statistiques;metodo estadistico;analyse multivariable;selection problem;problema seleccion;high dimensionality;aplicacion;methode parametrique;multivariate analysis;multivariate binary data;metodo parametrico;selection variable;parametric method;simulacion numerica;dna fingerprinting;non normality;estadistica rango;statistical method;erreur classification;discriminant analysis;estimation parametrique;analyse discriminante;variable selection;62f07;analisis discriminante;methode selection;boosting;estimation erreur;62h30;variable selection primary 62h30;error estimation;methode statistique;simulation numerique;high dimensional data;estimacion error;classification error;rank statistic;analisis multivariable;statistique rang;dna fingerprints;thresholding;cross validation;binary data;selection method;application;primary 62h30;non normalite;numerical simulation;probleme selection	We adopt boosting for classification and selection of high-dimensional binary variables for which classical methods based on normality and non singular sample dispersion are inapplicable. Boosting seems particularly well suited for binary variables. We present three methods of which two combine boosting with the relatively classical variable selection methods developed in Wilbur et al. (2002). Our primary interest is variable selection in classification with small misclassification error being used as validation of proposed method for variable selection. Two of the new methods perform uniformly better than Wilbur et al. (2002) in one set of simulated and three real life examples.	boosting (machine learning);command & conquer:yuri's revenge;real life;statistical classification;thresholding (image processing)	Junyong Park;Jayson D. Wilbur;Jayanta K. Ghosh;Cindy H. Nakatsu;Corinne Ackerman	2007	Communications in Statistics - Simulation and Computation	10.1080/03610910701419729	computer simulation;econometrics;dna profiling;mathematics;linear discriminant analysis;feature selection;statistics	ML	33.78774263786993	-23.521890415187773	14984
d16a315b84e41fe1c7c086a64c01ca2bf38fd7ab	fast motion estimation for hevc with adaptive search range decision on cpu and gpu	video coding graphics processing units motion estimation search problems;gpgpu hevc motion estimation adaptive search range;rate distortion loss motion estimation hevc adaptive search range decision cpu gpu graphics processing units high efficiency video coding central processing unit motion vector predictor temporal motion vector encoding time reduction;graphics processing units encoding algorithm design and analysis motion estimation acceleration classification algorithms video coding	In this paper, we propose a fast Motion Estimation (ME) algorithm with Adaptive Search Range (ASR) decision to further accelerate the Graphics Processing Units (GPU)-based ME for High Efficiency Video Coding (HEVC). The proposed approach adaptively decides search ranges on the Central Processing Unit (CPU) and transfers them to the GPU. Then, the GPU performs ME process in parallel. The proposed approach solves the dependency problem in the Motion Vector Predictor (MVP) derivation stage by using only temporal Motion Vectors (MVs). The proposed algorithm yields the total encoding time reduction of 40.3% with negligible Rate Distortion (RD) loss of 1.2%. In terms of ME, the GPU-based ME with ASR decision provides the time reduction of 54.7% and 1446.8× speed-up on average compared to the GPU-based ME without ASR decision and the full-search ME in the reference model, respectively.	algorithm;central processing unit;distortion;graphics processing unit;high efficiency video coding;motion estimation;reference model;ruby document format	Sangmin Kim;Dong-Kyu Lee;Chae-Bong Sohn;Seoung-Jun Oh	2014	2014 IEEE China Summit & International Conference on Signal and Information Processing (ChinaSIP)	10.1109/ChinaSIP.2014.6889262	computer vision;real-time computing;quarter-pixel motion;computer science;motion estimation;block-matching algorithm;motion compensation;computer graphics (images)	Robotics	47.20633275119969	-19.056416006796475	15056
712d25c66d8011b0c0926463f162022cc5faa8c1	quasi-affine transformation evolution (quatre) algorithm: a new simple and accurate structure for global optimization	differential evolution;continuous spaces;colony	QUasi-Affine TRansformation Evolution (QUATRE) algorithm is a simple but powerful structure for global optimization. Six different evolution schemes derived from this structure will be discussed in this paper. There is a close relationship between our proposed structure and Different Evolution (DE) structure, and DE can be considered as a special case of the proposed QUATRE algorithm. The performance of DE is usually dependent on parameter control and mutation strategy. There are 3 control parameters and several mutation strategies in DE, and this makes it a little complicated. Our proposed QUATRE is simpler than DE algorithm as it has only one control parameter and it is logically powerful from mathematical perspective of view. We also use COCO framework under BBOB benchmarks and CEC Competition benchmarks for the verification of the proposed QUATRE algorithm. Experiment results show that though QUATRE algorithm is simpler than DE algorithm, it is more powerful not only on unimodal optimization but also on multimodal optimization problem.	algorithm;global optimization;program optimization	Jeng-Shyang Pan;Zhenyu Meng;Huarong Xu;Xiaoqing Li	2016		10.1007/978-3-319-42007-3_57	mathematical optimization;algorithm	Vision	24.698772767403153	-4.675755993114764	15059
47b6ed8a10b2a0f4290e48c8c1c148a1c144968a	simulation from a target distribution based on discretization and weighting	autocorrelacion;discrete distribution;methode discretisation;association statistique;discretisation;mcmc algorithm;metodo estadistico;analyse multivariable;analisis numerico;mcmc methods;jump process;chaine markov;65c99;cadena markov;metodo monte carlo;theorie approximation;aplicacion;multivariate analysis;65c05;stochastic method;fonction repartition;62h20;62e17;simulacion numerica;espace etat;discretization;methode monte carlo;statistical association;discretizacion;statistical method;physical sciences;46fxx;distribucion estadistica;65c40;analyse numerique;autocorrelations;algorithme;approximation theory;algorithm;metodo discretizacion;properly weighted samples 65c05;funcion distribucion;distribution function;asociacion estadistica;numerical analysis;properly weighted samples;estimation erreur;distribution statistique;state space method;methode espace etat;markov chain monte carlo;error estimation;methode statistique;monte carlo method;state space;simulation numerique;estimacion error;methode stochastique;analisis multivariable;discretization method;60j10;estimation statistique;importance sampling;monte carlo;espacio estado;application;estimacion estadistica;60e05;statistical estimation;statistical distribution;autocorrelation;metodo espacio estado;numerical simulation;algoritmo;markov chain;metodo estocastico	Two of the most common problems in computational statistics are sampling from a complex multidimensional distribution π and integration. As the dimension of the state space increases such problems become more difficult to handle. For this reason, several Monte Carlo (MC) and Markov Chain Monte Carlo (MCMC) methods have been developed. A simple approach which can be adopted is based on the discretization of the state space X (or some of its components). Partitioning X into a finite number of subsets and treating it as discrete simplifies the problem, since simulation from discrete distributions with finite support is a standard procedure. Working among these lines we additionally weight properly the simulated observations similarly to importance sampling. Then, we associate a jump process with the weighted sequence which converges weakly to the target distribution π. The method can be used in order to simplify certain MCMC algorithms, but its main advantage is that often the autocorrelations in the weighted sample almost vanish, allowing us to estimate the Monte Carlo standard errors of the estimators of interest using techniques for independent samples. We apply the method to toy examples, as well as to the Challenger dataset.	algorithm;computation;computational statistics;discretization;importance sampling;markov chain monte carlo;monte carlo method;sampling (signal processing);simulation;state space	Sonia Malefaki;George Iliopoulos	2009	Communications in Statistics - Simulation and Computation	10.1080/03610910802657904	computer simulation;probability distribution;monte carlo method in statistical physics;quasi-monte carlo method;econometrics;dynamic monte carlo method;hybrid monte carlo;markov chain monte carlo;calculus;discretization;monte carlo molecular modeling;mathematics;kinetic monte carlo;monte carlo integration;statistics;monte carlo method	ML	34.050705171653796	-22.175554071119745	15141
b74a7100bc99f1856c285d41f76f752dda848dec	approximate arai dct architecture for hevc		This work describes an approximate DCT architecture for the High Efficiency Video Coding (HEVC) standard. Since the standard requires to support multiple block sizes, architectures based on exact implementation require a relevant amount of hardware resources, namely multipliers and adders. This work aims to reduce the amount of hardware resources while keeping the rate-distortion performance nearly optimal. To achieve this goal, this work exploits an exact factorization of the DCT of size N = 8, which is then extended to obtain approximate DCTs of size N = 16 and N = 32. Simulation and implementation results prove that the proposed approximate solution features a complexity reduction with respect to exact one of more than 43% with an average rate-distortion performance loss of 4.74% for the worst-case (all-intra) configuration.		Giovanni Renda;Maurizio Masera;Maurizio Martina;Guido Masera	2017	2017 New Generation of CAS (NGCAS)	10.1109/NGCAS.2017.38	architecture;reduction (complexity);discrete cosine transform;very-large-scale integration;factorization;theoretical computer science;adder;mathematics	Arch	48.06727087062626	-20.366598554749032	15325
3e36be7c7712630e578ebb80f03650c466f5a7a3	multiview depth coding based on combined color/depth segmentation	color depth segmentation;segmentation based video coding;multiview video coding;virtual view;texture coding;3dtv;depth map;article;region merging	In this paper a new coding method for multiview depth video is presented. Considering the smooth structure and sharp edges of depth maps, a segmentation based approach is proposed. This allows further preserving the depth contours thus introducing fewer artifacts in the depth perception of the video. To reduce the cost associated with partition coding, an estimation of the depth partition is built using the decoded color view segmentation. This estimation is refined by sending some complementary information about the relevant differences between color and depth partitions. For coding the depth content of each region, a decomposition into orthogonal basis is used in this paper although similar decompositions may be also employed. Experimental results show that the proposed segmentation based depth coding method outperforms H.264/AVC and H.264/MVC by more than 2dB at similar bitrates.	3d television;asp.net mvc;color depth;depth map;depth perception;discrete cosine transform;distortion;h.264/mpeg-4 avc;overhead (computing);peak signal-to-noise ratio;wavelet	Javier Ruiz Hidalgo;Josep Ramon Morros;P. Aflaki;Felipe Calderero;Ferran Marqués	2012	J. Visual Communication and Image Representation	10.1016/j.jvcir.2011.08.001	computer vision;color depth;coding tree unit;mathematics;multimedia;scale-space segmentation;multiview video coding;depth map;computer graphics (images)	Vision	44.44169789470643	-18.030682405523226	15334
42a644504aea5645c9d2921acc36b20d3c645cb0	modelling thermal processes in buildings using an object-oriented approach and modelica	intelligent building;object oriented simulation;control engineering;analog computation;multi domain;object oriented;system design;mathematical modelling;state space;building simulation;object oriented approach;model based control;solar radiation;thermal processing;object oriented modelling;modelling and simulation;energy saving	Most of today’s modelling and simulation concepts originate from the times and methods of analog computers. Usually, it is assumed that the model must be expressed in an explicit state-space form. Consequently, the topology of the system gets lost and any future extension and reuse of the model is tedious and error-prone. In other words, it is the modeller’s task to consider the computational order of the operations during a simulation. In this paper we discuss the re-implementation of a passive-solarbuilding simulator in an object-oriented environment; it was originally built in the non-object-oriented simulation environment of Matlab–Simulink. The former simulator was designed to resemble a real physical test chamber with regard to the thermal and solar radiation flows. However, due to the lack of object orientation in Matlab–Simulink it was very difficult to apply any configuration modifications and extensions. We start with a brief description of the mathematical modelling which includes thermal dynamics and solar radiation. Then the implementation in Modelica is presented. So, a much superior environment in comparison with Matlab-Simulink was obtained, giving us the possibility of high-level modular and object-oriented modelling. The model is also extremely efficient in multidisciplinary projects in which control-engineering specialists (our group) cooperate with specialists from civil engineering, because civil engineers can more easily understand graphical and textual models in Modelica than schemes in Simulink. We expect that such a model will fulfil and significantly improve several model properties in comparison to the Matlab–Simulink implementation, i.e., a better understanding of the influences of thermal and radiation flows on comfortable living conditions, a modelbased control-system design, which will enable the harmonization of active and passive energy resources, important energy savings, and a very suitable environment for education in modelling, simulation and control. 2009 Elsevier B.V. All rights reserved.		Anton Sodja;Borut Zupancic	2009	Simulation Modelling Practice and Theory	10.1016/j.simpat.2009.04.003	simulation;computer science;systems engineering;state space;engineering;electrical engineering;artificial intelligence;sunlight;machine learning;mathematical model;object-oriented programming;systems design;mechanical engineering	AI	52.57428267794134	-6.457853800299933	15337
40cf7d7ebfeec5ebdcf3ace0d597546da8af7756	modified soft brood crossover in genetic programming		Premature convergence is one of the important issues while using Genetic Programming for data modeling. It can be avoided by improving population diversity. Intelligent genetic operators can help to improve the population diversity. Crossover is an important operator in Genetic Programming. So, we have analyzed number of intelligent crossover operators and proposed an algorithm with the modification of soft brood crossover operator. It will help to improve the population diversity and reduce the premature convergence. We have performed experiments on three different symbolic regression problems. Then we made the performance comparison of our proposed crossover (Modified Soft Brood Crossover) with the existing soft brood crossover and subtree crossover operators. Index Terms – Intelligent Crossover, Genetic Programming, Soft Brood Crossover	algorithm;data modeling;experiment;genetic operator;genetic programming;polynomial;premature convergence;sextic equation;symbolic regression;tree (data structure)	Hardik M. Parekh;Vipul K. Dabhi	2013	CoRR		crossover;artificial intelligence;machine learning;algorithm	Robotics	25.97642207058881	-6.042692093681721	15347
6dd1e0bceb68f23a6302ea02b44dd0b6b5e732a9	error detection and recovery by hiding information into video bitstream using fragile watermarking	digital watermarking;data hiding;least significant bit;error detection;video	This paper proposes a method of error detection and recovery by hiding specific information into video bitstream using fragile watermarking and checking it later. The proposed method requires no additional bits into compressed bitstream since it embeds a user-specific data pattern in the least significant bits of LEVELu0027s in VLC codewords. The decoder can extract the information to check whether there is an error in the received bitstream. We also propose to use this method to embed essential data such as motion vectors that can be used for error recovery. The proposed method can detect corrupted MBu0027s that usually escape the conventional syntax-based error detection scheme. This proposed method is quite simple and of low complexity.	bitstream;error detection and correction	Woonki Park;Byeungwoo Jeon	2002			real-time computing;computer science;theoretical computer science;bitstream format;internet privacy	Vision	40.352402248587026	-13.317838844066753	15444
57afbc2df6371afef4c7078153cec11a64be3b9d	user-friendly visual secret sharing for color images based on random grids	visual secret sharing;meaningful shares;cryptography visualization signal processing algorithms color communication systems digital signal processing algorithm design and analysis;meaningful shares visual secret sharing pixel expansion random grids;pixel expansion;random grids;visual quality user friendly visual secret sharing scheme color images random grids meaningful shares producing method mass data management black and white pixels distribution stack image produced shared images probability allocation plan tradeoff flexibility;probability cryptography image colour analysis	A user-friendly visual secret sharing scheme without pixel expansion is presented for color images based on random grids. Since noise like shares are not user-friendly, a meaningful shares producing method can simplify mass data management. Firstly, black and white pixels distribution in shared images and stack image will be analyzed. Then, in order to control the quality of produced shared images and stack image a probability allocation plan will be proposed. In former methods there was a quality tradeoff between meaningful shares and stack image, but the proposed method increases tradeoff flexibility. Moreover the inability to adjust the visual quality is reduced by the proposed visual secret sharing scheme. The suggested method will be checked and compared to other schemes.	pixel;secret sharing;usability	S. Mohammad Paknahad;S. Abolfazl Hosseini;Mahdi R. Alagheband	2016	2016 10th International Symposium on Communication Systems, Networks and Digital Signal Processing (CSNDSP)	10.1109/CSNDSP.2016.7573959	computer vision;computer science;theoretical computer science;internet privacy;computer security	Vision	38.84358006309986	-10.447350322067985	15480
063d80a4b2fc812f6f672202fa4e314d016b330c	sample average approximation of expected value constrained stochastic programs	administracion financiera;optimisation;convergence;analisis estadistico;optimizacion;financial management;stochastic method;approximation method;portfolio selection;gestion risque;conditional value at risk;risk management;seleccion cartera;programmation stochastique;convergence rate;probabilistic approach;expected value constrained stochastic program;selection portefeuille;portfolio optimization;convergencia;expected value;statistical analysis;methode approximation;enfoque probabilista;approche probabiliste;analyse statistique;portfolio management;methode stochastique;optimization;gestion cartera;gestion riesgo;sample average approximation;gestion portefeuille;gestion financiere;stochastic programming;programacion estocastica;metodo estocastico	We propose a sample average approximation (SAA) method for stochastic programming problems involving an expected value constraint. Such problems arise, for example, in portfolio selection with constraints on conditional value-at-risk (CVaR). Our contributions include an analysis of the convergence rate and a statistical validation scheme for the proposed SAA method. Computational results using a portfolio selection problem with a CVaR constraint are presented.	approximation;approximation algorithm;cvar;computation;converge;data validation;mathematical optimization;optimization problem;rate of convergence;selection algorithm;stochastic programming;value at risk	Wei Wang;Shabbir Ahmed	2008	Oper. Res. Lett.	10.1016/j.orl.2008.05.003	stochastic programming;econometrics;mathematical optimization;convergence;risk management;expected shortfall;portfolio optimization;mathematics;rate of convergence;expected value;statistics	AI	30.45239726740994	-11.336740849910914	15511
1f4cb7e9d88d582808139f6378dde7a821a5dbec	improving dct-based coders through block oriented transforms	theorie vitesse distorsion;rate distortion;vision ordenador;image processing;transformation cosinus discrete;procesamiento imagen;intelligence artificielle;codigo bloque;traitement image;computer vision;rate distortion theory;senal video;signal video;discrete cosine transforms;video signal;artificial intelligence;code bloc;vision ordinateur;inteligencia artificial;information system;block code;systeme information;sistema informacion	This paper describes a pre-processing for DCT-based coders and more generally for block-based image or video coders taking advantage of the orientation of the blocks. Contrary to most solutions proposed so far, it is not the transform that adapts to the signal but the signal that is pre-processed to fit the transform. The blocks are oriented using circular shifts at the pixel level. Before applying these shifts, the orientation of each block is evaluated with the help of a selection based on a rate-distortion criterion. We show that the insertion of this pre-processing stage in an H.264 coder and applied on residual intra frames can improve its rate-distortion performance.	discrete cosine transform	Antoine Robert;Isabelle Amonou;Béatrice Pesquet-Popescu	2006		10.1007/11864349_34	block code;computer vision;speech recognition;decorrelation;rate–distortion theory;image processing;computer science;information system;computer graphics (images)	NLP	46.293569640212176	-14.047287147764408	15538
7676ca799bb40f629bf18f9e231644acdf5450a9	sampling from the gamma distribution on a computer	digital computers;random variables;sampling;computer programming;rejection;rejection method;analysis of variance;shape parameter;gamma distribution;algorithms;methodology;probability density functions;gamma variates	This paper describes a method of generating gamma variates that appears to be less costly than Wallace's recently suggested method. For large shape parameter <italic>α</italic>; the cost of computation is proportional to √<italic>α</italic>, whereas Wallace's method is proportional to <italic>α</italic>. Experimentation by Robinson and Lewis indicates that for small <italic>α</italic> the method suggested here also dominates methods recently suggested by Dieter and Ahrens, albeit those methods dominate for large <italic>α</italic>. The method suggested here uses the rejection technique.	computation;gamma correction;rejection sampling;wallace tree	George S. Fishman	1976	Commun. ACM	10.1145/360248.360256	gamma distribution;sampling;econometrics;mathematical optimization;analysis of variance;methodology;computer programming;mathematics;shape parameter;statistics	Graphics	31.112470762161387	-17.691311444211312	15567
ffd6dcaabe70647de44bb031a1e293af22428fa4	a systematic ehw approach to the evolutionary design of sequential circuits		The main difficulty in the evolutionary design of finite state machines (FSMs) is lack of effective systematic EHW approach. To accomplish the evolutionary design of FSMs, a systematic EHW method named genetic programming–evolutionary strategy (GP–ES), which is a combination of ES and GP, is proposed. ES optimizes the state assignment and provide them to GP for population generation; GP is responsible for evolving the combinational part of FSM, and feeding the fitness of population back to ES for the evaluation of corresponding state assignments. GP–ES is tested extensively on twenty FSMs from MCNC Library. The results demonstrate that the GP–ES-derived state assignments are more efficient than the ones of Xia, Ali, Almaini and NOVA in the evolutionary design of FSMs. The results also illustrate that the GP–ES is superior to conventional synthesis tools in terms of complexity reduction for the design of small and middle FSMs. GP–ES also performs well in comparison with 3SD-ES in most cases.	continuous design	Yanyun Tao;Qing Zhang;Lijun Zhang;Yuzhen Zhang	2016	Soft Comput.	10.1007/s00500-015-1791-5	algorithm	EDA	28.543152095943988	-2.0931166031341166	15576
965a8591453ef8a57606375d23583681c6d7bcc1	multiple description image coding framework for ebcot	rate distortion;degradation;multiple description;image coding;data compression;image coding bit rate robustness packet switching image reconstruction transform coding block codes rate distortion software performance degradation;kakadu software;code standards;packet switched;packet switching;transform coding;bit rate;packet loss rates;software performance;packet loss rate;rate distortion theory;ebcot algorithm;jpeg 2000 standard;image compression;bit allocation optimization;rate distortion performance multiple description coding packet switched networks packet loss rates image reconstruction ebcot algorithm jpeg 2000 standard still image compression block coding bit allocation optimization generalized bfos algorithm kakadu software;image reconstruction;telecommunication standards;rate distortion performance;multiple description coding;robustness;bit allocation;packet switching image coding data compression image reconstruction rate distortion theory code standards telecommunication standards;packet switched networks;performance optimization;generalized bfos algorithm;still image compression;block codes;block coding	Multiple description coding (MDC) schemes have become popular as robustness tools in packet switched networks where multiple paths with varying packet loss rates are available. The descriptions are designed so that the quality of the reconstructed image depends on the number of descriptions received and not on which descriptions are actually received. We propose a new MDC framework for the EBCOT algorithm used in the current JPEG 2000 standard for still image compression. The inherent block coding in EBCOT allows for more flexibility in the formation of basic groups that are then combined in different ways to form descriptions. The bit allocation problem (between the different groups in a description) is formulated and optimized for good rate-distortion performance. Optimization of the bit allocation between the primary and secondary positions in each description is obtained using the generalized BFOS algorithm. Experiments using the Kakadu software (for JPEG 2000) show that the performance of the proposed framework degrades gracefully with increase in the number of descriptions lost.	jpeg 2000	K. P. Subbalakshmi;Siva Somasundaram	2002		10.1109/ICIP.2002.1039027	block code;real-time computing;telecommunications;computer science;theoretical computer science;multiple description coding;mathematics;statistics	Vision	48.1723443561035	-16.055681975605918	15649
0f1ecca4a99282a6a6347d3a91895c0fc9255622	seeding strategies and recombination operators for solving the dna fragment assembly problem	dna;heuristique;genetique;procesamiento informacion;comportement;fragment assembly problem;51e24;algorithm analysis;heuristica;genetica;resolution math;combinatorial problems;estrategia;algoritmo genetico;68wxx;genetics;dna fragmentation;permutation;strategy;combinatorial problem;accuracy;probleme combinatoire;problema combinatorio;precision;conducta;application industrielle;informatique theorique;genome;permutacion;information processing;permutation operators;algorithme genetique;2 opt heuristic;resolucion matematica;industrial application;secuencia adn;genetic algorithm;analyse algorithme;heuristics;genoma;behavior;sequence dna;traitement information;dna sequence;strategie;solving;analisis algoritmo;aplicacion industrial;computer theory;informatica teorica	a r t i c l e i n f o a b s t r a c t The fragment assembly problem consists in building the DNA sequence from several hundreds (or even, thousands) of fragments obtained by biologists in the laboratory. This is an important task in any genome project since the rest of the phases depend on the accuracy of the results of this stage. Therefore, accurate and efficient methods for handling this problem are needed. Genetic Algorithms (GAs) have been proposed to solve this problem in the past but a detailed analysis of their components is needed if we aim to create a GA capable of working in industrial applications. In this paper, we take a first step in this direction, and focus on two components of the GA: the initialization of the population and the recombination operator. We propose several alternatives for each one and analyze the behavior of the different variants. Results indicate that using a heuristically generated initial population and the Edge Recombination (ER) operator is the best approach for constructing accurate and efficient GAs to solve this problem.	erdős–rényi model;genetic algorithm;heuristic;software release life cycle	Gabriela F. Minetti;Enrique Alba;Gabriel Luque	2008	Inf. Process. Lett.	10.1016/j.ipl.2008.04.005	information processing;artificial intelligence;mathematics;accuracy and precision;algorithm	AI	26.48123825338748	2.019594805068616	15752
f2d47bd6a031df24797144a9e33c08edc214abaf	effective fragile watermarking for image authentication with high-quality recovery capability	content recovery;fragile watermarking;image authentication;tampering detection	In this paper, we propose an effective fragile image watermarking scheme for tampering detection and content recovery. Cover image is divided into a series of non-overlapping blocks and a block mapping relationship is constructed by the secret key. Several DCT coefficients with direct current and lower frequencies of the MSBs for each block are used to generate the reference bits, and different coefficients are assigned with different bit numbers for representation according to their importance. To enhance recovery performance, authentication bits are generated by the MSBs and the reference bits, respectively. After LSB substitution hiding, the embedded watermark bits in each block consist of the information of itself and its mapping blocks. On the receiver side, all blocks with tampered MSBs can be detected and recovered using the valid extracted reference bits. Experimental results demonstrate the effectiveness of the proposed scheme.	authentication	Chuan Qin;Chin-Chen Chang;Tai-Jung Hsu	2013	TIIS	10.3837/tiis.2013.11.023	theoretical computer science;internet privacy;computer security	Vision	39.25245976430643	-11.367837928898457	15808
1c80ab5b144d265e11fbb23403d1c8be2e23efba	ghevc: an efficient hevc decoder for graphics processing units	kernel;decoding;video sequences;streaming media;graphics processing units;video decoding graphics processor units gpus high efficiency video coding hevc parallel processing real time;high definition video;ip networks;decoding graphics processing units video sequences ip networks high definition video streaming media kernel	The high compression efficiency that is provided by the high efficiency video coding (HEVC) standard comes at the cost of a significant increase of the computational load at the decoder. Such an increased burden is a limiting factor to accomplish real-time decoding, specially for high definition video sequences (e.g., Ultra HD 4K). In this scenario, a highly parallel HEVC decoder for the state-of-the-art graphics processor units (GPUs) is presented, i.e., GHEVC. Contrasting to our previous contributions, the data-parallel GHEVC decoder integrates the whole decompression pipeline (except for the entropy decoding), both for intra- and interframes. Furthermore, its processing efficiency was highly optimized by keeping the decompressed frames in the GPU memory for subsequent inter frame prediction. The proposed GHEVC decoder is fully compliant with the HEVC standard, where explicit synchronization points ensure the correct HEVC module execution order. Moreover, the GPU-based HEVC decoder is experimentally evaluated for different GPU devices, an extensive range of recommended HEVC configurations and video sequences, where an average frame rate of 145, 318, and 605 frames per second for Ultra HD 4K, WQXGA, and Full HD, respectively, was obtained in the Random Access configuration with the NVIDIA GeForce GTX TITAN X GPU.	central processing unit;data compression;experiment;geforce 900 series;graphics display resolution;graphics processing unit;high efficiency video coding;random access;real-time clock	Diego F. de Souza;Aleksandar Ilic;Nuno Roma;Leonel Sousa	2017	IEEE Transactions on Multimedia	10.1109/TMM.2016.2625261	kernel;real-time computing;computer hardware;computer science;computer graphics (images)	Arch	45.68527503190232	-20.45382535230355	15820
c697f70dc4467a91295beb5640d1119d944c6f45	simulation optimization using the cross-entropy method with optimal computing budget allocation	cross entropy;stochastic simulation;search algorithm;simulation optimization;objective function;estimation of distribution algorithm;continuous optimization;computing budget allocation;global optimization;weight function;numerical experiment;computational efficiency;estimation of distribution algorithms;cross entropy method	We propose to improve the efficiency of simulation optimization by integrating the notion of optimal computing budget allocation into the Cross-Entropy (CE) method, which is a global optimization search approach that iteratively updates a parameterized distribution from which candidate solutions are generated. This article focuses on continuous optimization problems. In the stochastic simulation setting where replications are expensive but noise in the objective function estimate could mislead the search process, the allocation of simulation replications can make a significant difference in the performance of such global optimization search algorithms. A new allocation scheme is developed based on the notion of optimal computing budget allocation. The proposed approach improves the updating of the sampling distribution by carrying out this computing budget allocation in an efficient manner, by minimizing the expected mean-squared error of the CE weight function. Numerical experiments indicate that the computational efficiency of the CE method can be substantially improved if the ideas of computing budget allocation are applied.	continuous optimization;cross entropy;cross-entropy method;experiment;global optimization;mathematical optimization;mean squared error;optimal computing budget allocation;optimization problem;sampling (signal processing);search algorithm;simulation;weight function	Donghai He;Loo Hay Lee;Chun-Hung Chen;Michael C. Fu;Segev Wasserkrug	2010	ACM Trans. Model. Comput. Simul.	10.1145/1667072.1667076	econometrics;mathematical optimization;estimation of distribution algorithm;computer science;theoretical computer science;mathematics;continuous optimization;statistics;global optimization	DB	28.470151650990843	-12.562472809918892	15895
5e1a8e71853429515b57d70dbf44d2592d4ee287	image encryption using a new chaos based encryption algorithm	image encryption;chaotic system;confusion;diffusion process;diffusion;computer simulation	Encryption is a process of converting an image from readable to unreadable form. Various Image encryption algorithm based on permutation and diffusion have been proposed However, most of the algorithm for the permutation process uses the chaos sequence to permute the image and considerably takes more time in shuffling the position of the image pixel. In this paper we proposed a new effective algorithm based on permutation-diffusion for image encryption to reduce the processing time of the encryption considerably. To make the encryption more stronger and confused, same image is fed to the diffusion process after permutation process. In the present work diffusion process is carried out with the traditional AES algorithm. Theoretical analyses and computer simulations both confirm that the new algorithm has high security and is very fast for practical image encryption.	algorithm;computer simulation;encryption;pixel	Anuja Kumar Acharya	2011		10.1145/1947940.1948060	multiple encryption;watermarking attack;disk encryption theory;40-bit encryption;plaintext-aware encryption;computer science;theoretical computer science;distributed computing;deterministic encryption;probabilistic encryption;algorithm	Graphics	38.61652390757924	-8.787471977246016	15930
900b08d47d4b726c18dbd5d5b19354f822617f36	optimal control problem of treatment for obesity in a closed population		Variety of intervention programs for controlling the obesity epidemic has been done worldwide. However, it is still not yet available a scientific tool to measure the effectiveness of those programs.This is due to the difficulty in parameterizing the human interaction and transition process of obesity. A dynamical model for simulating the interaction between healthy people, overweight people, and obese people in a randomly mixed population is discussed in here. Two scenarios of intervention programs were implemented in the model, dietary program for overweight people with healthy life campaign and treatment program for obese people. Assuming all control rates are constant, disease free equilibrium point, endemic equilibrium point, and basic reproductive ratio (R 0 ) as the epidemic indicator were shown analytically. We find that the disease free equilibrium point is locally asymptotical stable if and only if R 0 < 1. From sensitivity analysis of R 0 , we obtain that larger rate of dietary program and treatment program will reduce R 0 significantly. With control rates are continuous in time, an optimal control approach was applied into the model to find the best way to minimize the number of overweight and obese people. Some numerical analysis and simulations for optimal control of the intervention were shown to support the analytical results.	asymptote;control theory;numerical analysis;optimal control;randomness;simulation	Dipo Aldila;Niken Rarasati;Nuning Nuraini;Edy Soewono	2014	Int. J. Math. Mathematical Sciences	10.1155/2014/273037	mathematical optimization;mathematics	Vision	32.27884763688747	-13.174512009723713	15936
19d2367268ca655d8514ac27a6dcfd9942d0a564	a review of fuzzy and mathematic methods for dynamic parameter adaptation in the firefly algorithm		The firefly algorithm is a bioinspired metaheuristic based on the firefly’s behavior. This paper presents a review on previous works on parameters analysis and dynamical parameter adjustment, using different mathematical approches and fuzzy logic.	firefly algorithm	Oscar Castillo;Carlos Soto;Fevrier Valdez	2018		10.1007/978-3-319-67946-4_13	fuzzy logic;firefly algorithm;metaheuristic;firefly protocol;mathematical optimization;optimization problem;computer science	NLP	25.10940512286457	-5.730267072313804	15988
93715e2e0568e0ee2e127e61b88a78644dfc5fdb	dynamic distance minimization problems for dynamic multi-objective optimization		In this article we propose a new dynamic multi-objective optimization problem. This dynamic Distance Minimization Problem (dDMP) functions as a benchmark problem for dynamic multi-objective optimization and is based on the static versions from the literature. The dDMP introduces a useful property and challenge for dynamic multi-objective algorithms. Not only the positions of the Pareto-optimal solutions in the search space change over time, but also the complexity of the problem can be adjusted dynamically. In addition the problem is based on a simple geometric structure, which makes it useful to visualize the search behaviour of algorithms. We describe the basic principles of the problem, and introduce the possible dynamic changes and their implementation and effects of the Pareto-optimal areas. Our experiments show how a possible instance of the dynamic DMP can be defined and how different algorithms react to the dynamic changes.	algorithm;benchmark (computing);dynamic programming;experiment;mathematical optimization;multi-objective optimization;optimization problem;pareto efficiency	Heiner Zille;Andre Kottenhahn;Sanaz Mostaghim	2017	2017 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2017.7969411	computer science;artificial intelligence;mathematical optimization;machine learning;euclidean distance;dynamic problem;minification;multi-objective optimization;benchmark (computing);optimization problem	DB	24.816223022341248	-4.431684828672258	16006
1ffe0dcc4ea796d706da24d91b16164301b78599	key view selection in distributed multiview coding	key view positioning;depth image based rendering distributed multiview coding key view positioning inter view correlation;encoding correlation image reconstruction three dimensional displays cameras decoding image coding;3d scene geometry key view selection distributed multiview coding multiview image systems multiview video systems data volumes interview redundancies;distributed multiview coding;inter view correlation;depth image based rendering;optimisation image coding	Multiview image and video systems with large number of views lead to new problems in data representation, transmission and user interaction. In order to reduce the data volumes, most distributed multiview coding schemes exploit the inter-view redundancies at the decoder side, using view synthesis from key views. In the situation where many views are considered, the two following questions become fundamental: i) how many key views have to be chosen for keeping a good reconstruction quality with reasonable coding cost? ii) where to place them optimally in the multiview sequences? We propose in this paper an algorithm for selecting the key views in a distributed multiview coding scheme. Based on a novel metric for the correlation between the views, we formulate an optimization problem for the positioning of the key views such that both the distortion of the reconstruction and the coding rate cost are effectively minimized. We then propose a new optimization strategy based on shortest path algorithm that permits to determine both the optimal number of key views and their positions in the image set. We experimentally validate our solution in a practical distributed multiview coding system and we show that considering the 3D scene geometry in the key view positioning brings significant rate-distortion improvements compared to distance-based key view selection as it is commonly done in the literature.	data (computing);decimation (signal processing);dijkstra's algorithm;distortion;experiment;mathematical optimization;multiview video coding;optimization problem;shortest path problem;view synthesis	Thomas Maugey;Giovanni Petrazzuoli;Pascal Frossard;Marco Cagnazzo;Béatrice Pesquet-Popescu	2014	2014 IEEE Visual Communications and Image Processing Conference	10.1109/VCIP.2014.7051612	computer vision;computer science;theoretical computer science;multimedia;multiview video coding	Vision	46.58390308646646	-20.908804203159598	16034
bc6bb7e3fb00241615aa9cfadf6bda6ad5339e00	capacitated arc routing problem in uncertain environments	optimal solution;capacitated arc routing problem;optimisation;robustness vehicles stochastic processes random variables maintenance engineering benchmark testing routing;routing;combinatorial optimization problem;maintenance engineering;random variables;combinatorial optimization problem uncertain capacitated arc routing problem uncertain parameters random variables;evolutionary and fuzzy computation;stochastic processes;uncertain capacitated arc routing problem;optimisation combinatorial mathematics;random variable;neural;robustness;vehicles;uncertain parameters;combinatorial mathematics;benchmark testing	In this paper, the Uncertain CARP (UCARP) is investigated. In UCARP, the demands of tasks and the deadheading costs of edges are stochastic and one has to design a robust solution for all possible environments. A problem model and a robustness measure for solutions are defined according to the requirements in reality. Three benchmark sets with uncertain parameters are generated by extending existing benchmark sets for static cases. In order to explore the solution space of UCARP, the most competitive algorithms for static CARP are tested on one of the generated uncertain benchmark sets. The experimental results showed that the optimal solution in terms of robustness in uncertain environment may be far away from the optimal one in terms of quality in a static environment and thus, utilizing only the expected value of the random variables can hardly lead to robust solutions.	algorithm;arc routing;benchmark (computing);best, worst and average case;common address redundancy protocol;evolutionary algorithm;expectation propagation;feasible region;mathematical optimization;requirement;tree traversal	Yi Mei;Ke Tang;Xin Yao	2010	IEEE Congress on Evolutionary Computation	10.1109/CEC.2010.5586031	maintenance engineering;random variable;mathematical optimization;computer science;machine learning;mathematics;statistics	Robotics	30.052098302253665	1.5519774710932046	16073
47f0d7dc5c08690f1d7c29935cb1c145c39b388d	sure-let for orthonormal wavelet-domain video denoising	video denoising;temporal correlation;libre mercado;transformation ondelette;time correlation;nivel ruido;traitement signal;noise reduction motion compensation statistics video sequences image sequences image coding biomedical imaging educational institutions wavelet transforms motion estimation;transform;evaluation performance;additive white gaussian noise;image coding;grayscale video sequences;performance evaluation;threshold detection;motion compensation;complexite calcul;cycle spinning strategy;correlation temporelle;linear expansion of threshold approach;image matching;wavelet thresholding;evaluacion prestacion;wavelet base;simulation;search algorithm;base ondita;niveau bruit;biomedical imaging;stein unbiased risk estimator;simulacion;motion estimation;video sequences;awgn;bruit blanc gaussien additif;selective block matching;wavelet block matching stein s unbiased risk estimator linear expansion of thresholds sure let video denoising;gray scale;marche concurrentiel;motion compensated;reduccion ruido;algorithme;estimacion insesgada;wavelet transforms;etat actuel;algorithm;compensation mouvement;detection seuil;cibm sp;complejidad computacion;correlacion temporal;deteccion umbral;noise level;senal video;signal video;temporal correlations;risk estimation;computational complexity;filter;orthonormal wavelet domain video denoising algorithm;signal processing;noise reduction;state of the art;image sequence;reduction bruit;statistics;correspondencia bloque;video signal;block matching;spatio temporal correlations;estado actual;secuencia imagen;stein s unbiased risk estimator linear expansion of thresholds sure let;interframe noise statistics;strong correlation;image denoising;transformacion ondita;open market;unbiased estimation;block motion estimation;correspondance bloc;redundant wavelet based techniques;wavelet transforms awgn image denoising image matching image sequences motion compensation;echelle gris;base ondelette;procesamiento senal;global motion	We propose an efficient orthonormal wavelet-domain video denoising algorithm based on an appropriate integration of motion compensation into an adapted version of our recently devised Stein's unbiased risk estimator-linear expansion of thresholds (SURE-LET) approach. To take full advantage of the strong spatio-temporal correlations of neighboring frames, a global motion compensation followed by a selective block-matching is first applied to adjacent frames, which increases their temporal correlations without distorting the interframe noise statistics. Then, a multiframe interscale wavelet thresholding is performed to denoise the current central frame. The simulations we made on standard grayscale video sequences for various noise levels demonstrate the efficiency of the proposed solution in reducing additive white Gaussian noise. Obtained at a lighter computational load, our results are even competitive with most state-of-the-art redundant wavelet-based techniques. By using a cycle-spinning strategy, our algorithm is in fact able to outperform these methods.	additive white gaussian noise;algorithm;computation;distortion;global motion compensation;grayscale;noise reduction;simulation;stationary wavelet transform;thresholding (image processing);utility functions on indivisible goods;video denoising	Florian Luisier;Thierry Blu;Michael Unser	2010	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2010.2045819	additive white gaussian noise;computer vision;speech recognition;computer science;signal processing;mathematics;video denoising;algorithm;statistics	Vision	45.97146749710355	-15.019525086977008	16099
56d478375084ee08ea41da8469f3f920cb4b0eca	overview of friedman's test and post-hoc analysis	nonparametric tests;62j15;multiple comparison methods;related samples;62k10	When the null hypothesis of Friedman’s test is rejected, there is a wide variety of multiple comparisons that can be used to determine which treatments differ from each other. We will discuss the contexts where different multiple comparisons should be applied, when the population follows some discrete distributions commonly used to model count data in biological and ecological fields. Our simulation study shows that sign test is very conservative. Fisher’s LSD and Tukey’s HSD tests computed with ranks are the most liberal. Theoretical considerations are illustrated with data of the Azores Buzzard (Buteo buteo rothschildi) population from Azores, Portugal.	hoc (programming language)	Dulce G. Pereira;Anabela Afonso;Fátima Melo Medeiros	2015	Communications in Statistics - Simulation and Computation	10.1080/03610918.2014.931971	nonparametric statistics;econometrics;demography;mathematics;statistics	ML	30.588244046383856	-22.072837285139872	16156
95a6b260c288f0893eb8c61e36cd5c27a6800f4c	beyond multi-adjoint logic programming	fuzzy logic programming;06b23;90c70;97p50;adjoint pairs;multi adjoint logic programming;68n17	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	computational science;correctness (computer science);francis;fuzzy logic;multi-adjoint logic programming;multiverse;nl (complexity);primary source;term algebra;theory;visit	Ginés Moreno;Jaime Penabad;Carlos Vázquez	2015	Int. J. Comput. Math.	10.1080/00207160.2014.975218	fuzzy logic;mathematical optimization;mathematical analysis;discrete mathematics;mathematics;algorithm;algebra	Robotics	49.32729662471426	-2.9561282862156117	16167
a42f6ee1c088db546f94a75c692a55cc38724d53	coding algorithm for grayscale images - design of piecewise uniform quantizer with golomb-rice code and novel analytical model for performance analysis			algorithm;golomb coding;golomb ruler;grayscale;profiling (computer programming);quantization (signal processing)	Nikola Simic;Zoran H. Peric;Milan S. Savic	2017	Informatica, Lith. Acad. Sci.			EDA	42.18788432452765	-15.807721744017744	16244
5eb802fce5e36159f3bd3b55297614a5f4232f49	composite source modling based on vq and arithmetic coding for digital subband video compression	image sampling;image coding;nonstationary character;arithmetic coding;subband images;huffman coding;subband decomposition;video compression;entropy coding;composite source modeling;contracts;digital subband video compression;bit rate;huffman codes;image texture;local power spectra;video coding;scalar quantization;arithmetic codes;vq;vector quantization;image compression;lts3;entropy codes;subband samples;lower bit costs;digital arithmetic;image sampling video coding vector quantisation source coding arithmetic codes entropy codes;subband samples arithmetic coding digital subband video compression composite source modeling vq entropy coding subband images nonstationary character statistical dependencies image compression local power spectra image texture subband decomposition lower bit costs vector quantization entropy coded scalar quantization simulation results;statistical dependencies;vector quantizer;digital video;entropy coded scalar quantization;digital arithmetic video compression entropy coding switches image coding vector quantization bit rate huffman coding contracts video coding;lts1;vector quantisation;switches;simulation results;source coding	We describe a new source model for entropy coding of subband images. It considers and exploits the nonstationary character and the statistical dependencies present in such data in order to achieve higher compression compared to classical methods based on zero order modeling of the subbands. Indeed, images are made up of various regions with different local power spectra, or textures, separated by edges. Therefore, after subband decomposition, the complex and variable statistical dependencies across the subbands at a particular spatial location should be exploited by a proper modeling in order to achieve lower bit costs. A composite source model is proposed for this purpose. It is based on the combination of vector quantization and entropy-coded scalar quantization. An application to digital video compression is also described. Simulation results show a gain of the composite source model in term of bit rate of around 15% to 20% compared to the classical runlength/Huffman coding of the subband samples. >	arithmetic coding;data compression;vector quantization	André Nicoulin;Marco Mattavelli	1994		10.1109/ICIP.1994.413609	speech recognition;computer science;theoretical computer science;mathematics;algorithm;statistics;huffman coding	Graphics	44.298768688343664	-15.31259361135686	16293
228845610ac261efc472761ed849913c11a1b8ce	efficient emulations for x-trees and /w-ary trees	g 2 2;parallel algorithm;1 2 8;simulation;emulation;interconnection network;x trees;interconnection networks;load balance;m ary trees;algorithm design;c 2 1;parallel algorithms;binary tree	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Daw-Jong Shyu;Biing-Feng Wang;Chuan Yi Tang	1998	Parallel Algorithms Appl.	10.1080/01495739808947362	parallel computing;computer science;theoretical computer science;distributed computing;parallel algorithm	Robotics	48.429818410181454	-2.948988820489032	16390
8206b72fb25ca28d45821281d74aec88af946259	prediction interval with examples of similar pattern and prediction strength		In this paper, we formed prediction intervals using historical similarities, found through the direct correlation. At first, a string of 5 to 20 recent samples is correlated with a long training string of samples. Then, the highest normalized correlation values and corresponding indexes are picked. After that, the amplitudes of the matched samples are adjusted by multiplying the value with the amplitude of recent string and by dividing by the amplitude of matched strings. These adjusted samples are actually the prediction values. Each prediction value is given a weight (relevance) based on the value of normalized correlation and a function of the ratio between amplitudes of strings. A bar chart is drawn using the weighted (relevance) distribution and less relevant regions are discarded from sides. A prediction strength is calculated from relevances. Except for the calculation of relevance, everything is calculated without any assumption. The user can check similar occurrences and decide to search more when the prediction strength is low.	algorithm;black box;coefficient;gene prediction;polynomial;relevance;string (computer science)	H. M. Dipu Kabir;Mohammad Anwar Hosen;Saeid Nahavandi;Abbas Khosravi	2017	2017 IEEE 30th Canadian Conference on Electrical and Computer Engineering (CCECE)	10.1109/CCECE.2017.7946723	normalization (statistics);statistics;artificial neural network;bar chart;amplitude;prediction interval;division (mathematics);mathematics;correlation	Vision	29.802215244207368	-13.623128493944987	16434
e29b18446e4574a2a4cbec18b5e46a6ae1178ca1	the impact of variation operators on the performance of sms-emoa on the bi-objective bbob-2016 test suite	benchmarking;bi objective optimizatio;bi objective optimization;black box optimization	The S-metric-Selection Evolutionary Multi-objective Optimization Algorithm (SMS-EMOA) is one of the best-known indicator-based multi-objective optimization algorithms. It employs the S-metric or hypervolume indicator in its (steady-state) selection by deleting in each iteration the solution that has the smallest contribution to the hypervolume indicator. In the SMS-EMOA, the conceptual idea is this hypervolume-based selection. Hence the algorithm can, for example, be combined with several variation operators. Here, we benchmark two versions of SMS-EMOA which employ differential evolution (DE) and simulated binary crossover (SBX) with polynomial mutation (PM) respectively on the newly introduced bi-objective family bbob-biobj test suite of the Comparing Continuous Optimizers (COCO) platform. The results unsurprisingly reveal that the choice of the variation operator is crucial for performance with a clear advantage of the DE variant on almost all functions.	algorithm;benchmark (computing);differential evolution;iteration;mathematical optimization;multi-objective optimization;polynomial;steady state;test suite	Anne Auger;Dimo Brockhoff;Nikolaus Hansen;Dejan Tusar;Tea Tusar;Tobias Wagner	2016		10.1145/2908961.2931705	mathematical optimization;machine learning;mathematics;algorithm;benchmarking	EDA	24.86651197203526	-4.4603197892800415	16470
52d3f964fc7e7e246dd55a0d5e950cb75e785850	a new constraint handling method based on the modified alopex-based evolutionary algorithm	aea;eda;constrained optimization problems;期刊论文;adaptive penalty function	In this paper, a new constraint handling method based on a modified AEA (Alopex-based evolutionary algorithm) is proposed. Combined with a new proposed ranking and selecting strategy, the algorithm gradually converges to a feasible region from a relatively feasible region. By introduction of an adaptive relaxation parameter l, the algorithm fully takes into account different functions corresponding to different sizes of feasible region. In addition, an adaptive penalty function method is employed, which adaptively adjust the penalty coefficient so as to guarantee a moderate penalty. By solving 11 benchmark test functions and two engineering problems, experiment results indicate that the proposed method is reliable and efficient for solving constrained optimization problems. Also, it has great potential in handling many engineering problems with constraints, even with equations. 2014 Elsevier Ltd. All rights reserved.	alopex;benchmark (computing);coefficient;constrained optimization;constraint (mathematics);converge;distribution (mathematics);estimation of distribution algorithm;evolutionary algorithm;feasible region;global optimization;iteration;linear programming relaxation;mathematical optimization;penalty method	Zhen Wang;Shaojun Li;Zhixiang Sang	2014	Computers & Industrial Engineering	10.1016/j.cie.2014.04.011	mathematical optimization;electronic design automation;computer science;machine learning;penalty method;mathematics;algorithm	AI	28.612674259480023	-1.3108359356989174	16488
636c0b441350a68dd3ed0808f841ca3fe55743cc	a gradient based fast search algorithm for warping motion compensation schemes	motion compensation signal processing algorithms taylor series motion estimation decoding approximation algorithms performance gain layout motion control performance evaluation;warping motion compensation schemes;computational load;prediction error;motion control;full search;performance evaluation;motion compensation;decoding;approximation algorithms;performance;search algorithm;motion estimation;layout;motion compensated;conference paper;first order;prediction error gradient based fast search algorithm warping motion compensation schemes first order approximation taylor series full search algorithm computational load performance;computational complexity;series mathematics;performance gain;full search algorithm;gradient based fast search algorithm;signal processing algorithms;first order approximation;computational complexity motion compensation series mathematics;taylor series	This paper describes a fast search algorithm f o r warping motion compensation schemes which is bused on a first order approximation of a Taylor series. In comparison to a full search algorithm the technique significantly reduces the required computational load (by a factor of approximately f i f ty) whilst maintaining the performance in terms of prediction error. Further gains in prediction error performance are expected as the new algorithm is investigated further.	computation;error message;gradient;motion compensation;order of approximation;search algorithm;while	David Benedict Bradshaw;Nick G. Kingsbury;Anil C. Kokaram	1997		10.1109/ICIP.1997.632193	motion control;layout;mathematical optimization;performance;computer science;taylor series;theoretical computer science;machine learning;mean squared prediction error;motion estimation;first-order logic;mathematics;orders of approximation;computational complexity theory;motion compensation;series;approximation algorithm;algorithm;search algorithm	Vision	49.08161893448048	-19.20071371321913	16510
311db64cacbabc54f9f33a8de8f24d682b815727	lossless compression-based steganalysis of lsb embedded images	lsb embedded images;support vector machines cryptography data compression data encapsulation image coding;image coding;data compression;decoding;support vector machines;lossless image compression;lossless compression;data encapsulation;least significant bit;steganography;support vector machine classifier;image generation;cryptography;pixel;statistics;support vector machine classification;entropy;support vector machine;lsb steganalysis methods;lsb replacement lossless image compression lsb embedded images least significant bit support vector machine classifier lsb steganalysis methods;lsb replacement;image coding steganography statistics support vector machines support vector machine classification entropy pixel laboratories image generation decoding	A new method of steganalysis for images with embedded messages is presented. We consider two embedding methods: least significant bit (LSB) replacement and plusmn1 LSB embedding. Our method uses lossless image compression to generate statistics that are fed into a support vector machine classifier. We compare results against the pairs method, one of the best existing LSB steganalysis methods. Both our method and pairs performs well for LSB replacement. However, while pairs cannot detect plusmn1 LSB embedding at all, our method can.	embedded system;image compression;least significant bit;lossless compression;most significant bit;steganalysis;support vector machine	Charles G. Boncelet;Lisa M. Marvel	2007	2007 41st Annual Conference on Information Sciences and Systems	10.1109/CISS.2007.4298442	support vector machine;computer vision;computer science;theoretical computer science;pattern recognition;mathematics;lossless compression;statistics	EDA	41.73737167612418	-11.932308738347738	16563
488c2714d57618b6085256ef207d7fae85e7d198	64 kbit/s audio signal transmission approaches using 32 kbit/s adpcm channel banks	pulse code modulation;communication system;haute performance;telephone systems encoding filtering and prediction theory multiplexing equipment private telephone exchanges pulse code modulation;transmission son;performance test;codificacion adaptativa;modulacion impulsion codificacion;canal transmision;wide band;digital transmission;multiplexing equipment;codage adaptatif;red privada;filtering and prediction theory;sound transmission;large bande;private telephone exchanges;quadrature mirror filter;private network;taux transmission;adaptive coding;canal transmission;transmission channel;relacion transmision;signal audionumerique;smooth transition;alto rendimiento;transmision numerica;7 khz audio signal transmission adpcm channel banks wideband coding adaptive digital pulse code modulated telephone systems private networks audio program transmission samplers multiplexers quadrature mirror filters coding quality audio signal transmission wideband communication systems 64 kbits s 32 kbits s 4 khz;modulation impulsion codage;transmission rate;audiodigital signal;reseau prive;transmission numerique;telephone systems;encoding;high performance;transmision sonido;ancho banda;channel bank filters multiplexing codecs wideband telephony mirrors audio coding hardware circuits testing	Two simple 64-kb/s wideband coding approaches using 32-kb/s ADPCM (adaptive digital pulse-code modulated) channel banks are proposed and compared to CCITT 64 kb/s ADPCM, which is being recommended as CCITT G.722. These two, folding ADPCM and QMF ADPCM, are intended to pave the way for smooth transition from conventional 4-kHz band telephone systems to 7-kHz wideband systems in private networks. The first approach, supporting the high-quality audio program transmission, requires only samplers and multiplexers at the input and output ports of the channel banks. In the second approach, samplers and multiplexers are replaced by quadrature mirror filters in order to increase coding quality. Performance test results for audio signal transmission show that these simplified approaches provide an inexpensive way to introduce wideband communication systems. >	adaptive differential pulse-code modulation;data rate units	Masahiro Iwadare;Takao Nishitani	1988	IEEE Journal on Selected Areas in Communications	10.1109/49.607	pulse-code modulation;speech recognition;telecommunications;computer science;quadrature mirror filter;g.726;adaptive coding;private network;sound transmission class;communications system;encoding	Mobile	47.406521051821564	-8.528734072399622	16628
236d62ca6f04ad2bde22c955f01054cca5e5c8ea	wavelet coding of structured geometry data on triangular lattice plane considering rate-distortion properties	tecnologia electronica telecomunicaciones;wavelet coding;geometry compression;vrml;optimization;2 d structuring;tecnologias;grupo a			Hiroyuki Kaneko;Koichi Fukuda;Akira Kawanaka	2004	IEICE Transactions		simulation;vrml;computer graphics (images)	Theory	41.79768238520509	-18.3575679350744	16682
88cce0320950bb17bfce21090407492146746b7a	parameter identification of chaotic systems by hybrid nelder-mead simplex search and differential evolution algorithm	nelder mead simplex search;differential evolution;chaotic system;population size;parameter identification;optimization problem;multi dimensional;nelder mead;local search;chaotic systems;hybrid algorithm;numerical simulation	Parameter identification of chaotic systems is an important issue in nonlinear science and has attracted increasing interest from a variety of research and application fields. Essentially, parameter identification can be formulated as a multi-dimensional optimization problem. By combining differential evolution (DE) and Nelder-Mead (NM) simplex search, an effective hybrid algorithm named NMDE is proposed in this paper. By suitably fusing the DE-based evolutionary search and NM simplex-based local search, exploration and exploitation abilities can be well balanced and satisfactory optimization performances can be achieved. The NMDE hybrid algorithm is applied to parameter identification of several typical chaotic systems. Numerical simulation and comparisons with some typical existing algorithms demonstrate the effectiveness and robustness of the proposed hybrid NMDE algorithm. Moreover, the effects of noise and population size on the performances of NMDE are investigated as well.	chaos theory;differential evolution;nelder–mead method;simplex algorithm	Ling Wang;Ye Xu;Lingpo Li	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.08.110	computer simulation;differential evolution;optimization problem;mathematical optimization;population size;hybrid algorithm;computer science;artificial intelligence;local search;machine learning;mathematics;nelder–mead method	ML	26.326583451797934	-5.231584793229596	16699
431c1a57f48e7001ae8881c64ce62c4445e8adf5	modeling and generating multivariate time-series input processes using a vector autoregressive technique	vector autoregression;computacion informatica;vector autoregressive process;multivariate time series;ciencias basicas y experimentales;numerical integration;matematicas;input modeling;grupo a;computer simulation	We present a model for representing stationary multivariate time-series input processes with marginal distributions from the Johnson translation system and an autocorrelation structure specified through some finite lag. We then describe how to generate data accurately to drive computer simulations. The central idea is to transform a Gaussian vector autoregressive process into the desired multivariate time-series input process that we presume as having a VARTA (Vector-Autoregressive-To-Anything) distribution. We manipulate the autocorrelation structure of the Gaussian vector autoregressive process so that we achieve the desired autocorrelation structure for the simulation input process. We call this the correlation-matching problem and solve it by an algorithm that incorporates a numerical-search procedure and a numerical-integration technique. An illustrative example is included.	algorithm;autocorrelation;autoregressive model;computer simulation;machine translation;marginal model;numerical analysis;numerical integration;stationary process;time series	Bahar Biller;Barry L. Nelson	2003	ACM Trans. Model. Comput. Simul.	10.1145/937332.937333	computer simulation;econometrics;numerical integration;computer science;machine learning;star model;mathematics;autoregressive model;vector autoregression;statistics	Graphics	36.05694556281668	-20.67833461148977	16740
3b710219ab90993d0a84a013f53144df2b70390c	towards recent developments in the field of digital image forgery detection		Proliferation of powerful computers, innovative photo-editing software packages, and high resolution capturing devices leads to effortlessness in generating digital image falsification. Indeed, the security apprehension of digital images has ascended since a long period and diverse techniques for authenticating the integrity of digital images have been established. Conversely, significant exploration in this realm would be not only verifying the integrity of images but also correspondingly sensing the hints of tampering or falsification without necessitating additional prior information of the image content or any embedded watermarks. This paper reviews the recent developments in the field of digital image forgery detection. With the concept and methods, various applications of forgery detection are presented especially focusing on the use of soft computing for developing the techniques for forgery detection. The future of development and application of soft computing in digital image forgery detection is...	authentication;digital image	Tarun Kumar;Gourav Khurana	2018	IJCAT	10.1504/IJCAT.2018.10015265	digital image;systems engineering;digital image processing;engineering;computer vision;soft computing;software;authentication;artificial intelligence	Vision	36.547639475342734	-12.934356226714131	16857
39a4a1a8c745d7e87fa028e56bd5edc9fd5ca19f	bugs: a bug-based search strategy using genetic algorithms	genetic algorithm		genetic algorithm;software bug	Hitoshi Iba;Sumitaka Akiba;Tetsuya Higuchi;Taisuke Sato	1992			computer science;machine learning;artificial intelligence;genetic algorithm;best-first search;quality control and genetic algorithms	AI	25.21655625829294	0.31708734825045215	16905
1acbb992a24997cf028acabd2127fda00da4a78a	respondent-driven sampling and an unusual epidemic	stochastic epidemic model;probability theory and statistics;mathematical statistics;respondent driven sampling;reed frost;matematisk statistik;sannolikhetsteori och statistik;configuration model	Respondent-driven sampling (RDS) is frequently used when sampling hard-to-reach and/or stigmatized communities. RDS utilizes a peer-driven recruitment mechanism where sampled individuals pass on participation coupons to at most c of their acquaintances in the community (c = 3 being a common choice), who then in turn pass on to their acquaintances if they choose to participate, and so on. This process of distributing coupons is shown to behave like a new Reed-Frost type network epidemic model, in which becoming infected corresponds to receiving a coupon. The difference from existing network epidemic models is that an infected individual can not infect (i.e. sample) all of its contacts, but only at most c of them. We calculate R0, the probability of a major “outbreak”, and the relative size of a major outbreak in the limit of infinite population size and evaluate their adequacy in finite populations. We study the effect of varying c and compare RDS to the corresponding usual epidemic models, i.e. the case of c = ∞. Our results suggest that the number of coupons has a large effect on RDS recruitment. Additionally, we use our findings to explain previous empirical observations.	depth perception;population;sampling (signal processing)	Jens Malmros;Fredrik Liljeros;Tom Britton	2016	J. Applied Probability	10.1017/jpr.2016.17	econometrics;mathematical statistics;mathematics;statistics	AI	35.6414625861534	-17.92489173391625	16910
9f800cd099111966487e92e9b829d39c6a35b339	electronics development for the utah electrohydraulic total artificial heart	electrohydraulics;control systems;implanted electronics;pumps;totally implantable system embedded software electrohydraulic total artificial heart brushless dc motor commutator implanted electronics transcutaneous energy transfer telemetry system;energy transfer;electrohydraulic control equipment;electrohydraulics artificial heart brushless dc motors dc motors control systems circuits commutation blood telemetry pumps;blood;transcutaneous energy transfer;brushless dc motor commutator;electrohydraulic control equipment artificial organs;circuits;telemetry;dc motors;artificial organs;brushless dc motors;telemetry system;commutation;totally implantable system;brushless dc motor;electrohydraulic total artificial heart;embedded software;artificial heart	Recent developments in the electronics and embedded software of the electrohydraulic total artificial heart under development at the University of Utah are discussed. These include development of a new brushless DC motor commutator, implementation of the implanted electronics in hybrids, integration of a transcutaneous energy transfer (TET) and telemetry system into the controller, and packaging of a short-term, totally implantable system. A discussion of future plans for the totally implantable system is given. >		Gill B. Bearnson;S. R. Krivoy;R. D. Jarmin;J. R. Fratto;Pratap S. Khanwilkar;K. R. Crump;K. D. Smith	1993		10.1109/CBMS.1993.263011	energy transfer;electronic circuit;embedded software;computer science;control system;dc motor;telemetry	Robotics	53.06581477867796	-17.06749878078149	16916
f6aa699287c093ee4eff27cc1c00830406b9cdf8	boosting additive models using component-wise p-splines	prediccion;approximation l2;theorie filtrage;prediction error;stochastic process;41a15;approximation numerique;analisis datos;hyperparametre;62m20;shrinkage estimator;aproximacion l2;aproximacion numerica;additive model;variable selection;data analysis;aproximacion esplin;smoothing methods;iteraccion;estimation erreur;prediction theory;estimateur retrecissement;65d07;error estimation;spline approximation;smoothing;approximation spline;methode lissage;statistical computation;estimacion error;calculo estadistico;processus stochastique;l2 approximation;alisamiento;iteration;60g25;technische reports;analyse donnee;modelo aditivo;calcul statistique;numerical approximation;modele additif;estimation statistique;proceso estocastico;point of view;theorie prediction;estimacion estadistica;statistical estimation;prediction;lissage;smoothing spline;filtering theory;hyper parameter	We consider an efficient approximation of Bühlmann & Yu’s L2Boosting algorithm with component-wise smoothing splines. Smoothing spline base-learners are replaced by P-spline base-learners which yield similar prediction errors but are more advantageous from a computational point of view. In particular, we give a detailed analysis on the effect of various P-spline hyper-parameters on the boosting fit. In addition, we derive a new theoretical result on the relationship between the boosting stopping iteration and the step length factor used for shrinking the boosting estimates.	additive model;algorithm;approximation;b-spline;boosting (machine learning);computation;early stopping;iteration;smoothing spline;spline (mathematics);t-spline	Matthias Schmid;Torsten Hothorn	2008	Computational Statistics & Data Analysis	10.1016/j.csda.2008.09.009	stochastic process;shrinkage estimator;econometrics;hyperparameter;iteration;prediction;smoothing spline;calculus;mean squared prediction error;mathematics;additive model;data analysis;gradient boosting;statistics;smoothing	ML	32.75671182150348	-23.803543656362525	17006
cfa44442f873cf498e8451332772a037653292f5	incremental learning on a budget and its application to power electronics	shadow flicker;micro converter;photovoltaic;dc dc converter;model based control;kernel method;incremental learning on a budget	In this paper, we present an incremental learning method on a budget for embedded systems. We discuss its application for two power systems: a micro-converter for photovoltaic and a step down DC-DC-converter. This learning method is a variation of the general regression neural network but it is able to continue incremental learning on a bounded support set. The method basically learns new instances by adding new kernels. However, when the number of kernels reaches a predefined upper bound, the method selects the most effective learning option from several options: including replacing the most ineffective kernel with the new kernel, modifying of the parameters of existing kernels, and ignoring the new instance.#R##N##R##N#The proposed method is compared with other similar learning methods on a budget, which are based on kernel perceptron. Two examples of the application of the proposed method are demonstrated in power electronics. In these two examples, we show that the proposed system learns the properties of the control-objects during the services and realizes quick control.	power electronics	Koichiro Yamauchi;Yusuke Kondo;Akinari Maeda;Kiyotaka Nakano;Akihisa Kato	2013		10.1007/978-3-642-42042-9_43	kernel method;real-time computing;simulation;computer science;photovoltaic system;machine learning	HCI	37.4759897586903	-5.540687638572498	17108
d5910a46f7d29f5637df847d3b91209e221bd715	general methodology 2: a comparison of selective initialization bias elimination methods	performance measure;ease of use;steady state	When simulating a non-terminating system, the issue of initialization bias must be addressed. Many approaches have been developed to remove initialization bias from the output data. This paper provides a comparison of 5 selected methods applied to two slightly different 2-machine flow shop models. The experiment tests for statistical differences between mean and variance of the data used by each method to calculate steady state performance measures. Additionally, for each method, the practicality and ease-of-use for general applicability in larger modeled environments is discussed.	divergence (computer science);heuristic;linear programming relaxation;marginal model;rewriting;server (computing);simulation;steady state;truncation;welch's method	Jennifer R. Linton;Catherine M. Harmonosky	2002			econometrics;simulation;usability;computer science;steady state;statistics	AI	29.901581431331667	-22.881741361096193	17112
fca1bb51c33ceff7d4912cb0acfed3c34a8ab4ab	sparse high-noise gps trajectory data compression and recovery based on compressed sensing			compressed sensing;data compression;global positioning system;sparse	Guan Yuan;Mingjun Zhu;Shaojie Qiao;Zhixiao Wang	2018	IEICE Transactions		theoretical computer science;compressed sensing;mathematics;data compression;global positioning system;trajectory	Mobile	51.895146976499	-15.738276548446342	17124
ebca48cf98493c02541414893bec754c97ffd868	using organizational evolutionary particle swarm techniques to generate test cases for combinatorial testing	particle swarm;software;organizations particle swarm optimization testing software algorithms algorithm design and analysis lead software;pairwise coverage;pair wise coverage criterion combinatorial testing organizational evolutionary particle swarm algorithm;testing;organizational evolutionary;satisfiability;test cases;pairwise coverage organizational evolutionary particle swarm test cases;particle swarm optimizer;particle swarm optimisation combinatorial mathematics;lead;particle swarm optimization;software algorithms;organizations;particle swarm optimisation;combinatorial mathematics;algorithm design;algorithm design and analysis	Based on the analysis of the characteristics of combinatorial testing, an organizational evolutionary particle swarm algorithm (OEPST) to generate test cases for combinatorial testing is proposed. This algorithm is used to select the test cases of local optimal coverage in current environment based on these test cases, and then a test suite satisfying the pair-wise coverage criterion is built. The empirical results show that this approach can effectively reduce the number of test case.	eisenstein's criterion;evolutionary algorithm;mainframe computer;mathematical optimization;particle swarm optimization;swarm intelligence;test case;test suite	Xiaoying Pan;Hao Chen	2011	2011 Seventh International Conference on Computational Intelligence and Security	10.1109/CIS.2011.354	algorithm design;mathematical optimization;model-based testing;simulation;computer science;equivalence partitioning;machine learning;particle swarm optimization;algorithm	SE	25.775009464977252	-1.841766842960102	17233
27d6c4b8220a5e2c8352d4a877be7f3529d76f3e	a quickly convergent continuous ant colony optimization algorithm with scout ants	optimal solution;high dimensionality;research outputs;heuristic information;adaptive optimization;research publications;scout ant;function optimization;optimization problem;continuous optimization;success rate;chaotic sequence;numerical experiment;random search;ant colony optimization algorithm	Many studies on ants behavior have demonstrated that their food searching process starts with Scout Ants’ scouting all around for food. In this paper, we propose a novel Scout Ant Continuous Optimization (SACO) algorithm which can simulate the food searching process of the Scout Ants. In this algorithm, the solution space of an optimization problem is divided into m subspaces. One Scout Ant is assigned to each subspace, and a total number of m Scout Ants in m subspaces will cooperate in the whole food searching process. The location of a Scout Ant in a subspace corresponds to a solution in that subspace. When an ant moves, the change of its position is driven by two factors. The first factor is the independent, random ergodic search with a small moving step in the ant’s assigned subspace, and the second is the collaborative, global search inspired by the global heuristic information accumulated among m ants. Each of these two factors is weighed by an appropriate weight to balance its contribution to the moving step size. This balanced computation helps adapt optimization problems with different features. Our numerical experiments have demonstrated that, in addition to the high accuracy and success rate in seeking the optimized solutions, our algorithm also has very fast convergence speed and impressive performance in optimization applications in high-dimensional spaces.	algorithm;ant colony optimization algorithms;langton's ant;mathematical optimization;scout	Qingbao Zhu;Zhijun Yang;Wei Ma	2011	Applied Mathematics and Computation	10.1016/j.amc.2011.06.065	optimization problem;adaptive optimization;mathematical optimization;simulation;meta-optimization;random search;artificial intelligence;mathematics;continuous optimization;metaheuristic	ML	28.08277660713389	-3.1240442400555954	17259
a3cd805a3164366f764393a8131ba8958d863169	blocking artifacts suppression in block-coded images using overcomplete wavelet representation	artefacto;transformation ondelette;image coding;image processing;280203;data compression;institute for integrated and intelligent systems;transformation cosinus discrete;faculty of science environment engineering and technology;procesamiento imagen;indexing terms;traitement image;journal article;artefact;algorithme;wavelet representation artifacts suppression image compression block codes block discrete cosine transform wavelet based deblocking algorithm ringing artifacts dc quantization interval wavelet modulus maxima evolution;quantisation signal;wavelet transforms;algorithm;codage image;compression image;image compression;discrete cosine transforms;quantisation signal image coding block codes data compression discrete cosine transforms wavelet transforms;comparative study;block discrete cosine transform;quantization discrete cosine transforms image coding iterative algorithms discrete wavelet transforms bit rate information technology wavelet analysis image analysis wavelet domain;transformacion ondita;pre2009 image processing;block codes;wavelet transformation;blocking artifact;algoritmo;compresion imagen	It is well known that at low-bit-rate block discrete cosine transform compressed image exhibits visually annoying blocking and ringing artifacts. In this paper, we propose a noniterative, wavelet-based deblocking algorithm to reduce both types of artifacts. The algorithm exploits the fact that block discontinuities are constrained by the dc quantization interval of the quantization table, as well as the behavior of wavelet modulus maxima evolution across wavelet scales to derive appropriate threshold maps at different wavelet scales. Since ringing artifacts occur near strong edges, which can be located either along block boundaries or within blocks, suppression of block discontinuities does not always reduce ringing artifacts. By exploiting the behavior of ringing artifacts in the wavelet domain, we propose a simple yet effective method for the suppression of such artifacts. The proposed algorithm can suppress both block discontinuities and ringing artifacts effectively while preserving true edges and textural information. Simulation results and extensive comparative study with both iterative and noniterative methods reported in the literature have shown the effectiveness of our algorithm.	algorithm;artifact (error);blocking (computing);coefficient;deblocking filter;discrete cosine transform;effective method;iterative method;map;maxima;modulus of continuity;peak signal-to-noise ratio;pixel;ringing (signal);ringing artifacts;simulation;thresholding (image processing);wavelet;wavelet transform;zero suppression	Alan Wee-Chung Liew;Hong Yan	2004	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2004.825555	data compression;block code;computer vision;speech recognition;index term;image processing;image compression;computer science;theoretical computer science;comparative research;ringing artifacts;mathematics;algorithm;wavelet transform	Vision	45.74317875399345	-13.801709479913425	17371
141027875f4ffb866f1c62d176c17efb5766469b	a hybrid approach to artificial bee colony algorithm	varying population;life cycle;comprehensive learning;artificial bee colony algorithm;期刊论文	In this paper, we put forward a hybrid approach based on the life cycle for the artificial bee colony algorithm to generate dynamical varying population as well as ensure appropriate balance between exploration and exploitation. The bee life-cycle model is firstly constructed, which means that each individual can reproduce or die dynamically throughout the searching process and population size can dynamically vary during execution. With the comprehensive learning, the bees incorporate the information of global best solution into the search equation for exploration, while the Powell’s search enables the bees deeply to exploit around the promising area. Finally, we instantiate a hybrid artificial bee colony (HABC) optimizer based on the proposed model, namely HABC. Comprehensive test experiments based on the well-known CEC 2014 benchmarks have been carried out to compare the performance of HABC against other bio-mimetic algorithms. Our numerical results prove the effectiveness of the proposed hybridization scheme and demonstrate the performance superiority of the proposed algorithm.	artificial bee colony algorithm;british informatics olympiad;computational complexity theory;enhanced graphics adapter;evolution;experiment;genetic algorithm;mathematical optimization;numerical analysis;phase-shift oscillator;powell's method;whole earth 'lectronic link	Lianbo Ma;Yunlong Zhu;Dingyi Zhang;Ben Niu	2015	Neural Computing and Applications	10.1007/s00521-015-1851-x	biological life cycle;simulation;computer science;artificial intelligence;artificial bee colony algorithm	AI	26.437117548563187	-4.349996978597379	17443
13842046f6cc6f88557a0eaba10e64361fe93eb8	an improved random neighborhood graph approach	nearest neighbor searches;topology;random perturbation;path planning;point location;orbital robotics;navigation;performance improvement;feedback;data structures;robots;navigation feedback robots orbital robotics computer science sampling methods path planning topology data structures nearest neighbor searches;nearest neighbor search;computer science;sampling methods;potential function	As a general framework to determine a collision-free feedback motion strategies, the Random Neighborhood Graph (RNG) approach [19] defines a global navigation function over an approximate representation of the free configuration. In this paper, we improve the RNG approach in several aspects. We present an ANN-accelerated RNG construction algorithm to achieve near logarithmic running time in each iteration of the RNG expansion. Two probabilistic termination conditions of the RNG construction algorithm are presented and analyzed. To help overcome the difficulty of narrow corridors, we also introduce a randomized perturbation algorithm to enhance the sampling quality. Our implementation illustrates a significant performance improvement.	approximation algorithm;iteration;navigation function;randomized algorithm;sampling (signal processing);time complexity	Libo Yang;Steven M. LaValle	2002		10.1109/ROBOT.2002.1013370	robot;sampling;navigation;simulation;data structure;computer science;artificial intelligence;theoretical computer science;machine learning;point location;feedback;mathematics;motion planning;nearest neighbor search	Robotics	51.80402039545905	-23.940198479531595	17469
f3ad4275f446bee864ed2dae872886fb857be038	adaptive discretization of convex multistage stochastic programs	multistage stochastic programming;multistage;optimization problem;scenario tree;power management;portfolio management;adaptive discretization;scenario reduction;stochastic programming	min x FI(x,ξ) := ∑ i∈I pi f (xi,ξi) s.t. gt(n)(x 1, . . . ,xn,ζn) ≤ 0, xn ∈ Xt(n), n ∈ N(I) f (x,ξ), gt(x,ξ) convex in x, Xt convex, t = 1, . . . ,T Notation ξi i ∈ I scenarios of stoch. process ξ ξn := ξi,t(n) ζn := (ξ1, . . . ,ξn) xi i ∈ I decision vector for scenario i xn := xi,t(n) pi i ∈ I scenario probabilities N(I) nodes of scenario tree defined by scenarios in I t(n) timestage of node n T number of timestages	convex function;discretization;maxima and minima;multistage amplifier	Stefan Vigerske;Ivo Nowak	2007	Math. Meth. of OR	10.1007/s00186-006-0124-y	stochastic programming;optimization problem;mathematical optimization;discrete mathematics;machine learning;mathematics;project portfolio management	ML	33.94314673686581	3.624132739652134	17486
360e4c07518d492eabbb04ebc68ab3bb00409dfd	unequal loss protection for robust transmission of motion compensated video over the internet	transmision paquete;prediccion;error correcting code;motion compensation;codigo corrector error;packet loss;video quality;correction directe erreur;qualite image;motion compensated;packet loss rate;group of picture;asignacion optima;compensation mouvement;forward error correction;internet;senal video;signal video;error propagation;image quality;graceful degradation;allocation optimale;video signal;packet transmission;calidad imagen;motion compensated prediction;unequal loss protection;code correcteur erreur;optimal allocation;transmission paquet;prediction;video over the internet	This paper presents an unequal loss protection (ULP) scheme for robust transmission of motion compensated video over the Internet. By exploiting the temporal dependency between frames, forward error correction (FEC) codes across packets are assigned to different frames in a group of pictures in the sense of minimizing the effect of error propagation, thus improving video quality significantly. To achieve optimal allocation of FEC codes, we formulate the effect of packet loss on video quality degradation as an expected length of error propagation (ELEP) model, which makes sense intuitively, as fewer frames corrupted implies better quality of reconstructed video. Experimental results show the validity of the proposed ELEP model and that the associated ULP scheme is robust to burst packet loss in the Internet. More importantly, graceful degradation of video quality is achieved by the proposed scheme as the packet loss rate of an Internet connection increases.		Xiaokang Yang;Ce Zhu;Zhengguo Li;Xiao Lin;Zhengguo Feng;Si Wu;Nam Ling	2003	Sig. Proc.: Image Comm.	10.1016/S0923-5965(02)00128-5	image quality;fault tolerance;real-time computing;the internet;error detection and correction;prediction;telecommunications;computer science;video quality;propagation of uncertainty;forward error correction;packet loss;motion compensation;statistics	Vision	47.96031248499991	-15.220059787927157	17518
2dccd65e2e6484a197c29c93d71d79bcf500bceb	a methodology for pseudo-genetic stochastic modeling of discrete fracture networks	stochastic simulation;pseudo genetic;fracture;genetic approach;connectivity;growth	Stochastic simulation of fracture systems is an interesting approach to build a set of dense and complex networks. However, discrete fracture models made of planar fractures generally fail to reproduce the complexity of natural networks, both in terms of geometry and connectivity. In this study a pseudogenetic method is developed to generate stochastic fracture models that are consistent with patterns observed on outcrops and fracture growth principles. The main idea is to simulate evolving fracture networks through geometric proxies by iteratively growing 3D fractures. The algorithm defines heuristic rules in order to mimic the mechanics of fracture initiation, propagation, interaction and termination. The growth process enhances the production of linking structure and impacts the connectivity of fracture networks. A sensitivity study is performed on synthetic examples. The method produces unbiased fracture dip and strike statistics and qualitatively reproduces the fracture density map. The fracture length distribution law is underestimated because of the early stop in fracture growth after intersection.	algorithm;approximation;complex network;dynamic data;heuristic;linear algebra;quad flat no-leads package;simulation;software propagation;stochastic modelling (insurance);synthetic intelligence	François Bonneau;Vincent Henrion;Guillaume Caumon;Philippe Renard;Judith Sausse	2013	Computers & Geosciences	10.1016/j.cageo.2013.02.004	fracture;connectivity;stochastic simulation;mathematics;geotechnical engineering;statistics	ML	38.82684402990504	-22.50129333553199	17672
35b099d70582f29f5749f775cbf4808b928e201d	live demonstration: a sensor-processor array integrated circuit for high-speed real-time machine vision	sensor processor array integrated circuit discrete address events high level object information focal plane software programmable pixel parallel simd processor array scamp 5 vision chip high speed real time image processing capability high speed real time machine vision;sensor arrays computer vision image sensors integrated circuits;integrated circuits shape arrays real time systems educational institutions image processing measurement	A demonstration is made of the high-speed real-time image processing capabilities of the SCAMP-5 vision chip. The device provides a software-programmable 256×256 pixel-parallel SIMD processor array. In the example application, the IC can determine dimensions and the location of a single object, at a sustained rate of 100,000fps. At 30,000fps, the chip can return the same metrics from 5 objects. This is accomplished by use of near-sensor processing which circumvents the requirement to digitise images; all processing is done “on the focal plane” and only high-level object information is transmitted off-chip as discrete address-events.	array data structure;focal (programming language);high- and low-level;image processing;integrated circuit;machine vision;pixel;processor array;real-time transcription;requirement;simd	Stephen J. Carey;David Robert Wallace Barr;Bin Wang;Alexey Lopich;Piotr Dudek	2014	2014 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2014.6865166	embedded system;computer vision;electronic engineering;computer hardware;computer science	Embedded	42.56771883303776	-3.422355939406119	17713
12a756070cf9503643c5bc5b09e26dbba565f795	a multi-thread tabu search algorithm		Abstract This paper describes a novel refinement to a Tabu search algorithm that has been implemented in an attempt to improve the robustness of the search when applied to particularly complex problems. In this approach, two Tabu searches are carried out in parallel. Each search thread is characterised by it's own short term memory which forces that point out of local optima. However, the two search threads share an intermediate term memory so allowing a degree of information to be passed between them. Results are presented for both unconstrained and constrained numerical functions as well as a problem in the field of hydraulic circuit optimization. Simulation of hydraulic circuit performance is achieved by linking the optimization algorithm to the commercial simulation package Bathfp.	local optimum;loss function;mathematical optimization;numerical analysis;optimization problem;refinement (computing);search algorithm;simulation;tabu search;thread (computing)	Andy M. Connor	1999	CoRR		artificial intelligence;computer science;machine learning;theoretical computer science;tabu search;hill climbing;guided local search;metaheuristic;algorithm;intermediate-term memory;mathematical optimization;best-first search;search algorithm;beam search	AI	26.648931900609888	-0.6485021033845726	17718
bcdce6325b61255c545b100ef51ec7efa4cced68	an overview of gradient descent optimization algorithms		Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.	algorithm;black box;gradient descent;mathematical optimization	Sebastian Ruder	2016	CoRR		gradient method;nonlinear conjugate gradient method	ML	31.211410320989472	-0.6546369685561235	17722
1ca078a2ac12c53b81b4c322b199a125fccc6deb	bayesian degradation modeling for reliability prediction of organic light-emitting diodes	gibbs sampler;degradation model;organic light emitting diode oled;burn in;hierarchical bayesian model;stochastic compartment;change point	Simpler degradation models are generally preferred to simplify analytical procedure of failure-time estimation which follows the degradation modeling. However, the luminosity degradation of organic light-emitting diode (OLED) tends to exhibit an initial unstable period followed by stable and more gradual degradation. The degradation mechanisms of OLED luminosity are illustrated via a stochastic two-compartment model. Conjoining the data with prior information accumulated from field testing, we propose two hierarchical Bayesian models to characterize the nonlinear degradation path of OLED: Bayesian change-point regression model and Bayesian bi-exponential model. The hierarchical Bayesian models effectively fit the nonlinear degradation paths of OLEDs. Analytical results of OLED degradation indicate that reliability estimation from the hierarchical Bayesian models can be substantially improved over the log-linear model which has been widely accepted as a degradation model of light displays.	diode;elegant degradation;oled	Suk Joo Bae;Tao Yuan;Seong-Joon Kim	2016	J. Comput. Science	10.1016/j.jocs.2016.08.006	econometrics;simulation;gibbs sampling;computer science;machine learning;burn-in	Theory	29.768143611916802	-18.9993588013544	17772
6b44ca5097032cc1b9d1690fc1de9afa9c19b9ec	problem solving by intelligent water drops	rivers;problem solving algorithm intelligent water drops natural river system lake sea traveling salesman problem;lakes;problem solving evolutionary computation;computational complexity;travelling salesman problems;travelling salesman problems computational complexity lakes problem solving rivers;problem solving	"""In this paper, we propose a new problem solving algorithm called """"intelligent water drops"""" or IWD algorithm which is based on the processes that happen in the natural river systems and the actions and reactions that take place between water drops in the river and the changes that happen in the environment that river is flowing. It is observed that a river often chooses an optimum path regarding the conditions of its surroundings to get to its ultimate goal which is often a lake or sea. These ideas are embedded into the proposed algorithm for solving the traveling salesman problem or the TSP. The IWD algorithm is tested with artificial and standard TSP problems and the experimental results demonstrate that it is a very promising problem solving algorithm and deserves more research to improve it and/or to adapt it to other engineering problems."""	list of metaphor-based metaheuristics;problem solving;swarm intelligence	Hamed Shah-Hosseini	2007		10.1109/CEC.2007.4424885	2-opt;mathematical optimization;greedy algorithm;computer science;artificial intelligence;computational complexity theory;operations research;algorithm;3-opt;bottleneck traveling salesman problem	AI	24.768006998048268	1.232206960452799	17797
05fd9d9dd22f9ba5c83974717ec6ea2aecba1e66	the adaptive modulated wavelet transform image representation	rate distortion;adaptive modulation;performance comparison;instantaneous frequency;amplitude modulation frequency modulation modeling;modulated wavelet transform;hilbert transform;wavelet transform;frequency modulated;image representation;peak signal to noise ratio;amplitude modulated;adjusted 2d hilbert transform	In this paper, a new, called the adaptive modulated wavelet transform (AMWT) image representation is presented. One of the attractive features is that the informative instantaneous frequencies of images can be taken into account to improve the representation performance via adaptation of the modulating frequencies involved. The transform coefficients in both wavelet and modulated wavelet domains are uniformly quantized with several quantization levels. The computed peak signal-to-noise ratio values and entropies are used as rate distortion curves for performance comparison. Experimental results show that AMWT out performs wavelet transform for representing images containing textures with rapid variation in grays. 2002 Elsevier Science B.V. All rights reserved.	blocking (computing);coefficient;data integrity field;distortion;fm broadcasting;information;modulation;national supercomputer centre in sweden;peak signal-to-noise ratio;rate–distortion theory;wavelet transform	Hsi-Chin Hsin;Ching-Chung Li	2002	Pattern Recognition Letters	10.1016/S0167-8655(02)00154-X	wavelet;instantaneous phase;constant q transform;mathematical optimization;mathematical analysis;s transform;harmonic wavelet transform;hilbert transform;second-generation wavelet transform;peak signal-to-noise ratio;short-time fourier transform;continuous wavelet transform;link adaptation;fractional fourier transform;hilbert–huang transform;discrete fourier transform;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;fast wavelet transform;lifting scheme;hilbert spectral analysis;wavelet transform	Vision	41.9843086606743	-14.973374621963666	17820
9abdd8e28701fbf8827ad7096b23272947a9f477	a model based on biological invasions for island evolutionary algorithms	distributed eas;biological invasion;massive migration	Migration strategy plays an important role in designing effective distributed evolutionary algorithms. Here, a novel migration model inspired to the phenomenon known as biological invasion is adopted. The migration strategy is implemented through a multistage process involving large invading subpopulations and their competition with native individuals. In this work such a general approach is used within an island parallel model adopting Differential Evolution as the local algorithm. The resulting distributed algorithm is evaluated on a set of well known test functions and its effectiveness is compared against that of a classical distributed Differential Evolution.	differential evolution;distributed algorithm;distribution (mathematics);dynamic data exchange;evolutionary algorithm;experiment;instant messaging;local algorithm;multistage amplifier;polynomial-time approximation scheme	Ivanoe De Falco;Antonio Della Cioppa;Domenico Maisto;Umberto Scafuri;Ernesto Tarantino	2011		10.1007/978-3-642-35533-2_14	biology;simulation;ecology	AI	25.145032788153376	-6.0269347391304136	17833
d63cbcb024560921a9173c6a8742155fba894196	application of deterministic low-discrepancy sequences in global optimization	sample size;high dimensionality;theory and practice;multi level single linkage method;stochastic optimization;clustering method;random sequence;quasi monte carlo;global optimization;low discrepancy sequences	It has been recognized through theory and practice that uniformly distributed deterministic sequences provide more accurate results than purely random sequences. A quasi Monte Carlo (QMC) variant of a multi level single linkage (MLSL) algorithm for global optimization is compared with an original stochastic MLSL algorithm for a number of test problems of various complexities. An emphasis is made on high dimensional problems. Two different low-discrepancy sequences (LDS) are used and their efficiency is analysed. It is shown that application of LDS can significantly increase the efficiency of MLSL. The dependence of the sample size required for locating global minima on the number of variables is examined. It is found that higher confidence in the obtained solution and possibly a reduction in the computational time can be achieved by the increase of the total sample size N . N should also be increased as the dimensionality of problems grows. For high dimensional problems clustering methods become inefficient. For such problems a multistart method can be more computationally expedient.	algorithm;circuit complexity;cluster analysis;computation;discrepancy function;global optimization;linkage (software);loss function;low-discrepancy sequence;mathematical optimization;maxima and minima;monte carlo method;multi-storey car park;multistage amplifier;optimization problem;pseudorandom number generator;pseudorandomness;quantum monte carlo;quasi-monte carlo method;sampling (signal processing);time complexity	Sergei S. Kucherenko;Yury Sytsko	2005	Comp. Opt. and Appl.	10.1007/s10589-005-4615-1	sample size determination;quasi-monte carlo method;mathematical optimization;combinatorics;random sequence;stochastic optimization;mathematics;statistics;global optimization	ML	30.32965379091269	-11.905235193323017	17835
50e8d7533b52cd760f891b36c73a8bc6cfca8e31	rate-distortion optimized composition of hevc-encoded video	rate distortion;qp rate distortion optimized composition hevc encoded video global rate control scheme encoding quality network based video composition system input video sources rate distortion function psnr quantization points rd optimization problem video quality quantization parameter;standards;psnr;bit rate encoding video coding psnr resource management standards rate distortion;resource management;bit rate;video coding;video coding optimisation;encoding	This paper proposes a global rate control scheme to maximize the overall encoding quality of a network-based video composition system. The input video sources are encoded using HEVC. The rate-distortion function for each input video source is modelled based on the known PSNR and bitrates at two quantization points and the R-λ model in HEVC. By solving an R-D optimization problem, the proposed scheme is able to optimally allocate rate for each video source and maximize the overall video quality of the composition with respect to the global bandwidth budget. We have evaluated the proposed scheme against a coding scheme where the same quantization parameter (QP) is employed for all video sources. The experimental results show that up to 14.3% average bitrate reduction is achieved by the proposed rate allocation scheme.	distortion;high efficiency video coding;mathematical optimization;optimization problem;peak signal-to-noise ratio;rate–distortion theory	Weiwei Chen;Jan Lievens;Adrian Munteanu;Jürgen Slowack;Steven Delputte	2015	2015 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2015.7066316	scalable video coding;real-time computing;peak signal-to-noise ratio;computer science;resource management;theoretical computer science;coding tree unit;multimedia;rate–distortion optimization;motion compensation;h.261;encoding;multiview video coding	Robotics	47.05106492407237	-16.995648221853646	17861
3d97387fc0e7b7286cdd6b41b5c82b4b6cac3b9c	a reversible data hiding scheme for digital images using lau-side match prediction	data hiding;image coding;lau side match prediction;psnr;image matching;information hiding;stego image;reversible information hiding;image restoration;exclusive or operation;data mining;visual quality;reversible data hiding scheme;steganography;reversible information hiding reversible data hiding scheme lau side match prediction stego image exclusive or operation security enhancement left and upper side match prediction method;image edge detection;image quality;pixel;security enhancement;side match prediction steganography reversible data hiding;digital image;pixel image edge detection image restoration psnr data mining image coding image quality;steganography image coding image matching;left and upper side match prediction method;reversible data hiding;side match prediction	In 2004, Chang and Tseng proposed a side-match approach that used neighboring pixels value to evaluate how many secret will be embed into the cover pixels. Although, their approach can provide high capacity and keep acceptable quality of stego-image, pixel recovery can not be accomplished after data have been extracted. The proposed method is based on left-and-upper side-match prediction (also called LAU-SMP) method for reversible information hiding. This method is derived from irreversible data hiding by Chang and Tseng and also makes use of the difference between the predicted and original pixel values. An exclusive-OR operation is adopted so as to enhance security. The experimental results show that our embedding capacity is more than 1bpp and the PSNR value of the image can be maintained at around 40dB which is in very good performance in terms of embedding and visual quality.	bitstream;distortion;exclusive or;image quality;least significant bit;message authentication code;peak signal-to-noise ratio;pixel;resultant;server (computing);steganography	Chin-Fang Lee;Huei-Ju Tsai	2010	2010 Sixth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIHMSP.2010.16	computer vision;computer science;data mining;mathematics;internet privacy;information hiding;statistics	Robotics	39.98560675491242	-12.087036824386098	17886
015271506463077ade0e780000903ff880cddc2d	diffusion filtration with approximate bayesian computation	tuning particle filters;approximate bayesian computation bayesian filtration diffusion distributed filtration;tuning;wireless sensor networks belief networks monte carlo methods state space methods;particle filters;distributed filtration framework diffusion filtration approximate bayesian computation state space models sensor networks	Distributed filtration of state-space models with sensor networks assumes knowledge of a model of the data-generating process. However, this assumption is often violated in practice, as the conditions vary from node to node and are usually only partially known. In addition, the model may generally be too complicated, computationally demanding or even completely intractable. In this contribution, we propose a distributed filtration framework based on the novel approximate Bayesian computation (ABC) methods, which is able to overcome these issues. In particular, we focus on filtration in diffusion networks, where neighboring nodes share their observations and posterior distributions.	approximation algorithm;computation;state space	Kamil Dedecius;Petar M. Djuric	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178563	econometrics;mathematical optimization;particle filter;computer science;mathematics;statistics	Robotics	53.25455058913749	3.757970120230957	17897
f10695b708dea151c556af9269015567fa2d55cf	using rzl coding to enhance histogram-pair based image reversible data hiding		An improvement of histogram-pair based image reversible data hiding by using RZL (Reverse Zero-run Length) coding is proposed in this paper. The pre-processing to compress data to a shortest one is usually adopted for raising the PSNR (Peak Signal to Noise Ratio) in data hiding. Recently, the disagreements appear that we can get better PSNR by using RZL coding after compression. We proved that our histogram-pair based image reversible data hiding is suitable to use RZL to improve the performance. The PSNR can be raised by using different RZL methods, different parameters, different embedded capacity and different images. It is hard to apply RZL to given original data with different lengths. We proposed a method to solve that by adding some 0s to the original data to form a complete block, and the RZL needs an attached mark for lossless recovery. In our experiments it has been shown that the PSNR of image with histogram-pair based reversible data hiding by using RZL is higher than that without using RZL as the embedding data rate is not high. Zhang et al.’s RZL is better than Wong et al.’s in most cases. The average PSNR gain is about 1 dB for five test images at different payloads with the RZL used in this paper.	data rate units;decibel;embedded system;experiment;lossless compression;peak signal-to-noise ratio;preprocessor;run-length encoding	Xuefeng Tong;Guorong Xuan;Guangce Shen;Xiaoli Huan;Yun Q. Shi	2013		10.1007/978-3-662-43886-2_11	theoretical computer science;data mining;mathematics;algorithm	Robotics	40.36751359502599	-12.720637601028542	17941
064d6ccc20a284f43e83c15534d266bb1bd32666	formulation and preliminary application of an integrated model of microbial fuel cell processes		Microbial Fuel Cells (MFCs) are bioelectrochemical systems that directly convert chemical energy contained in organic matter bioconvertible substrate into electrical energy. Since the mid-90’s, researchers have attempted to simulate the bioelectrochemical activity of MFCs: in this paper, in order to develop an enhanced model capable of describing a complex bacterial community, such as that of a MFC, an earlier model formulated by Pinto et al. (2010) has been integrated with the ASM2d model, representing complex biological systems with multiple substrates (Henze et al., 2013). The resulting model is herein described, together with its application to long series of MFC operational data. Results are discussed, confirming the good performance of the new model.	biological system;microsoft foundation class library;simulation	Andrea G. Capodaglio;Daniele Molognoni;Arianna Callegari	2015		10.7148/2015-0340	waste management		53.225193280154926	-6.871399450554551	17955
0fafcc0238b39435ae6dbd2cc80a4f8d8a20963d	recent advances in simulation optimization: a conservative adjustment to the etss procedure	sample size;performance evaluation;simulation optimization	Two-stage selection procedures have been widely studied and applied in determining the required sample size (i.e., the number of replications or batches) for selecting the best of <i>k</i> designs. The <i>Enhanced Two-Stage Selection</i> (ETSS) procedure is a heuristic two-stage selection procedure that takes into account not only the variance of samples, but also the difference of sample means when determining the sample sizes. This paper discusses the use of a conservative adjustment with the ETSS procedure to increase the probability of correct selection. We show how the adjustment allocates more simulation replications or batches to more promising designs at the second stage. An experimental performance evaluation demonstrates the efficiency of the adjustment.	heuristic;mathematical optimization;performance evaluation;simulation	E. Jack Chen	2002			sample size determination;econometrics;engineering;statistics	AI	29.00914874224352	-16.509380354855004	18007
5f7b8c3a38a57a5f59285aef29756a60d86854fb	distributed sparse diffusion estimation based on set membership and affine projection algorithm		Abstract The wireless sensor networks (WSNs) is the result of evolution of wireless and networking technologies, micro-electromechanical systems, and micro-services. One important task which can be performed by nodes in WSNs is to find a common solution to a problem by using distributed processing. In this paper, we study the problem of distributed estimation, where a group of nodes are required to collectively estimate a sparse parameter vector of interest, and we solve it by an estimation algorithm based on the set membership (SM) and affine projection (AP) methods. At each iteration of the algorithm, in addition to the current data, the proposed algorithm uses data obtained from previous measurements to attain faster convergence rate. A method is also proposed to select the constraint bound for set membership such that the computational load is uniformly distributed among the nodes of network. Then the distributed estimation algorithm is analyzed and a closed form expression for the steady state mean square deviation (MSD) is developed. The performance of proposed method is assessed via computer simulations. The simulation results show that the proposed algorithm provides faster convergence rate and smaller steady state MSD value when compared to conventional methods such as diffusion least mean squares (LMS), distributed set membership normalized LMS (DSM-NLMS), and distributed APA. Moreover, it achieves lower computational load compared to the AP method. These advantages make the proposed method useful in sparse parameter vector estimation whenever the nodes have sufficient memory size.	algorithm;sparse matrix	Hamid Shiri;Mohammad Ali Tinati;Marian Codreanu;Sabalan Daneshvar	2018	Digital Signal Processing	10.1016/j.dsp.2017.10.022	mathematics;artificial intelligence;wireless sensor network;steady state;mathematical optimization;pattern recognition;rate of convergence;root-mean-square deviation;normalization (statistics);dykstra's projection algorithm;affine transformation;least mean squares filter	EDA	53.09725518103568	4.134618511395956	18058
bd6761fb990f33dfbfefbcbe77051515b3ad703f	spatial domain digital watermarking of multimedia objects for buyer authentication	watermarking;error correction codes;spread spectrum;authorisation;digital watermark;indexing terms;error correction code;error correction;visual databases error correction codes watermarking authorisation;watermark insertion algorithm spatial domain digital watermarking disjoint subsets error correcting codes frequency domains multimedia objects buyer authentication;watermarking authentication error correction codes image databases spatial databases error correction multimedia databases frequency domain analysis spread spectrum communication prom;frequency domain;visual databases	"""Most of the existing watermarking processes become vulnerable when the attacker knows the watermark insertion algorithm. This paper presents an invisible spatial domain watermark insertion algorithm for which we show that the watermark can be recovered, even if the attacker tries to manipulate the watermark with the knowledge of the watermarking process. The process incorporates buyer specific watermarks within a single multimedia object, and the same multimedia object has different watermarks that differ from owner to owner. Therefore recovery of this watermark not only authenticates the particular owner of the multimedia object but also could be used to identify the buyer involved in the forging process. This is achieved after spatially dividing the multimedia signal randomly into a set of disjoint subsets (referred to as the image key) and then manipulating the intensity of these subsets differently depending on a buyer specific key. These buyer specific keys are generated using a secret permutation of error correcting codes so that exact keys are not known even with the knowledge of the error correcting scheme. During recovery process a manipulated buyer key (due to attack) is extracted from the knowledge of the image key. The recovered buyer key is matched with the exact buyer key in the database utilizing the principles of error correction. The survival of the watermark is demonstrated for a wide range of transformations and forging attempts on multimedia objects both in spatial and frequency domains. We have shown that quantitatively our watermarking survives rewatermarking attack using the knowledge of the watermarking process more efficiently compared to a spread spectrum based technique. The efficacy of the process increases in scenarios in which there exist fewer numbers of buyer keys for a specific multimedia object. We have also shown that a minor variation of the watermark insertion process can survive a """"Stirmark"""" attack. By making the image key and the intensity manipulation proms specific for a buyer and with proper selection of error correcting codes, certain categories of collusion attacks can also be precluded."""	algorithm;authentication;code;digital watermarking;error detection and correction;existential quantification;programmable read-only memory;randomness;watermark (data file)	Dipti Prasad Mukherjee;Subhamoy Maitra;Scott T. Acton	2004	IEEE Transactions on Multimedia	10.1109/TMM.2003.819759	error detection and correction;digital watermarking;computer science;theoretical computer science;internet privacy;watermark;world wide web;computer security;statistics	Vision	37.441262248766115	-12.379328507977794	18163
11b68f9087b703049d4ac9e046164e57725aab0e	variable neighbourhood search based heuristic for k-harmonic means clustering	thesis	Although there has been a rapid development of technology an d increase of computation speeds, most of the real-world optimization problems still cannot be solved in a reasonable time. Some times it is impossible for them to be optimally sol ved, as there are many instances of real problems which cannot be addressed by comput ers at their present speed. In such cases, the heuristic approach can be used. Heuristic re search has been used by many researchers to supply this need. It gives a su fficient solution in reasonable time. The clustering problem is one example of this, formed in many applications. In this thesis, I suggest a Variable Neighbourhood Search (V NS) to improve a recent clustering local search called K-Harmonic Means (KHM). Many experi ments are presented to show the strength of my code compared with some algorithms from th e literature. Some counter-examples are introduced to show that KHM may de generate entirely, in either one or more runs. Furthermore, it degenerates and then stops in some familiar datasets, which significantly affects the final solution. Hence, I present a removing degenera cy code for KHM. I also apply VNS to improve the code of KHM after remov ing the evidence of degeneracy. Certificate of Originality I hereby certify that the work presented in this thesis is my o riginal research and has not been presented for a higher degree at any other university or inst itute.	algorithm;cluster analysis;computation;cylinder-head-sector;degeneracy (graph theory);heuristic;k-means clustering;local search (optimization);mathematical optimization	Abdulrahman Al-Guwaizani	2011			machine learning	AI	24.938067003828525	3.9516564667604177	18217
804e72793d68614b737b2fe5f0619eac40f05a84	rfid networks planning using a multi-swarm optimizer	multi swarm;metodo adaptativo;evaluation performance;reader;mise a jour;performance evaluation;identificacion por radiofrecuencia;network planning;optimizacion pso;telecommunication network planning;implementation;evaluacion prestacion;simulation;rfid network planning;ps 2 o;simulacion;methode adaptative;hierarchical interaction topology;pso;identification par radiofrequence;algoritmo genetico;actualizacion;optimization problem;ps2o;accuracy;funcion matematica;precision;particle swarm optimizer;lecteur;biomimetique;particle swarm optimization;robustesse;adaptive method;planification reseau telecommunication;algorithme genetique;radio frequency identification;mathematical function;optimisation pso;algorithme evolutionniste;lector;genetic algorithm;robustness;algoritmo evolucionista;fonction mathematique;evolutionary algorithm;implementacion;updating;optimization model;adaptive evolution;biomimetics;robustez	In this paper, we develop an optimization model for planning the positions of readers in the RFID network based on a novel Multi-swarm Particle Swarm Optimizer called PS2O. The main idea of PS2O is to extend the single population PSO to the interacting multi-swarms model by constructing hierarchical interaction topology and enhanced dynamical update equations. This algorithm, which is conceptually simple and easy to implement, has considerable potential for solving complex optimization problems. Simulation results show that the proposed PS2O algorithm proves to be superior for planning RFID networks than the standard PSO and other two evolutionary algorithms, namely Genetic Algorithm (GA) and Evolution Strategy (ES), in terms of optimization accuracy and computation robustness.	benchmark (computing);computation;converge;distribution (mathematics);enhanced graphics adapter;evolution strategy;evolutionary algorithm;extensibility;genetic algorithm;global optimization;interaction;interference (communication);load balancing (computing);mathematical optimization;multi-objective optimization;network topology;particle swarm optimization;phase-shift oscillator;radio-frequency identification;requirement;simulation;system requirements;tree network	Hanning Chen;Yunlong Zhu;Kunyuan Hu;Tao Ku	2009	2009 Chinese Control and Decision Conference	10.1016/j.jnca.2010.04.004	simulation;computer science;artificial intelligence;evolutionary algorithm;accuracy and precision;algorithm	AI	28.288994709388064	1.1354581342084653	18245
f68576361823e43c7d792fdec4a1eafadf867a90	a new parallel algorithm for simulation of spin-glasses in external fields	spin glass;metropolis algorithm;parallel algorithm;statistical parameters;external field;biological system modeling;glass;single instruction multiple data;numerical analysis;single instruction multiple data parallel algorithm spin glass system external field prototypical model disordered system metropolis algorithm monte carlo simulations method stable spin chains statistical parameters cluster computation gpu technology;prototypical model;heuristic algorithms;monte carlo simulations method;spin glasses;gpu technology;materials science;mathematical model;mathematical model glass biological system modeling equations numerical models graphics processing unit heuristic algorithms;numerical models;graphics processing unit;cluster computation;disordered system;spin glass system;monte carlo methods;stable spin chains;spin glasses monte carlo methods numerical analysis	Spin glasses are prototypical models for disordered systems which provide a rich source for investigations of a number of important and difficult applied problems of physics, chemistry, material science, biology, evolution, organization dynamics, hard-optimization, environmental and social structures, human logic systems, financial mathematics etc. Numerical studies of spin-glass systems are difficult to accomplish and in general only small moderate system sizes can be accessed. An effective algorithm for parallel simulation of spin-glass system is developed. In contrast to well known algorithms of Metropolis and others which are based on the Monte Carlo simulations method, the developed algorithm allows with high efficacy to construct stable spin-chains of arbitrary length in parallel and calculate all statistical parameters of spin-glass system of large sizes. We have implemented software using cluster computation (MPI technology) and GPU technology (CUDA programming language is used) as well. Since the ideology of GPU technology is SIMD (Single Instruction Multiple Data) and our implemented algorithm is from that class of problems, we obtained fully parallel implementation. We have tested the developed code on example of simulation 1D spin-glass in external fields and we were convinced of reliability and efficiency of calculations.	cuda;computation;formal system;graphics processing unit;mathematical optimization;metropolis;monte carlo method;parallel algorithm;programming language;simd;simulation;social structure	Ashot Gevorkyan;Hakob G. Abajyan;Hayk S. Sukiasyan	2011	2011 Proceedings of the 34th International Convention MIPRO		computational science;simulation;spin glass;computer science;theoretical computer science;statistics	HPC	43.047357599888244	1.5785555088251362	18246
386969a48c7e7b9817f1b63d981cc190f2583b11	solving 0-1knapsack problem based on rough set theory	directed evolution;rough set knapsack problem genetic algorithm;knapsack problems;approximation algorithms;search space;rough set theory;set theory;searching space knapsack problem rough set theory searching efficiency improvement genetic algorithm knowledge discovery function directed evolution;knapsack problem;heuristic algorithms;genetic algorithm;algorithm design and analysis gallium approximation algorithms heuristic algorithms set theory genetic algorithms europe;genetic algorithms;knowledge discovery function;europe;rough set theory genetic algorithms knapsack problems;rough set;searching space;algorithm design and analysis;gallium;knowledge discovery;searching efficiency improvement	A kind of algorithm is proposed in this paper to improve the searching efficiency, which combinates Rough Set Theory (RST) and Genetic Algorithm (GA) for 0-1 knapsack problem???The study is to utilize the knowledge discovery function of RST to find the important genes in GA. Then directed evolution is carried out according to the important genes. Finally,an example of four knapsack problem is used to test. The searching space is reduced and the important genes ensure the effective information will not be lost. The algorithm is able to improve the searching efficiency and the quality of GA .	genetic algorithm;intel matrix raid;knapsack problem;mathematical optimization;rough set;set theory	Zhijun Zhang;Yan Wu;Gaowei Yan	2010	2010 International Conference on Computational Aspects of Social Networks	10.1109/CASoN.2010.52	continuous knapsack problem;mathematical optimization;combinatorics;genetic algorithm;rough set;computer science;machine learning;mathematics;knowledge extraction;knapsack problem;approximation algorithm	Robotics	26.178010710665784	-1.4332904777246267	18271
3a825762b2474d81ac7c8a8f5c990da2b7e01597	statistical learning makes the hybridization of particle swarm and differential evolution more efficient—a novel hybrid optimizer	particle swarm;evolution methods;differential evolution;statistical learning;particle swarm optimizer;particle swarm optimization;multimodal functions;hybridization;global optimization;hybrid algorithm;chen jie xin bin peng zhihong pan feng 粒子群优化 统计学习 混合优化 差分进化 微分 杂交 混合算法 多式联运 statistical learning makes the hybridization of particle swarm and differential evolution more efficient a novel hybrid optimizer	This brief paper reports a hybrid algorithm we developed recently to solve the global optimization problems of multimodal functions, by combining the advantages of two powerful population-based metaheuristics—differential evolution (DE) and particle swarm optimization (PSO). In the hybrid denoted by DEPSO, each individual in one generation chooses its evolution method, DE or PSO, in a statistical learning way. The choice depends on the relative success ratio of the two methods in a previous learning period. The proposed DEPSO is compared with its PSO and DE parents, two advanced DE variants one of which is suggested by the originators of DE, two advanced PSO variants one of which is acknowledged as a recent standard by PSO community, and also a previous DEPSO. Benchmark tests demonstrate that the DEPSO is more competent for the global optimization of multimodal functions due to its high optimization quality.	benchmark (computing);differential evolution;global optimization;hybrid algorithm;machine learning;mathematical optimization;metaheuristic;multimodal interaction;particle swarm optimization	Jie Chen;Bin Xin;Zhihong Peng;Feng Pan	2009	Science in China Series F: Information Sciences	10.1007/s11432-009-0119-4	mathematical optimization;multi-swarm optimization;computer science;artificial intelligence;machine learning;mathematics;particle swarm optimization;global optimization	AI	25.165036891039353	-4.2159968801377365	18272
2ff84490ee1c6650c8ecf43f66f5a89094d9f9fe	enhanced two-bit transform based motion estimation via extension of matching criterion	psnr;image matching;motion estimation;binary matching characteristic;accuracy;artificial neural networks;computational complexity;peak signal to noise ratio;motion estimation block matching fullsearch two bit transform;transforms;artificial neural networks computational complexity motion estimation transforms psnr dynamic range accuracy;dynamic range;block matching;transforms computational complexity image matching motion estimation;fullsearch;binary matching characteristic motion estimation two bit transform matching criterion computational complexity;two bit transform;two bit transform matching criterion	Improved two-bit transform-based motion estimation algorithms are proposed in this paper. By extending the typical two-bit transform (2BT) matching criterion, the proposed algorithms enhance the motion estimation accuracy with almost the same computational complexity, while preserving the binary matching characteristic. Experimental results show that the proposed algorithm achieves peak-to-peak signal-to-noise ratio (PSNR) gains up to 0.42 dB compared with the conventional 2BT-based motion estimation.	algorithm;computational complexity theory;lossless compression;motion estimation;peak signal-to-noise ratio	Changryoul Choi;Jechang Jeong	2010	IEEE Transactions on Consumer Electronics	10.1109/TCE.2010.5606342	computer vision;mathematical optimization;peak signal-to-noise ratio;quarter-pixel motion;computer science;machine learning;pattern recognition;mathematics;artificial neural network	Vision	45.72862151713409	-17.21253460868304	18300
55c074fc3b19acf1aaa878887d9e33abb2bb8cc2	robust and efficient content-based digital audio watermarking	filigranage numerique;digital watermarking;analisis contenido;metodo adaptativo;audio signal processing;protection copie;digital watermark;copy protection;human auditory system;methode adaptative;copyright protection;content analysis;audibility;traitement signal audio;feature extraction;senal numerica;robustesse;filigrana digital;adaptive method;signal numerique;robustness;digital signal;extraction caracteristique;analyse contenu;digital audio watermarking;robustez;audio watermarking	This paper proposes a set of digital watermarking schemes for WAV audio, WAV-table synthesis audio and compressed audio. The watermark embedding scheme is closely related to audio content and based on the human auditory system. The experimental results in listening and robustness illustrate that the proposed watermarking schemes can achieve an optimal balance between audibility and robustness of the watermarked audio. The proposed methods are also very useful and effective for copyright protection, tracing illegal distribution and other applications.	digital watermarking	Changsheng Xu;David Dagan Feng	2002	Multimedia Systems	10.1007/s005300200055	computer vision;speech recognition;content analysis;telecommunications;digital watermarking;computer science;electrical engineering;speech coding;multimedia	EDA	43.688627845873846	-10.271799823155664	18314
c41540907a457256dcbe28c0773996438a77a425	on confidence bounds for one-parameter exponential families	relative surprise credible interval;62f25;one parameter exponential family;coverage probability;62f15;hpd credible interval;unbiased confidence interval;coverage length;pivotal quantity method	There exist various methods for providing confidence intervals for unknown parameters of interest on the basis of a random sample. Generally, the bounds are derived from a system of non-linear equations. In this paper, we present a general solution to obtain an unbiased confidence interval with confidence coe fficient 1− α in one-parameter exponential families. Also we discuss two Bayesian credible intervals, the highest posterior density (HPD) and relative surprise (RS) credible intervals. Standard criteria like the coverage length and coverage probability are used to assess the performance of the HPD and RS credible intervals. Simulation studies and real data applications are presented for illustrative purposes.	bayesian network;linear equation;nonlinear system;reed–solomon error correction;simulation;software studies;system of linear equations;time complexity	Mojtaba Alizadeh;S. Nadarajah;M. Doostparast;Abbas Parchami;Mashaallah Mashinchi	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2015.1006776	econometrics;confidence interval;prediction interval;data mining;mathematics;cdf-based nonparametric confidence interval;credible interval;robust confidence intervals;statistics	ML	29.746955817523368	-22.54839268024435	18414
e9cf01b2d10a1f25d5f639a6dbdf8aa0c5f0c16d	an improved magnetic charged system search for optimization of truss structures with continuous and discrete variables	size optimization;discrete variables;magnetic charged system search;harmony search;truss structures	An improved magnetic charged system search (IMCSS) is presented for optimization of truss structures.In IMCSS some of the most effective parameters in the convergence rate of HS scheme have been improved to achieve the best convergence.The IMCSS algorithm is applied for optimal design problem with both continuous and discrete variables. In this study, an improved magnetic charged system search (IMCSS) is presented for optimization of truss structures. The algorithm is based on magnetic charged system search (MCSS) and improved scheme of harmony search algorithm (IHS). In IMCSS some of the most effective parameters in the convergence rate of the HS scheme have been improved to achieve a better convergence, especially in the final iterations and explore better results than previous studies. The IMCSS algorithm is applied for optimal design problem with both continuous and discrete variables. In comparison to the results of the previous studies, the efficiency and robustness of the proposed algorithm in fast convergence and achieving the optimal values for weight of structures, is demonstrated.	mathematical optimization	A. Kaveh;B. Mirzaei;A. Jafarvand	2015	Appl. Soft Comput.	10.1016/j.asoc.2014.11.056	mathematical optimization;combinatorics;harmony search;computer science;control theory;mathematics	Logic	29.384430619109814	-3.370780549286976	18442
b600def3c525a972c2f5e29fcfa6a6c2a17b7ce6	goodness-of-fit tests based on spacings for progressively type-ii censored data from a general location-scale distribution	goodness of fit;sample size;normal distribution;extreme value distribution goodness of fit test graphical technique location scale family progressively type ii censored data test statistic sample spacing null distribution s normal distribution extreme value model gumbel model monte carlo simulation power approximation real data life testing;extreme value distribution;reliability theory;censored data;gumbel distribution;extreme value;life testing;goodness of fit test;simulation study;reliability theory normal distribution monte carlo methods life testing statistical testing;statistical testing;monte carlo simulation;critical value;testing statistical distributions statistical analysis gaussian distribution mathematics statistics distribution functions probability density function density functional theory shape;normal approximation;monte carlo methods	There has been extensive research on goodness-of-fit procedures for testing whether or not a sample comes from a specified distribution. These goodness-of-fit tests range from graphical techniques, to tests which exploit characterization results for the specified underlying model. In this article, we propose a goodness-of-fit test for the location-scale family based on progressively Type-II censored data. The test statistic is based on sample spacings, and generalizes a test procedure proposed by Tiku . The null distribution of the test statistic is shown to be approximated closely by a s-normal distribution. However, in certain situations it would be better to use simulated critical values instead of the s-normal approximation. We examine the performance of this test for the s-normal and extreme-value (Gumbel) models against different alternatives through Monte Carlo simulations. We also discuss two methods of power approximation based on s-normality, and compare the results with those obtained by simulation. Results of the simulation study for a wide range of sample sizes, censoring schemes, and different alternatives reveal that the proposed test has good power properties in detecting departures from the s-normal and Gumbel distributions. Finally, we illustrate the method proposed here using real data from a life-testing experiment. It is important to mention here that this test can be extended to multi-sample situations in a manner similar to that of Balakrishnan et al.	approximation algorithm;censoring (statistics);monte carlo method;sensor;simulation	N. Balakrishnan;H. K. T. Ng;N. Kannan	2004	IEEE Transactions on Reliability	10.1109/TR.2004.833317	econometrics;mathematical optimization;test statistic;chi-square test;mathematics;goodness of fit;anderson–darling test;statistics;monte carlo method;z-test	ML	30.398512047676654	-20.49423503143547	18477
eacfaec2d81945aafacb0264482546d35807712d	a novel svd and ls-svm combination algorithm for blind watermarking	ls svm;image blind watermarking;svd;geometrical distortion	This paper proposes a novel algorithm for blind watermarking by applying singular value decomposition and least squares support vector machine into watermark embedding and detection. In coding process, singular value decomposition is performed on coefficient blocks to obtain singular values after host image is transformed into integer wavelet transform domain. Subsequently, watermark image is embedded into transformed image by adaptively modulating the sample feature vectors constructed by singular values. In decoding process, the trained least square support vector machine is employed to extract the watermark image blindly by classifying samples derived from watermarked image. Experimental results show that the proposed scheme is not only robust against common noise-like attacks, such as noise, filter, crop, sharpen and JPEG compression, but also robust against geometrical distortions.	algorithm;least squares;singular value decomposition	Pan-Pan Zheng;Jun Feng;Zhan Li;Mingquan Zhou	2014	Neurocomputing	10.1016/j.neucom.2014.04.005	computer vision;theoretical computer science;pattern recognition;mathematics;singular value decomposition	ML	40.982579814798314	-10.465315471319357	18558
d3597a01ae87e9db80fc1cec6daa593f1d0aa901	bootstrap prediction for returns and volatilities in garch models	62f40;prediccion;finite sample;garch model;computacion informatica;metodo monte carlo;nonlinear models;corresponding author;stock market;marche financier;bootstrap;65c05;estimacion densidad;analisis datos;garch process;resampling methods;62m20;estimation densite;62e17;simulation;echantillon fini;intervalle prediction;methode monte carlo;simulacion;non linear model;non gaussian distributions;time series;stock markets;marche valeurs;density estimation;data analysis;modele garch;ciencias basicas y experimentales;monte carlo method;matematicas;indexation;statistical computation;calculo estadistico;financial market;60g25;finite sample properties;analyse donnee;calcul statistique;62p05;resampling method;monte carlo;grupo a;prediction interval;prediction;gaussian distribution;mercado financiero;nonlinear model	A new bootstrap procedure to obtain prediction densities of returns and volatilities of GARCH processes is proposed. Financial market participants have shown an increasing interest in prediction intervals as measures of uncertainty. Furthermore, accurate predictions of volatilities are critical for many financial models. The advantages of the proposed method are that it allows incorporation of parameter uncertainty and does not rely on distributional assumptions. The finite sample properties are analyzed by an extensive Monte Carlo simulation. Finally, the technique is applied to the Madrid Stock Market index, IBEX-35.	autoregressive integrated moving average;booting;bootstrapping (statistics);distributional semantics;estimation theory;experiment;gene prediction;monte carlo method;population parameter;simulation;spatial variability;density	Lorenzo Pascual;Juan Romo;Esther Ruiz	2006	Computational Statistics & Data Analysis	10.1016/j.csda.2004.12.008	econometrics;mathematics;statistics;monte carlo method	ML	32.81255588264394	-22.310467589924986	18620
47d3a186a2bf022cad7294fe5b60aeddc9d4f0b8	classical and imprecise probability methods for sensitivity analysis in engineering: a case study	65n30;modelizacion;analisis sensibilidad;aeroespacial;optimisation;probabilidad imprecisa;fiabilidad;reliability;aerospace;aerospace engineering;fuzzy set;computacion informatica;metodo monte carlo;analisis estadistico;65c05;optimizacion;securite;62p30;04e72;aerospatiale;reliability of structures;methode monte carlo;conjunto difuso;ensemble flou;intelligence artificielle;fuzzy sets;modelisation;simulation methods;imprecise probability;statistical analysis;probabilite imprecise;ciencias basicas y experimentales;sensitivity analysis;fiabilite;monte carlo method;safety;analyse statistique;analyse sensibilite;architecture basee modele;ensemble aleatoire;artificial intelligence;optimization;inteligencia artificial;grupo a;monte carlo simulation;seguridad;modeling;model driven architecture;random set;random sets;large scale problem;60 do5;arquitectura basada modelo;60d05;conjunto aleatorio	This article addresses questions of sensitivity of output values in engineering models with respect to variations in the input parameters. Such an analysis is an important ingredient in the assessment of the safety and reliability of structures. A major challenge in engineering applications lies in the fact that high computational costs have to be faced. Methods have to be developed that admit assertions about the sensitivity of the output with as few computations as possible. This article serves to explore various techniques from precise and imprecise probability theory that may contribute to achieving this goal. It is a case study using an aerospace engineering example and compares sensitivity analysis methods based on random sets, fuzzy sets, interval spreads simulated with the aid of the Cauchy distribution, and sensitivity indices calculated by direct Monte Carlo simulation. Computational cost, accuracy, interpretability, ability to incorporate correlated input and applicability to large scale problems will be discussed. 2008 Published by Elsevier Inc.	algorithmic efficiency;computation;fuzzy set;monte carlo method;simulation	Michael Oberguggenberger;Julian King;Bernhard Schmelzer	2009	Int. J. Approx. Reasoning	10.1016/j.ijar.2008.09.004	econometrics;computer science;artificial intelligence;mathematics;fuzzy set;statistics;monte carlo method	SE	27.53187249738647	-15.714493172899152	18674
9200fb746e8bd03e5f4ce5b7e083005f9259032a	adaptive digital filtering using the bio-inspired firefly algorithm		The Lévy Flights variation of the bio-inspired Firefly Algorithm (LFFA) combines the Firefly Algorithm based on how fireflies interact through flashes and a random-step function based on the Lévy distribution. The LFFA, the Genetic Algorithm (GA), and the Particle Swarm Optimization Algorithm (PSOA) are compared when used for adaptive IIR system identification. Comparisons of the LFFA in direct, cascade, and parallel form structures suggest that 1) the LFFA is more effective than the GA and PSOA, 2) the LFFA adaptive algorithm works well for all structures that were experimentally tested, including a nonlinear Volterra structure, and 3) the LFFA appears to perform most effectively.	adaptive algorithm;british informatics olympiad;experiment;firefly (cache coherence protocol);firefly algorithm;genetic algorithm;infinite impulse response;lévy flight;nonlinear system;particle swarm optimization;system identification	M. Hussain;W. K. Jenkins	2017	2017 51st Asilomar Conference on Signals, Systems, and Computers	10.1109/ACSSC.2017.8335460	mathematical optimization;genetic algorithm;firefly algorithm;adaptive algorithm;computer science;digital filter;adaptive filter;system identification;psoa;particle swarm optimization	Robotics	32.04325451416488	-5.3339788647008906	18685
afd9a93ccd46f65a7702e08b5148f76099afc6b2	rightful ownership through image adaptive dwt-svd watermarking algorithm and perceptual tweaking	tuning matrix;adaptive and blended watermarking;principal component;false positive problem;singular value decomposition	In this paper, a tailored blended image adaptive watermarking scheme has been presented, which is based on DWT and SVD. Through this paper an attempt has been made to solve the problem of false positive while maintaining the robustness and imperceptibility with the help of principal component and perceptual tuning of the image. Perceptual tuning is a non-blind technique and based on the objective quality of image. The embedding strength is made dependent on watermark features as well as of host in wavelet domain by using tuning parameter which is user specific. The idea of embedding the principal component of intermediate frequency sub-bands of watermark image into singular values of perceptually tuned intermediate frequency sub-bands of host image have been exploited. The proposed algorithm is providing the adaptive behavior towards the image content for perceptual transparency and at the same time avoiding the possibility of false watermark extraction well supported by a private key, which is necessary at the time of extraction. Thus the proposed watermarking algorithm is a kind of non-blind, image adaptive and suitable for rightful ownership. Various comparative results make the algorithm superior in terms of intentional and non-intentional attacks. Also the algorithm is strong against the print and scan attack.	adaptive behavior;algorithm;coefficient;database tuning;digital watermarking;discrete wavelet transform;experiment;image processing;intentionality;intermediate frequency;morphing;peak signal-to-noise ratio;principal component analysis;public-key cryptography;singular value decomposition;sub-band coding;tweaking	Punit Pandey;Shishir Kumar;Satish Kumar Singh	2013	Multimedia Tools and Applications	10.1007/s11042-013-1375-2	computer vision;theoretical computer science;multimedia	Vision	40.818361245587354	-10.424654621573996	18714
6a63b01cc173e91b1b57fd27ee473c8b11343b4f	optimal adaptive multistage image transform coding	transforms encoding optimisation picture processing;optimisation;image coding;mean square reconstruction error;optimal method;picture processing;mean square;transform coding image coding image reconstruction karhunen loeve transforms bit rate discrete transforms discrete cosine transforms statistics statistical analysis decorrelation;transform coding;bit rate;marginal analysis error minimisation optimisation adaptive multistage image transform coding optimal method bit allocation mean square reconstruction error statistics coefficients;error minimisation;karhunen loeve transforms;statistical analysis;discrete transforms;discrete cosine transforms;coefficients;image reconstruction;marginal analysis;transforms;statistics;decorrelation;bit allocation;encoding;adaptive multistage image transform coding	Adaptive multistage image transform coding is discussed, and an optimal method is introduced for bit allocation. The optimality is in the sense of minimizing the mean square reconstruction error with a given total number of bits and a given number of stages. The statistics of the coefficients in different stages and marginal analysis are used to optimize the division of the total number of bits among the stages. Experimental results indicate that, with two stages, more than 14% improvement for one class and more than 11% improvement for multiclasses is achieved in mean square reconstruction error over one-stage image transform coding. Higher improvements are achieved with three stages. The reconstructed images with multistage coding are subjectively much more preferable than the reconstructed images with one-stage coding.	coefficient;marginal model;mean squared error;multistage amplifier;transform coding	Sabzali Aghagolzadeh;Okan K. Ersoy	1991	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.120770	iterative reconstruction;marginalism;mathematical optimization;transform coding;decorrelation;coefficient;theoretical computer science;mathematics;encoding;statistics	Vision	47.25256839528308	-13.006812284526315	18720
bf5bcf37564fb282233cfedd65669202cdee5d54	image protection via watermarking on perceptually significant wavelet coefficients	image protection;watermarking;quantization;perceptually significant wavelet coefficients;image dependent;image coding;digital watermark;frequency domain analysis;frequency domain analysis image coding cryptography wavelet transforms transform coding search problems;transform coding;wavelet transforms;search;protection;casting;digital watermark casting;subbands;energy weighting factors;discrete cosine transforms;cryptography;coefficients;image reconstruction;adaptive method;broadband watermark;broadband watermark image protection perceptually significant wavelet coefficients search digital watermark casting adaptive method subbands coefficients image dependent energy weighting factors;search problems;frequency domain;protection watermarking wavelet coefficients discrete cosine transforms casting image coding transform coding quantization frequency domain analysis image reconstruction;wavelet coefficients	A new scheme to search perceptually signifficant wavelet coefficients for effective digital watermark casting is proposed in this research. Unlike other watermark casting algorithms, which select a fixed set of DCT or wavelet coefficients in the frequency domain, we use an adaptive method to lind significant subbands and a number of coefficients in these subbands. The resulting method is image dependent. Furthermore, the threshold of the selected subband is used as one of t,he energy weighting factors in the generation of a broadband watermark so tha t i t cannot be easily damaged by frequency-selective filtering, DCT or wavelet based compression attack.	algorithm;coefficient;digital watermarking;discrete cosine transform;wavelet	Houng-Jyh Mike Wang;C.-C. Jay Kuo	1998		10.1109/MMSP.1998.738947	computer vision;speech recognition;digital watermarking;computer science;electrical engineering;mathematics;wavelet packet decomposition;stationary wavelet transform;frequency domain;statistics	Graphics	41.91945182803416	-10.722059824963468	18763
6a480350e281a35fa1ebc3ad548e93f800d52104	a device-independent efficient actigraphy signal-encoding system for applications in monitoring daily human activities and health	actigraphy;activity monitoring;data compression;denoising;edge computing;encoding;machine learning;signal processing;wearables	Actigraphs for personalized health and fitness monitoring is a trending niche market and fit aptly in the Internet of Medical Things (IoMT) paradigm. Conventionally, actigraphy is acquired and digitized using standard low pass filtering and quantization techniques. High sampling frequencies and quantization resolution of various actigraphs can lead to memory leakage and unwanted battery usage. Our systematic investigation on different types of actigraphy signals yields that lower levels of quantization are sufficient for acquiring and storing vital movement information while ensuring an increase in SNR, higher space savings, and in faster time. The objective of this study is to propose a low-level signal encoding method which could improve data acquisition and storage in actigraphs, as well as enhance signal clarity for pattern classification. To further verify this study, we have used a machine learning approach which suggests that signal encoding also improves pattern recognition accuracy. Our experiments indicate that signal encoding at the source results in an increase in SNR (signal-to-noise ratio) by at least 50⁻90%, coupled with a bit rate reduction by 50⁻80%, and an overall space savings in the range of 68⁻92%, depending on the type of actigraph and application used in our study. Consistent improvements by lowering the quantization factor also indicates that a 3-bit encoding of actigraphy data retains most prominent movement information, and also results in an increase of the pattern recognition accuracy by at least 10%.	actigraphy;clarity measurement;data acquisition;experiment;extravasation;high- and low-level;machine learning;memory leak;niche blogging;pattern recognition;personalization;programming paradigm;quantization (signal processing);recorders, physiologic, physical activity;sampling (signal processing);sampling - surgical action;signal-to-noise ratio;spectral leakage	Yashodhan Athavale;Sridhar Krishnan	2018		10.3390/s18092966	computer engineering;electronic engineering;encoding (memory);actigraphy;engineering	HCI	41.14567288130893	-5.414404900205514	18796
35db77d9af9389bb71608be86913772b205e2c48	comparison results for garch processes	settore secs s 06 metodi mat dell economia e d scienze attuariali e finanz	We consider the problem of stochastic comparison of general Garchlike processes, for different parameters and different distributions of the innovations. We identify several stochastic orders that are propagated from the innovations to the Garch process itself, and discuss their interpretations. We focus on the convex order and show that in the case of symmetric innovations it is also propagated to the cumulated sums of the Garch process. More generally, we discuss multivariate comparison results related to the multivariate convex and supermodular order. Finally we discuss ordering with respect to the parameters in the Garch (1,1) case.		Fabio Bellini;Franco Pellerey;Carlo Sgarra;Salimeh Yasaei Sekeh	2014	J. Applied Probability	10.1017/S0021900200011608	econometrics;engineering;performance art;cartography	AI	34.266788385526375	-19.361571140378754	18823
b40255df3f0876133d7787ba5c14615881911a72	tutorial cma-es: evolution strategies and covariance matrix adaptation	cma es;covariance matrix adaptation;evolution strategy	1 Problem Statement Black Box Optimization and Its Difficulties Non-Separable Problems Ill-Conditioned Problems 2 Evolution Strategies A Search Template The Normal Distribution Invariance 3 Step-Size Control Why Step-Size Control One-Fifth Success Rule Path Length Control (CSA) 4 Covariance Matrix Adaptation Covariance Matrix Rank-One Update Cumulation—the Evolution Path Covariance Matrix Rank-μ Update 5 CMA-ES Summary 6 Theoretical Foundations 7 Comparing Experiments 8 Summary and Final Remarks	black box;cma-es;evolution strategy;gnu octave;java;matlab;program optimization;python;scilab	Anne Auger;Nikolaus Hansen	2012		10.1145/2330784.2330919	mathematical optimization;cma-es;computer science;machine learning;statistics	ML	31.116890202266436	-1.3267741580292722	18836
9e44db013ff3a6744b483e319d0e272130fc5f10	application of an optimization procedure to steady-state simulation	simulation experiment;technical report;steady state	Nozari and Morris (13) have proposed an optimization procedure for simulation experiments. In this paper we discuss how that method can be applied to simulation experiments. An example is provided to demonstrate the effectiveness of the method.	experiment;mathematical optimization;simulation;steady state	Ardavan Nozari;John S. Morris	1984			dynamic simulation;simulation;computer science;technical report;management science;steady state;world wide web	Robotics	28.761474428078994	-15.592323935332077	18838
8d73d2a5206efba67aae5c2091b796dd4f4fbcd2	kriging of financial term-structures	no arbitrage constraints;model risk;ois discount curve;implied default distribution;interest rate curve;kriging;yield curve	Due to the lack of reliable market information, building financial term-structures may be associated with a significant degree of uncertainty. In this paper, we propose a new term-structure interpolation method that extends classical spline techniques by additionally allowing for quantification of uncertainty. The proposed method is based on a generalization of kriging models with linear equality constraints (market-fit conditions) and shape-preserving conditions such as monotonicity or positivity (no-arbitrage conditions). We define the most likely curve and show how to build confidence bands. The Gaussian process covariance hyper-parameters under the construction constraints are estimated using cross-validation techniques. Based on observed market quotes at different dates, we demonstrate the efficiency of the method by building curves together with confidence intervals for term-structures of OIS discount rates, of zero-coupon swaps rates and of CDS implied default probabilities. We also show how to construct interest-rate surfaces or default probability surfaces by considering time (quotation dates) as an additional dimension. JEL classification C63; E43; G12	algorithm;ambiguous name resolution;computer experiment;consortium;cross-validation (statistics);earliest deadline first scheduling;embedded system;gaussian process;gri;interpolation;kriging;linear equation;rejection sampling;sampling (signal processing);spline (mathematics);monotone	Areski Cousin;Hassan Maatouk;Didier Rullière	2016	European Journal of Operational Research	10.1016/j.ejor.2016.05.057	financial economics;econometrics;mathematical optimization;economics;mathematics;kriging;welfare economics;yield curve;statistics	ML	31.41250163841825	-21.33167058248433	18846
6d198374fd883c7991b89ecb390c8911ca7a3935	multiresolution vector transform coding for video compression	degraded channel;video signals block codes computational complexity correlation theory data compression image coding markov processes packet radio networks telecommunication channels transforms vectors;computational load;first order markov process;image coding;image resolution;data compression;correlation theory;video compression algorithm;packet video;video compression;packet radio networks;transform coding;bit rate;discrete cosine transform;vector transformations;block transformation;first order;vectors;discrete transforms;computational complexity;discrete cosine transforms;network congestion computational load image coding video compression algorithm compression gains first order markov process block transformation vector transformations discrete cosine transform hadamard robustness degraded channel packet video;compression gains;hadamard;transforms;markov process;robustness;video signals;markov processes;transform coding video compression discrete cosine transforms image resolution image coding block codes discrete transforms markov processes robustness bit rate;telecommunication channels;block codes;network congestion	A novel image and video compression algorithm based on block and vector transformations used in a multiresolution approach is described. The algorithm is shown to yield substantial compression gains over regular block coding with only a small increase in the computational load. In particular, the transform efficiency on a first-order Markov process is demonstrated to exceed that of a block transformation and approach the efficiency of the full transformation as the correlation factor approaches unity. Experiments with real video data also illustrate the compression gains for a variety of vector transformations, including vector DCT (discrete cosine transform) and vector Hadamard. Combined with an inherent robustness to vector loss, the proposed transformation is very well suited for applications requiring low bit rates over a degraded channel such as packet video during network congestion. >		Brian DeCleene;Henrik V. Sorensen	1993		10.1109/ICASSP.1993.319835	data compression;discrete mathematics;lapped transform;computer science;theoretical computer science;mathematics;markov process;motion compensation;vector quantization;statistics;sum of absolute transformed differences	Vision	49.27412016207267	-14.272703649548351	18862
5b1117b95c9feadc6d015ac1ad0975b39aa8b43f	an interactive spreadsheet-based tool to support teaching design of experiments	design of experiments;response surface methods;central composite design ccd;design of experiment	This paper describes an interactive spreadsheet-based tool that can be used to generate data representative of the type that might be obtained running a structured set of experiments. The purpose of this tool is to help the user experience the iterative nature of design and analysis of experiments. The tool supports quick and simple generation of data for one and two-factor problems. The underlying relationships are based on queuing approximations for a single-stage batch production environment. Factor levels are related to product lot sizes and the response is assumed to be average lot flowtimes. Variability due to replication is emulated by sampling from a statistical distribution. Statistical software packages can be used to generate linear or quadratic models from the results generated. Analysis can include the examination of main and interaction effects or the optimization of lot sizes to minimize flowtimes.	design of experiments;spreadsheet	S. T. Enns	2008	INFORMS Trans. Education	10.1287/ited.1080.0008	mathematical optimization;simulation;computer science;operations management;mathematics;design of experiments;statistics	HCI	28.99551805723387	-15.464816316442205	18881
feedcd1a5aa4ee3c5ed95d0279760345be92f96a	optimal nearly uniform scalar quantizer design for wavelet coding	image compression;wavelets;high resolution;quantization;distortion;jpeg2000	 Uniform scalar quantizers are widely used in image coding. They are known to be optimum entropy constrained scalar quantizers within the high resolution assumption. In this paper, we focus on the design of nearly uniform scalar quantizers for high performance coding of wavelet coefiqcients whatever the bitrate is. Some codecs use uniform scalar quantizers with a zero quantization bin size (deadzone) equal to two times the other quantization bin sizes (for example JPEG2000). We address the... 	quantization (signal processing);wavelet transform	Christophe Parisot;Marc Antonini;Michel Barlaud	2002			mathematical optimization;electronic engineering;theoretical computer science;mathematics	HCI	43.02758027605991	-14.994795230702069	18895
8c6f28c406c03c134de1dd536b03f0129e4b5e34	solving the 0-1 knapsack problem based on a parallel intelligent molecular computing model system		The 0-1 knapsack problem is one of the classical NP-hard problems and has been one of the hot research topics in the last few decades. It offers many practical applications in optimization and applied mathematics, such as project selection, resource distribution, investment decision-making and so on. Simultaneity in previous studies DNA molecular computation usually be used to solve NP-complete continuous path search problems (for example HPP, traveling salesman problem), rarely for NP problems with discrete solutions result, such as the 0-1 knapsack problem, graph coloring problem. In this paper, we present a new algorithm for solving the 0-1 knapsack problem with DNA molecular operations. For 0-1 knapsack problem with n distinct items, weight capacity C and total profits W, we reasonably design fixed length DNA strands representing items, take appropriate steps and get the solutions of the problem in proper length range using O(n + C + W) time. We extend the application of DNA molecular operations and simultaneity simplify the complexity of the computation.	algorithm;computation;computational complexity theory;dna computing;graph coloring;knapsack problem;mathematical optimization;np-completeness;np-hardness;natural computing;travelling salesman problem	Zuwen Ji;Zhaocai Wang;Tunhua Wu;Wei Huang	2017	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-169321	theoretical computer science;mathematics;continuous knapsack problem;knapsack problem;cutting stock problem;mathematical optimization	Theory	25.79138915082118	3.834757986423003	19013
f3b767dd4fa48f0e083e5833d6784330999987f8	bandwidth extension of g.729 speech coder using search-free codebook mapping	wideband narrowband speech speech coding decoding vectors;g 729;wideband;decoding;speech enhancement artificial bandwidth extension codebook mapping g 729;speech;speech coding;speech enhancement;artificial bandwidth extension;vectors;vocoders speech enhancement;vocoders;speech enhancement g 729 speech coder search free codebook mapping algorithm wideband line spectrum pair codebook speech quality perceptual evaluation spectral distortion informal listening tests weighted million operations per second calculations;narrowband;codebook mapping	A search-free codebook mapping algorithm for bandwidth extension in G.729 compressed domain is introduced in this paper. We design a wideband line spectrum pair (LSP) codebook which is coupled with the same index as the LSP codebook of the G.729 speech codec. The received narrowband LSP codebook indices are used to directly induce wideband LSP codewords. Thus, the proposed scheme eliminates codebook search processing to estimate the wideband spectrum envelope. Its performance was assessed via the perceptual evaluation of speech quality (PESQ), spectral distortion (SD), informal listening tests, and weighted million operations per second (WMOPS) calculations.	algorithm;bandwidth extension;code word;codebook;codec;distortion;flops;g.729;pesq;speech coding	Youngwoo Kwon;Yaxing Li;Sangwon Kang	2012	2012 35th International Conference on Telecommunications and Signal Processing (TSP)	10.1109/TSP.2012.6256331	voice activity detection;g.729;speech recognition;computer science;speech;speech coding;mathematics;linde–buzo–gray algorithm	Robotics	48.340245325691626	-8.951334354057174	19016
c43361d69e37517604c33205a403a037bb596c05	adaptive reversible data hiding via integer-to-integer subband transform and adaptive generalized difference expansion method	frequency domain	We propose an adaptive reversible data hiding method with superior visual quality and capacity in which an adaptive generalized difference expansion (AGDE) method is applied to an integer-to-integer subband transform (I2I-ST). I2I-ST performs the reversible subband transform and the AGDE method is a state-of-the-art method of reversible data hiding. The results of experiments we performed objectively and perceptually show that the proposed method has better visual quality than conventional methods at the same embedding rate due to low variance in the frequency domain. key words: adaptive reversible data hiding, adaptive generalized difference expansion, frequency domain, integer-to-integer subband transform	experiment;sub-band coding	Taichi Yoshida;Taizo Suzuki;Masaaki Ikehara	2014	IEICE Transactions		mathematical optimization;mathematical analysis;discrete mathematics;computer science;mathematics;frequency domain	Vision	43.00584925150915	-14.765279596118695	19021
e558642d85ab0a0cc9ef5bfe57a1afd4775752b4	adaptive image encryption for high capacity reversible data hiding		In this paper, an adaptive image encryption method which achieves high capacity reversible data hiding is proposed. In the proposed encryption method, Burrow's-wheeler transform was applied before encrypting the given image. The resultant encrypted images are comparatively smoother and have higher spatial correlation, which allow high embedding rates. To preserve good visual quality for the approximate image obtained from direct decryption without watermark extraction, data hiding process are carried out block by block within the encrypted image and sorted-variance block embedding is adopted in watermark embedding procedure. In the experimental results, the reversibility of the proposed method is verified, and in comparison with state-of-the-art methods, a higher maximum embedding rate is obtained in the proposed method.	approximation algorithm;burrows–wheeler transform;encryption;exclusive or;pixel;resultant	Ka-Cheng Choi;Chi-Man Pun	2017	2017 IEEE Conference on Dependable and Secure Computing	10.1109/DESEC.2017.8073862	spatial correlation;information hiding;watermark;theoretical computer science;encryption;embedding;mathematics	Vision	39.90809518371226	-10.901617116394043	19073
8e9374febed8b59388cc287a041c11dfa24cb71f	secure watermark detection with nonparametric decision boundaries	detectors;watermarking;watermarking detectors companies;companies	In this paper we will address the problem of constructing a nonparameteric decision boundary for watermark detection. Most current watermarking algorithms have a parametric decision boundary that can be undone if the pirate has unlimited access to the detector. In this work we propose a fractal decision boundary which can not be estimated. That boundary is obtained by processing the decision boundary corresponding to the underlying watermarking algorithm. The performance of the new technique is essentially similar to any watermarking algorithm from which it is derived.	algorithm;decision boundary;digital watermarking;fractal	Ahmed H. Tewfik;Mohamed F. Mansour	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5745046	computer vision;detector;digital watermarking;computer science;theoretical computer science;mathematics;computer security	Robotics	41.03309482058527	-7.504202802953554	19077
98ef244ba1ede5722392fbf3a72be641f10952ef	new energetic selection principle in differential evolution	evolutionary algorithms;heuristics;selection.;optimization;differential evolution;evolutionary algorithm;population size;barrier function	The Differential Evolution (DE) algorithm goes back to the class of Evolutionary Algorithms and inherits its philosophy and concept. Possessing only three control parameters (size of population, differentiation and recombination constants) DE has promising characteristics of robustness and convergence. In this paper we introduce a new principle of Energetic Selection. It consists in both decreasing the population size and the computation efforts according to an energetic barrier function which depends on the number of generation. The value of this function acts as an energetic filter, through which can pass only individuals with lower fitness. Furthermore, this approach allows us to initialize a population of a sufficient (large) size. This method leads us to an improvement of algorithm convergence.	barrier function;computation;differential evolution;fitness function;population;vergence	Vitaliy Feoktistov	2004			differential evolution;mathematical optimization;barrier function;population size;computer science;evolutionary algorithm	ML	26.428027953696024	-6.432819845033047	19119
3aab5233e2a6baab0f3d18fc1043be692fc2bbb2	quatre algorithm with sort strategy for global optimization in comparison with de and pso variants	differential evolution;particle swarm optimization;quatre;real parameter optimization;swarm intelligence	Optimization algorithm in swarm intelligence is getting more and more prevalent both in theoretical field and in real-world applications. Many nature-inspired algorithms in this domain have been proposed and employed in different applications. In this paper, a new QUATRE algorithm with sort strategy is proposed for global optimization. QUATRE algorithm is a simple but powerful stochastic optimization algorithm proposed in 2016 and it tackles the representational/positional bias existing in DE structure. Here a sort strategy is used for the enhancement of the canonical QUATRE algorithm. This advancement is verified on CEC2013 test suite for real-parameter optimization and also is contrasted with several state-of-the-art algorithms including Particle Swarm Optimization (PSO) variants, Differential Evolution (DE) variants on COCO framework under BBOB2009 benchmarks. Experiment results show that the proposed QUATRE algorithm with sort strategy is competitive with the contrasted algorithms. © 2018, Springer International Publishing AG.	algorithm;global optimization;particle swarm optimization	Jeng-Shyang Pan;Zhenyu Meng;Shu-Chuan Chu;John F. Roddick	2017		10.1007/978-3-319-68527-4_34	swarm intelligence;machine learning;computer science;global optimization;stochastic optimization;differential evolution;artificial intelligence;coco;algorithm;sort;test suite;mathematical optimization;particle swarm optimization	Vision	25.148705392678565	-4.162490341112105	19170
e3c667fed226a9ea10211e870f188347657735a8	parallel numerical algorithms (t. l. freeman and c. phillips)	numerical algorithm	listed at the end of each chapter, for theoretical aspects. It covers virtually all well-known applications of the bootstrap method and much more. Most ofthe numerous examples given to illustrate the methodologies use real data sets. The topics covered range from the standard applications such as estimation of standard errors, regression model, and confidence intervals to such topics as approximate likelihoods and bioequivalence. Other topics include permutations tests, cross validation estimates of prediction error, and comparison with bootstrap estimates, adaptive estimates, jackknife approximations and estimates of bias using jackknife, geometric representation ofbootstrap andjackknife, and efficient bootstrap computations. The book is very nicely written and organized, and is enjoyable to read. There are twenty-six chapters. Each of the first twenty-five chapters ends with bibliographic notes and a set of problems. The last chapter has a discussion. Finally, there is an appendix containing a description of the computer program used in the book. This book certainly is a very valuable addition to the literature on bootstrap, and the authors ought to be commended for the excellent work they have done. The book can be easily adapted either for a one semester or two semester courses, as suggested by the authors.	approximation algorithm;booting;bootstrapping (statistics);computation;computer program;cross-validation (statistics);jackknife resampling;numerical analysis	Henk A. van der Vorst	1994	SIAM Review	10.1137/1036172	mathematics;algorithm	ML	28.44196888147341	-21.945271441097557	19218
dff208f25137a44ecca2fef077bdb61521bab4ae	an elitist learning particle swarm optimization with scaling mutation and ring topology		Particle swarm optimization (PSO) is an evolutionary algorithm that is well known for its simplicity and effectiveness. It usually has strong global search capability but has the drawback of being easily trapped by local optima. A scaling mutation strategy and an elitist learning strategy are presented in this paper. Based on these strategies, an improved PSO variant (LSERPSO) is developed through a local search and ring topology strategy. The new scaling mutation strategy involved an exploration and exploitation balance focusing on mutation operation. A collection of elite individuals is maintained such that an array of current particles can learn from them. A ring topology-based neighborhood structure is adopted to maintain the population diversity and to reduce the possibility of particles being trapped in local optima. Finally, a quasi-Newton-based local search is incorporated to enhance the fine-grained capability. The effects of these proposed strategies and their cooperation are verified step by step. The performance of LSERPSO is comprehensively studied using IEEE CEC2015 benchmark functions.		Guangzhi Xu;Xinchao Zhao;Tong Wu;Rui Li;Xingmei Li	2018	IEEE Access	10.1109/ACCESS.2018.2885036	mathematical optimization;evolutionary algorithm;local optimum;scaling;local search (optimization);distributed computing;computer science;convergence (routing);population;particle swarm optimization;ring network	Visualization	27.0177032135257	-3.9914595441545737	19243
e6eff6622a969b1652745d96d4cd03fdd9a47526	architecture based on fpga's for real-time image processing	field programmable gate array;parametre ordre;high resolution;detecteur image;image processing;convolution;procesamiento imagen;real time processing;convolucion;tecnologia mos complementario;red puerta programable;reseau porte programmable;traitement image;captador medida;tratamiento tiempo real;parametro orden;haute resolution;traitement temps reel;measurement sensor;capteur mesure;alta resolucion;order parameter;detector imagen;0707d;technologie mos complementaire;real time image processing;image sensor;complementary mos technology	In this paper an architecture based on FPGA’s for real time image processing is described. The system is composed of a high resolution (1280×1024) CMOS sensor connected to a FPGA that will be in charge of acquiring images from the sensor and controlling it too. A PC sends certain orders and parameters, configured by the user, to the FPGA. The connexion between the PC and the FPGA is made through the parallel port. On the other hand, the resolution of the captured image, as well as the selection of a window of interest inside the image, are configured by the user in the PC. Finally, a system to make the convolution between the captured image and a nxn-mask is shown.	cmos;computer display standard;convolution;field-programmable gate array;image processing;image resolution;parallel port;personal computer;real-time transcription	Ignacio Bravo Muñoz;Pedro Jiménez;Manuel Mazo;José Luis Lázaro;Ernesto Martín Gorostiza	2006		10.1007/11802839_21	embedded system;image resolution;image processing;computer science;image sensor;convolution;field-programmable gate array;computer graphics (images)	Robotics	41.73868632523787	-3.951130904923513	19341
99a5dd3eb03639dd0f24f67e55ab2ec48d96a832	regional sensitivity analysis using revised mean and variance ratio functions	variance ratio function;mean ratio function;variance reduction;regional sensitivity analysis	The variance ratio function, derived from the contribution to sample variance (CSV) plot, is a regional sensitivity index for studying how much the output deviates from the original mean of model output when the distribution range of one input is reduced and to measure the contribution of different distribution ranges of each input to the variance of model output. In this paper, the revised mean and variance ratio functions are developed for quantifying the actual change of the model output mean and variance, respectively, when one reduces the range of one input. The connection between the revised variance ratio function and the original one is derived and discussed. It is shown that compared with the classical variance ratio function, the revised one is more suitable to the evaluation of model output variance due to reduced ranges of model inputs. A Monte Carlo procedure, which needs only a set of samples for implementing it, is developed for efficiently computing the revised mean and variance ratio functions. The revised mean and variance ratio functions are compared with the classical ones by using the Ishigami function. At last, they are applied to a planar 10-bar structure. & 2013 Elsevier Ltd. All rights reserved.	monte carlo method;sensitivity index	Pengfei Wei;Zhenzhou Lu;Wenbin Ruan;Jingwen Song	2014	Rel. Eng. & Sys. Safety	10.1016/j.ress.2013.08.001	multivariate analysis of variance;econometrics;mathematical optimization;direct material usage variance;realized variance;direct material price variance;allan variance;variance-based sensitivity analysis;mathematics;variance function;variance;law of total variance;algebraic formula for the variance;variance decomposition of forecast errors;one-way analysis of variance;variance inflation factor;statistics;variance reduction;control variates	AI	27.354990613404315	-19.818751283912164	19352
67ceb5304492b7d8af42e9e71c9b80348fb700f0	analysis of correlated binary data under partially linear single-index logistic models	62 07;health research;uk clinical guidelines;classification automatique statistiques;association statistique;biological patents;parametro dano;metodo estadistico;analyse multivariable;62g10 secondary;biomedical journals;multivariate analysis;analisis datos;text mining;europe pubmed central;undersmooth;62h20;primary;metodo semiparametrico;modele lineaire;citation search;62g08;estimacion promedio;logistic model;statistical association;semiparametric estimation;parametre nuisance;statistical method;distribucion logistica;methode semiparametrique;modelo lineal;citation networks;correlated binary data;estimating equation;data analysis;asociacion estadistica;nuisance parameter;statistical analysis;research articles;62g10;marginal distribution;uk phd theses thesis;secondary 62g20;methode statistique;abstracts;missing at random;secondary;open access;indexation;modele logistique;semiparametric regression;linear model;semiparametric method;analyse correlation;donnee binaire;life sciences;distribution logistique;clinical guidelines;marginal mean;modelo logistico;ley marginal;analisis multivariable;analyse donnee;primary 62g08;62g20 association binary outcomes clustered data estimating equation marginal mean semiparametric estimation undersmooth;association;dato binario;binary data;mean estimation;cluster analysis statistics;full text;estimation statistique;methodology;estimation moyenne;uk research reports;estimacion estadistica;medical journals;statistical estimation;clustered data;logistic distribution;rest apis;orcids;binary outcomes;loi marginale;europe pmc;analisis correlacion;biomedical research;correlation analysis;bioinformatics;literature search	Clustered data arise commonly in practice and it is often of interest to estimate the mean response parameters as well as the association parameters. However, most research has been directed to address the mean response parameters with the association parameters relegated to a nuisance role. There is relatively little work concerning both the marginal and association structures, especially in the semiparametric framework. In this paper, our interest centers on inference on both the marginal and association parameters. We develop a semiparametric method for clustered binary data and establish the theoretical results. The proposed methodology is investigated through various numerical studies.		Grace Y. Yi;Wenqing He;Hua Liang	2009	Journal of multivariate analysis	10.1016/j.jmva.2008.04.012	association;econometrics;text mining;data mining;mathematics;statistics	ML	33.26804259226003	-22.839625398640383	19386
841a4a439515af81cfefafc5283dd23ee9e13fff	sensitivity estimates for portfolio credit derivatives using monte carlo	likelihood ratio;65c05;credit spread;efficiency;conditional expectation;pathwise method;sensitivity calculation;monte carlo;91b28;monte carlo simulation;credit derivatives;credit derivative	Portfolio credit derivatives are contracts that are tied to an underlying portfolio of defaultable reference assets and have payoffs that depend on the default times of these assets. The hedging of credit derivatives involves the calculation of the sensitivity of the contract value with respect to changes in the credit spreads of the underlying assets, or, more generally, with respect to parameters of the default-time distributions. We derive and analyze Monte Carlo estimators of these sensitivities. The payoff of a credit derivative is often discontinuous in the underlying default times, and this complicates the accurate estimation of sensitivities. Discontinuities introduced by changes in one default time can be smoothed by taking conditional expectations given all other default times. We use this to derive estimators and to give conditions under which they are unbiased. We also give conditions under which an alternative likelihood ratio method estimator is unbiased. We illustrate the application and verification of these conditions and estimators in the particular case of the multifactor Gaussian copula model, but the methods are more generally applicable.	monte carlo method;smoothing	Zhiyong Chen;Paul Glasserman	2008	Finance and Stochastics	10.1007/s00780-008-0071-y	financial economics;econometrics;credit derivative;actuarial science;economics;finance;mathematics;statistics;monte carlo method	AI	31.581115557873208	-19.183031096399805	19387
a74078d9e2fc6874ce11b7efbfcdd42741e3a1cc	quality-aware video	watermarking;video sequence;data hiding;quality assessment feature extraction degradation data encapsulation video compression data mining video sequences watermarking telecommunication network reliability decoding;image distortion;network visualization;natural video statistics video quality assessment quality aware video reduced reference data hiding;prior knowledge;3d discrete cosine transform;indexing terms;statistical model;discrete cosine transform;data encapsulation;video quality assessment system;video coding;network visual communication;natural video statistics;statistical analysis;discrete cosine transforms;feature extraction;watermarking data encapsulation discrete cosine transforms feature extraction image sequences security of data statistical analysis video coding;image distortion network visual communication video quality assessment system feature extraction video sequence video coding statistical model 3d discrete cosine transform data hiding;security of data;reduced reference;quality aware video;video quality assessment;image sequences	Development in network visual communications has emphasized on the need of objective, reliable and easy-to-use video quality assessment (VQA) systems. This paper introduces a novel idea of quality-aware video (QAV), in which extracted features about the original video sequence are invisibly embedded into the same video data. When such a QAV sequence is distributed over an error-prone network, a network user who receives it can decode the hidden messages and use them to evaluate the quality degradations between the original and the received video sequences. Our first implementation of QAV employs (1) a novel reduced-reference VQA method based on a statistical model of natural video, and (2) a 3D discrete cosine transform-based data hiding algorithm. The proposed approach does not assume any prior knowledge about image distortions, and the simulation results demonstrate its potentials to be generalized for different types and degrees of image distortions.	algorithm;cognitive dimensions of notations;discrete cosine transform;distortion;embedded system;simulation;statistical model	Basavaraj Hiremath;Qiang Li;Zhou Wang	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379348	video compression picture types;statistical model;computer vision;index term;feature extraction;digital watermarking;computer science;video quality;theoretical computer science;discrete cosine transform;video tracking;multimedia;graph drawing;information hiding;statistics	Vision	45.11136636440391	-21.78681583457951	19389
d1cc39b3dbf85920d7ccb4c133483c40414dbc09	deviation inequalities for an estimator of the conditional value-at-risk	administracion financiera;financial management;variable aleatoire;gestion risque;conditional value at risk;risk management;variable aleatoria;grande deviation;probabilistic approach;operations research;estimator;deviation inequality;gran desviacion;mathematical programming;recherche operationnelle;enfoque probabilista;approche probabiliste;random variable;gestion riesgo;gestion financiere;programmation mathematique;large deviation;programacion matematica;investigacion operacional	In this paper, we present a deviation inequality for a common estimator of the conditional value-at-risk for bounded random variables. The result improves a deviation inequality which is obtained by Brown [D.B. Brown, Large deviations bounds for estimating conditional value-at-risk, Operations Research Letters 35 (2007) 722-730].	expected shortfall;value at risk	Ying Wang;Fuqing Gao	2010	Oper. Res. Lett.	10.1016/j.orl.2009.11.008	conditional probability distribution;random variable;econometrics;estimator;root-mean-square deviation;risk management;expected shortfall;conditional variance;calculus;mathematics;bias of an estimator;statistics	Logic	33.53809394309167	-20.396132940317575	19393
86a47ad3d3d0b7e5c705e333dd21a9d796a958c4	a mixture of mixture models for a classification problem: the unity measure error	statistical simulation;computacion informatica;measurement error;analisis datos;melange loi probabilite;systematic error;mixed distribution;ecuesta estadistica;erreur classification;classification;editing;sample survey;data analysis;simulacion estadistica;estimation erreur;mixture model;62h30;error estimation;ciencias basicas y experimentales;simulation statistique;matematicas;statistical computation;estimacion error;calculo estadistico;classification error;settore secs s 01 statistica;mezcla ley probabilidad;simulation study;analyse donnee;calcul statistique;mixture of gaussians;grupo a;mixture models;em algorithm;clasificacion;sondage statistique	A mixture of Gaussian mixture models is proposed to deal with the identification of survey respondents providing values in a wrong unity measure. The “two-level” mixture model allows effective classification in a non-normal setting. The natural constraints of the problem make the model identifiable. The effectiveness of the proposal is shown by simulation studies and an application to the 1997 Italian Labour Cost Survey. © 2006 Elsevier B.V. All rights reserved.	mixture model;simulation;software studies	Marco Di Zio;Ugo Guarnera;Roberto Rocci	2007	Computational Statistics & Data Analysis	10.1016/j.csda.2006.01.001	econometrics;speech recognition;mixture model;mathematics;statistics	AI	31.948614415065123	-23.545248407187447	19413
7d694e48236edbd3f4f0ef8d5710d0b79b418a45	a study on the impact of compression and packet losses on rendered 3d views	receivers;computer science and informatics;video	In 3D video delivery, the rendered 3D video quality at the receiver-side can be affected by rendering artifacts as well as by concealment errors which occur in the process of recovering missing 3D video packets. Therefore it is vital to have an understanding of the artifacts prior to transmitting data. This work proposes a model to quantify rendering and concealment errors at the sender-side and to use the information generated through the model to effectively deliver 3D video content.	digital video;network packet;transmitter	Chaminda T. E. R. Hewage;Maria G. Martini;Harsha D. Appuhami	2012		10.1117/12.910561	video compression picture types;scalable video coding;computer vision;video;uncompressed video;computer science;video quality;video capture;video tracking;block-matching algorithm;multimedia;video processing;smacker video;motion compensation;video post-processing;multiview video coding;computer graphics (images)	Visualization	49.0106927183591	-15.698693508631466	19420
0ee393d0eeeaa5e9edd7d5f3b8abc5a4f5c67667	using uncertainty as a model selection and comparison criterion	bayesian prediction;model selection;uncertainty;standard deviation;model evaluation;accuracy;bayesian prediction intervals;settore ing inf 05 sistemi di elaborazione delle informazioni;cost estimation;prediction interval;calibration;cost model	Over the last 25+ years, software estimation research has been searching for the best model for estimating variables of interest (e.g., cost, defects, and fault proneness). This research effort has not lead to a common agreement. One problem is that, they have been using accuracy as the basis for selection and comparison. But accuracy is not invariant; it depends on the test sample, the error measure, and the chosen error statistics (e.g., MMRE, PRED, Mean and Standard Deviation of error samples). Ideally, we would like an invariant criterion. In this paper, we show that uncertainty can be used as an invariant criterion to figure out which estimation model should be preferred over others. The majority of this work is empirically based, applying Bayesian prediction intervals to some COCOMO model variations with respect to a publicly available cost estimation data set coming from the PROMISE repository.	cocomo;cost estimation in software engineering;mathematical model;model selection	Salvatore Alessandro Sarcià;Victor R. Basili;Giovanni Cantone	2009		10.1145/1540438.1540464	calibration;uncertainty;prediction interval;data mining;accuracy and precision;standard deviation;model selection;cost estimate	ML	27.06373938100212	-19.28543769595574	19446
32b05e90d1d7a24212b9a7bf801ad6ffaa500673	joint exploitation of residual source information and mac layer crc redundancy for robust video decoding	decodage sequentiel;controle acces;channel coding;protocols;desciframiento secuencial;code treillis;variable length codes;communication system;image coding;complexity theory;measurement;data compression;decoding;signal module;video signal processing;telecommunication sans fil;redundancia;information source;communication systems;source information;implementation;codigo longitud variable;articulation;sequential decoding;senal modulada;variable length code;metric;codigo treillis;estimation a posteriori;metodo secuencial;trellis code;video coding access protocols awgn channels channel coding phase shift keying variable length codes;sequential method;phase shift keying;a posteriori estimation;articulacion;codes communication systems map estimation video coding sequential decoding;algorithme;algorithm;codage image;video coding;desciframiento treillis;awgn channels;compression image;redundancy;senal video;signal video;image compression;codage video;estimacion a posteriori;code longueur variable;streaming media;codes;telecomunicacion sin hilo;exact computation;channel coding joint exploitation residual source information mac layer crc redundancy robust video decoding map estimation method robust decoding video streams bitstream structure variable length codes mac layer sequential decoding algorithm h 264 avc standard wifi like packet structure bpsk modulated signals awgn channels;image sequence;binary phase shift keying;access protocols;map estimation;traitement signal video;cyclic redundancy check robustness decoding video compression streaming media automatic voltage control binary phase shift keying modulation coding awgn channels channel coding;methode sequentielle;video signal;metrico;robustness;secuencia imagen;access control;wireless lan;modulated signal;compresion dato;joint;trellis decoding;implementacion;reseau local sans fil;metrique;decodage treillis;mac layer;sequence image;compression donnee;compressed video;redondance;fuente informacion	This paper presents a MAP estimation method allowing the robust decoding of compressed video streams by exploiting the bitstream structure (Le., information about the source, related to variable-length codes and source characteristics) together with the knowledge of the MAC layer CRC (here considered as additional redundancy on the MAC packet). This method is implemented via a sequential decoding algorithm in which the branch selection metric in the decoding trellis incorporates a CRC-dependent factor, and the paths which are not compatible with the source constraints are pruned. A first implementation of the proposed algorithm performs exact computations of the metrics, and is thus computationally expensive. Therefore, we also introduce a suboptimal (with tunable complexity) version of the proposed metric computation. This technique is then applied to the robust decoding of sequences encoded using the H.264/AVC standard based on CAVLC and transmitted using a WiFi-like packet structure. Significant link budget improvement results are demonstrated for BPSK modulated signals sent over AWGN channels, even in the presence of channel coding.		Cédric Marin;Khaled Bouchireb;Michel Kieffer;Pierre Duhamel	2010	IEEE Transactions on Wireless Communications	10.1109/TWC.2010.07.090238	list decoding;telecommunications;sequential decoding;computer science;phase-shift keying;theoretical computer science;communications system;statistics	Vision	49.000192808367274	-15.189563917162046	19477
b4463050ab5b8d4266279d8d6716e775d2e2bb8d	a spiking neural network chip for odor data classification	power 3 6 muw artificial nose e nose system power consumption power efficient odor data classification chip neuromorphic spiking neural network chip electronic nose system integrate and fire neurons spike timing dependent plasticity cmos process gas data voltage 1 2 v size 0 18 mum;neurons biological neural networks threshold voltage power demand electronic noses firing training;neural chips electronic noses;neural chips;electronic noses	An artificial nose, also known as an “electronic nose” (E-Nose), has found many applications. One of the restrictions for E-Nose becoming popular is its size and power consumption. To reduce the power consumption and physical size of an E-Nose system, a power-efficient odor data classification chip is advantageous. This paper presents a low-power, neuromorphic spiking neural network chip which can be integrated in an electronic nose system to perform odor data classification. The network is composed of integrate-and-fire neurons, using spike-timing dependent plasticity for learning. The network has been fabricated by TSMC 0.18 μm CMOS process. The chip area is 1.033×1.383 mm2. Measurement results show that the chip can correctly classify real world gas data (hami and lemon) sampled by the commercial E-Nose, Cyranose 320. The supply voltage is 1.2 V; the power consumption is 3.6 μW. This learning chip features small area, low voltage and low power, and is very suitable for being integrated in an E-Nose system. The power and size of the E-Nose can be reduced and have more extensive applications.	biological neuron model;cmos;low-power broadcasting;neuromorphic engineering;power supply;spiking neural network;very-large-scale integration	Hung-Yi Hsieh;Kea-Tiong Tang	2012	2012 IEEE Asia Pacific Conference on Circuits and Systems	10.1109/APCCAS.2012.6418978	embedded system;electronic engineering;engineering;electrical engineering	EDA	38.75423635253289	-1.8187830940527834	19540
d4c0e43dc084ef8a7af09e1e5fc2449ad0f56104	clustering-based fast intra prediction mode algorithm for hevc	prediction algorithms;video coding;coding efficiency clustering based fast intraprediction mode algorithm ipm cluster center high efficiency video coding hevc next generation video coding standard equivalent perceptual quality h 264 avc intraprediction mode decision algorithm termination method k medoid clustering algorithm rate distortion optimization rdo process high resolution test video sequences hm16 0 encoding time;visual perception optimisation prediction theory statistical analysis video coding;early termination hevc intra prediction mode decision;clustering algorithms;encoding prediction algorithms clustering algorithms signal processing algorithms video coding europe;europe;signal processing algorithms;encoding	The High Efficiency Video Coding (HEVC) is the next generation video coding standard. The HEVC provides equivalent perceptual quality with bit-budget saving greater than 50% compared to the H.264/AVC. In this paper, we propose a new fast intra prediction mode decision algorithm for the HEVC. We apply an early termination method based on the statistics of the resulted IPMs from both rough mode decision and most probable mode stages. The resulted IPMs are clustered into a K cluster by means of the K-medoid clustering algorithm, and each IPM cluster center represents all the IPMs within a cluster for the RDO process. The sugeested algorithm has been evaluated on high resolution test video sequences. Compared with the current HM16.0 and state-of-the-art scheme in all intra high efficiency configuration cases, the proposed algorithm outperforms the state-of-the-art scheme in terms of encoding time with similar coding efficiency.	algorithm;algorithmic efficiency;cluster analysis;data compression;h.264/mpeg-4 avc;high efficiency video coding;image resolution;intra-frame coding;k-means clustering;k-medoids;medoid;next-generation network;raster document object;video coding format	Sami Jaballah;Kais Rouis;Jamel Belhadj Tahar	2015	2015 23rd European Signal Processing Conference (EUSIPCO)	10.1109/EUSIPCO.2015.7362704	computer vision;real-time computing;computer science;theoretical computer science;coding tree unit;context-adaptive binary arithmetic coding;h.261	AI	46.82972666184146	-19.259448240103467	19594
29087505307ad0f3919757277161bba3926c266f	a permutation-based dual genetic algorithm for dynamic optimization problems	dynamic combinatorial optimization;benchmark problem;combinatorial optimization problem;group theory;permutation;dynamic environment;genetic algorithm;combinatorial optimization;attribute based dualism;dynamic optimization problem;partial dualism scheme	Adaptation to dynamic optimization problems is currently receiving growing interest as one of the most important applications of genetic algorithms. Inspired by dualism and dominance in nature, genetic algorithms with the dualism mechanism have been applied for several dynamic problems with binary encoding. This paper investigates the idea of dualism for combinatorial optimization problems in dynamic environments, which are also extensively implemented in the real-world. A new variation of the GA, called the permutation-based dual genetic algorithm (PBDGA), is presented. Within this GA, two schemes based on the characters of the permutation in group theory are introduced: a partial-dualism scheme motivated by a new multi-attribute dualism mechanism and a learning scheme. Based on the dynamic test environments constructed by stationary benchmark problems, experiments are carried out to validate the proposed PBDGA. The experimental results show the efficiency of PBDGA in dynamic environments.	benchmark (computing);binary file;combinatorial optimization;dynamic programming;experiment;genetic algorithm;mathematical optimization;property dualism;software release life cycle;stationary process	Lili Liu;Dingwei Wang;Andrew W. H. Ip	2009	Soft Comput.	10.1007/s00500-008-0345-5	optimization problem;mathematical optimization;combinatorics;genetic algorithm;combinatorial optimization;mathematics;permutation;group theory;algorithm	AI	24.844636472764343	-2.8474628781420614	19601
6fd2cd6c772eafc81bc57ea96e5113806aa8ba8e	efficient sub-optimal temporal decomposition with dynamic weighting of speech signals for coding applications.		The Optimized Temporal Decomposition (OTD) technique for Line Spectral Frequencies (LSF) speech envelope representation, under a MMSE criterion, has been shown to be promising for very low bit rate speech coding for storage and broadcast applications. In order to improve perceptual speech quality, a dynamically weighted OTD (DW-OTD) technique is introduced in this work. It extends the OTD by allowing temporally changing weights, so as to improve the perceived speech quality. Use of Gardner's weighted MSE with DWOTD is found to reduce the Log Spectral Distance (LSD) measure by 0.3 dB, as compared to OTD. The original OTD algorithm delay and complexity requirements make it inappropriate for real-time speech coding. In this paper we also introduce a modification of this technique, which is suboptimal but suitable for on-line speech coding purposes, with negligible degradation of performance (of only about 0.06 dB in LSD). With the proposed techniques we were able to encode speech spectral envelopes at 300-370 bps at LSD of 2.25-2.1 dB, respectively, with a delay of just 7 frames.	algorithm;dreamwidth;encode;elegant degradation;lsf;line spectral pairs;online and offline;real-time transcription;requirement;speech coding	David Malah;Slava Shechtman	2004			speech recognition;speech coding;pattern recognition;speech processing	ML	48.72405068964301	-9.085768394273185	19632
c6040986a9770835bd89ec87721890eb8085cc29	practical, real-time, and robust watermarking on the spatial domain for high-definition video contents	filigranage numerique;protection information;digital watermarking;gestion de derechos de autor digitales;evaluation performance;tecnologia electronica telecomunicaciones;low bit rate;protection copie;high resolution;multimedia;performance evaluation;robust watermarking;drm system;image processing;video signal processing;display equipment;reduccion de colores;real time;evaluacion prestacion;factor conversion;real time video watermarking;copy protection;authentication;digital transmission;procesamiento imagen;real time processing;video processing;format conversion;satisfiability;traitement image;velocidad de bit debil;digital rights management;authentification;copyright protection;tratamiento tiempo real;codificacion;haute resolution;traitement temps reel;practical video watermarking;autenticacion;proteccion informacion;gestion des droits numeriques;portable equipment;human visual system;information protection;robustesse;conversion de formato;filigrana digital;multimedia communication;conversion rate;coding;sistema drm;alta resolucion;dithering;digital audio broadcasting;traitement signal video;equipement affichage;transmision numerica;radiodiffusion sonore numerique;robustness;tramage;systeme drm;tecnologias;equipo visualizacion;transmission numerique;conversion de format;grupo a;communication multimedia;high definition videos;video watermarking;article;high definition;debit binaire faible;images;digital right management;appareil portatif;aparato portatil;taux conversion;codage;robust video watermarking;robustez	Commercial markets employ digital right management (DRM) systems to protect valuable high-definition (HD) quality videos. DRM system uses watermarking to provide copyright protection and ownership authentication of multimedia contents. We propose a real-time video watermarking scheme for HD video in the uncompressed domain. Especially, our approach is in aspect of practical perspectives to satisfy perceptual quality, real-time processing, and robustness requirements. We simplify and optimize human visual system mask for real-time performance and also apply dithering technique for invisibility. Extensive experiments are performed to prove that the proposed scheme satisfies the invisibility, real-time processing, and robustness requirements against video processing attacks. We concentrate upon video processing attacks that commonly occur in HD quality videos to display on portable devices. These attacks include not only scaling and low bit-rate encoding, but also malicious attacks such as format conversion and frame rate change. key words: real-time video watermarking, robust video watermarking, practical video watermarking, high-definition videos	authentication;digital rights management;digital video;digital watermarking;dither;experiment;hdmi;image scaling;neural coding;personal digital assistant;real-time clock;real-time operating system;real-time transcription;requirement;video processing	Kyung-Su Kim;Hae-Yeoun Lee;Dong-Hyuck Im;Heung-Kyu Lee	2008	IEICE Transactions	10.1093/ietisy/e91-d.5.1359	computer vision;telecommunications;image processing;computer science;authentication;computer security;computer graphics (images)	Embedded	43.49319968659827	-10.776814826793153	19634
a1ac30244321f1c8ac60bf56b94b71ade8507591	recursive block coding - a new approach to transform coding	methode recursive;rate distortion;transform coding autoregressive processes block coding;karhunen loeve transformation;information compression;image processing;transform coding karhunen loeve transforms block codes data compression image coding signal processing image processing bit rate markov processes rate distortion;procesamiento de imagen;recursive method;image;transform coding;traitement image;higher order;codificacion;first order;boundary effect;autoregressive processes;imagen;processus markov;compression information;coding;markov process;code bloc;code;transformation karhunen loeve;cosine transform;block code;block codes;codigo;hardware implementation;codage;random field;block coding	The concept of fast KL transform coding introduced earlier [7], [8] for first-order Markov processes and certain random fields has been extended to higher order autoregressive (AR) sequences and practical images yielding what we call recursive block coding (RBC) algorithms. In general, the rate-distortion performance for these algorithms is significantly superior to that of the conventional block KL transform algorithm. Moreover, these algorithms permit the use of small size transforms, thereby removing the need for fast transforms and making the hardware implementation of such coders more appealing. This improved performance has been verified for practical image data and results in suppression of the block-boundary effect commonly observed in traditional transform coding techniques. This is illustrated by comparing RBC with cosine transform coding using both one- and twodimensional algorithms. Examples of RBC encoded images at various rates are given.	algorithm;autoregressive model;data compression;discrete cosine transform;distortion;fast fourier transform;first-order predicate;markov property;principal component analysis;recursion (computer science);role-based collaboration;transform coding;zero suppression	Paul M. Farrelle;Anil K. Jain	1986	IEEE Transactions on Communications	10.1109/TCOM.1986.1096509	block code;discrete mathematics;transform coding;speech recognition;lapped transform;image processing;theoretical computer science;mathematics;statistics	Vision	45.84240442994994	-14.622027266019392	19654
e547194ddcef5cc51b9c4a9417956f524ef2af54	bi-directional channel modeling for implantable uhf-rfid transceivers in brain-computer interface applications		Abstract Bi-directional Brain–Computer Interfacing (BBCI) faces major challenges due to, in part, the difficulty of transmission of Electro-corticographical (ECoG) signals from the brain to external devices. For human subjects, safety and convenience would be greatly increased if we could replace the wired interface between the electrodes and the extra-cutaneous receiver with a wireless interface. All the technology that we have today using wires to connect the transmitter which setting on top of the electrodes with a reader located on the scalp which can create brain infection due to scar tissue and that might lead to serious brain injury. We have eliminated this risk by using passive transmitter setting on the electrode and transmit wirelessly to a reader sitting on the scalp using a back-scatter technique which shows a great potential in BCI Applications. This paper investigates the feasibility of passive Ultra High Frequency Radio Frequency Identification (UHF–RFID) for wireless communication between multiple transmitters inside the brain that collect vital data continuously and transmit them to an external controller located on the scalp outside the brain in order to design wireless communication channel inside the human brain considering network lifetime and minimize power consumption. Also, we emphasize the effect of increasing number of transmitters to maximize the throughput. Extensive literature survey shows that there is exists no model has been made for invasive BBCI applications based on UHF passive RFID. Results are presented with calculated Received Signal Strength (RSSI), Signal to Noise Ratio (SNR), Channel Capacity, Maximum number of the electrode, and Path Loss. These analyses are essential for building a brain–computer interface application. We showcase theoretical and experimental results based on a phantom model of the human brain using passive RFID as the implantable transmitter operating in UHF range. Based on these values we have concluded that UHF–RFID is a viable technology with multiple transmitters to a depth of 4 cm.	brain–computer interface;transceiver;ultra high frequency	Shams Al Ajrawi;Hayden Bialek;Mahasweta Sarkar;Ramesh Rao;Syed Hassan Ahmed	2018	Future Generation Comp. Syst.	10.1016/j.future.2018.03.036	real-time computing;radio-frequency identification;wireless network interface controller;transceiver;transmitter;path loss;computer science;ultra high frequency;communication channel;electronic engineering;signal-to-noise ratio	Arch	46.085126060058286	0.684260470379829	19698
68c27bebbbd5de3311d52a525aa2a60098c16972	speech watermarking for air traffic control	bch codes;air traffic control;speech processing;watermarking;bch-code;cd-audio copyright protection;error control coding;linear prediction;single vhf frequency;spectral shaping;speech signal;speech watermarking;spread spectrum technology	In air traffic control the voice communication between a controller and all pilots in a delimited airspace are handled on a single VHF frequency. To identify a speaking aircraft, pilots have to start all verbal communications with the aircraft call sign. For automatic identification, it is desirable to transmit additional hidden aircraft identification data in time with this voice message over the VHF channel. That means the additional digital data has to be embedded into the speech signal. Such watermarking systems are used e.g. for CD-audio copyright protection, where copyright data is embedded into the music without any audible distortion. A system for speech watermarking in an air traffic control environment is developed. The system uses spread spectrum technology with linear prediction for spectral shaping of the watermark to achieve perceptual hiding. Error control coding is done using the BCH-code, which can both detect and correct errors. Results show that, for 12 bits/s, the watermark can be transmitted at a level which is hardly audible with very low error rate (≤ 0.1%). For the transmission of 24 and 36 bits/s the watermark level has to be increased to an audible but not annoying level so as to stay in a low error rate region.	add-ons for firefox;algorithm;automatic identification and data capture;bch code;bit error rate;channel (communications);delimiter;digital data;digital watermarking;distortion;embedded system;error detection and correction;error message;floor and ceiling functions;interference (communication);k-mer;noise (electronics);noise shaping;voice analysis	Martin Hagmüller;Horst Hering;Andreas Kröpfl;Gernot Kubin	2004	2004 12th European Signal Processing Conference		speech recognition;telecommunications;engineering;communication	Mobile	49.432880649801305	-8.144827525542201	19738
39ff8c462603d84b591d18d7f96bc3a9c919a200	early determination of mode decision for hevc	video coding standards;standards;encoding complexity theory video coding transforms vectors software indexes;video coding;motion vector;hm 4 0 reference software mode decision hevc fast decision method scheme encoder complexity reduction skip mode detection cu level differential motion vector dmv coded block flag cbf encoding complexity random access configuration low delay configuration high efficiency video coding test model;high efficiency;early detection;mode decision;random access	In this paper, we propose a fast decision method scheme to reduce encoder complexity of high efficiency video coding. It is an early detection of SKIP mode in one CU-level based on the differential motion vector (DMV) and coded block flag (CBF). Experimental results show that the encoding complexity can be reduced by up to 34.55% in random access (RA) configuration and 36.48% in low delay (LD) configuration with only a little bit of rate increment compared to the high efficiency video coding test model (HM) 4.0 reference software.	data compression;encoder;high efficiency video coding;preprocessor;random access;tip (unix utility);traffic collision avoidance system	Jaehwan Kim;Jungyoup Yang;Kwanghyun Won;Byeungwoo Jeon	2012	2012 Picture Coding Symposium	10.1109/PCS.2012.6213251	real-time computing;computer science;theoretical computer science;coding tree unit;context-adaptive binary arithmetic coding;h.261;random access;multiview video coding	EDA	47.072147479053	-19.32508111531785	19810
d888d6f265a4ef9c5f46498fffae4dde5b9ff4d5	a particle swarm optimization for steiner tree problem	topology;marine animals;steiner trees particle swarm optimization approximation algorithms heuristic algorithms topology network topology marine animals;steiner trees;approximation algorithms;tree structure representation;search space;pso;trees mathematics;particle mutation method particle swarm optimization steiner tree problem tree structure representation r neighborhood ring topology pso;trees mathematics particle swarm optimisation;tree structure coding;network topology;simulation experiment;combinatorial optimization particle swarm optimization steiner tree problem tree structure coding;particle swarm optimizer;heuristic algorithms;particle swarm optimization;tree structure;r neighborhood ring topology;particle mutation method;steiner tree problem;combinatorial optimization;particle swarm optimisation;steiner tree	This paper presents a particle swarm optimization for solving Steiner tree problem. In the algorithm a tree structure representation is used to encode a particle. To realize the transmission of tree structure information a novel method of particles flying in search space is proposed. We also present the r-neighborhood ring topology of particles to enhance the ability of local and global search of PSO algorithm, and a particle mutation method to keep the diversity of particle population. Exhaustive simulation experiments are carried out on different problems and different network topologies. The results indicate that the proposed algorithm has good searching performance for finding optimal Steiner tree.	algorithm;encode;experiment;mathematical optimization;network topology;particle swarm optimization;ring network;simulation;software release life cycle;steiner tree problem;tree structure	Xuan Ma;Qing Liu	2010	2010 Sixth International Conference on Natural Computation	10.1109/ICNC.2010.5583217	optimal binary search tree;mathematical optimization;multi-swarm optimization;combinatorics;discrete mathematics;steiner tree problem;mathematics;tree traversal	Robotics	26.58953072396001	-1.3556288736934674	19843
f41be6d24d7a571eaa221ee594f36e47c6f3934a	multi-robot path planning based on improved d* lite algorithm	d;sql control engineering computing minimisation multi robot systems path planning;path planning;prediction algorithms;genetics;fast re planning multi robot path planning d lite;navigation;robots;lite;fast re planning;robots navigation planning prediction algorithms genetics;planning;mysql server multirobot path planning d lite algorithm replanning algorithm;multi robot	This paper proposes an improved multi-robot path planning algorithm for finding the path via interacting with multiple robots. The task is to find the path with a minimum amount of computation time by using fast re-planning algorithm. To solve multi-robot path planning problem which cannot be executed in real-time, we regard other robots, exclusive the origin robot, as obstacles. Therefore, the robot uploads location information to the MySQL server to plan a safe distance between robots.	adobe flash lite;algorithm;automated planning and scheduling;computation;interaction;motion planning;mysql;real-time clock;robot;server (computing);time complexity;upload	Jung-Hao Peng;I-Hsum Li;Yi-Hsing Chien;Chen-Chien James Hsu;Wei-Yen Wang	2015	2015 IEEE 12th International Conference on Networking, Sensing and Control	10.1109/ICNSC.2015.7116061	planning;robot;computer vision;navigation;simulation;prediction;any-angle path planning;computer science;artificial intelligence;machine learning;motion planning	Robotics	53.17857979772597	-23.64360178285517	19911
5e067181031a828407faac0e17e0b6c14e67a273	more secure lossless visible watermarking by dct		This study proposes a scheme for using modified coefficients of the DCT of an image to generate a lossless visible watermark. The major contribution of the proposed technique is the improved security against attack to remove watermarks under stricter assumption of Kerckhoffs’ principle. After the host images and watermarks are decomposed into several frequencies, the DCT coefficients of the watermark are embedded into the DCT coefficients of the host image. Integer mapping is then used to perform 2-dimensional DCT. The major advantage of the method is the improved security achieved by using a random permutation matrix to factorize the transformation matrix. That is, since the embedding stage multiplies the transformation matrix by a random permutation matrix, illicit users, even under the stricter assumption of Kerckhoffs principle that the proposed embedding method is known by illicit users, cannot properly recover the host image without the correct permutation matrix. Unlike methods that embed the watermark in quantized frequency-domain coefficients, the watermarked image remains in raw lossless image form instead of some lossy form of quantized coefficients e.g., JPEG-formatted. Maintaining the lossless format of the watermarked image provides reversibility.	algorithm;authorization;coefficient;digital watermarking;discrete cosine transform;embedded system;experiment;jpeg;kerckhoffs's principle;lossless compression;lossy compression;overhead (computing);quantization (signal processing);random permutation;simulation;transformation matrix	Yih-Kai Lin;Cheng-Hsing Yang;Jinn-Tsong Tsai	2017	Multimedia Tools and Applications	10.1007/s11042-017-4753-3	permutation matrix;artificial intelligence;kerckhoffs's principle;random permutation;computer science;lossy compression;watermark;theoretical computer science;matrix (mathematics);digital watermarking;pattern recognition;lossless compression	ML	40.18442813753895	-10.642787968868332	19920
40b79800932bc7aa0870403b1825d6337fe682c4	leveraging ami data for distribution system model calibration and situational awareness	power transmission and distribution graphical user interfaces;reliability;power transmission and distribution;state estimation;power distribution;voltage measurement accuracy decision support systems smart meters reliability substations load modeling;historical smart meter data infrastructure ami data distribution system model calibration situational awareness distributed energy resources distribution system state estimation dsse smart meters quality distribution system models intelligent methods parameter estimation method regression analysis voltage drop equation distribution system model 3 d graphical user interface university distribution network;accuracy;smart grids;power system measurements;visualization;graphical user interfaces;visualization graphical user interfaces load modeling parameter estimation pe power distribution power system measurements smart grids state estimation se;decision support systems;substations;parameter estimation;load modeling;voltage measurement;smart meters;state estimation calibration distribution networks graphical user interfaces regression analysis smart meters	The many new distributed energy resources being installed at the distribution system level require increased visibility into system operations that will be enabled by distribution system state estimation (DSSE) and situational awareness applications. Reliable and accurate DSSE requires both robust methods for managing the big data provided by smart meters and quality distribution system models. This paper presents intelligent methods for detecting and dealing with missing or inaccurate smart meter data, as well as the ways to process the data for different applications. It also presents an efficient and flexible parameter estimation method based on the voltage drop equation and regression analysis to enhance distribution system model accuracy. Finally, it presents a 3-D graphical user interface for advanced visualization of the system state and events. We demonstrate this paper for a university distribution network with the state-of-the-art real-time and historical smart meter data infrastructure.	algorithm;big data;convex optimization;data infrastructure;estimation theory;graphical user interface;level of detail;mathematical optimization;online and offline;r language;radial (radio);real-time clock;real-time transcription;sensor;smart meter;systems modeling;transformer	Jouni Peppanen;Matthew J. Reno;Mohini Thakkar;Santiago Grijalva;Ronald G. Harley	2015	IEEE Transactions on Smart Grid	10.1109/TSG.2014.2385636	embedded system;electronic engineering;simulation;visualization;decision support system;computer science;reliability;graphical user interface;mathematics;accuracy and precision;smart grid;estimation theory;statistics	Visualization	35.731572543720056	-2.1396285717748427	19939
5bfd98cb8419c4703a89b8c1d1c5e1c2fbce3b02	intra block copy for next generation video coding		Intra block copy, or IBC for short, has shown its effectiveness in coding computer-generated contents such as texts and graphics. Therefore, it has been adopted into the HEVC standard version 4, extensions for screen content coding. It is also desirable that intra block copy is considered as a part of the next generation video coding standard, as screen contents have become one of the mainstream video applications. In this paper, an IBC implementation is incorporated into the video coding research software platform hosted by JVET for developing the next generation video coding standard. Simulation results show that with all the advanced coding tools enabled beyond the HEVC in the most-up-to-date JVET, the IBC mode can still achieve a significant coding efficiency improvement on screen content test materials.		Xiaozhong Xu;Xiang Li;Shan Liu	2018	2018 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)	10.1109/ICMEW.2018.8551528	coding (social sciences);computer vision;computer science;theoretical computer science;artificial intelligence;software;graphics;algorithmic efficiency	DB	44.42394144957826	-19.96590121004291	20000
05aaf5c8da1dd478594071315cedb174d025e026	enhanced differential evolution using center-based sampling	differential evolution;evolutionary computation;high dimensionality;perforation;search space;center based sampling;large scale;visualization;search space center based sampling evolutionary algorithms opposition based differential evolution center based differential evolution large scale problem;heuristic algorithms;visual search;exact algorithm;dynamic range;center based differential evolution;evolutionary algorithms;opposition based differential evolution;search problems;evolutionary algorithm;sampling methods;call center;heuristic algorithms algorithm design and analysis benchmark testing evolutionary computation visualization search problems dynamic range;sampling methods evolutionary computation;algorithm design;algorithm design and analysis;benchmark testing;large scale problem;heuristic algorithm;evolutionary computing	The classical Differential Evolution (DE) has showed to perform efficiently in solving both benchmark functions and real-world problems. However, DE, similar to other evolutionary algorithms deteriorate in performance during solving high-dimensional problems. Opposition-based Differential Evolution (ODE) was introduced and, in general, has shown better performance comparing to classical DE for solving large-scale problems. In this paper, we propose an enhancement to ODE in order to improve its ability to solve large-scale problems more effectively. The proposed modified version of ODE is called Center-Based Differential Evolution (CDE) which utilizes the exact algorithm of ODE except replacing of opposite points with center-based individuals. This paper compares DE and ODE with the proposed algorithm, CDE. Furthermore, CDE with dynamic range (CDEd) will be compared to CDE with fixed range (CDEf). Experimental verifications are conducted on seven well-known shifted large-scale benchmark functions for dimensions of 100 and 500, including detailed parameter analysis for CDE. The shifted version of the functions ensures there is no bias towards the center of search space, in favor of CDE algorithm. The results clearly show that the CDE outperforms DE and ODE during solving large-scale problems, and also clarifies the superiority of CDEd to CDEf.	benchmark (computing);differential evolution;dynamic range;evolutionary algorithm;exact algorithm;sampling (signal processing)	Ali Esmailzadeh;Shahryar Rahnamayan	2011	2011 IEEE Congress of Evolutionary Computation (CEC)	10.1109/CEC.2011.5949948	algorithm design;mathematical optimization;computer science;artificial intelligence;machine learning;evolutionary algorithm;mathematics;algorithm;evolutionary computation	AI	25.31716936507717	-3.384183179200701	20024
81d9b0994104c62f68b1ae62f5bc53df92839e8b	optimal correlating transform for erasure channels	channel coding;optimisation;erasure channel;correlation theory;correlation theory transform coding optimisation gradient methods discrete transforms channel coding error statistics;transform coding discrete transforms robustness signal processing algorithms statistics performance loss computational modeling probability algorithm design and analysis quantization;transform coding;transform coding erasure channel multiple description coding;multiple description coding optimal correlating transform erasure channel gradient based algorithm discrete transform coding;performance improvement;discrete transforms;gradient methods;multiple description coding;error statistics;loss probability	In this letter, we derive a gradient-based algorithm for computing the optimal transform when coefficients are transmitted over an erasure channel whose statistics are known. The discrete transform introduces correlation among the coefficients with consequent performance improvement against losses. Simulations show appreciable improvements over standard schemes and also good robustness when loss probabilities are only roughly estimated.	algorithm;binary erasure channel;coefficient;computer simulation;discrete transform;gradient	Gianmarco Romano;Pierluigi Salvo Rossi;Francesco Palmieri	2005	IEEE Signal Processing Letters	10.1109/LSP.2005.855557	binary erasure channel;transform coding;s transform;lapped transform;shannon–fano coding;channel code;variable-length code;computer science;theoretical computer science;multiple description coding;mathematics;statistics	Theory	48.5994105273694	-13.13831626841924	20048
3c6ca398f74886a0dc81c6e12ce932612a364b76	nonparametric estimation in α-series processes	trend analysis;linear regression;renewal process	trend analysis;linear regression;renewal process		Halil Aydogdu;Mahmut Kara	2012	Computational Statistics & Data Analysis	10.1016/j.csda.2011.06.037	nonparametric regression	ML	28.91657946464391	-21.10452333537997	20110
005cb48b1bf96bcd0c60d22d56cd17253812cbc4	a new error concealment technique for audio transmission with packet loss	packet loss;correlation;speech;time domain;merging	We present a new error concealment technique for audio transmission over packet networks with high packet loss rate. Unlike other techniques it modifies the time-scale of correctly received packets instead of repeating them. This is done by a time-domain algorithm, WSOLA, whose parameters are redefined so that short audio segments like lost packets can be extended. Particular attention is paid to the additional delay introduced by the new technique. For subjective hearing tests, single and double packet loss is simulated at high packet loss rates, and the new technique is compared to previous proposals by category judgment and component judgment of sound quality. Mean Opinion Score (MOS) curves show that sound distortions due to packet repetition can be reduced.	algorithm;definition;distortion;error concealment;network packet;sound quality;terminator 2: judgment day	Alexander Stenger;Khaled Ben Younes;Richard Reng;Bernd Girod	1996	1996 8th European Signal Processing Conference (EUSIPCO 1996)		radio link protocol;speech recognition;fast packet switching;telecommunications;computer science;processing delay;end-to-end delay;transmission delay;packet loss;communication	Networks	49.680006786379614	-8.551041965053267	20119
ed3999711a0621fa9cddfe6c04affde3f1312e1c	a study on non-octave scalable image coding and its performance evaluation using digital cinema test material	tecnologia electronica telecomunicaciones;image coding;performance evaluation;resolution conversion;wavelet transform;digital cinema;jpeg2000;tecnologias;grupo a;super high definition images	JPEG2000, an international standard for still image compression, offers 1) high coding performance, 2) unified lossless/lossy compression, and 3) resolution and SNR scalability. Resolution scalability is an especially promising attribute given the popularity of Super High Definition (SHD) images like digital-cinema. Unfortunately, its current implementation of resolution scalability is restricted to powers of two. In this paper, we introduce non-octave scalable coding (NSC) based on the use of filter banks. Two types of non-octave scalable coding are implemented. One is based on a DCT filter bank and the other uses wavelet transform. The latter is compatible with JPEG2000 Part2. By using the proposed algorithm, images with rational scale resolutions can be decoded from a compressed bit stream. Experiments on digital cinema test material show the effectiveness of the proposed algorithm.	cinema 4d;performance evaluation	Takayuki Nakachi;Tomoko Sawabe;Junji Suzuki;Tetsuro Fujii	2006	IEICE Transactions	10.1093/ietfec/e89-a.9.2405	telecommunications;computer science;jpeg 2000;mathematics;multimedia;wavelet transform;computer graphics (images)	Vision	43.447302664863216	-17.74963802263323	20204
ce2ea857c503956b1a64a318e46da504d398291d	a tale of two transformers: an algorithm for estimating distribution secondary electric parameters using smart meter data	linear programming optimization power transformers distribution transformer secondary networks distribution secondary electric parameters smart meter data smart metering and infrastructure smi bc hydro electricity theft geographic information system gis;voltage measurement geographic information systems linear programming power transformers smart meters smart power grids;topology impedance convergence network topology accuracy load modeling mathematical model;power transformers;smart power grids;geographic information systems;linear programming;data analytics smart meters smart grid distribution secondary distribution network topology;voltage measurement;smart meters	One of the main objectives for the Smart Metering and Infrastructure (SMI) System currently being installed at BC Hydro, is to improve the company's ability to detect and locate electricity theft [1]. Some of the methods used to detect theft depend on accurate topology data for distribution transformer secondary networks and accurate estimates of the voltage at the transformer. The topology data from the Geographic Information System (GIS) is sometimes inaccurate. This paper describes a methodology to confirm the accuracy of the topology and to estimate the transformer secondary voltage, based only on the topology structure and hourly load and voltage data from each smart meter connected to the transformer. The method uses Linear Programming (LP) optimization and simultaneously provides estimates of the impedances of each segment of the secondary line. The degree of convergence indicates the accuracy of the given topology, and in many cases the locations of topology errors can be identified.	algorithm;geographic information system;linear programming;mathematical optimization;smart meter;transformer;transformers	Andrew John Berrisford	2013	2013 26th IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)	10.1109/CCECE.2013.6567690	embedded system;electronic engineering;engineering;linear programming;electrical engineering;smart grid;transformer	Embedded	35.59478038563573	-1.788341522624844	20218
2beb9e27d5bc2e603aca9c68ad32ebad1895912b	fuzzy classification maximum likelihood algorithms for mixed-weibull distributions	fuzzy classification;fuzzy set;fuzzy clustering algorithm;maximum likelihood;efficient algorithm;weibull distribution;fuzzy clustering;fuzzy class model;mixed weibull distribution;em algorithm	In this paper we propose an efficient algorithm based on Yang’s (Fuzzy Sets Syst 57:365–337, 1993) concept, namely the fuzzy classification maximum likelihood (FCML) algorithm, to estimate the mixed-Weibull parameters. Compared with EM and Jiang and Murthy (IEEE Trans Reliab 44:477–488, 1995) methods, the proposed FCML algorithm presents better accuracy. Thus, we recommend FCML as another acceptable method for estimating the mixed-Weibull parameters.	algorithm;fuzzy classification	Wen-Liang Hung;Yen-Chang Chang;Shun-Chin Chuang	2008	Soft Comput.	10.1007/s00500-007-0266-8	weibull distribution;expectation–maximization algorithm;fuzzy clustering;fuzzy classification;computer science;machine learning;pattern recognition;mathematics;maximum likelihood;fuzzy set;statistics	ECom	25.16982411718333	-23.148966671752298	20230
53fadd30de8976b8f5b45e291248e5b81a9e4777	sums, products, and ratios for the bivariate lomax distribution	hypergeometric function;statistical moment;sums;point pourcentage;analisis datos;fonction repartition;moment statistique;62e17;calculo automatico;ley 2 variables;percentage point;computing;gauss hypergeometric function;calcul automatique;funcion distribucion;momento estadistico;data analysis;distribution function;lomax distribution;33cxx;statistical computation;calculo estadistico;fonction hypergeometrique;analyse donnee;funcion hipergeometrica;bivariate distribution;calcul statistique;gauss function;60e05;loi 2 variables;ratios;62h12;products;fonction gauss	We derive the distributions of S = X + Y , D = X − Y , P = XY andW = X/(X + Y ) and the corresponding moment properties when X andY follow the bivariate lomax distribution. Some expressions involve the Gauss hypergeometric function. We also provide an extensive tabulation of the associated percentage points. This tabulation—obtained using intensive computing power— will be of use to practitioners of the lomax distribution. © 2004 Elsevier B.V. All rights reserved.	bivariate data;classical xy model;table (information)	Saralees Nadarajah	2005	Computational Statistics & Data Analysis	10.1016/j.csda.2004.05.003	computing;hypergeometric function;percentage point;distribution function;calculus;mathematics;geometry;lomax distribution;data analysis;statistics	AI	34.13838392464612	-20.828400773881345	20409
0ac0b0c5d0678b68923cc155618999308272fdd3	lct-wavelet based algorithms for data compression	hyper spectral;hybrid compression;seismic;wavelet transform;local cosine transform lct;spiht;compression;fingerprints	We present an algorithm that compresses two-dimensional data arrays, which are piece-wise smooth in one direction and have oscillatory events in the other direction. Seismic and hyperspectral data have this mixed structure. The transform part of the compression process is an algorithm that combines wavelet and the local cosine transform (LCT). The quantization and the entropy coding parts in the compression process were taken from the SPIHT codec. To efficiently apply the SPIHT codec to a mixed coefficients array, reordering of the LCT coefficients takes place. When oscillating events are present in different directions as in fingerprints or when the image comprises a fine texture, the 2D LCT with reordering of coefficients is applied. These algorithms outperform algorithms that are solely based on the 2D wavelet transforms and SPIHT coding including JPEG2000 compression standard. The algorithms retain fine oscillating events including texture even at a low bitrate. Its compression capabilities are also demonstrated on multimedia images that have a fine texture. The wavelet part in the mixed transform of the hybrid algorithm utilizes the library of Butterworth wavelet transforms that outperforms the 9/7 biorthogonal wavelet library.	biorthogonal wavelet;butterworth filter;codec;coefficient;data compression;entropy encoding;experiment;fingerprint;hybrid algorithm;hyper-heuristic;jpeg 2000;linear canonical transformation;olap cube;set partitioning in hierarchical trees;seven of nine;wavelet transform	Amir Averbuch;Valery A. Zheludev;Moshe Guttmann;Dan D. Kosloff	2013	IJWMIP	10.1142/S021969131350032X	data compression;wavelet;fingerprint;mathematical analysis;speech recognition;harmonic wavelet transform;second-generation wavelet transform;continuous wavelet transform;computer science;theoretical computer science;mathematics;stationary wavelet transform;discrete wavelet transform;texture compression;compression;set partitioning in hierarchical trees;wavelet transform;computer graphics (images)	Visualization	42.639903612085746	-15.319800545958449	20540
b60c572b881d44ab46939abd9cea700ae9b9c85d	velocity measurement among correlation sonar by hybrid particle swarm optimization algorithm	simulated annealing;correlation sonar;particle swarm optimization;hpso;particle swarm optimization algorithm	Velocity measurement among correlation sonar using a modified particle swarm optimization (PSO) algorithm is proposed in this paper. This new swarm intelligence method is based on a hybrid of PSO and simulated annealing (SA), and is thus called HPSO. In HPSO, the particles updated depending on the acceptance probability which will maintain the population's diversity for a large number of iterations to converge to the global optimum. The performance of HPSO is compared to PSO in velocity measurement in deep water, demonstrating its superiority.	algorithm;particle swarm optimization;sonar;velocity	Jinghong Xue;Ming Jin;Xiaolin Qiao	2007			mathematical optimization;multi-swarm optimization;simulated annealing;computer science;artificial intelligence;machine learning;particle swarm optimization	Robotics	27.587074716604853	-4.432289686573841	20570
4be51578067ffd93c3bd7a8193ad401414af7ecd	an optimized template matching approach to intra coding in video/image compression	image compression;video	The template matching prediction is an established approach to intra-frame coding that makes use of previously coded pixels in the same frame for reference. It compares the previously reconstructed upper and left boundaries in searching from the reference area the best matched block for prediction, and hence eliminates the need of sending additional information to reproduce the same prediction at decoder. In viewing the image signal as an auto-regressive model, this work is premised on the fact that pixels closer to the known block boundary are better predicted than those far apart. It significantly extends the scope of the template matching approach, which is typically followed by a conventional discrete cosine transform (DCT) for the prediction residuals, by employing an asymmetric discrete sine transform (ADST), whose basis functions vanish at the prediction boundary and reach maximum magnitude at far end, to fully exploit statistics of the residual signals. It was experimentally shown that the proposed scheme provides substantial coding performance gains on top of the conventional template matching method over the baseline.	baseline (configuration management);basis function;codec;column (database);discrete cosine transform;discrete sine transform;experiment;image compression;intra-frame coding;pixel;reference frame (video);star trek:;template matching;transform coding;vp9	Hui Su;Jingning Han;Yaowu Xu	2014		10.1117/12.2040890	computer vision;template matching;video;image compression;theoretical computer science;pattern recognition;physics	Vision	45.027372927241856	-17.00770724754626	20651
c3664c4efa8fcbf1696af7690b62a47e06abb917	optimal entropy constrained scalar quantization for exponential and laplacian random variables	quantization;exponential distribution;image coding;probability;data compression;uniform threshold quantizer optimal entropy constrained scalar quantization laplacian random variables exponential random variables design problem speech compression image compression one dimensional nonlinear equations two dimensional solution;entropy codes quantisation signal data compression image coding speech coding exponential distribution probability nonlinear equations;probability density function;entropy quantization laplace equations random variables design optimization nonlinear equations speech image coding iterative methods design methodology;speech;speech coding;random variables;design optimization;uniform threshold quantizer;quantisation signal;iterative methods;laplace equations;scalar quantization;image compression;two dimensional solution;one dimensional nonlinear equations;entropy codes;random variable;nonlinear equation;laplacian random variables;nonlinear equations;optimal entropy constrained scalar quantization;entropy;design problem;iterative solution;exponential random variables;speech compression;design methodology	This paper presents solutions t o the entropy-constrained scalar quantizer (ECSQ) design problem for two sources commonly encountered in image and speech compression applications: sources having exponential and Laplacian probability density functions. We obtain the optimal ECSQ either with or without an additional constraint on the number of levels in the quantizer. In contrast to prior methods, which require iterative solution of a large number of nonlinear equations, the new method needs only a single sequence of solutions t o one-dimensional nonlinear equations (in some Laplacian cases, one additional two-dimensional solution is needed). As a result, the new method is orders of magnitude faster than prior ones. We also show that as the constraint on the number of levels in the quantizer is relaxed, the o p timal ECSQ becomes a uniform threshold quantizer (UTQ) for exponential, but not for Laplacian sources.		Gary J. Sullivan	1994		10.1109/ICASSP.1994.389481	random variable;mathematical optimization;combinatorics;discrete mathematics;nonlinear system;mathematics;statistics	Robotics	49.33332370648312	-11.769908103823017	20655
267734c46ba04e24a193b6fd6573f9a98701cb8a	a multivariate logistic distance model for the analysis of multiple binary responses	multivariate binary data;biplots;multidimensional scaling;multidimensional unfolding;marginal model;clustered bootstrap;generalized estimating equations	We propose a Multivariate Logistic Distance (MLD) model for the analysis of multiple binary responses in the presence of predictors. The MLD model can be used to simultaneously assess the dimensional/factorial structure of the data and to study the effect of the predictor variables on each of the response variables. To enhance interpretation, the results of the proposed model can be graphically represented in a biplot, showing predictor variable axes, the categories of the response variables and the subjects’ positions. The interpretation of the biplot uses a distance rule. The MLD model belongs to the family of marginal models for multivariate responses, as opposed to latent variable models and conditionally specified models. By setting the distance between the two categories of every response variable to be equal, the MLD model becomes equivalent to a marginal model for multivariate binary data estimated using a GEE method. In that case the MLD model can be fitted using existing statistical packages with a GEE procedure, e.g., the genmod procedure from SAS or the geepack package from R. Without the equality constraint, the MLD model is a general model which can be fitted by its own right. We applied the proposed model to empirical data to illustrate its advantages.		Hailemichael M. Worku;Mark de Rooij	2018	J. Classification	10.1007/s00357-018-9251-4	statistics;factorial;mathematics;biplot;marginal model;binary data;multivariate statistics;multidimensional scaling;generalized estimating equation;latent variable	ML	31.16092595046318	-23.042878903868512	20665
2315adb2f34f23802a9995b8fbe3e30da08e7109	numerical challenges in particle-based approaches for the simulation of semiconductor devices	software tool;charge transport;semiconductor devices;three dimensional;inp;continuous improvement;ensemble monte carlo;charge transport modeling;monte carlo;electronic structure;3d structure;cellular automaton;device simulation	The aim of this paper is to review and discuss the most challenging aspects of the particle-based methods for simulation of charge transport in semiconductor devices. Since the early theoretical works on the Ensemble Monte Carlo (EMC) method applied to device simulation, and several successive works addressing both the physics and the numerical aspects of the EMC method, the basic algorithmic approaches have been modified to exploit the continuous improvements of both hardware and software tools. Typical examples of the algorithmic evolution are the adoption of the full band representation of the electronic structure, the so-called cellular automaton (CA), and the simulation of increasingly complex three-dimensional (3D) structures. This paper will address some of the most significant problems which are still considered open in spite of the recent technological and scientific progresses.	semiconductor device;simulation	Marco Saraniti;J. Tang;Stephen M. Goodnick;S. J. Wigger	2003	Mathematics and Computers in Simulation	10.1016/S0378-4754(02)00229-X	statistical physics;cellular automaton;three-dimensional space;simulation;semiconductor device;computer science;theoretical computer science;mathematics;algorithm;electronic structure;statistics;monte carlo method	EDA	43.30594337028361	2.813052411168522	20726
8eab00383acf9f42b16a0468e6e4f6aa88c5338a	an 8-segmented 256×512 cmos image sensor for processing element-coupled unified system in machine vision application	image segmentation cmos image sensors computer vision focal planes image denoising;image segmentation object segmentation image sensors engines noise bandwidth;image segmentation;size 0 18 mum cmos image sensor processing element coupled unified system machine vision focal plane imaging parameter pixel count exposure time control fixed pattern noise temporal noise image segmentation;focal planes;computer vision;cmos image sensors;image denoising	A CMOS image sensor whose focal plane is split into 8 segments is presented. The proposed architecture adjusts imaging parameters on a segmented base that consists of 128×128 pixels. Each segment is coupled to an external processing engine, which enables faster readout and real-time processing as well as co-operation among the segments regardless of the pixel count. Based on these characteristics, the architecture can provide an excellent unified imaging and processing system that is especially beneficial for machine vision applications. A chip was fabricated using a 0.18-μm 1P5M standard CMOS process and the sensor was demonstrated to achieve individual frame rate and exposure time control with <;0.04% column fixed pattern noise (FPN) and <;0.11% temporal noise.	cmos;focal (programming language);fixed-pattern noise;image resolution;image sensor;machine vision;real-time clock;region of interest;run time (program lifecycle phase)	Toshinori Otaka;Tomohiro Yamasaki;Takayuki Hamamoto	2012	10th IEEE International NEWCAS Conference	10.1109/NEWCAS.2012.6329023	image noise;computer vision;electronic engineering;feature detection;binary image;image processing;computer science;digital image processing;image sensor;image segmentation;scale-space segmentation;non-local means;computer graphics (images)	Robotics	43.00832181654138	-4.1512131903266	20766
770f37a3393f025825f3a33b2484a6b569acbe0b	a new approach for weibull modeling for reliability life data analysis	goodness of fit tests gof;weibull models;expectation and maximization em algorithm;life data analysis;weibull probability paper wpp;maximum likelihood estimation mle method	This paper presents a proposed approach for modeling the life data for system components that have failure modes by different Weibull models. This approach is applied for censored, grouped and ungrouped samples. To support the main idea, numerical applications with exact failure times and censored data are implemented. The parameters are obtained by different computational statistical methods such as graphic method based on Weibull probability plot (WPP), maximum likelihood estimates (MLE), Bayes estimators, non-linear Benard’s median rank regression. This paper also presents a parametric estimation method depends on expectation–maximization (EM) algorithm for estimation the parameters of finite Weibull mixture distributions. GOF is used to determine the best distribution for modeling life data. The performance of the proposed approach to model lifetime data is assessed. It’s an efficient approach for moderate and large samples especially with a heavily censored data and few exact failure times.		Emad E. Elmahdy	2015	Applied Mathematics and Computation	10.1016/j.amc.2014.10.036	econometrics;exponentiated weibull distribution;weibull fading;mathematics;statistics	ML	29.82771628045766	-20.649486272503676	20801
8d6f76fef6f7a56e8a8774863ef5f5b03f1b320e	batch selection for parallelisation of bayesian quadrature		Integration over non-negative integrands is a central problem in machine learning (e.g. for model averaging, (hyper-)parameter marginalisation, and computing posterior predictive distributions). Bayesian Quadrature is a probabilistic numerical integration technique that performs promisingly when compared to traditional Markov Chain Monte Carlo methods. However, in contrast to easily-parallelised MCMC methods, Bayesian Quadrature methods have, thus far, been essentially serial in nature, selecting a single point to sample at each step of the algorithm. We deliver methods to select batches of points at each step, based upon those recently presented in the Batch Bayesian Optimisation literature. Such parallelisation significantly reduces computation time, especially when the integrand is expensive to sample.		Ed Wagstaff;Saad Hamid;Michael Osborne	2018	CoRR			AI	30.634310523399055	-16.40759741735226	20880
37a210c21bea2d2a5975fa3ec0d4237d9a13291b	blind kriging: implementation and performance analysis	blind kriging;benchmark;selection;surrogate modelling;technology and engineering;support;metamodelling;feature selection;optimization;ta engineering general civil engineering general;europearl;variable subset selection	When analysing data from computationally expensive simulation codes or process measurements, surrogate modelling methods are firmly established as facilitators for design space exploration, sensitivity analysis, visualisation and optimisation. Kriging is a popular surrogate modelling technique for data based on deterministic computer experiments. There exist several types of Kriging, mostly differing in the type of regression function used. Recently a promising new variable selection technique was proposed to identify a regression function in the Kriging framework. In this paper this type of Kriging, i.e., blind Kriging, has been efficiently implemented in Matlab® and has been extended. The implementation is validated and tested on several examples to illustrate the strength and weaknesses of this new, promising modelling technique. It is shown that the performance of blind Kriging is as good as, or better than ordinary Kriging. Though, blind Kriging comes at double the computational cost with respect to ordinary Kriging.	algorithmic efficiency;analysis of algorithms;code;design space exploration;experiment;feature selection;kriging;matlab;mathematical optimization;profiling (computer programming);simulation;surrogate model	Ivo Couckuyt;A. Forrester;Dirk Gorissen;Filip De Turck;Tom Dhaene	2012	Advances in Engineering Software	10.1016/j.advengsoft.2012.03.002	selection;econometrics;mathematical optimization;support;benchmark;computer science;engineering;machine learning;data mining;mathematics;feature selection	AI	26.868966778749567	-14.789218028079727	20897
3b4f9885c0f6fd406802a4b527372c26bb7a2e28	a note on the use of multiple comparison scenario techniques in education and practice	des experimentation;multiple comparison scenario technique;multiple comparison control procedure;current practice;exploratory manner;des study;multiple comparison control;des textbook;fwer control method;multiple scenario comparison;exploratory style study;space exploration;multiple comparisons;scenario analysis;family wise error rate;false discovery rate;discrete event simulation;probability	Our main aim in this paper is to highlight current practice and education in multiple scenario comparison within DES experimentation and to illustrate the possible benefits of employing false discovery rate (FDR) control as opposed to strict family-wise error rate (FWER) control when comparing large numbers of scenarios in an exploratory manner. We present the results of a small survey into the current practice of scenario analysis by simulation practitioners and academics. The results indicated that the range of scenarios used in DES studies may prohibit the use of FWER control methods such as the Bonferroni Correction referred to in DES textbooks. Furthermore, 80% of our sample were not familiar with any of the multiple comparison control procedures presented to them. We provide a practical example of the FDR in action and argue that it is preferable to employ FDR instead of no multiple comparison control in exploratory style studies.	scenario analysis;simulation	Kathryn Hoad;Thomas Monks	2011	Proceedings of the 2011 Winter Simulation Conference (WSC)		econometrics;false discovery rate;simulation;familywise error rate;computer science;engineering;discrete event simulation;space exploration;probability;mathematics;scenario analysis;multiple comparisons problem;statistics	HCI	27.798201305688174	-21.137659961629534	20934
fd46cf7a490a4078aecd5d8b4ae5a199d1bf2b51	building blocks for a temperature-compensated analog vlsi neural network with on-chip learning	high density;building block;transfer functions;transfer functions compensation analogue processing circuits neural chips vlsi learning artificial intelligence;55 to 125 degc temperature compensated analog neural network vlsi on chip learning synapse circuits neuron circuits weight increment circuits nonvolatile weight storage hybrid dynamic storage long term storage power dissipation weight increment circuit weight perturbation algorithm bias circuits synapse neuron transfer function 2 micron;temperature dependence;chip;very large scale integration neural networks network on a chip circuits temperature dependence nonvolatile memory neurons temperature distribution voltage power dissipation;neural chips;compensation;low power;transfer function;non volatile memory;vlsi;analog vlsi;learning artificial intelligence;analogue processing circuits;temperature compensation;neural network	Synapse, neuron, and weight increment circuits for a high density, temperature-compensated analog VLSI neural network are introduced. The synapse circuit, which consumes 4500 /spl mu/m/sup 2/ in a 2-/spl mu/m technology, uses hybrid dynamic and non-volatile weight storage. Dynamic memory allows fast learning while non-volatile memory allows reliable long-term storage and low power dissipation. Measured test results for the synapse are presented. The synapse includes a weight increment circuit that adds offset of only 1 part in 16 bits, thus allowing analog-domain on-chip learning using a weight perturbation algorithm. Simple bias circuits cancel temperature dependent parameters in the synapse-neuron transfer function, allowing reliable operation in the temperature range -55 C to 125 C. >		Antonio J. Montalvo;Ronald S. Gyurcsik;John J. Paulos	1994		10.1109/ISCAS.1994.409601	embedded system;electronic engineering;telecommunications;computer science;electrical engineering;control theory;transfer function;artificial neural network	ML	38.58696111022349	-1.9817734097735342	20938
857273aa0a6bb2cb7f9b969d46ab1174b612c464	an improved aea algorithm with harmony search(hsaea) and its application in reaction kinetic parameter estimation	eda;alopex;journal;期刊论文;parameter estimation;step length;harmony search	Alopex-based evolutionary algorithm (AEA) is one kind of evolutionary algorithm. It possesses the basic characteristics of evolutionary algorithms as well as the advantages of gradient descent methods and simulation anneal algorithm, but it is also easy to trap into a local optimum. For the AEA algorithm, the unreasonably settings of compared population and step length are two typical drawbacks, which lead to the lack of communication between individuals in each generation. In this paper, estimation of distribution algorithm (EDA) is employed to generate the compared population. Then the moving step length in AEA is improved to vary with different phases of the iteration during the actual operation DA tep length armony search arameter estimation process. And more importantly, harmony search algorithm (HS) is introduced to improve the quality of population of every generation. By compared with original AEA, the performance of the improved algorithm (HSAEA) was tested on 22 unconstrained benchmark functions. The testing results show that HSAEA clearly outperforms original AEA for almost all the benchmark functions. Furthermore, HSAEA is used to estimate reaction kinetic parameters for a heavy oil thermal cracking three lumps model and Homogeneous mercury (Hg) oxidation, and satisfactory results are obtained.	alopex;apache harmony;benchmark (computing);estimation of distribution algorithm;estimation theory;evolutionary algorithm;gradient descent;harmony search;iteration;local optimum;mercury;password cracking;search algorithm;simulation	Shunlong Ma;Yuehua Dong;Zhixiang Sang;Shaojun Li	2013	Appl. Soft Comput.	10.1016/j.asoc.2013.04.006	econometrics;mathematical optimization;electronic design automation;harmony search;computer science;artificial intelligence;estimation theory;statistics	AI	29.11813160016635	-4.599921298176536	20952
630ec63931ae649b55da2c9635a97a6db8149f28	a new algorithm for importance analysis of the inputs with distribution parameter uncertainty	single loop monte carlo method;surrogate sampling function;importance analysis;input variable;parameter uncertainty	A new algorithm for importance analysis of the inputs with distribution parameter uncertainty Luyi Li & Zhenzhou Lu To cite this article: Luyi Li & Zhenzhou Lu (2015): A new algorithm for importance analysis of the inputs with distribution parameter uncertainty, International Journal of Systems Science, DOI: 10.1080/00207721.2015.1088099 To link to this article: http://dx.doi.org/10.1080/00207721.2015.1088099	algorithm;algorithmic efficiency;computation;effective method;lu decomposition;mathematical optimization;monte carlo method;nested sampling algorithm;numerical analysis;portable document format;reliability engineering;sampling (signal processing);systems science	Luyi Li;Zhenzhou Lu	2016	Int. J. Systems Science	10.1080/00207721.2015.1088099	econometrics;mathematical optimization;uncertainty analysis;mathematics;sensitivity analysis;statistics	Robotics	32.302949825798876	-18.901200113319216	21047
7f6a8671b5c6de4f0d6615d77a8cc48a23c33f71	an image watermarking algorithm robust to geometric distortion	transformation ondelette;filigranage;watermarking;transformacion discreta;steganographie;robust watermarking;image processing;steganografia;distance measure;filigrana;search space;efficient algorithm;cryptanalyse;procesamiento imagen;analyse multiresolution;traitement image;cryptanalysis;criptoanalisis;steganography;senal video;signal video;discrete transformation;linear transformation;video signal;error probability;image watermarking;transformacion ondita;bch code;multi resolution;multiresolution analysis;transformation discrete;wavelet transformation;analisis multiresolucion	Digital image watermark is an invisible mark embedded in an image that can be used for copyright protection. The current paper proposes a new watermarking scheme by improving image normalization based watermarking (INW). Image normalization is based on the moments of the image, however, in general, RST attacks bring the image boundary cropping and the moments are not preserved original ones. Thereafter the normalized images of before and after are not same form. To solve the cropping problem of INW, Invariant Centroid (IC) is proposed and the only central region(R), which has less cropping possibility by RST, is used for normalization. In addition, the watermark is embedded and detected in the normalized form of the image. Experimental results demonstrated that the proposed watermarking scheme is robust to RST attacks with cropping.	algorithm;digital watermarking;distortion	Xiangui Kang;Jiwu Huang;Yun Q. Shi	2002		10.1007/3-540-36617-2_18	multiresolution analysis;cryptanalysis;computer vision;discrete mathematics;image processing;digital watermarking;computer science;theoretical computer science;probability of error;mathematics;linear map;steganography;bch code;statistics	Theory	43.17000910068559	-11.027182997428115	21065
6ba58b317094e9f13461c5573073b9cf18f474e4	packet loss protection for interactive speech object rendering: a multiple description approach	joint source channel coding quality of experience speech compression;spatial teleconferencing interactive speech object rendering packet loss protection schemes interactive real time audio services speech source mixture compression hybrid forward error correction multiple description coding approach psychoacoustic based analysis by synthesis pabs coder coder outputs hybrid fec mdc techniques perceptual evaluation of speech quality pesq measurements;speech based user interfaces audio coding data compression forward error correction graphical user interfaces interactive systems rendering computer graphics speech coding speech synthesis;speech packet loss encoding forward error correction psychoacoustic models bit rate	This paper investigates the application of novel packet loss protection schemes to compress mixtures of speech sources for interactive real-time audio services such as spatial teleconferencing. Hybrid Forward Error Correction (FEC) and Multiple Description Coding (MDC) packet loss protection techniques are applied to the outputs of a psychoacoustic-based Analysis-By-Synthesis (PABS) coder designed for such applications. The protection approaches split the coder outputs into two descriptions that are separately protected using the hybrid FEC-MDC techniques. Perceptual Evaluation of Speech Quality (PESQ) measurements compare the performance of different protection schemes for a range of typical packet loss conditions. Results indicate the proposed scheme maintains the perceptual quality of the speech sources across a wide variety of packet loss conditions.	forward error correction;multiple description coding;network packet;pesq;psychoacoustics;real-time cmix;smart data compression;speech coding;the australian;time-compressed speech	Xiguang Zheng;Christian Ritz	2014	2014 IEEE China Summit & International Conference on Signal and Information Processing (ChinaSIP)	10.1109/ChinaSIP.2014.6889201	voice activity detection;adaptive multi-rate audio codec;codec2;g.729;linear predictive coding;speech recognition;computer science;speech coding;speech processing;multimedia;communication	Mobile	48.15923577960273	-8.46074033708605	21121
24c660839b14d75caa4219456c20aaa85491ca30	smart 3d video coding	ssim mvvd 3dv ftv fvv h 264 mvc hevc;three dimensional television;3dv smart 3d video coding lsb least significant bit encode decode multiview texture associated supplementary data single bitstream transform block depth encoded bits psnr backward compatibility hd sequences free viewpoint video fvv 3dtv;video coding block codes three dimensional television transform coding;transform coding;video coding;merging encoding decoding psnr bit rate image coding video coding;block codes	In this paper we explore a method to encode/decode multiview textures and associated supplementary data (depth map as an example) together in a single bitstream. Least significant bit (LSB) of last two levels in each transform block has been modified according to the depth encoded bits, thus resulting in very minimal loss in PSNR of textures (0.1 - 0.5dB) with overall bitrate gain of about 13% for natural scenes. The remaining encoded information have been inserted after the slice NAL of corresponding texture ensuring backward compatibility and correct extraction and decoding by a modified decoder. Benchmarking results in terms of compression efficiency and quality on HD sequences have been reported to explore further the viability of using this technique for Free Viewpoint Video (FVV) or 3DV use cases. Experiments are also carried out to evaluate subjective quality of the synthesized views.	backward compatibility;bitstream;code;data compression;depth map;encode;least significant bit;most significant bit;network abstraction layer;peak signal-to-noise ratio;stereoscopic video coding;viewpoint	Srijib Narayan Maiti;Parth Desai;Bhargav Patel;Yuvraj Goel;Emiliano Mario Piccinelli;Davide Aliprandi;Pasqualina Fragneto;Beatrice Rossi	2012	2012 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)	10.1109/3DTV.2012.6365431	scalable video coding;sub-band coding;computer vision;telecommunications;computer science;coding tree unit;multimedia;multiview video coding	Vision	45.06274348737956	-18.747255305230222	21196
dbe057216bdeb187d0453833cdd0ed29acf5bd6b	toward the establishment of a family of distributions that may fit any dataset	62 07;kurtosis;full family of distributions;data modeling;62e10;skewness;mixture distribution;62f99	AbstractThis paper presents a review and comparison of the most important expanded families of distributions. We set the essential requirements by which an expanding family can fit any data set successfully. A new method is proposed to construct families, which fulfill these essential requirements. Consequently, two families are suggested, which are more tractable than many other known families and possess very wide range of the indices of skewness and kurtosis. The paper is motivated by two applications to real data set.		H. M. Barakat;O. M. Khaled	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2016.1197245	data modeling;econometrics;skewness;mixture distribution;kurtosis;data mining;mathematics;statistics	ML	27.961985004555345	-23.457579945160056	21255
fc550cb2022105d0c40a33af62e42bf188f58349	on the design of optimization algorithms for prediction of molecular interactions	dna;optimisation;optimization algorithm;evolutionary computation;high dimensionality;proteins bioinformatics evolutionary computation molecular biophysics optimisation;rank based adaptive mutation evolutionary algorithm;application software;protein ligand interaction;vector space;prediction algorithms;design optimization;energy function;protein ligand interactions;computational modeling;proteins;high dimensional vector space optimization algorithm molecular interactions protein ligand interactions bioinformatics rank based adaptive mutation evolutionary algorithm protein ligand docking genetic algorithm;molecular biophysics;protein folding;genetic algorithm;genetic algorithms;high dimensional vector space;optimization;genetic mutations;computational biology;evolutionary optimization;optimal algorithm;protein ligand docking;molecular interactions;computer simulation;algorithm design and analysis;gaussian distribution;benchmark testing;algorithm design and analysis design optimization prediction algorithms proteins computational modeling application software bioinformatics computer simulation genetic mutations genetic algorithms;bioinformatics	This article presents a comprehensive study on the main characteristics of a novel optimization algorithm specifically designed for simulation of protein-ligand interactions. Though design of optimization algorithms has been a research issue extensively studied by computer scientists for decades, the emerging applications in bioinformatics such as simulation of protein-ligand interactions and protein folding introduce additional challenges due to (1) the high dimensionality nature of the problem and (2) the highly rugged landscape of the energy function. As a result, optimization algorithms that are not carefully designed to tackle these two challenges may fail to deliver satisfactory performance. This study has been motivated by the observation that the RAME (Rank-based Adaptive Mutation Evolutionary) optimization algorithm specifically designed for simulation of protein-ligand docking has consistently outperformed the conventional optimization algorithms by a significant degree. Accordingly, it is of interest to conduct a comprehensive investigation on the characteristics of the proposed algorithm and to learn how it will perform in the more general cases.The experimental results reveal that the RAME algorithm proposed in this article is capable of delivering superior performance to several alternative versions of the genetic algorithm in handling highly-rugged functions in the high-dimensional vector space. This article also reports experiments conducted to analyze the causes of the observed performance difference. The experiences learned provide valuable clues for how the proposed algorithm can be effectively exploited to tackle other computational biology problems.	bioinformatics;computation;computational biology;computer scientist;docking (molecular);experiment;genetic algorithm;interaction;mathematical optimization;protein–ligand docking;rugged computer;simulation	Darby Tien-Hao Chang;Jung-Hsin Lin;Chih-Hung Hsieh;Yen-Jen Oyang	2009	2009 Ninth IEEE International Conference on Bioinformatics and BioEngineering	10.1109/BIBE.2009.57	computer simulation;biochemistry;meta-optimization;genetic algorithm;computer science;bioinformatics;theoretical computer science;machine learning;genetics;evolutionary computation;molecular biophysics	Visualization	29.49868346125641	-8.110512520411774	21298
93f9be5f70a7e279e67065bfa30fe1b0ae66fe8a	fractal image compression with region-based functionality	internet fractal image compression region based functionality mpeg 4 video compression standard object based queries still image database still image compression fractal image coding dct discrete cosine transform rectangular range blocks domain blocks irregular shapes merging process coded region segmentation map standard dissimilarity measure transform domain spatial domain psnr codebook size;evaluation performance;fractals;image coding;image segmentation;performance evaluation;image processing;data compression;imagen fija;dissimilarity measure;fractals image coding image segmentation shape measurement image databases testing mpeg 4 standard video compression merging measurement standards;evaluacion prestacion;image database;video compression;procesamiento imagen;transform coding;indexing terms;traitement image;codage image;functional imaging;compression image;fixed image;image compression;discrete cosine transforms;segmentation image;test methods;fractal;image fixe;image coding data compression discrete cosine transforms transform coding fractals image segmentation visual databases;compresion imagen;fractal image compression;visual databases	Region-based functionality offered by the MPEG-4 video compression standard is also appealing for still images, for example to permit object-based queries of a still-image database. A popular method for still-image compression is fractal coding. However, traditional fractal image coding uses rectangular range and domain blocks. Although new schemes have been proposed that merge small blocks into irregular shapes, the merging process does not, in general, produce semantically-meaningful regions. We propose a new approach to fractal image coding that permits region-based functionalities; images are coded region by region according to a previously-computed segmentation map. We use rectangular range and domain blocks, but divide boundary blocks into segments belonging to different regions. Since this prevents the use of standard dissimilarity measure, we propose a new measure adapted to segment shape. We propose two approaches: one in the spatial and one in the transform domain. While providing additional functionality, the proposed methods perform similarly to other tested methods in terms of PSNR but often result in images that are subjectively better. Due to the limited domain-block codebook size, the new methods are faster than other fractal coding methods tested. The results are very encouraging and show the potential of this approach for various internet and still-image database applications.	codebook;fractal compression;image compression;internet;license;object-based language;peak signal-to-noise ratio;segmentation action;video coding format	Kamel Belloulata;Janusz Konrad	2002	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/TIP.2002.999669	data compression;computer vision;fractal;image processing;computer science;theoretical computer science;mathematics;fractal compression;algorithm;statistics;computer graphics (images)	Vision	44.994447309019286	-14.417816174073764	21309
552b7861ce4dd3d05cfd9e88c49db54fc52991e0	robust scale estimation from ensemble inlier sets for random sample consensus methods	random sampling;auxiliary information	This paper proposes a RANSAC modification that performs automatic estimation of the scale of inlier noise. The scale estimation takes advantage of accumulated inlier sets from all proposed models. It is shown that the proposed method gives robust results in case of high outlier ratio data, in spite that no user specified threshold is needed. The method also improves sampling efficiency, without requiring any auxiliary information other than the data to be modeled.	random sample consensus	Lixin Fan;Timo Pylvänäinen	2008		10.1007/978-3-540-88690-7_14	sampling;econometrics;computer science;data mining;mathematics;statistics	Vision	28.70439009761879	-23.00785888665876	21331
1d4078426d046c073b16632dfde60d883dd48924	field-programmable gate array-based hardware architecture for image processing with complementary metal-oxide-semiconductor image sensor	field programmable gate array;complementary metal oxide semiconductor;control algorithm;image processing;hardware architecture;automatic exposure;cmos image sensor;noise reduction;color filter array;cmos sensors;algorithms;computer hardware;field programmable gate arrays;electro optic;image sensor;control method	Abstract. We present a ﬁeld-programmable gate array (FPGA)-based hardware architecture for image processing as well as novelalgorithms for fast autoexposure control and color ﬁlter array (CFA)demosaicing utilizing a CMOS image sensor (CIS). The proposedhardware architecture includes basic color processing functions ofblack-level correction, noise reduction, autoexposure control, auto-white-balance adjustment, CFAdemosaicing, and gamma correctionwhile applying advanced peripheral bus architecture to implementthe hardware architecture. The speed of traditional autoexposurecontrol algorithms to reach a proper exposure level is so slow that itis necessary to develop a fast autoexposure control method. Basedon the optical-electrical characteristics of the CIS, we present a fastauto-exposure-control algorithm that can guarantee speed and ac-curacy. To ensure the peak SNR performance of the demosaicedimages of the CIS and reduce the computational cost at the sametime, the proposed demosaicing algorithm improves on the adaptiveedge-sensitive algorithm and the fuzzy assignment algorithm. Theexperimental results show that the proposed hardware architectureworks well on the FPGA development board and produces betterquality images.	cmos;field-programmable gate array;image processing;image sensor;semiconductor	Zhiwei Ge;Suying Yao;Jiangtao Xu	2010	J. Electronic Imaging	10.1117/1.3483904	embedded system;computer vision;real-time computing;image processing;computer science;image sensor;hardware architecture;field-programmable gate array	EDA	43.09622631420069	-4.326439927455344	21348
53779ea9742009cd35e2295286f4c05d4318ffca	cooperative change detection for voltage quality monitoring in smart grids	detectors monitoring noise delays noise measurement meter reading;central meter cooperative change detection real time voltage quality monitoring smart grid system disturbance detection nominal sinusoidal voltage signal autoregressive model generalized local likelihood ratio detector test statistic lower detection delay data fusion data detection wireless channel level triggered sampling scheme multimeter scheme distributed meter readings;smart power grids autoregressive processes power system faults power system measurement power system reliability smart meters;level triggered sampling voltage signal disturbance autoregressive model change detection	This paper considers the real-time voltage quality monitoring in smart grid systems. The goal is to detect the occurrence of disturbances in the nominal sinusoidal voltage signal as quickly as possible such that protection measures can be taken in time. Based on an autoregressive model for the disturbance, we propose a generalized local likelihood ratio detector, which processes meter readings sequentially and alarms as soon as the test statistic exceeds a prescribed threshold. The proposed detector not only reacts to a wide range of disturbances, but also achieves lower detection delay compared with the conventional block processing method. Then, we further propose to deploy multiple meters to monitor the voltage signal cooperatively. The distributed meters communicate wirelessly to a central meter, where the data fusion and detection are performed. In light of the limited bandwidth of wireless channels, we develop a level-triggered sampling scheme, where each meter transmits only one-bit each time asynchronously. The proposed multi-meter scheme features substantially low communication overhead, while its performance is close to that of the ideal case where distributed meter readings are perfectly available at the central meter.	autoregressive model;centralized computing;cooperative mimo;electric power quality;interrupt;nominal type system;overhead (computing);real-time clock;sampling (signal processing);sensor;short-time fourier transform;simulation;waveform	Shang Li;Xiaodong Wang	2016	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2015.2477796	real-time computing	Embedded	43.85805976089259	-0.3364997721362464	21385
adef7a8e6ac12c8671816a7cec93e988f0e49c66	gaining certainty about uncertainty - testing cyber-physical systems in the presence of uncertainties at the application level			cyber-physical system	Martin A. Schneider;Marc-Florian Wendland;Leon Bornemann	2016		10.1007/978-3-319-57858-3_10	econometrics;reliability engineering;cyber-physical system;certainty;computer science	SE	28.523229651976198	-17.664506032311944	21393
03b61b4ea9fc21241197b055a3bc0ea4deff2671	integrated squared error estimation of normal mixtures	parseval theorem;validacion cruzada;estimacion densidad;fonction poids;melange loi probabilite;maximum likelihood;kernel density estimation;estimation densite;standard deviation;maximum vraisemblance;error sistematico;mixed distribution;estimateur noyau;qa mathematics inc computing science;curva gauss;simulated annealing;empirical characteristic function;parseval s theorem;theoreme parseval;smoothed cross validation selector;integral square error;fonction caracteristique;density estimation;recuit simule;normal mixtures;normal mixture;bias;kernel density estimate;validation croisee;funcion peso;characteristic function;loi normale;mezcla ley probabilidad;simulation study;recocido simulado;cross validation;weight function;estimation statistique;estimacion estadistica;statistical estimation;funcion caracteristica;kernel estimator;gaussian distribution;maxima verosimilitud;erreur systematique;integrated squared error	Based on the empirical characteristic function, the integrated squared error criterion for normal mixtures is shown to have a simple form for a particular weight function. When the parameter of that function is chosen as the smoothed cross-validation selector in kernel density estimation, the estimator which minimises the criterion is shown to perform well in a simulation study. In comparison with maximum likelihood and a new recently proposed method there are better bias and standard deviation results for the method of this paper. Furthermore, the new estimator is less likely to fail and is appreciably more robust.		Panagiotis Besbeas;Byron J. T. Morgan	2004	Computational Statistics & Data Analysis	10.1016/S0167-9473(02)00251-7	efficient estimator;minimax estimator;kernel density estimation;econometrics;estimator;bessel's correction;calculus;mathematics;mean squared error;bias of an estimator;parseval's theorem;mean integrated squared error;statistics	ML	32.169008208708085	-23.389295717333265	21417
f57f9290433110e2e2fd99a4625aff9e76c1798b	guest editors' introduction: stochastic modeling of complex systems	uncertainty quantification;dynamics stochastic random modeling;dynamic model;stochastic systems predictive models uncertainty biological system modeling stochastic resonance stochastic processes computational modeling power system modeling virtual manufacturing information analysis;ecology;environmental science computing;random;manufacturing design stochastic modelling complex systems stochastic spectral methods information theory sensitivity analysis uncertainty quantification ecology;manufacturing processes;stochastic;stochastic processes;complex system;dynamics;sensitivity analysis;stochastic processes design ecology environmental science computing information theory large scale systems manufacturing processes sensitivity analysis;spectral method;design;stochastic model;modeling;information theoretic;information theory;large scale systems	This special issue of CiSE highlights various aspects of and outstanding problems in stochastic modeling of complex systems. The techniques covered include stochastic spectral methods, the information-theoretic approach, and sensitivity analysis for uncertainty quantification. Two application articles-one in the field of ecology and the other in the field of manufacturing design-emphasize the diversity of research disciplines in which the ability to handle uncertainty is crucial to obtaining reliable science-based predictions	complex systems;ecology;information theory;spectral method;stochastic modelling (insurance);uncertainty quantification	Daniel M. Tartakovsky;Dongbin Xiu	2007	Computing in Science & Engineering	10.1109/MCSE.2007.32	econometrics;design;dynamics;uncertainty quantification;simulation;information theory;computer science;management science;stochastic;sensitivity analysis;statistics;spectral method	Robotics	34.03437685266447	-16.57427445548815	21471
0a38bd87ce26327b5e7bcf6aa66854860efd1a25	downscaled inverses for m-channel lapped transforms	image sampling;discrete wavelet transforms;decompression;image coding;filter bank;image resolution;data compression;block transforms;band pass filters;finite impulse response filter;space domain;m channel lapped transforms;lapped transform;transform coding;downscaled inverses;m channel fir filter banks;finite impulse response filter discrete transforms image resolution filter bank image coding discrete wavelet transforms signal processing mimo image reconstruction pixel;general solution;forward transform;image sampling image coding image resolution transform coding data compression band pass filters filtering theory inverse problems discrete cosine transforms;image compression;discrete transforms;discrete cosine transforms;image reconstruction;signal processing;fir filter;pixel;fast inverses;algorithms;image recovery;resampling;mimo;algorithms downscaled inverses m channel lapped transforms image compression image resolution decompression rescaling space domain reduced inverse transform image recovery fast inverses forward transform m channel fir filter banks block transforms dct resampling;reduced inverse transform;filtering theory;dct;inverse problems;rescaling	Compressed images may be decompressed for devices using di erent resolutions. Full decompression and rescaling in space domain is a very expensive method. We studied downscaled inverses where the image is decompressed partially and a reduced inverse transform is used to recover the image. We studied the design of fast inverses, for a given forward transform. General solutions are presented for M -channel FIR lter banks of which block and lapped transforms are a subset.	data compression;finite impulse response;lapped transform	Ricardo L. de Queiroz;Reiner Eschbach	1997		10.1109/ICASSP.1997.599576	computer vision;mathematical optimization;computer science;finite impulse response;signal processing;mathematics;statistics	Vision	46.202415585661	-11.005097809037386	21506
33a1356dbb200dd39e7674490902735e8ec35145	option-based monte carlo algorithm with conditioned updating to learn conflict-free task allocation in transport applications	reinforcement learning		monte carlo algorithm;monte carlo method	Alex Valdivielso;Toshiyuki Miyamoto	2011	IEICE Transactions		real-time computing;simulation;computer science;machine learning;reinforcement learning	ML	38.510052594656	4.0246936929426935	21523
750336cac636f4339587bcc25152279331391330	unbiased monte carlo for optimization and functions of expectations via multi-level randomization		We present general principles for the design and analysis of unbiased Monte Carlo estimators for quantities such as α = g(E (X)), where E (X) denotes the expectation of a (possibly multidimensional) random variable X, and g(·) is a given deterministic function. Our estimators possess finite work-normalized variance under mild regularity conditions such as local twice differentiability of g(·) and suitable growth and finite-moment assumptions. We apply our estimator to various settings of interest, such as optimal value estimation in the context of Sample Average Approximations, and unbiased steady-state simulation of regenerative processes. Other applications include unbiased estimators for particle filters and conditional expectations.	approximation;karush–kuhn–tucker conditions;mathematical optimization;monte carlo method;optimization problem;particle filter;simulation;steady state	Jose H. Blanchet;Peter W. Glynn	2015	2015 Winter Simulation Conference (WSC)		u-statistic;quantum monte carlo;random variable;econometrics;mathematical optimization;estimation;dynamic monte carlo method;convergence;hybrid monte carlo;monte carlo molecular modeling;mathematics;social simulation;monte carlo integration;xenon;statistics;monte carlo method;control variates	ML	32.549995803487974	-16.23640195680275	21541
896c80eeb2d749834fa9e18b14ec97b597b73a73	simple, high performance lossless image compression	detectors;image coding;data compression;edge detection;lossless image compression;integer arithmetic;testing;invertible transformation;median edge detector predictor;compressors;image edge detection;edge detection data compression image coding;general purpose file compressor;jpeg ls;arithmetic;median edge detector predictor lossless image compression jpeg ls general purpose file compressor invertible transformation integer arithmetic;performance loss image coding testing compressors arithmetic entropy encoding image edge detection detectors;entropy;encoding;high performance;performance loss	We present a two-part method for lossless image compression: the first is a simple transformation based on JPEG-LS and the second is the use of a general purpose file compressor such as gzip, bzip2 or rk. The transformation is invertible and uses only integer arithmetic. It achieves no compression directly; however, on some images, it can significantly reduce the entropy. The gzip, bzip2, and rk compressors, with and without the transformation, are tested against JPEG-LS on a standard data set. Overall rk (both with and without the transformation) is best. Furthermore, by combining the various algorithms, we obtain compression that is 15% better than JPEG-LS alone.	image compression;lossless compression	Charles G. Boncelet	2001		10.1109/ICIP.2001.958160	data compression;lossy compression;computer vision;entropy;data compression ratio;detector;discrete mathematics;edge detection;gas compressor;image compression;computer science;theoretical computer science;mathematics;lossless compression;software testing;encoding;statistics	HPC	43.07363014254306	-16.95075035951407	21622
f5e517f46a20e27b2e6533bae084af7e487a2fca	compression of computer generated hologram based on phase-shifting algorithm	image coding;hologram reconstruction computer generated holography phase shifting digital holography image compression;computer generated holography;transform coding image coding image reconstruction holography computers laser beams interference;optical testing;psdh computer generated hologram phase shifting algorithm computational holographic imaging interference pattern ray tracing method optical propagation phase shifting digital holography coding techniques standard baseline joint photographic experts group standard jpeg 2000 image reconstruction fresnel approximation compressed cgh 3d virtual objects jpeg 2000 compression method;image reconstruction;ray tracing computer generated holography image coding image reconstruction light interference light propagation optical testing;ray tracing;light propagation;light interference	In this paper, we propose an algorithm that combines computational holographic imaging and its reconstruction with a compression method to reduce the storage burden of digital holograms. The interference patterns are generated by using ray tracing method which can simulate the optical propagation of virtual objects based on phase-shifting digital holography (PSDH). Coding techniques, e.g. Standard baseline Joint Photographic Experts Group (JPEG) and standard JPEG 2000, are applied to compress the computer generated holograms (CGHs). The image of virtual objects is reconstructed by Fresnel approximation with compressed CGHs. In simulations, we use simply defined 3-Dimensional (3D) virtual objects to test the efficiency of these two coding techniques. We show that acceptable reconstruction quality is obtained by using JPEG and JPEG 2000 compression methods. Nevertheless, more efficient compression methods applied on CGHs for complicated virtual objects can be expected in the future.	algorithm;approximation;baseline (configuration management);data compression;digital holography;interference (communication);jpeg 2000;moving picture experts group;ray tracing (graphics);simulation;software propagation	Yafei Xing;Béatrice Pesquet-Popescu;Frédéric Dufaux	2013	European Workshop on Visual Information Processing (EUVIP)		computer vision;digital holographic microscopy;computer science;optics;computer graphics (images)	Graphics	41.626543074514274	-19.16864635106411	21630
453fbab72686e371f957857a09e24fbdc95b5d44	research on intelligent welding robot path optimization based on ga and pso algorithms		To make the welding robot more reasonable and furthermore improve the productivity and reduce costs, two intelligent algorithms for welding path optimization, genetic algorithm (GA) and discrete particle swarm optimization, are proposed to optimize the welding robot path. Through the improved selection of the operator, the GA achieves the fastest iterative efficiency. By introducing the “swap operator” and “swap sequence” in the particle swarm optimization algorithm, the PSO algorithm is improved for the solution of the discrete problem (welding robot path planning) which is superior to the continuous optimization problem. Besides, for the better iterative efficiency of PSO, the parameters of traditional inertia weight are determined by a linear inertia weigh, which can improve the convergence performance of the algorithm. The modeling and solutions of the two algorithms are discussed in detail to illustrate the applications in the welding robot path optimization. In order to compare the pros and cons of the two algorithms, the same welding tasks are presented, and Matlab simulation is carried out. The simulation results show that both genetic algorithm and particle swarm optimization algorithm can obtain the optimal or near-optimal welding path by iterative calculations.		Tong Yifei;Zhong Meng;Li Jingwei;Li Dongbo;Wang Yulin	2018	IEEE Access	10.1109/ACCESS.2018.2878615	continuous optimization;operator (computer programming);genetic algorithm;welding;motion planning;robot welding;computer science;algorithm;particle swarm optimization;convergence (routing)	Robotics	30.42664147419013	-2.9333511308841325	21635
398da73b61e1cb1055aed1a60b0e08497bbaf043	periodical switching between related goals for improving evolvability to a fixed goal in multi-objective problems	engineering design;evolutionary computation;varying environments;intelligent design;premature convergence;adaptive filtering;multi objective optimization;evolutionary programming;robustness;modularity;evolutionary computation multi objective optimization varying environments;evolvability;environmental change;adaptive filter;evolutionary computing	Evolutionary computation plays a principal role in intelligent design automation. Evolutionary approaches have discovered novel and patentable designs. Nonetheless, evolutionary techniques may lead to designs that lack robustness. This critical issue is strongly connected to the concept of evolvability. In nature, highly evolvable species tend to be found in rapidly changing environments. Such species can be considered robust against environmental changes. Consequently, to create robust engineering designs it could be beneficial to use variable, rather than fixed, fitness criteria. In this paper, we study the performance of an evolutionary programming algorithm with periodical switching between goals, which are selected randomly from a set of related goals. It is shown by a dual-objective filter optimization example that altering goals may improve evolvability to a fixed goal by enhancing the dynamics of solution population, and guiding the search to areas where improved solutions are likely to be found. Our reference algorithm with a single goal is able to find solutions with competitive fitness, but these solutions are results of premature convergence, because they are poorly evolvable. By using the same algorithm with switching goals, we can extend the productive search length considerably; both the fitness and robustness of such designs are improved.		Seppo J. Ovaska;Bernhard Sick;Alden H. Wright	2009	Inf. Sci.	10.1016/j.ins.2009.08.019	evolutionary programming;adaptive filter;mathematical optimization;simulation;interactive evolutionary computation;human-based evolutionary computation;computer science;artificial intelligence;machine learning;evolutionary algorithm;evolutionary computation	AI	25.528811106547785	-8.344866731499259	21650
d2d43da44c09b1655f46333341d3d3196f024ca7	tow stage design for estimating the reliability of series/parallel systems	reliability;62l10;62l12;two stage design;62d05;62n05;sequential sampling	This work aims to determine in a practical and straightforward manner an efficient sampling scheme for estimating the reliability of series/parallel systems. One of the most crucial tasks is to determine the optimal number of units to sample. The sampling schemes given by the authors are complex and costly. We give a reliability sequential scheme which performs much better than the balanced allocation. For a large total number of units, the first order asymptotic optimality of such a scheme is validated by Monte Carlo simulations. © 2011 IMACS. Published by Elsevier B.V. All rights reserved. MSC: 62D05; 62L10; 62L12; 62N05	asymptotically optimal algorithm;monte carlo method;sampling (signal processing);simulation	Zohra Benkamra;Mekki Terbeche;Mounir Tlemcani	2011	Mathematics and Computers in Simulation	10.1016/j.matcom.2010.12.033	econometrics;mathematical optimization;sequential analysis;reliability;mathematics;statistics	AI	29.048447299553718	-18.656377715514356	21663
06a0485604de184ee51031dec7f0d1756046b193	on a general sensor fusion method by state space modeling approach using particle filters	microphones;state space methods;state estimation;sensor fusion state space methods particle filters state estimation equations proposals target tracking acoustic sensors cameras microphones;general methods;particle filter;acoustic sensors;sensor fusion;particle filters;target tracking;state space model;proposals;cameras	Sensor fusion aims at obtaining information, which cannot be obtained by single sensor, by combining signals from multiple sensors. The main problem of sensor fusion is that computational cost increases exponentially with the number of sensors because the combination number of association between signals and sensors is large. We propose a general method to solve this problem in nonlinear state space model to deal with the unknown associations. We adapt the model to a specific situation of a sensor fusion. The target states and the associations are simultaneously estimated through the state estimation. In the estimation of the association, we apply particle filters with clever proposal. The associations are estimated in probabilistic way, not deterministic way, to avoid falling into a bad solution. As an example of the sensor fusion, we deal with tracking problem of sound target using camera and two microphones. Keyword: Sensor fusion, unknown association, particle filters, clever proposal, target tracking.	computational complexity theory;image sensor;microphone;nonlinear system;particle filter;sensor web;state space;state-space representation	Masato Kawanishi;Norikazu Ikoma;Hiroshi Maeda	2005	Proceedings of the Eighth International Symposium on Signal Processing and Its Applications, 2005.	10.1109/ISSPA.2005.1581000	computer vision;particle filter;soft sensor;computer science;machine learning;control theory;statistics	Robotics	53.43007723786576	3.953172744255251	21671
e20a04551fd6ba0c83a6eae090552246041ae95a	effective multi-caste ant colony system for large dynamic traveling salesperson problems		Multi-caste ant algorithms allow the coexistence of different search strategies, thereby enhancing search effectiveness in dynamic optimization situation. We present two new variants for a multi-caste ant colony system that promote a better migration of ants between alternative behaviors. Results obtained with large and highly dynamic traveling salesperson instances confirm the effectiveness and robustness of the approach. A detailed analysis reveals that one of the castes should adopt a clearly exploratory behavior, as this minimizes the recovery time after an environmental change.		Leonor Albuquerque Melo;Francisco Baptista Pereira;Ernesto Costa	2013		10.1007/978-3-319-11683-9_6	robustness (computer science);ant colony;machine learning;ant colony optimization algorithms;caste;ant;artificial intelligence;computer science	AI	25.735843346929297	-4.178610913968691	21715
931d904bf1097b71bb43fac4ca9a6c10e05467c0	gpu-optimized hybrid neighbor/cell list algorithm for coarse-grained md simulations of protein and rna folding and assembly	performance analyses;cell list;verlet neighbor list	Molecular dynamics (MD) simulations provide a molecular-resolution view of biomolecular folding and assembly processes, but the computational demands of the underlying algorithms limit the lenth- and time-scales of the simulations one can perform. Recently, graphics processing units (GPUs), specialized devices that were originally designed for rendering images, have been repurposed for high performance computing, and there have been significant increases in the performances of parallel algorithms such as the ones in MD simulations. Previously, we implemented a GPU-optimized parallel neighbor list algorithm for our coarsegrained MD simulations, and we observed an N-dependent speed-up (or speed-down) compared to a CPU-optimized algorithm, where N is the number of interacting beads representing amino acids or nucleotides for proteins or RNAs, respectively. We had demonstrated that for MD simulations of the 70s ribosome (N=10,219), our GPU-optimized code was about 30x as fast as a CPU-optimized version. In our present study, we implement a hybrid neighbor/cell list algorithm that borrows components from the well-known neighbor list and the cell-list algorithms. We observe about 10% speedup as compared to our previous implementation of a GPU-optimized parallel neighbor list algorithm.	cell (microprocessor);central processing unit;computation;computer graphics;graphics processing unit;interaction;molecular dynamics;parallel algorithm;performance;simulation;speedup;supercomputer;verlet list;whole earth 'lectronic link	Andrew J. Proctor;Cody A. Stevens;Samuel S. Cho	2013		10.1145/2506583.2506649	parallel computing;computer science;bioinformatics;theoretical computer science;cell lists	HPC	42.82128218344009	1.5511987952004518	21741
57aaf34eef6a32c7068650d5c82d1fc780ce0599	an analysis of particle properties on a multi-swarm pso for dynamic optimization problems	particle swarm optimizer;dynamic optimization problem	The particle swarm optimization (PSO) algorithm has successfully been applied to dynamic optimization problems with very competitive results. One of its best performing variants is the one based on the atomic model, with quantum and trajectory particles. However, there is no precise knowledge on how these particles contribute to the global behavior of the swarms during the optimization process. This work analyzes several aspects of each type of particle, including the best combination of them for different scenarios, and how many times do they contribute to the swarm's best. Results show that, for the Moving Peaks Benchmark (MPB), a higher number of trajectory particles than quantum particles is the best strategy. Quantum particles are most helpful immediately after a change in the environment has occurred, while trajectory particles lead the optimization in the final stages. Suggestions on how to use this knowledge for future developments are also provided.	particle swarm optimization	Ignacio José García del Amo;David A. Pelta;Juan Ramón González;Pavel Novoa-Hernández	2009		10.1007/978-3-642-14264-2_4	mathematical optimization;multi-swarm optimization;simulation;artificial intelligence;particle swarm optimization;physics;metaheuristic	Theory	27.686662425402282	-3.6402497639779137	21826
bd1599d6068427ad36cbd06f1b3f1e75ce12cd62	the moea/d algorithm with gaussian neighbourhoods for the multiobjective travelling salesman problem		In this paper the MOEA/D-G algorithm is proposed which is a modification of the MOEA/D algorithm using Gaussian distributions to determine the probability with which neighbours of a given subproblem are selected as parents of new specimens assigned to this subproblem. The proposed method is applied to the Multiobjective Travelling Salesman Problem (MOTSP). Solutions found by the MOEA/D-G algorithm have a better quality than those found by the original MOEA/D version. Also, given equal computation time, all versions of the MOEA/D outperform the NSGA-II algorithm.	algorithm;computation;moea framework;time complexity;travelling salesman problem	Krzysztof Michalak	2017		10.1145/3067695.3076102	machine learning;artificial intelligence;mathematical optimization;computation;travelling salesman problem;divide and conquer algorithms;gaussian;algorithm;mathematics	AI	24.740011326560854	-0.2761410091928984	21922
73042972128fa2f98710399fccacfd5d319e590e	fuzzy recombination for the breeder genetic algorithm	breeder genetic algorithm	A new recombination operator called fuzzy recombination FR is introduced for contin uous genes The performance of the operator is analyzed by means of the equation describ ing the response to selection The operator is evaluated according to a new design cri terion maximizing the product of heritabil ity and standard deviation The breeder ge netic algorithm BGA with FR converges lin early for a test suite of benchmark functions The computational complexity is also com puted We believe that linear convergence is the optimum to be achieved by random search methods The question remains open Can a random search method be found which gives the best linear convergence i e the smallest constant for a well de ned class of functions	ball grid array;benchmark (computing);computational complexity theory;genetic algorithm;local interconnect network;random search;rate of convergence;test suite	Hans-Michael Voigt;Heinz Mühlenbein;Dragan Cvetkovic	1995			fuzzy logic;genetic algorithm;breeder (animal);artificial intelligence;recombination;pattern recognition;computer science	ML	26.172971178672682	-6.466674685933787	21964
d771233dc61799d9a60045f2b80d677e8c5302c1	mkvpci: a computer program for markov models with piecewise constant intensities and covariates	piecewise constant intensities;computer program;time dependent;informatica biomedical;biomedical data processing;modelo markov;multi state models;coeficiente regresion;maximum likelihood;tabaquismo;school children;maximum vraisemblance;informatique biomedicale;ajustement;hombre;tobacco smoking;standard error;enfant;fitting;markov model;maximum likelihood estimate;dependance du temps;time dependence;nino;human;markov process;covariate;estimacion parametro;time dependent covariates;child;time dependent covariate;covariable;markov processes;parameter estimation;estimation parametre;modele markov;tabagisme;ajuste;regression coefficient;dependencia del tiempo;maxima verosimilitud;coefficient regression;homme	We present a computer program for fitting Markov models with piecewise constant intensities and for estimating the effect of covariates on transition intensities. The basic idea of the proposed approach is to introduce artificial time-dependent covariates in the data to represent the time dependence of the transition intensities, and to use a modified time-homogeneous Markov model to estimate the baseline transition intensities and the regression coefficients. The program provides the maximum likelihood estimates of the parameters together with their estimated standard errors, and allows testing various statistical hypotheses. To illustrate the use of the program, we present a three-state model for analyzing the smoking habits of school children.	baseline (configuration management);behaviorial habits;coefficient;computer program;estimated;markov chain;markov model;smoke;three-state logic	Ahmadou Alioum;Daniel Commenges	2001	Computer methods and programs in biomedicine	10.1016/S0169-2607(00)00094-8	econometrics;mathematical optimization;mathematics;maximum likelihood;markov process;statistics	ML	35.460268595948875	-23.095682634346524	22116
dd044fe8065a98ca2e848d0864927da38fbc12b8	a new differential evolution for constrained optimization problems	differential evolution;convergence;evolutionary computation;nonlinear programming;nonlinear programming computational complexity convergence evolutionary computation functions;uniform design;objective function;nondifferentiable objective functions;augmented lagrangian function;computational complexity;constraint optimization evolutionary computation robustness design optimization stochastic processes cost function mathematics convergence lagrangian functions functional programming;augmented lagrangian function differential evolution nondifferentiable objective functions nonlinear objective functions multimodal objective functions evolutionary algorithm nonlinear constrained optimization;nonlinear objective functions;evolutionary algorithm;constrained optimization problem;augmented lagrangian;functions;nonlinear constrained optimization;multimodal objective functions	Differential evolution (DE) is a novel evolutionary approach capable of handling non-differentiable, nonlinear and multi-modal objective functions. Previous studies have shown that DE is an efficient, effective and robust evolutionary algorithm, but usually it takes large computational time for optimizing the computationally expensive objective function, therefore it is necessary to find a trade-off between convergence speed and robustness. For this purpose, in this paper, a new DE based on uniform design is presented for solving nonlinear constrained optimization problems. Constraints are handled by embodying them in an augmented Lagrangian function, where the penalty parameters and multipliers are adapted as the execution of the algorithm proceeds. The efficiency of the proposed methodology is illustrated by solving numerous constrained optimization problems that can be found in the literature	analysis of algorithms;augmented lagrangian method;computation;constrained optimization;differential evolution;iterative and incremental development;mathematical optimization;modal logic;multi-objective optimization;nonlinear system;optimization problem;simulation;time complexity	Jihui Zhang;Jun-Qin Xu;Qiyuan Zhou	2006	Sixth International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2006.253751	differential evolution;mathematical optimization;constrained optimization;mathematical analysis;augmented lagrangian method;convergence;nonlinear programming;computer science;machine learning;evolutionary algorithm;penalty method;mathematics;computational complexity theory;function;evolutionary computation	EDA	29.564559858409037	-0.736573632165253	22159
e623ff97323e0bcf7cdb2f705f35bd19b87bb7d5	forecasting-based state estimation for three-phase distribution systems with limited sensing		State Estimation is an essential technique to provide observability in power systems. Traditionally developed for high-voltage transmission networks, state estimation requires equipping networks with many real-time sensors, which remains a challenge at the scale of distribution networks. This paper proposes a method to complement a limited set of real-time measurements with voltage predictions from forecast models. The method differs from the classical weighted least-squares approach, and instead relies on Bayesian estimation formulated as a linear least squares estimation problem. We integrate recently developed linear models for unbalanced 3-phase power flow to construct voltage predictions as a linear mapping of load predictions. The estimation step is a linear computation allowing high resolution state estimate updates, for instance by exploiting a small set of phasor measurement units. Uncertainties can be determined a priori and smoothed a posteriori, making the method useful for both planning, operation and post hoc analysis. The method is applied to an IEEE benchmark and on a real network testbed at the Dutch utility Alliander.	automated planning and scheduling;benchmark (computing);computation;estimation theory;hoc (programming language);ibm power systems;linear least squares (mathematics);linear model;phasor;real-time clock;sensor;smoothing;testbed;unbalanced circuit	Roel Dobbe;Werner van Westering;Stephan Liu;Daniel Arnold;Duncan Callaway;Claire J. Tomlin	2018	CoRR		mathematical optimization;electric power system;linear model;observability;post-hoc analysis;mathematics;three-phase;linear least squares;bayes estimator;phasor	Robotics	35.67625029918987	-2.1884942829486596	22179
d18765cdc396e3c06606bc6715ba82f27732a9bc	variable-size vector entropy coding of speech and audio	entropy coding;speech coding;transform coding;audio coding;transform coding entropy codes speech coding audio coding discrete cosine transforms;discrete cosine transforms;entropy codes;entropy coding speech coding image coding code standards laboratories video compression source coding streaming media huffman coding arithmetic;modified discrete cosine transform;information rates variable size vector entropy coding speech coding audio coding analog media coders complexity constrained method vector size modified discrete cosine transform mdct coding rate reduction image coder video coder huffman coder fixed radix	Many modern analog media coders employ some form of entropy coding (EC). Usually, a simple per-letter EC is used to keep the coder's complexity and price low. In some coders, individual symbols are grouped into small fixed-size vectors before EC is applied. We extend this approach to form variable-size vector EC (VSVEC) in which vector sizes may be from 1 to several hundreds. The method is, however, complexity-constrained in the sense that the vector size is always as large as allowed by a pre-set complexity limit. The idea is studied in the framework of a modified discrete cosine transform (MDCT) coder. It is shown experimentally, using diverse audio material, that a rate reduction of about 37% can be achieved. The method is, however, not specific to MDCT coding but can be incorporated in various speech, audio, image and video coders.	entropy encoding	Yair Shoham	2001		10.1109/ICASSP.2001.941028	data compression;arithmetic coding;sub-band coding;transform coding;speech recognition;shannon–fano coding;modified discrete cosine transform;vector sum excited linear prediction;harmonic vector excitation coding;variable-length code;computer science;entropy encoding;theoretical computer science;context-adaptive variable-length coding;speech coding;coding tree unit;modified huffman coding;mathematics;tunstall coding;context-adaptive binary arithmetic coding;h.261;statistics;huffman coding;code-excited linear prediction;range encoding	ML	47.49771858726376	-10.550262085219437	22188
17c2f0eb01a0387f390a3debb53fc7e8f436c118	nonparametric steganalysis of qim steganography using approximate entropy	quantization index modulation;qim steganography;quantization;steganography image coding probability;image coding;complexity theory;probability;steganalysis;dither modulation;stego image;nonparametric steganalysis method;message recovery;complexity;discrete cosine transform;algorithmic entropy;cover image;steganography;secret message length;approximate entropy;first order;estimation;discrete cosine transforms;quantization index modulation based steganography;embedding rate;decoding error probability;error probability;entropy;algorithmic entropy measure;quantization entropy complexity theory discrete cosine transforms estimation modulation noise;steganography algorithmic entropy approximate entropy complexity dither modulation embedding rate entropy message recovery quantization index modulation steganalysis;decoding error probability nonparametric steganalysis method qim steganography approximate entropy quantization index modulation based steganography cover image stego image algorithmic entropy measure secret message length first order statistics;first order statistics;noise;modulation	This paper proposes an active steganalysis method for quantization index modulation (QIM)-based steganography. The proposed nonparametric steganalysis method uses irregularity (or randomness) in the test image to distinguish between the cover image and the stego image. We have shown that plain quantization (quantization without message embedding) induces regularity in the resulting quantized object, whereas message embedding using QIM increases irregularity in the resulting QIM-stego. Approximate entropy, an algorithmic entropy measure, is used to quantify irregularity in the test image. The QIM-stego image is then analyzed to estimate secret message length. To this end, the QIM codebook is estimated from the QIM-stego image using first-order statistics of the image coefficients in the embedding domain. The estimated codebook is then used to estimate secret message. Simulation results show that the proposed scheme can successfully estimate the hidden message from the QIM-stego with very low decoding error probability. For a given cover object the decoding error probability depends on embedding rate and decreases monotonically, approaching zero as the embedding rate approaches one.	approximate entropy;codebook;coefficient;embedded system;first-order predicate;kolmogorov complexity;modulation;quantization (signal processing);randomness;sensor;simulation;standard test image;steganalysis;steganography	Hafiz Malik;K. P. Subbalakshmi;Ramamurti Chandramouli	2012	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2011.2169058	approximate entropy;entropy;estimation;discrete mathematics;complexity;steganalysis;quantization;computer science;noise;theoretical computer science;probability of error;discrete cosine transform;pattern recognition;probability;first-order logic;mathematics;steganography;statistics;modulation	Vision	42.11165302754715	-9.640116562290087	22215
b673c0c92cf388861b0ee21fbea1dab464b0ea0f	polynomial-based secret sharing scheme based on the absolute moment block truncation coding technique	secret sharing;polynomials block codes data compression image coding;high payload;exclusive or;voting;absolute moment block truncation coding;high payload absolute moment block truncation coding secret sharing exclusive or voting;cryptography polynomials image coding image reconstruction encoding internet decoding;ambtc compressed image polynomial based secret sharing scheme absolute moment block truncation coding technique partial secret information ambtc compression technique exclusive or operations	Secret sharing is a technique that can distribute partial secret information (also called 'shares') to a specific group member. These individual shares are of no use on their own, but they can reconstruct the original secret information when the members collect all of the shares. In this paper, we proposed a novel, polynomial-based, secret sharing scheme using the absolute moment block truncation coding technique block truncation coding (AMBTC) compression technique. By using simple, exclusive-OR operations, the group members can easily recover the secret image with 100% correctness. Experimental results demonstrated that the AMBTC-compressed image in our scheme had the same quality as the original AMBTC scheme, but the compression ratio in our scheme was better than that of the original AMBTC scheme.	bitmap;block truncation coding;correctness (computer science);exclusive or;payload (computing);polynomial;shamir's secret sharing	Chin-Chen Chang;Chin-Yu Sun	2014	2014 Tenth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2014.127	exclusive or;discrete mathematics;block truncation coding;voting;computer science;theoretical computer science;shamir's secret sharing;mathematics;homomorphic secret sharing;secret sharing;computer security;algorithm	EDA	38.95885952868527	-10.688839822487154	22291
8ff61bf63daa35d2f2d5e23be5ed6fa5df94fefc	kriging metamodel with modified nugget-effect: the heteroscedastic variance case	heterogeneous variance;nugget effect;simulation;metamodel;kriging	Metamodels are commonly used to approximate and analyze simulation models. However, in cases where the simulation output variances are non-zero and not constant, many of the current metamodels which assume homogeneity, fail to provide satisfactory estimation. In this paper, we present a kriging model with modified nugget-effect adapted for simulations with heterogeneous variances. The new model improves the estimations of the sensitivity parameters by explicitly accounting for location dependent non-constant variances and smoothes the kriging predictor's output accordingly. We look into the effects of stochastic noise on the parameter estimation for the classic kriging model that assumes deterministic outputs and note that the stochastic noise increases the variability of the classic parameter estimation. The nugget-effect and proposed modified nugget-effect stabilize the estimated parameters and decrease the erratic behavior of the predictor by penalizing the likelihood function affected by stochastic noise. Several numerical examples suggest that the kriging model with modified nugget-effect outperforms the kriging model with nugget-effect and the classic kriging model in heteroscedastic cases.	kriging;metamodeling	Jiateng Yin;S. H. Ng;Kien Ming Ng	2011	Computers & Industrial Engineering	10.1016/j.cie.2011.05.008	metamodeling;econometrics;mathematical optimization;simulation;computer science;variogram;mathematics;kriging;statistics	SE	28.79457257044127	-21.884886508791528	22294
7669f726c7163c53001d393c3e45fefac2f8e0ee	antioptimisation of trusses using two-level population-based incremental learning		Practical optimum design of structures often involves parameters with uncertainties. There have been several ways to deal with such optimisation problems, and one of the approaches is an antioptimisation process. The task is to find the optimum solution of common design variables while simultaneously searching for the worst case scenario of those parameters with uncertainties. This paper proposed a metaheuristic based on population-based incremental learning (PBIL) for solving antioptimisation of trusses. The new algorithm is called two-level PBIL which consists of outer and inner loops. Five antioptimisation problems are posed to test the optimiser performance. The numerical results show that the concept of using PBIL probability vectors for handling the antioptimisation of truss is powerful and effective.The two-level PBIL can be considered a powerful optimiser for antioptimisation of trusses.		Phinit Tontragunrat;Sujin Bureerat	2013	J. Applied Mathematics	10.1155/2013/434636	mathematical optimization;machine learning	Robotics	28.998125793897152	-1.357897251665642	22295
0130f549a686718099a9fc82603b516cf7bb62b4	neural network based mos transistor geometry decision for tsmc 0.18µ process technology	modelizacion;caracteristique courant tension;mos technology;integrated circuit;circuit mos;operating conditions;circuit design;terminal;caracteristica corriente tension;circuito integrado;circuito mos;non linear model;modele non lineaire;modelisation;tension electrique;structure reseau;modelo no lineal;condition operatoire;process parameters;parametre critique;design environment;voltage;parametro critico;nonlinear equation;tecnologia mos;voltage current curve;mos transistor;information system;network structure;neural network model;mos circuit;reseau neuronal;voltaje;condicion operatoria;modeling;red neuronal;systeme information;transistor mos;circuit integre;critical parameter;technologie mos;neural network;sistema informacion	In sub-micron technologies MOSFETs are modeled by complex nonlinear equations. These equations include many process parameters, terminal voltages of the transistor and also the transistor geometries; channel width (W) and length (L) parameters. The designers have to choose the most suitable transistor geometries considering the critical parameters, which determine the DC and AC characteristics of the circuit. Due to the difficulty of solving these complex nonlinear equations, the choice of appropriate geometry parameters depends on designer’s knowledge and experience. This work aims to develop a neural network based MOSFET model to find the most suitable channel parameters for TSMC 0.18μ technology, chosen by the circuit designer. The proposed model is able to find the channel parameters using the input information, which are terminal voltages and the drain current. The training data are obtained by various simulations in the HSPICE design environment with TSMC 0.18μm process nominal parameters. The neural network structure is developed and trained in the MATLAB 6.0 program. To observe the utility of proposed MOSFET neural network model it is tested through two basic integrated circuit blocks.	analogue electronics;approximation;artificial neural network;digital electronics;experience;integrated circuit design;matlab;network model;nonlinear system;spice 2;simulation;transistor;very-large-scale integration	Mutlu Avci;Tülay Yildirim	2006		10.1007/11758549_84	voltage;computer science;electrical engineering;artificial intelligence;artificial neural network	EDA	38.26996728153175	-3.4818380426352915	22474
8635e8da6aaefdf5e2d23be4bec13ad4db44902a	developing analytic models based on simulation results	analytic model building;simulation model;analytic model;simulation result;model relationship;discrete event simulation;system analysis and design;random variables;steady state;simultaneous localization and mapping;gpss	This paper suggests that a simulation model provides results that lead to the development of analytic models. The analytic models can be used to uncover model relationships. Illustrations are given of simulation models that provide direction for analytic model building.	simulation	A. A. B. Prisker	1989		10.1109/WSC.1989.718739	random variable;mathematical optimization;verification and validation of computer simulation models;simulation;computer science;engineering;technical report;gpss;discrete event simulation;simulation modeling;steady state;structured systems analysis and design method;simultaneous localization and mapping	EDA	34.620973179754465	-16.533363188126792	22477
54a3d655bc699b6f07ea0ebe17b8435d8ed6a3d3	quantile sensitivity estimation for dependent sequences	sensitivity analysis;ϕ mixing;quantile;monte carlo simulation;regenerative process	In this paper we estimate quantile sensitivities for dependent sequences via infinitesimal perturbation analysis, and prove asymptotic unbiasedness, weak consistency, and a central limit theorem for the estimators under some mild conditions. Two common cases, the regenerative setting and φ-mixing, are analyzed further, and a new batched estimator is constructed based on regenerative cycles for regenerative processes. Two numerical examples, the G/G/1 queue and the Ornstein–Uhlenbeck process, are given to show the effectiveness of the estimator.	numerical analysis;perturbation theory;weak consistency	Guangxin Jiang;Michael C. Fu	2016	J. Applied Probability	10.1017/jpr.2016.36	econometrics;mathematical optimization;quantile;mathematics;sensitivity analysis;statistics;monte carlo method	Metrics	31.00270733201942	-20.939667670423056	22512
ba0e05658ef0ec1f28678b90c4a03bd13ba3cb39	brain slicer: a high-performance internet-based neuromedical imaging system	algoritmo paralelo;codage lineaire;code lineaire;medical imagery;image numerique;brain;octree;haute performance;high resolution;parallel algorithm;quad tree;systeme nerveux central;modelo 3 dimensiones;octarbol;image resolution;quad arbol;modele 3 dimensions;octarbre;three dimensional model;estructura archivo;encefalo;three dimensional;algorithme parallele;linear coding;sistema nervioso central;data storage;haute resolution;imaging system;internet;level of detail;medical image;encephale;linear code;estructura datos;imaging;structure fichier;imagen numerica;file structure;alta resolucion;imagineria medica;imagerie medicale;alto rendimiento;quad arbre;formation image;computing systems;procesador oleoducto;structure donnee;brain atlas;formacion imagen;digital image;processeur pipeline;high performance;data structure;resolution image;central nervous system;imaging systems;pipeline processor;neuroinformatique;codificacion lineal;codigo lineal;brain vertebrata	In this paper, we present a neuro-medical imaging system called the Brain Slicer, which allows neuroscientists to construct a three-dimensional digital brain atlas from an array of high-resolution parallel section images and obtain arbitrary oblique section images from the digital atlas. This application is based on a new data structure, the Scalable Hyper-Space File (SHSF). The SHSF is a generalized data structure that can represent a hyperspace of any dimension. The two-dimensional SHSF is a scalable linear quadtree and the three-dimensional SHSF is a scalable linear octree. Unlike the normal linear quadtree and octree, the data structure uses a scalable linear coding scheme. It recursively uses fixed-length linear code to encode the hyperspace, which is efficient in terms of storage space and accessing speed. The structure lends itself well to pipelined parallel operations in constructing the volumetric data set, so that it enjoys excellent performance even though the huge data set imposes heavy disk I/O requirements. The data structure can provide different levels of detail; therefore it can be used in an environment where the bandwidth and computation power is limited, such as the Internet and slow desktop computers. We envision that this methodology can be used in many areas other than neuro-medical imaging.© (2002) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	internet	Rongkai Zhao;Tao Tao;Michael Gabriel;Geneva G. Belford	2002		10.1117/12.458801	simulation;computer science;cartography;computer graphics (images)	HPC	39.463112536675425	-20.217068033951623	22521
09322f463d964334d1e63bb30d10d27dccba11e8	cross-entropy optimisation of importance sampling parameters for statistical model checking	optimal importance;cross-entropy optimisation;probabilistic model checking;good importance;entire state space;exponential growth;simple algorithm;confidence bound;key objective;statistical model checking;intractable explicit representation	Statistical model checking avoids the exponential growth of states associated with probabilistic model checking by estimating probabilities from multiple executions of a system and by giving results within confidence bounds. Rare properties are often important but pose a particular challenge for simulation-based approaches, hence a key objective for statistical model checking (SMC) is to reduce the number and length of simulations necessary to produce a result with a given level of confidence. Importance sampling can achieves this, however to maintain the advantages of SMC it is necessary to find good importance sampling distributions without considering the entire state space. Here we present a simple algorithm that uses the notion of crossentropy to find an optimal importance sampling distribution. In contrast to previous work, our algorithm uses a naturally defined low dimensional vector of parameters to specify this distribution and thus avoids the intractable explicit representation of a transition matrix. We show that our parametrisation leads to a unique optimum and can produce many orders of magnitude improvement in simulation efficiency. We demonstrate the efficacy of our methodology by applying it to models from reliability engineering and biochemistry.	algorithm;cross entropy;executable;guarded command language;importance sampling;markov chain;mathematical optimization;model checking;monte carlo method;monte carlo methods for option pricing;rare events;reliability engineering;sampling (signal processing);simulation;state space;statistical model;stochastic matrix;systems biology;time complexity;world-system	Cyrille Jégourel;Axel Legay;Sean Sedwards	2012		10.1007/978-3-642-31424-7_26	econometrics;mathematical optimization;mathematics;statistics	ML	33.168859189378935	-12.54510741575095	22523
62562e667cc3fa5e82b053dab6b9939f224aea8b	asymptotically optimal multi-armed bandit activation policies under side constraints		This paper introduces the first asymptotically optimal strategy for a multi armed bandit (MAB) model under side constraints. The side constraints model situations in which bandit activations are limited by the availability of certain resources that are replenished at a constant rate. The main result involves the derivation of an asymptotic lower bound for the regret of feasible uniformly fast policies and the construction of policies that achieve this lower bound, under pertinent conditions. Further, we provide the explicit form of such policies for the case in which the unknown distributions are Normal with unknown means and known variances, for the case of Normal distributions with unknown means and unknown variances and for the case of arbitrary discrete distributions with finite support.		Apostolos Burnetas;Odysseas Kanavetas;Michael N. Katehakis	2018	CoRR			ECom	37.07828181427382	3.7749622082251326	22535
94183331d23435aeec74d8cfd401807501a9ce24	a flexible error resilient scheme for jpeg 2000	image coding;streaming media resilience error correction codes robustness redundancy decoding discrete wavelet transforms multimedia communication radio communication bandwidth;packet loss;wireless multimedia applications;reed solomon codes;multimedia application;computational complexity;reed solomon codes multimedia communication image coding computational complexity;rate allocation;unequal error protection;bit error flexible error resilient scheme jpeg 2000 standard wireless multimedia application packet loss band limitation bit level error corruption robust multimedia delivery image coding standard multiple description coding technique post processing rate allocation embedded bitstream;multimedia communication;multiple description coding;error resilience	Nowadays, wireless multimedia applications are experiencing a rapid growth; in this scenario, challenging obstacles, such as packet losses due to congestion and band limitation along with bit-level error corruption, require the design of novel solutions for robust multimedia delivery. New standards for multimedia applications are incorporating many tools for error resilience; as an example, JPEG 2000 part 11 is explicitly devoted to the wireless applications of the image coding standard. In this paper we address a novel multiple description coding technique, based on post-processing rate allocation of embedded bitstreams. The proposed approach is compliant with the JPEG 2000 standard and has the ability to jointly cope with both packet losses and bit errors. Experimental results show that the designed algorithm significantly outperforms other techniques based on unequal error protection by means of RS codes.	algorithm;bandlimiting;bit-level parallelism;code;embedded system;jpeg 2000;multiple description coding;network congestion;network packet;reed–solomon error correction;video post-processing	Tammam Tillo;Marco Grangetto;Gabriella Olmo	2004	IEEE 6th Workshop on Multimedia Signal Processing, 2004.	10.1109/MMSP.2004.1436551	real-time computing;telecommunications;computer science;theoretical computer science;multiple description coding;packet loss;computational complexity theory	Embedded	48.23176620241817	-16.190752520919176	22536
8f52908e446342f715cd060fe2c3c2eee6126f01	using suffix trees for gapped motif discovery	alignement sequence;algoritmo busqueda;algorithm performance;motif discovery;gibbs sampler;gibbs sampling;algorithme recherche;protein sequence;search algorithm;biologia molecular;alineacion secuencia;suffix tree;resultado algoritmo;informatique theorique;molecular biology;performance algorithme;analyse combinatoire;appariement chaine;sequence alignment;string matching;local search;analisis combinatorio;combinatorial analysis;computer theory;biologie moleculaire;informatica teorica	Gibbs sampling is a local search method that can be used to find novel motifs in a text string. In previous work [8], we have proposed a modified Gibbs sampler that can discover novel gapped motifs of varying lengths and occurrence rates in DNA or protein sequences. The Gibbs sampling method requires repeated searching of the text for the best match to a constantly evolving collection of aligned strings, and each search pass previously required θ(nl) time, where l is the length of the motif and n the length of the original sequence. This paper presents a novel method for using suffix trees to greatly improve the performance of the Gibbs sampling approach.	sequence motif	Emily Rocke	2000		10.1007/3-540-45123-4_28	gibbs sampling;computer science;bioinformatics;artificial intelligence;machine learning;mathematics;algorithm	Theory	26.727468806927455	2.685213273395707	22539
e2320f3d029fbb5a2d773cc8d731b305d8bf6b2f	ensemble classification of paired data	methode jackknife;ensemble classification;62f40;paired data;classification automatique statistiques;glaucoma diagnosis;dato observacion;analyse multivariable;forests;healthy control;repeated measurement;ensemble classification glaucoma diagnosis paired data;aplicacion medical;multivariate analysis;bootstrap;analisis datos;62j05;05c05;metodo jackknife;qa mathematics;62g09;repeated measures;simulation;estimation non parametrique;bundle method;linear regression;simulacion;mesure repetee;linear discriminate analysis;logistic regression;statistical regression;foret;discriminant analysis;estimation parametrique;analyse discriminante;non parametric estimation;data analysis;analisis discriminante;regresion logistica;analisis regresion;62h30;regresion estadistica;mauvaise classification;regression logistique;random forest;statistical computation;calculo estadistico;regresion lineal;modele simulation;analyse regression;analisis multivariable;analyse donnee;regression analysis;calcul statistique;medical application;methode reechantillonnage;donnee observation;estimacion no parametrica;bad classification;modelo simulacion;resampling method;estimation statistique;jackknife method;regression statistique;bosque;estimacion estadistica;r medicine general;statistical estimation;simulation model;medida repetida;regression lineaire;observation data;application medicale	In many medical applications, data are taken from paired organs or from repeated measurements of the same organ or subject. Subject based as opposed to observation based evaluation of these data results in increased efficiency of the estimation of the misclassification rate. A subject based approach for classification in the generation of bootstrap samples of bagging and bundling methods is analyzed. A simulation model is used to compare the performance of different strategies to create the bootstrap samples which are used to grow individual trees. The proposed approach is compared to linear discriminant analysis, logistic regression, random forests and gradient boosting. Finally, the simulation results are applied to glaucoma diagnosis using both eyes of glaucoma patients and healthy controls. It is demonstrated that the proposed subject based resampling reduces the misclassification rate.		Werner Adler;Alexander Brenning;Sergej Potapov;Matthias Schmid;Berthold Lausen	2011	Computational Statistics & Data Analysis	10.1016/j.csda.2010.11.017	econometrics;machine learning;mathematics;linear discriminant analysis;regression analysis;statistics	ML	32.70061027759515	-22.800622061478343	22577
dccf05b4f1f55c219c6795f8deb6ff30b4e271fc	watermarking and copyright labeling of printed images	digital watermarking;image processing;digital watermark;image compression;digital image	iding gies sper000 Abstract. Digital watermarking is a labeling technique for digital images which embeds a code into the digital data so the data are marked. Watermarking techniques previously developed deal with on-line digital data. These techniques have been developed to withstand digital attacks such as image processing, image compression and geometric transformations. However, one must also consider the readily available attack of printing and scanning. The available watermarking techniques are not reliable under printing and scanning. In fact, one must consider the availability of watermarks for printed images as well as for digital images. An important issue is to intercept and prevent forgery in printed material such as currency notes, bank checks, etc., and to track and validate sensitive and secret printed material. Watermarking in such printed material can be used not only for verification of ownership but as an indicator of date and type of transaction or date and source of the printed data. In this work we propose a method of embedding watermarks in printed images by inherently taking advantage of the printing process. The method is visually unobtrusive to the printed image, the watermark is easily extracted and is robust under reconstruction errors. The decoding algorithm is automatic given the watermarked image. © 2001 SPIE and IS&T. [DOI: 10.1117/1.1382612]	algorithm;aliasing;binary code;blu-ray;color management;compression artifact;digital data;digital image;digital watermarking;dither;embedded system;human visual system model;image compression;image processing;image resolution;meta-object facility;monochrome;naruto shippuden: clash of ninja revolution 3;north american mesoscale model;online and offline;original net animation;printer (computing);printer driver;printing;randomness;sion's minimax theorem;unobtrusive javascript;visual intercept;visual artifact;xilinx ise;monotone	Hagit Hel-Or	2001	J. Electronic Imaging	10.1117/1.1382612	computer vision;image processing;digital watermarking;computer science;digital image processing;multimedia;internet privacy;watermark	Graphics	37.29825041396646	-12.138383087910785	22630
14819ee20d5cf420e3b7c9843de381646a9ac6b6	on-chip peripherals are on for chaos - an image fused encryption		Abstract Image encryption is being employed as an important security provider to facilitate the communication of confidential images over various confidential networks. In this work, a RGB image encryption procedure based on Chaotic and Cellular Automata (CA) attractors is proposed. Lorenz, Lu and Rule 42 of CA have been used as encryption mediums in red, green and blue planes respectively. Besides scrambling and XORing operations on secret image, a random synthetic image has also been used for diffusion on the three planes. Cyclone II FPGA EP2C35F672C6 has been utilized to generate the low correlation yielding random synthetic image aided by beat frequency detection using PLLs and diffused bit generation process. The proposed approach satisfies the various statistical parameters and offers tangible resistance to differential, occlusion and chosen plain text attacks on RGB images.	encryption;peripheral	Sundararaman Rajagopalan;Sivaraman Rethinam;Har Narayan Upadhyay;John Bosco Balaguru Rayappan;Rengarajan Amirtharajan	2018	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2018.06.011	field-programmable gate array;chip;parallel computing;theoretical computer science;encryption;attractor;cellular automaton;plain text;computer science;rgb color model	EDA	38.34225336902272	-8.798594156836483	22663
ffac656356c8178026183c3c8e6d94fe8d31a804	performance of localization algorithm under corrupted measurement data and lopsided sensor arrangement	local algorithm;robust estimator;trilateration algorithm corrupted measurement data lopsided sensor arrangement radar network estimation algorithm target position robust estimation receiver;indexing terms;estimation algorithm;receivers;accuracy;monitoring system;estimation;monitoring;sensor placement;position estimation;mathematical model;robustness;target tracking sensor placement;target tracking;receivers estimation radar accuracy robustness monitoring mathematical model;computer simulation;radar	Radar networks show an interesting potential for safety and comfortable applications such as indoor monitoring or short-range automotive monitoring system. This paper presents our novel estimation algorithm of a target position. Especially we evaluate the performance about estimation accuracy and resistance to ghost targets under multipath environment. In above applications, the robust estimation is needed because the receivers tend to output corrupted measurement data. The corrupted data are mostly generated by multipath, sensitivity of receivers. Moreover, our arrangement of radars is one-side watching for easy fitting in typical applications. As a result of computer simulations, our algorithm has fine accuracy and robust detections compared with a popular trilateration algorithm. And the relationship between the estimation accuracy and the arrangement of receivers is introduced.	algorithm;computer simulation;cross section (geometry);electronic counter-countermeasure;multipath propagation;radar;sensitivity and specificity;sensor	Hiroyuki Hatano;Tomoharu Mizutani;Kazuya Sugiyama;Yoshihiko Kuwahara	2011	2011 4th IFIP International Conference on New Technologies, Mobility and Security	10.1109/NTMS.2011.5720595	computer simulation;robust statistics;computer vision;estimation;index term;computer science;mathematical model;accuracy and precision;radar;statistics;robustness	EDA	50.5503582711214	1.1074503889020157	22733
253f90ad6cc11ca8b24ea8509191121b2e1fdfbf	modeling and analysis of genetic algorithms based on the viewpoint of mixture systems	modeling by learning;mixture model;genetic algorithm;vector field;modeling and analysis	Some mathematical models have been proposed for theoretical analyses of genetic algorithms (GAs). However, these works have limited their objects to a few kinds of GAs in order to formulate them accurately. In this paper, we regard a GA as an information source that generates input-output data. That is, we regard a population and its next population generated by the GA as input and output respectively. Then we model the GA by learning from these data. Since this method uses only the input-output relations of data and ignores interior structures, we can describe a variety of GAs in a common form, and analyze them from a new point of view. We use some mixture models for a representation of these input-output relations in this paper. By using a mixture model for modeling a GA, we can represent the GA system as a combination of some partial systems. In this paper, we treat two types of mixture models, and investigate how these models are effective for analyzing GAs through some numerical experiments.	experiment;genetic algorithm;information source;input/output;mathematical model;mixture model;numerical analysis;point of view (computer hardware company);population;software release life cycle;string (computer science);viewpoint	Jun-ichi Imai;Hiroyuki Shioya;Masahito Kurihara	2003	JACIII	10.20965/jaciii.2003.p0268	vector field;genetic algorithm;computer science;machine learning;pattern recognition;mixture model	AI	35.772345146927805	-15.348897806402622	22783
e8a96c7231cce4c2543a4b8bf17688a55f955e01	conditional sensor deployment using evolutionary algorithms	deployment;ga;hybrid;pso;sensor network	Sensor deployment is a critical issue, as it affects the cost and detection capabilities of a wireless sensor network. Although many previous efforts have addressed this issue, most of them assume that the sensing field is an open space. In this work, we consider the sensing field as conditional regions. The Sensor Location Problem (SLP) is a nonlinear nonconvex programming problem which aims to locate sensors to monitor a constrained region. The objective is to determine the locations that will maximize the coverage. Three evolutionary algorithms, particle swarm optimization (PSO), genetic algorithm (GA) and Adaptive Hybrid Optimization (AHO) were used to solve the SLP. Several variants (sensing patterns, sensor counts and region constrains) were tested and results show that the three algorithms are able to obtain good solutions. The proposed AHO proved that it is able to achieve the benefits of both methods and avoid their drawback by smartly switching between them during optimization process. AHO is capable of accomplishing the best solution within less number of iterations.	evolutionary algorithm;genetic algorithm;iteration;mathematical optimization;nonlinear system;particle swarm optimization;sensor;software deployment;successive linear programming	M. Sami Soliman;Guanzheng Tan	2010	JCIT		computer science;artificial intelligence;machine learning	Mobile	29.055901417397823	-2.172306720032658	22818
506650a3a2e279aa5f318cca3a55089cd7224431	two-stage multiple comparisons with the best for computer simulation	simulation ordinateur;experimental design;eficacia sistema;intervalo confianza;simulation;plan experiencia;performance systeme;moyenne groupe;system performance;batch mean;stochastic system;confidence interval;design of experiments;statistical analysis;plan experience;multiple comparisons;intervalle confiance;comparacion multiple;comparaison multiple;simulacion computadora;sistema estocastico;multiple comparison;computer simulation;systeme stochastique	We consider the problem of comparing a small number of stochastic systems via computer simulation when the basis for comparison is the expected value of some system performance measure. To solve this problem we develop two-stage sampling procedures that provide confidence intervals for the difference between the expected performance of each system and the best of the others. These confidence intervals are valid under mild conditions, and the procedures allow the experimenter to specify the desired precision in advance. Special cases of our results include standard indifference-zone selection procedures. The paper includes guidelines for experiment design and an illustrative example.	computer simulation	Frank J. Matejcik;Barry L. Nelson	1995	Operations Research	10.1287/opre.43.4.633	computer simulation;simulation;computer science;mathematics;design of experiments;multiple comparisons problem;algorithm;statistics	Theory	31.578549263903177	-21.018750415628375	22829
c26e86bd79d26ecea263c61141121f815dcbb030	an algorithm with reduced operations for connected components detection in itu-t group 3/4 coded images	image coding;connected components;document image analysis;data compression;document image analysis reduced operations algorithm connected components detection itu t group 3 4 coded images ccitt facsimile group mh compressed images mr compressed images mmr compressed images mode color transition element black runs adjacent scan lines data structures image vector description;decoding;ccitt facsimile group;itu t group 3 4 coded images;mh mr mmr;adjacent scan lines;code standards;image vector description;connected components detection;data mining;reduced operations algorithm;mh compressed images;facsimile;image color analysis;data structures;feature extraction;ccitt itu group 3 4 compression;mode color;document image processing;image coding image color analysis image analysis decoding data mining facsimile optical devices feature extraction information analysis code standards;data structures document image processing image coding data compression;image analysis;black runs;mmr compressed images;document image;mr compressed images;connected component;data structure;information analysis;group 3;optical devices;transition element	ÐAn algorithm, which performs connected components detection in the course of decoding ITU-T (former CCITT) facsimile Group 3/4, i.e., MH/MR/MMR compressed images is presented. New definitions of mode color and a new transition element are introduced that allow MR/MMR codes to analyze and derive information about connection of black runs in two adjacent scan lines in the course of decoding. The experiments on the standard set of eight CCITT documents have shown that, on the average, the complexity of direct processing of MR/MMR codes is lower by a factor of 20 and 2.5 than that for raster images and MH codes processing respectively. Data structures for image vector description are discussed. Index TermsÐDocument image, CCITT (ITU) Group 3/4 compression, MH/MR/MMR, connected components.	2.5d;algorithm;code;connected component (graph theory);data compression;experiment;meet-me room;modified huffman coding;multi-master replication;raster data;raster graphics;scan line	Emma E. Regentova;Shahram Latifi;Shulan Deng;Dongsheng Yao	2002	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/TPAMI.2002.1023801	computer vision;speech recognition;connected component;data structure;telecommunications;computer science;statistics	Vision	40.86821180644786	-17.57551470415859	22890
128f331ec33455c04deb449d211cc9ee80bd7565	efficient image compression algorithm for computer-animated images	lempel ziv;compression algorithm;computer graphic;image compression;operating system;time use;computing systems;animal imaging;computer animation	An image compression algorithm is described. The algorithm is an extension of the run-length image compression algorithm and its implementation is relatively easy. This algorithm was implemented and compared with other existing popular compressionrnalgorithms and with the Lempel-Ziv (LZ) coding. The Lempel- Ziv algorithm is available as a utiilty in the UNIX operating system and is also referred to as the UNIX uncompress. Sometimes our algorithm is best in terms of saving memory space, and sometimes one of the competing algorithms is best. The algorithm is lossless, and the intent is for the algorithm to be used in computer graphics animated images. Comparisons made with the LZalgorithm indicaternthat the decompression time using our algorithm is faster than that using the LZ algorithm. Once the data are in memory, a relatively simple and fast transformation is applied to uncompress the file.	algorithm;computer animation;data compression;image compression	Evangelos A. Yfantis;Matthew Y. Au;George Miel	1992	J. Electronic Imaging	10.1117/12.59970	data compression;computer vision;image compression;computer science;theoretical computer science;lossless compression;computer animation;texture compression;algorithm;computer graphics (images)	Robotics	40.25070986827265	-20.776007295233853	22916
caf748148a8d22f3da13d8c23e62feac8821a950	level-3 trigger for a heavy ion experiment at lhc	circuit declenchement;circuito desenganche;heavy ions;modeling technique;efecto memoria forma;storage system;image processing;large hadron collider;procesamiento imagen;traitement image;ion lourd;captador medida;measurement sensor;capteur mesure;ion pesado;low latency;heavy ion;pattern recognition;trigger;effet memoire forme;reconnaissance forme;reconocimiento patron;off the shelf;shape memory effect	At the upcoming Large Hadron Collider (LHC) at CERN one expects to measure 20,000 particles in a single Pb-Pb event resulting in a data rate of ∼75MByte/event. The event rate is limited by the bandwidth of the storage system. Higher rates are possible by selecting interesting events and subevents (Level-3 trigger) or compressing the data efficiently withmo deling techniques. Both require a fast parallel pattern recognition. One possible solution to process the detector data at such rates is a farm of clustered SMP nodes, based on off-the-shelf PCs, and connected by a high bandwidth, low latency network.		U. Frankenfeld;H. Helstrup;J. Lien;Volker Lindenstruth;Dieter Röhrich;M. W. Schulz;B. Skaali;Timm M. Steinbeck;K. Ullaland;Anders Strand Vestbø;Arne Wiebalck	2001		10.1007/3-540-44681-8_54	simulation;large hadron collider;telecommunications;image processing;computer science;operating system;low latency	NLP	42.698108888003745	-1.2778159081308975	22957
ff33578df94ff2aef169a35c640c6a4a4a6e6ca6	hyperspectral images lossless compression by a novel three-dimensional wavelet coding	asymmetric 3d quincunx structured wavelet transform;hyperspectral images;lossless compression;three dimensional;wavelet transform;spatial correlation;structural transformation;adaptive classification coding;hyperspectral image	In this paper, we present a 3D hyperspectral images lossless compression algorithm, which is based on the asymmetric 3D quincunx structured wavelet transform (A3D-QWT) and the adaptive classification coding. A3D-QWT is a simpler 3D transform compared to the classical asymmetric 3D pyramidal wavelet structure (A3D-DWT) with nearly equal entropy; furthermore the spectral correlation after this quincunx structured transform is higher, which is useful to the adaptive classification coding. The adaptive classification coding can make full use of not only the spatial correlation but also the spectral correlation characteristics of hyperspectral images. Experiments show that our method is capable of providing a higher compression performance for hyperspectral images.	a3d;algorithm;discrete wavelet transform;lossless compression;qwt	Jing Zhang;Guizhong Liu	2007		10.1145/1291233.1291404	data compression;three-dimensional space;computer vision;spatial correlation;continuous wavelet transform;theoretical computer science;pattern recognition;lossless compression;stationary wavelet transform;discrete wavelet transform;statistics;wavelet transform	Vision	43.45428087476115	-15.030528466537596	22989
bf3aff3207b744f14b1b9e0621f5e1b640c7627c	distributed video coding in pixel domain using spatial correlation at the decoder	joint source channel decoding;wyner ziv;iterative source channel decoding;iterative decoding;psnr;data compression;decoding;combined source channel coding;data exchange;joint source channel decoding distributed video coding turbo coding;one dimensional stationary markov process;joint source channel;soft input soft output;low complexity encoder device;video coding;joint source channel decoding scheme;wyner ziv frame spatial correlation;video coding combined source channel coding data compression markov processes;joint source channel decoding scheme low complexity encoder device low power encoder device pixel domain distributed video coding scheme dvc scheme slepian wolf decoder wyner ziv frame spatial correlation one dimensional stationary markov process;spatial correlation;virtual channel;slepian wolf;pixel;dvc scheme;markov process;distributed video coding;low power encoder device;markov processes;decoding iterative decoding correlation encoding pixel psnr markov processes;turbo coding;correlation;side information;encoding;slepian wolf decoder;turbo decoding;pixel domain distributed video coding scheme;turbo code	Distributed coding is a new paradigm for video compression based on Slepian-Wolf (1973) and Wyner-Ziv (1976) theorems. In this paper, we propose a new pixel-domain Distributed Video Coding (DVC) scheme, in which both the temporal and the spatial correlations are exploited only at the decoder. A joint source-channel decoding is implemented to exploit the source statistics. Iterations between the Map channel decoder and the BCJR source decoder are made to improve the global decoder performance. Simulations results show that a gain is obtained, in term of rate, with iterative joint source-channel decoding, compared to the basic decoding.	bcjr algorithm;computer simulation;data compression;iteration;pixel;programming paradigm	Cyrine Lahsini;Sonia Zaibi;Ramesh Pyndiah;Ammar Bouallègue	2011	2011 Data Compression Conference	10.1109/DCC.2011.67	distributed source coding;turbo code;soft-decision decoder;telecommunications;computer science;theoretical computer science;mathematics;markov process;statistics	Vision	49.504725105657805	-16.957511366763903	23023
860055c4e86e19fb80fdfaaffffd25530830cc66	determining process death based on censored activity data	autocorrelacion;developpement logiciel;software;metodo estadistico;ajustamiento modelo;60g99;stochastic process;aplicacion;proceso censurado;logiciel;62m10;simulacion numerica;donnee censuree;logistic model;regression model;statistical method;distribucion logistica;process activity;time series;discrete event simulations;logistic regression;statistical regression;62jxx;activity pattern;processus censure;ajustement modele;regresion logistica;modelo regresion;62n02;censored data;methode statistique;60g05;desarrollo logicial;regresion estadistica;open source software development;modele regression;model matching;regression logistique;modele logistique;simulation numerique;software development;serie temporelle;survival analysis;processus stochastique;distribution logistique;censored process;62n99;serie temporal;modelo logistico;time series data;logicial;estimation statistique;proceso estocastico;regression statistique;application;estimacion estadistica;statistical estimation;logistic distribution;logistic regression model;62n01;autocorrelation;open source software;numerical simulation;discrete event simulation	This article addresses the problem of estimating the time of apparent death in a binary stochastic process. We show that, when only censored data are available, a fitted logistic regression model may estimate the time of death incorrectly. We improve this estimation by utilizing discrete-event simulation to produce simulated complete time series data. The proposed methodology may be applied to situations where time of death cannot be formally determined and has to be estimated based on prolonged inactivity. As an illustration, we use observed monthly activity patterns from 300 real Open Source Software development projects sampled from Sourceforge.net.		Nicholas Evangelopoulos;Anna Sidorova;Stergios Fotopoulos;InduShobha N. Chengalur-Smith	2008	Communications in Statistics - Simulation and Computation	10.1080/03610910802140224	computer simulation;stochastic process;econometrics;mathematics;logistic regression;statistics	Mobile	33.493805093392936	-22.21551179861086	23058
359ab378cb6b0df3af37c9a8024066dd0581e3be	a nonparametric assessment of model adequacy based on kullback-leibler divergence	goodness of fit;polya tree;teletraffic data;packet train;nonparametric alternative	A discrepancy measure to assess model fitness against a nonparametric alternative is proposed. First, a Polya tree prior is constructed so that the centering distribution is the null. Second, the prior is updated in the light of data to obtain the posterior centering distribution as the alternative. Third, a Kullback-Leibler divergence type of test statistic is derived to assess the discrepancy between the two centering distributions. The properties of the test statistic are derived, and a power comparison with several well-known test statistics is conducted. The use of the test statistic is illustrated using network traffic data.	kullback–leibler divergence	Ping-Hung Hsieh	2013	Statistics and Computing	10.1007/s11222-011-9298-0	econometrics;test statistic;machine learning;mathematics;goodness of fit;statistics	AI	29.64871997525953	-21.990059520876112	23059
4085184400b7b5e703c01ef9e9b12e01f3187368	a wireless intelligent valve controller for agriculture integrated irrigation system	remote control;satisfiability;wireless communication;control system;seasonality	Abstract. Based on the problems caused by current automatic irrigation system with a large number of cables, such as installation, maintenance, and difficult expansion, this paper proposed a wireless integrated irrigation control system and developed a wireless intelligent valve controller which can satisfy the requirements for agricultural irrigation in our country. The controller consisted of the control unit, power unit, wireless communication unit, relay boost driver unit, state feedback switch, etc. It received the control command from the remote control center by the wireless communication unit, actuated the relay boost driver unit to turn on/off irrigation solenoid valves. The controller includes the functions of remote control, parameter setting, feedback status detecting, etc. Since the rational design, simple command, and low cost, the controllers have been pilot in Beijing and Xinjiang. The results indicate that the controller is effective in practice and can be used a normal irrigation season.	control system;control unit;low-power broadcasting;microprocessor;power supply;relay;remote control;requirement;semiconductor consolidation;sensor;technical support;transceiver	Nannan Wen;Daoliang Li;Daokun Ma;Qisheng Ding	2010		10.1007/978-3-642-18369-0_79	control engineering;real-time computing;engineering;control theory	Robotics	53.41480779518431	-11.380675090380398	23060
89b421dde306397423e42859d6508e6e51af43ff	color quantization using the fast k-means algorithm	color quantization;k means algorithm;clustering	Many color images use 24 bits for color information. However, this number of colors is not always necessary. In this paper, an algorithm that quantizes a full-color image from about 17 million to 256 colors is proposed. The quantized colors are calculated so that the sum of squared quantization errors is minimized. To quantize the color image, the fast K-means algorithm is proposed. This algorithm classifies K clusters into groups. These groups are used to decrease the number of calculations and the execution time. The fast K-means algorithm is effective when K is large. The results show that for K = 256 the execution time is less than a quarter of that for the K-means algorithm. © 2000 Scripta Technica, Syst Comp Jpn, 31(8): 33–40, 2000	algorithm;color quantization;k-means clustering	Hideo Kasuga;Hiroaki Yamamoto;Masayuki Okamoto	2000	Systems and Computers in Japan	10.1002/1520-684X(200007)31:8%3C33::AID-SCJ4%3E3.0.CO;2-C	combinatorics;color quantization;color depth;ramer–douglas–peucker algorithm;computer science;theoretical computer science;machine learning;mathematics;cluster analysis;linde–buzo–gray algorithm;algorithm;k-means clustering	Robotics	43.75820791232199	-12.802619668862233	23174
444a68e3dd6ae4dcb1d88caaca74d98051248292	predictive coding of images using an adaptive intra-frame predictor and motion compensation	prediccion;quantization;error correcting code;cuantificacion;image processing;codificacion adaptativa;filtre reponse impulsion finie;codigo corrector error;information transmission;finite impulse response filter;procesamiento imagen;codage adaptatif;quantification;traitement image;predictor;motion compensated;algorithme;algorithm;codage predictif;predicteur;filtro respuesta impulsion acabada;adaptive coding;codificacion predictiva;filtro adaptable;transmision informacion;filtre adaptatif;transmission information;code correcteur erreur;predictive coding;prediction;adaptive filter;algoritmo	In this article we present an image predictive coding method using both intraand inter-frame predictors. The intraframe predictor is an adaptive FIR filter using the well-known LMS algorithm to track continuously spatial local characteristics of the intensity. The inter-frame predictor is motion-adaptive using a pel-recursive method estimating the displacement vector. Weight coefficients are continuously adapted in order to favor the prediction mode which performs better between intra-frame and motion compensation mode. It is a backwards adaptation which does not necessitate the transmission of an overhead information. Neither the weight coefficients nor displacement vectors are transmitted. Apart from the quantized prediction error, it may be necessary to transmit the detection of a discontinuity of the displacement vector. For the examined image sequence a significant improvement is obtained in comparison with only adaptive intra-frame or only motion compensation mode. We give and discuss the extension of a known adaptive quantizer for 2D signals. A crucial problem in predictive coding, particularly with adaptive techniques, is the sensitivity to transmission errors. A method ensuring the self-adjustment of the decoder in the presence of transmission errors, which do not affect the pixel synchronization, is proposed for the intra-frame mode. Neither overhead information nor error-correcting codes are needed.	algorithm;code;coefficient;displacement mapping;error detection and correction;finite impulse response;forward error correction;intra-frame coding;kerrison predictor;least mean squares filter;motion compensation;overhead (computing);pixel;predictor–corrector method;quantization (signal processing);recursion;reflections of signals on conducting lines;sensitivity and specificity;whole earth 'lectronic link	Georgios Tziritas;Jean-Christophe Pesquet	1992	Sig. Proc.: Image Comm.	10.1016/0923-5965(92)90031-A	adaptive filter;computer vision;error detection and correction;prediction;quantization;telecommunications;image processing;computer science;finite impulse response;control theory;adaptive coding;algorithm;statistics	Vision	47.03847923895202	-14.61826932926987	23184
2be91850f802f681dc7160bd679705950229933c	differential attack on image encryption algorithm using binary bitplane	computational complexity differential attack image encryption algorithm binary bitplane image relationship key recovering algorithms secrete permutation subkey addition data complexity;binary bitplane;subkey addition binary bitplane equivalent key secrete permutation;image processing cryptography;equivalent key;secrete permutation;subkey addition;encryption ciphers algorithm design and analysis prediction algorithms computational complexity	Image encryption using binary bitplane is first analyzed. Then based on the characteristic of the relationship between cipher image and plain image, equivalent key recovering algorithms for secrete permutation and subkey addition is proposed. Analysis and experimental results show the correctness and validity of the two equivalent key recovering algorithms. The data complexity of the attack for plain image P=(pi,j)M × N is 1+log28MN, computational complexity is 2+ 2log28MN.	algorithm;binary-coded decimal;cipher;computational complexity theory;correctness (computer science);differential cryptanalysis;encryption;random permutation	Xin Ge;Bin Lu;Hui Guan;Kai Zhang	2015	2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2015.7382351	discrete mathematics;theoretical computer science;mathematics;algorithm	EDA	38.46986311609364	-8.802560324638863	23200
3f4b396e3ee1b86e8b07feb32854625720baedb5	adaptive non-uniform particle swarm application to plasmonic design	particle swarm;meta heuristics;particle swarm optimization;algorithms;optimization;plasmonics	"""The metaheuristic approach has become an important tool for the optimization of design in engineering. In that way, its application to the development of the plasmonic based biosensor is apparent. Plasmonics represents a rapidly expanding interdisciplinary field with numerous transducers for physical, biological and medicine applications. Specific problems are related to this domain. The plasmonic structures design depends on a large number of parameters. Second, the way of their fabrication is complex and industrial aspects are in their infancy. In this study, the authors propose a non-uniform adapted Particle Swarm Optimization (PSO) for rapid resolution of plasmonic problem. The method is tested and compared to the standard PSO, the meta-PSO (Veenhuis, 2006) and the ANUHEM (Barchiesi, 2009).These approaches are applied to the specific problem of the optimization of Surface Plasmon Resonance (SPR) Biosensors design. Results show great efficiency of the introduced method. label-free and commercially developed optical sensors. This technique is currently employed in biomolecular engineering, drug design, monoclonal antibody characterization, virus-protein interaction, environmental pollutants detection, among other interesting fields. The fabrication process of the SPR biosensors-nanometer scaleis complex and costly. Moreover, specific problems are related to this domain. Therefore, the design optimization of DOI: 10.4018/jamc.2011010102 International Journal of Applied Metaheuristic Computing, 2(1), 18-28, January-March 2011 19 Copyright © 2011, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. SPR biosensors is of great interest although it has been rarely addressed (Barchiesi, 2009; Barchiesi, Kremer, Mai & Grosges, 2008a; Lecaruyer, Canva & Rolland 2006; Ekgasit, Thammacharoen & Knoll, 2005; Kolomenskii, Gershon & Schuessler, 1997). Therefore appropriate adjustments to the most successful optimization technique must be introduced to solve these problems. The Particle Swarm Optimization (PSO) was first introduced by Kennedy and Eberhart in 1995 and imitates the swarm behaviour to search the globally best solution. In this method, particle moves using its own experience and collaboration with neighbor swarm particles. This technique attracted a high level of interest because of its encouraging results and was the subject of many improvements. A first set of improvements is related to the PSO parameters. We will be basically interested with this issue to introduce a new optimization algorithm as it will be fully reported in the section 2.3. A second set of improvements is related to testing different topologies for different problems. The main discussed issues (Kennedy & Mendes, 2002; Hu & Eberhart, 2002; Mendes, Kennedy, & Neves, 2004; Liang, Qin, Suganthan & Baskar, 2006) are: • Whether to adopt GBEST (a global search technique for best position) or one of the LBEST (local) configurations; • If the PSO with small or high neighborhood might perform better; • The weight/influence of each particle on its neighbours; • How to move if the collaboration and self experience part dictates displacements in opposite sides. In this paper, we will introduce a new PSO algorithm – basically using adaptive PSO parameters with non-uniform lawand compare its efficiency with some other techniques while applied to the plasmonic problem. The second section will introduce SPR biosensor optimization problem and give the related fitness function and decision variables. The proposed optimization method as well as other methods used for the benchmarking will be presented in section 2. In the third section, numerical results will be reported and discussed. Finally, in the last section, some conclusions and perspectives will be drawn. 1. THE OPTIMIZATION PROBLEM OF SPR BIOSENSORS The operating principle of the SPR biosensors is based on the shift of the position of mathematical poles. This mathematical issue is in fact related to a critical change of the interaction between light and matter/substrate that happens in presence of slight environment changes (especially the presence of substances to be detected by the sensor) (Barchiesi et al., 2008b; Kretschman & Raether, 1968). Basically, a sudden absorption of light by metal layer of the biosensor occurs, for a given incidence angle of the illumination, leading to a device with high sensitivity to any change in the surrounding biological environment (Leracuyer et al., 2006; Kolomenskii et al., 1997). The problems encountered in plasmonics depend on complex parameters (characteristics of materials, ni (i=1,...,4) in Figure 1) varying with the illumination condition (Homola, 1997). Moreover, in general case, they could be a function of more than ten parameters (Davy, Barchiesi, Spajer, & Courjon, 1999; Fikri, Grosges, & Barchiesi, 2003; Fikri, Grosges, & Barchiesi, 2004, Pagnot, Barchiesi, Labeke & Pieralli, 1997). Therefore, an appropriate optimization method should be introduced. The plasmonic structure, considered in this paper, is illuminated by light with incidence angle θ and made of a glass prism and two metallic layers: a stick Chromium layer and a Gold one (Figure 1). The performance of the structure corresponds to a maximum of energy transfer or, in other words, a vanishing reflection R(x) (the light reflected by the biosensor), where x is the vector of decision variables. The resulting objective function is then the mini9 more pages are available in the full version of this document, which may be purchased using the """"Add to Cart"""" button on the product's webpage: www.igi-global.com/article/adaptive-non-uniform-particleswarm/52790?camid=4v1 This title is available in InfoSci-Journals, InfoSci-Journal Disciplines Computer Science, Security, and Information Technology, InfoSci-Artificial Intelligence and Smart Computing eJournal Collection, InfoSci-Computer Systems and Software Engineering eJournal Collection, InfoSciJournal Disciplines Engineering, Natural, and Physical Science, InfoSci-Select. Recommend this product to your"""	algorithm;artificial intelligence;chromium (web browser);colette rolland;computer science;diffuse reflection;fitness function;high-level programming language;holographic principle;incidence matrix;information science;mathematical optimization;metaheuristic;network topology;numerical analysis;optimization problem;particle swarm optimization;plasmon;resonance;russell c. eberhart;section 508 amendment to the rehabilitation act of 1973;semiconductor device fabrication;sensor;software engineering;transducer;tree rearrangement;web page;word lists by frequency	Sameh Kessentini;Dominique Barchiesi;Thomas Grosges;Laurence Giraud-Moreau;Marc Lamy de la Chapelle	2011	Int. J. of Applied Metaheuristic Computing	10.4018/jamc.2011010102	mathematical optimization;multi-swarm optimization;computer science;artificial intelligence;particle swarm optimization;metaheuristic	AI	33.26863902983837	-2.954107935694284	23202
b4f748412042f741321075833c706b2e85c5d205	progressive mean control chart for monitoring process location parameter	progressive mean pm;cusum;statistical process control;memory control charts;ewma;average run length arl	Control charts are widely used for process monitoring. They show whether the variation is due to common causes or whether some of the variation is due to special causes. To detect large shifts in the process, Shewhart-type control charts are preferred. Cumulative sum (CUSUM) and exponentially weightedmoving average (EWMA) control charts are generally used to detect small and moderate shifts. Shewhart-type control charts (without additional tests) use only current information to detect special causes, whereas CUSUM and EWMA control charts also use past information. In this article, we proposed a control chart called progressive mean (PM) control chart, in which a PM is used as a plotting statistic. The proposed chart is designed such that it uses not only the current information but also the past information. Therefore, the proposed chart is a natural competitor for the classical CUSUM, the classical EWMA and some recent modifications of these two charts. The conclusion of this article is that the performance of the proposed PM chart is superior to the compared ones for small and moderate shifts, and its performance for large shifts is better (in terms of the average run length). Copyright © 2012 John Wiley & Sons, Ltd.	chart;john d. wiley;run-length encoding	Nasir Abbas;Raja Fawad Zafar;Muhammad Riaz;Zawar Hussain	2013	Quality and Reliability Eng. Int.	10.1002/qre.1386	ewma chart;engineering;cusum;operations management;mathematics;operations research;statistical process control;statistics	ML	27.75459989600542	-18.972440273087926	23218
0da4d51694eccbfa6247b9be456505790955c1c8	a genetic algorithm for the longest common subsequence problem	dynamic pro gramming;longest common subsequence;strings;genetic algorithm;fitness function	A genetic algorithm for the longest common subsequence problem encodes candidate sequences as binary strings that indicate subsequences of the shortest or first string. Its fitness function penalizes sequences not found in all the strings. In tests on 84 sets of three strings, a dynamic programming algorithm returns optimum solutions quickly on smaller instances and increasingly slowly on larger instances. Repeated trials of the GA always identify optimum subsequences, and it runs in reasonable times even on the largest instances.	dynamic programming;fitness function;genetic algorithm;http 404;longest common subsequence problem;software release life cycle	Brenda Hinkemeyer;Bryant A. Julstrom	2006		10.1145/1143997.1144105	mathematical optimization;combinatorics;discrete mathematics;genetic algorithm;longest increasing subsequence;computer science;artificial intelligence;machine learning;longest common subsequence problem;mathematics;longest alternating subsequence;fitness function	Theory	26.74611113530914	2.596283364870374	23258
e07ce501bb2b5d2b979773e301d21fd72055d468	grammar model-based program evolution	genetic programming australia evolutionary computation genetic algorithms educational institutions genetic mutations sampling methods electronic design automation and methodology stochastic processes context modeling;genetic program;genetic operator;probability;building block;benchmark problem;context free grammars;probabilistic context free grammar;scfg model grammar model based program evolution evolutionary computation mutation crossover genetic algorithm genetic programming tree representation probabilistic context free grammar genetic operator gp tree representation;genetic algorithm;genetic algorithms;probability genetic algorithms context free grammars;evolutionary computing	In evolutionary computation, genetic operators, such as mutation and crossover, are employed to perturb individuals to generate the next population. However these fixed, problem independent genetic operators may destroy the sub-solution, usually called building blocks, instead of discovering and preserving them. One way to overcome this problem is to build a model based on the good individuals, and sample this model to obtain the next population. There is a wide range of such work in genetic algorithms; but because of the complexity of the genetic programming (GP) tree representation, little work of this kind has been done in GP. In this paper, we propose a new method, grammar model-based program evolution (GMPE) to evolved GP program. We replace common GP genetic operators with a probabilistic context-free grammar (SCFG). In each generation, an SCFG is learnt, and a new population is generated by sampling this SCFG model. On two benchmark problems we have studied, GMPE significantly outperforms conventional GP, learning faster and more reliably.	benchmark (computing);context-free language;crossover (genetic algorithm);evolutionary computation;genetic algorithm;genetic operator;genetic programming;mutation (genetic algorithm);perturbation theory;sampling (signal processing);stochastic context-free grammar	Yin Shan;Robert I. McKay;Rohan Baxter;Hussein A. Abbass;Daryl Essam;Nguyen Xuan Hoai	2004	Proceedings of the 2004 Congress on Evolutionary Computation (IEEE Cat. No.04TH8753)	10.1109/CEC.2004.1330895	genetic programming;defining length;mathematical optimization;genetic algorithm;java evolutionary computation toolkit;computer science;artificial intelligence;theoretical computer science;genetic operator;machine learning;genetic representation;algorithm;evolutionary computation	AI	27.751973314995787	-8.494512400857047	23353
130b9d67021bda19b8907d3005f18146a9f64ebc	accelerating geoscience and engineering system simulations on graphics hardware	parallel computing;computadora;tratamiento datos;linear algebra;computers;donnee meb;donnee rx;modele numerique;ponce;paper;dato meb;lattice boltzmann model;methode element fini;sismologie;dense linear algebra;seismology;numerical method;ordinateur;accelerators;earth and space sciences;performance;simulacion numerica;computation fluid dynamics;rock magnetism;data processing;roche ignee;traitement donnee;analyse moindres carres;finite element method;roca ignea;igneous rocks;onde sismique;magnetic force microscopy;finite element;onda sismica;metodo magnetico;roche volcanique;dinamica fluido;engineering system;cuda;sem data;least squares;geoscience;dato rx;seismic waves;propagacion;graphics hardware;numerical model;seismic wave propagation;volcanic rocks;methode magnetique;simulation numerique;spectral finite element method;least square;tomographie;emergent behavior;parallel computer;nvidia;pyroclastics;graphic processing unit;general purpose graphics processing units;geofluids;lattice boltzmann;fluid dynamics;performances;finite element analysis;numerical models;piedra pomez;pumice;tomografia;dynamique fluide;roca volcanica;system simulation;pyroclastite;least squares minimization;tomography;materiel informatique;magnetic methods;parallel processing;propagation;central processing unit;digital simulation;x ray data;hardware	Many complex natural systems studied in the geosciences are characterized by simple local-scale interactions that result in complex emergent behavior. Simulations of these systems, often implemented in parallel using standard central processing unit (CPU) clusters, may be better suited to parallel processing environments with large numbers of simple processors. Such an environment is found in graphics processing units (GPUs) on graphics cards. This paper discusses GPU implementations of three example applications from computational fluid dynamics, seismic wave propagation, and rock magnetism. These candidate applications involve important numerical modeling techniques, widely employed in physical system simulations, that are themselves examples of distinct computing classes identified as fundamental to scientific and engineering computing. The presented numerical methods (and respective computing classes they belong to) are: (1) a lattice-Boltzmann code for geofluid dynamics (structured grid class); (2) a spectralfinite-element code for seismic wave propagation simulations (sparse linear algebra class); and (3) a least-squares minimization code for interpreting magnetic force microscopy data (dense linear algebra class). Significant performance increases (between 10 and 30 in most cases) are seen in all three applications, demonstrating the power of GPU implementations for these types of simulations and, more generally, their associated computing classes. & 2009 Elsevier Ltd. All rights reserved.	algorithm;central processing unit;compiler;computation;computational fluid dynamics;computer graphics;computer simulation;emergence;emoticon;finite element method;general-purpose computing on graphics processing units;graphics hardware;graphics processing unit;interaction;lattice boltzmann methods;least squares;linear algebra;mit engineering systems division;maxima and minima;numerical analysis;numerical method;parallel computing;regular grid;software propagation;solver;sparse matrix;spectral element method;supercomputer;video card	Stuart D. C. Walsh;Martin O. Saar;Peter Bailey;David. J. Lilja	2009	Computers & Geosciences	10.1016/j.cageo.2009.05.001	simulation;computer science;finite element method;mathematics;tomography;algorithm;computer graphics (images)	HPC	44.54348545492427	3.4093444248241194	23378
636a9acf6907a416c3971d7fd455f23f8bdd9c8b	a novel discrete cosine transform algorithm for optimized hardware implementation in audio decoding	algorithm optimization circuit sharing audio decoding discrete cosine transform dct	This paper presents a novel algorithm of discrete cosine transform (DCT) for digital theatre system (DTS) and windows media audio (WMA) decoding based on the type IV discrete cosine transform (DCT-IV). By virtue of the format conversion, the DTS and WMA decoding shares the same DCT-IV kernel, thus increasing the efficiency. The computation of DCT-IV is based on fast Fourier transform (FFT) and the coefficients are calculated in recursive algorithm. The performance of the proposed algorithm is compared with other reported algorithms. Both the efficiency of multiplication computation and the simplicity of hardware implementation are largely improved.	algorithm;coefficient;computation;discrete cosine transform;fast fourier transform;microsoft windows;recursion (computer science);windows media	Junfeng Zhu;Dapeng Sun;Heng Yang	2011		10.1049/cp.2011.1066	discrete hartley transform;transform coding;speech recognition;lapped transform;modified discrete cosine transform;computer science;theoretical computer science;bruun's fft algorithm;discrete cosine transform;prime-factor fft algorithm;algorithm	EDA	45.78678963171779	-9.645264888196268	23403
0f72f4fddabc092bd3f40ceede682bc8c201e7d2	walsh and haar functions in genetic algorithms	walsh function;haar functions;walsh functions;efficiency of computation;theoretical analysis;genetic algorithm;genetic algorithms;fitness functions;fitness function	Theoretical analysis of fitness functions in genetic algorithms has included the use of Walsh functions [14]. They form a convenient basis for the expansion of fitness functions [31. These orthogonal, rectangular functions have also been used to compute the average fitness values of schemata [5]. This work explores the use of Haar functions [7] for the same purposes. While 2 t non-zero terms are required for the expansion of a given function as a linear combination of Walsh functions, at most e + 1 non-zero terms are required with the Haar expansion, where g is the size of each binary string in the solution space. Similarly, Ha'ucoefficients require less computation than their Walsh counterparts. The Iolal number of terms required for the expansion of the fitness function at a given point using Haar is of order 2 ~. substantially less than Walsh's 22~. A comparison of l-Iaar functionsand Walsh functions with respect to fitness averages shows that the use of Haar functions will reduce computation time. Furthermore, the advantage of Haar over Walsh functions remains large (of order g • 2 t) when fast transforms are used.	computation;feasible region;fitness function;genetic algorithm;haar wavelet;hadamard transform;time complexity	Sami Khuri	1994		10.1145/326619.326722	mathematical optimization;genetic algorithm;computer science;machine learning;walsh function;algorithm	Logic	32.53884128575706	-1.0623787501443154	23414
d9f6b24838820325494624115545726a39ad533d	mean and variance of the sampling distribution of particle swarm optimizers during stagnation	particle swarm;stagnation;sampling methods particle swarm optimization character generation stochastic processes computer science equations;swarm intelligence;methode particulaire;intelligence en essaim;optimizacion pso;echantillonnage;metodo particula;sampling distribution;approche deterministe;sampling;deterministic approach;particle method;qa75 electronic computers computer science;mouvement particule;stochasticity;particle swarm optimizer;stagnation particle swarm optimization pso theory sampling distribution;stochastic processes;stochastic processes particle swarm optimisation sampling methods;particle motion;particle swarm optimization;movimiento particula;character generation;enfoque determinista;pso theory;optimisation pso;algorithme evolutionniste;algoritmo evolucionista;computer science;evolutionary algorithm;reseau neuronal;sampling methods;muestreo;stochasticity sampling distribution particle swarm optimization stagnation;inteligencia de enjambre;particle swarm optimisation;red neuronal;variance;neural network;variancia	Several theoretical analyses of the dynamics of particle swarms have been offered in the literature over the last decade. Virtually all rely on substantial simplifications, often including the assumption that the particles are deterministic. This has prevented the exact characterization of the sampling distribution of the particle swarm optimizer (PSO). In this paper we introduce a novel method that allows us to exactly determine all the characteristics of a PSO sampling distribution and explain how it changes over any number of generations, in the presence stochasticity. The only assumption we make is stagnation, i.e., we study the sampling distribution produced by particles in search for a better personal best. We apply the analysis to the PSO with inertia weight, but the analysis is also valid for the PSO with constriction and other forms of PSO.	mathematical optimization;particle swarm optimization;sampling (signal processing);stochastic process	Riccardo Poli	2009	IEEE Transactions on Evolutionary Computation	10.1109/TEVC.2008.2011744	sampling;mathematical optimization;swarm intelligence;computer science;artificial intelligence;evolutionary algorithm;mathematics;particle swarm optimization;algorithm;statistics	Visualization	28.46405300862313	1.3375587938333922	23420
54773c5c80a45b615c679305d0be7bcdce9a00bd	frame-level rho-domain r-d optimization in h.264	iterative method;theorie vitesse distorsion;optimisation;tecnologia electronica telecomunicaciones;optimizacion;complexite calcul;algorithme glouton;r d optimization;simulation;simulacion;recherche developpement;metodo iterativo;rate distortion theory;codificacion;complejidad computacion;research and development;p domain;ρ domain;frame level;computational complexity;investigacion desarrollo;methode iterative;coding;h 264;greedy algorithm;algoritmo gloton;optimization;rapport signal bruit;relacion senal ruido;tecnologias;signal to noise ratio;grupo a;algoritmo optimo;algorithme optimal;optimal algorithm;codage	The frame-level R-D optimization in H.264 is very important in video storage scenarios. Among all of the sub-optimal algorithms, a greedy iteration algorithm (GIA) can best lower the computational complexity of frame-level R-D optimization. In order to further lower the computational complexity, a ρ-domain frame-level R-D optimization algorithm is proposed in this letter. Different from GIA, every frame's rate and distortion can be estimated accurately without actual encoding in our proposed algorithm. Simulation results show that our proposed algorithm can lower the computational complexity greatly with negligible variation in peak signal-to-noise ratio (PSNR) compared with GIA.	h.264/mpeg-4 avc	Yutao Dong;Xiangzhong Fang;Jing Yang	2007	IEICE Transactions	10.1093/ietisy/e90-d.5.872	greedy algorithm;average-case complexity;rate–distortion theory;computer science;artificial intelligence;calculus;mathematics;iterative method;coding;computational complexity theory;signal-to-noise ratio;algorithm	Vision	47.14245779048803	-15.583368351185815	23436
80dd789773e24b38a770f97a6e68d4b9e72527ad	runtime analysis comparison of two fitness functions on a memetic algorithm for the clique problem	runtime algorithm design and analysis polynomials memetics search problems blogs evolutionary computation;evolutionary algorithms runtime analysis fitness functions memetic algorithm clique problem global optimum solution 1 1 restart memetic algorithm best improvement local search 1 1 rma graphs g 1 family;search problems evolutionary computation graph theory	It is commonly accepted that a proper fitness function can guide the algorithm to find a global optimum solution faster. This paper will use the runtime analysis to provide the theoretical evidence that a small change of the fitness function (additional one step looking forward) can result in a huge performance gap in terms of finding a global optimum solution. It also shows that the fitness function that gives the best results in an Memetic Algorithm on the Clique Problem is entirely instance specific. In detail, we will formalize a (1+1) Restart Memetic Algorithm with a Best-Improvement Local Search, and run them on two different fitness functions, fOL and fOPL, to solve the Clique Problem respectively. We then construct two families of graphs, G1 and G2, and show that, for the first family of graphs G1, the (1+1) RMA on the fitness function fOPL drastically outperforms the (1+1) RMA on the fitness function fOL, and vice versa for the second family of graphs G2.	analysis of algorithms;clique problem;first-order logic;fitness function;global optimization;like button;local search (optimization);memetic algorithm;memetics;revolution in military affairs;run time (program lifecycle phase)	Kuai Wei;Michael J. Dinneen	2014	2014 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2014.6900359	mathematical optimization;combinatorics;interactive evolutionary computation;machine learning;mathematics;memetic algorithm	ML	24.87350669107408	4.174636921036032	23483
aea575c8768e0cd5aceedf563e24a2fc18d928ed	least significant qubit algorithm for quantum images	least significant qubit;unitary operator;color digital images	To study the feasibility of the classical image least significant bit (LSB) information hiding algorithm on quantum computer, a least significant qubit (LSQb) information hiding algorithm of quantum image is proposed. In this paper, we focus on a novel quantum representation for color digital images (NCQI). Firstly, by designing the three qubits comparator and unitary operators, the reasonability and feasibility of LSQb based on NCQI are presented. Then, the concrete LSQb information hiding algorithm is proposed, which can realize the aim of embedding the secret qubits into the least significant qubits of RGB channels of quantum cover image. Quantum circuit of the LSQb information hiding algorithm is also illustrated. Furthermore, the secrets extracting algorithm and circuit are illustrated through utilizing control-swap gates. The two merits of our algorithm are: (1) it is absolutely blind and (2) when extracting secret binary qubits, it does not need any quantummeasurement operation or any other help from classical computer. Finally, simulation and comparative analysis show the performance of our algorithm.	algorithm;comparator;digital image;embedded system;information extraction;least significant bit;measurement in quantum mechanics;most significant bit;paging;pixel;qualitative comparative analysis;quantum circuit;quantum computing;quantum gate;qubit;simulation;steganography	Jianzhi Sang;Shen Yun Wang;Qiong Li	2016	Quantum Information Processing	10.1007/s11128-016-1411-z	quantum fourier transform;quantum information;theoretical computer science;quantum network;mathematics;quantum channel;quantum computer;quantum algorithm;algorithm;one-way quantum computer;quantum mechanics;quantum phase estimation algorithm;quantum sort;quantum error correction	EDA	38.55889694393045	-8.824946066770014	23520
1178c890c51d253a9deef3589133e46c929b4a35	improved reversible data hiding based on residue histogram shifting	image coding;difference expansion transform;histogram shifting;psnr reversible data hiding grayscale images secret data capacity difference expansion transform residue histogram shifting complex image image quality;reversible hiding;residue number system reversible hiding difference expansion transform histogram shifting;data encapsulation;image coding data encapsulation;residue number system;histograms equations mathematical model image restoration image quality data mining standards	This paper proposes an improved reversible data hiding method for grayscale images. Specifically, this method integrates the superiority of two existing schemes to improve secret data capacity. the main idea of this method is to classify image blocks into smooth or complex ones and then to implement disparate reversible data hiding schemes on them. in data embedding process, our hybrid method uses difference expansion transform and residue histogram shifting to carry secret data adaptively without any distortions. Experimental results show that the proposed method improves the problem of low capacity for complex image from 0.02 to 0.39 bpp. at the same time, it indeed provides a good image quality of PSNR in comparisons with previous methods.	distortion;grayscale;image quality;peak signal-to-noise ratio;pitch shift;residue number system;steganography	Wen-Chuan Wu;Yu-Chieh Wu	2012	2012 Sixth International Conference on Genetic and Evolutionary Computing	10.1109/ICGEC.2012.104	computer vision;residue number system;histogram matching;theoretical computer science;data mining;mathematics;image histogram	Robotics	40.11149486554512	-12.204465808170701	23566
ed90705d128c0fe5677350f5a9505372423d784a	theory of swarm intelligence		Social animals as found in fish schools, bird flocks, bee hives, and ant colonies are able to solve highly complex problems in nature. This includes foraging for food, constructing astonishingly complex nests, and evading or defending against predators. Remarkably, these animals in many cases use very simple, decentralized communication mechanisms that do not require a single leader. This makes the animals perform surprisingly well, even in dynamically changing environments. The collective intelligence of such animals is known as swarm intelligence and it has inspired popular and very powerful optimization paradigms, including ant colony optimization (ACO) and particle swarm optimization (PSO).  The reasons behind their success are often elusive. We are just beginning to understand when and why swarm intelligence algorithms perform well, and how to use swarm intelligence most effectively. Understanding the fundamental working principles that determine their efficiency is a major challenge.  This tutorial will give a comprehensive overview of recent theoretical results on swarm intelligence algorithms, with an emphasis on their efficiency (runtime/computational complexity). In particular, the tutorial will show how techniques for the analysis of evolutionary algorithms can be used to analyze swarm intelligence algorithms and how the performance of swarm intelligence algorithms compares to that of evolutionary algorithms.  The results shed light on the working principles of swarm intelligence algorithms, identify the impact of parameters and other design choices on performance, and thus help to use swarm intelligence more effectively.  The tutorial will be divided into a first, larger part on ACO and a second, smaller part on PSO. For ACO we will consider simple variants of the MAX-MIN ant system. Investigations of example functions in pseudo-Boolean optimization demonstrate that the choices of the pheromone update strategy and the evaporation rate have a drastic impact on the running time. We further consider the performance of ACO on illustrative problems from combinatorial optimization: constructing minimum spanning trees, solving shortest path problems with and without noise, and finding short tours for the TSP.  For particle swarm optimization, the tutorial will cover results on PSO for pseudo-Boolean optimization as well as a discussion of theoretical results in continuous spaces.	ant colony optimization algorithms;collective intelligence;combinatorial optimization;computational complexity theory;evaporation;evolutionary algorithm;file spanning;flocking (behavior);mathematical optimization;max;minimum spanning tree;particle swarm optimization;shortest path problem;swarm intelligence;time complexity	Dirk Sudholt	2015		10.1145/2739482.2756570		AI	25.27853241450322	-5.293320185722656	23596
1f30f7f30ba5db9847c609c85ce4b5feedbba507	an adaptive high-speed lossy data compression	rate distortion;data compression;local adaptation;computer architecture;a priori knowledge;vlsi data compression digital signal processing chips encoding;adaptive method;vlsi;on the fly;compression ratio;digital signal processing chips;vector quantizer;encoding;data compression rate distortion image coding vector quantization statistics very large scale integration signal processing image processing computer architecture decoding;compression ratio high speed lossy data compression adaptive method source statistics codebook training local textual features rate distortion function memoryless sources computing architecture vector quantizer encoded data generator high speed vlsi processor;high speed;vlsi architecture	An adaptive method for lossy data compression and the associated VLSI architecture have been developed. This scheme does not require a-priori knowledge of the source statistics and codebook training. The codebook is generated on the fly and is constantly updated to capture local textual features of data. The algorithm is proven to reach rate distortion function for memoryless sources. The authors also propose a computing architecture which consists of a vector quantizer and an encoded-data generator. By using this method, a high-speed VLSI processor with good local adaptivity, less complexity and fair compression ratio can be achieved.<<ETX>>	algorithm;codebook;computer architecture;data compression;distortion;lossy compression;on the fly;quantization (signal processing);rate–distortion theory;very-large-scale integration	Oscal T.-C. Chen;Z. Zhang;Bing J. Sheu	1992	Data Compression Conference, 1992.	10.1109/DCC.1992.227446	data compression;computer vision;data compression ratio;a priori and a posteriori;computer science;theoretical computer science;compression ratio;very-large-scale integration;linde–buzo–gray algorithm;encoding;statistics	ML	49.35574648770425	-12.424194075637141	23600
1daaf84c649798af9adcc7414ce61aa574d5c3f0	a hybrid dct-svd image-coding algorithm	eigenvalues and eigenfunctions;metodo adaptativo;evaluation performance;decomposition valeur singuliere;image coding;performance evaluation;image processing;evaluacion prestacion;transformation cosinus discrete;online adaptive vector quantization hybrid dct svd image coding algorithm discrete cosine transform singular value decomposition source image correlation high frequency content simulation results distortion bit rate image quality;singular value decomposition;simulation;correlation methods image coding transform coding vector quantisation discrete cosine transforms singular value decomposition;procesamiento imagen;high frequency content;online adaptive vector quantization;simulacion;methode adaptative;transform coding;correlation methods;indexing terms;bit rate;traitement image;discrete cosine transform;algorithme;rate distortion theory;source image;algorithm;codage image;distortion;adaptive vector quantization;discrete cosine transforms eigenvalues and eigenfunctions matrix decomposition image coding discrete transforms singular value decomposition vector quantization transform coding rate distortion theory bit rate;cuantificacion vectorial;spatial correlation;vector quantization;discrete transforms;matrix decomposition;discrete cosine transforms;image quality;adaptive method;decomposicion valor singular;correlation;vector quantisation;simulation results;high frequency;hybrid dct svd image coding algorithm;hybrid algorithm;algoritmo;quantification vectorielle	We propose an image-coding algorithm which combines the discrete cosine transform (DCT) and the singular value decomposition (SVD). The DCT is used to transform those blocks in the source image that exhibit a high correlation, while blocks with greater high-frequency content are transformed using the SVD. A simple criterion is used to decide which transform should be used on each block. Simulation results show that the new hybrid algorithm provides good distortion, bit rate, and image quality-especially in images which are less spatially correlated.	algorithm;discrete cosine transform;singular value decomposition	Adriana Dapena;Stanley C. Ahalt	2002	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.988658	image quality;mathematical optimization;discrete mathematics;spatial correlation;transform coding;index term;distortion;rate–distortion theory;hybrid algorithm;image processing;computer science;theoretical computer science;discrete cosine transform;high frequency;mathematics;matrix decomposition;singular value decomposition;correlation;vector quantization;algorithm;statistics	EDA	45.740518632623406	-13.886756812826478	23651
24978c674b2c6b8847c46c25172ffaa61a6b71a2	racing with a fixed budget and a self-adaptive significance level		F-Race is an offline parameter tuning method which efficiently allocates samples in order to identify the parameter setting with the best expected performance, out of a given set of parameter settings. Using non parametric statistical tests, F-Race discards parameter settings which perform significantly worse than the current best, allowing the surviving parameter settings to be tested on more instances and hence obtaining better estimates for their performance. The statistical tests require setting significance levels which directly affect the algorithm’s ability of detecting the best parameter setting, and the total runtime. In this paper, we show that it is not straightforward to set the significance level and propose a simple modification to automatically adapt the significance level such that the failure rate is minimized. This is tested empirically using data drawn from probability distributions with pre-defined characteristics. Results indicate that, under a strict computational budget, F-Race with online adaptation performs significantly better than its counterpart with even the best fixed value.	algorithm;computation;experiment;failure rate;mathematical optimization;online and offline;race condition;sensor	Juergen Branke;Jawad Elomari	2013		10.1007/978-3-642-44973-4_29	econometrics;simulation;computer science;statistics	NLP	27.39300478400581	-18.77110794401827	23674
2623048ef4d622a7613be244b754b5fef59cf67e	expected utility from multinomial second-order probability distributions	imprecise probability;computer science	We consider the problem of maximizing expected utility when utilities and probabilities are given by discrete probability distributions so that expected utility is a discrete stochastic variable. As for discrete second-order distributions, that is probability distributions where the variables are themselves probabilities, the multinomial family is a reasonable choice at least if first-order probabilities are interpreted as relative frequencies. We suggest a decision rule that reflects the uncertainty present in distribution-based probabilities and utilities and we show an example of this rule in action with multinomial second-order distributions.	expected utility hypothesis;first-order predicate;multinomial logistic regression;rule 90	David Sundgren	2010	Polibits		probability distribution;marginal distribution;dirichlet distribution;random variable;probability and statistics;econometrics;geometric distribution;probability mass function;imprecise probability;conditional probability;convolution of probability distributions;symmetric probability distribution;regular conditional probability;statistical parameter;circular distribution;pattern recognition;mathematics;law of total probability;location parameter;posterior probability;joint probability distribution;k-distribution;empirical probability;statistics	ML	30.90347137229622	-19.19044358282518	23693
5ff3df721203ee74ca18cebe01aeb0978366e582	a digital image watermarking method based on host image analysis and genetic algorithm		This paper mainly comes up with a new watermarking method based on host image analysis and genetic algorithm. Through the characteristics of human visual system, the host image analysis can ensure the imperceptibility. It greatly enhance the embedding capacity of the watermark under the same visual effect, which can embed more watermark in the host image. The genetic algorithm is used in embedding process. It can not only improve the image quality, but also enhance the security and robustness of the watermarked image in a large extent. This algorithm satisfies an optimal compromise between the robustness and image quality. Besides that, the experimental results show that this method has great influence on transparency and robustness.	digital image;digital watermarking;genetic algorithm;image analysis	Jialing Han;Xiaohui Zhao;Chunyan Qiu	2016	J. Ambient Intelligence and Humanized Computing	10.1007/s12652-015-0298-3	computer vision;computer science;theoretical computer science;world wide web	AI	40.45126158426437	-10.950511833186225	23714
01dd756b6ce0b12cfedf4207145ccd5c34a79ef2	rounding transform based approach for lossless subband coding	image coding filter bank matrix decomposition entropy;image coding;filter bank;data compression;band pass filters;lossless image compression;z transforms;lossless first order entropy rounding transform based approach lossless subband coding overlapping rt ort z transform domain polyphase matrix analysis filter bank ort matrices lifting representation filter bank lossless image compression;transform coding;matrix algebra;high pass filters;first order;band pass filters transform coding image coding data compression z transforms matrix algebra low pass filters high pass filters;low pass filters;subband coding	In this paper, a new approach is proposed, the rounding transform (RT) based lossless subband coding. For the such purpose, an extended version of RT, namely overlapping RT (ORT), is defined in the Z-transform domain and used to develop the lossless subband coding system. The main idea is that the polyphase matrix of the analysis filter bank is decomposed into several ORT matrices of size 2 by 2. Compared to the previous scheme (the lifting), the proposed scheme gives not only simpler mathematical representation, but also allows various possibilities when a given filter bank is implemented. As examples, five filter banks are applied to lossless image compression by the proposed ORT and their performances are compared in terms of the total lossless first order entropy.	lossless compression;rounding;sub-band coding	Ho-Youl Jung;Rémy Prost	1997		10.1109/ICIP.1997.638741	data compression;lossy compression;arithmetic;sub-band coding;lossless jpeg;computer vision;z-transform;transform coding;low-pass filter;computer science;entropy encoding;theoretical computer science;context-adaptive variable-length coding;first-order logic;filter bank;mathematics;band-pass filter;lossless compression;adaptive coding;context-adaptive binary arithmetic coding;high-pass filter;statistics	Theory	43.446603956810506	-14.496955484499006	23767
a2ecb6becafa5a7a2f1e353833771d07c8ade839	fast gradient descent method for mean-cvar optimization	conditional value at risk;portfolio optimization	We propose an iterative gradient descent procedure for computing approximate solutions for the scenario-based mean-CVaR portfolio selection problem. This procedure is based on an algorithm proposed by Nesterov [13] for solving non-smooth convex optimization problems. Our procedure does not require any linear programming solver and in many cases the iterative steps can be solved in closed form. We show that this method is significantly superior to the linear programming approach as the number of scenarios becomes large.	approximation algorithm;cvar;convex optimization;experiment;gradient descent;iterative method;linear programming;mathematical optimization;numerical analysis;selection algorithm;solver	Garud Iyengar;Alfred Ka Chun Ma	2013	Annals OR	10.1007/s10479-012-1245-8	gradient descent;mathematical optimization;expected shortfall;gradient method;machine learning;portfolio optimization;mathematics;stochastic gradient descent;mathematical economics	ML	33.29310017392593	3.824490064273239	23778
acedc9e280fe87650fdaa757d856bbca476e435c	nonlinear model predictive control based on collective neurodynamic optimization	recurrent neural networks rnns collective neurodynamic optimization model predictive control mpc;search problems concave programming neurocontrollers nonlinear control systems particle swarm optimisation predictive control recurrent neural nets;vectors;predictive models;optimization;recurrent neural networks;optimization neurodynamics vectors biological neural networks recurrent neural networks predictive models;neurodynamics;biological neural networks;precise local search nonlinear model predictive control nmpc entails sequential global optimization problem nonconvex cost function collective neurodynamic optimization approach recurrent neural networks rnns global optimization problems constrained local search local best known solution global best known solution particle swarm optimization global optimal solutions global search	In general, nonlinear model predictive control (NMPC) entails solving a sequential global optimization problem with a nonconvex cost function or constraints. This paper presents a novel collective neurodynamic optimization approach to NMPC without linearization. Utilizing a group of recurrent neural networks (RNNs), the proposed collective neurodynamic optimization approach searches for optimal solutions to global optimization problems by emulating brainstorming. Each RNN is guaranteed to converge to a candidate solution by performing constrained local search. By exchanging information and iteratively improving the starting and restarting points of each RNN using the information of local and global best known solutions in a framework of particle swarm optimization, the group of RNNs is able to reach global optimal solutions to global optimization problems. The essence of the proposed collective neurodynamic optimization approach lies in the integration of capabilities of global search and precise local search. The simulation results of many cases are discussed to substantiate the effectiveness and the characteristics of the proposed approach.	approximation;artificial neural network;converge;emulator;global optimization;local search (optimization);loss function;mathematical optimization;mesenchymal stem cells;minimax;neural network simulation;neural oscillation;nonlinear programming;nonlinear system;optimal control;optimization problem;particle swarm optimization;performance;random neural network;recurrent neural network;solutions	Zheng Yan;Jun Wang	2015	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2014.2387862	mathematical optimization;multi-swarm optimization;computer science;artificial intelligence;recurrent neural network;machine learning;mathematics;predictive modelling;metaheuristic;global optimization	ML	32.51336163997248	-7.391031206828885	23793
a5f829a8d105cc4fb7476ed701c6ae112c81f357	a distributed spectral-screening pct algorithm	imaging spectrometer;prediccion;capteur imagerie hyperspectral;criblage;analisis componente principal;sensor hiperespectral de formacion de imagenes;shared memory;image resolution;screening;high performance networks;hyper spectral;hyperspectral imaging sensor;spectral angle classification;medical image analysis;remote sensing data;human visual system;clasificacion espectral;principal component analysis;remote sensing;image quality;algorithme reparti;performance model;analyse composante principale;cernido;performance prediction;algoritmo repartido;principal component transform;classification spectrale;distributed algorithm;prediction;analytical model;spectral classification;principal component;shared memory multiprocessor	code for the sensor is shown in Program 2. It repeatedly obt ains multi-spectral image cubes from the sensor (1), waits for an appropriate reques t for work from a worker (2), decomposes the image cube to generate an unassigned sub-cube (3) and sends the sub-cube to the requesting worker (4). Program 2: Sensor Thread Operation main() { mp = get_my_multiprocessor_id() if(mp == 0) { numsubcubes = get_num_subcubes() sensor(numsubcubes) manager(numsubcubes) } foreach remaining available processor worker() } sensor() { while(sensor device operating) { cube = grab_cube() /* 1 */ while(subcubes available) { request = recv(aworker) /* 2 */ work = generate_subcube(cube) /* 3 */ send(aworker, work) /* 4 */ } } }	algorithm;entry point;foreach loop;multispectral image;olap cube;waits	Tiranee Achalakul;Stephen Taylor	2003	J. Parallel Distrib. Comput.	10.1016/S0743-7315(03)00017-0	distributed algorithm;computer vision;simulation;computer science;operating system;statistics;principal component analysis	DB	38.53328719307654	-20.12641775749876	23794
2092540effc88bbac6f1fb93d656ea8e5d1ef93c	locating phase transitions in computationally hard problems	new applications of statistical mechanics;statistical mechanics;travelling salesman problem;utility function;analysis of algorithm;anytime algorithm;analysis of algorithms;critical exponent;statistical physics;phase transition;response function;critical phenomena;phase transitions and critical phenomena;heuristics	We discuss how phase-transitions may be detected in computationally hard problems in the context of Anytime Algorithms. Treating the computational time, value and utility functions involved in the search results in analogy with quantities in statistical physics, we indicate how the onset of a computationally hard regime can be detected and the transit to higher quality solutions be quantified by an appropriate response function. The existence of a dynamical critical exponent is shown, enabling one to predict the onset of critical slowing down, rather than finding it after the event, in the specific case of a Travelling Salesman Problem. This can be used as a means of improving efficiency and speed in searches, and avoiding needless computations.	anytime algorithm;computation;dynamical system;frequency response;onset (audio);time complexity;travelling salesman problem	B. Ashok;T. K. Patra	2009	CoRR	10.1007/s12043-010-0138-0	phase transition;statistical mechanics;analysis of algorithms;heuristics;critical phenomena;travelling salesman problem;critical exponent;physics;quantum mechanics	AI	27.992202704169816	3.60293851080299	23834
08ca99a7f5c48995a1cc1dd8f788ab81f121f1c2	convergence properties of an interval probabilistic approach to system reliability estimation	system reliability;lower and upper bound;monte carlo sampling;interval probability;system modeling;computer model;evidence theory;probabilistic approach;random intervals;complex system;dempster shafer;reliability analysis;dempster shafer evidence theory;cumulant;random set;random sets;interval analysis	Based on a black box model of a complex system, and on intervals and probabilities describing the known information about the inputs, we want to estimate the system’s reliability. This problem is motivated by a number of problem areas, most specifically in engineering reliability analysis under conditions of poor measurement and high complexity of system models. Using the results of tests performed on the system’s computer model, we can estimate the lower and upper bounds of the probability that the system is in a desirable state. This is equivalent to using Monte-Carlo sampling to estimate cumulative belief and plausibility values of functionally propagated finite random intervals. In this paper, we prove that these estimates are correct in the sense that under reasonable assumptions, these estimates converge to the actual probability bounds.	black box;complex system;computer simulation;converge	Cliff Joslyn;Vladik Kreinovich	2005	Int. J. General Systems	10.1080/03081070500033880	econometrics;complex systems;systems modeling;dempster–shafer theory;machine learning;mathematics;statistics;monte carlo method;cumulant	DB	29.39916881535633	-17.125956230308503	23835
d58a4b2a6a3c1630f29250f3c8216199321956f7	compressive sensing based image compression-encryption using novel 1d-chaotic map	chaotic map;compressed sensing;compression;encryption;security	Compressive sensing based encryption achieves simultaneous compression-encryption by utilizing a low complex sampling process, which is computationally secure. In this paper, a new novel 1D–chaotic map is proposed that is used to construct an incoherence rotated chaotic measurement matrix. The chaotic property of the proposed map is experimentally analysed. The linear measurements obtained are confused and diffused using the chaotic sequence generated using the proposed map. The chaos based measurement matrix construction results in reduced data storage and bandwidth requirements. As it needs to store only the parameters required to generate the chaotic sequence. Also, the sensitivity of the chaos to the parameters makes the data transmission secure. The secret key used in the encryption process is dependent on both the input data and the parameter used to generate the chaotic map. Hence the proposed scheme can resist chosen plaintext attack. The key space of the proposed scheme is large enough to thwart statistical attacks. Experimental results and the security analysis verifies the security and effectiveness of the proposed compression-encryption scheme.	algorithm;brute-force attack;chaos theory;compressed sensing;computer data storage;decibel;encryption;experiment;image compression;key (cryptography);key schedule;key space (cryptography);peak signal-to-noise ratio;plaintext;requirement;sampling (signal processing);scheduling (computing);simulation;standard test image	R. Ponuma;R. Amutha	2017	Multimedia Tools and Applications	10.1007/s11042-017-5378-2	image compression;compressed sensing;computer science;theoretical computer science;encryption;chosen-plaintext attack;matrix (mathematics);key space;computer data storage;bandwidth (signal processing)	Security	38.710025029408584	-9.131298302096605	23849
584858713eb7536561d5916e3e6e5a53a1ec2f17	motion-compensated video coding with adaptive perceptual quantization	perceptual quality;motion picture experts group;1 0 mbit s mpeg quantizer motion compensated video coding adaptive perceptual quantization motion picture experts group bit rates reconstructed video mpeg syntax encoder one pass causal scheme prestored models perceptual quality locally varying scene characteristics luminance;picture processing;video coding quantization discrete cosine transforms video compression transform coding motion pictures bit rate standards organizations hdtv decoding;motion compensated;video coding;video signals encoding picture processing;video signals;encoding	The authors address the problem of adapting the Motion Picture Experts Group (MPEG) quantizer for scenes of different complexity (at bit rates around 1 Mb/s), such that the perceptual quality of the reconstructed video is optimized. Adaptive quantisation techniques conforming to the MPEG syntax can significantly improve the performance of the encoder. The authors concentrate on a one-pass causal scheme to limit the complexity of the encoder. The system employs prestored models for perceptual quality and a bit rate that have been experimentally derived. A framework is provided for determining these models as well as adapting them to locally varying scene characteristics. The variance of an 8*8 (luminance) block is basic to the techniques developed. Following standard practice, it is defined as the average of the square of the deviations of the pixels in the block from the mean pixel value. >	color quantization;data compression	Atul Puri;Rangarajan Aravind	1991	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.120774	video compression picture types;computer vision;speech recognition;computer science;video quality;motion estimation;block-matching algorithm;multimedia;video processing;rate–distortion optimization;motion compensation;h.262/mpeg-2 part 2;h.261;encoding;multiview video coding	Vision	46.20154945710817	-17.45270795262363	23850
1f0a7bd02c45f42127174bfc50335c2c87d934f9	a roi-based high capacity reversible data hiding scheme with contrast enhancement for medical images		In this paper, we attempt to investigate the secure archiving of medical images which are stored on semi-trusted cloud servers, and focus on addressing the complicated and challenging integrity control and privacy preservation issues. With the intention of protecting the medical images stored on a semi-trusted server, a novel ROI-based high capacity reversible data hiding (RDH) scheme with contrast enhancement is proposed in this paper. The proposed method aims at improving the quality of the medical images effectively and embedding high capacity data reversibly meanwhile. Therefore, the proposed method adopts “adaptive threshold detector” (ATD) segmentation algorithm to automatically separate the “region of interest” (ROI) and “region of non-interest” (NROI) at first, then enhances the contrast of the ROI region by stretching the grayscale and embeds the data into peak bins of the stretched histogram without extending the histogram bins. Lastly, the rest of the required large of data are embedded into NROI region regardless its quality. In addition, the proposed method records the edge location of the segmentation instead of recording the location of the overflow and underflow. The experiment shows that the proposed method can improve the quality of medical images obviously whatever in low embedding rate or high embedding rate when compared with other contrast-based RDH methods.	adaptive histogram equalization;algorithm;approximation;archive;arithmetic underflow;cloud computing;comparator applications;dvd region code;embedded system;grayscale;lossless compression;medical imaging;peak signal-to-noise ratio;region of interest;semiconductor industry;server (computing)	Yang Yang;Weiming Zhang;Dong Liang;Nenghai Yu	2017	Multimedia Tools and Applications	10.1007/s11042-017-4444-0	arithmetic underflow;information hiding;computer vision;artificial intelligence;grayscale;computer science;pattern recognition;region of interest;histogram;server;image segmentation;segmentation	Vision	39.84722368293027	-12.104565714030384	23874
b23294d94bf532e8c84dc1af7a2621768bd53540	reversible data hiding scheme with high embedding capacity using semi-indicator-free strategy	image coding;data encapsulation;side matched vector quantization data hiding;vector quantisation data encapsulation image coding security of data;side matched vector quantization compressed image reversible data hiding high embedding capacity semiindicator free strategy secret data embedding;indexes image coding bit rate encoding streaming media image restoration image reconstruction;vector quantisation;security of data	This work presents a novel reversible data-hiding scheme which embeds secret data into a side matched Vector Quantization (SMVQ)-compressed image, and achieves lossless reconstruction of a Vector Quantization (VQ)-compressed image. The rather random distributed histogram of a VQ-compressed image can be re-located to locations close to zero by SMVQ prediction. Thus, fewer bits can be used to encode SMVQ indices with very small values, and no indicator is required to encode these indices, which yields extra hiding space to hide secret data. Consequently, high embedding capacity and low bit rate scenarios can be deposited. Experimental results demonstrate the effectiveness and reversibility of the proposed scheme. Moreover, in terms of the embedding rate, the bit rate, and the embedding capacity, experimental results show that the performance of the proposed scheme is better than those of the former data hiding schemes for VQ-based and VQ/SMVQ-based compressed images.		Jiann-Der Lee;Yaw-Hwang Chiou;Jing-Ming Guo	2013	SiPS 2013 Proceedings	10.1109/SiPS.2013.6674522	computer vision;discrete mathematics;theoretical computer science;mathematics	Vision	40.30858870817813	-12.170288913590609	23879
475d85fd875f55f413476bdda906b984b78f604a	ant colony optimization with different crossover schemes for continuous optimization		In this paper we present three ant colony optimization (ACOR) with different crossover operations to solve the continuous optimization problems. Crossover operations in the genetic algorithm are employed to generate some new probability density function set (PDFs) of ACOR in the promising space, which is aimed at improving the global exploration ability of ACOR, and avoiding falling into the local minima and exploiting the correlation information among the design variables. The proposed algorithm is evaluated on some benchmark functions and the simulation results show that the proposed algorithm performs quite well and outperforms other algorithms.	ant colony optimization algorithms;benchmark (computing);continuous optimization;genetic algorithm;mathematical optimization;maxima and minima;portable document format;program optimization;simulation	Zhiqiang Chen;Ying Jiang;Ronglong Wang	2015		10.1007/978-3-662-49014-3_5	multi-swarm optimization;ant colony optimization algorithms;metaheuristic	EDA	26.800996244929596	-4.428638496937031	23904
143f598652988860788225c0298e7f350198ce85	a note on the posterior expected loss as a measure of accuracy in bayesian methods	bayes estimation;computer program;casual probabilistic networks;metodo estadistico;densite probabilite;bayesian network;stochastic process;posterior risk;probability density;erreur quadratique moyenne;prior information;statistical method;risque;reseau;fonction perte;funcion perdida;mathematical expectation;bayesian method;red;perdida;densidad probabilidad;bayes estimator;algorithme;algorithm;reseau bayes;riesgo;estimacion bayes;estimation erreur;posterior distribution;error estimation;risk;methode statistique;loss function;mean square error;risque a posteriori;estimacion error;inferencia;processus stochastique;bayes network;ley a posteriori;posterior expected loss;fortran;esperanza matematica;error medio cuadratico;proceso estocastico;loss;programa computador;loi a posteriori;perte;inference;programme ordinateur;network;probabilistic network;esperance mathematique;estimation bayes;variance;variancia;algoritmo;bayesian networks	For many Bayesians, the posterior risk (posterior expected loss), which is a function of the data and the prior information, is the only measure of accuracy of interest. It is shown here that the value of this measure of accuracy may increase (uncertainty increases) as more data are obtained. The value of the posterior variance, which the posterior risk when the loss function is the squared error one, may exceed the value of the initial measure of accuracy, which is the prior variance. A specific example is analyzed numerically using Fortran language.	bayesian network	Mohammad Fraiwan Al-Saleh;Fawaz A. Masoud	2003	Applied Mathematics and Computation	10.1016/S0096-3003(01)00298-3	stochastic process;econometrics;pattern recognition;bayesian network;mathematics;statistics	ML	32.623980711217975	-22.680845774864633	23932
1f0d02a7e3c4898b328263862b12bf1b2502df49	calypso: a method for crystal structure prediction	structure prediction;crystal structure;particle swarm optimization algorithm	a b s t r a c t We have developed a software package CALYPSO (Crystal structure AnaLYsis by Particle Swarm Optimization) to predict the energetically stable/metastable crystal structures of materials at given chemical compositions and external conditions (e.g., pressure). The CALYPSO method is based on several major techniques (e.g. particle-swarm optimization algorithm, symmetry constraints on structural generation, bond characterization matrix on elimination of similar structures, partial random structures per generation on enhancing structural diversity, and penalty function, etc.) for global structural minimization from scratch. All of these techniques have been demonstrated to be critical to the prediction of global stable structure. We have implemented these techniques into the CALYPSO code. Testing of the code on many known and unknown systems shows high efficiency and the highly successful rate of this CALYPSO method [Y. Wang, J. Lv, L. Zhu, Y. Ma, Phys. Rev. B 82 (2010) 094116] [29]. In this paper, we focus on descriptions of the implementation of CALYPSO code and why it works.	crystal structure prediction	Yanchao Wang;Jian Lv;Li Zhu;Yanming Ma	2012	Computer Physics Communications	10.1016/j.cpc.2012.05.008	simulation;crystal structure	ML	30.474012527504506	-8.285392528643696	23949
73fdefb0a1e9a539e78b705cf6486574076915b2	an annealing framework with learning memory	boltzmann equation;combinatorial optimization learning memory simulated annealing markov chain sequence configuration space boltzmann style probability qualitative analyses quantitative analyses;combinatorial optimization problem;indexing terms;simulated annealing;configuration space;boltzmann equation simulated annealing learning artificial intelligence markov processes;markov processes;learning artificial intelligence;combinatorial optimization;simulated annealing space technology temperature control space exploration sampling methods temperature distribution mathematical model convergence probability distribution microscopy;mean field annealing;markov chain	Simulated annealing can be viewed as a process that generates a sequence of Markov chains, i.e., it keeps no memory about the states visited in the past of the process. This property makes simulated annealing time-consuming in exploring needless states and difficult in controlling the temperature and transition number. In this paper, we propose a new annealing model with memory that records important information about the states visited in the past. After mapping applications onto a physical system containing particles with discrete states, the new annealing method systematically explores the configuration space, learns the energy information of it, and converges to a well-optimized state. Such energy information is encoded in a learning scheme. The scheme generates states distributed in Boltzmann-style probability according to the energy information recorded in it. Moreover, with the assistance of the learning scheme, controlling over the annealing process become simple and deterministic. From qualitative and quantitative analyses in this paper, we can see that this convenient framework provides an efficient technique for combinatorial optimization problems and good confidence in the solution quality.	combinatorial optimization;markov chain;mathematical optimization;simulated annealing;simulation	Chun-Chi Lo;Ching-Chi Hsu	1998	IEEE Trans. Systems, Man, and Cybernetics, Part A	10.1109/3468.709611	quantum annealing;configuration space;markov chain;mathematical optimization;boltzmann equation;index term;simulated annealing;combinatorial optimization;computer science;artificial intelligence;theoretical computer science;machine learning;mathematics;markov process;adaptive simulated annealing;statistics	ML	27.50984858232799	-12.417361412073374	24019
88d4c5917d71742dd1c3f383e556364cd70f1751	fast coding unit depth decision method for hevc encoders			encoder;high efficiency video coding	Chou-Chen Wang;Hsiang-Chun Wang;Ming-Shum Jhou	2014		10.3233/978-1-61499-484-8-1287	parallel computing;coding (social sciences);encoder;computer science;decision model	Vision	44.29997850513372	-21.68686228847947	24082
3b8f1ff8ee817bb929b6a676e2fbb4cd74d159d9	3-d medical image compression using 3-d wavelet coders	medical image compression;3 d spiht;cohen daubechies feauveau;decoupled wavelet transform;symmetric wavelet transform;3 d speck and 3 d bisk;magnetic resonance image;wavelet transform;medical image;peak signal to noise ratio;indexation;structural similarity;x rays	This paper presents compression of 3-D medical images using 3-D wavelet encoders. Four wavelet transforms, namely, Daubechies 4, Daubechies 6, Cohen-Daubechies-Feauveau 9/7 and Cohen-Daubechies-Feauveau 5/3 are used in the first stage with encoders such as 3-D SPIHT, 3-D SPECK and 3-D BISK used in the second stage for the compression and the optimal wavelet-encoder combination is identified. Two versions of wavelet transform, symmetric and decoupled wavelet transform are considered. Experiments are performed using medical test images such as magnetic resonance images (MRI) and X-ray angiograms (XA). The performances of the proposed scheme are evaluated in terms of peak signal-to-noise ratio and bit rate. Further mean structural similarity (MSSIM) index is introduced to evaluate the structural similarity between the original and the reconstructed images. It is found from the test results that the 3-D Cohen-Daubechies-Feauveau 9/7 symmetric wavelet along with the 3-D SPIHT encoder yields the best compression result.	image compression;wavelet	Natarajan Sriraam;R. Shyamsunder	2011	Digital Signal Processing	10.1016/j.dsp.2010.06.002	wavelet;computer vision;speech recognition;harmonic wavelet transform;second-generation wavelet transform;peak signal-to-noise ratio;continuous wavelet transform;theoretical computer science;magnetic resonance imaging;structural similarity;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;wavelet transform	Graphics	42.5061745377856	-15.406135121114836	24195
a0b0317f1c7e096d53515d440eb3cfb87974d748	integrating post-newtonian equations on graphics processing units	statistical study;black hole;post newtonian approximation;correlations;vector product;binary black hole;black holes;pacs numbers;post newtonian;trou noir;simulation numerique;cosmology;estudio estadistico;graphic processing unit;etude statistique;producto vectorial;correlation;cosmologie;produit vectoriel;digital simulation;uniform distribution;approximation post newtonienne	The merger of 2 black holes is expected to be a common event in the universe. We perform a statistical (Monte-Carlo) analysis of an approximate solution to the binary black hole inspiral problem to find preferred system states before merger. Using GPUs we achieve a speed-up of a factor 50 over a CPU solution making a large scale study feasible on the NCSA Lincoln cluster with 96 Tesla S1070 compute units. Details: http://arxiv.org/abs/0908.3889	approximation;black hole;central processing unit;graphics processing unit;numerical analysis;randomness;simulation;spinor	Frank Herrmann;John Silberholz;Matias Bellone;Gustavo Guerberoff;Manuel Tiglio	2009	CoRR	10.1088/0264-9381/27/3/032001	classical mechanics;black hole;cosmology;physics;quantum mechanics	ML	44.90392038250122	2.1309801112746634	24227
898fcbda45d24d81787b4af90626d2340ab959b3	on the use of simulated annealing for error protection of celp coders employing lsf vector quantizers	error correction codes;speech coding;simulated annealing;linear predictive coding;simulated annealing protection speech bit rate telephony code standards robustness telecommunications error analysis vectors;indexation;error rate;not significant;vector quantizer;vector quantisation;large outliers simulated annealing error protection lsf vector quantizers celp coders output levels parameter representation short term predictor channel error rate index assignment average distortion;error correction codes simulated annealing linear predictive coding vector quantisation speech coding	Examines the application of simulated annealing to assign indices to the output levels of a vector quantizer (VQ). The quantizer is used for the LSF parameter representation of the short-term predictor of a CELP coder. Three classes of error protection and a 0.1 channel error rate are considered. Simulation results show that the use of the index assignment provides significant gains both in terms of average distortion as well as in terms of reduction of large outliers. The improvements afforded by the index assignment, however, were not significant when the VQ was used inside the structure of a CELP coder. >	code-excited linear prediction;lsf;simulated annealing	Sonia L. Q. Dall'Agnol;José Roberto Boisson de Marca;Abraham Alcaim	1994		10.1109/VETEC.1994.345386	electronic engineering;linear predictive coding;speech recognition;simulated annealing;vector sum excited linear prediction;word error rate;computer science;speech coding;pattern recognition;mathematics;code-excited linear prediction	EDA	48.72776246390185	-9.920468369361139	24247
b716bbc399d56acb247ed74308793259ad16608f	new gwma-cusum control chart for monitoring the process dispersion				Rizwan Ali;Abdul Haq	2018	Quality and Reliability Eng. Int.	10.1002/qre.2304	econometrics;statistics;dispersion (optics);engineering;control chart;statistical process control;cusum	SE	28.691826750483127	-20.482854373028648	24275
c6976b0517e4a8206fee94f737029684c1f6851b	neighborhood guided differential evolution		Differential evolution (DE) relies mainly on its mutation mechanism to guide its search. Generally, the parents involved in mutation are randomly selected from the current population. Although such a mutation strategy is easy to use, it is inefficient for solving complex problems. Hence, how to utilize population information to further enhance the search ability of the mutation operator has become one of the most salient and active topics in DE. To address this issue, a new DE framework with the concept of index-based neighborhood, is proposed in this study. The proposed framework is named as neighborhood guided DE (NGDE). In NGDE, a neighborhood guided selection (NGS) is introduced to guide the mutation process by extracting the promising search directions with the neighborhood information. NGS includes four main operators: neighborhood construction, neighbors grouping, two-level neighbors ranking, and parents selection. With these four operators, NGS can utilize the topology and fitness information of population simultaneously. To evaluate the effectiveness of the proposed approach, NGS is applied to several original and advanced DE algorithms. Experimental results have shown that NGDE generally outperforms most of the corresponding DE algorithms on different kinds of optimization problems.	differential evolution	Yiqiao Cai;Meng Zhao;Jingliang Liao;Tian Wang;Hui Tian;Yonghong Chen	2017	Soft Comput.	10.1007/s00500-016-2088-z	mathematical optimization;artificial intelligence;differential evolution;theoretical computer science;salient;machine learning;computer science;operator (computer programming);ranking;optimization problem;population	Logic	25.072327283629566	-3.923529020165365	24365
bf13464fa83a6429cb14b8d4dbcd5ad5c03bd76b	robust h.263 video coding for transmission over the internet	error recovery;video streaming;performance evaluation;video quality;video coding;video conferencing;error resilience;world wide web;video sequence robust h 263 video coding error robustness world wide web desktop computers low bit rate video applications itu t h 263 standard video conferencing phone lines internet video applications compressed h 263 video streams intra coded frames error resilience bursty output traffic picture quality frame dropping macroblock force updating temporal dependencies performance evaluation error recovery speeds video quality;robustness video coding internet videoconference video compression application software streaming media standards development web sites resilience	The widely popular World Wide Web along with advances in desktop computers has brought the world into a new age of computing and communications. Low bit-rate video applications across the Internet are quickly emerging. The ITU-T H.263 standard was designed for low bit-rate video conferencing across phone lines and is an ideal candidate to be extended for Internet video applications. This paper focuses on the error robustness issue of compressed H.263 video streams when transmitted over the Internet. The traditional approach of inserting intra-coded frames increases the error resilience at the expense of bursty output traffic, lower picture quality, and uneven frame dropping. By extending the macroblock force update feature of the H.263 standard, we developed a scheme that complies with the standard and increases the robustness of the video stream. This macroblock updating scheme analyzes the temporal dependencies of macroblocks in successive frames and selectively updates the macroblocks which have the most impact on later frames. The performance evaluation of the proposed technique demonstrates that it achieves a good balance between error recovery speeds and video quality.	internet	Marc Willebeek-LeMair;Zon-Yin Shae;Yuan-Chi Chang	1998		10.1109/INFCOM.1998.659658	video compression picture types;scalable video coding;reference frame;real-time computing;simulation;h.263;telecommunications;computer science;video quality;deblocking filter;video capture;video tracking;block-matching algorithm;multimedia;video processing;smacker video;videoconferencing;motion compensation;h.261;computer network;multiview video coding	Networks	45.8207068869552	-20.46911181924826	24381
cde9022586535121038d5d8ad1818869aca21329	testing for unit roots in short panels allowing for a structural break	unit roots;bootstrap;structural breaks;panel data models;sequential tests;trade openness	Panel data unit root tests which allow for a common structural break in the individual effects or linear trends of the AR(1) panel data model are suggested. These allow the date of the break to be unknown. The tests assume that the time-dimension of the panel (T ) is fixed (finite) while the cross-section (N) is large. Under the null hypothesis of unit roots, they are similar to the initial conditions of the model and its individual effects. Extensions of the tests to the AR(2) model are provided. These highlight the difficulties in extending the tests to higher order serial correlation of the error terms. Monte Carlo experiments indicate that the small sample performance of the tests is very satisfactory. Application of the tests to the trade openness variable of the non-oil countries indicates that evidence of persistence of this variable can be attributed to trade liberalization policies adopted by many developing countries since the early nineties. © 2012 Elsevier B.V. All rights reserved.	autocorrelation;autoregressive model;data model;experiment;initial condition;monte carlo method;openness;panel data;persistence (computer science)	Yiannis Karavias;Elias Tzavalis	2014	Computational Statistics & Data Analysis	10.1016/j.csda.2012.10.014	econometrics;bootstrapping;statistics	AI	30.896986011737823	-22.201977187742465	24577
fbb9870d5a53725b79f799d2cc556dd49a6d0520	uncertainty propagation and sensitivity analysis in system reliability assessment via unscented transformation	importance index;uncertainty propagation;sensitivity analysis;unscented transformation;monte carlo simulation	The reliability of a system, notwithstanding it intended function, can be significantly affected by the uncertainty in the reliability estimate of the components that define the system. This paper implements the Unscented Transformation to quantify the effects of the uncertainty of component reliability through two approaches. The first approach is based on the concept of uncertainty propagation, which is the assessment of the effect that the variability of the component reliabilities produces on the variance of the system reliability. This assessment based on UT has been previously considered in the literature but only for system represented through series/parallel configuration. In this paper the assessment is extended to systems whose reliability cannot be represented through analytical expressions and require, for example, Monte Carlo Simulation. The second approach consists on the evaluation of the importance of components, i.e., the evaluation of the components that most contribute to the variance of the system reliability. An extension of the UT is proposed to evaluate the so called “main effects” of each component, as well to assess high order component interaction. Several examples with excellent results illustrate the proposed approach. & 2014 Elsevier Ltd. All rights reserved.	monte carlo method;propagation of uncertainty;simulation;software propagation;spatial variability	Claudio M. Rocco Sanseverino;Jose Emmanuel Ramirez-Marquez	2014	Rel. Eng. & Sys. Safety	10.1016/j.ress.2014.07.024	reliability engineering;econometrics;engineering;propagation of uncertainty;mathematics;sensitivity analysis;statistics;monte carlo method	SE	28.117824052782908	-17.683734444952712	24583
f1095f895f8bf9a897ae23ca9172e8284ea4239a	a decomposition method for nonlinear parameter estimation in transcend	graph theory;system modeling;nonlinear control systems;computer model;adaptive control;mathematical model computational modeling parameter estimation equations estimation junctions robustness;junctions;decomposition method;reverse osmosis;computational modeling;water treatment adaptive control fault diagnosis graph theory large scale systems nonlinear control systems parameter estimation reverse osmosis;estimation;complex system;possible conflicts pcs;mathematical model;system decomposition;system development;robustness;advanced water recovery system decomposition method nonlinear parameter estimation transcend diagnosis approach fault isolation fault adaptive control system reconfiguration nonlinear systems fault identification scheme temporal causal graph model reverse osmosis subsystem nasa johnson space center possible conflicts;graph model;parameter estimation;nonlinear system;fault isolation;simulation model;johnson space center;system decomposition fault identification fault isolation parameter estimation possible conflicts pcs;fault identification;large scale systems;water treatment;fault diagnosis	Fault isolation and identification are necessary components for system reconfiguration and fault adaptive control in complex systems. However, accurate and timely on-line fault identification in nonlinear systems can be difficult and computationally expensive. In this paper, we improve the quantitative fault identification scheme in the TRANSCEND diagnosis approach. First, we propose to use possible conflicts (PCs) to find the set of minimally redundant subsystems that can be used for parameter estimation. Second, we introduce new algorithms for computing PCs from the temporal causal graph model used in TRANSCEND. Third, we use the minimal estimators to decompose the system model into smaller, independent subsystems for the parameter estimation task. We demonstrate the feasibility of this method by running experiments on a simulated model of the reverse osmosis subsystem of the advanced water recovery system developed at the NASA Johnson Space Center. Our results show a considerable reduction in parameter estimation time without loss of accuracy and robustness in the estimation.	algorithm;analysis of algorithms;causal graph;complex systems;estimation theory;experiment;fault detection and isolation;identification scheme;nonlinear system;online and offline;type system	Aníbal Bregón;Gautam Biswas;Belarmino Pulido Junquera	2012	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2011.2170065	mathematical optimization;estimation;simulation;systems modeling;decomposition method;reverse osmosis;adaptive control;nonlinear system;artificial intelligence;graph theory;machine learning;simulation modeling;mathematical model;control theory;water treatment;mathematics;estimation theory;computational model;fault detection and isolation;statistics;robustness	Robotics	40.13748748098871	-4.763252557464006	24635
a82322e5930cf13dc1cbe74fba3d889f2409ec71	range statistics and the exact modeling of discrete non-gaussian distributions on learnability data		A measure called i-bar is presented, which is the inverse of the mid-range derived from data on trials-to-criterion in tasks that require practice. This measure is interpreted as a conjoint measurement scale, permitting: (a) evaluation of sensitivity of the principal performance measure (which is used to set the metric for trials to criterion); (b) evaluation of the learnability of the work method (i.e. the goodness of the software tool); (c) evaluation of the resilience of the work method. It is possible to mathematically model such order statistics using negative binomial and logistic growth equations, and derive methods for generating prediction intervals. This approach involves novel ways of thinking about statistical analysis for practical significance. The is applicable to the study of the effects of any training or intervention, including software interventions designed to improve legacy work methods and interventions that involve creating entirely new cognitive work systems.	learnability	Robert Hofman	2011		10.1007/978-3-642-21708-1_48	discrete mathematics;machine learning;exact statistics;mathematics;statistics	ML	27.636196864117014	-21.734203174859697	24647
1fe191c8f593b3fd93fa31d3f8f1664e052cf88a	analytical rate model for compressed video considering impacts of spatial, temporal and amplitude resolutions	quantization;scalable video coding;image resolution;data compression;h 264 avc;hevc;quantisation signal;video coding;temporal resolution;video codecs;video coding data compression image resolution quantisation signal video codecs;rate model;spatial resolution streaming media bit rate adaptation models quantization signal scalability predictive models;spatial resolution;network video adaptation analytical rate model video compression spatial resolution temporal resolution amplitude resolution rate control algorithm rc algorithm networked video applications quantization stepsize video frame size frame rate rate quantization model r q model mobile video applications rate adaptation fs fr qs quantization adjustment wireless network bandwidth switch power functions coding scenarios nonscalable video temporal prediction hierarchical b ippp structure h 264 avc hevc quality model encoder rate control;scalable video coding rate model spatial resolution temporal resolution quantization h 264 avc hevc	Rate-control (RC) algorithm is highly desirable for networked video applications. Almost all existing RC methods are only adapting the quantization stepsize (QS) to meet the target bit rate at fixed video frame size (FS) and frame rate (FR) using the rate-quantization (R-Q) model. Recent mobile video applications demand more advanced rate adaptation with different FS, FR and QS, rather merely quantization adjustment, to meet rapid wireless network bandwidth switch. Towards this goal, it requires an accurate rate model with respect to the FS, FR and QS. Hence, we investigate the impacts of spatial, temporal and amplitude resolution (STAR) on the bit rate of a compressed video. We propose a rate model as the product of power functions of the FS, FR and QS, respectively. The proposed rate model is analytically tractable, requiring only four content dependent parameters. The same model works for different coding scenarios (including scalable and non-scalable video, temporal prediction using either hierarchical B or IPPP structure, etc.) with very high accuracy using both H.264/AVC and HEVC. Using the proposed rate model and a quality model, we show how to optimize the STAR for a given rate constraint, which is important for both encoder rate control and network video adaptation.	cobham's thesis;data compression;encoder;frame grabber;h.264/mpeg-4 avc;high efficiency video coding;quantization (signal processing);rc algorithm;scalability	Zhan Ma;Felix C. A. Fernandes;Yao Wang	2013	2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)	10.1109/ICMEW.2013.6618414	computer vision;real-time computing;image resolution;telecommunications;computer science;algorithm;statistics	Mobile	47.25270706018833	-17.08157641313459	24728
a9758b7beb00dcab1627285cd4cf50b55b6f9063	fragile and semi-fragile image authentication based on image self-similarity	watermarking;compression algorithm;fractals;authentication digital signatures image coding feature extraction automata transform coding watermarking discrete cosine transforms frequency domain analysis image representation;image coding;compact signature length;fragile image authentication;frequency domain analysis;image self similarity parameter;compression algorithms;authentication;digital signatures;transform coding;authentication signature;illegitimate manipulations;automata;image authentication;generalized finite automata;jpeg;perceptual self similarity;discrete cosine transforms;image representation;feature extraction;finite automata;image representation fractals finite automata message authentication;compact signature length semi fragile image authentication fragile image authentication authentication signature image representation generalized finite automata perceptual self similarity illegitimate manipulations compression algorithms jpeg image self similarity parameter;semi fragile image authentication;message authentication	We propose a new approach for fragile and semi-fragile image authentication based on image self-similarity. The authentication signature is extracted from image representation using generalized finite automata which represents the image in terms of its perceptual self-similarity. This self-similarity is more likely to be damaged due to illegitimate manipulations rather than compression algorithms such as JPEG. The proposed technique is also capable of changing the degree of fragility by setting the value of the image self-similarity parameter /spl alpha/. Furthermore, our approach features an extremely compact signature length whilst maintaining a rather simple implementation. Experimental results are presented for well known images to support these claims.	authentication;self-similarity;semiconductor industry;software brittleness	Sherif Nour El-Din;Mansour Moniri	2002		10.1109/ICIP.2002.1040096	data compression;computer vision;feature detection;discrete mathematics;computer science;theoretical computer science;mathematics;finite-state machine;statistics	Vision	41.70225363228718	-10.788081224591938	24842
34b0aba6b1a9c32c0522374d0d572c87de3c103b	approximate and generalized confidence bands for the mean and mode functions of the lognormal diffusion process	coverage error;statistical simulation;confidence band;proceso difusion;51e24;intervalo confianza;analisis datos;fonction repartition;lognormal diffusion process;estudio comparativo;62f25;lognormal distribution;processus diffusion;estimacion promedio;loi lognormale;moyenne;etude comparative;funcion distribucion;data analysis;confidence interval;distribution function;ley lognormal;simulacion estadistica;bande confiance;estimation erreur;62g15;error estimation;simulation statistique;promedio;intervalle confiance;statistical computation;estimacion error;calculo estadistico;comparative study;simulation study;analyse donnee;average;calcul statistique;diffusion process;mean estimation;estimation moyenne;60e05;60j60	Approximate and generalized confidence bands for the mean and mode functions of the univariate lognormal diffusion process are obtained. To this end, the already existing methods for building confidence intervals for the mean of the lognormal distribution have been suitably adapted. Moreover, a new method has been proposed. The bands obtained from the above procedures are compared through a simulation study and the comparisons are made both in terms of coverage errors and average widths. This comparative study allows to choose the most appropriate confidence band for each particular case in practical situations. This is shown in an application to a real data set. © 2007 Elsevier B.V. All rights reserved.	simulation	Ramón Gutiérrez Jáimez;Nuria Rico;Patricia Román-Román;Francisco Torres-Ruiz	2007	Computational Statistics & Data Analysis	10.1016/j.csda.2006.12.027	econometrics;confidence interval;coverage error;diffusion process;distribution function;comparative research;calculus;log-normal distribution;mathematics;data analysis;confidence and prediction bands;statistics	AI	32.720891180653304	-22.195244129056775	24898
3ae562de83fe8500043e08a93ea16ad37013df65	a multi-branch convolutional neural network for detecting double jpeg compression.		Bin Li, Hu Luo, Haoxin Zhang, Shunquan Tan, Zhongzhou Ji Shenzhen Key Lab of Media Information Security, Shenzhen University, P. R. China    ABSTRACT Detecting double JPEG compression is important to forensics analysis. A few methods were proposed based on convolutional neural networks (CNNs). These methods only accept inputs from pre-processed data, such as histogram features and/or decompressed images. In this paper, we present a CNN solution by using raw DCT (discrete cosine transformation) coefficients from JPEG images as input. Considering the DCT sub-band nature in JPEG, a multiple-branch CNN structure has been designed to reveal whether a JPEG format image has been doubly compressed. Comparing with previous methods, the proposed method provides end-to-end detection capability. Extensive experiments have been carried out to demonstrate the effectiveness of the proposed network.	artificial neural network;coefficient;convolutional neural network;discrete cosine transform;end-to-end principle;experiment;holographic principle;image histogram;information security;jpeg;os-tan;sensor;zhi-li zhang	Bin Li;Hu Luo;Haoxin Zhang;Shunquan Tan;Zhongzhou Ji	2017	CoRR		convolutional neural network;discrete cosine transform;compression (physics);lossless jpeg;jpeg;artificial intelligence;computer science;pattern recognition;histogram	Vision	40.424167641054005	-11.139295422420327	24902
ef4a5f0b8cf17a8559e1b53c0a09a0795a849506	reducing epistasis in combinatorial problems by expansive coding	search space;combinatorial optimization problem;combinatorial problems;function optimization;fitness function	This paper describes a new technique for tackling highly epistatic combinatorial optimization problems. Rather than having a simple representation, simple operators, a simple fitness function, but a highly epistatic search space, this technique is intended to spread the problem’s complexity more evenly. Using our new technique, known as expansive coding, the representation, operators and fitness function become more complicated, but the search space becomes less epistatic, and therefore easier for a GA to tackle. In effect, the combinatorial task is changed to a function optimization one. We demonstrate how this technique can be applied in the field of arithmetic algorithm design/electronic circuit simplification. In the design of a multiplier for quaternion numbers, consistently good results are obtained.	algorithm design;circuit design;combinatorial optimization;complexity;electronic circuit;fitness function;heuristic (computer science);level of detail;mathematical optimization;optimization problem;software release life cycle	David Beasley;David R. Bull;Ralph R. Martin	1993			optimization problem;extremal optimization;mathematical optimization;combinatorics;combinatorial optimization;combinatorial explosion;mathematics;combinatorial search;fitness approximation;quadratic assignment problem	Graphics	27.382138882782108	4.081788595462008	24987
739f8720b310311e082d93c4d5ba2dfcc9e4923b	a global minlp approach to symbolic regression	integer nonlinear optimization;machine learning;global optimization;symbolic regression;62j02;68t05;68q99;65k05;90c26	Symbolic regression methods generate expression trees that simultaneously define the functional form of a regression model and the regression parameter values. As a result, the regression problem can search many nonlinear functional forms using only the specification of simple mathematical operators such as addition, subtraction, multiplication, and division, among others. Currently, state-of-the-art symbolic regression methods leverage genetic algorithms and adaptive programming techniques. Genetic algorithms lack optimality certifications and are typically stochastic in nature. In contrast, we propose an optimization formulation for the rigorous deterministic optimization of the symbolic regression problem. We present a mixed-integer nonlinear programming (MINLP) formulation to solve the symbolic regression problem as well as several alternative models to eliminate redundancies and symmetries. We demonstrate this symbolic regression technique using an array of experiments based upon literature instances. We then use a set of 24 MINLPs from symbolic regression to compare the performance of five local and five global MINLP solvers. Finally, we use larger instances to demonstrate that a portfolio of models provides an effective solution mechanism for problems of the size typically addressed in the symbolic regression literature.	symbolic regression	Alison Cozad;Nikolaos V. Sahinidis	2018	Math. Program.	10.1007/s10107-018-1289-x	regression analysis;global optimization;nonlinear programming;genetic algorithm;symbolic regression;mathematical optimization;mathematical operators;mathematics;the symbolic;nonlinear system	ML	30.675847218255853	-0.12756159473071718	25054
3bd00cbfba2d221ed1b0dcd010d115df9cf4c782	parameteric coding of speech signals	speech intelligibility;speech;speech coding;lpc analysis;bit rate;data mining;speech quality;hindi;linear predictive coding;speech signal coding;digital filters;subjective mean opinion scores;vocoders;mean opinion score;hindi language;fs1015 lpc coder;vocoders linear predictive coding natural language processing speech coding;hindi speech signal coding fs1015 lpc coder speech intelligibility subjective mean opinion scores speech quality;speech coding linear predictive coding bit rate speech synthesis digital filters quantization information technology vocoders decoding mathematical model;correlation;natural language processing;hindi language fs1015 lpc coder lpc analysis	In this paper an FS1015 LPC coder has been designed using Matlab to produce intelligible speech. This paper focuses on the different methods of implementation and compared to determine, which gives the best performance. The coder has been tested on Hindi. Subjective mean opinion scores have been used to measure the speech quality based on the score given by the listeners.	fs-1015;matlab	Vivek Kumar Sehgal;Shilpy Arora;Sahil Jain;Shantanu Agarwal;Ujjawal Khandelwal;Karan Jain	2009	2009 International Conference on Ultra Modern Telecommunications & Workshops	10.1109/ICUMT.2009.5345381	natural language processing;speech recognition;hindi;computer science	Robotics	48.00336032393664	-7.963215256481562	25206
57e99d17ee3318f6e46418edb28759267b7e8e3b	bayesian stopping rules for multistart global optimization methods	multinomial distribution;optimisation;optimizacion;stopping rule;regle arret bayesienne;regla parada;objective function;posterior distribution;loi multinomiale;clustering method;global optimization;optimization;ley multinomial;optimisation globale;regle arret	By far the most efficient methods for global optimization are based on starting a local optimization routine from an appropriate subset of uniformly distributed starting points. As the number of local optima is frequently unknown in advance, it is a crucial problem when to stop the sequence of sampling and searching. By viewing a set of observed minima as a sample from a generalized multinomial distribution whose cells correspond to the local optima of the objective function, we obtain the posterior distribution of the number of local optima and of the relative size of their regions of attraction. This information is used to construct sequential Bayesian stopping rules which find the optimal trade off between reliability and computational effort.	global optimization;mathematical optimization	C. G. E. Boender;Alexander H. G. Rinnooy Kan	1987	Math. Program.	10.1007/BF02591684	econometrics;mathematical optimization;mathematics;posterior probability;multinomial distribution;statistics;global optimization	ML	30.343445111509848	-11.820179108661904	25244
2f471fc76bdd25657f86cddb232da96ed9a12f46	comparing optimal convergence rate of stochastic mesh and least squares method for bermudan option pricing	pricing;estimation theory;stochastic processes	We analyze the stochastic mesh method (SMM) as well as the least squares method (LSM) commonly used for pricing Bermudan options using the standard two phase methodology. For both the methods, we determine the decay rate of mean square error of the estimator as a function of the computational budget allocated to the two phases and ascertain the order of the optimal allocation in these phases. We conclude that with increasing computational budget, while SMM estimator converges at a slower rate compared to LSM estimator, it converges to the true option value whereas LSM estimator, with fixed number of basis functions, usually converges to a biased value.	basis function;computation;least squares;mathematical optimization;mean squared error;rate of convergence	Ankush Agarwal;Sandeep Juneja	2013	2013 Winter Simulations Conference (WSC)		pricing;minimum mean square error;stochastic process;econometrics;mathematical optimization;mathematics;estimation theory;statistics;recursive least squares filter	Metrics	31.00973754652749	-16.98994503738995	25251
e095b6478ade7d85efdac0a47376cc56d0929d5e	simulated annealing based-ga using injective contrast functions for bss	optimisation;optimum;separacion ciega;optimizacion;electroencefalografia;genie biomedical;blind source separation;linear source;fuente lineal;independent component analysis;simulated annealing;algoritmo genetico;electroencephalographie;blind separation;recuit simule;biomedical engineering;source lineaire;optimo;separacion senal;algorithme genetique;analyse composante independante;separation aveugle;genetic algorithm;recocido simulado;ingenieria biomedica;optimization;separation source;electroencephalography;cumulant;analisis componente independiente;source separation;biomedical application;independent component;cumulante	In this paper we present a novel GA-ICA method which converges to the optimum. The new method for blindly separating unobservable independent component signals from their linear mixtures (Blind Source Separation BSS), uses genetic algorithms (GA) to find the separation matrices which minimize a cumulant based contrast function. The paper also include a formal prove on the convergence of the proposed algorithm using guiding operators, a new concept in the genetic algorithms scenario. This approach is very useful in many fields such as biomedical applications i.e. EEG which usually use a high number of input signals. The Guiding GA (GGA) presented in this work converges to uniform populations containing just one individual, the optimum.	blind signal separation;electroencephalography;genetic algorithm;independent computing architecture;population;separation kernel;simulated annealing;software release life cycle;source separation	Juan Manuel Górriz;Carlos García Puntonet;J. D. Morales;Juan José González de la Rosa	2005		10.1007/11428831_72	independent component analysis;mathematical optimization;genetic algorithm;simulated annealing;electroencephalography;computer science;artificial intelligence;machine learning;mathematics;blind signal separation;algorithm;cumulant	AI	28.669547243108916	0.8968489129560863	25272
2732e6889dcd4ff5601624ccb8948fe075ffff6a	emergence of tool use in an articulated limb controlled by evolved neural circuits	neural controller neural circuits cognitive function mammals avian species two degree of freedom articulated limb arbitrary topology tool pick up frequency;neurocontrollers artificial limbs	Tool use requires high levels of cognitive function and is only observed in higher mammals and some avian species such as corvids. In this paper, we will investigate how the capability to use tools can spontaneously emerge in a simulated evolution of a two degree-of-freedom articulated limb. The controller for the limb was evolved as neural circuits that can gradually take on arbitrary topologies (NeuroEvolution of Augmenting Topologies, or NEAT). First, we show how very broad fitness criteria such as distance to the target, number of steps to reach the target, and tool pick-up frequency are enough to give rise to tool using behavior. Second, we analyze the evolved neural circuits to find properties that enable tool use. We expect our results to help us understand the origin of tool use and the kind of neural circuits that enabled such a powerful trait.	algorithm;cognition;emergence;neat chipset;neuroevolution of augmenting topologies;recurrent neural network;robotic arm;sensor	Qinbo Li;Jaewook Yoo;Yoonsuck Choe	2015	2015 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2015.7280564	simulation;computer science;artificial intelligence	EDA	25.21989559495199	-13.047364865875918	25287
4b46caf68d0357a83e463f0e3933f832c4a74e55	applications of discrete pso algorithm to two-dimensional non-guillotine rectangular packing problems		algorithm was applied to the two-dimensional non-guillotine rectangular packing problems. In this study, two different weight mechanisms in the PSO algorithm are used: fixed inertia weight and nonlinear varying inertia weight. The nonlinear varying inertia weights help to improve the speed of convergence and the search ability. In order to compare the performances of these weight mechanisms, a test problem consisting of 25 rectangular pieces taken from Jakobs [12] is given as an example. For different PSO parameters, trim loss values versus iteration numbers were also examined. This study shows that the nonlinear varying inertia weight simulations yielded approximately 9.3% better results than the fixed inertia weight simulations. Also, the solutions obtained in this study are in the acceptable boundaries for this kind of the packing problems.	algorithm;iteration;nonlinear system;particle swarm optimization;performance;phase-shift oscillator;rate of convergence;set packing;simulation;vergence	Alev Soke;Zafer Bingul	2007			metering mode;compressibility;packing problems;thread (computing);topology;mathematical optimization;particle swarm optimization;mathematics	Theory	29.49141778073699	-3.555196996443449	25298
7656e080223305ef6fcda0d4348b6f2502b04010	a sequential constant-stress accelerated life testing scheme and its bayesian inference	bayesian inference;sequential method;accelerated life testing;parameter estimation	Abstract#R##N##R##N#In the analysis of accelerated life testing (ALT) data, some stress-life model is typically used to relate results obtained at stressed conditions to those at use condition. For example, the Arrhenius model has been widely used for accelerated testing involving high temperature. Motivated by the fact that some prior knowledge of particular model parameters is usually available, this paper proposes a sequential constant-stress ALT scheme and its Bayesian inference. Under this scheme, test at the highest stress is firstly conducted to quickly generate failures. Then, using the proposed Bayesian inference method, information obtained at the highest stress is used to construct prior distributions for data analysis at lower stress levels. In this paper, two frameworks of the Bayesian inference method are presented, namely, the all-at-one prior distribution construction and the full sequential prior distribution construction. Assuming Weibull failure times, we (1) derive the closed-form expression for estimating the smallest extreme value location parameter at each stress level, (2) compare the performance of the proposed Bayesian inference with that of MLE by simulations, and (3) assess the risk of including empirical engineering knowledge into ALT data analysis under the proposed framework. Step-by-step illustrations of both frameworks are presented using a real-life ALT data set. Copyright © 2008 John Wiley & Sons, Ltd.	accelerated life testing;bayesian approaches to brain function	Xiao Liu;Loon Ching Tang	2009	Quality and Reliability Eng. Int.	10.1002/qre.958	bayesian average;econometrics;statistical inference;fiducial inference;bayesian experimental design;accelerated life testing;frequentist inference;computer science;data mining;mathematics;bayesian linear regression;bayesian statistics;estimation theory;bayesian inference;statistics	SE	29.68130428233233	-19.721162179530122	25313
587b8fc7dffb5ab37ea08f82254e01ab24079c6b	content-dependent watermarking scheme in compressed speech with identifying manner and location of attacks	filigranage numerique;protection information;digital watermarking;codage parole;sound quality;tabla codificacion;watermarking;watermarking speech codecs digital recording robustness speech processing data mining speech analysis authentication frequency protection;calidad sonora;bit rate 6 3 kbit s content dependent watermarking scheme attacks location speech compression technologies digital recording devices codebook excited linear prediction celp speech codec line spectrum frequency least significant bits perceptual evaluation of speech quality pesq group index pitch extraction g 723 1 speech codecs;record format;metodo espectral;group index;speech codec;qualite sonore;image processing;line spectrum frequency;watermark attack;data compression;pitch acoustics;bit rate 6 3 kbit s;format enregistrement;localization;speech processing;codebook excited linear prediction;tratamiento palabra;procesamiento imagen;traitement parole;registro numerico;speech coding;perceptual evaluation of speech quality;localizacion;tonie;speech watermark;estimacion a priori;data mining;data format;traitement image;linear predictive;g 723 1 speech codecs;attacks location;a priori estimation;codage predictif;accuracy;indexes;least significant bit;precision;localisation;spectre raie;proteccion informacion;speech codecs;digital recording;watermark attack content dependent watermark speech codec speech watermark;codebook;feature extraction;table codage;information protection;altura sonida;filigrana digital;indexation;enregistrement numerique;prediccion lineal;celp;spectral method;codificacion predictiva;codec parole;controle qualite;estimation a priori;content dependent watermark;signal acoustique;advanced technology;line spectrum;methode spectrale;digital recording devices;linear prediction;acoustic signal;least significant bits;formato grabacion;technologie avancee;pitch extraction;quality control;speech compression technologies;predictive coding;pesq;senal acustica	As speech compression technologies have advanced, digital recording devices have become increasingly popular. However, data formats used in popular speech codecs are known a priori, such that compressed data can be modified easily via insertion, deletion, and replacement. This work proposes a content-dependent watermarking scheme suitable for codebook-excited linear prediction (CELP)-based speech codec that ensures the integrity of compressed speech data. Speech data are initially partitioned into many groups, each of which includes multiple speech frames. The watermark embedded in each frame is then generated according to the line spectrum frequency (LSF) feature in the current frame, the pitch extracted from the succeeding frame, the watermark embedded in the preceding frame, and the group index which is determined by the location of the current frame. Finally, some of the least significant bits (LSBs) of the indices indicating the excitation pulse positions or excitation vectors are substituted for the watermark. Conventional watermarking schemes can only detect whether compressed speech data are intact. They cannot determine where compressed speech data are altered by insertion, deletion, or replacement, whereas the proposed scheme can. Experiments established that the proposed scheme used in the G.723.1 6.3 kb/s speech codecs embeds 12 bits in each compressed speech frame with 189 bits, and only decreases the perceptual evaluation of speech quality (PESQ) by 0.11. Additionally, its accuracy in detecting the locations of attacked frames is very high, with only two normal frames mistaken as attacked frames. Therefore, the proposed watermarking scheme effectively ensures the integrity of compressed speech data.	advanced digital recording;code-excited linear prediction;codebook;codec;data compression;data rate units;digital watermarking;embedded system;g.723.1;jumbo frame;lsf;least significant bit;pesq;part-of-speech tagging;sensor;speech coding;time-compressed speech	Oscal T.-C. Chen;Chia-Hsiung Liu	2007	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2007.896658	voice activity detection;reference frame;codec2;linear predictive coding;speech recognition;telecommunications;image processing;digital watermarking;computer science;speech coding;speech processing;accuracy and precision;statistics	Arch	44.40400920826945	-11.030404318776199	25379
7b9e858e3f7f7fe7f79aadee0fa444d7936cca8f	adaptive procedure for generalized familywise error rate control	generalized familywise error rate;secondary 62h99;generalized bonferroni procedure;adaptive procedure;primary 62h15;multiple hypotheses testing	AbstractThis paper considers multiple hypotheses testing with the generalized familywise error rate k-FWER control, which is the probability of at least k false rejections. We firstly assume the p-values corresponding to the true null hypotheses are independent, and propose adaptive generalized Bonferroni procedure with k-FWER control based on the estimation of the number of true null hypotheses. Then we assume the p-values are dependent, satisfying block dependence, and propose adaptive procedure with k-FWER control. Extensive simulations compare the performance of the adaptive procedures with different estimators.		Li Wang	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2016.1267753	econometrics;mathematical optimization;false discovery rate;closed testing procedure;familywise error rate;bonferroni correction;mathematics;per-comparison error rate;holm–bonferroni method;statistics	ML	29.98384464508392	-21.696468524200743	25397
c639d89144b3c13c3ec6f2127f297c6e81a6a1ad	novel two-step method for joint synchronization and localization in asynchronous networks	synchronization;wireless sensor networks;complexity theory;random variables;noise level;computational modeling	In this letter, the joint synchronization and localization problem in asynchronous networks using time-of-arrival measurements is addressed. A novel two-step method is proposed. In Step 1, we estimate the clock skew by a linear weighted least squares estimator. In Step 2, we obtain a second-order cone program (SOCP) by applying second-order cone relaxation technique to jointly estimate the source node location and clock offset. The relaxed SOCP problem is further tightened by adding a regularization term. Moreover, a simple procedure of selecting the regularization factor is proposed. Simulation results show the superior performance of the proposed method over the existing methods.	clock skew;lagrangian relaxation;least squares;linear programming relaxation;matrix regularization;second-order cone programming;simulation;time of arrival	Fan Zhang;Gang Wang;Wei Wang	2017	IEEE Wireless Communications Letters	10.1109/LWC.2017.2748135	mathematical optimization;wireless sensor network;estimator;mathematics;synchronization;clock skew;regularization (mathematics);asynchronous communication;offset (computer science);least squares	Vision	51.89681009391066	4.012890168416171	25501
be4610ac2fece9af051bab8c4cfc38936c072ad2	a discrete-time recurrent neural network for shortest-path routing	minimisation;directed graphs;optimal solution;neural networks;approximation algorithms;exact optimal solutions;routing;path planning;discrete time systems;discrete time;global convergence;convergence rate;indexing terms;discrete time recurrent neural network;operating characteristics;computer networks;shortest path routing;artificial neural networks;telecommunication traffic;discrete time systems recurrent neural nets minimisation directed graphs;recurrent neural networks routing shortest path problem neural networks approximation algorithms artificial neural networks costs telecommunication traffic path planning computer networks;operating characteristic;performance characteristics;recurrent neural nets;recurrent neural networks;recurrent neural network;combinatorial optimization;hardware implementation;performance characteristics discrete time recurrent neural network shortest path routing global convergence exact optimal solutions convergence rate operating characteristics;shortest path problem;neural network	This paper presents a discrete-time recurrent neural network, with a fixed step parameter, for solving the shortest path problem. The proposed discrete-time recurrent neural network with a simple architecture is proven to be globally convergent to exact optimal solutions and is suitable for hardware implementation. Furthermore, an improved network with a larger step size independent of the problem size is proposed to increase its convergence rate. The performance and operating characteristics of the proposed neural network are demonstrated by means of simulation results.	analysis of algorithms;artificial neural network;rate of convergence;recurrent neural network;routing;shortest path problem;simulation	Youshen Xia;Jun Wang	2000	IEEE Trans. Automat. Contr.	10.1109/9.887639	mathematical optimization;probabilistic neural network;random neural network;computer science;recurrent neural network;machine learning;time delay neural network;distributed computing;artificial neural network	ML	30.91606314632575	3.7963325446021967	25563
b1cda2002d1543d432ae2da8cb009d3141363918	crowding-distance-based multiobjective artificial bee colony algorithm for pid parameter optimization		This work presents a crowding-distance(CD)-based multiobjective artificial bee colony algorithm for Proportional-Integral-Derivative (PID) parameter optimization. In the proposed algorithm, a new fitness assignment method is defined based on the nondominated rank and the CD. An archive set is introduced for saving the Pareto optimal solutions, and the CD is also used to wipe off the extra solutions in the archive. The experimental results compared with NSGAII over two test functions show its effectiveness, and the simulation results of PID parameter optimization verify that it is efficient for applications.	artificial bee colony algorithm;crowding;pid;program optimization	Xia Zhou;Jiong Shen;Yi-Guo Li	2014		10.1007/978-3-319-11857-4_25	mathematical optimization;artificial intelligence;machine learning	EDA	25.80600297415494	-4.1714647459942915	25641
86b022fb6d0586d5d2d3fa48232c94ea98d0f334	solving the chemical master equation using sliding windows	software;escherichia coli;simulation and modeling;stochastic process;probability;probability density;models theoretical;ordinary differential equation;systems biology;stochastic simulation;gene regulatory networks;population size;physiological cellular and medical topics;expression;models chemical;computational biology bioinformatics;chemical master equation;upper bound;enzymes;stochastic processes;approximate solution;probability distribution;state space;numerical algorithm;gene expression regulation;reproducibility of results;models statistical;algorithms;matrix;upper and lower bounds;probabilities;chemical reaction;computer simulation;gene expression profiling;sliding window;point of interest;noise;markov chains;bioinformatics;global analysis;approximations	"""The chemical master equation (CME) is a system of ordinary differential equations that describes the evolution of a network of chemical reactions as a stochastic process. Its solution yields the probability density vector of the system at each point in time. Solving the CME numerically is in many cases computationally expensive or even infeasible as the number of reachable states can be very large or infinite. We introduce the sliding window method, which computes an approximate solution of the CME by performing a sequence of local analysis steps. In each step, only a manageable subset of states is considered, representing a """"window"""" into the state space. In subsequent steps, the window follows the direction in which the probability mass moves, until the time period of interest has elapsed. We construct the window based on a deterministic approximation of the future behavior of the system by estimating upper and lower bounds on the populations of the chemical species. In order to show the effectiveness of our approach, we apply it to several examples previously described in the literature. The experimental results show that the proposed method speeds up the analysis considerably, compared to a global analysis, while still providing high accuracy. The sliding window method is a novel approach to address the performance problems of numerical algorithms for the solution of the chemical master equation. The method efficiently approximates the probability distributions at the time points of interest for a variety of chemically reacting systems, including systems for which no upper bound on the population sizes of the chemical species is known a priori."""	accidents, chemical;analysis of algorithms;approximation algorithm;estimated;microsoft windows;numerical analysis;point of interest;population;state space;stochastic process;subgroup;window function	Verena Wolf;Rushil Goel;Maria Mateescu;Thomas A. Henzinger	2009		10.1186/1752-0509-4-42	stochastic process;mathematical optimization;computer science;bioinformatics;theoretical computer science;probability;upper and lower bounds;algorithm	AI	33.59434007902754	-12.742293924471307	25663
154ab0a793bbb550675685c987287425959953e6	inside a predator-prey model for multi-objective optimization: a second study	predator prey model;multi objective optimization;predator prey;simplex recombination;variation operators	In this article, new variation operators for evolutionary multi-objective algorithms (EMOA) are proposed. On the basis of a predator-prey model theoretical considerations as well as empirical results lead to the development of a new recombination operator, which improves the approximation of the set of efficient solutions significantly. Furtheron, it is shown that applying speciation to the analysed model makes it possible to handle even more complex problems.	approximation;evolutionary algorithm;lotka–volterra equations;mathematical optimization;multi-objective optimization;prey	Christian Grimme;Karlheinz Schmitt	2006		10.1145/1143997.1144121	mathematical optimization;artificial intelligence;multi-objective optimization;mathematics;predation	ML	24.718522132811618	-7.5046770448919595	25671
2d164471e8cd6bbcf2fd04d63bd114af50409667	noise post-processing for low bit-rate celp coders	background noise;itu t;tecnologia electronica telecomunicaciones;frame erasure;celp;tecnologias;grupo a;post processing		code-excited linear prediction;video post-processing	Hiroyuki Ehara;Kazutoshi Yasunaga;Koji Yoshida;Yusuke Hiwasaki;Kazunori Mano;Takao Kaneko	2004	IEICE Transactions		speech recognition;telecommunications;computer science;background noise;video post-processing;code-excited linear prediction	Vision	47.98114753242459	-8.66901778948781	25683
10fc5d2b395c3c212fc8474c6f053c445adca357	efficient lossless coding model for medical images by applying integer-to-integer wavelet transform to segmented images	transformation ondelette;modelizacion;radiodiagnostic;medical imagery;entropia;algorithm performance;information loss;image processing;data compression;integer wavelet transform;procesamiento imagen;hombre;lossless compression;radiografia;segmentation;traitement image;radiography;modelisation;wavelet transforms;radiodiagnostico;codificacion;wavelet transform;medical image;perdida informacion;resultado algoritmo;entropie;coding;human;performance algorithme;imagerie medicale;compression ratio;entropy;imageneria medical;compresion dato;transformacion ondita;radiodiagnosis;modeling;perte information;wavelets;segmentacion;wavelet transformation;radiographie;compression donnee;codage;color image;homme	Existing lossless coding models yield only up to 3:1 compression. However, a much higher lossless compression can be achieved for certain medical images when the images are segmented prior to applying integer to integer wavelet transform and lossless coding. The methodology used in this research work is to apply a contour detection scheme to segment the image first. The segmented image is then wavelet transformed with integer to integer mapping to obtain a lower weighted entropy than the original. An adaptive arithmetic model is then applied to code the transformed image losslessly. For the male visible human color image set, the overall average lossless compression using the above scheme is around 10:1 whereas the compression ratio of an individual slice can be as high as 16:1. The achievable compression ratio depends on the actual bit rate of the segmented images attained by lossless coding as well as the compression obtainable from segmentation alone. The computational time required by the entire process is fast enough for application on large medical images.© (2000) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	lossless compression;wavelet transform	Shuyu Yang;Gilberto Zamora;Mark P. Wilson;Sunanda Mitra	2000		10.1117/12.387670	data compression;lossy compression;lossless jpeg;computer vision;data compression ratio;electronic engineering;image compression;jbig2;entropy encoding;theoretical computer science;context-adaptive variable-length coding;mathematics;lossless compression;chain code;adaptive coding;context-adaptive binary arithmetic coding;golomb coding	Vision	46.05868662297918	-13.614126067524804	25730
5917a8a4df6b1d4772c899d8af54070e6cc30abd	effective search of the energy landscape for protein folding	protein folding;global optimization;local minima;energy landscape	We propose a new algorithmic approach for global optimization in protein folding. We use the information found in various local minima to direct the search for the global minimum. In this way, we explore the energy landscape efficiently by considering only the space of local minima instead of the whole feasible space of conformations. Our fundamental approach is to sample only the space of local minima and guide the sampling process by exploring protein structure building blocks found in sampled local minima. These building blocks form the basis of information in searching for the global minimum. In particular we employ an iterative algorithm that begins with an initial pool of local minima; construct a new pool of solutions by combining the various building blocks found in the original pool; take each solution and map them to their representative local minima; and, repeat the process. Our procedure seems to share a great deal of commonality with evolutionary computing techniques. Indeed, we even employ genetic operators in our algorithm. However, unlike existing hybrid evolutionary computing algorithms where local minimization algorithms are simply used to “fine-tune” the solutions, we focus primarily on constructing local minima from previously explored minima and only use genetic operators to assist in diversification. Hence, our total number of iterations/generations were demonstrated (empirically) to be quite low (≈ 50) whereas standard genetic algorithms and Monte Carlo are very high ranging from 150,000 to nearly 20,000,000 generations in order to provide sufficient opportunity for these methods to converge and achieve their best solution. We applied our idea to several proteins from the Protein Data Bank (PDB) using the UNRES model[1]. We compared against Standard Genetic Algorithms(SGA) and Metropolis Monte Carlo(MMC) approaches. In all cases, our new approach computed the lowest energy conformation.	converge;diversification (finance);evolutionary computation;genetic algorithm;genetic operator;global optimization;iteration;iterative method;mathematical optimization;maxima and minima;metropolis;metropolis–hastings algorithm;monte carlo method;protein data bank;sampling (signal processing)	Eugene Santos;Keum Joo Kim;Eunice E. Santos	2003		10.1007/3-540-45105-6_78	protein folding;mathematical optimization;bioinformatics;energy landscape;maxima and minima;global optimization	Graphics	29.629431246224446	-7.595660978639781	25787
819ad2c458743084bb49a224478c6feb80f77684	traveling salesman problem solutions by using passive neural networks		Presented in this paper numeric experiments on random, relative large travelling salesman problems, show that the passive neural networks can be used as an efficient, dynamic optimization tool for combinatorial programming. Moreover, the passive neural networks, when implemented in VLSI technology, could be a basis for structure of bio-inspired processors, for real-time optimizations.	british informatics olympiad;central processing unit;convolutional neural network;dynamic programming;experiment;hopfield network;mathematical optimization;neural networks;real-time clock;travelling salesman problem;very-large-scale integration	Andrzej Luksza;Wieslaw Sienko	2014		10.5220/0005132800690077	2-opt;3-opt	AI	26.705484407809713	0.021352922287398383	25790
095fea0f343464df88e65a705995d7ff8294f3f0	population size estimation using a few individuals as agents	population estimation;statistical analysis bluetooth mobile handsets;population size;population size estimation;statistical model;opportunistic sampling;statistical analysis;mobile handsets;mobile phone population size estimation open air music festival bluetooth probes bluetooth devices statistical model;ground truth;bluetooth;bluetooth optical wavelength conversion mobile handsets animals biological system modeling maximum likelihood estimation	We conduct an experiment where ten attendees of an open-air music festival are acting as Bluetooth probes. We then construct a parametric statistical model to estimate the total number of visible Bluetooth devices in the festival area. By comparing our estimate with ground truth information provided by probes at the entrances of the festival, we show that the total population can be estimated with a surprisingly low error (1.26% in our experiment), given the small number of agents compared to the area of the festival and the fact that they are regular attendees who move randomly. Also, our statistical model can easily be adapted to obtain more detailed estimates, such as the evolution of the population size over time.	bluetooth;ground truth;mobile device;parametric model;population;randomness;statistical model	Farid Movahedi Naini;Olivier Dousse;Patrick Thiran;Martin Vetterli	2011	2011 IEEE International Symposium on Information Theory Proceedings	10.1109/ISIT.2011.6034016	statistical model;population size;simulation;telecommunications;ground truth;mathematics;bluetooth;statistics	Embedded	35.959493563796954	-17.8524915346211	25801
cf29df437a410031b9d45c3e1ab3f8d65a03d8cd	adaptive mutation in genetic algorithms	least significant bit;genetic algorithm	In Genetic Algorithms mutation probability is usually assigned a constant value, therefore all chromosome have the same likelihood of mutation irrespective of their fitness. It is shown in this paper that making mutation a function of fitness produces a more efficient search. This function is such that the least significant bits are more likely to be mutated in high-fitness chromosomes, thus improving their accuracy, whereas low-fitness chromosomes have an increased probability of mutation, enhancing their role in the search. In this way, the chance of disrupting a high-fitness chromosome is decreased and the exploratory role of low-fitness chromosomes is best exploited. The implications of this new mutation scheme are assessed with the aid of numerical examples.	genetic algorithm	Stefano Marsili-Libelli;Pasquale S. Alba	2000	Soft Comput.	10.1007/s005000000042	least significant bit;genetic algorithm;mutation;computer science;bioinformatics;artificial intelligence;machine learning	Logic	26.437326370397717	-8.970317378627593	25829
e900def86c890542e99bb10f0937f5c7f63c34c8	fractal image compression with predicted dihedral transformation	discrete cosine transform fractal image compression predicted dihedral transformation conventional algorithm image retrieval image quality image encoding direct allocating method;fractals;ac coefficients fractal image compression pifs dihedral transformation dct;pifs;image coding;data compression;ac coefficients;image compression;discrete cosine transforms;image retrieval data compression discrete cosine transforms fractals image coding;image quality;similarity measure;dct;fractals image coding image quality search methods image retrieval discrete cosine transforms jacobian matrices performance evaluation inverse problems;fractal image compression;image retrieval;dihedral transformation	Fractal image compression exploits the self-similarity of an image to achieve image compression. The conventional algorithm allows the transformations on domain blocks to obtain eight orientations so as to increase the quality of retrieved image. On the other hand, if no transformation is performed in order to speedup the encoder, the image quality will decay. In this paper, a direct allocating method to predict the desired transformation for similarity measure is proposed. Simulations show that the encoding time is almost the same as that of the method without transformations while the image quality is close to that of the standard method.	algorithm;computer simulation;encoder;fractal compression;image compression;image quality;self-similarity;similarity measure;speedup	Der-Jyh Duh;Jyh-Horng Jeng;Shu-Yuan Chen	2007	2007 12th IEEE Symposium on Computers and Communications	10.1109/ISCC.2007.4381598	data compression;image quality;computer vision;feature detection;fractal;image retrieval;image compression;computer science;theoretical computer science;discrete cosine transform;fractal transform;fractal compression	Vision	44.86305201838053	-14.937500724387192	25831
793fdc4022289383e7418543ca9d2a39b1666705	stress-strength reliability analysis with extreme values based on q-exponential distribution	reliability engineering;q exponential distribution;bootstrap methods;maximum likelihood estimators;stress strength reliability	Quality and Reliability Engineering International#R##N#Early View (Online Version of Record published before inclusion in an issue)	reliability engineering	Romero Sales Filho;Enrique López Droguett;Isis Didier Lins;Márcio das Chagas Moura;Mehdi Amiri;Rafael Valença Azevedo	2017	Quality and Reliability Eng. Int.	10.1002/qre.2020	reliability engineering;econometrics;computer science;engineering;statistics	SE	29.172379416076687	-20.127510075753825	25839
a3289ec8ab3c3cee449349c8f9fff5dda464d757	order statistics of dependent sequences consisting of two different sets of exchangeable variables	computacion informatica;ciencias basicas y experimentales;matematicas;grupo a	We consider two different sets of exchangeable samples which are assumed to be dependent. A single set of observations is obtained from these two dependent samples. The distribution of single order statistic, and the joint distribution of the minimum and an arbitrary order statistic are derived. The results are illustrated in the context of reliability problem.	reliability engineering	Ismihan Bayramoglu;Serkan Eryilmaz	2015	J. Computational Applied Mathematics	10.1016/j.cam.2015.02.045	econometrics;calculus;mathematics;statistics	Theory	31.316156691758767	-19.970524540256093	25922
c8b9494b67c40f44810292cf9d00fbaeb6dfebf1	visual cryptography based watermarking		"""The combination of watermarking schemes with visual cryp- tography has been recently used for the copyright protection of digital images. The derived schemes try to satisfy the increasing need for the security of multimedia data, caused in turn by the enormous amount of digital information that is daily created and distributed over differ- ent kinds of communication channels. Watermarking is generally used to insert """"secret"""" information into an original image, with different pur- poses and different features, usually as a means to assess the ownership of the modified image. Visual cryptography refers to a way to decompose a secret image into shares and distribute them to a number of partici- pants, so that only legitimate subsets of participants can reconstruct the original image by combining their shares. The combination of both techniques can provide some important so- lutions for tampering verification and the resolution of disputes on the ownership of a given image, as provided by several proposals appeared in literature. In this work we present a general model for the watermark- ing schemes obtained from the combination with visual cryptography Furthermore we discuss also the improved robustness such schemes can achieve, trying to analyze the effects that random or induced errors can have on the reconstructed watermark. Finally, some possible extensions of the combined approach are also introduced considering different visual cryptographic schemes and their possible applications in new scenarios."""	visual cryptography	Stelvio Cimato;James C. N. Yang;Chih-Cheng Wu	2014	Trans. Data Hiding and Multimedia Security	10.1007/978-3-642-55046-1_6	theoretical computer science;mathematics;internet privacy;computer security	Crypto	38.24339163348859	-11.736207779938297	25944
fc83d6d43b1cd0e7e0fd0785463b77d2f59d5f17	modeling of packet-loss-induced distortion in 3-d synthesized views	rendering view quality estimation packet loss induced distortion modelling 3d video coding synthesis distortion depth map error frequency domain energy density texture reconstruction channel error;decoding;packet loss;distortion;image reconstruction;distortion image reconstruction packet loss rendering computer graphics decoding discrete fourier transforms;video coding distortion estimation theory frequency domain analysis image reconstruction image texture rendering computer graphics;rendering computer graphics;discrete fourier transforms;synthesis distortion modeling 3 d video coding packet losses	This paper analyzes how transmission errors in the texture and depth map jointly affect the synthesized virtual view in 3-D video coding. In particular, we propose a framework that decouples the effects attributed to transmission errors in texture and depth to facilitate theoretical analysis. The synthesis distortion due to depth map errors is characterized in the frequency domain using a new approach that combines the energy density of the reconstructed texture and channel errors. Experimental results show that our analytical model can accurately estimate the rendering view quality.	3d film;data compression;depth map;distortion;network packet;simulation	Pan Gao;Wei Xiang	2015	2015 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2015.7457795	iterative reconstruction;computer vision;distortion;rendering;telecommunications;computer science;theoretical computer science;packet loss;computer graphics (images)	Graphics	43.668045070761835	-18.388247388833175	26025
66021049a9dc5d7b09f48d6423bdaadb7d5f5419	a system to test the performance of rfid-tagged objects	perforation;electronic products rfid tagged object reader antenna;optimal location;testing;rfid tag;null;testing antennas electronic products radiofrequency identification;antennas;system testing attenuators attenuation antenna measurements supply chains interference service robots radiofrequency identification position measurement rfid tags;supply chain;electronic products;radiofrequency identification	A system is described which has been developed to assess how well RFID-tagged products can be read with various positionings of the tag on the product face. The system can search the entire product surface and provide a measure of tag performance in any location, allowing also for variation in the position of the reader antenna. Simulation of moving products is also possible. Results are shown	automation;cuboid;fits;rfid testing;radio-frequency identification;simulation;tag (metadata)	Hugo Mallinson;Steve Hodges;Alan Thorne	2007	2007 International Symposium on Applications and the Internet Workshops	10.1109/SAINT-W.2007.18	radio-frequency identification;embedded system;telecommunications;antenna;software testing;supply chain	Arch	49.58225545454622	-0.3215345294805136	26102
0ac503327f57ec36ac7ea8f02fbe81b4d991bb7f	minimum distortion variance concatenated block codes for embedded source transmission	convolutional codes;parity check codes;video coding block codes concatenated codes convolutional codes encoding higher order statistics parity check codes;optimization block codes streaming media decoding image reconstruction receivers;low density parity check codes concatenated block codes minimum distortion variance embedded source transmission state of art multimedia source encoders embedded source bit streams reliable reception total bit stream decoder reconstruction quality h 264 avc progressive image coders jpeg2000 quality fluctuations second order statistics optimization image transmission convolutional codes;higher order statistics;video coding;concatenated codes;encoding;block codes	Some of the state-of-art multimedia source encoders produce embedded source bit streams that upon the reliable reception of only a fraction of the total bit stream, the decoder is able reconstruct the source up to a basic quality. Reliable reception of later source bits gradually improve the decoder reconstruction quality. Examples include scalable extensions of H.264/AVC and progressive image coders such as JPEG2000. To provide an efficient protection for embedded source bit streams, a concatenated block coding mechanism using a minimum mean distortion criterion was considered in the past. Although, the original design was shown to achieve mean distortion characteristics better than that of previous research, the proposed coding structure was leading to dramatic quality fluctuations. In this paper, a modification of the original design is first presented and then the second order statistics of the distortion is taken into account in the optimization. More specifically, an extension scheme is proposed using a minimum distortion variance optimization criterion. This robust system design is tested for an image transmission scenario. Numerical results show that the proposed extension achieves significantly lower variance than the original design, while showing similar mean distortion performance using both convolutional codes and low density parity check codes.	bitstream;block code;concatenated error correction code;concatenation;convolutional code;distortion;embedded system;encoder;h.264/mpeg-4 avc;jpeg 2000;line code;low-density parity-check code;mathematical optimization;parity bit;rate–distortion theory;scalability;simulation;systems design	Suayb S. Arslan	2014	2014 International Conference on Computing, Networking and Communications (ICNC)	10.1109/ICCNC.2014.6785365	block code;concatenated error correction code;turbo code;convolutional code;real-time computing;online codes;fountain code;telecommunications;theoretical computer science;serial concatenated convolutional codes;tornado code;linear code;expander code;mathematics;raptor code;error floor;encoding;statistics	EDA	48.09028166311952	-15.993800220964918	26148
104817a0980649682d9119c14b406c7f18df98fb	information hiding for g.711 speech based on substitution of least significant bits and estimation of tolerable distortion	selective embedding method;speech coding adaptive modulation data encapsulation differential pulse code modulation;g 711;data hiding;degradation;bit rate 10 kbit s g 711 speech data hiding technique lsb substitution method least significant bits tolerable distortion estimation low bitrate encoder selective embedding method speech quality degradation adpcm adaptive differential pulse code modulation bit rate 4 kbit s;adaptive modulation;information hiding;adpcm;speech;speech coding;lsb substitution speech coding data hiding g 711 adpcm;speech enhancement;bit rate;data mining;differential pulse code modulation;data encapsulation;low bitrate encoder;nonlinear distortion;least significant bit;lsb substitution;bit rate 10 kbit s;g 711 speech;speech quality degradation;data encapsulation speech enhancement codecs degradation bandwidth decoding quantization indium tin oxide data engineering speech coding;tolerable distortion estimation;least significant bits;lsb substitution method;bit rate 4 kbit s;adaptive differential pulse code modulation;data hiding technique	In this paper, we propose a novel data hiding technique for G.711 speech based on the LSB substitution method. The novel feature of the proposed method is that a low-bitrate encoder, G.726 ADPCM, is used as a reference for deciding how many bits can be embedded in a sample. Experiments showed that the method outperformed the simple LSB substitution method and the selective embedding method proposed by Aoki. We achieved 4-kbit/s embedding with almost no subjective degradation of speech quality, and 10 kbit/s while keeping good quality.	adaptive differential pulse-code modulation;data rate units;distortion;elegant degradation;embedded system;encoder;g.711;g.726;least significant bit;substitution method	Akinori Ito;Shun'ichiro Abe;Yôiti Suzuki	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4959857	speech recognition;telecommunications;computer science;information hiding	Robotics	48.42647948394001	-8.774280737348832	26314
54a83243a73e1bb8db014f0e6859b5918f473e21	subjective assessment of super multiview video with coding artifacts	3d high efficiency video coding 3d hevc multiview perceptual disparity model mvpdm quality of experience qoe super multiview smv video subjective assessment video coding;image quality bit rate visualization three dimensional displays observers stereo image processing;bit rate;observers;visualization;three dimensional displays;image quality;stereo image processing	The subjective assessment of super multiview (SMV) video considers two main perceptual factors: image quality and visual comfort at the viewpoint transition. While previous works only covered raw content with high levels of visual comfort, this work supersedes them by targeting the subjective assessment of SMV content with coding artifacts. The outcome of this analysis yields important conclusions regarding the relationship between these two factors, indicating that 1) the perceived image quality is independent from the view point change speed, and 2) the perceived visual comfort at the view point transition is independent from the image quality. These conclusions facilitate the extension of the scope of existing subjective perception models, designed for raw SMV content, to coded content.		Rocio Recio;Pablo Carballeira;Jes&#x00FA;s Guti&#x00E9;rrez;Narciso Garcia	2017	IEEE Signal Processing Letters	10.1109/LSP.2017.2695897	image quality;subjective video quality;computer vision;visualization;computer science;video quality;multimedia;multiview video coding;computer graphics (images)	Vision	44.35866274718715	-20.063333276327857	26340
2ad9be14eeb89105e49b3f7fc180aa4dfe4b3636	fast direct interpolation for computer graphics applications	algorithme rapide;interpolation;computer graphics;interpolacion;virgule fixe;coma fija;computer graphic;fixed point;fast algorithm;grafico computadora;infographie;algoritmo rapido	Abstract#R##N##R##N#Interpolation by the widely used spline function method requires excessive computation time because floating point arithmetic is used whenever a polynomial coefficient is determined. This paper presents a direct interpolation method (Fast Direct Interpolation) which attains sufficient precision with fixed point arithmetic. The processing speed and accuracy of the FDI method are also investigated. Since the FDI method can easily be implemented in hardware and adapted to parallel processing, it is suitable for the high-speed processing demands of computer graphics. An optimum FDI interpolation parameter is determined by considering various practical applications. Finally, continuity is proved for results obtained from fast direct interpolation.	computer graphics;interpolation	Norio Akamatsu;Nobuaki Sakou;Robert Craigin	1988	Systems and Computers in Japan	10.1002/scj.4690190311	mathematical optimization;bilinear interpolation;interpolation;computer science;theoretical computer science;stairstep interpolation;algorithm;statistics;image scaling;computer graphics (images)	Graphics	51.22352242344547	-14.068273484039338	26342
35196ae601b5a13fa4ac011302c3561650699a8d	evaluating board level solder interconnects reliability using vibration test methods	solder interconnect;reliability;vibration test;期刊论文	In this paper, three types of accelerated test methods based on vibration loadings are conducted and compared for board level mechanical reliability evaluation. The first type is fixed frequency sine vibration. The second type is swept sine vibration within a narrow-band of frequency. And the third type is swept random vibration within a narrow-band of frequency. The PCB responses were recorded using a high speed strain data acquisition system. The eigenfrequency of test boards were obtained with the FFT (Fast Fourier Transform) of the strain data of the PCBs during vibration. The PCBs' responses under different tests are compared. The failure processes were monitored and characterized. Results show that the vibrating amplitude is highly dependent upon the frequency ratio. The variation of PCBs’ eigenfrequency may cause the difference of loading amplitudes for fixed frequency vibration, which reduced the repeatability and comparability. The other two vibration methods within a narrow-band frequency could eliminate the influence from the frequency variation of the test boards. The differences of these methods are the loading density and repetitions. The failure processes of the three types of test methods are similar. Four failure stages were found from collected failure data. Weibull plot results show the characteristic life of the solder interconnects which are verified with loading repetition.	desoldering;speaker wire	Yang Liu;Fenglian Sun;Hongwu Zhang;Jun Wang;Zhen Zhou	2014	Microelectronics Reliability	10.1016/j.microrel.2014.07.058	structural engineering;electronic engineering;engineering;reliability;mathematics;forensic engineering;statistics	EDA	52.833047677410846	-1.6709832590451272	26369
1e8650b082a75bdc82840ec51b6a21317409a0b6	multimedia data recovery using information hiding	data hiding;interpolation;image coding;decoding;information hiding;copy protection;digital watermarking multimedia data recovery information hiding data hiding copyright protection content associative signature image remote region imperfect transmission embedded signature extraction neighborhood information decoder receiver image restoration interpolation method;copyright;data encapsulation;copyright protection;interpolation method;multimedia data;multimedia communication;data encapsulation watermarking copyright protection data mining image restoration access control heat engines propagation losses decoding interpolation;error resilience;copy protection multimedia communication data encapsulation copyright security of data image coding decoding interpolation;security of data	Abstruct In this work we study certain aspect of data hiding problem, not for the classical application of copyright protection, but instead for media data recovery. A medium data, for example, an image, is partitioned into N components. A content-associative signature of each component is generated and inserted imperceptibly into remote region of the image. When imperfect transmission of image causes loss of blocks, the embedded corresponding signature is extracted and used to recover the lost blocks of data. Compared to conventional approaches, this new method can provide the knowledge of the lost block content, in addition to neighborhood information, to the decoder at the receiver end. With the knowledge of the lost block content, better restoration results can be reached, especially when conventional neighborhood interpolation method is used together with data hiding approach.	circuit restoration;data recovery;embedded system;interpolation	Hong Heather Yu;Peng Yin	2000		10.1109/GLOCOM.2000.891850	computer science;theoretical computer science;internet privacy;information hiding;computer security	HCI	39.68652629295958	-12.213092282681487	26400
a30ecbca9a9a9c332d7ddce5bfe39b292f7f40ee	a robust without intra-frame distortion drift data hiding algorithm based on h.264/avc	data hiding;bch code;intra frame distortion drift;h 264 advanced video coding avc	This paper presents an improved robust data hiding algorithm based on BCH syndrome code (BCH code) technique and without distortion drift technique. The BCH code technique, which can correct the error bits caused by network transmission, packet loss, video-processing operations, various attacks, etc., encodes the embedded data by using BCH code before data hiding. The without distortion drift technique in this paper is that we exploit several paired-coefficients of a 4 × 4 discrete cosine transform (DCT) block to accumulate the embedding induced distortion, and the directions of intra-frame prediction are utilized to avert the distortion drift. It is proved analytically and shown experimentally that the new data hiding algorithm can get more robustness, effectively avert intra-frame distortion drift and get high visual quality.	algorithm;bch code;coefficient;discrete cosine transform;distortion;embedded system;experiment;h.264/mpeg-4 avc;intra-frame coding;mega man network transmission;network packet;streaming media;visual effects	Yunxia Liu;Zhitang Li;Xiaojing Ma;Jian Liu	2013	Multimedia Tools and Applications	10.1007/s11042-013-1393-0	real-time computing;telecommunications;computer science;theoretical computer science;bch code;information hiding	EDA	48.397064549434695	-16.263468067084204	26407
9db427839711eaaa3cfd31a9bbdd6d9a6a03a868	robust copyright-protection scheme based on visual secret sharing and bose-chaudhuri-hocquenghem code techniques	discrete wavelet transforms;digital watermarking;gaussian filters	Abstract. A robust copyright scheme for image protection based onvisual secret sharing (VSS) and Bose–Chaudhuri–Hocquenghem(BCH) code techniques is proposed. This scheme not only maintainsthe quality of a host image without the change of any pixel value butalsogeneratesameaningfulownershipsharetoimprovethemanage-mentofimagecopyright.Inaddition,nocodebookisrequiredtostore,and the watermark size is independent of the host image. The robust-ness of watermarking can be enhanced by BCH code. The proposedscheme contains ownership share construction and watermarkextraction. In the first phase, an encoded watermark is generatedby BCH code from a watermark. Next, an image feature is thenextracted by the discrete wavelet transform decomposing from thehost image. Finally, an ownership share can be generated by VSStechnique from the image feature and the encoded watermark. Inthe second phase, a master share can be produced from a suspectimage. By stacking the master and the ownership shares andusing BCH code, an extracted watermark can be obtained. Theexperimental results show that the proposed scheme using theBCH(15,5) has better robust performance and practicability thanexisting schemes. © 2012 SPIE and IS&T. [DOI: 10.1117/1.JEI.21.4.043018]	bch code;secret sharing	Tzuo-Yau Fan;Bin-Chang Chieu;Her-Chang Chao	2012	J. Electronic Imaging	10.1117/1.JEI.21.4.043018	digital watermarking;computer science;theoretical computer science;mathematics;internet privacy;watermark;computer security	Crypto	39.85486087691247	-10.725698143788414	26433
fed400eb8e294b8a46c0206672d68adc7a749cc6	a unified scheme of some nonhomogenous poisson process models for software reliability estimation	stochastic processes software reliability;mean value function mvf;nonhomogeneous poisson process nhpp;software reliability growth model srgm;harmonic mean;weighted geometric mean;weighted arithmetic mean;software reliability solid modeling arithmetic software quality programming reliability engineering software measurement statistical distributions earth observing system power system harmonics;weighted harmonic mean;stochastic processes;power transformer;software reliability growth model;power transformation;software reliability;mean value function software reliability nonhomogeneous poisson processes weighted arithmetic weighted geometric weighted harmonic mean weighted means;nonhomogeneous poisson process	In this paper, we describe how several existing software reliability growth models based on Nonhomogeneous Poisson processes (NHPPs) can be comprehensively derived by applying the concept of weighted arithmetic, weighted geometric, or weighted harmonic mean. Furthermore, based on these three weighted means, we thus propose a more general NHPP model from the quasi arithmetic viewpoint. In addition to the above three means, we formulate a more general transformation that includes a parametric family of power transformations. Under this general framework, we verify the existing NHPP models and derive several new NHPP models. We show that these approaches cover a number of well-known models under different conditions.	reliability engineering;software quality;software reliability testing	Chin-Yu Huang;Michael R. Lyu;Sy-Yen Kuo	2003	IEEE Trans. Software Eng.	10.1109/TSE.2003.1183936	stochastic process;mathematical optimization;harmonic mean;statistics	SE	31.329481536996536	-18.66313580270397	26457
18f1506a8a80c615476f9e7ae81d6c5b6854f7ce	rare event simulation via randomized quasi-monte carlo (abstract)			monte carlo method;quasi-monte carlo method;randomized algorithm;simulation	Hongmei Chi;Deidre W. Evans	2009			computational physics;quasi-monte carlo method;physics	Logic	41.473697923822975	1.4247776954564568	26459
763f85f75fdc43c6736a157a0057d30463a17021	quantum novel genetic algorithm based on parallel subpopulation computing and its application	subpopulation parallel computing;space division;quantum genetic algorithm	A quantum novel genetic algorithm based on subpopulation parallel computing is presented, where quantum coding and rotation angle are improved to inspire more efficient genetic computing methods. In the algorithm, each axis of the solution space is divided into k parts, the individual (or chromosome) from each different subspace being coded differently, and the probability amplitude of each quantum bit or Q-bit is regarded as two paratactic genes. The basic quantum computing theory and classical quantum genetic algorithm are briefly introduced before a novel algorithm is presented for the function optimum and PID problem. Through a comparison between the novel algorithm and the classical counterpart, it is shown that the quantum inspired genetic algorithm performs better on running speed and optimization capability.	apache axis;definition;feasible region;genetic algorithm;mathematical optimization;pid;parallel computing;probability amplitude;quantum computing;quantum gate;quantum programming;qubit;software release life cycle	Rigui Zhou;Jian Cao	2012	Artificial Intelligence Review	10.1007/s10462-012-9312-8	natural computing;quantum fourier transform;theoretical computer science;parallel algorithm;quantum algorithm for linear systems of equations;quantum algorithm;algorithm;quantum phase estimation algorithm;quantum sort;population-based incremental learning	AI	28.94250415114276	-4.743243669588955	26526
1f0c0b0c7cba098a421d8e1de8def865697286ad	special issue on emerging h.264/avc video coding standard	video coding	The growing availability and widespread use of digital multimedia information and personal wireless communication devices have motivated development of several video compression standards over the past two decades. These compression standards have been developed to achieve better picture quality, higher coding efficiency, and more error robustness for various applications. The H.264/MPEG-4 part 10 is the powerful and state-of-the-art video compression standard recently developed by the ITU-T/ISO/IEC Joint Video Team (JVT) consisting of experts from ITU-T’s Video Coding Experts Group (VCEG) and ISO/IEC’s Moving Picture Experts Group (MPEG). This new international video coding standard aims at having significant improvements in coding efficiency and error robustness in comparison to the previous standards such as MPEG-2, H.263, and MPEG-4 part 2. This special issue brings together practitioners and researchers working in all aspects of the H.264/AVC standard based video compression, communication, and networking. We are very pleased with the large number of high-quality submissions for this special issue. This issue consists of 17 papers, which are organized into the following sections: survey, motion estimation, encoding, rate control, networks and implementation. This special issue opens with an overview paper about the H.264/AVC standard by Kwon et al., who discuss various important features of the standard and compare the coding scheme with the other standards. Efficient motion compensation is one of the key reasons for superior performance of the H.264/AVC standard compared to older standards such as MPEG-2/4 and H.263. Unfortunately, the motion estimation is the most computation intensive part of a video encoder. A set of four papers presents techniques for fast motion estimation. The first paper, ‘‘Fast Motion Search with Efficient Inter-Prediction Mode Decision for H.264’’ by Kuo et al., presents a fast inter-prediction mode decision and motion search algorithm by employing a multiresolution motion estimation scheme and an adaptive rate-distortion model with early termination rules. The next paper, ‘‘Fast Motion Estimation and Inter Mode Decision for H.264/MPEG-4 AVC Encoding’’ by Zhou et al., introduces a new concept of ‘adaptive diversity search strategy’ for fast motion vector (MV) search. The authors investigate three fast motion estimation strategies by MV merging and splitting for variable size block motion estimation, which explores the correlation of MVs of overlapping blocks. The third paper, ‘‘Fast Integer-Pel and Fractional-Pel Motion Estimation for H.264/AVC’’ by Chen et al., describes a fast motion estimation algorithm for both the fast integer-pel and fractional-pel search. It has almost the same quality performance as that of the full-search scheme. The final paper, ‘‘Fast H.264 Intra-Prediction Mode Selection Using Joint Spatial and Transform Domain Features’’ by Kim et al., presents a low-complexity intra-prediction mode selection scheme without significant rate-distortion performance degradation. The proposed scheme uses spatial and transform domain features of the target block to filter out the majority of candidate modes. Intra-frame mode selection and inter-frame mode selection are new features introduced in the H.264 standard. Intra-frame mode selection dramatically reduces spatial redundancy in I frames, while inter-frame mode selection significantly affects the output quality of P-/B frames by selecting an optimal block size with motion vector(s) or a mode for each macroblock. Unfortunately, this feature requires a myriad amount of encoding time when a full-search method is utilized. The third section consists of four papers on efficient encoding.	algorithmic efficiency;block size (cryptography);computation;data compression;distortion;elegant degradation;encoder;entity–relationship model;fractional fourier transform;h.264/mpeg-4 avc;image quality;intra-frame coding;mpeg-2;mv-algebra;macroblock;motion compensation;motion estimation;moving picture experts group;pdf/a;pixel;search algorithm;video coding format	Sunil Kumar;Mrinal Kanti Mandal;Sethuraman Panchanathan	2006	J. Visual Communication and Image Representation	10.1016/j.jvcir.2005.12.002	scalable video coding;h.263;computer science;h.261;multiview video coding	Vision	46.045460822206834	-18.65586726387694	26528
402439a2475b3d55cf6459d3601e9d3d4b008fe5	adaptive reversible data hiding scheme based on integer transform	adaptive embedding;integer transform;reversible data hiding	In this paper, we present a new reversible data hiding algorithm based on integer transform and adaptive embedding. According to the image block type determined by the pre-estimated distortion, the parameter in integer transform is adaptively selected in different blocks. This allows embedding more data bits into smooth blocks while avoiding large distortion generated by noisy ones, and thus enables very high capacity with good image quality. For instance, by the proposed method, we can embed as high as 2.17bits per pixel into Lena image with a reasonable PSNR of 20.71dB. Experimental results demonstrate that the proposed method outperforms some state-of-the-art algorithms, especially for high capacity case.		Fei Peng;Xiaolong Li;Bin Yang	2012	Signal Processing	10.1016/j.sigpro.2011.06.006	combinatorics;discrete mathematics;theoretical computer science;mathematics	Crypto	41.25418744049875	-13.061318086579279	26585
6e964dc3735fab603a0e45ef8ac28464b422f0c4	reiss and thomas' automatic selection of the number of extremes	ley pareto;evaluation performance;sample size;generalized extreme value distribution;62g05;variation reguliere;analisis estadistico;performance evaluation;analisis datos;semi parametric estimation;random sampling;evaluacion prestacion;generalized pareto distribution;simulation;metodo semiparametrico;erreur quadratique moyenne;metodo penalidad;62g32;methode semiparametrique;upper extremity;queue distribution;loi pareto;variacion regular;62f07;data analysis;distribution function;penalty method;60g70;methode penalite;statistical analysis;cola distribucion;extreme value;pareto distribution;mean square error;statistical computation;valeur extreme;analyse statistique;calculo estadistico;semiparametric method;regular variation;62d05;analyse donnee;calcul statistique;asymptotic mean squared error;error medio cuadratico;60e05;distribution tail;valor extremo;mean squared error	Most widely used semi-parametric estimators of the extreme value parameter depend on the number of upper extremes which locate where the tail of a distribution begins. In the presence of a random sample with 5nite size, the problem concerning the choice of the number of upper extremes is not easy to handle. This number k is not only governed by the sample size n, but also ruled by parameters characterizing F . When the underlying distribution function is known, the optimum value k can be attained through the minimization of the asymptotic mean squared error of the considered estimator. Nevertheless, the merit of such procedure may be compromised by the assessment of k values equaling the sample size n. An alternative procedure entails that the adequate number k should be the value which minimizes a mean distance encapsulating a penalty term just as presented by Reiss and Thomas (Statistical Analysis of Extreme Values, Birkh> auser, Basel, 1997, P. 121). The performance evaluation of Reiss and Thomas’ heuristic procedure will be carried out undertaking the asymptotically determined k values as a reference. c © 2003 Elsevier B.V. All rights reserved.	heuristic;maxima and minima;mean squared error;performance evaluation;semiconductor industry;tridiagonal matrix algorithm	Cláudia Neves;M. Isabel Fraga Alves	2004	Computational Statistics & Data Analysis	10.1016/j.csda.2003.11.011	econometrics;calculus;mathematics;mean squared error;statistics	AI	32.465229940711666	-22.85256419090882	26639
93c059dfea98ded5f39894050cf3717f4607a100	microgrid generation units optimum dispatch for fuel consumption minimization		This paper utilizes the Karush–Kuhn–Tucker (KKT) conditions to develop a scheme for economic dispatch of load to generation units in a microgrid. By using the (pure) KKT conditions, the true optimum can be found in the sense of minimizing the fuel consumption. This is the most economic way of dispatching the load to generators, but it may not be directly applicable in the real world. In practice extra constraints, in the form of swing generator(s), base load generator upper and lower operating limits, and spinning reserve are also required. These lead to a set of constrained KKT conditions, which are utilized to solve the problem. Note that although this solution obtained from constrained KKT conditions is not the true optimum, the extra constraints are there to enhance the system stability and reliability.This new approach is tested extensively in simulation against standard dispatch approaches, and its performance is seen to be superior in all cases.	dynamic dispatch;microgrid	Yi Han;Peter Michael Young;Daniel J Zimmerle	2013	J. Ambient Intelligence and Humanized Computing	10.1007/s12652-012-0158-3	mathematical optimization;karush–kuhn–tucker conditions	AI	33.44188690751222	1.6853002963665158	26662
7dd87d6f376c47c07448a808d39e50102909d24b	two-dimensional software reliability assessment with testing-coverage	software reliability mean square error methods parameter estimation;software reliability software reliability mathematical model equations numerical models data models;software;two dimensional nhpp;goodness of fit;reliability;growth factor;software reliability growth models;cobb douglas production function;parameter estimation method;testing coverage;mean square error;test coverage;srgm;mathematical model;mean square error methods;software reliability assessment measures;mean squared errors;two dimensional software reliability assessment;parameter estimation;numerical models;software reliability growth model;mean squared errors two dimensional software reliability assessment srgm software reliability growth models testing time testing coverage parameter estimation method;growth process;software reliability;testing time;goodness of fit software reliability growth model testing coverage cobb douglas production function two dimensional nhpp software reliability assessment measures parameter estimation;data models	We propose a two-dimensional SRGM describing a software reliability growth process on the two-dimensional time-space which consists of testing-time and testing-coverage. Our two-dimensional SRGM conserves the theoretical means for the effect of such two-types of the reliability growth factors to the software reliability growth process. And we also discuss a parameter estimation method and a software reliability assessment measure of our model. Further, we show numerical examples for software reliability analysis by using actual data. Finally, we conduct goodness-of-fit evaluation of our model to the actual data, and see that our model has a better performance than representative one-dimensional SRGMs in terms of mean squared errors.	estimation theory;numerical analysis;software bug;software project management;software quality;software reliability testing	Shinji Inoue;Shigeru Yamada	2008	2008 Second International Conference on Secure System Integration and Reliability Improvement	10.1109/SSIRI.2008.21	reliability engineering;data modeling;computer science;software reliability testing;mathematical model;reliability;mean squared error;code coverage;goodness of fit;estimation theory;software quality;statistics	SE	30.169828737650672	-18.610372879295856	26709
6d9d4f361ee398690479f8662a6274e1c6eadcb1	the centred parameterization and related quantities of the skew-t distribution	parameterization;skew t distribution;approximation of moments;62h05;information matrix;60e05;62e15	The skew-t family, in its univariate and multivariate versions, is a parametric family of probability distributions which is currently under intense investigation because of several appealing properties. The present paper addresses the question of the choice of its parameterization, and more generally of the selection of quantities of interest associated to this distribution. © 2011 Elsevier Inc. All rights reserved. 1. Background and motivation The skew-symmetric distributions constitute a broad set of probability distributions which currently is a very active theme in distribution theory. The term ‘skew-symmetric’ refers to a quite general formulation which starts from a d-dimensional symmetric density f0(x), such that f0(−x) = f0(x) for all x ∈ Rd, to generate a whole set of perturbed versions of f0 via the expression f (x) = 2f0(x)G{w(x)}, x ∈ Rd, which is a proper density function provided thatw(−x) = −w(x) and that G is the distribution function of a scalar random variable symmetrically distributed around 0. Introductory overviews of this research area are provided by the book edited by Genton [16] and the review paper by Azzalini [6]. Within this vast area, there are certain families of distributions which are of special interest. A noteworthy case is the skew-normal distribution, since this has been the first case examined, and because it represents a superset of the normal family, with an additional parameter to regulate skewness. In the univariate case its density function at x (x ∈ R) is 2 ω φ(z)Φ(α z), z = x − ξ ω , (1) where φ and Φ denote the N(0, 1) density and distribution function, respectively, and ξ , ω and α are parameters which regulate location, scale and shape (ω > 0); we shall use the notation SN(ξ , ω2, α) to refer to this distribution. When α = 0 we return to the familiar N(ξ , ω2) distribution. The skew-normal family is by construction a more flexible class of distributions than the normal one, because of the additional parameter α which regulates skewness. This fact, combined with a number of formal properties which this distribution enjoys, makes it an attractive proposal for statistical work. ∗ Corresponding author. E-mail address: azzalini@stat.unipd.it (A. Azzalini). 0047-259X/$ – see front matter© 2011 Elsevier Inc. All rights reserved. doi:10.1016/j.jmva.2011.05.016 74 R.B. Arellano-Valle, A. Azzalini / Journal of Multivariate Analysis 113 (2013) 73–90 Although more flexible than the normal family, the presence of a single parameter to regulate the density shape in (1) cannot suffice to handle adequately the very diverse types of situations which aremet in practical work. A frequent problem is that the tails of the observed distribution are thicker than the normal ones, a situation encountered in a range of application areas, almost systematically in econometrics and finance. While statistical modelling is an art, and therefore it escapes fixed rules, there is a widespread perception that in a large number of practical cases concerning continuous type of observations with unbounded support one can produce a satisfactory result working with a distribution which allows independent regulation of location, scale, skewness and kurtosis. Several formulations exist in the literature to tackle this requirement, from the classical Pearson’s families to more recent proposals, such as [15,18], and in another direction [21]. Within the framework of skew-symmetric distributions, the currently most credited formulation in the above sense is provided by the skew-t (ST) distribution, whose density function at x (x ∈ R) is	distribution (mathematics);emoticon;skew heap;statistical model;tails	Reinaldo Boris Arellano-Valle;Adelchi Azzalini	2013	J. Multivariate Analysis	10.1016/j.jmva.2011.05.016	parametrization;mathematical optimization;combinatorics;fisher information;mathematics;statistics	ML	35.01344085281464	-18.38933293583422	26782
de8dfeb6c410c4e1fec6a166ceeda29a7bad1240	a fast speech feature extraction method based on perceptual hashing		Aiming at the existing problems of speech feature extraction algorithms based on perceptual hashing, such as problems with low efficiency and weak security, we present a fast speech perceptual hashing feature extraction method based on modified discrete cosine transform (MDCT) and compressed sensing (CS). Firstly, the speech signal is conducted with MDCT transform after processed by applying preprocessing, framing, adding window, and then the MDCT coefficients are regarded as the perceptual feature value. Secondly the measurement matrix in CS is applied to reduce the dimension of perceptual feature values. Finally, the feature value is used to conduct a hashing structure process to produce the perceptual hashing sequence. Experimental results show that the proposed method has a high efficiency in terms of time consumption, security and distinction.	algorithm;authentication;coefficient;compressed sensing;feature extraction;framing (world wide web);hash function;mp3;modified discrete cosine transform;perceptual hashing;preprocessor;zobrist hashing	Qiu-Yu Zhang;Sibin Qiao;Tao Zhang;Yi-Bo Huang	2017	2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2017.8392951	compressed sensing;robustness (computer science);machine learning;hash function;feature extraction;artificial intelligence;perceptual hashing;modified discrete cosine transform;matrix (mathematics);speech processing;computer science	AI	43.48478680545432	-8.609737313441624	26790
4c90f74f6da43cf70b79cdc9aff6e5300bab2119	an improved quantum-behaved particle swarm optimization for multi-peak optimization problems	68t20;species;evolutionary computation;quantum behaved particle swarm optimization;qa mathematics;multiple solution;multi peak optimization problems;optimization problem;qa75 electronic computers computer science;particle swarm optimizer;particle swarm optimization;is success;systems of nonlinear equations;68w20;fitness function;evolutionary computing	In this paper, we propose an improved quantum-behaved particle swarm optimization (QPSO), namely species-based QPSO (SQPSO), using the notion of species for solving multi-peak optimization problems. In the proposed SQPSO, the population is divided into subpopulations (species) based on their similarities. Each species is grouped around a dominating particle called the species seed. During the process of iterations, species are able to simultaneously optimize towards multiple optima by using QPSO, so each peak will definitely be searched in parallel, regardless of whether it is global or local optima. Further, SQPSO is applied to solve systems of nonlinear equations describing certain fitness functions, which are multi-peak functions. Our experiments demonstrate that SQPSO is able to search multiple peaks of a given function as accurate and efficient as possible. Finally the experiments for the solutions of systems of nonlinear equations show that the algorithm is successful in locating multiple solutions with better accuracy.	algorithm;distribution (mathematics);mathematical optimization;nonlinear system;particle swarm optimization	Ji Zhao;Jun Sun;Choi-Hong Lai;Wenbo Xu	2011	Int. J. Comput. Math.	10.1080/00207160903521501	optimization problem;mathematical optimization;multi-swarm optimization;artificial intelligence;machine learning;mathematics;particle swarm optimization;fitness function;evolutionary computation	Theory	27.44428003012793	-4.216220509249027	26890
0d9032e34b2fc8374163aa2d759d2d7d5b9e702f	control of noisy differential-drive vehicles from time-bounded temporal logic specifications	formal methods;reactive and sensor based planning	We address the problem of controlling a noisy differential drive mobile robot such that the probability of satisfying a specification given as a Bounded Linear Temporal Logic (BLTL) formula over a set of properties at the regions in the environment is maximized. We assume that the vehicle can precisely determine its initial position in a known map of the environment. However, inspired by practical limitations, we assume that the vehicle is equipped with noisy actuators and, during its motion in the environment, it can only measure the angular velocity of its wheels using limited accuracy incremental encoders. Assuming the duration of the motion is finite, we map the measurements to a Markov Decision Process (MDP). We use recent results in Statistical Model Checking (SMC) to obtain an MDP control policy that maximizes the probability of satisfaction. We translate this policy to a vehicle feedback control strategy and show that the probability that the vehicle satisfies the specification in the environment is bounded from below by the probability of satisfying the specification on the MDP. We illustrate our method with simulations and experimental results.	angularjs;computation;control theory;encoder;feedback;linear temporal logic;markov chain;markov decision process;mobile robot;model checking;sampling (signal processing);simulation;statistical model;time complexity;velocity (software development);wheels	Igor Cizelj;Calin Belta	2013	2013 IEEE International Conference on Robotics and Automation	10.1177/0278364914522312	control engineering;simulation;computer science;artificial intelligence;control theory;statistics	Robotics	52.68346284261901	-21.274930548523432	26908
ca7d753699f18512d5534a6937ec67f44c47078a	early detection algorithms for 8×8 all-zero blocks in h.264/avc	quantization;sum of absolute difference;detection algorithms;h 264 avc;signal detection;video quality;all zero block;video sequences;early detection algorithms;sum of absolute transformed;video coding;accuracy;automatic voltage control;theoretical analysis;detection algorithm;transforms;proposals;early detection;sum of absolute differences	8 times 8 transform has been introduced in H.264psilas high profile to improve the video quality. After transform and quantization, if all the coefficients of the blockpsilas residue data become zero, this block is called all-zero block (AZB). Many 4 times 4 transform AZB early detection algorithms have been proposed to skip transform and quantization process. In this paper, after theoretical analysis performed for the sufficient condition of 8 times 8 AZB detection, sum of absolute differences(SAD) and sum of absolute transformed(SATD) based 8 times 8 AZB detection algorithms are proposed. Experimental results show that the proposed algorithms achieve major improvement of computation reduction from 0.95% to 79.48% for 720p sequences. The computation reduction increases as QP increases.	algorithm;coefficient;computation;h.264/mpeg-4 avc;sum of absolute transformed differences	Qin Liu;Yiqing Huang;Takeshi Ikenaga	2008	2008 IEEE 10th Workshop on Multimedia Signal Processing	10.1109/MMSP.2008.4665103	sum of absolute differences;computer vision;mathematical optimization;discrete mathematics;quantization;computer science;video quality;theoretical computer science;mathematics;accuracy and precision;detection theory	Vision	47.34113859584431	-18.43418769725336	26939
e5abb0e1c926122c9578ca0cb36d594aae31f890	search performance quantification on evolutionary algorithm -derivation and application of times of improved solutions	evolutionary computation performance evaluation testing shape cybernetics biological system modeling genetic algorithms traveling salesman problems biological cells genetic mutations;probabilistic behavior;times of improved solutions;genetic operator;number of searchable solutions;evolutionary computation;search performance quantification;search problems evolutionary computation;times of improved solutions search performance quantification evolutionary algorithm biological evolutionary process probabilistic behavior genetic operation number of searchable solutions;genetic operation;biological evolutionary process;hill climbing;search problems;evolutionary algorithm;evolutionary process	"""Recently, Evolutionary Algorithms (EAs), which are based on the biological evolutionary process, are widely studied in a lot of fields. However, their probabilistic behavior makes the theoretical investigation difficult. As a result, the search performances of EAs have been tested through actual simulations in most of the studies. The parameters of genetic operations have also been decided experientially or by trial and error. This study tries to mathematically quantify the search performance of EAs using the parameters of the operations for the purpose of theoretical investigation of them. This paper defines """"Number of Searchable Solutions (NSS)"""" and """"Times of Improved Solutions (TIS)"""" to indicate search performance of EAs and attempts to formulate them. This paper employs hill climbing method to an unimodal increasing function as a basic study for this purpose, and it derives the formulas for NSS and TIS that consist of the parameters of the genetic operation. This paper shows that appropriate formulas are derived and it enables us to compare the performance and optimize the parameters of the operations without actual trials."""	evolutionary algorithm;hill climbing;performance;simulation	Tomohiro Yoshikawa;Naotoshi Hoshiya	2006	2006 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2006.385008	evolutionary programming;mathematical optimization;evolutionary music;genetic algorithm;interactive evolutionary computation;human-based evolutionary computation;computer science;artificial intelligence;hill climbing;genetic operator;machine learning;evolutionary algorithm;genetic representation;mathematics;evolutionary computation	Robotics	27.401857652833343	-7.535170766736492	26963
f19cb5eb1a3d983b61ac3c729d0746e4707bd88f	global feature extraction operations for near-sensor image processing	detection forme;image processing;circuit vlsi;procesamiento imagen;cmos logic circuits image processing feature extraction vlsi digital signal processing chips;circuito logico;segmentation;feature extraction image processing photodiodes image segmentation threshold voltage logic circuits filling hysteresis very large scale integration image sensors;shape detection;cmos global feature extraction operations near sensor image processing low level image processing 2d global logic unit logical circuits regular network image segmentation hole filling smallest circumscribing rectangle thresholding hysteresis propagation patterns object position global functions single chip vlsi technology;pulga electronica;traitement image;chip;deteccion forma;vlsi circuit;circuit logique;feature extraction;cmos logic circuits;pattern recognition;vlsi;digital signal processing chips;reconnaissance forme;circuito vlsi;reconocimiento patron;logic circuit;puce electronique;segmentacion	Near-sensor image processing (NSIP) is a new approach to low-level image processing in which the ability to perform global operations is crucial. We define a 2-D global logic unit (GLU), which consists of simple logical circuits arranged in a regular net, and describe how to perform powerful segmentation and feature-extracting operations using this net. Typical segmentation operations performed by the net are hole filling, smallest circumscribing rectangle, thresholding with hysteresis, and arbitrary propagation patterns. We describe how to obtain features like the position of an object in the image. Finally, we show that the NSIP concept, including the global functions, can be implemented in a single chip using today's VLSI technology.		Anders Åström;Robert Forchheimer;Jan-Erik Eklund	1996	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.481674	chip;computer vision;logic gate;image processing;feature extraction;computer science;theoretical computer science;very-large-scale integration;segmentation	Vision	40.70778411639425	-2.9503814692602783	26997
a8800c8207b5564f1b98d5816b086e49c1a2e316	nature-inspired intelligent optimisation using the bees algorithm.	honey bees;swarm intelligence;intelligent optimisation;bees algorithm	The Bees Algorithm models the foraging behaviour of honey bees in order to solve optimisation problems. The algorithm performs a kind of exploitative neighbourhood search combined with random explorative search. This paper describes the Bees Algorithm, and compares its functioning and performance with those of other state-of-the-art nature-inspired intelligent optimisation methods. Two application cases are presented: the minimisation of a set of well-known benchmark functions, and the training of neural networks to reproduce the inverse kinematics of a robot manipulator. In both cases, the Bees Algorithm proved its effectiveness and speed. Compared with other state-of-the-art methods, the performance of the Bees Algorithm was very competitive.		Duc Truong Pham;Marco Castellani;Le Thi Hoai An	2014	Trans. Computational Collective Intelligence	10.1007/978-3-642-54455-2_2	mathematical optimization;engineering;artificial intelligence;machine learning;bees algorithm	AI	26.604406564291697	-4.05305695413648	27002
4601842235ce5c730bcc1b5cc914d4f655292361	a chaos-based robust watermarking algorithm for rightful ownership protection	digital watermarking;fourier transform;singular value decomposition;chaotic map;zig zag scan;dc coefficient	Traditional singular value decomposition (SVD)-based watermarking schemes, already exist for watermark embedding on the image as a whole. In this paper, a chaos-based hybrid watermarking scheme that combines the Fourier transform and the SVD is proposed. Instead of modifying the original cover image to embed the watermark, a reference image is formed in the proposed scheme to embed the watermark. The watermark is embedded by modifying the singular values of the reference image. The watermark extraction process is semi-blind, i.e. it does not require the original image. Chaotic map is used to shuffle the pixel positions of the image, which can be used as a secret key to improve the security of the algorithm. The security of the scheme is further strengthened by applying Zig-Zag scan on the watermark before embedding. A series of experiments is conducted to prove the fidelity and robustness property of the proposed scheme. Experimental results show that our scheme is strong enough to resist common image-processing attacks, geometric distortions, and some composite attacks. The results also show that our scheme outperforms related works in most cases.	algorithm;arnold's cat map;distortion;embedded system;experiment;fidelity of quantum states;image processing;key (cryptography);pixel;semiconductor industry;singular value decomposition	Sanjay Rawat;Balasubramanian Raman	2011	Int. J. Image Graphics	10.1142/S0219467811004263	fourier transform;discrete mathematics;digital watermarking;theoretical computer science;dc bias;mathematics;watermark;singular value decomposition;algorithm	Vision	39.98119991836899	-10.432718268895151	27097
4d769d513f41b9ec00ca315c86a6c81af1a41e07	impact of h.264 advanced video coding inter-frame block sizes on video quality		In this paper, we present a perceptual-based encoding benchmarking of the H.264 Advanced Video Coding (AVC) inter-frame prediction variable block sizes for various spatial and temporal contents. This paper in order to quantify the impact on the video quality of the AVC inter-frame variable block sizes and the responsible prediction algorithm has disabled the motion estimation mechanism of the encoder and manually each block size is selected. Thus each time only one available block size out of the total seven is available to be searched for each MB and it is possible to examine the video quality impact of each block size independently to the remaining ones. The scope of this paper is to study if the use of sophisticated predictions algorithms and variable block sizes enhance the perceived quality of the encoded video signal or if there is not any significant quality degradation when the option of variable size is disabled.	algorithm;block size (cryptography);data compression;elegant degradation;encoder;h.264/mpeg-4 avc;motion estimation	Harilaos Koumaras;Michail-Alexandros Kourtis;Drakoulis Martakos;Christian Timmerer	2012			video compression picture types;scalable video coding;uncompressed video;deblocking filter;video processing;h.261;multiview video coding	Metrics	47.141138609909014	-18.059905214497004	27182
ad813f147c8bc1e38a99ce8223cd448180fd32ff	high capacity image data hiding scheme for grouping palette index	threshold value selection;data hiding image palette watermarking;watermarking;data hiding;private key cryptography;extraction procedure;image index table;image coding;probability;information security;color pixel group appearance probability;color;computational geometry;euclidean distance;data engineering;information concealment;data mining;secret key value;data encapsulation;watermarking computational geometry data encapsulation feature extraction image coding image colour analysis private key cryptography probability;protection;indexes;system evaluation;image color analysis;image colour analysis;cryptography;feature extraction;information management;image quality;pixel;indexation;data encapsulation image quality data security information security data mining watermarking data engineering cryptography protection information management;high capacity image data hiding scheme;image palette;image watermarking;color picking;security;image palette index grouping;information embedding procedure;image watermarking high capacity image data hiding scheme image palette index grouping image index table information concealment threshold value selection color picking color pixel group appearance probability euclidean distance information embedding procedure secret key value extraction procedure image quality;data security	Applying the image palette to hide information, we usually embed them into the palette itself or into the index table of the image. Although the hiding schemes in palette could be a simpler way, the palette becomes more complicated and reveals that the information is embedded. In this paper, we will use the index table of image to conceal information. The proposed method selects a threshold value and picks the color with highest probability of appearance as a center to group the color pixels. The system evaluates Euclidean distances from the center to each of the pixels and collects the color pixels in the same group if the distance is less than or equal to the threshold value. In the embedding procedure, the system applies orderly the value of each bit of a secret key to shift the coordinate of color pixel. The extraction procedure groups the pixels, shifts according to the key value and determines whether the group is valid. Then, the information can be extracted from each group till all the pixels are determined. From the experimental results, the method increases the embedding capacity of information and improves the image quality efficiently. Furthermore, our method enhances the security of the system because the secret key is applied to shift the coordinate.	attribute–value pair;color;embedded system;euclidean distance;image quality;key (cryptography);palette (computing);pixel	Ching-Te Wang;Chiu-Hsiung Liao;Ruey-Maw Chen	2009	2009 Third International Conference on Multimedia and Ubiquitous Engineering	10.1109/MUE.2009.44	image quality;computer vision;feature extraction;computer science;cryptography;information security;theoretical computer science;probability;euclidean distance;data security;internet privacy;computer security;pixel	Vision	39.09197418309721	-10.933657590215402	27236
19b72727a605b661748e56b24447f03b3f21fc55	distributed video coding of wyner-ziv frames using trellis coded modulation	channel coding;wyner ziv;interpolation;decoding;ttcm;design engineering;turbo trellis coded modulation;videoconference;trellis coded modulation;point location;pixel interpolation technique;video sequences;turbo codes;wyner ziv frame;video coding modulation coding decoding design engineering costs streaming media video sequences turbo codes videoconference interpolation;video coding;parity bit stream distributed video coding wyner ziv frame turbo trellis coded modulation pixel interpolation technique;streaming media;image reconstruction;modulation coding;parity bit stream;turbo coding dvc wyner ziv coding ttcm;dvc;distributed video coding;wyner ziv coding;turbo coding;side information;turbo code;image sequences;video coding channel coding decoding image reconstruction image sequences interpolation trellis coded modulation turbo codes	Distributed video coding (DVC) is known as an emerging video coding technique, which primarily has a modified complexity balance in the encoder and decoder, in contrast to its traditional competitors. In DVC, we have a very simple low cost encoder which is an ideal feature for applications involving a large number of video capturing points located remotely and a centralized shared decoder. In this paper, we introduce a novel approach for DVC, using turbo trellis coded modulation (TTCM) to generate the parity information at the encoder to be sent over the channel and then to decode the parity with the side information at the decoder. The side information is generated using key frames passed to the decoder by the use of a pixel interpolation technique. TCM symbols are formed using the side information and the parity bit stream which are fed to the TTCM decoder. The decoded output is used to reconstruct the final video sequence. The results are compared with a turbo coding based DVC and it is evident that the proposed method outperforms its turbo code based counterpart by a significant margin	bitstream;centralized computing;data compression;encoder;interpolation;key frame;modulation;parity bit;pixel;trellis quantization;turbo code	W. A. Rajitha Jayaruwan Weerakkody;Warnakulasuriya Anil Chandana Fernando;A. B. B. Adikari	2006	2006 Canadian Conference on Electrical and Computer Engineering	10.1109/CCECE.2006.277503	distributed source coding;turbo code;electronic engineering;soft-decision decoder;telecommunications;computer science;theoretical computer science;statistics	Vision	49.26726795417738	-17.07424034694893	27242
88b8818b27e5245de8017e3a951641a96c7f30d6	a bandwidth-efficient stereo video streaming system		Stereo video streaming is an emerging topic ever since the popularity of stereo video in the recent years. In this paper, we consider the anaglyph video, which displays the red channel in left view and green/blue channel in right view, respectively. The traditional anaglyph streaming system transmits full RGB components of both views and those results in greater bandwidth consumption. In this paper, we present two novel algorithms to improve the traditional anaglyph system. For the first, we transmit the color channels that will be viewed by the users, and then we rearrange the data to adapt to the standard video compression format. For the second, we propose to perform video demosaicing in the post-processing stage instead of the pre-processing stage as in the traditional system. In addition, we develop a gradient-based stereo matching technique integrated with our demosaicing algorithm for better retrieval of observed data. Experimental results demonstrate that the proposed anaglyph video streaming system outperforms the state-of-the-art algorithms in both objective and subjective comparisons.	algorithm;anaglyph 3d;channel (digital image);color;computer stereo vision;demosaicing;gradient;preprocessor;streaming media;video coding format;video post-processing	Kai-Lung Hua	2017	IEEE Access	10.1109/ACCESS.2017.2710100	stereo cameras;video capture;video processing	Vision	43.61671633997878	-19.56813328891835	27290
c2d94716bfdabb57078a4dfe52a2d71f05f0cb21	an improved artificial fish swarm algorithm in image segmentation application	image segmentation;artificial fish swarm algorithm;adaptive strategy;niching	This paper proposes an artificial fish swarm clustering algorithm based on dynamic niche (FSDN), and a dynamic identification of the niches is performed at each generation to automatically evolve the optimal number of clusters as well as the cluster centers of the image data set. The experimental results show that FSDN algorithm has good performance, effectiveness and flexibility.	algorithm;cluster analysis;image segmentation;niche blogging;swarm	Ming-an Zhang;Yong Deng	2015		10.1145/2739482.2764882	mathematical optimization;computer science;artificial intelligence;machine learning	Robotics	26.893640847295497	-5.175982866928345	27371
7e207b66765059836b3334bd70eefda003522a82	an adaptive invasive weed optimization algorithm	step size;adaptation;global optimization;invasive weed optimization	With regards to the low search accuracy of the basic invasive weed optimization algorithm which is easy to get into local extremum, this paper proposes an adaptive invasive weed optimization (AIWO) algorithm. The algorithm sets the initial step size and the ̄nal step size as the adaptive step size to guide the global search of the algorithm, and it is applied to 20 famous benchmark functions for a test, the results of which show that the AIWO algorithm owns better global optimization search capacity, faster convergence speed and higher computation accuracy compared with other advanced algorithms.	adaptive stepsize;algorithm;benchmark (computing);computation;global optimization;mathematical optimization;maxima and minima	Shuo Peng;A.-J. Ouyang;Jeff Jun Zhang	2015	IJPRAI	10.1142/S0218001415590041	mathematical optimization;simulation;meta-optimization;derivative-free optimization;control theory;global optimization;adaptation	AI	28.09203924505729	-4.385266019284319	27479
ac37e38adbfe4a2e3853129881901670d01f2762	a saturation binary neural network for crossbar switching problem	crossbar switching problem;combinatorial optimization problems;saturation binary neuron model	A saturation binary neural network is proposed to solve the crossbar switching problem. In the proposed algorithm, neurons are updated according to different formula, then neurons enter into saturating state, and as a result, it makes the neural network escape from a local minimum stagnation. The proposed algorithm has been tested on a large number of instances and compared with other algorithms. The experimental results show that the proposed algorithm is superior to its competitors.	algorithm;artificial neural network;artificial neuron;biological neuron model;combinatorial optimization;crossbar switch;hopfield network;mathematical optimization;maxima and minima;simulation	Cui Zhang;Li-Qing Zhao;Rong Long Wang	2011		10.1007/978-3-642-23896-3_30	mathematical optimization;machine learning;mathematics;algorithm	ML	30.686001565440005	3.457075490935939	27506
28428aaef79e9e85453f4d6091367ce02a6a82f9	error-resilient delivery of region of interest video using multiple representation coding	streaming media video coding encoding image coding bit rate receivers bandwidth;video coding signal reconstruction;reconstruction fidelity error resilient delivery region of interest video multiple representation coding video delivery wireless networks cellular networks burst losses signal loss intervals low bitrate channels error prone channels;unequal protection region of interest video coding multiple representation coding;video coding;signal reconstruction	Video delivery over wireless and cellular networks can be severely impaired due to bandwidth limitations and also due to the presence of burst losses and signal-loss intervals. This paper proposes unequal protection of Region of interest (ROI) encoded videos for transmission over low-bitrate and error-prone channels. Here, a flexible and interactive ROI is introduced in a low-complexity, and standard-compliant fashion to encode the critical regions within each frame at a higher quality as compared to the background. Further, the reconstruction fidelity of the high-quality ROI is improved by unequally protecting the ROI using an error-resilient video delivery scheme known as Multiple Representation Coding (MRC). Simulation results indicate that the proposed strategy can facilitate a graceful recovery of the ROI in the presence of burst and signal losses.	cognitive dimensions of notations;encode;region of interest;simulation	Sourabh Khire;Arturo A. Rodriguez;Scott Robertson;Nikil Jayant	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6638015	scalable video coding;signal reconstruction;sub-band coding;real-time computing;telecommunications;computer science;coding tree unit;multimedia;context-adaptive binary arithmetic coding;multiview video coding	Robotics	48.98146171330498	-15.734914401040658	27510
4c474189009d6197fc08160d7e0c3ff57809f4ed	a comparison among combination-based permutation statistics for randomized complete block design	experimental design;block design;metodo estadistico;analisis numerico;permutation statistic;statistical simulation;metodo monte carlo;theorie approximation;65c05;stochastic method;test sans biais;05bxx;fonction repartition;estadistica test;statistique test;62g09;test permutation;62e17;simulacion numerica;estimation non parametrique;plan experiencia;methode monte carlo;permutation tests;statistical method;plan randomise;distribucion estadistica;plan bloc;analyse numerique;permutation;approximation theory;non parametric estimation;plan bloque;psychometrie;funcion distribucion;distribution function;plan aleatorizado;simulacion estadistica;numerical analysis;estimation erreur;62k99;distribution statistique;plan experience;62g10;error estimation;methode statistique;randomized design;simulation statistique;62p15;nonparametric combination;randomized complete block design;monte carlo method;simulation numerique;unbiased test;permutacion;estimacion error;psychometrics;methode stochastique;estimacion no parametrica;psicometria;monte carlo simulation;60e05;statistical distribution;62k10;test statistic;rcb design;numerical simulation;metodo estocastico;permutation test	It is well known that a best permutation test for all population distributions P does not generally exist, because the most powerful unbiased permutation test is a function of the population distribution P which is assumed to be unknown (Pesarin and Salmaso, 2010). In this work, we focus our attention on the randomized complete block design in case of an ordered categorical response variable, which is the typical reference setting in many psychometric studies. We compared via a Monte Carlo simulation study several combination-based permutation test statistics and we found out that the Multi-focus statistic (Finos and Salmaso, 2004) using the Fisher's combining function appears to be the more powerful solution which we proved also to be better under non normal errors than traditional parametric and rank-based nonparametric counterparts.	randomized algorithm	Rosa Arboretti Giancristofaro;Livio Corain;Susanna Ragazzi	2012	Communications in Statistics - Simulation and Computation	10.1080/03610918.2012.625752	computer simulation;econometrics;random permutation;calculus;mathematics;psychometrics;statistics;monte carlo method	ECom	32.16993026466836	-21.89952093526506	27550
534796386098476c8314974b60f56affad40173a	selecting random latin hypercube dimensions and designs through estimation of maximum absolute pairwise correlation	correlation theory;random processes;sampling methods;complex algorithms;degree of correlation;high-dimensional computer experiments;maximum absolute pairwise correlation;multicollinearity issues;random latin hypercube dimensions;uncorrected latin hypercube designs	Latin hypercubes are the most widely used class of design for high-dimensional computer experiments. However, the high correlations that can occur in developing these designs can complicate subsequent analyses. Efforts to reduce or eliminate correlations can be complex and computationally expensive. Consequently, researchers often use uncorrected Latin hypercube designs in their experiments and accept any resulting multicollinearity issues. In this paper, we establish guidelines for selecting the number of runs and/or the number of variables for random Latin hypercube designs that are likely to yield an acceptable degree of correlation. Applying our policies and tools, analysts can generate satisfactory random Latin hypercube designs without the need for complex algorithms.	algorithm;analysis of algorithms;computer experiment	Alejandro S. Hernandez;Thomas W. Lucas;Paul J. Sanchez	2012	Proceedings Title: Proceedings of the 2012 Winter Simulation Conference (WSC)		stochastic process;sampling;econometrics;combinatorics;latin hypercube sampling;mathematics;statistics	EDA	29.903840061474362	-16.457551681872527	27603
a57c3eb619abc73776e7fc85a3f194b7c1857f88	discrete sine transform-based interpolation filter for video compression		Fractional pixel motion compensation in high-efficiency video coding (HEVC) uses an 8-point filter and a 7-point filter, which are based on the discrete cosine transform (DCT), for the 1/2-pixel and 1/4-pixel interpolations, respectively. In this paper, discrete sine transform (DST)-based interpolation filters (DST-IFs) are proposed for fractional pixel motion compensation in terms of coding efficiency improvement. Firstly, a performance of the DST-based interpolation filters (DST-IFs) using 8-point and 7-point filters for the 1/2-pixel and 1/4-pixel interpolations is compared with that of the DCT-based IFs (DCT-IFs) using 8-point and 7-point filters for the 1/2-pixel and 1/4-pixel interpolations, respectively, for fractional pixel motion compensation. Finally, the DST-IFs using 12-point and 11-point filters for the 1/2-pixel and 1/4-pixel interpolations, respectively, are proposed only for bi-directional motion compensation in terms of the coding efficiency. The 8-point and 7-point DST-IF methods showed average Bjøntegaard Delta (BD)-rate reductions of 0.7% and 0.3% in the random access (RA) and low delay B (LDB) configurations, respectively, in HEVC. The 12-point and 11-point DST-IF methods showed average BD-rate reductions of 1.4% and 1.2% in the RA and LDB configurations for the Luma component, respectively, in HEVC.	algorithmic efficiency;blu-ray;coefficient;data compression;discrete cosine transform;discrete sine transform;frequency response;high efficiency video coding;interpolation;kalman filter;motion compensation;pixel;random access;traffic collision avoidance system;video coding format	MyungJun Kim;Yung-Lyul Lee	2017	Symmetry	10.3390/sym9110257	interpolation;discrete sine transform;discrete cosine transform;sinc function;data compression;mathematics;luma;quarter-pixel motion;motion compensation;mathematical optimization	Vision	44.43206805758591	-16.75272388892273	27618
b7b21945c46be473b19ea976869e1a0f41de3970	the estimation of probability distribution of sde by only one sample trajectory	euler scheme;simulation;logistic model;stochastic differential equation;journal;stochastic system;monte carlo method;probability distribution;state space;logistic equation;monte carlo;computer simulation;markov chain	This paper focuses on the method of the simulation of a stochastic system and the main method of our paper is the Monte Carlo computation simulation method. Taking the stochastic Logistic equation as an example, we present the simulation of the sample trajectory by Euler scheme and the invariant probability distribution of stochastic differential equations with the Monte Carlo method. We also compare the simulation result with the analytical result for the autonomous stochastic Logistic model. Moreover, the stochastic Logistic equation with Markovian switching which is described by a Markov chain taking values in a finite state space is considered.		Guixin Hu;Ke Wang	2011	Computers & Mathematics with Applications	10.1016/j.camwa.2011.06.023	direct simulation monte carlo;computer simulation;econometrics;markov chain;mathematical optimization;dynamic monte carlo method;stochastic differential equation;hybrid monte carlo;markov chain monte carlo;continuous-time stochastic process;markov property;continuous-time markov chain;stochastic modelling;tau-leaping;monte carlo molecular modeling;mathematics;stochastic tunneling;kinetic monte carlo;parallel tempering;stochastic;statistics;monte carlo method	ML	33.56758708571282	-15.728874874484221	27675
29fc455c93052dd2bca46a495a1762a08421f301	nonparametric bootstrap of sample means of positive-definite matrices with an application to diffusion-tensor-imaging data analysis	frechet mean;fast algorithms;center of mass;intrinsic mean;non parametric bootstrap;extrinsic mean;diffusion tensor imaging	This paper presents nonparametric two-sample bootstrap tests for means of random symmetric positive-definite (SPD) matrices, according to two different metrics: the Frobenius (or Euclidean) metric, inherited from the embedding of the set of SPD metrics in the Euclidean set of symmetric matrices, and the canonical metric, which is defined without an embedding and suggests an intrinsic analysis. Fast algorithms are used to compute the bootstrap instrinsic means in the case of the latter. The methods are illustrated in a two-group comparison of means of diffusion tensors (DTs) obtained from a single voxel of registered DT images of children in a dyslexia study. Short title: Nonparametric Data Analysis for SPD Matrices	algorithm;booting;bootstrapping (statistics);euclidean distance;fast fourier transform;voxel	Leif Ellingson;David Groisser;Daniel Osborne;Vic Patrangenaru;Armin Schwartzman	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2015.1136413	center of mass;diffusion mri;mathematical optimization;combinatorics;mathematics;statistics	ML	31.59432515436704	-23.10644332031821	27811
31d93f194a1427a1a83b9f4084a70a1b0b6764c4	a fast matching criterion for vlsi implementation of block-based motion estimation	rate distortion;estimation mouvement;champ vectoriel;image processing;real time;estimacion movimiento;circuit vlsi;procesamiento imagen;motion estimation;traitement image;motion compensated;mean absolute difference;campo vectorial;vlsi circuit;mean square error;temps reel;displacement vector field;correspondencia bloque;tiempo real;block matching;real time implementation;circuito vlsi;correspondance bloc;displaced frame difference;vector field;electrical communication engineering;block matching algorithm	The main bottleneck in the real-time implementation of block matching algorithms is their huge computational load. One of the ways of reducing the complexity is to use a simpler matching criterion than the mean-squared error criteria. In this paper, we propose a matching criterion which is computationally more efficient than the existing criterion. The proposed criterion is similar to the pel difference classification criterion but applied to a reduced number of bits as the reduced bit mean absolute difference criterion. From the simulation results on the displaced frame difference and the displacement vector field, it is expected that motion compensation with the proposed matching criterion will have similar rate-distortion performance as with other existing criteria.	motion estimation;very-large-scale integration	Gagan B. Rath;Anamitra Makur	1999	Signal Processing	10.1016/S0165-1684(98)00241-2	computer vision;mathematical optimization;electronic engineering;vector field;image processing;computer science;motion estimation;mathematics;mean squared error;geometry;block-matching algorithm;mean difference;statistics	Vision	46.856692367686584	-14.961841319092708	27817
0354a29441c2916ca4dbe491c36c992faa2d0b08	a novel multiple description approach to predictive video coding	least square error estimator lse;multiple description;multiple description coding mdc;path diversity;least square error;packet loss;video coding;multiple description coding;error resilience;source code;h 263;pb frame	Multiple description coding (MDC) is a source coding technique that exploits path diversity to combat packet losses over errorprone channels. In this paper, we proposed a novel drift-free multistate MDC method. At the encoder side, the original video is compressed into multiple independently decodable H.263 streams, each with its own coding structure and prediction process, such that if one stream is lost, the other stream can still be used to produce video with acceptable quality. At the decoder side, each description is considered as a noisy observation of the original video. A Least square-error (LSE) based merge algorithm is proposed to combine the descriptions. The experimental results show that the proposed algorithm has similar coding efficiency to [1], yet with improved error resilience.		Zhiqin Liang;Jiantao Zhou;Liwei Guo;Mengyao Ma;Oscar C. Au	2007		10.1007/978-3-540-77255-2_31	sub-band coding;h.263;shannon–fano coding;telecommunications;variable-length code;computer science;theoretical computer science;multiple description coding;coding tree unit;context-adaptive binary arithmetic coding;packet loss;computer network;source code	Vision	48.28366039853458	-16.0012336047288	27825
9a27c6193af0031d1e7716a9f7162146ccdf617c	improved adaptive global replacement scheme for moea/d-agr	focusing;convergence;evolutionary computation;statistics;optimization;search problems;sociology	Multi-Objective Evolutionary Algorithm based on decomposition (MOEA/D) has been proposed for a decade in solving complex multi-objective problems (MOP) by decomposing it into a set of single-objective problems. MOEA/D-AGR is one of the improved algorithm introduced recently to substitute the original replacement scheme in MOEA/D with a new adaptive global replacement (GR) scheme so that the neighborhood replacement size Tr is increased among the generation to achieve the shifting of focus from solution diversity to convergence. However, the new replacement scheme only considers the one-way convergence of the objective solutions among all sub-problems. It is hard to re-achieve the solution diversity once the algorithm reaches another steeper landscape of solution from a flatten one while it is focusing on convergence. This paper proposes a new adaptive GR scheme to prolong the period of Tr increment so that it can fit the re-increment of fitness landscape. To compensate the shorten period for solution convergence, local searching is adapted for those individuals which has stopped improving its solution value by Simulated Annealing (SA) algorithm. In order to suppress the degree of local searching at the early stage of focusing solution diversity, Fuzzy Logic is used here to coordinate the frequency of local searching according to the average change of objective solution values. To demonstrate the performance of the proposed algorithm, several common benchmark MOPs are used in this paper for comparing with the several state-of-the-art MOEA/D algorithms in terms of IGD. The performance investigation found that the performance of the proposed algorithm was generally better than the other compared algorithms.	advanced graphics riser;benchmark (computing);display resolution;evolutionary algorithm;fuzzy logic;internet gateway device protocol;moea framework;mathematical optimization;metaheuristic;one-way function;simulated annealing	Hiu-Hin Tam;Man-Fai Leung;Zhenkun Wang;Sin Chun Ng;Chi-Chung Cheung;Andrew K. Lui	2016	2016 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2016.7744054	mathematical optimization;convergence;computer science;artificial intelligence;machine learning;mathematics;algorithm;statistics;evolutionary computation	AI	27.389597914087005	-4.282425443066723	27827
6273ec368633728d70918df79fa04e55426ac2d9	difference expansion based reversible data hiding using two embedding directions	transformation ondelette;filigranage numerique;digital watermarking;funcion haar;watermarking;data hiding;difference expansion embedding;dynamical expandable difference selection mechanism;reversible watermarking difference expansion embedding technique reversible image data hiding embedding direction vertical difference image embedding image quality integer haar wavelet transform dynamical expandable difference search mechanism dynamical expandable difference selection mechanism histogram;reversible watermarking;image coding;steganographie;schema differences;fonction haar;image databank;haar function;difference expansion embedding technique;vertical difference image embedding;qualite image;data encapsulation;qa76 electronic computers computer science computer software;wavelet transforms;difference scheme;steganography;histogram;esteganografia;data encapsulation image quality wavelet transforms watermarking image coding degradation data mining protection authentication video coding;dynamical expandable difference search mechanism;histogramme;statistical analysis;embedding direction;reversible image data hiding;esquema diferencias;image quality;filigrana digital;banco imagen;banque image;calidad imagen;search problems;wavelet transforms data encapsulation haar transforms image coding search problems statistical analysis watermarking;transformacion ondita;haar transforms;reversible watermarking data hiding difference expansion embedding integer haar wavelet transform;histograma;haar wavelet transform;integer haar wavelet transform;wavelet transformation;reversible data hiding	Current difference-expansion (DE) embedding techniques perform one layer embedding in a difference image. They do not turn to the next difference image for another layer embedding unless the current difference image has no expandable differences left. The obvious disadvantage of these techniques is that image quality may have been severely degraded even before the later layer embedding begins because the previous layer embedding has used up all expandable differences, including those with large magnitude. Based on integer Haar wavelet transform, we propose a new DE embedding algorithm, which utilizes the horizontal as well as vertical difference images for data hiding. We introduce a dynamical expandable difference search and selection mechanism. This mechanism gives even chances to small differences in two difference images and effectively avoids the situation that the largest differences in the first difference image are used up while there is almost no chance to embed in small differences of the second difference image. We also present an improved histogram-based difference selection and shifting scheme, which refines our algorithm and makes it resilient to different types of images. Compared with current algorithms, the proposed algorithm often has better embedding capacity versus image quality performance. The advantage of our algorithm is more obvious near the embedding rate of 0.5 bpp.	algorithm;distortion;extended memory;finite difference;haar wavelet;image quality;pitch shift;smoothing;wavelet transform	Yongjian Hu;Heung-Kyu Lee;Kaiying Chen;Jianwei Li	2008	IEEE Transactions on Multimedia	10.1109/TMM.2008.2007341	computer vision;discrete mathematics;digital watermarking;theoretical computer science;mathematics;statistics	Vision	43.022262553054425	-12.97568061415543	27890
56fb185490101aba3882b0480d6f54e15975bbd9	improving perceptual quality in video watermarking using motion estimation	motion analysis;watermarking;perceptual quality;visual system motion analysis motion measurement multimedia communication security video coding;watermarking motion estimation video compression history robustness humans signal processing motion measurement masking threshold image processing;spatial distribution perceptual quality video watermarking embedding motion estimation video signals copy mode macroblock;watermarking data encapsulation motion estimation video coding;motion estimation;indexing terms;data encapsulation;video coding;motion vector;multimedia communication;motion measurement;video watermarking;security;visual system	Motion is inherent to video signals, and high perceptual quality cannot be achieved without accounting for motion. Although the information inferred from motion vectors in an encoder does not always represent the true motion of objects, in this paper, we show that it still carries useful information for finding an estimate of motion characteristics. We use the number of copy-mode macroblocks to measure motion intensity, and we use motion history to capture the spatial distribution of motion. By this means, we can identify moving areas in video frames. We do not embed watermark bits in moving areas to avoid watermarking artifacts introduced into P- and B-frames from watermark bits embedded in I-frames. Our simulations show that the proposed algorithm reduces undesirable watermarking artifacts in the video when compared to methods that do not exploit motion information.	algorithm;compression artifact;digital video;digital watermarking;embedded system;encoder;frame (video);macroblock;motion estimation;simulation;video compression picture types	Maneli Noorkami;Russell M. Mersereau	2006	2006 International Conference on Image Processing	10.1109/ICIP.2006.312594	computer vision;structure from motion;index term;visual system;digital watermarking;quarter-pixel motion;computer science;information security;motion interpolation;motion estimation;block-matching algorithm;multimedia;motion field;motion compensation;computer graphics (images)	Robotics	42.09335917894023	-12.781820983070885	27941
33d83df4883bcf57cd40022b0b916e6f5aac3aad	profiles for evaluation: the usage of audio wet	filigranage numerique;protection information;digital watermarking;compresion con perdida;audio signal processing;image processing;securite;lossy compression;signal audio;digital watermark;time frequency;audio signal;procesamiento imagen;test bench;acoustic signal processing;banco prueba;traitement image;algorithme;algorithm;real world application;proteccion informacion;methode domaine frequence;traitement signal audio;application profile;frequency domain method;information protection;robustesse;filigrana digital;safety;signal acoustique;algorithms;robustness;acoustic signal;traitement signal acoustique;metodo dominio frecuencia;banc essai;seguridad;wavelets;senal acustica;senal audio;compression degradante;robustez;algoritmo;audio watermarking	The evaluation of digital watermarks is an active and important research area. From the variety there are different types of attacks like geometric attacks, lossy compression, security or protocol attacks [1, 2] available to evaluate the robustness of digital watermarks. Furthermore, different attack strategies like single attacks or profile attacks are known to improve the evaluation process [3]. If for example, the robustness of a watermarking algorithm is evaluated, then the signal of the audio content and the embedded watermark is modified with the goal to remove or weaken the watermark information. In this paper, the focus is set on audio signals. We introduce the evaluation process of an existing benchmark service, the Audio Watermark Evaluation Testbed (Audio WET) [4] by evaluating five different audio watermarking algorithms, which work in time, frequency and wavelet domain. Therefore, we introduce basic, extended and application profiles which improve the evaluation of watermarking algorithms to provide comparability. Whereas the basic profiles measure single properties on a watermarking algorithm, the extended and application profiles reflect real world application scenarios. Furthermore a test scenario and test environment for the evaluation of five audio watermarking algorithms by using basic profiles is described and discussed. The test results of the first evaluation by using basic profiles are introduced and a comparison of the evaluated watermarking algorithms using different parameter sets for embedding function is provided. 1 Motivation Digital watermarking has been proposed for a variety of applications, including content protection, authentication, digital rights management and others. Many watermarking techniques have made claims regarding performance, such as transparency, robustness, capacity, complexity and security but currently it is not easy to objectively evaluate their performance claims. Watermark evaluation methodologies and tools allow an objective comparison of performance claims and facilitate the development of improved watermarking techniques. The performance of watermarking techniques may also be compared with the specific requirements of applications. Mostly, the development of digital watermarking algorithms is connected with their evaluation. For example, the robustness claimed by watermarking algorithms or their implementations should be verified using a fair and objective process. This process can be very complex and therefore the idea is to describe the evaluation process with special attacks or profiles and their specified or selected attack or profile parameters or with a set of signal processing functions, which could be introduced to the content. To evaluate other properties of the watermarking algorithms (for e.g. transparency or capacity) other methodologies are needed to get an objective comparison. To improve the evaluation of digital watermarking algorithms the Watermark Evaluation Testbed (WET) system was developed at Purdue University, West Lafayette Indiana, USA for images [5] and for audio at Otto-von-Guericke University, Magdeburg, Germany [4]. Both systems are featuring a web-based user interface for the evaluation of watermarking algorithms. Furthermore, the user interface and the general ideas are very similar. In this paper, we will discuss and describe the evaluation process of audio watermarking algorithms, the benchmark process and their usage in Audio WET. Furthermore, the used watermarking algorithms are introduced with their parameters and test results. In this paper is the meaning of the word watermarking an acronym of digital audio watermarks. The paper is organized as follows. In section 2, the Audio WET system is introduced. In section 3 are basic, extended and application profiles described, which are useable to evaluate digital audio watermarking algorithms. The evaluated watermarking algorithms are introduced in section 4. Section 5 introduces the test goals, test environment, test parameters and the test results for the evaluation process by using the Audio WET system. The paper closes in section 6 with a conclusion and future work. Security, Steganography, and Watermarking of Multimedia	algorithm;authentication;benchmark (computing);copy protection;deployment environment;digital rights management;digital watermarking;embedded system;least significant bit;lossy compression;magdeburg;requirement;robustness (computer science);scenario testing;signal processing;steganography;testbed;transparency (graphic);usability;user interface;watermark (data file);wavelet;web application	Andreas Lang;Jana Dittmann	2006		10.1117/12.642695	simulation;speech recognition;telecommunications;image processing;digital watermarking;computer science	HCI	42.33413672284379	-8.152345416386973	28008
222b0b163d9607b3fba2a58a40450eda0d69186f	distributed sparse canonical correlation analysis in clustering sensor data	minimization;sparsity distributed processing canonical correlation analysis;standards;cost function;correlation minimization noise distributed databases cost function standards data models;distributed processing;sparsity;canonical correlation analysis;distributed databases;correlation;noise;data models	The problem of determining information-bearing sensors in the presence of multiple field sources and (non-)linear data models is considered. To this end, a novel canonical correlation analysis (CCA) framework combined with norm-one regularization is introduced to identify correlated measurements across the distributed sensors and cluster the sensor data based on their source content. A distributed algorithm is also put forth for informative sensor identification in nonlinear settings using the novel CCA approach. Toward this end, the sparsity-aware CCA framework is reformulated as a separable constrained minimization problem which is solved by utilizing block coordinate descent techniques combined with the alternating direction method of multipliers. Numerical tests demonstrate that the distributed sparse CCA scheme put forth outperforms existing alternatives when it comes to clustering the sensor data based on their source content.	augmented lagrangian method;cluster analysis;coordinate descent;data model;distributed algorithm;information;nonlinear system;sensor;sparse matrix	Jia Chen;Ioannis D. Schizas	2013	2013 Asilomar Conference on Signals, Systems and Computers	10.1109/ACSSC.2013.6810359	computer science;machine learning;pattern recognition;data mining	AI	52.00997889852984	1.8892920202660615	28160
26b19af6e15fed290c0cc91b1cb0daf0e89c32e3	a modified particle swarm optimization with adaptive selection operator and mutation operator	particle swarm;convergence;premature convergence;selection modified pso adaptive mutation operator;selection;modified particle swarm optimization;searching ability;local optimization;global best particle;adaptive selection operator;adaptive mutation operator;particle swarm optimizer;birds;particle swarm optimization;particle swarm optimization genetic mutations birds convergence genetic algorithms space technology computer science software engineering aerospace engineering arithmetic;genetic algorithm;genetic algorithms;optimization;particle swarm optimisation genetic algorithms;particle swarm optimisation;horn antennas;global best particle particle swarm optimization adaptive selection operator local optimization adaptive mutation operator genetic algorithms searching ability searching scope premature convergence;gallium;searching scope;modified pso	In order to overcome the drawback of classical particle swarm optimization (PSO) such as being subject to being poor in performance of precision and falling into local optimization, a modified PSO is proposed by inducing adaptive mutation operator and selection operator of in PSO. The selection operator of Genetic Algorithms (GA) can improve the fitness of the particle swarm to enhance the searching ability of arithmetic in local. The mutation operator of GA can enlarge the searching scope to avoid premature convergence. The particle swarm will fly to the most optimization by adaptively adjusting the selection operator and mutation operator according to the change of the fitness of the global best particle. The experiment results for typical functions show that the modified PSO can improve the performance of precision and avoid the premature convergence.	converge;genetic algorithm;inductive reasoning;mathematical optimization;mutation (genetic algorithm);particle swarm optimization;premature convergence;software release life cycle	Jize Li;Ping Song;Kejie Li	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.892	mathematical optimization;multi-swarm optimization;genetic algorithm;computer science;artificial intelligence;genetic operator;machine learning;mathematics;particle swarm optimization	DB	28.039011885027122	-5.534048356003588	28188
394668c9edd08abcb89a9ffaa00c021b579fc196	adapting arbitrary normal mutation distributions in evolution strategies: the covariance matrix adaptation	genetic mutations covariance matrix evolutionary computation electronic switching systems stochastic processes;evolutionary computation;arbitrary normal mutation distributions;covariance matrix adaptation;matrix algebra;derandomised adaptation;mutation distributions;self adaptation;mutation distribution;rotation invariance;matrix algebra genetic algorithms;stochastic processes;strategy parameters arbitrary normal mutation distributions evolution strategies covariance matrix adaptation mutation distributions mutation distribution rotation invariance complex fitness functions covariance matrix derandomised adaptation evolutionary algorithms evolution strategy self adaptation;evolution strategies;evolutionary algorithms;electronic switching systems;evolution strategy;genetic algorithms;complex fitness functions;genetic mutations;fitness function;covariance matrix;coordinate system;strategy parameters	|A new formulation for coordinate system independent adaptation of arbitrary normal mutation distributions with zero mean is presented. This enables the evolution strategy (ES) to adapt the correct scaling of a given problem and also ensures invariance with respect to any rotation of the tness function (or the coordinate system). Especially rotation invariance, here resulting directly from the coordinate system independent adaptation of the mutation distribution, is an essential feature of the ES with regard to its general applicability to complex tness functions. Compared to previous work on this subject, the introduced formulation facilitates an interpretation of the resulting mutation distribution, making sensible manipulation by the user possible (if desired). Furthermore it enables a more eeec-tive control of the overall mutation variance (expected step length).	cma-es;evolution strategy;image scaling	Nikolaus Hansen;Andreas Ostermeier	1996		10.1109/ICEC.1996.542381	mathematical optimization;combinatorics;machine learning;mathematics	ML	28.339192106672062	-8.365362547469712	28201
b419b8d365a52791ec67c62632fa42158b2da62a	compression of segmented 3d seismic data	discrete cosine transform;seismic;compression;wavelet	We present a preliminary investigation of compression of segmented 3D seismic volumes for the rendering purposes. Promising results are obtained on the base of 3D discrete cosine transforms followed by the SPIHT coding scheme. An accelerated version of the algorithm combines 1D discrete cosine transform in vertical direction with the 2D wavelet transform of horizontal slices. In this case the SPIHT scheme is used for coding the mixed sets of cosine-wavelet coefficients.	algorithm;coefficient;discrete cosine transform;set partitioning in hierarchical trees;wavelet transform	Valery A. Zheludev;Dan D. Kosloff;Eugene Y. Ragoza	2004	IJWMIP	10.1142/S0219691304000536	wavelet;computer vision;mathematical optimization;transform coding;lapped transform;modified discrete cosine transform;discrete cosine transform;mathematics;stationary wavelet transform;discrete wavelet transform;compression;algorithm;statistics;wavelet transform	ML	42.6544842787706	-15.356380594473594	28230
b2d40a93ef7492013e2ca952dd8e090bc5aa1e3c	identification versus cbcd: a comparison of different evaluation techniques	watermarking;watermarking fingerprint identification;evaluation method;watermarking identification approach content based copy detection cbcd evaluation technique fingerprint technique;cbcd;identification approach;content based copy detection;fingerprint technique;evaluation technique;point of view;fingerprint identification	Fingerprint techniques have a significant advantage in respect of watermarking: a fingerprint can be extracted in each moment of the lifetime of a multimedia content. This aspect is fundamental to solve the problem of copy detection mainly because many copies can be available in huge amount of data in circulation and because each copy can be attacked in several ways (compression, re-encoding, text-overlay, etc.). In this paper the problem of copy detection is studied and tested from two different point of views: content based and identification approaches. The results show that the proposed system is quite robust to some copy modifications and most of all show that the overall results depend on the evaluation method used for testing.	digital watermarking;fingerprint;mpeg-7;moving picture experts group;performance evaluation	Marzia Corvaglia;Fabrizio Guerrini;Riccardo Leonardi;Eliana Rossi	2010	11th International Workshop on Image Analysis for Multimedia Interactive Services WIAMIS 10		fingerprint;digital watermarking;computer science;multimedia;internet privacy;world wide web	Web+IR	37.51344362462357	-11.979127138015427	28429
beeb6ffc5d0be7b4b9714b6e88aca9203fb0978f	the p-value line: a way to choose from different test results		It is common practice to perform an Exploratory Data Analysis to decide whether to use a classical or a robust test; i.e., to choose between a classical or a robust result. What is not so clear is how to choose among the results provided by different competing robust tests. In this paper we propose to use the function p-value line, that we shall define later, to com- pare the results obtained by different tests in order to choose one: the result with largest p-value line. This function takes into account the usual trade-off between robustness and power that is present in most, if not all, robust tests, trade-off that is expressed thought a parameter fixed in a subjective way. With our proposal we can fix it in an objective manner. We shall apply this proposal to choose the trimming fraction in the location test based on the trimmed mean.		Alfonso García-Pérez	2012		10.1007/978-3-642-33042-1_25	econometrics;engineering;operations management;statistics	EDA	27.349675408030794	-19.806695881771606	28498
14acb95801ee35fb09bd1c1067c6b6f5e52b46a5	an asymptotically unbiased minimum density power divergence estimator for the pareto-tail index	pareto type distribution tail index bias correction density power divergence robust distributions threshold exponent;62g35;62g05;62g20;62g32;bias correction;pareto type distribution;density power divergence;tail index	We introduce a robust and asymptotically unbiased estimator for the tail index of Pareto-type distributions. The estimator is obtained by fitting the extended Pareto distribution to the relative excesses over a high threshold with the minimum density power divergence criterion. Consistency and asymptotic normality of the estimator is established under a second order condition on the distribution underlying the data, and for intermediate sequences of upper order statistics. The finite sample properties of the proposed estimator and some alternatives from the extreme value literature are evaluated by a small simulation experiment involving both uncontaminated and contaminated samples. AMS Subject Classifications: 62G05, 62G20, 62G32, 62G35.	long tail;maxima and minima;pareto efficiency;simulation;vhdl-ams	Goedele Dierckx;Yuri Goegebeur;Armelle Guillou	2013	J. Multivariate Analysis	10.1016/j.jmva.2013.06.011	efficient estimator;econometrics;mathematical optimization;minimum-variance unbiased estimator;estimator;stein's unbiased risk estimate;trimmed estimator;pareto distribution;efficiency;mathematics;mean squared error;bias of an estimator;consistent estimator;statistics	ML	30.1948658521178	-22.342817451203803	28500
269fe3546a922b37906706d406a0b1f1bfca93ba	nonlinear estimation of missing δlsf parameters by a mixture of dirichlet distributions	line spectral frequency;speech transmission;lpc;dirichlet distribution;optimal nonlinear minimum mean square error estimator;packet loss;acoustics;speech processing;niobium;optimal nonlinear minimum mean square error estimator nonlinear estimation missing δlsf parameters dirichlet distribution mixture packet networks packet loss speech transmission linear predictive coding lpc line spectral frequency parameters speech quantization δlsf representation dirichlet mixture model dmm conditional distribution;speech;speech coding;quantization signal;neutrality property;quantisation signal;linear predictive coding;mixture modeling;vectors;voice communication;speech quantization;voice communication linear predictive coding mean square error methods nonlinear estimation quantisation signal;line spectral frequency parameters;mean square error methods;neutrality property line spectral frequency packet loss dirichlet distribution mixture modeling;δlsf representation;nonlinear estimation;packet networks;missing δlsf parameters;dirichlet distribution mixture;conditional distribution;dirichlet mixture model;vectors speech niobium speech processing quantization signal acoustics speech coding;dmm	In packet networks, a reliable scheme to handle packet loss during speech transmission is of great importance. As a common representation of the linear predictive coding (LPC) model, the line spectral frequency (LSF) parameters are widely used in speech quantization and transmission. In this paper, we propose a novel scheme to estimate the missing values occurring during LPC model transmission. In order to exploit the boundary and ordering properties of the LSF parameters, we utilize the ΔLSF representation and apply the Dirichlet mixture model (DMM) to capture the correlations among the elements in the ΔLSF vector. With the conditional distribution of the missing part given the received part, an optimal nonlinear minimum mean square error estimator for the missing values is proposed. Compared to the previously presented Gaussian mixture model based method, the proposed DMM based nonlinear estimator shows a convincing improvement.	digital molecular matter (dmm);lsf;linear predictive coding;mean squared error;missing data;mixture model;network packet;nonlinear system	Zhanyu Ma;Rainer Martin;Jun Guo;Honggang Zhang	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854943	conditional probability distribution;dirichlet distribution;niobium;concentration parameter;linear predictive coding;speech recognition;generalized dirichlet distribution;computer science;speech;speech coding;pattern recognition;mixture model;speech processing;mathematics;packet loss;statistics	Robotics	49.663811096566356	-9.641062107299273	28531
593257d3fcfe0f694c733228dfc9a0aa3401ae3a	safe open-loop strategies for handling intermittent communications in multi-robot systems	thesis;open loop motion;reachable sets;multi robot systems	In multi-robot systems where a central decision maker is specifying the movement of each individual robot, a communication failure can severely impair the performance of the system. This paper develops a motion strategy that allows robots to safely handle critical communication failures for such multi-robot architectures. For each robot, the proposed algorithm computes a time horizon over which collisions with other robots are guaranteed not to occur. These safe time horizons are included in the commands being transmitted to the individual robots. In the event of a communication failure, the robots execute the last received velocity commands for the corresponding safe time horizons leading to a provably safe open-loop motion strategy. The resulting algorithm is computationally effective and is agnostic to the task that the robots are performing. The efficacy of the strategy is verified in simulation as well as on a team of differential-drive mobile robots.	algorithm;mobile robot;reachability;robotics;simulation;velocity (software development)	Siddharth Mayya;Magnus Egerstedt	2017	2017 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2017.7989683	real-time computing;simulation;artificial intelligence;aisoy1	Robotics	53.332831632198854	-21.982023834361264	28681
7e4d717bee2321c3e8004a13e8e936e899d8d289	hevc double compression detection based on sn-pupm feature		During the process of video forgery detection, double compression is a significant evidence. A novel scheme based on the Sequence of Number of Prediction Unit of its Prediction Mode (SN-PUPM) is proposed to conduct double compression detection on videos under HEVC standard, together with estimation on GOP structures. Number of PU with three kinds of prediction mode (INTRA, INTER and SKIP) is firstly extracted from each frame inside a given video sequence. Then the SN-PUPM is calculated by Absolute Difference Values from adjacent three frames in original extracted features and filtered with Twice Averaging Filter to reduce noises induced by the process. Then, an initiative Abnormal Value Classifier is trained with SVM to label I-P frames and have a final sequence for double compression detection and GOP analysis. Nineteen original YUV sequences are adopted for dataset in experiments. Results have demonstrated better performance in HEVC double compression than previous method adapted to HEVC.	authentication;entity–relationship model;experiment;group of pictures;high efficiency video coding;ibm systems network architecture	Qianyi Xu;Tanfeng Sun;Xinghao Jiang;Yi Dong	2017		10.1007/978-3-319-64185-0_1	support vector machine;artificial intelligence;computer vision;absolute difference;computer science;compression (physics)	Vision	47.339900071342704	-20.712427846135196	28873
a1ca29b709822dcb649cd0ca2de1801708547473	light field hevc-based image coding using locally linear embedding and self-similarity compensated prediction	image coding;decoding;video coding image capture;transform coding;gain 0 89 db light field hevc based image coding locally linear embedding self similarity compensated prediction prediction tools jpeg psnr gains gain 4 73 db;image coding encoding prediction methods transform coding lenses redundancy decoding;prediction methods;redundancy;lenses;hevc light field image coding self similarity image prediction locally linear embedding;encoding	Light field imaging is a promising new technology that allows the user not only to change the focus and perspective after taking a picture, as well as to generate 3D content, among other applications. However, light field images are characterized by large amounts of data and there is a lack of coding tools to efficiently encode this type of content. Therefore, this paper proposes the addition of two new prediction tools to the HEVC framework, to improve its coding efficiency. The first tool is based on the local linear embedding-based prediction and the second one is based on the self-similarity compensated prediction. Experimental results show improvements over JPEG and HEVC in terms of average bitrate savings of 71.44% and 31.87%, and average PSNR gains of 4.73dB and 0.89dB, respectively.	algorithmic efficiency;arc diagram;encode;high efficiency video coding;jpeg;light field;nonlinear dimensionality reduction;peak signal-to-noise ratio;self-similarity	Ricardo J. S. Monteiro;Luis F. R. Lucas;Caroline Conti;Paulo J. L. Nunes;Nuno M. M. Rodrigues;Sérgio M. M. de Faria;Carla L. Pagliari;Eduardo A. B. da Silva;Luís Ducla Soares	2016	2016 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)	10.1109/ICMEW.2016.7574670	computer vision;transform coding;speech recognition;theoretical computer science;coding tree unit;lens;mathematics;redundancy;encoding;statistics;code-excited linear prediction	Visualization	44.18974224176342	-18.346744972838255	28882
5501f091b1c93e8219b496f9d67998a59e3f3435	bees swarm optimization for web association rule mining	web mining association rule mining genetic metaheuristic bso metaheuristic solution quality optimization problem;bso metaheuristic;association rule mining;optimization problem;genetic metaheuristic;bso arm algorithm bees swarm optimization web association rule mining very large database polynomial exact algorithm genetic metaheuristic fitness criterion iarmga algorithm;web mining;solution quality;very large databases data mining genetic algorithms internet particle swarm optimisation	This paper deals with Association Rules Mining algorithms for very large databases and especially for those existing on the web. The numerous polynomial exact algorithms already proposed in literature treated somehow in an efficient way data sets with reasonable size. However they are not capable to cope with a huge amount of data in the web context where the respond time must be very short. This paper, mainly proposes two new Association Rules Mining algorithms based on Genetic metaheuristic and Bees Swarm Optimization respectively. Experimental results show that concerning both the fitness criterion and the CPU time, IARMGA algorithm improved AGA and ARMGA two other versions based on genetic algorithm already proposed in the literature. Moreover, the same experience shows that concerning the fitness criterion, BSO-ARM achieved slightly better than all the genetic approaches. On the other hand, BSO-ARM is more time consuming. In all cases, we observed that the developed approaches yield useful association rules in a short time when comparing them with previous works.	arm architecture;association rule learning;central processing unit;database;fitness function;genetic algorithm;mathematical optimization;metaheuristic;polynomial;swarm	Youcef Djenouri;Habiba Drias;Zineb Habbas;Hadia Mosteghanemi	2012	2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2012.148	optimization problem;web mining;association rule learning;parallel metaheuristic;computer science;artificial intelligence;machine learning;data mining;metaheuristic	AI	25.49246519550076	-2.1032698288290086	28894
b4c17d8878a9fa137d0def41ba27f839fc80c3c7	the hybrid genetic algorithm for blind signal separation	minimisation;metodo separacion;simulation ordinateur;traitement signal;minimization;optimisation;separation method;algoritmo busqueda;separacion ciega;optimizacion;cross correlation;divergence;algorithme recherche;correlation croisee;kullback leibler divergence;search algorithm;minimizacion;source signal;algoritmo genetico;busca local;blind separation;signal processing;separacion senal;algorithme genetique;separation aveugle;blind signal separation;algorithme evolutionniste;genetic algorithm;algoritmo evolucionista;signal sources;optimization;separation source;simulacion computadora;methode separation;evolutionary algorithm;reseau neuronal;source separation;procesamiento senal;local search;computer simulation;red neuronal;recherche locale;divergencia;correlacion cruzada;hybrid genetic algorithm;neural network	In this paper, a hybrid genetic algorithm for blind signal separation that extracts the individual unknown independent source signals out of given linear signal mixture is presented. The proposed method combines a genetic algorithm with local search and is called the hybrid genetic algorithm. The implemented separation method is based on evolutionary minimization of the separated signal cross-correlation. The convergence behaviour of the network is demonstrated by presenting experimental separating signal results. A computer simulation example is given to demonstrate the effectiveness of the proposed method. The hybrid genetic algorithm blind signal separation performance is better than the genetic algorithm at directly minimizing the Kullback-Leibler divergence. Eventually, it is hopeful that this optimization approach can be helpful for blind signal separation engineers as a simple, useful and reasonable alternative.	blind signal separation;computer simulation;cross-correlation;genetic algorithm;kullback–leibler divergence;local search (optimization);mathematical optimization;network convergence;technological convergence	Wen-Jye Shyr	2006		10.1007/11893295_105	computer simulation;minimisation;econometrics;meta-optimization;genetic algorithm;computer science;artificial intelligence;local search;cross-correlation;machine learning;evolutionary algorithm;signal processing;mathematics;blind signal separation;kullback–leibler divergence;divergence;artificial neural network;algorithm;statistics;search algorithm	AI	28.342770144038184	0.8810012244664075	28916
82bbb8d68be4cf9919bb45c09e671bba035d2af4	reachability-based self-triggered scheduling and replanning of uav operations		Modern unmanned aerial vehicles (UAVs) rely on constant periodic sensor measurements to help avoid obstacles, deal with system disturbances and correct uncertainties. However, constant checking is time and energy consuming and is often not necessary especially in situations in which the UAV can safely fly using open loop control without entering unsafe states. Thus, in this paper we focus on two main problems: i) how to predict the possible reachable states of the UAV considering disturbances and system noise and ii) when to schedule the next state monitoring and replanning time to avoid unnecessary executions. To this end, we propose a self-triggered framework in which we consider realistic UAV dynamics and reachability analysis to predict future events while scheduling replanning times to guarantee safe high performance behavior under various conditions. Finally we validate the proposed approach with simulations and experiments in which a quadrotor UAV is tasked to navigate to a goal while avoiding obstacles in the environment.	dynamic programming;experiment;liveness;motion planning;reachability;scheduling (computing);simulation;television antenna;unmanned aerial vehicle	Esen Yel;Tony X. Lin;Nicola Bezzo	2017	2017 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)	10.1109/AHS.2017.8046382	computer science;real-time computing;reachability;scheduling (computing);open-loop controller	Robotics	53.19720277392868	-21.798507492694124	28932
99bcbeffe8e2cfc35c5e9b244ea8021db4c6605d	3dtv system supporting delivery of full hd resolution 3dtv contents over hfc network	3dtv system supporting delivery;3dtv over hfc;telecommunication cables;interpolation;3dtv system;hd resolution 3dtv contents;image resolution;three dimensional television hybrid fibre coax networks image resolution interpolation stereo image processing telecommunication cables television broadcasting;frame compatible format 3dtv system full hd resolution 3dtv stereoscopic 3dtv 3dtv over hfc;image quality perspective;three dimensional television;stereoscopic images;digital cable broadcasting system;decimation;television broadcasting;hfc network;high definition video hybrid fiber coaxial cables image resolution streaming media pixel multiplexing image quality;multiplexing;full hd resolution 3dtv;hybrid fiber coaxial cables;frame compatible format;streaming media;image quality;pixel;stereo image processing;high definition video;hybrid fibre coax networks;stereoscopic 3dtv;3dtv broadcasting service;interpolation processes 3dtv system supporting delivery hd resolution 3dtv contents hfc network frame compatible format 3dtv broadcasting service digital cable broadcasting system image quality perspective decimation stereoscopic images;interpolation processes	In this paper, we propose a 3DTV system supporting delivery of full HD resolution 3DTV contents over HFC network.	3d television;hybrid fibre-coaxial	Woongshik You;GwangSoon Lee;Dong-Joon Choi;Oh-Seok Kwon	2010	2010 International Conference on Information and Communication Technology Convergence (ICTC)	10.1109/ICTC.2010.5674656	embedded system;telecommunications;computer science;computer graphics (images)	Robotics	43.06670288138122	-20.977170343076462	28970
8290260ec2657ac92644b8dad7bc920cd94c6b52	classification using residual vector quantization with markov-bayesian structure	computers;data compression;bayes methods;signal classification bayes methods encoding markov processes quantisation signal;training;memory cost improvement signal classification residual vector quantization markov bayesian structure code vector assignments multistage residual vector quantizer bayesian framework most probable class membership markov structure;accuracy;markov processes ieee members accuracy computers bayes methods training data compression;ieee members;markov processes	In this work, for a given a set of code vector assignments to an input by a multistage residual vector quantizer RVQ [1], Bayesian framework is formulated to find the most probable class membership of the input. Furthermore, Markov structure is also used to improve the memory cost of the classification.	markov chain;multistage amplifier;quantization (signal processing);vector quantization	Syed Irteza Ali Khan;David V. Anderson;Christopher F. Barnes	2015	2015 Data Compression Conference	10.1109/DCC.2015.63	data compression;machine learning;pattern recognition;mathematics;accuracy and precision;markov process;markov model;vector quantization;statistics;variable-order markov model	ML	49.30534955231765	-11.412646757392542	28971
db78bad56200be48cc14fc028759677f3cc91244	expanding from discrete to continuous estimation of distribution algorithms: the idea	densite probabilite;algorithm performance;algorithm complexity;probability density;complejidad algoritmo;selective sampling;wiskunde en informatica;densidad probabilidad;density estimation;estimation of distribution algorithm;complexite algorithme;resultado algoritmo;performance algorithme;algorithme evolutionniste;algoritmo evolucionista;evolutionary algorithm;optimisation fonction;evolutionary computing	The direct application of statistics to stochastic optimization based on iterated density estimation has become more important and present in evolutionary computation over the last few years. The estimation of densities over selected samples and the sampling from the resulting distributions, is a combination of the recombination and mutation steps used in evolutionary algorithms. We introduce the framework named IDEA to formalize this notion. By combining continuous probability theory with techniques from existing algorithms, this framework allows us to define new continuous evolutionary optimization algorithms.	continuous optimization;estimation of distribution algorithm;evolutionary algorithm;evolutionary computation;experiment;iterated function;iteration;kernel density estimation;mathematical optimization;sampling (signal processing);search algorithm;stochastic optimization	Peter A. N. Bosman;Dirk Thierens	2000		10.1007/3-540-45356-3_75	mathematical optimization;probability density function;density estimation;estimation of distribution algorithm;computer science;artificial intelligence;machine learning;evolutionary algorithm;mathematics;algorithm;statistics;evolutionary computation	ML	28.286186630981547	0.9223105889441415	29061
330ab22ff9c4d180223d9290d1a67a3e110d65c0	earthworm optimisation algorithm: a bio-inspired metaheuristic algorithm for global optimisation problems		Earthworms can aerate the soil with their burrowing action and enrich the soil with their waste nutrients. Inspired by the earthworm contribution in nature, a new kind of bio-inspired metaheuristic algorithm, called earthworm optimisation algorithm (EWA), is proposed in this paper. The EWA method is inspired by the two kinds of reproduction (Reproduction 1 and Reproduction 2) of the earthworms. Reproduction 1 generates only one offspring by itself. Reproduction 2 is to generate one or more than one offspring at one time, and this can successfully be done by nine improved crossover operators. In addition, Cauchy mutation (CM) is added to EWA method. Nine different EWA methods with one, two and three offsprings based on nine improved crossover operators are respectively proposed. The results show that EWA23 performs the best and it can find the better fitness on most benchmarks than others.	algorithm;british informatics olympiad;global optimization;mathematical optimization;metaheuristic	Gaige Wang;Suash Deb;Leandro dos Santos Coelho	2018	IJBIC	10.1504/IJBIC.2015.10004283	mathematical optimization;multi-swarm optimization;meta-optimization;parallel metaheuristic;tabu search;derivative-free optimization;firefly algorithm;metaheuristic	Vision	26.248842773979238	-4.164228462471926	29091
0974628a28551a021aeda1ce1e6c3c8aac986c7e	a robust and improved visual quality data hiding method for hevc		"""This paper presents a novel and robust data hiding method based on <italic>H.265</italic>/High Efficiency Video Coding (<italic>HEVC</italic>) standard. To improve the robustness of data hiding, the embedded data are first encoded into the encoded data by using the <italic>BCH</italic> syndrome code (<italic>BCH code</italic>) technique. To improve the visual quality of data hiding, three groups of the prediction directions are provided to limit the intra-frame distortion drift. Then, the encoded data are embedded into the multi-coefficients of the selected <inline-formula> <tex-math notation=""""LaTeX"""">$4 \times 4$ </tex-math></inline-formula> luminance discrete sine transform blocks, which meet the groups. It is experimentally proven that the proposed method can achieve greater robustness, better visual quality, and higher embedding capacity than previously studied methods."""	bch code;coefficient;discrete sine transform;distortion;embedded system;experiment;high efficiency video coding;intra-frame coding	Yunxia Liu;Hongguo Zhao;Shuyang Liu;Cong Feng;Si Liu	2018	IEEE Access	10.1109/ACCESS.2018.2869148	information hiding;robustness (computer science);visualization;theoretical computer science;discrete sine transform;embedding;distributed computing;computer science;distortion;robust statistics;bch code	Visualization	43.849814851880524	-16.844523404696833	29166
5f38801ddda017939ac33278d31b64d4babba710	distributed compressed video sensing	video compression decoding video coding compressed sensing design engineering encoding analog digital conversion optical imaging technological innovation predictive models;compressed sensing theory;silicon;compressed sensing;data compression;decoding;sensors;real time;intraframe decoding distributed compressed video sensing discos distributed video coding dvc compressed sensing theory encoder interframe sparsity model;intraframe decoding;low complexity;distributed compressed video sensing;structurally random matrices distributed video coding wyner ziv coding compressed sensing compressive sensing sparse recovery with decoder side information;analog to digital conversion;discos;sparse recovery with decoder side information;distributed sensors;video coding data compression decoding distributed sensors;video coding;terahertz;interframe sparsity model;ieee;random matrices;compressive sensing;structurally random matrices;dvc;sparse recovery;encoder;distributed video coding;wyner ziv coding;side information;encoding;gallium nitride;compressed video;compressive sampling	This paper proposes a novel framework called Distributed Compressed Video Sensing (DISCOS) - a solution for Distributed Video Coding (DVC) based on the recently emerging Compressed Sensing theory. The DISCOS framework compressively samples each video frame independently at the encoder. However, it recovers video frames jointly at the decoder by exploiting an interframe sparsity model and by performing sparse recovery with side information. In particular, along with global frame-based measurements, the DISCOS encoder also acquires local block-based measurements for block prediction at the decoder. Our interframe sparsity model mimics state-of-the-art video codecs: the sparsest representation of a block is a linear combination of a few temporal neighboring blocks that are in previously reconstructed frames or in nearby key frames. This model enables a block to be optimally predicted from its local measurements by l1-minimization. The DISCOS decoder also employs a sparse recovery with side information to jointly reconstruct a frame from its global measurements and its local block-based prediction. Simulation results show that the proposed framework outperforms the baseline compressed sensing-based scheme of intraframe-coding and intraframe-decoding by 8 – 10dB. Finally, unlike conventional DVC schemes, our DISCOS framework can perform most encoding operations in the analog domain with very low-complexity, making it be a promising candidate for real-time, practical applications where the analog to digital conversion is expensive, e.g., in Terahertz imaging.	analog-to-digital converter;baseline (configuration management);codec;compressed sensing;data compression;encoder;intra-frame coding;key frame;real-time clock;simulation;sparse matrix	Thong T. Do;Yi Chen;Dzung T. Nguyen;Nam P. Nguyen;Lu Gan;Trac D. Tran	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/CISS.2009.5054678	telecommunications;computer science;theoretical computer science;mathematics;compressed sensing;statistics	Vision	49.36646976234715	-17.27737834863986	29203
65372e33f742ce155d662645bfd4788e3ee2fe74	significant region based robust watermarking scheme in lifting wavelet transform domain	blind watermarking;block selection procedure;lifting wavelet transform;significant region	It discusses the experimental flaw in Lin et al. (2009) and Run et al. (2011).Different secret keys and region based strategy helps the system to be more secure.Remarkably efficient especially in case of JPEG compression attack.Also provides more robustness against various signal processing operations.Shows noteworthy comparisons with currently existing techniques. With the aim of designing a more robust digital watermarking scheme against various unintentional and intentional attacks, a significant region (SR) based image watermarking technique is proposed in the present paper using lifting wavelet transform (LWT). While the energy compaction property of LWT provides higher tolerance against image distortion as opposed to conventional wavelet transform, the proposed block selection procedure provides greater security over the existing watermarking approaches. Non-overlapping coefficient blocks from the lowpass subband are selected after applying three levels of LWT and using certain criterion based on minimum coefficient difference and a threshold value. To disguise the intruder completely, secret key based randomization of coefficients, blocks, and watermark bits is incorporated. Maximum coefficients difference of each selected block and the same threshold value are then used for deciding which block to choose for embedding the bit 0 or 1. Performance of the proposed method is analyzed and compared with some of the existing schemes that demonstrates that the proposed scheme not only outperforms other methods with respect to various attacks for most of the cases, but also maintains a satisfactory image quality.	lifting scheme;wavelet transform	Vivek Singh Verma;Rajib Kumar Jha;Aparajita Ojha	2015	Expert Syst. Appl.	10.1016/j.eswa.2015.06.041	arithmetic;second-generation wavelet transform;theoretical computer science;mathematics;lifting scheme;computer security	DB	40.71261876760148	-10.333230693772885	29229
9f7f5b372e4ca7f5bcb79c01674df6b0b6effab5	logistic regression		1 Many statistical tests require the dependent (response) variable to be continuous so a different set of tests are needed when the dependent variable is categorical. One of the most commonly used tests for categorical variables is the Chi-squared test which looks at whether or not there is a relationship between two categorical variables but this doesn’t make an allowance for the potential influence of other explanatory variables on that relationship. For continuous outcome variables, Multiple regression can be used for	chi;logistic regression;poo-chi	Chris Piech	2010		10.1007/978-0-387-30164-8_493		ML	26.477067192085073	-22.266815538095305	29274
13980e46630bea0d42400701d3a5f9eba75cee77	entropy-driven parameter control for evolutionary algorithms	evolutionary algorithm	Every evolutionary algorithm needs to address two important facets: exploration and exploitation of a search space. Evolutionary search must combine exploration of the new regions of the space with exploitation of the potential solutions already identified. The necessity of balancing exploration with exploitation needs to be intelligent. This paper introduces an entropy-driven parameter control approach for exploring and exploiting evolutionary algorithms. Entropy represents the amount of disorder of the population, where an increase in entropy represents an increase in diversity. Four kinds of entropy to express diversity and to control the entropy-driven approach are discussed. The experimental results of a unimodal, a multimodal with many local minima, and a multimodal with only a few local minima functions show that the entropy-driven approach achieves good and explicit balance between exploration and exploitation.	entropy (information theory);evolutionary algorithm;evolutionary computation;exploit (computer security);maxima and minima;multimodal interaction;population	Shih-Hsi Liu;Marjan Mernik;Barrett R. Bryant	2007	Informatica (Slovenia)		evolutionary programming;genetic programming;genetic algorithm;interactive evolutionary computation;human-based evolutionary computation;cultural algorithm;computer science;artificial intelligence;machine learning;evolutionary algorithm;evolutionary acquisition of neural topologies;genetic representation;evolution strategy;evolutionary robotics;memetic algorithm;evolutionary computation	ML	26.0337896012656	-7.165591000171383	29299
e79f677c0e135ead71b3e163ac0a064395aaa6ce	image compression using learned dictionaries by rls-dla and compared with k-svd	recursive least square;least squares approximations;dictionaries image coding training wavelet domain discrete cosine transforms psnr transform coding;learning algorithm;image coding;psnr;data compression;rls dla;training;singular value decomposition;entropy coding;overcomplete dictionary;transform coding;sparse approximation;discrete cosine transform;wavelet transforms;image compression;image compression dictionary learning rls dla sparse approximation overcomplete dictionary;discrete cosine transforms;dictionaries;straightforward compression scheme image compression rls dla k svd dictionary learning algorithm recursive least squares pixel domain 9 7 wavelet domain;dictionary learning;wavelet transforms data compression dictionaries image coding learning artificial intelligence least squares approximations singular value decomposition;learning artificial intelligence;wavelet domain	The recently presented recursive least squares dictionary learning algorithm (RLS-DLA) is tested in a general image compression application. Dictionaries are learned in the pixel domain and in the 9/7 wavelet domain, and then tested in a straightforward compression scheme. Results are compared with state-of-the-art compression methods. The proposed compression scheme using RLS-DLA learned dictionaries in the 9/7 wavelet domain performs better than using dictionaries learned by other methods. The compression rate is just below the JPEG-2000 rate which is promising considering the simple entropy coding used.	algorithm;dictionary;drive letter assignment;entropy encoding;image compression;jpeg 2000;k-svd;machine learning;pixel;recursion;recursive least squares filter;singular value decomposition;wavelet	Karl Skretting;Kjersti Engan	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5946782	data compression;transform coding;speech recognition;peak signal-to-noise ratio;image compression;computer science;entropy encoding;machine learning;discrete cosine transform;pattern recognition;sparse approximation;mathematics;lossless compression;singular value decomposition;texture compression;statistics;wavelet transform	Vision	45.007452721986375	-15.720474995441753	29352
2c133899468b00eb392c6a01d2c2eedd42188cc9	a model predictive controller for frame-level rate control in multiview video coding	predictive control;video coding optimal control predictive control;bit rate predictive models encoding correlation video coding streaming media predictive control;bit rate;model predictive control;optimal control;rate control;multiview video coding;video coding;model predictive control rate control multiview video coding bit allocation;streaming media;predictive models;bit allocation;correlation;encoding;mean bit estimation error model predictive controller frame level rate control multiview video coding bitrate fluctuation video quality interview intergop phase based bitrate prediction group of pictures temporal intragop target bitrate linear weighting optimal control frame level qp value selection	In this work, we present a novel frame-level Rate Control algorithm for Multiview Video Coding encoder that adopts the Model Predictive Control technique in order to provide low bitrate fluctuation and high video quality. Our Model Predictive Rate Control (MPRC) predicts the bitrate for a frame by employing (i) inter-view inter-GOP (Group of Pictures) phase-based bitrate prediction, and (ii) temporal (intra-GOP) target bitrate linear weighting. Moreover, the MPRC also defines an optimal control action through frame-level QP value selection. Experimental results demonstrate that our MPRC bitrate prediction incurs a Mean Bit Estimation Error (MBEE) of 1.13% compared to 2.46% provided by single view-based Rate Control and 1.61% provided by the state-of-the-art MVC Rate Control. Our solution also provides on average 0.876dB BD-PSNR increase and 28.92% BD-Bitrate reduction while providing smoother quality and bitrate variations when compared to state-of-the-art.	algorithm;blu-ray;encoder;group of pictures;model–view–controller;multiview video coding;optimal control;peak signal-to-noise ratio;quantum fluctuation	Bruno Boessio Vizzotto;Bruno Zatt;Muhammad Shafique;Sergio Bampi;Jörg Henkel	2012	2012 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2012.69	average bitrate;computer vision;real-time computing;simulation;computer science;variable bitrate;constant bitrate;model predictive control	Robotics	47.175655864978296	-18.205534431370907	29482
7caa89079da9fbf50e33c67607d09cecaa60bc59	identification of linear systems: a practical guideline to accurate modeling: j. schoukens and r. pintelon	linear system;practice guideline	ADVANCED ENGINEERING applications need useful mathematical models. System identification (SI) deals with the problem of obtaining models of dynamical systems from measured data. Software packages have made SI an everyday tool for many engineers and virtually a myriad of successful applications have been reported in the literature. As indicated by Ljung (1991), although system identification has developed into a mature, practical tool, it is not dead as a research area. The reason is that progress and new application areas require increasingly advanced modeling concepts. System identification is often divided into non-parameteric and parameteric methods. As the reviewer was brought up in the Swedish school of SI, starting with the classical work of AstrGm and Bohlin (1965), he may tend to show a bias in favor of time domain parameteric SI methods; this should be taken into account when reading the review. The standard references Ljung and S/SderstrGm (1983), Ljung (1987) and SGderstr~im and Stoica (1989) thus act as the bible when reviewing new books in the area of SI. This is not completely fair when judging work that is motivated by application areas slightly outside the mainstream. It should be noted that the authors of the book are active in the area of instrumentation and measurements. The aim is to obtain dynamical models from noisy measurements, leading to physical interpretation. Con~quently, a frequency domain parametric approach is a natural choice. In order to relate the book to the current status of SI, ~ m e relevant research issues will first be discussed. The recent progress in robust control design has triggered frantic research activity around modeling of uncertainty in control systems; see e.g. the I E E E Trart~. Aut. Control Special Issue on System Identification for Robust Control Design, July 1992. A result of this effort is a renewed interest in frequency domain SI methods. Most of the frequency domain methods give non-parametric estimates. In Ljung and Glover (1981) frequency domain versus time domain methods in system identification are discussed. The idea of fitting parametric models to estimated frequency responses is classical. Recently, there has been extensive interest in such methods. For more details and relevant references see e.g. Parker and Bitmead (1987), LaMarie et al. (1991), Sidman et al. (1991) and Hemicki et al. (1991). The Empirical Transfer Function Estimate studied in Ljung (1985) gives insight into the connection between time domain and frequency domain SI methods. The same idea was used by Peter Whittle at the beginning of the 50s to derive the so-called Whittle estimator for time series analysis, see e.g. Chapter 6.2 in Hannan and Deistler (1988). The Whittle estimator can be derived by the maximum likelihood principle using statistical properties of Fourier coefficients. This is also the starting point for the main method of the book under review. The input and output signals are transformed into the frequency domain. A parameteric model of the system is then fitted to the corresponding spectral lines using a cost-function motivated by the maximum likelihood principle. The first two chapters of the book introduce system identification in general and the maximum likelihood	book;coefficient;control system;dynamical system;emoticon;frequency response;input/output;linear system;loss function;mathematical model;peter stoica;robust control;system identification;time series;transfer function	Bo Wahlberg	1993	Automatica	10.1016/0005-1098(93)90036-S	control theory;mathematics;linear system	ML	52.46363590733056	-0.7055178688332068	29501
611b7c356783fe8b31d60fae6238115adf7270ed	special issue on bayesian econometrics			bayesian network	Luc Bauwens;Gary Koop;John M. Maheu;Yasuhiro Omori	2016	Computational Statistics & Data Analysis	10.1016/j.csda.2016.02.007	bayesian econometrics	ML	26.984395166849644	-23.256763560571464	29568
ff091b5d0860565f428ced3976fc07f28dc8b90a	lcu-level rate control for hierarchical prediction structure of hevc	bit rate video coding encoding standards telecommunications educational institutions quantization signal;data compression;video coding data compression;video coding;temporal layer high efficiency video coding hevc hierarchical prediction structure lcu level rate control scheme video coding standard bit allocation schemes quantization parameter decision schemes	This paper presents a rate control scheme for the emerging HEVC which is currently being developed as a new video coding standard. The proposed scheme employs the largest coding unit of HEVC as a basic unit for the rate control. Moreover, bit allocation and quantization parameter decision schemes are introduced for the hierarchical prediction structure by taking into consideration the relative importance of each temporal layer and frame type.	coding tree unit;data compression;high efficiency video coding;lookahead carry unit;video coding format	Dong-Il Park;Haechul Choi;Jin-Soo Kim;Jin Soo Choi;Jae-Gon Kim	2013	2013 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2013.6486819	data compression;electronic engineering;real-time computing;harmonic vector excitation coding;computer science;theoretical computer science;coding tree unit;context-adaptive binary arithmetic coding;h.261;statistics;multiview video coding	Robotics	46.50952718608999	-18.23459792995797	29667
5b94e37c47ef1ac2db05d3ff38438652c80feeb7	optimizing genetic algorithm for protein crystallization screening using an exploratory fitness function	protein crystallization screening;genetic algorithm;novelty search;fitness function	Protein crystallization is the process of formation of protein crystals. Many combinations of chemicals need to be tried to obtain a crystal for some difficult proteins. This paper discusses a novel way of identifying the various conditions necessary for a successful crystal growth by using a variation of genetic algorithm which explores unexplored territories of the chemical search space, thereby increasing the probability of finding a new crystalline condition. Our analysis shows that our method has 6 common crystalline conditions with associative experimental design(AED) and 9 common crystalline conditions with GenScreen for the protein AbIPPase.	chemical space;fitness function;genetic algorithm;hypothetical protein;k-nearest neighbors algorithm;optimizing compiler;parsing expression grammar;salt (cryptography)	Bidhan Bhattarai;Midusha Shrestha;Ramazan Savas Aygün;Marc L. Pusey	2017	2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2017.8217981	crystallization;machine learning;crystal growth;artificial intelligence;bioinformatics;computer science;genetic algorithm;associative property;protein engineering;protein crystallization;fitness function	Robotics	29.785651831695557	-8.673230928656642	29714
0fc887e354c330e95f7ffda95c0d5f1efe13e181	brain storm optimization with agglomerative hierarchical clustering analysis		Brain storm optimization (BSO) is a relatively new swarm intelligence algorithm, which simulates the problem-solving process of human brainstorming. In General, BSO employs flat clustering which has a number of drawbacks. In this paper, the agglomerative hierarchical clustering is introduced into BSO and its impact on the performance of the creating operator is then analyzed. The proposed algorithm is applied to numerical optimization problems in comparison with the BSO with k-means Clustering. Experimental results show that the proposed algorithm achieves satisfactory results and guarantees a high coverage rate.	hierarchical clustering	Junfeng Chen;Jingyu Wang;Shi Cheng;Yuhui Shi	2016		10.1007/978-3-319-41009-8_12	hierarchical clustering;single-linkage clustering;hierarchical clustering of networks	Theory	27.076708607181647	-5.170009681659812	29741
d995081f3f7f0ea802b1420fe439a7fd37d18e73	exergy analysis of a ground-coupled heat pump heating system with different terminals	exergy analysis;exergy efficiency;fan coil unit;ground coupled heat pump;radiant floor;exergy loss	In order to evaluate and improve the performance of a ground-coupled heat pump (GCHP) heating system with radiant floors as terminals, an exergy analysis based on test results is performed in this study. The system is divided into four subsystems, and the exergy loss and exergy efficiency of each subsystem are calculated using the expressions derived based on exergy balance equations. The average values of the measured parameters are used for the exergy analysis. The analysis results show that the two largest exergy losses occur in the heat pump and terminals, with losses of 55.3% and 22.06%, respectively, and the lowest exergy efficiency occurs in the ground heat exchange system. Therefore, GCHP system designers should pay close attention to the selection of heat pumps and terminals, especially in the design of ground heat exchange systems. Compared with the scenario system in which fan coil units (FCUs) are substituted for the radiant floors, the adoption of radiant floors can result in a decrease of 12% in heating load, an increase of 3.24% in exergy efficiency of terminals and an increase of 1.18% in total exergy efficiency of the system. The results may point out the direction and ways of optimizing GCHP systems.	radiant ai	Xiao Chen;Xiaoli Hao	2015	Entropy	10.3390/e17042328	exergy efficiency;thermodynamics;fan coil unit;exergy;physics	HCI	52.798013114829374	-4.176632454687757	29780
48e8dc4344745f0c98fe37d943fdc1f06cce903f	the improved hill encryption algorithm towards the unmanned surface vessel video monitoring system based on internet of things technology		Depending on the actual demand of maritime security, this paper analyzes the specific requirements of video encryption algorithm for maritime monitoring system. Based on the technology of Internet of things, the intelligent monitoring system of unmanned surface vessels (USV) is designed and realized, and the security technology and network technology of the Internet of things are adopted. The USV are utilized to monitor and collect information on the sea, which is critical to maritime security. Once the video data were captured by pirates and criminals during the transmission, the security of the sea will be affected awfully. The shortcomings of traditional algorithms are as follows: the encryption degree is not high, computing cost is expensive, and video data is intercepted and captured easily during the transmission process. In order to overcome the disadvantages, a novel encryption algorithm, i.e., the improved Hill encryption algorithm, is proposed to deal with the security problems of the unmanned video monitoring system in this paper. Specifically, the Hill algorithm of classical cryptography is transplanted into image encryption, using an invertible matrix as the key to realize the encryption of image matrix. The improved Hill encryption algorithm combines with the process of video compression and regulates the parameters of the encryption process according to the content of the video image and overcomes the disadvantages that exist in the traditional encryption algorithm and decreases the computation time of the inverse matrix so that the comprehensive performance of the algorithm is optimal with different image information. Experiments results validate the favorable performance of the proposed improved encryption algorithm.	algorithm;internet of things;unmanned aerial vehicle	Tingting Yang;Yangyang Li;Chengzhe Lai;Jie Dong;Minghua Xia	2018	Wireless Communications and Mobile Computing	10.1155/2018/5183451	computer science;computation;encryption;invertible matrix;maritime security;distributed computing;data compression;cryptography;matrix (mathematics);internet of things	Mobile	39.10818145133072	-12.80932113243033	29846
4103d95cfc6b5b099ca22cfea11d061f245b9726	non-genetic transmission of memes by diffusion	cultural evolution;genetics;memetic algorithm;continuous optimization;genetic algorithm;genetic algorithms;optimization;cultural transmission;cellular automata;local search	"""In recent years, there has been an increase in research activities on Memetic Algorithm (MA). MA works with memes; a meme being defined as """"the basic unit of cultural transmission, or imitation"""" [5]. In this respect, a Memetic Algorithm essentially refers to """"an algorithm that mimics the mechanisms of cultural evolution"""". To date, there has been significant effort in bringing MA closer to the idea of cultural evolution. In this paper we assess MAs from the perspectives of """"Universal Darwinism"""" and """"Memetics"""". Subsequently, we propose a Diffusion Memetic Algorithm where the memetic material is transmitted by means of non-genetic transfer. Numerical studies are presented based on some of the commonly used synthetic problems in continuous optimization."""	continuous optimization;mathematical optimization;meme;memetic algorithm;memetics;numerical method;optimization problem;synthetic intelligence	Quang Huy Nguyen;Yew-Soon Ong;Meng-Hiot Lim	2008		10.1145/1389095.1389285	mathematical optimization;genetic algorithm;cultural algorithm;computer science;artificial intelligence;machine learning;continuous optimization;memetic algorithm	AI	26.267685421430883	-2.7898289536631653	29883
0d02c5d051d9a113d5125d7d4c54292be7c06072	memory efficient set partitioning in hierarchical tree (mesh) for wavelet image compression	hierarchical tree set partitioning;rate distortion;coding efficiency;set partitioning in hierarchical trees;image coding;sorting;reusable list;set partitioning in hierarchical tree spiht;performance comparison;trees mathematics;bit plane coding pass list reinitialization;wavelet transforms;reduced memory;wavelet transform;image compression;mesh;coding efficiency memory efficient set partitioning hierarchical tree set partitioning mesh wavelet image compression spiht reusable list bit plane coding pass list reinitialization single bit plane coding pass;merging;single bit plane coding pass;spiht;arithmetic;error resilience;article;block codes;wavelet coefficients;image coding sorting merging block codes arithmetic rate distortion wavelet coefficients;memory efficient set partitioning;wavelet transforms image coding trees mathematics;wavelet image compression	This paper presents a memory efficient version of set partitioning in hierarchical tree (SPIHT). The proposed coder termed as memory efficient SPIHT (MESH), uses a single re-usable list instead of three continuously growing linked lists as in conventional SPIHT. The list is re-initialized at the beginning of each bit-plane (coding pass) and is exhausted within that bit-plane itself. Another feature of the proposed coder is that it uses a single pass for each bit-plane by merging the sorting and refinement passes of conventional SPIHT together. The reinitialization of the list in each bit-plane makes the proposed coder inherently error resilient. The performance of the proposed coder is measured in terms of coding efficiency and the worst case memory requirements for list entries in each bit-plane. The performance comparison with SPIHT shows that the proposed algorithm results in 50-70% memory saving while retaining a coding efficiency comparable to SPIHT.	algorithm;algorithmic efficiency;best, worst and average case;bit plane;image compression;linked list;refinement (computing);requirement;set partitioning in hierarchical trees;sorting;tree structure;wavelet	Farid Ghani;Abdul Kader;Ekram Khan;R. Badlishah Ahmad	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1415422	parallel computing;computer science;theoretical computer science;pattern recognition;mathematics;set partitioning in hierarchical trees;wavelet transform	Visualization	44.62756385648346	-15.898908564135676	29985
4c2028ac74780a6261c23fad984099b7e06cb945	dna inspired bi-directional lempel-ziv-like compression algorithms	data compression biocomputing;dictionaries bidirectional control dna image coding indexes rna proteins;text files dna replication bi directional lempel ziv like compression algorithms gene expression alternative splicing algorithm universal lossless data compression algorithms audio files image files	The bi-directional reading processes in DNA replication and gene expression together with the similarities between the so-called alternative splicing and Lempel-Ziv (LZ) algorithms has motivated us to incorporate bi-directional readings into LZ algorithms. LZ77, LZ78, and LZW84 are universal lossless data compression algorithms. A modified version of these algorithms that takes into account both forward and reverse readings is presented in this work. It is shown that bi-directional reading can improve the compression ratio at the expense of slight modifications in LZ algorithms provided that there exists some symmetry in the information content. Results are presented for text, image, and audio files.	algorithm;bi-directional text;conjugate variables;data compression;dictionary;encoder;image;lz77 and lz78;lempel–ziv–stac;lempel–ziv–welch;self-information	Attiya Mahmood;Nazia Islam;Dawit Nigatu;Werner Henkel	2014	2014 8th International Symposium on Turbo Codes and Iterative Information Processing (ISTC)	10.1109/ISTC.2014.6955106	data compression;speech recognition;computer science;bioinformatics;theoretical computer science;lossless compression	Theory	39.99257104111441	-16.746883220994008	29997
08476777ad0f2d940656ebce1662963f86289a5b	estimation of a probability with guaranteed normalized mean absolute error	estimation sequentielle;error absoluto;closed form expression;closed form solution;metodo monte carlo;probability;unbiased estimator probability estimation normalized mean absolute error bernoulli trials sequential approach closed form expression;communication systems;bit error rate;stopping rule;bit error rate closed form solution upper bound estimation error error analysis sampling methods random variables mean square error methods telecommunications distribution functions;probability density function;unbiased estimator;simulacion numerica;simulation;methode monte carlo;random variables;measurement uncertainty;data mining;erreur absolue moyenne;normalized mean absolute error;regla parada;estimacion insesgada;upper bound;error analysis;mean absolute error;estimation;erreur absolue;estimacion probabilista;monte carlo methods sequential estimation mean absolute error simulation;monte carlo method;matematicas;simulation numerique;estimacion parametro;mean square error methods;absolute error;estimation probabiliste;estimation error;sequential estimation;distribution functions;unbiased estimation;parameter estimation;estimation parametre;sequential approach;sampling methods;bernoulli trials;borne superieure;estimacion secuencial;monte carlo methods;estimation sans biais;probabilistic assessment;probability estimation;regle arret;telecommunications;cota superior;sequential estimation error analysis probability;numerical simulation	The estimation of a probability p from repeated Bernoulli trials is considered in this letter. A sequential approach is followed, using a simple stopping rule. A closed-form expression and an upper bound are obtained for the mean absolute error of the unbiased estimator of p. The results given permit the estimation of an arbitrary probability with a prescribed level of normalized mean absolute error.	approximation error;bernoulli polynomials	Luis Mendo	2009	IEEE Communications Letters	10.1109/LCOMM.2009.091128	sequential estimation;computer simulation;econometrics;symmetric mean absolute percentage error;closed-form expression;mathematical optimization;mean absolute percentage error;mathematics;mean integrated squared error;mean absolute error;statistics;mean;monte carlo method	Vision	30.76611439598162	-19.96038364873487	30044
541b0910395b4aa23a8f1d76eb7a9e2640ea82bb	extending evolutionary algorithms to discover tri-criterion and non-supported solutions for the minimum spanning tree problem	optimal solution;search space;pareto front;optimization problem;minimum spanning tree;evolutionary algorithms;multi criterion optimization;evolutionary algorithm;minimum spanning trees;knowledge base	The study of multi-criterion minimum spanning trees is important as many optimization problems in networks, such as communication, transport and utilities can be represented by this model. Conventional evolutionary approaches struggle to discover near-optimal solutions due to the combinatorial search space, and the difficulty in discovering the non-supported solutions. Recently, a knowledge-based evolutionary approach, KEA, has been developed that overcomes some of the problems of the earlier algorithms as it is not restricted to the bi-criterion case, finds non-supported solutions and scales well to larger problems; however, the mid-point of its Pareto front is often dominated by alternative algorithms where they are applicable. Novel extensions to KEA, increasing the knowledge of the mid-point, termed KEA-W are examined, eliminating the mid-point deficiencies at the cost of computational time.	combinatorial search;computation;evolutionary algorithm;file spanning;iterative and incremental development;mathematical optimization;minimum spanning tree;optimization problem;pareto efficiency;time complexity;triangular function	Madeleine Davis-Moradkhan;Will N. Browne;Peter Grindrod	2009		10.1145/1569901.1570186	optimization problem;mathematical optimization;combinatorics;combinatorial optimization;computer science;artificial intelligence;minimum spanning tree;multi-objective optimization;machine learning;evolutionary algorithm;mathematics;distributed minimum spanning tree	ML	25.05451445827789	1.1538307458114228	30066
fc36195247d39029a733b0a378696f23188cfb09	high security image steganography using iwt and graph theory	graph theory;steganalysis steganography iwt integer wavelet transform graph theory lsb least significant bit substitution;wavelet transforms graph theory haar transforms image colour analysis random processes steganography;high security image steganography techniques sc sb imperceptibility selection of co effecients selection of bit length wavelet subband subband selection secret data embedding secret data extraction graph theory wavelet coefficients random selection r planes g planes b planes reversible integer haar wavelet transform transform domain steganography color image steganography spatial domain steganography secret information iwt;hafnium robustness;wavelet transforms;steganography;image colour analysis;random processes;haar transforms	Steganography conceals the secret information inside the cover medium. There are two types of steganography techniques available practically. They are spatial domain steganography and Transform domain steganography. The objectives to be considered in the steganography methods are high capacity, imperceptibility and robustness. In this paper, a Color image steganography in transform domain is proposed. Reversible Integer Haar wavelet transform is applied to the R, G and B planes separately and the data is embedded in a random manner. Random selection of wavelet coefficients is based on the graph theory. This proposed system uses three different keys for embedding and extraction of the secret data, where key1(Subband Selection - SB) is used to select the Wavelet subband for embedding, key2(Selection of Co-effecients-SC) is used to select the co-efficients randomly and key3 (Selection of Bit length-SB) is used to select the number of bits to be embedded in the selected co-efficients. This method shows good imperceptibility, High capacity and Robustness.	bit-length;coefficient;color image;embedded system;graph theory;haar wavelet;randomness;robustness (computer science);sandy bridge;steganography;wavelet transform	V. Thanikaiselvan;P. Arulmozhivarman	2013	2013 IEEE International Conference on Signal and Image Processing Applications	10.1109/ICSIPA.2013.6708029	discrete mathematics;graph theory;theoretical computer science;pattern recognition;mathematics;steganography;discrete wavelet transform;statistics;wavelet transform	EDA	40.765522993736305	-11.496333250145351	30102
19bceb19d1f09325294202e0d84b247173b0c849	the adaption of the cuckoo search algorithm applied in the leach-c protocol		Cuckoo search algorithm has got extensive attention and application as a swarm intelligent optimization algorithm. Levy flight can expand the search area and jump out of the local optimal solution so as to improve the convergence speed of algorithm. But Levy flight track is continuous. To adapt the cuckoo search algorithm to the discrete problem, Levy flight step size should be discretization. In this paper, the Levy flight discretization includes the mirror reflection method to limit the search area and area blocking method so as to find the nearest discrete point quickly. Two matrices are introduced to record the point numbers and the point IDs in every block. Finally, the LEACH-C protocol is improved by the discretization cuckoo search algorithm. The simulation result shows the convergence is faster, the cluster head distributions are more uniform, and the network energy consumption is lower.	cuckoo search;search algorithm	Yunsheng Ge;Jie Kong;Kun Tang;Chuanxian Jiang	2016			mathematical optimization;artificial intelligence;machine learning;mathematics	Vision	28.179763467715294	-3.079047228326571	30145
2221d7fd17d692bf9eeb0a90f20c3b34b1a9da53	experiments with new stochastic global optimization search techniques	fuzzy measure;search method;simulated annealing;global optimizasyon;olasilikli arama yontemleri;probabilistic search methods;bulanik onlemler;clustering method;adaptive partitioning algorithms;global optimization;adaptif bolumleme algoritmalari;fuzzy measures;article	"""In this paper several probabilistic search techniques are developed for global optimization under three heuristic classi""""cations: simulated annealing, clustering methods and adaptive partitioning algorithms. The algorithms proposed here combine di!erent methods found in the literature and they are compared with well-established approaches in the corresponding areas. Computational results are obtained on 77 small to moderate size (up to 10 variables) nonlinear test functions with simple bounds and 18 large size test functions (up to 400 variables) collected from literature."""	algorithm;cluster analysis;computation;distribution (mathematics);global optimization;heuristic;mathematical optimization;nonlinear system;simulated annealing	Linet Özdamar;Melek Demirhan	2000	Computers & OR	10.1016/S0305-0548(99)00054-4	mathematical optimization;simulated annealing;computer science;machine learning;data mining;mathematics;adaptive simulated annealing;metaheuristic;global optimization	EDA	26.992361068552096	-6.6640001433760006	30180
048dcddccb6a22f5ebb2d711b234153bd57dc06a	gpdoemd: a python package for design of experiments for model discrimination		GPdoemd is an open-source python package for design of experiments for model discrimination that uses Gaussian process surrogate models to approximate and maximise the divergence between marginal predictive distributions of rival mechanistic models. GPdoemd uses the divergence prediction to suggest a maximally informative next experiment.		Simon Olofsson;Ruth Misener	2018	CoRR		optimal design;mathematical optimization;experimental data;design of experiments;mathematics;approximate inference;mathematical model;gaussian process;python (programming language)	ML	26.80583602469816	-15.596766674115528	30181
c725f83c27f36a5a0beb4c3d7c5c8eb90e7479aa	a simulation study on the probability of correct selection for large k populations	metodo estadistico;selection problem;test statistique;problema seleccion;statistical simulation;probability;aplicacion;methode parametrique;bootstrap;loi probabilite;ley probabilidad;metodo parametrico;test estadistico;parametric method;simulacion numerica;simulation;estimation non parametrique;statistical test;estadistica rango;statistical method;ranking and selection;estimation parametrique;non parametric estimation;62f07;simulacion estadistica;methode statistique;simulation statistique;probability distribution;probabilidad;simulation numerique;probabilite;rank statistic;simulation study;statistique rang;probability of correct selection;pcs;estimacion no parametrica;application;numerical simulation;probleme selection	An increasing number of contemporary datasets are high dimensional. Applications require these datasets be screened (or filtered) to select a subset for further study. Multiple testing is the standard tool in such applications, although alternatives have begun to be explored. In order to assess the quality of selection in these high-dimensional contexts, Cui and Wilson (2008b) proposed two viable methods of calculating the probability that any such selection is correct (PCS). PCS thereby serves as a measure of the quality of competing statistics used for selection. The first simulation study of this article investigates the two PCS statistics of the above article. It shows that in the high-dimensional case PCS can be accurately estimated and is robust under certain conditions. The second simulation study investigates a nonparametric estimator of PCS.	population;simulation	Xinping Cui;Jason Wilson	2009	Communications in Statistics - Simulation and Computation	10.1080/03610910902898457	computer simulation;probability distribution;econometrics;statistical hypothesis testing;probability;mathematics;statistics	AI	30.594942588201892	-21.355099829368687	30198
8028793e6f6e6e194988834898105dee31badf0f	a hybrid quantum-inspired immune algorithm for multiobjective optimization	similar individuals;immune algorithm;genetics;multiobjective;quantum computer;quantum algorithm;immune system;multiobjective optimization;optimization;pareto optimal solution;pareto optimality	This study suggests a novel quantum immune algorithm for finding Pareto-optimal solutions to multiobjective optimization problems based on quantum computing and immune system. In the proposed algorithm, there are distinct characteristics as follows. First, the encoding method is based on Q-bit representation, and thus a chaos-based approach is suggested to initialize the population. Second, a new chaos-based rotation gate and Q-gates are presented to perform mutation and improve the quality of the population, respectively. Finally, especially, a new truncation algorithm with similar individuals (TASI) is utilized to preserve the diversity of the population. Also, a new selection operator is proposed to create the new population based on TASI. Simulation results on six standard problems (ZDT6, CP, SP, VNT, OSY and KIT) show the proposed algorithm is able to find a much better spread of solutions and has better convergence near the true Pareto-optimal front compared to the vector immune algorithm (VIS) and the elitist non-dominated sorting genetic system (NSGA-II).	algorithm;mathematical optimization;multi-objective optimization	Jiaquan Gao	2011	Applied Mathematics and Computation	10.1016/j.amc.2010.11.030	mathematical optimization;immune system;theoretical computer science;multi-objective optimization;mathematics;quantum computer;quantum algorithm;algorithm;quantum mechanics;population-based incremental learning	EDA	27.5593210617937	-5.165741846666911	30295
10dd76b99906a6c2036e54f2d868a82eab25baa6	recent advances in video compression: what’s next?	multi view video coding;complexity theory;decoding;multi view video coding video compression distributed coding texture based methods scalable coding;video compression video coding mpeg 4 standard automatic voltage control image coding decoding motion estimation standards development high definition video video surveillance;video compression;transform coding;satisfiability;distributed coding;texture based methods;video coding;pixel;scalability;encoding;scalable coding	In the last two decades, there have been significant advances in video coding. Since the early 1990s, a series of video coding standards have been developed to satisfy the growing requirements of video applications. Among these, H.264/MPEG-4 AVC is the most recent standard achieving high compression efficiency. The video compression research community has continued working on new advances that go beyond traditional video coding architectures. In this paper, we overview of some recent advances in video coding and their potential applications. We discuss work in distributed coding, texture-based methods, scalable coding, and multi-view video coding.	data compression;h.264/mpeg-4 avc;requirement;scalability;video coding format	Limin Liu;Fengqing Zhu;Marc Bosch;Edward J. Delp	2007	2007 9th International Symposium on Signal Processing and Its Applications	10.1109/ISSPA.2007.4555634	video compression picture types;data compression;scalable video coding;computer vision;scalability;transform coding;h.263;computer science;theoretical computer science;context-adaptive variable-length coding;video tracking;coding tree unit;multimedia;video processing;smacker video;context-adaptive binary arithmetic coding;motion compensation;h.261;pixel;encoding;statistics;multiview video coding;satisfiability	Arch	44.002026881187945	-19.35744692065396	30344
1a46f5e8df5a011eda81f295960803d02249403f	a correlation test for normality based on the lévy characterization	goodness of fit;62e17;jarque bera test;invariant test;normality test;secondary 62g20;primary 62g10;sums of normal variables;shapiro wilk test	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	approximation algorithm;characterization test;entity–relationship model;francis;monte carlo method;primary source;shapiro polynomials;simulation	José A. Villaseñor-Alva;Elizabeth González-Estrada	2015	Communications in Statistics - Simulation and Computation	10.1080/03610918.2013.810261	pearson's chi-squared test;f-test;econometrics;jarque–bera test;goldfeld–quandt test;combinatorics;sign test;student's t-test;test statistic;score test;chi-square test;f-test of equality of variances;shapiro–wilk test;mathematics;normality test;exact test;goodness of fit;wald test;statistics;brown–forsythe test;z-test	Robotics	31.485058719669087	-22.605667329931894	30347
8566584cd8e2ace2d57741cbfbc2bc395c4cb1b3	computational intelligence based algorithm for node localization in wireless sensor networks	biogeography based optimization bbo;convergence;wireless sensor networks wsns biogeography based optimization bbo particle swarm optimization pso node localization;sensors;particle swarm optimization pso;wireless sensor networks wsns;accuracy;wireless sensor networks sensors convergence accuracy optimization biogeography sociology;gbest computational intelligence based algorithm node localization wireless sensor networks wsn h best particle swarm optimization hpso biogeography based optimization bbo algorithms distributed optimal localization randomly deployed sensors collective learning biological organisms np hard optimization problem optimization algorithms genetic algorithms ga simulated annealing algorithm saa global best;biogeography;computational complexity;artificial intelligence;optimization;node localization;particle swarm optimisation;wireless sensor networks artificial intelligence computational complexity particle swarm optimisation;wireless sensor networks;sociology	Accurate location of target nodes is highly desirable in a Wireless Sensor Network (WSN) as it has a strong impact on overall performance of the WSN. This paper proposes the application of H-Best Particle Swarm Optimization (HPSO) and Biogeography Based Optimization (BBO) algorithms for distributed optimal localization of randomly deployed sensors. The proposed HPSO algorithm is modeled for fast and mature convergence, though previous PSO models had only fast convergence but less mature. Biogeography is a school work (collective learning) of geographical allotment of biological organisms. BBO has a new inclusive vigor based on the science of biogeography and employs migration operator to share information between different habitats, i.e., problem solutions. WSN localization problem is formulated as an NP-Hard optimization problem because of its size and complexity. In this work, an error model is described for estimation of optimal node location in a manner such that the location error is minimized using HPSO and BBO algorithms. Proposed HPSO and BBO algorithms are matured to optimize the sensors' locations and perform better as compared to the existing optimization algorithms such as Genetic Algorithms (GAs), and Simulated Annealing Algorithm (SAA). Comparative study reveals that the HPSO yields improved performance in terms of faster, matured, and accurate localization as compared to global best (gbest) PSO.	black bag operation;centralized computing;collective intelligence;computation;computational intelligence;error detection and correction;genetic algorithm;html element;habitat;internationalization and localization;iteration;mathematical optimization;maxima and minima;np-hardness;optimization problem;particle swarm optimization;phase-shift oscillator;randomness;refinement (computing);sensor;simulated annealing;simulation;vigor (software)	Anil Kumar Rose;Arun Khosla;Jasbir Singh Saini;Satvir Singh	2012	2012 6th IEEE International Conference Intelligent Systems	10.1109/IS.2012.6335173	mathematical optimization;engineering;artificial intelligence;machine learning	Robotics	28.995100285027643	-2.995244916565752	30370
552d2e7bca5a7f0e33a5540a0e68ba59bf306745	scalable architecture for use in an over-hdtv real-time codec system for multiresolution video	real time;video;frame synchronization	In this paper, we propose a multi-frame synchronization method which has sufficient scalability, and describe an SHR codec system we have developed that uses MPEG-2 codecs and a multi-HDTV frame synchronizerrnbased on our method.	codec;real-time clock	Takeshi Yoshitome;Ken Nakamura;Yoshiyuki Yashima;Makoto Endo	2003			synchronization;real-time computing;real-time operating system;telecommunications;computer science;computer graphics (images)	Embedded	43.479138493736926	-21.397693447930898	30400
807c519bb57af7329db35dce846849c900b7097a	a new joint spectral radius analysis of random pso algorithm	convergence analysis;the joint spectral radius;particle swarm optimization;monte carlo method	The existing stability analysis of particle swarm optimization (PSO) algorithm is chiefly concluded by the assumption of constant transfer matrix or time-varying random transfer matrix. Firstly, one counterexample is provided to show that the existing convergence analysis is not possibly valid for PSO system involving random variables. Secondly, the joint spectral radius, mainly calculated by the maximum eigenvalue of the product of all asymmetric random transfer matrices, is introduced to analyze and discuss convergence condition and convergence rate from numerical viewpoint with the aid of Monte Carlo method. Numerical results show that there is one major discrepancy between some preview convergence results and our corresponding results, helping us to deeply understand the tradeoff between exploration ability and exploitation ability as well as providing certain guideline for parameter selection.	algorithm;discrepancy function;mathematical optimization;monte carlo method;numerical analysis;numerical method;particle swarm optimization;rate of convergence;transfer matrix	Jun Liu;Hongbin Ma;Xuemei Ren;Tianyun Shi;Ping Li	2014	Int. J. Comput. Intell. Syst.	10.1080/18756891.2014.960291	econometrics;mathematical optimization;convergence of random variables;mathematics;particle swarm optimization;statistics;monte carlo method	ML	40.6711798917854	3.7168708822387693	30488
2541d3235bfce42d70793d0f177674d13a8626cb	speaker recognition using temporal decomposition of lsf for mobile environment	line spectral frequency;speaker recognition;decomposition method;mobile environment;ip networks	A novel approach to speaker recognition in mobile or IP network environment is described. In this approach, we use decoded line spectral frequency (LSF) parameters directly from compressed speech packets instead of using parameters from decompression and analysis procedure. Furthermore, we reduce the number of LSF series based on a restricted temporal decomposition method. Consequently, proposed approach gets more than three times faster than a traditional speaker recognition approach without losing any accuracy according to our experiments.		Sung-Joo Kim;Min-Seok Kim;Ha-Jin Yu	2007		10.1007/978-3-540-72685-2_32	speaker recognition;speech recognition;decomposition method;computer science;pattern recognition	Robotics	47.2854850150892	-9.221181552853594	30495
0132a2fc292311e2b6aa4e9cc8857a439cd48ebf	spatially variant flexible sampling control integrated on an image sensor	image sampling;dynamic change;cmos image sensors image sampling digital signal processing chips;pixel position spatially variant flexible sampling control image sensor sampling control system random access pixels spatially variant pixels cmos sampling position memory array memory;cmos image sensors;control system;image sampling image sensors sampling methods sensor arrays prototypes decoding intelligent sensors sensor phenomena and characterization sensor systems control systems;digital signal processing chips;random access;high speed;image sensor	W e propose a n e w sampling control s y s t e m in te grated o n a n image sensor. Contrary t o the convent ional r a n d o m access pixels, t he proposed sensor is able to read out spatially var iant pixels a t high speed, without input t ing pixel address for each access. T h e s a m pling pos i t ions c a n be changed dynamically by rewriting the sampling pos i t ion m e m o r y . Since the proposed sensor has a array m e m o r y tha t keeps the pixel pos i t ion to be sampled. T h e sampling pos i t ion can be dynamically changed by rewriting the m e m o r y array. It can achieve a n y spatially varying sampling patterns. W e have m a d e a f i r s t prototype and show results obtained by the prototype.	array data structure;image sensor;pixel;point of sale;prototype;rewriting;sampling (signal processing)	Yasuhiro Ohtsuka;Takayuki Hamamoto;Kiyoharu Aizawa;Mitsutoshi Hatori	1998		10.1109/ICIP.1998.723538	color co-site sampling;computer vision;computer science;control system;image sensor;random access	Vision	42.89673541100603	-3.668378588717927	30624
787b378de2f90e43d8c5359dfb47b03fce7764fd	predicting the yield curve using forecast combinations	dynamic factor models;kalman;economic value of forecasts;forecast combinations;yield curve	We examine the statistical accuracy and economic value of modeling and forecasting the term structure of interest rates using forecast combinations. We adopt five alternative methods to combine point forecasts from several univariate and multivariate autoregressive specifications, as well as from factor models for the yield curve such as the dynamic versions of the Nelson-Siegel and Svensson specifications. Moreover, we conduct a detailed performance evaluation based not only on statistical measures of forecast accuracy, but also an economic evaluation using Sharpe ratios of optimal meanvariance fixed income portfolios constructed based upon forecasts from individual models and their alternative combinations. Our empirical application based on a large panel of Brazilian interest rate future contracts with different maturities shows that combined forecasts consistently outperform individual models in several instances, specially when economic criteria is taken into account. JEL: C53; E43; G17. key-words: yield curve; dynamic factor models; forecast combinations; economic value of forecasts; Kalman filter. ∗Universidade Federal de Santa Catarina, Departamento de Economia. Campus Universitário Trindade 88049-970, Florianopolis, SC Brasil. E-mail: andre.portela@ufsc.br.		João F. Caldeira;Guilherme V. Moura;André A. P. Santos	2016	Computational Statistics & Data Analysis	10.1016/j.csda.2014.05.008	kalman filter;econometrics;forecast skill;forecast error;forecast verification;consensus forecast;yield curve;statistics	Web+IR	26.63422600376868	-21.676181470990567	30632
ab29d34ad6e0ed55f64dde4d7b6de33fcad8ca0e	identification of geochemical anomalies through combined sequential gaussian simulation and grid-based local singularity analysis		Abstract Local singularity analysis (LSA) has been proven to be an effective tool for identifying weak geochemical anomalies. The common practice of grid-based LSA is to firstly interpolate irregularly distributed observations onto a raster map by using either kriging or inverse distance weighting (IDW). The inherent nature of the weighted moving averaging of these methods typically subjects the interpolated map to a smoothing effect. Additionally, the traditional procedure did not allow for uncertainties on the values of geochemical attributes at unsampled locations. As such, these two aspects might affect LSA results. This paper presents a hybrid method, which combines sequential Gaussian simulation and grid-based LSA to identify geochemical anomalies. A case study of processing soil samples collected from the Jilinbaolige district, Inner Mongolia, China, further illustrates the hybrid method and helps compare the results with those from kriging-based LSA. The findings indicate that (1) the uncertainties of values at unsampled locations could affect the results of grid-based LSA, and (2) singularity exponents from kriging-based LSA roughly represent the trend (median) of singularity exponent distributions from simulation-based LSA, but the latter can also provide a measure of uncertainty of singularity exponent propagated from the uncertain values at unsampled locations, and (3) the procedure combining simulation-based LSA and analysis of distance is a feasible way for identifying geochemical anomalies with uncertainty being considered. The anomaly probability map obtained can provide a more generalized perspective than interpolation-based LSA to delineate anomalous areas.	simulation	Jian Wang;Renguang Zuo	2018	Computers & Geosciences	10.1016/j.cageo.2018.05.010	statistics;computer science;grid;interpolation;smoothing;inverse distance weighting;gaussian;raster graphics;kriging;singularity	AI	25.81163792598216	-18.202719400859454	30672
9cf06c87e0e8236cd1736c6b01efc4ce3c2f93c0	statistical results on control variables with application to queueing network simulation	queueing network;767 statistical results on control variables;694 simulation of closed queueing networks;700 variance reduction in queueing network simulation	The development and application of control variables for variance reduction in the simulation of a wide class of closed queueing networks is discussed. These networks allow multiple types of customers, priorities and blocking. Alternative methods of generating confidence intervals from independent replications of a simulation are investigated. A result is given which quantifies the loss in variance reduction caused by the estimation of the optimum control coefficients. This loss is an increasing function of the number of control variables. Good variance reduction is obtained providing that the number of control variables remains small.	simulation	Stephen S. Lavenberg;Thomas L. Moeller;Peter D. Welch	1982	Operations Research	10.1287/opre.30.1.182	g-network;mean value analysis;econometrics;computer science;operations management;layered queueing network;statistics	Metrics	29.136254647196363	-16.812192507873704	30712
39001d79708f0cbc74301b16be4730893d093c68	eigenvalues and condition numbers of complex random matrices	linear algebra;hypergeometric function;matrice aleatoire;analisis numerico;polynome zonal;analisis estadistico;62h15;complex matrix;fonction polynomiale;62h10;15a52;random matrix;distribucion estadistica;statistical hypothesis testing;condition number distribution;analyse numerique;eigenvalue;zonal polynomial;matrice complexe;eigenvalue distribution;matriz wishart;numerical analysis;statistical analysis;distribution statistique;62g10;33cxx;random matrices;algebre lineaire;valor propio;62f03;analyse statistique;matrice wishart;numero de condicionamiento;fonction hypergeometrique;funcion matricial;matrix function;condition number;condition number distribution 15a52;wishart matrix;algebra lineal;fonction matricielle;funcion hipergeometrica;15a15;valeur propre;funcion polinomial;polynomial function;matriz aleatoria;complex zonal polynomials;60e05;65f15;statistical distribution;complex random matrix;indice conditionnement;complex wishart matrix;matriz compleja	In this paper, the distributions of the largest and smallest eigenvalues of complex Wishart matrices and the condition number of complex Gaussian random matrices are derived. These distributions are represented by complex hypergeometric functions of matrix arguments, which can be expressed in terms of complex zonal polynomials. Several results are derived on complex hypergeometric functions and complex zonal polynomials and are used to evaluate these distributions. Finally, applications of these distributions in numerical analysis and statistical hypothesis testing are mentioned.	condition number;numerical analysis;polynomial	Tharmalingam Ratnarajah;Rémi Vaillancourt;M. Alvo	2004	SIAM J. Matrix Analysis Applications	10.1137/S089547980342204X	matrix analysis;combinatorics;complex normal distribution;generalized hypergeometric function;linear algebra;random matrix;calculus;mathematics;statistics;algebra	Theory	34.02278413000067	-21.26893568188102	30760
6865cf4d5519422e4652a1b775d0728c4e3d02a6	bayesian longitudinal paired comparison model and its application to sports data using weighted likelihood bootstrap	longitudinal data analysis;bayesian data analysis;bradley terry model;paired comparison;sports data	Bayesian Longitudinal Paired Comparison Model and Its Application to Sports Data Using Weighted Likelihood Bootstrap Satoshi Usami To cite this article: Satoshi Usami (2015): Bayesian Longitudinal Paired Comparison Model and Its Application to Sports Data Using Weighted Likelihood Bootstrap, Communications in Statistics Simulation and Computation, DOI: 10.1080/03610918.2015.1026989 To link to this article: http://dx.doi.org/10.1080/03610918.2015.1026989	communications in statistics – simulation and computation	Satoshi Usami	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2015.1026989	pairwise comparison;econometrics;data mining;mathematics;statistics	AI	28.238587421748765	-22.655271816496484	30767
02f9fc121ba05f390475b9b0fa9b510336953fd3	adaptive compressed sensing for depthmap compression using graph-based transform	compressed sensing;coherence transforms video coding encoding compressed sensing image edge detection image coding;dibr technique adaptive compressed sensing depthmap compression graph based transform gbt greedy algorithm mutual coherence edge structure cs measurement matrix low complexity estimate iterative process h 264 avc intra frame bitrate psnr gain depth image based rendering;data compression;depthmap compression compressed sensing cs graph based transform gbt;edge detection;greedy algorithms;iterative methods;iterative methods compressed sensing data compression edge detection greedy algorithms	In this paper we present an adaptive compressed sensing (CS) framework for depth map compression using a family of graph-based transforms (GBT). To improve overall performance, we propose a greedy algorithm that selects for each block a GBT minimizing a metric, based on average mutual coherence, that takes into consideration both the edge structure of the block and the characteristics of the CS measurement matrix. This algorithm uses a low-complexity estimate of the mutual coherence, so that explicit construction of the GBT at the encoder is not required in the iterative process. As compared to coding using H.264/AVC, the proposed approach applied to intra-frames shows an average of 39 % bitrate savings or 3.8 dB PSNR gain for views rendered using a depth image based rendering (DIBR) technique.	compressed sensing;depth map;encoder;government and binding theory;greedy algorithm;h.264/mpeg-4 avc;heightmap;iteration;mutual coherence (linear algebra);peak signal-to-noise ratio	Sungwon Lee;Antonio Ortega	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6467013	data compression;computer vision;mathematical optimization;greedy algorithm;edge detection;computer science;theoretical computer science;mathematics;iterative method;compressed sensing	Robotics	45.01805186927155	-17.937388576107203	30843
370f87d5b313baccc0c6fffabeeb3348f279d72c	dna computing model for the 0/1 knapsack problem	numerical optimization;knapsack problem;dna computing;profitability	We have devised a DNA encoding method and a corresponding DNA algorithm for the 0/1 knapsack problem. Suppose that item set I={1,2 ... n}, profit set P={p_1,p_2,...,p_n}, weight set W={w_1,w_2,...,w_n}, and knapsack capacity is c. We use two DNA strands s_i1 and s_i2 to encode each item i, where the DNA strand s_i1 is with a length of wi whose center part is with a length of p_i, and the DNA strand s_i2 is the reverse complement of the center part of s_i1. For any two items i,j we add one DNA strand s_aij as an additional code, which is the reverse complement of the last part of s_i1 and the first part of s_j1. The proposed DNA encoding method is an improvement on the previous ones, and it provides further evidence for the ability of DNA computing to solve numerical optimization problems.	algorithm;computation;dna computing;dspace;encode;knapsack problem;mathematical optimization;numerical analysis;parallel computing;pure function;strand (programming language);time complexity	Aili Han	2006	2006 Sixth International Conference on Hybrid Intelligent Systems (HIS'06)	10.1109/HIS.2006.21	mathematical optimization;theoretical computer science;mathematics;algorithm	Robotics	25.924616540762674	3.8211565509127787	30886
1f739db968853a3d6dc0102f7572b08124c8baef	efficient pitch filter encoding for variable rate speech processing	analyse parole;analysis by synthesis;tabla codificacion;traitement signal;optimisation;vector quantisation speech coding speech synthesis variable rate codes filtering theory linear predictive coding adaptive codes optimisation;speech synthesis;error function;variable rate;analisis palabra;speech processing;speech analysis;erreur quadratique moyenne;tratamiento palabra;traitement parole;speech coding;adaptive codes;indexing terms;experimental result;variable rate codes;linear predictive coding;cuantificacion vectorial;evaluation subjective;varying speed;vector quantization;codebook;mean square error;table codage;signal processing;velocidad variable;funcion error;resultado experimental;sintesis palabra;fonction erreur;vector quantizer;vitesse variable;filters encoding speech processing vector quantization code standards speech coding testing speech analysis bit rate bandwidth;2 kbit s pitch filter encoding variable rate speech processing analysis by synthesis techniques speech coding standards long term predictor adaptive codebook coder performance encoding rate computational requirements pitch filter parameters predictor order allocated bit rate long term predictor optimization vector quantization pitch filter coefficients subjective quality coding rate closed loop computation variable rate celp coder federal standard 1016 celp;error medio cuadratico;resultat experimental;vector quantisation;subjective evaluation;procesamiento senal;filtering theory;synthese parole;evaluacion subjetiva;quantification vectorielle	Analysis-by-synthesis techniques are used in a wide variety of speech coding standards and applications for rates below 16 kbps. The presence of a long-term predictor, commonly known as the adaptive codebook, is critical to coder performance at the lower rates. Unfortunately, the encoding rate and computational requirements for high-quality encoding of pitch filter parameters can be excessive. Several popular approaches explore the trade-off between predictor order, allocated bit rate, and computational requirements for long-term predictor optimization. We investigate the relative performance of several long-term predictor structures and present a new approach to vector quantization of the pitch filter coefficients having a subjective quality equivalent to other schemes, but at a lower coding rate and requiring significantly less closed-loop computation. The performance is evaluated in a variable-rate CELP coder at an average rate of 2 kbps and in Federal Standard 1016 CELP.	speech processing	Stan A. McClellan;Jerry D. Gibson;B. Keith Rutherford	1999	IEEE Trans. Speech and Audio Processing	10.1109/89.736327	speech recognition;computer science;speech coding;signal processing;speech processing;speech synthesis;code-excited linear prediction	Visualization	47.62603014773091	-9.68065788629212	30925
2f00c3fc76dd683b9829d99370fecf94ad97fe88	augmented lagrange hopfield network initialized by quadratic programming for economic dispatch with piecewise quadratic cost functions and prohibited zones	quadratic programming;piecewise quadratic fuel cost function;prohibited zones;augmented lagrange hopfield network;economic dispatch	This paper proposes a method based on quadratic programming (QP) and augmented Lagrange Hopfield network (ALHN) for solving economic dispatch (ED) problem with piecewise quadratic cost functions and prohibited zones. The ALHN method is a continuous Hopfield neural network with its energy function based on augmented Lagrange function which can properly deal with constrained optimization problems. In the proposed method, the QP method is firstly used to determine the fuel cost curve for each unit and initialize for the ALHN method, then a heuristic search is used for repairing prohibited zone violations, and the ALHN method is finally applied for solving the problem if any violations found. The proposed method has been tested on different systems and the obtained results are compared to those from many other methods in the literature. The result comparison has indicated that the proposed method has obtained better solution quality than many other methods. Therefore, the proposed QP-ALHN method could be a favorable method for solving the ED problem with piecewise quadratic cost functions and prohibited zones.	dynamic dispatch;hopfield network;quadratic programming	Vo Ngoc Dieu;Peter Schegner	2013	Appl. Soft Comput.	10.1016/j.asoc.2012.08.026	mathematical optimization;control theory;mathematics;mathematical economics;economic dispatch;quadratic programming	Crypto	29.585323730182328	0.17497531190578922	30966
e8085694757aba98250f83affb282be40ceb0dc0	wavelet scalable speech coding using algebraic quantization	speech coding quantization bit rate wideband speech codecs narrowband wavelet packets signal synthesis speech synthesis delay;algebraic codes;embedded coding;embedded codec itu t g 729 1;wavelet transforms algebraic codes speech coding;itu t g 722 codecs;algebraic quantization;wavelet scalable speech coding;speech coding;bandwidth extension;itu t g 729;wavelet packet;wavelet transforms;linear prediction coding;linear predictive coding;enhancement layer;algebraic quantization embedded coding linear predictive coding bandwidth extension wavelets;scalable codecs;audio signals;embedded codec itu t g 729 1 wavelet scalable speech coding algebraic quantization scalable codecs audio signals itu t g 729 wavelet packet coefficients itu t g 722 codecs;wavelets;wavelet packet coefficients	This paper proposes a new structure for a scalable codec. Our proposed codec works with 10 ms input frame for wideband speech and audio signals at bit rates ranging from 8 to 32 kbit/s. The core layer is the ITU-T G.729 at 8 kbit/s producing a narrowband output. The first enhancement layer is a band-width extension providing a wideband output with 2 kbit/s. The second enhancement layer is based on algebraic quantization of wavelet packet coefficients and improves gradually the synthesized signal as the bitrate increases. For speech signals, at bitrates of 24 and 32 kbit/s, the codec is shown to be equivalent to the ITU-T G.722 codec at 56 and 64 kbit/s, respectively. Moreover, the codec at 32 kbit/s is assessed to be equivalent to the recently standardized embedded codec ITU-T G.729.1 at the same bitrate with a lower algorithmic delay.	codec;coefficient;data rate units;embedded system;g.722;g.729;g.729.1;network packet;scalability;speech coding;wavelet	Mickaël De Meuleneire;Hervé Taddei;Dominique Pastor	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518719	enhanced variable rate codec;wavelet;adaptive multi-rate audio codec;linear predictive coding;speech recognition;full rate;bandwidth extension;telecommunications;computer science;g.726;audio signal;speech coding;g.711;wavelet transform;g.723	Robotics	47.78310090346082	-8.558002471601629	31025
29474a38f87cdaddeda89da60514beba64933fba	a linear map-based mutation scheme for real coded genetic algorithms	convergence;gene mutation;satisfiability;genetics;gene mutation linear map based mutation real coded genetic algorithm continuous optimization problem chromosome space randomly generated mapping matrix;genetics biocybernetics cellular biophysics genetic algorithms;accuracy;biocybernetics;biological cells;vectors;continuous optimization;biological cells vectors benchmark testing convergence accuracy encoding genetics;randomly generated mapping matrix;genetic algorithms;floating point;continuous optimization problem;encoding;linear map based mutation;cellular biophysics;benchmark testing;real coded genetic algorithm;chromosome space	Real coded genetic algorithms (RCGAs) have been widely studied and applied to deal with continuous optimization problems for years. However, how to improve the degree of accuracy so as to produce high quality solutions is still one of the main difficulties that RCGAs face with. This paper proposes a novel mutation scheme for RCGAs. The mutation operator is defined as a linear map in the space of chromosomes (in RCGAs each chromosome is a floating point vector). It operates on a whole chromosome instead of several single genes to produce the new chromosome. The linear map is represented by a randomly generated mapping matrix which satisfies some predefined constraints. By this way, the constraints restrict the mutations of genes on a same chromosome as a whole. RCGA with the proposed mutation scheme is tested on 16 benchmark functions. Results demonstrate that the proposed scheme not only improves the solution accuracy that RCGA can obtain, but also presents a very fast convergence speed. The linear map-based mutation scheme has a bright future to improve RCGAs.	benchmark (computing);continuous optimization;display resolution;genetic algorithm;mathematical optimization;procedural generation	Yue-jiao Gong;Xiaomin Hu;Jun Zhang;Ou Liu;Hai-lin Liu	2010	IEEE Congress on Evolutionary Computation	10.1109/CEC.2010.5586270	benchmark;mathematical optimization;genetic algorithm;convergence;computer science;bioinformatics;floating point;artificial intelligence;biocybernetics;mathematics;accuracy and precision;continuous optimization;encoding;satisfiability	Vision	27.103260849190253	-7.181016704179599	31045
717d42c9eabf0df8ed7c3bcb0c928a9b15fff8de	non-essentiality of correlation between image and depth map in free viewpoint image coding: accurate depth map case	rendering computer graphics filtering theory image coding;joints encoding image coding image edge detection noise transform coding video coding;free viewpoint image coding nonjoint filter post filter set weighed mode filter edge preserving filter dibr depth image based rendering depth map;free viewpoint image synthesis depth map coding post filtering depth image based rendering	We show non-essentiality of using a correlation between an image and a depth map for depth-image-based rendering (DIBR), when we use an accurate depth map. For coding of DIBR, an edge preserving filter, which jointly use the image and the depth map, as a post filter is a suitable approach. The joint filter not only removes coding distortion, but also improves accuracy of the coded depth map itself. Considering the development 3D technology, the accuracy of the input depth map will be improved. If we use an accurate depth map, e.g. the ground truth, the accuracy improvement of the joint filter becomes little. To reveal the fact, we use various codecs (JPEG, JPEG2000, and H.264/AVC) and use two state-of-the-arts filters, which are the post filter set as the non-joint filter, and the weighed mode filter as the joint filter. Experimental results show that the post filters do not require the joint image, and self-sustained types of the non-joint filter have better performance.	codec;depth map;distortion;ground truth;h.264/mpeg-4 avc;jpeg 2000	Tomohiko Inoue;Norishige Fukushima;Yutaka Ishibashi	2014	2014 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)	10.1109/3DTV.2014.6874739	computer vision;computer science;multimedia;computer graphics (images)	Vision	44.49557072568218	-18.282196230748415	31205
4ff76489376ed4f28885a84ee7ec4301d912c0dc	distributed video coding using wavelet	wyner ziv;decoding;turbo coder;turbo codes;zte coding;tree data structures;bit rate;zero tree structure;zero tree entropy coding;wavelet transforms;video coding;wavelet transform;spatial correlation;wyner ziv theory;intra codec;entropy codes;tree structure;distributed video coding;source code;entropy;motion compensated prediction;video coding decoding entropy source coding encoding wavelet transforms wavelet coefficients tree data structures turbo codes bit rate;side information;turbo coder distributed video coding zero tree entropy coding zte coding wyner ziv theory wavelet transform zero tree structure intra codec;wavelet transforms entropy codes tree data structures turbo codes video coding;encoding;wavelet coefficients;source coding	This paper proposes a distributed video coding scheme based on the zero tree entropy (ZTE) coding. Wyner-Ziv theory on source coding with side information is taken as the basic coding principle, which makes independent encoding and joint decoding possible. In this scheme, wavelet transform is used to exploit the spatial correlation of a Wyner-Ziv frame. The quantized wavelet coefficients are reorganized in terms of the zero tree structure so as to identify the significant and insignificant coefficients. The significance map is intra-codec and transmitted. In particular, the significant coefficients are independently encoded with turbo coder, and only the parity bits are transmitted. At the decoder, a predictive frame generated through motion-compensated prediction is used as the side information, with which the Wyner-Ziv frame can be conditionally decoded. Experimental results show that, compared to the traditional intra-frame coding and pixel-domain Wnyer-Ziv video coding, the proposed scheme can achieve a better coding performance, especially at low bit rates	codec;coefficient;data compression;encoder;entropy encoding;frame language;intra-frame coding;parity bit;pixel;tree structure;wavelet transform	Xun Guo;Yan Lu;Feng Wu;Wen Gao	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1693861	arithmetic coding;sub-band coding;linear network coding;speech recognition;shannon–fano coding;telecommunications;harmonic vector excitation coding;variable-length code;computer science;theoretical computer science;context-adaptive variable-length coding;coding tree unit;mathematics;tunstall coding;context-adaptive binary arithmetic coding;statistics;huffman coding;wavelet transform;source code	Arch	45.3738741302019	-16.363523381131905	31258
14bdae778c9d07345d592aac7da1978235bb69ce	test for exponentiality against weibull and gamma decreasing hazard rate alternatives	empirical laplace transform;goodness of fit test;likelihood test			Simos G. Meintanis	2007	Kybernetika		econometrics;score test;mathematics;goodness of fit;statistics	Crypto	30.85876118756766	-21.42165461967629	31302
00f43184ada226df02cedfb82d41d3f4530ed595	dominant and recessive genes in evolutionary systems applied to spatial reasoning	genetic operator;bottom up;spatial reasoning;value analysis;genetics;evolutionary system;evolutionary computing	Learning genetic representation has been shown to be a useful tool in evolutionary computation. It can reduce the time required to find solutions and it allows the search process to be biased towards more desirable solutions. Learning genetic representation involves the bottom-up creation of evolved genes from either original (basic) genes or from other evolved genes and the introduction of those into the population. The evolved genes effectively protect combinations of genes that have been found useful from being disturbed by the genetic operations (cross-over, mutation). However, this protection can rapidly lead to situations where evolved genes interlock in such a way that few or no genetic operations are possible on some genotypes. To prevent the interlocking previous implementations only allow the creation of evolved genes from genes that are direct neighbours on the genotype and therefore form continuous blocks. In this paper it is shown that the notion of dominant and recessive genes can be used to remove this limitation. Using more than one gene at a single location makes it possible to construct genetic operations that can separate interlocking evolved genes. This allows the use of non-continuous evolved genes with only minimal violations of the protection of evolved genes from those operations. As an example, this paper shows how evolved genes with dominant and recessive genes can be used to learn features from a set of Mondrian paintings. The representation can then be used to create new designs that contain features of the examples. The Mondrian paintings can be coded as a tree, where every node represents a rectangle division, with values for direction, position, linewidth and colour. The modified evolutionary operations allow the system to create non-continuous evolved genes, for example associate two divisions with thin lines, without specifying other values. Analysis of the behaviour of the system shows that about one in ten genes is a dominant/recessive gene pair. This shows that while dominant and recessive genes are important to allow the use of noncontinuous evolved genes, they do not occur often enough to seriously violate the protection of evolved genes from genetic operations. 1 Learning Genetic Representations Evolutionary systems, where random and probability-driven choices play an important part, are strongly influenced by the representation used. Instead of trying to create a neutral representation and to minimize any influence on the outcome of the process, previous work has shown that it can make sense to intentionally introduce a bias into the representation. This can improve the generation of solutions in optimization problems, and can also be used to bias the results of an evolutionary system so that it produces more desirable results. Starting with a simple representation, a ‘conventional’ evolutionary system is used to create individuals according to an application-specific fitness function. At the same time, a process observes how the system is using the initial representation. If sufficiently strong patterns are observed, new, evolved genes that represent these patterns are produced and added to the representation (Schnier & Gero 1995). The new evolved genes are used by the system, and can again be incorporated into other, larger evolved genes. Using the evolved genes can lead to faster problem solving times. The evolved representation can also be re-used for new applications, where the new solution is expected to share features of the original application (see e.g. Gero & Kazakov 1996). If the initial system is given a set of examples, and the fitness is set to be a measure of the resemblance to the examples, the evolved coding that is created in this process will represent features of the examples. In a second step, this representation can be used to create new individuals that share features of the examples (Schnier & Gero 1996) The function of evolved genes is the same in both cases: protection of a set of genes from disruption by evolutionary operations like mutation and cross-over. 2 Interlocking Evolved Genes Genetic operations are used to permutate the genetic material from one or two individuals, creating a new individual. Cross-over and mutation are the most common operations (see e.g. Michalewicz 1992). Cross-over operations work by cutting the genotypes into two parts, and then swapping the parts of two parents. Evolved genes, however, can easily interlock and lead to situations where no point can be found where cutting is possible without destroying one or more evolved genes, Fig. 1(a). If the operation was allowed to cut through an evolved gene, the protection of gene groups would be violated and the evolved genes would lose their function. In previous work, evolved genes have therefore been created by combining only directly adjacent genes on the genotype into higher-level genes. In a bottom-up process, this leads to large, continuous groups of basic genes that are represented by an evolved gene. These genes cannot interlock, and conventional genetic operations thus work with little change, Fig. 1(b). This ‘simple evolved gene’ model has been shown to be applicable to a range of applications. However, in some applications its restrictions may be undesirable. Figure 1(c) shows an example: while there is no simple evolved gene that can be established, it is still possible to identify useful evolved genes with non-local neighbours. b c d a a a c c d c a a c d a a a a d a a b d d a a a d d b a a a f a c d a b c a d a a a f b a c d a c a d a b b b Possible cut Evolved genes (b) (a) (c) Fig. 1. (a) Interlocking evolved genes, (b) simple evolved genes, and (c) example for use of complex evolved genes 3 Using Dominant/Recessive Genes To allow the use of arbitrary evolved genes, the genetic operations and the method used to create new individuals from evolved genes have to be modified. In doing this it is important to ensure that evolved genes will still be protected. The notion of diploid genetic representations with dominant and recessive genes has been used in genetic algorithms before, but with a very different purposes: a recessive set of genes can act as a memory of previously useful gene sequences, and help the genetic algorithm to adapt in applications with non-stationary fitness functions (Goldberg & Smith 1987, Ng & Wong 1995). Other work has shown that diploidity can help protect genetic algorithms against premature convergence (Greene 1996, Yukiko & Nobue 1994).	bottom-up parsing;denial-of-service attack;emoticon;evolutionary computation;evolutionary systems;exponent bias;fitness function;gene expression programming;genetic algorithm;genetic representation;interlock (engineering);kero blaster;mathematical optimization;mondrian olap server;mutation (genetic algorithm);paging;premature convergence;problem solving;spatial–temporal reasoning;stationary process;the australian;top-down and bottom-up design;xfig	Thorsten Schnier;John S. Gero	1997		10.1007/3-540-63797-4_65	evolutionary programming;interactive evolutionary computation;human-based evolutionary computation;computer science;bioinformatics;artificial intelligence;genetic operator;machine learning;top-down and bottom-up design;spatial intelligence;evolutionary computation	ML	26.027153078362314	-9.35490292143785	31356
7aaf74506ea815e4ee5227da6c49bb09a7aeded4	theoretical analysis and parameter setting of hopfield neural networks	modelizacion;hopfield model;systeme degenere;modele hopfield;eigenvalue problem;fonction energie;hopfield network;modelo hopfield;travelling salesman problem;vector space;hopfield neural nets;probleme valeur propre;intelligence artificielle;degenerate system;energy function;hopfield neural network;storage tank;problema viajante comercio;modelisation;sistema degenerado;reservoir stockage;degeneration;probleme commis voyageur;reseau neuronal hopfield;theoretical analysis;funcion energia;artificial intelligence;inteligencia artificial;espace vectoriel;reseau neuronal;tanque almacenamiento;modeling;espacio vectorial;red neuronal;problema valor propio;neural network	This paper analyzes the behavior of Hopfield networks as a method of solving the travelling salesman problem (TSP) with an enhanced formulation of energy function, which is more available than the Hopfield-Tank (H-T) one. The analysis is based on the geometry of the subspace set up by the degenerate eigenvalues of the connection matrix. A set of criterion for parameter settings is derived. The new parameters performed well in the simulations.	hopfield network;mathematical optimization;simulation;travelling salesman problem	Hong Qu;Zhang Yi;XiaoLin Xiang	2005		10.1007/11427391_118	storage tank;systems modeling;vector space;computer science;artificial intelligence;machine learning;mathematics;travelling salesman problem;operations research;hopfield network;artificial neural network;algorithm	Vision	29.74496857160898	4.112303213224413	31375
b19d5160b9f5b6eb77cb8be7af7f0462aa17c5cd	multiple description wavelet coding of layered video using optimal redundancy allocation	transformation ondelette;signal image and speech processing;traitement signal;evaluation performance;multiple description;layered video;performance evaluation;video signal processing;redundancia;resource allocation;information transmission;evaluacion prestacion;resource management;asignacion optima;video coding;gestion recursos;quantum information technology spintronics;redundancy;senal video;signal video;codage video;signal processing;allocation optimale;traitement signal video;video signal;gestion ressources;asignacion recurso;transformacion ondita;transmision informacion;transmission information;allocation ressource;algoritmo optimo;algorithme optimal;optimal allocation;optimal algorithm;procesamiento senal;wavelet transformation;redondance	1 Department of Electronic Engineering, Division of Engineering, King’s College London, WC2R 2LS London, United Kingdom 2 The Kellogg School of Management, Northwestern University, IL 60208, USA 3 The European Patent Office, Munich 80298, Germany 4 The Informatics and Telematics Institute, Thessaloniki GR-57001, Greece 5 The Electrical and Computer Engineering Department of the University of Thessaloniki, Thessaloniki GR-54124, Greece	computer engineering;electronic engineering;informatics;telematics;wavelet transform	Nikolaos V. Boulgouris;Konstantinos E. Zachariadis;Angelos Kanlis;Michael G. Strintzis	2006	EURASIP J. Adv. Sig. Proc.	10.1155/ASP/2006/83542	telecommunications;resource allocation;computer science;resource management;theoretical computer science;signal processing;redundancy	DB	47.43834605308142	-13.732755554751767	31400
6695f65b63d799c4b605b02515d6c6989a73f322	how adaptive agents in stock market perform in the presence of random news: a genetic algorithm approach	human nature;prediccion;computer program;evaluation performance;optimisation;stock market;performance evaluation;optimizacion;generic model;evaluacion prestacion;existencias;rule based;rule extraction;economic model;time series;algoritmo genetico;trading strategy;investment strategies;simple genetic algorithm;correlation function;algorithme genetique;autoorganizacion;self organization;stock;genetic algorithm;decision process;optimization;prediction;technical analysis;white noise;autoorganisation;historical data	The effect of random news on the performance of adaptive agents as investors in stock market is modelled by genetic algorithm and measured by their portfolio values. The agents are defined by the rules evolved from a simple genetic algorithm, based on the rate of correct prediction on past data. The effects of random news are incorporated via a model of herd effect to characterize the human nature of the investors in changing their original plan of investment when the news contradicts their prediction. The random news is generated by white noise, with equal probability of being good and bad news. Several artificial time series with different memory factors in the time correlation function are used to measure the performance of the agents after the training and testing. A universal feature that greedy and confident investors outperform others emerges from this study.	genetic algorithm;greedy algorithm;time series;white noise	Kwok Yip Szeto;L. Y. Fong	2000		10.1007/3-540-44491-2_74	rule-based system;human nature;self-organization;simulation;genetic algorithm;investment strategy;prediction;artificial intelligence;economic model;trading strategy;time series;white noise;stock;correlation function;technical analysis;statistics	ML	26.87666841548504	-10.455763117291195	31401
63223280039b38fda734e07950dd6195bab06eb5	metaheuristic tuning of type-ii fuzzy inference systems for data mining	particle swarm optimization metaheuristic tuning data mining type 2 fuzzy set fuzzy membership function interval type 2 fuzzy inference system optimum rule base selection it2fis learning metaheuristic learning artificial bee colony gray wolf optimization;training;fuzzy sets;fuzzy logic;particle swarm optimisation ant colony optimisation data mining fuzzy reasoning fuzzy set theory knowledge based systems learning artificial intelligence;statistics;optimization fuzzy sets fuzzy logic microorganisms sociology statistics training;optimization;microorganisms;sociology	Introduction of the fuzzy-set enabled the modeling of uncertain and noisy information. Type-2 fuzzy set took this further ahead by allowing fuzzy membership function to be fuzzy itself. In this work, we discussed an interval type-2 fuzzy inference system (IT2FIS). The training of the IT2FIS was provided in supervised manner by using metaheuristic algorithms. We comprehensively illustrated the formulation of the IT2FIS into an optimization problem. A precise genotype (a real vector) mapping of IT2FIS and a population-based strategy for optimum rule-base selection is described in this work. Since the IT2FIS learning is computationally difficult and costly, which we described in detail in this work, a comprehensive comparison between the performances of the metaheuristic algorithms were examined. The obtained results suggest that the IT2FIS learning was faster at the initial iterations of the metaheuristic learning, but tend to slow and get stuck in local minima. However, the metaheuristic algorithms, differential evaluation and bacteria foraging optimization offered significantly better results when compared to artificial bee colony, gray wolf optimization, particle swarm optimization and the other fuzzy inference models chosen for comparisons from literature.	algorithm;analysis of algorithms;data mining;fuzzy logic;fuzzy set;inference engine;integrated development environment;iteration;mathematical optimization;maxima and minima;metaheuristic;optimization problem;particle swarm optimization;performance;rule-based system;serial ata;supervised learning	Varun Kumar Ojha;Ajith Abraham;Václav Snásel	2016	2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2016.7737743	fuzzy logic;mathematical optimization;parallel metaheuristic;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;mathematics;fuzzy set;fuzzy associative matrix;microorganism;fuzzy set operations;fuzzy control system;metaheuristic	Robotics	25.254380821229915	-7.32215903702169	31434
3332babae300faf65cbe132ee0ca5948dcdab593	pic/mc code bit1 for plasma simulations on hpc	mc;neutral particle modeling pic mc code bit1 plasma simulations hpc optimization technique high performance computing;optimization technique;high performance computing;boundary conditions;pic mc code bit1;plasmas;sol;scaling up;plasma edge;electric fields;hpc;plasma simulations;hpc pic mc plasma edge sol;plasma simulation plasma density plasma diagnostics plasma applications electrostatics computational modeling physics nuclear and plasma sciences high performance computing particle collisions;high performance computer;pic;electric potential;neutral particle modeling;particle modeling;ieee potentials;program processors;plasma applications;poisson equations	In this work we describe an optimization technique for PIC/MC modeling of the plasma edge using High Performance Computing. The developed code BIT1 can be used for 1D3V plasma and 2D3V neutral particle modeling with a reasonable scaling up to 1000 and more processors.	central processing unit;image scaling;mathematical optimization;plasma active	David Tskhakaya;Alejandro Soba;Ralf F Schneider;Matthias Borchardt;Evren Yurtesen;Jan Westerholm	2010	2010 18th Euromicro Conference on Parallel, Distributed and Network-based Processing	10.1109/PDP.2010.47	plasma;computational science;supercomputer;parallel computing;sol;boundary value problem;electric field;electric potential	HPC	44.9762564731636	2.1161407129596492	31449
b372310a3470bc17715afc934fac45ff2aa7234a	a goodness-of-fit test for multivariate multiparameter copulas based on multiplier central limit theorems	sample size;random number generator;parametric bootstrap;plackett formula;large scale simulation;parametric estimation;central limit theorem;t copula;pseudo observation;monte carlo experiment;goodness of fit test;normal copula;pseudo likelihood;rank	Recent large scale simulations indicate that a powerful goodness-of-fit test for copulas can be obtained from the process comparing the empirical copula with a parametric estimate of the copula derived under the null hypothesis. A first way to compute approximate p-values for statistics derived from this process consists of using the parametric bootstrap procedure recently thoroughly revisited by Genest and Rémillard. Because it heavily relies on random number generation and estimation, the resulting goodness-of-fit test has a very high computational cost that can be regarded as an obstacle to its application as the sample size increases. An alternative approach proposed by the authors consists of using a multiplier procedure. The study of the finitesample performance of the multiplier version of the goodness-of-fit test for bivariate one-parameter copulas showed that it provides a valid alternative to the parametric bootstrap-based test while being orders of magnitude faster. The aim of this work is to extend the multiplier approach to multivariate multiparameter copulas and study the finite-sample performance of the resulting test. Particular emphasis is put on elliptical copulas such as the normal and the t as these are flexible models in a multivariate setting. The implementation of the procedure for the latter copulas proves challenging and requires the extension of the Plackett formula for the t distribution to arbitrary dimension. Extensive Monte Carlo experiments, which could be carried out only because of the good computational properties of the multiplier approach, confirm in the	algorithmic efficiency;approximation algorithm;bivariate data;bootstrapping (statistics);computation;experiment;monte carlo method;random number generation;simulation	Ivan Kojadinovic;Jun Yan	2011	Statistics and Computing	10.1007/s11222-009-9142-y	sample size determination;econometrics;mathematical optimization;rank;central limit theorem;mathematics;goodness of fit;copula;statistics;multivariate t-distribution	AI	30.7800461134973	-21.942579348501226	31528
60742764f8ec9c3459c8a9c67aa2928f68396a0c	spur gears and leadscrew based, efficient and flexible infusion system design		This paper presents the design of an inexpensive infusion system for low and middle-income countries like Pakistan. A flexible improvised design in terms of accuracy and user-interface is presented, based on spur gears and leadscrew. The trade-offs between accuracy and other design parameters such as weight, cost and size are also discussed. A detailed analysis of accuracy dependence on leadscrew pitch and number of teeth of each gear is given. Two prototypes are presented, with and without gearbox. An android application is developed considering widespread use of smartphones. It provides access to internet making it easy to share the data. Several alarms and feedback components have also been added to the system. Final design is efficient in terms of number of hardware components, robustness, portability, cost, power and size achieving an accuracy of 23.7nL per step of the stepper motor with 20cc reservoir.		M. Rizwan Khan;Muhammad Tariq;F. Munir;Muhammad Awais Bin Altaf	2018	2018 IEEE Biomedical Circuits and Systems Conference (BioCAS)	10.1109/BIOCAS.2018.8584685	robustness (computer science);embedded system;the internet;android (operating system);systems design;leadscrew;software portability;stepper;transmission (mechanics);electronic engineering;computer science	EDA	53.257935014000225	-15.016453154972929	31555
3cd28f93d130befecbf2082679e720d3b723bc56	a novel application of crossover operator to a hybrid optimization framework: investigation into cutting problems	integer linear programming;bin packing;hybrid methods;genetic algorithms optimization biological cells heuristic algorithms integer linear programming search problems hybrid power systems;cutting and packing hybrid methods uniform order based crossover combinatorial optimization;mathematical operators;uniform order based crossover;biological cells;hybrid power systems;integer programming;mathematical operators bin packing genetic algorithms integer programming;heuristic algorithms;cutting and packing;genetic algorithms;optimization;search problems;constrained 2d nonguillotine cutting problem crossover operator hybrid optimization framework generate and solve framework metaheuristic engine genetic algorithm reduced instance generator optimization problem integer programming solver packing problem uniform order based crossover;combinatorial optimization	The Generate and Solve (GS) is a hybrid optimization framework that combines a metaheuristic engine (genetic algorithm), which works as a generator of reduced instances of the original optimization problem, and an integer programming solver. GS has been recently introduced in the literature and achieved promising results in cutting and packing problem instances. In this paper, we present a novel application of crossover operator, the Uniform Order-Based Crossover, to the GS framework. As a means to assess the potentialities behind the novel application, we provide as instantiation of the framework for dealing specifically with the constrained two-dimensional non-guillotine cutting problem. Computational experiments performed over standard benchmark problems are reported and discussed here, evidencing the effectiveness of the novel operator.	benchmark (computing);computation;experiment;genetic algorithm;integer programming;mathematical optimization;metaheuristic;optimization problem;roland gs;set packing;solver;universal instantiation	Rommel Dias Saraiva;Plácido Rogério Pinheiro	2012	2012 IEEE Congress on Evolutionary Computation	10.1109/CEC.2012.6256627	mathematical optimization;combinatorics;bin packing problem;genetic algorithm;integer programming;computer science;mathematics;algorithm	AI	24.618232036526322	-0.7819962653404849	31582
e6b32e091b62d29e64f131299ee941d199d612c3	reliability bounds for fault-tolerant systems with deferred repair using bounding split regenerative randomization	deferred repair;bounds;fault tolerant systems;60j22;68u01;60j25;continuous time markov chains;randomization;article	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	algorithmic efficiency;central processing unit;francis;fault tolerance;markov chain;parameter (computer programming);primary source;simpl;time complexity	Jamal Temsamani;Juan A. Carrasco	2014	Communications in Statistics - Simulation and Computation	10.1080/03610918.2012.700360	randomization;mathematics;statistics	Robotics	50.54143186651953	-3.3206254786868703	31584
aa7533d9f90139205352ea88f34e310930ff56e6	a novel image steganographic approach for hiding text in color images using hsi color model		Image Steganography is the process of embedding text in images such that its existence cannot be detected by Human Visual System (HVS) and is known only to sender and receiver. This paper presents a novel approach for image steganography using Hue-Saturation-Intensity (HSI) color space based on Least Significant Bit (LSB). The proposed method transforms the image from RGB color space to Hue-Saturation-Intensity (HSI) color space and then embeds secret data inside the Intensity Plane (I-Plane) and transforms it back to RGB color model after embedding. The said technique is evaluated by both subjective and Objective Analysis. Experimentally it is found that the proposed method have larger Peak Signal-to Noise Ratio (PSNR) values, good imperceptibility and multiple security levels which shows its superiority as compared to several existing methods.	color space;experiment;horizontal situation indicator;human visual system model;least significant bit;peak signal-to-noise ratio;steganography	Khan Muhammad;Jamil Ahmad;Haleem Farman;Muhammad Zubair	2014	CoRR	10.5829/idosi.mejsr.2014.22.05.21946	color histogram;computer vision;speech recognition;hsl and hsv;color depth;color image;high color;mathematics;color balance;color space;computer graphics (images)	Vision	40.68861234075821	-11.442861693740193	31646
4f2e0a741c07a9410489a3e226a0aa25cb1e8e56	rf sounding	3d geographic routing;gdstr;sensor networks	The demo we propose represents a first step toward RF Sounding [1]. Such a project is an open space installation which comprises both artistic and technological innovations; its aim is to provide the user, while entering a specifically defined area, with awareness of radio frequency signals characterizing the cellular networks band. Indeed, radio signals are shifted, with proper elaboration, to the audible band and the result is spread all over the specific area through a certain number of loudspeakers. For this procedure we have been inspired by the eternal metaphor of the impossible human dream to fly, thus the limitations of our senses which are capable of feeling audio waves (sounds) but (maybe luckily) are not capable of directly feeling radio frequency (RF) waves are highlighted. The translation is the starting point for a more complex and exciting musical composition. It has to be underlined that some features of RF sensed signals are kept unchanged in order to give to the listener the clear feeling of the amount (power) of RF signals present in the monitored area. The aim of this project is twofold. Indeed, from one side we want to increase end users knowledge of the strength of the power emitted by their cellular phones with respect to the electromagnetic fields produced in the environment, on the other hand we want to provide for an artistic and interactive installation that can also be remotely joined through a web interface.	automatic sounding;loudspeaker;mobile phone;radio frequency;user interface	Claudia Rinaldi;Luigi Pomante;Roberto Alesii;Fabio Graziosi	2010		10.1145/1869983.1870024	embedded system;wireless sensor network;telecommunications;computer science	HCI	44.26195999790544	-1.4638968727650497	31649
2d8d2ed1fe7db310579b2f18aaf1896f37a3e9ac	improving ewma plans for detecting unusual increases in poisson counts		Automated public health records provide the necessary data for rapid outbreak detection. An adaptive exponentially weighted moving average EWMA plan is developed for signalling unusually high incidence when monitoring a time series of nonhomogeneous daily disease counts. A Poisson transitional regression model is used to fit background/expected trend in counts and provides “one-day-ahead” forecasts of the next day’s count. Departures of counts from their forecasts are monitored. The paper outlines an approach for improving early outbreak data signals by dynamically adjusting the exponential weights to be efficient at signalling local persistent high side changes. We emphasise outbreak signals in steady-state situations; that is, changes that occur after the EWMA statistic had run through several in-control counts.	incidence matrix;persistent data structure;steady state;time complexity;time series	Ross Sparks;Tim Keighley;David Muscatello	2009	JAMDS	10.1155/2009/512356	ewma chart;econometrics;operations management;statistics	ML	27.366442968575804	-20.16190950223983	31668
01a75e0ff34c10b0cf35cb80465b39c097785264	estimation of the reliability of a stress-strength system from power lindley distributions	primary 62f10;62f12;power lindley distribution;maximum likelihood estimation;stress strength model;62f15;secondary 62f40;bootstrap estimation	In this paper, we are interested in the estimation of the reliability parameter R = P(X > Y) where X, a component strength, and Y, a component stress, are independent power Lindley random variables. The point and interval estimation of R, based on maximum likelihood, nonparametric and parametric bootstrap methods, are developed. The performance of the point estimate and confidence interval of R under the considered estimation methods is studied through extensive simulation. A numerical example, based on a real data, is presented to illustrate the proposed procedure.		M. E. Ghitany;Dhaifalla K. Al-Mutairi;S. M. Aboukhamseen	2015	Communications in Statistics - Simulation and Computation	10.1080/03610918.2013.767910	econometrics;mathematical optimization;interval estimation;mathematics;maximum likelihood;maximum likelihood sequence estimation;statistics	HPC	30.318577052626722	-21.746333180065747	31677
38fa1334c46aa3242b45617471224270cb8d3fe3	multi-armed bandits with constrained arms and hidden states		The problem of rested and restless multi-armed bandits with constrained availability of arms is considered. The states of arms evolve in Markovian manner and the exact states are hidden from the decision maker. First, some structural results on value functions are claimed. Following these results, the optimal policy turns out to be a threshold policy. Further, indexability of rested bandits is established and index formula is derived. The performance of index policy is illustrated and compared with myopic policy using numerical examples.	coat of arms;multi-armed bandit;numerical analysis	Varun Mehta;Rahul Meshram;Kesav Kaza;S. N. Merchant	2017	CoRR		mathematical optimization;mathematics;markov process	ML	37.105637379056745	3.7266299024530847	31747
21e075b697eb6bd340955e8e56be5c774a27cf20	exploring target change related fitness reduction in the moving point dynamic environment		Dynamic Environments present many challenges for Evolutionary Computing. Frequency of change and amplitude of change, all have dramatic effects of how a system will behave. This in conjunction with poor search operators can lead to populations that can’t react to change quickly, as they have become converged in the search space. This study presents an overview of some methods to minimize the impact of change, and allow algorithms to better react to change in Dynamic Environments. Through the use of a bare bones tunable dynamic environment, it is shown how the approaches implemented can provide algorithms with faster responses to change. These approaches are also shown to do this without having to redesign the algorithms search operators, and maintaining the same computational effort.		David Fagan;Michael O'Neill	2017		10.1007/978-3-319-71069-3_5	operator (computer programming);real-time computing;simulation;evolutionary computation;geography	HCI	25.699615251710295	-6.093554011246533	31803
7c4dc1e470a1e4190e33fd15a3a4feea59a1752e	cost–benefit optimization of structural health monitoring sensor networks	bayesian experimental design;bayesian inference;cost–benefit analysis;information theory;model order reduction;stochastic optimization;structural health monitoring;surrogate modeling	Structural health monitoring (SHM) allows the acquisition of information on the structural integrity of any mechanical system by processing data, measured through a set of sensors, in order to estimate relevant mechanical parameters and indicators of performance. Herein we present a method to perform the cost⁻benefit optimization of a sensor network by defining the density, type, and positioning of the sensors to be deployed. The effectiveness (benefit) of an SHM system may be quantified by means of information theory, namely through the expected Shannon information gain provided by the measured data, which allows the inherent uncertainties of the experimental process (i.e., those associated with the prediction error and the parameters to be estimated) to be accounted for. In order to evaluate the computationally expensive Monte Carlo estimator of the objective function, a framework comprising surrogate models (polynomial chaos expansion), model order reduction methods (principal component analysis), and stochastic optimization methods is introduced. Two optimization strategies are proposed: the maximization of the information provided by the measured data, given the technological, identifiability, and budgetary constraints; and the maximization of the information⁻cost ratio. The application of the framework to a large-scale structural problem, the Pirelli tower in Milan, is presented, and the two comprehensive optimization methods are compared.	algorithmic efficiency;analysis of algorithms;approximation algorithm;cma-es;computation (action);dynamic testing;emoticon;entropy maximization;evaluation;expectation–maximization algorithm;gradient;health occupations;information gain in decision trees;information theory;input/output;leucaena pulverulenta;loss function;mathematical model;mathematical optimization;maxillary right canine abutment;maxima and minima;maximum;metamodeling;model order reduction;monte carlo method;numerical analysis;optimization problem;pareto efficiency;polynomial;principal component analysis;quantity;sampling (signal processing);shannon (unit);stochastic optimization;structural integrity and failure;super high material cd;surrogate model;chaperone-mediated autophagy;cisplatin/cyclophosphamide/etoposide protocol;recycling endosome;sensor (device);spiromustine	Giovanni Capellari;Eleni Chatzi;Stefano Mariani	2018		10.3390/s18072174	structural health monitoring;electronic engineering;systems engineering;engineering;wireless sensor network	Robotics	47.82987460694724	4.009166991172755	31865
d59c21449e09c2568e5bf9b486bfac65977bbf5e	leveraging polynomial approximation for non-linear image transformations in real time		Display Omitted A hardware architecture for non-linear real-time image remapping is proposed.Finite differences are used to calculate the remapping function on-the-fly.It is shown that sub-pixel accuracy can be guaranteed at low hardware costs.The architecture features minimum latency and little utilization of FPGA resources.Results for lens distortion removal and a gesture recognition system are presented. Applying non-linear image transformations in real time remains a challenge for cost-sensitive embedded systems today. This paper offers a method to perform such transformations efficiently using polynomial approximations. It is based on a previously published method, but is not limited to transformations with little vertical variation, such as lens distortion removal. Instead, a wider range of image transformations is supported.Thus, a new, improved hardware architecture is presented and discussed. It can be configured to achieve virtually any required accuracy for practical transformation tasks at low hardware costs. As a proof of concept, the architecture has been integrated into a gesture recognition and gaze analysis system as an intelligent stereo camera delivering undistorted and rectified images.	approximation;nonlinear system;polynomial	Matthias Pohl;Michael Schaeferling;Gundolf Kiefer;Plamen Petrow;Egmont Woitzel;Frank Papenfuß	2014	Computers & Electrical Engineering	10.1016/j.compeleceng.2013.12.011	computer vision;telecommunications;computer science;electrical engineering;artificial intelligence;theoretical computer science;machine learning;algorithm;computer graphics (images)	ML	53.50711653892924	-16.385268188393038	31917
4828c33b841590ea032cd30c0d00db6379272656	premature convergence in constrained continuous search spaces	theoretical model;experimental analysis;premature convergence;search space;self adaptation;constrained real parameter optimization;evolution strategies;success rate;distributed models;evolution strategy;evolutionary algorithm;state transition	The optimum of numerical problems quite often lies on the constraint boundary or even in a vertex of the feasible search space. In such cases the evolutionary algorithm (EA) frequently suffers from premature convergence because of a low success probability near the constraint boundaries. We analyze premature fitness stagnation and the success rates experimentally for an EA using self-adaptive step size control. For a (1+1)-EA with a Rechenberg-like step control mechanism we prove premature step size reduction at the constraint boundary. The proof is based on a success rate analysis considering a simplified mutation distribution model. From the success rates and the possible state transitions, the expected step size change can be derived at each step. We validate the theoretical model with an experimental analysis.	adaptive stepsize;evolutionary algorithm;experiment;linear function;microsoft outlook for mac;nonlinear system;numerical analysis;premature convergence;spaces;theory	Oliver Kramer	2008		10.1007/978-3-540-87700-4_7	mathematical optimization;computer science;artificial intelligence;evolutionary algorithm;calculus;mathematics;evolution strategy;experimental analysis of behavior;premature convergence	AI	28.926958636896458	-4.071844654043937	31941
a9678c573f638da683792cc528e8bc6f9a48f7da	efficiency and mutation strength adaptation of the (mu, mui, lambda)-es in a noisy environment	algorithm performance;algorithm complexity;algorithm analysis;algoritmo adaptativo;complejidad algoritmo;minimizacion funcion;reduccion ruido;adaptive algorithm;algorithme adaptatif;complexite algorithme;function minimization;resultado algoritmo;noise reduction;reduction bruit;performance algorithme;algorithme evolutionniste;algoritmo evolucionista;analyse algorithme;evolutionary algorithm;analisis algoritmo;minimisation fonction	Noise is present in many optimization problems. Evolutionary algorithms are frequently reported to be robust with regard to the effects of noise. In many cases, there is a tradeoff between the accuracy with which the fitness of a candidate solution is determined and the number of candidate solutions that are evaluated in every time step. This paper addresses this tradeoff on the basis of recently established results from the analysis of the local performance of a recombinant multi-parent evolution strategy on a noisy sphere. It is shown that, provided that mutation strengths are appropriately adapted, the strategy is indeed able to cope with noise, and that results previously obtained for single-parent evolution strategies do not carry over to multi-parent strategies. Then, the problem of mutation strength adaptation in noisy environments is addressed. Mutative self-adaptation and the cumulative mutation strength adaptation algorithm are compared empirically in a simple fitness environment. The results suggest that both algorithms are prone to failure in the presence of noise.		Dirk V. Arnold;Hans-Georg Beyer	2000		10.1007/3-540-45356-3_4	computer science;artificial intelligence;evolutionary algorithm;noise reduction;mathematics;algorithm	NLP	27.3974612589911	1.9209713906787511	31987
0a013edff38576f342967eeae625ebef85cd54b1	energy efficient wireless communication using genetic algorithm guided faster light weight digital signature algorithm (gadsa)		In this paper GA based light weight faster version of Digital Signature Algorithm (GADSA) in wireless communication has been proposed. Various genetic operators like crossover and mutation are used to optimizing amount of modular multiplication. Roulette Wheel selection mechanism helps to select best chromosome which in turn helps in faster computation and minimizes the time requirements for DSA. Minimization of number of modular multiplication itself a NP-hard problem that means there is no polynomial time deterministic algorithm for this purpose. This paper deals with this problem using GA based optimization algorithm for minimization of the modular multiplication. Proposed GADSA initiates with an initial population comprises of set of valid and complete set of individuals. Some operators are used to generate feasible valid offspring from the existing one. Among several exponents the best solution reached by GADSA is compared with some of the existing techniques. Extensive simulations shows competitive results for the proposed GADSA.	computation;crossover (genetic algorithm);deterministic algorithm;digital signature;fitness proportionate selection;genetic algorithm;genetic operator;mathematical optimization;mutation (genetic algorithm);np-hardness;requirement;simulation;software release life cycle;time complexity	Arindam Sarkar;J. K. Mandal	2012	CoRR	10.5121/ijassn.2012.2302	mathematical optimization;theoretical computer science;mathematics;algorithm	Robotics	27.470786862930794	-1.8312558895927304	31999
250dc3f3017a4353ddc7d821358de91d751de89c	region representation: parallel connected stripes	representation polygone;computer aided design;algorithm analysis;cartographie;information processing;region;coding;conception assistee;cartography;analyse algorithme;traitement information;codage	"""A new coding scheme is presented for the representation of polygons or, more generally, of regions. This method is particularly suitable for the simultaneous management of several regions and hence for the solution of problems connected with automatic cartography or with CAD. The basic idea lies in the subdivision of the regions to be represented as horizontal stripes of uniform height and the management of these stripes as basic units of information. In this way, the data to be represented are """"rasterized."""" Algorithms are given for coding and decoding vectorial information (the boundaries of the regions to be represented) and for the main operations that may be done on the resulting data structure. In comparison with classical methods, the scheme presented here offers the possibility of distinguishing boundaries of regions from those of holes, efficiency of the algorithms used, and some compression of the memory needed. In addition, the particular nature of the method eliminates the problems of information ambiguity and redundancy of other schemes. 9 zgs4 Academic Press, Inc."""	algorithm;cartography;code;computer-aided design;data structure;polygon (computer graphics);raster graphics;rasterisation;redundancy (engineering);stripes;subdivision surface;units of information	Claudio Montani	1984	Computer Vision, Graphics, and Image Processing	10.1016/S0734-189X(84)80018-3	region;information processing;mathematics;coding	Graphics	39.44859564083619	-18.670545033746457	32009
e12267dbe482aac556f48d95deaf3d3d2d730fbb	joint prior distributions for variance parameters in bayesian analysis of normal hierarchical models	hyperparameter;multivariate log gamma;directional derivative;hyperprior;random effect;sensitivity analysis;62f15;variance components;random coefficient;hierarchical models;multi level models	In random effect models, error variance (stage 1 variance) and scalar random effect variance components (stage 2 variances) are a priori modeled independently. Considering the intrinsic link between the stages 1 and 2 variance components and their interactive effect on the parameter draws in Gibbs sampling, we propose modeling the variances of the two stages a priori jointly in a multivariate fashion. We use random effects linear growth model for illustration and consider multivariate distributions to model the variance components jointly including the recently developed generalized multivariate log gamma (G-MVLG) distribution. We discuss these variance priors as well as the independent variance priors exercised in the literature in different aspects including noninformativeness and propriety of the associated posterior density. We show through an extensive simulation experiment that modeling the variance components of different stages multivariately results in better estimation properties for the response and random effect model parameters compared to independent modeling. We scrutinize the sensitivity of response model coefficient estimates to the parameters of considered noninformative variance priors and find that their full conditional expectations are insensitive to noninformative G-MVLG prior parameters. We apply independent and joint models for analysis of a real dataset and find that multivariate priors for variance components lead to better fitted hierarchical model than the univariate variance priors.	bayesian network	Haydar Demirhan;Zeynep Kalaylioglu	2015	J. Multivariate Analysis	10.1016/j.jmva.2014.12.013	multivariate analysis of variance;econometrics;hyperparameter;conditional variance;directional derivative;variance-based sensitivity analysis;mathematics;variance function;law of total variance;algebraic formula for the variance;variance decomposition of forecast errors;sensitivity analysis;one-way analysis of variance;statistics;random effects model	NLP	29.19611143527632	-23.86734352827327	32100
35b933ff6d7beaabba6087d31429981e13d2faac	standard deviation of line objects in geographic information science	probability;standard deviation;line features;geographic information science gisci;positional error	Standard deviation of points is regarded as an effective precision indicator and has been used widely for over 100 years. However, to date, no standard deviation for line objects exists, despite lines being the most fundamental geometric objects in geographic information science. This paper proposes a new theory: the measurement of random line precision using standard deviation. The new theory involves: (1) standard deviation presented graphically, in a band-shape: termed the standard deviation band; (2) the rigorous derivation of analytical equations for the standard deviation band; (3) the probability that a line falls within the standard deviation band. The main contributions of this research include: (1) the derivation of an analytical equation of the standard deviation band; (2) a method to estimate the probability of a line falling within a standard deviation band. These contributions form a foundation for the proposition of further control measures for spatial data quality.	geographic information science	Wenzhong Shi;Yangsheng You;Pan Shao;Yiliang Wan	2014	Annals of GIS	10.1080/19475683.2013.862297	econometrics;root-mean-square deviation;bessel's correction;unbiased estimation of standard deviation;calculus;geometric standard deviation;robust measures of scale;mathematics;tracking error;standardized moment;standard error;standard deviation;coefficient of variation;statistics	Robotics	35.455063584131274	-20.659798795251145	32130
99e3d8269628e4bbacbc9eb3108c640f483cb7c6	application of multi-objective evolutionary algorithm in coordinated design of pss and svc controllers	optimal solution;control optimo;multiobjective programming;sistema multiple;optimum pareto;programmation multiobjectif;optimisation;solution optimale;optimizacion;reseau electrique;securite;electrical network;teoria sistema;red electrica;multi objective optimization;multi objective evolutionary algorithm;multiple system;intelligence artificielle;algoritmo genetico;angular distribution;optimal control;accord frequence;compensador;tuning;systems theory;mathematical programming;power system;commande optimale;theorie systeme;solucion optima;safety;distribucion angular;algorithme genetique;artificial intelligence;algorithme evolutionniste;genetic algorithm;coordinacion;algoritmo evolucionista;sintonizacion frecuencia;optimization;inteligencia artificial;compensateur;evolutionary algorithm;pareto optimal solution;compensator;pareto optimum;power system stability;seguridad;programmation mathematique;static var compensator;optimo pareto;multi objective optimization problem;programacion matematica;coordination;programacion multiobjetivo;systeme multiple;distribution angulaire	A multi-objective evolutionary algorithm (MOEA) based approach to Power System Stabilizer (PSS) and Static Var Compensators (SVC) tuning has been investigated in this paper. The coordinated design problem of PSS and SVC is formulated as a multi-objective optimization problem, in which the system response is optimized by minimizing several system-behavior measure criterions. MOEA is employed to search optimal controller parameters.Design of the multi-objective optimization aims to find out the Pareto optimal solution which is a set of possible optimal solutions for controller parameters. And effectiveness of the proposed control scheme has been demonstrated in a multiple power system.	evolutionary algorithm;physical symbol system	Zhenyu Zou;Quanyuan Jiang;Pengxiang Zhang;Yijia Cao	2005		10.1007/11596448_165	electrical network;mathematical optimization;genetic algorithm;optimal control;computer science;artificial intelligence;multi-objective optimization;evolutionary algorithm;control theory;static var compensator;electric power system;systems theory	EDA	53.04523821619353	-4.801635581731044	32133
5e6721e2494f70d54f50290896449df6a06234b6	a characterization of the law of lotka in terms of sampling	bibliometrie;infometrie;informetrics;echantillonnage;bibliometria;lotka law;definicion;production process;sampling;incomplete information;scale free;loi lotka;sampling technique;definition;ley lotka;ipp information production process;bibliometrics;infometria;muestreo;systematic sampling	In order to model the variable T (the age of citations received by scientific works) with data elaborated by the Institute of Scientific Information, we have used some of the instruments already developed in the survival models to this type of retrospective analyses in the presence of censored data. This analysis is used because, usually, the citations of ages greater than or equal to 10 years appear added together. For a set of journals related to the field of Applied Economics, we have explored which models fit better among those commonly used. Two different approaches to assess the goodness-of-fit for each selected model have been suggested: an analysis through graphical methods and a formal analysis to estimate the parameters of each model by the method of maximum likelihood estimation with data censored to the right.	censoring (statistics);list of graphical methods;sampling (signal processing);scientific literature	Leo Egghe	2005	Scientometrics	10.1007/s11192-005-0024-6	sampling;econometrics;computer science;data mining;statistics	SE	34.6229493726396	-20.193209895451776	32165
16a4fe16b45016d85538d130ca627dd4f0f88b60	a modified particle swarm optimization predicted by velocity	velocity;premature convergence;fitness value;modified particle swarm optimization;convergence rate;particle swarm optimizer;particle swarm optimization;position velocity;computational efficiency;local search;positional	In standard particle swarm optimization (PSO), the velocity only provides a position displacement contrast with the longer computational time. To avoid premature convergence, a new modified PSO is proposed in which the velocity considered as a predictor, while the position considered as a corrector. The algorithm gives some balance between global and local search capability, and results the high computational efficiency. The optimization computing of some examples is made to show the new algorithm has better global search capacity and rapid convergence rate.	algorithm;computation;displacement mapping;kerrison predictor;local search (optimization);mathematical optimization;particle swarm optimization;phase-shift oscillator;premature convergence;rate of convergence;time complexity;velocity (software development)	Zhihua Cui;Jianchao Zeng	2005		10.1145/1068009.1068054	mathematical optimization;multi-swarm optimization;computer science;local search;control theory;mathematics;rate of convergence;velocity;particle swarm optimization;metaheuristic;premature convergence	AI	28.599401012452788	-4.138644604956677	32179
33d83265459329d14423c2af06a2472b0d8755b3	lapped nonlinear interpolative vector quantization and image super-resolution	algorithm training lapped nonlinear interpolative vector quantization image super resolution nlivq image restoration lapped blocks decoding diffraction limited image pairs discrete cosine transform codebook design complexity control simulation results nonlapped algorithm image quality peak signal to noise ratio nonlinearity nonlinear algorithm;codebook design;interpolation;transformation cosinus;design process;lapped blocks;image coding;algorithm performance;restauration image;psnr;image processing;image resolution;data compression;decoding;diffraction;image super resolution;simulacion numerica;vector quantization image restoration algorithm design and analysis decoding diffraction discrete cosine transforms process design process control image quality psnr;lapped nonlinear interpolative vector quantization;procesamiento imagen;image restoration;transform coding;nonlinear algorithm;nonlapped algorithm;qualite image;traitement image;diffraction limited image pairs;discrete cosine transform;experimental result;process design;restauracion imagen;tratamiento numerico;complexity control;cuantificacion vectorial;vector quantization;resultado algoritmo;discrete cosine transforms;peak signal to noise ratio;image quality;simulation numerique;transformacion coseno;process control;performance algorithme;resultado experimental;non linearite;super resolution;no linealidad;nonlinearity;digital processing;calidad imagen;vector quantizer;compresion dato;superresolution;nlivq;discrete cosine transforms interpolation image restoration image resolution vector quantisation decoding image coding transform coding;cosine transform;resultat experimental;vector quantisation;simulation results;algorithm design;superresolucion;algorithm design and analysis;traitement numerique;compression donnee;algorithm training;numerical simulation;quantification vectorielle	This paper presents an improved version of an algorithm designed to perform image restoration via nonlinear interpolative vector quantization (NLIVQ). The improvement results from using lapped blocks during the encoding process. The algorithm is trained on original and diffraction-limited image pairs. The discrete cosine transform is used in the codebook design process to control complexity. Simulation results are presented which demonstrate improvements over the non-lapped algorithm in both observed image quality and peak signal-to-noise ratio. In addition, the nonlinearity of the algorithm is shown to produce super-resolution in the restored images.	algorithm;circuit restoration;codebook;discrete cosine transform;image quality;image restoration;lapped transform;nonlinear system;peak signal-to-noise ratio;simulation;super-resolution imaging;support vector machine;vector quantization	David G. Sheppard;Kannan Panchapakesan;Ali Bilgin;Bobby R. Hunt;Michael W. Marcellin	1997	Conference Record of the Thirty-First Asilomar Conference on Signals, Systems and Computers (Cat. No.97CB36136)	10.1109/83.821746	algorithm design;computer vision;peak signal-to-noise ratio;image processing;computer science;theoretical computer science;discrete cosine transform;process control;mathematics;linde–buzo–gray algorithm;algorithm;superresolution	Vision	46.086444361718954	-13.56797805980096	32254
348d778c367c3be2ffb84273276e926496486712	a novel software toolkit for graph edit distance computation		Graph edit distance is one of the most flexible mechanisms for error-tolerant graph matching. Its key advantage is that edit distance is applicable to unconstrained attributed graphs and can be tailored to a wide variety of applications by means of specific edit cost functions. The computational complexity of graph edit distance, however, is exponential in the number of nodes, which makes it feasible for small graphs only. In recent years the authors of the present paper introduced several powerful approximations for fast suboptimal graph edit distance computation. The contribution of the present work is a self standing software tool integrating these suboptimal graph matching algorithms. It is about being made publicly available. The idea of this software tool is that the powerful and flexible algorithmic framework for graph edit distance computation can easily be adapted to specific problem domains via a versatile graphical user interface. The aim of the present paper is twofold. First, it reviews the implemented approximation methods and second, it thoroughly describes the features and application of the novel graph matching software.		Kaspar Riesen;Sandro Emmenegger;Horst Bunke	2013		10.1007/978-3-642-38221-5_15	computer science;theoretical computer science;machine learning;programming language	ML	26.082763049143765	-15.085180283236735	32380
d056240f64680a7906de07eb90edcdd7b644d4e1	development of a lifetime prediction model for lithium thionyl chloride batteries based on an accelerated degradation test	tecnologia electronica telecomunicaciones;tecnologias;grupo a	Abstract Battery life prediction is critical for lithium/thionyl chloride cells with a long storage life. The objective of this study was to develop models for rapidly estimating the storage life of Li/SOCl2 cells using the semiempirical approach. An accelerated degradation test involving numerous cells stored at various temperatures (room temperature or RT, 40, 50, 60, and 70 °C) was conducted to investigate the effect of the storage time and temperature on capacity degradation. The degradation law can be summarized on the basis of the test data for constructing the semiempirical equation; this law demonstrates that the residual capacity of aging cells exponentially changes with the storage time and temperature. The degradation data are also used for parameterization with the multiple nonlinear curve fitting method based on the universal global optimization algorithm. According to the simulation and comparison between the experimental data and prediction curve, the fitting prediction curve accurately fits the experimental data. This finding indicates that the semiempirical model is useful because of its satisfactory ability to approximate the measured data. In addition, characteristic values of the battery, including the storage life under various storage conditions, average self-discharge rate, and acceleration factor, can be calculated on the basis of the mathematical model.	elegant degradation	Sijie Cheng;Bimei Li;Zhongzhi Yuan;Fuyi Zhang;Jincheng Liu	2016	Microelectronics Reliability	10.1016/j.microrel.2016.07.152	simulation;engineering;forensic engineering	ML	33.92221708577896	-8.67328307600612	32438
625da1f2e4645d8095db478cd01aad7d4ad800ce	optimization of filter designs with dependent and asymmetrically distributed parameters	dependent design variables;copula distribution;filter design;design centering;tolerance design	This paper presents a new statistical design method for maximizing the manufacturing yield of engineering systems for which the realizations of design parameters are assumed to be dependent random variables. Like in many practical situations, the method assumes that the joint distribution of design parameters is unknown and their marginal distributions can be estimated but are not necessarily symmetrical. This is a difficult problem to which little research has been devoted, other than using some brute force search methods. We use a Frank copula to construct the joint distribution of correlated random variables. Kumaraswamy density function is used to approximate their marginal distributions because of its flexibility and simplicity. The proposed method is based on the approximation of the yield integral over the largest rectangular hypercube (n-box) that is contained in the feasible region. It tries to maximize the yield by relocating and rescaling the box so that higher portion of the manufacturing yield is captured by this box. No integration is necessary since the yield of any given design is approximated by evaluating the cumulative distribution of the copula at the endpoints of the associated n-box. Finally, the actual yield of the optimal design is tested using Monte-Carlo simulation. This requires generating correlated random samples from the chosen copula distribution. One tutorial example and two practical design problems demonstrate the applications of the proposed method. Computational results show that the optimal designs significantly increase the manufacturing yield and this observation is verified using Monte-Carlo simulation.	program optimization	Abbas Seifi;Kumaraswamy Ponnambalam;Jirí Vlach	2013	J. Franklin Institute	10.1016/j.jfranklin.2012.11.013	econometrics;mathematical optimization;engineering;electrical engineering;filter design;statistics	EDA	29.908987222031627	-16.58667681531891	32445
2a0867254412e2e9fc4046418a0a4cca7af2936b	extending memory capacity of neural associative memory based on recursive synaptic bit reuse		Neural associative memory (AM) is one of the critical building blocks for cognitive workloads such as classification and recognition. It learns and retrieves memories as humans brain does, i.e., changing the strengths of plastic synapses (weights) based on inputs and retrieving information by information itself. One of the key challenges in designing AM is to extend memory capacity (i.e., memories that a neural AM can learn) while minimizing power and hardware overhead. However, prior arts show that memory capacity scales slowly, often logarithmically or in squire root with the total bits of synaptic weights. This makes it prohibitive in hardware and power to achieve large memory capacity for practical applications. In this paper, we propose a synaptic model called recursive synaptic bit reuse, which enables near-linear scaling of memory capacity with total synaptic bits. Also, our model can handle input data that are correlated, more robustly than the conventional model. We experiment our proposed model in Hopfield Neural Networks (HNN) which contains the total synaptic bits of 5kB to 327kB and find that our model can increase the memory capacity as large as 30X over conventional models. We also study hardware cost via VLSI implementation of HNNs in a 65nm CMOS, confirming that our proposed model can achieve up to 10X area savings at the same capacity over conventional synaptic model.	block code;cmos;content-addressable memory;hopfield network;overhead (computing);recursion;statistical classification;synapse;synaptic weight;very-large-scale integration	Tianchan Guan;Xiaoyang Zeng;Mingoo Seok	2017	Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017		interleaved memory;electronic engineering;semiconductor device modeling;computer science;artificial intelligence;theoretical computer science;machine learning;bidirectional associative memory;flat memory model;registered memory;computational model;correlation;cache-only memory architecture;memory map;memory management	EDA	38.07387227459764	-1.9528193754349596	32563
8d3c85a4ac988b023e4e351ca0462a9dffd852ae	finite ridgelet packet based listless block-partitioning image coding algorithm with rate-distortion optimization	embedded block coding;image coding;hierarchical tree listless block partitioning image coding rate distortion optimization orthonormal finite ridgelet packet transform listless block partitioning coding ridgelet transform set partitioning;listlessness;transforms image coding;orthonormal finite ridgelet packets transform ofrpt;image coding transforms memory management transform coding partitioning algorithms psnr algorithm design and analysis;embedded block coding orthonormal finite ridgelet packets transform ofrpt image coding listlessness	In this paper, we propose a orthonormal finite ridgelet packet transform. An image coding algorithm based on a rate-distortion optimized orthonormal finite ridgelet packet transform (OFRPT) decomposition and on an improved listless block-partitioning coding scheme which quantizes each sub band separately is proposed. The ridge let transform can provide optimally sparse representation of objects with singularities along straight edges and the orthonormal finite ridge let packet can decompose the high frequency parts of the images. Instead of lists, a state table with four bits per coefficient keep s track of the significance of the set and pixel. Each sub band is encoded by a quad tree based set partitioning process. This algorithm needs no lists and thus can avoid unfixed memory requirement and the operations of list nodes. The experimental results show that the proposed algorithm runs faster than SPIHT and JPEG2000 and set partitioning in hierarchical trees. The proposed algorithm outperforms SPIHT and JPEG2000 schemes in novel image with straight lines significantly or curve lines significantly coding in terms of both PSNR and visual quality, it has a fixed predetermined memory requirement of about 51% of the image size.	algorithm;coefficient;computational complexity theory;distortion;image resolution;jpeg 2000;network packet;peak signal-to-noise ratio;pixel;quadtree;rate–distortion optimization;set partitioning in hierarchical trees;sparse approximation;sparse matrix	Zhenghua Shu;Guodong Liu;Zhihua Xie;Zhong Ren;Lixin Gan	2013	2013 Ninth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2013.145	combinatorics;discrete mathematics;shannon–fano coding;theoretical computer science;mathematics	Vision	44.58092389827218	-15.886485120192559	32566
bf2ba8d584c594f5b0798dda7b39b90128501ffc	recognition of the upper structure using the rcs characteristic of the automotive radar		Recently, Automotive radar sensors achieve a greatly increasingly market influence and essentially used for providing driving environment information required by driver assistance systems. Although they are good at measuring the distance and the radial velocity of other objects even under bad weather circumstance, they have difficulty in distinguishing clearly upper structure and stationary vehicle for ambiguity in elevation. As a result, upper structure(e. g overpass, road signal, bridge so on) crossing lane of the ego vehicle may look similar to stationary vehicle. For this reason, AEB(Autonomous Emergency Braking) system fails to operate properly. As closer to the upper structure and the stationary vehicle, the pattern of RCS(Radar Cross Section) measured from the object is different. This difference of RCS pattern can be used to provide features to distinguish the upper structure and the stationary vehicle. There may exists certain features that can help classify them base only on RCS. Feature calculated from real FMCW(Frequency Modulated Continuos Wave) radar sensor data show potential availability in real traffic condition.	modulation;radar;radial (radio);revision control system;sensor;stationary process;velocity (software development)	Junsu Oh;Heemang Song;Hyun-Chool Shin	2018	2018 International Conference on Information Networking (ICOIN)	10.1109/ICOIN.2018.8343264	advanced driver assistance systems;elevation;radar engineering details;ambiguity;radar;computer science;radial velocity;distributed computing;radar cross-section;lidar;electronic engineering	Robotics	48.773831170496635	1.46348687238638	32591
2b3ec240c3f86ed52362e43dc8c1dbc221146983	octagonal-axis raster pattern for improved test zone search motion estimation		Test Zone Search (TZS) is considered the current state-of-the-art fast Motion Estimation algorithm because it presents the best tradeoff between compression efficiency and complexity in comparison to the Full Search strategy. However, it is still one of the most computationally-demanding tools of current video coding standards, such as the High Efficiency Video Coding (HEVC). This paper presents an analysis on the search area opportunities and best match distributions in TZS, which led to the proposal of a novel search pattern in its most complex step, the Raster Search (RS). The new pattern, named Octagonal-Axis Raster Pattern (OARP), allowed an average complexity reduction of 61 % in TZS, with a negligible BD-rate increase of 0.0371 % in comparison to the original algorithm.	algorithm;blu-ray;data compression;high efficiency video coding;motion estimation;optic axis of a crystal;raster graphics;reduction (complexity);video coding format	Paulo Goncalves;Marcelo Schiavon Porto;Bruno Zatt;Luciano Volcan Agostini;Guilherme Corrêa	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462580	reduction (complexity);artificial intelligence;pattern matching;motion estimation;raster scan;pattern recognition;computer science;raster graphics	Vision	47.664902079079425	-19.795939244646036	32645
47bf8eaa05aa7f91342534ef91b3c37ed1bce66d	the isi distribution of the stochastic hodgkin-huxley neuron	health research;uk clinical guidelines;biological patents;isi histogram;europe pubmed central;citation search;stochastic differential equation;gillespie algorithm;uk phd theses thesis;life sciences;isi distribution;kurtz approximation;stochastic dynamics;uk research reports;medical journals;hodgkin huxley;europe pmc;biomedical research;bioinformatics	The simulation of ion-channel noise has an important role in computational neuroscience. In recent years several approximate methods of carrying out this simulation have been published, based on stochastic differential equations, and all giving slightly different results. The obvious, and essential, question is: which method is the most accurate and which is most computationally efficient? Here we make a contribution to the answer. We compare interspike interval histograms from simulated data using four different approximate stochastic differential equation (SDE) models of the stochastic Hodgkin-Huxley neuron, as well as the exact Markov chain model simulated by the Gillespie algorithm. One of the recent SDE models is the same as the Kurtz approximation first published in 1978. All the models considered give similar ISI histograms over a wide range of deterministic and stochastic input. Three features of these histograms are an initial peak, followed by one or more bumps, and then an exponential tail. We explore how these features depend on deterministic input and on level of channel noise, and explain the results using the stochastic dynamics of the model. We conclude with a rough ranking of the four SDE models with respect to the similarity of their ISI histograms to the histogram of the exact Markov chain model.	algorithmic efficiency;approximation algorithm;bbc micro;bistability;computation;computational neuroscience;differential diagnosis;fortran;gillespie algorithm;hl7publishingsubsection <operations>;histogram;hodgkin disease;hodgkin–huxley model;huxley: the dystopia;information sciences institute;interval arithmetic;iontophoresis;list of intel core i5 microprocessors;markov chain;neuron;noise (electronics);oscillator device component;programming languages;python;random number generation;scientific publication;simulation;stochastic process;tail call;v-optimal histograms;exponential;voltage	Peter F. Rowat;Priscilla E. Greenwood	2014		10.3389/fncom.2014.00111	econometrics;stochastic differential equation;computer science;bioinformatics;artificial intelligence;machine learning;mathematics;gillespie algorithm;statistics;hodgkin–huxley model	ML	42.088956954098705	2.4802616758132694	32647
5fd95f1f8486f4e27564e5e94f1084f54877c750	loop filtering and post-filtering for low-bit-rates moving picture coding	artefacto;ringing noise;evaluation performance;restauration image;performance evaluation;data compression;adaptive filtering;quantization noise;filtrado adaptable;evaluacion prestacion;simulation;simulacion;image restoration;post filtering;artefact;reduccion ruido;restauracion imagen;codificacion;computational complexity;noise reduction;peak signal to noise ratio;image quality;coding;reduction bruit;loop filtering;filtrage adaptatif;compresion dato;quantization effect;adaptive filter;blocking artifact;corner outlier;bruit quantification;compression donnee;codage;ruido cuantificacion	When an image is highly compressed by using the current coding standards, the decompressed image has noticeable image degradations such as blocking artifacts near the block boundaries, corner outliers at cross points of blocks and ringing noise near image edges. These image degradations are caused by quantization process of the 8×8 DCT coefficients. In order to restore the decompressed image, a loop-filtering algorithm and a post-filtering algorithm have been developed. The developed methods perform an adaptive filtering on the decompressed image according to blocking and ringing flags that are defined to reduce computation complexity. Performances of both algorithms are compared with respect to the image quality and the computation complexity. The comparison results show that the post-filtering is slightly better than or similar to the loop filtering with respect to peak signal-to-noise ratio (PSNR), whereas the subjective image qualities of both methods are quite similar. However, the computation complexity of the loop filtering is much less than that of the post-filtering.	picture-in-picture	Yung Lyul Lee;Hyun Wook Park	2001	Sig. Proc.: Image Comm.	10.1016/S0923-5965(00)00048-5	adaptive filter;image restoration;computer vision;feature detection;speech recognition;computer science;mathematics	HCI	45.813251103391636	-13.994858991785156	32686
2c11a8c89f3193e44e45d242c58b41975723d1f3	the roles of diversity preservation and mutation in preventing population collapse in multiobjective genetic programming	genetic program;population collapse;genotypic diversity;genetic programming;machine learning;bloat control;multiobjective optimization;diversity preservation	It has been observed previously that genetic programming populations can collapse to all single node trees when a parsimony measure (tree node count) is used in a multiobjective setting. We have investigated the circumstances under which this can occur for both the 6-parity boolean learning task and a range of benchmark machine learning problems. We conclude that mutation is an important -- and we believe a hitherto unrecognized -- factor in preventing population collapse in multiobjective genetic programming; without mutation we routinely observe population collapse. From systematic variation of the mutation operator, we conclude that a necessary condition to avoid collapse is that mutation produces, on average, an increase in tree sizes (bloating) at each generation which is then counterbalanced by the parsimony pressure applied during selection. Finally, we conclude that the use of a genotype diversity preserving mechanism is ineffective at preventing population collapse.	benchmark (computing);genetic programming;machine learning;maximum parsimony (phylogenetics);occam's razor;population	Khaled M. S. Badran;Peter Rockett	2007		10.1145/1276958.1277272	genetic programming;mathematical optimization;mutation;computer science;bioinformatics;multi-objective optimization;machine learning;algorithm	HCI	25.23954810773066	-9.130321399904707	32766
49b446e90ecec53d322b7dc58aa2e1e7a197a7b2	a color digital watermarking in nonsampled contourlet domain using generic algorithm	discrete wavelet transforms;watermarking;generic algorithm;copyright;nsct;image colour analysis;image watermarking copyright discrete wavelet transforms genetic algorithms image colour analysis;watermarking filter banks color transforms genetic algorithms robustness;genetic algorithms;color digital watermarking discrete wavelet transformation ga cycle embedding factor hvs masking technique nsct nonsubsampled contourlet transform color images copyright protection genetic algorithm optimization principles nonsampled contourlet domain;image watermarking;generic algorithm watermarking nsct	A novel watermarking approach for copyright protection of color images based on the nonsubsampled contour let transform (NSCT). We employ the genetic algorithm optimization principles and HVS masking technique to obtain high robustness and transparency with respect to the existing algorithms. In our scheme, the embedding factor is controlled locally according to the result of Generic Algorithm (GA) cycle. The simulation results show that the proposed algorithm yields a watermark which is invisible to human eyes and robust to a wide variety of common attacks.	coefficient;contourlet;digital watermarking;dijkstra's algorithm;directive (programming);elegant degradation;generic programming;genetic algorithm;grayscale;human visual system model;image quality;logo;mathematical optimization;robustness (computer science);scheme;simulation;software release life cycle;unsharp masking	Peng Luo;Ping Wei;Yiqun Liu	2013	2013 5th International Conference on Intelligent Networking and Collaborative Systems	10.1109/INCoS.2013.128	computer vision;genetic algorithm;computer science;artificial intelligence;theoretical computer science;machine learning;computer graphics (images)	EDA	41.39782088762186	-10.52967828920331	32897
5418caaf19686e6921f8073b63bc57a0b40d24f8	erratum to: global optimization of stochastic black-box systems via sequential kriging meta-models	global optimization;stochastic black-box system;sequential kriging meta-models		black box;global optimization;kriging;mathematical optimization	Delu Huang;T. T. Allen;W. I. Notz;N. Zheng	2012	J. Global Optimization	10.1007/s10898-011-9821-z	econometrics;mathematical optimization;stochastic optimization;statistics	EDA	31.49055221303875	-14.903212067434069	32927
435f2bf529ba9432a9c7c9d7775397bd257d1be4	on sequential confidence estimation of parameters of stochastic dynamical systems with conditionally gaussian noises		We consider the problem of non-asymptotical confidence estimation of linear parameters in multidimensional dynamical systems defined by general regression models with discrete time and conditionally Gaussian noises under the assumption that the number of unknown parameters does not exceed the dimension of the observed process. We develop a non-asymptotical sequential procedure for constructing a confidence region for the vector of unknown parameters with a given diameter and given confidence coefficient that uses a special rule for stopping the observations. A key role in the procedure is played by a novel property established for sequential least squares point estimates earlier proposed by the authors. With a numerical modeling example of a two-dimensional first order autoregression process with random parameters, we illustrate the possibilities for applying confidence estimates to construct adaptive predictions.	dynamical system;gaussian process	Sergey E. Vorobeichikov;Victor V. Konev	2017	Automation and Remote Control	10.1134/S0005117917100058	confidence region;dynamical systems theory;mathematical optimization;mathematics;regression analysis;econometrics;discrete time and continuous time;point estimation;statistics;autoregressive model;confidence interval;least squares	Robotics	33.349468150599726	-19.104289101538182	33001
515112dcfce5bcb8412adc56254f2751788027bd	nonlinear programming theory and algorithms	nonlinear programming	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	algorithm;francis;nonlinear programming;primary source	Katta G. Murty	2007	Technometrics	10.1198/tech.2007.s458	fractional programming;mathematical optimization;constraint programming;criss-cross algorithm;reactive programming;nonlinear programming;sequential quadratic programming;inductive programming	Robotics	49.52486086580567	-2.981509964699772	33035
aabb4f7c9d31c3375260d826b698c653d78c6155	optimal multistage vector quantization of lpc parameters over noisy channels	line spectral frequency;codage parole;canal con ruido;speech transmission;perceptual quality;transmission parole;channel coding;evaluation performance;optimisation;methode section divisee;communication system;sequential design;performance evaluation;optimizacion;evaluacion prestacion;noisy channel;vq channel optimized multistage vector quantization channel matched multistage vector quantization suboptimal sequential design procedure linear predictive coding stage codebooks lpc parameters noisy channels channel coding codebook sizes vector dimensions communication channels line spectral frequencies reconstructed speech spectral distortions speech signals speech quality speech coding codecs cm msvq co msvq;speech coding;vector quantization speech coding linear predictive coding speech synthesis image reconstruction acoustic noise distortion measurement speech codecs signal synthesis speech analysis;canal avec bruit;indexing terms;codage canal;algorithme;algorithm;linear prediction coding;linear predictive coding;cuantificacion vectorial;vector quantization;speech codecs;vector quantisation speech coding linear predictive coding channel coding speech codecs;optimization;vector quantizer;transmision palabra;communication channels;vector quantisation;codage predictif lineaire;algoritmo;multistage method;quantification vectorielle	The direct use of vector quantization (VQ) to encode LPC parameters in a communication system suffers from the following two limitations: 1) complexity of implementation for large vector dimensions and codebook sizes and 2) sensitivity to errors in the received indices due to noise in the communication channel. In the past, these issues have been simultaneously addressed by designing channel matched multistage vector quantizers (CM-MSVQ). A sub-optimal sequential design procedure has been used to train the codebooks of the CM-MSVQ. In this paper, a novel channel-optimized multistage vector quantization (CO-MSVQ) codec is presented, in which the stage codebooks are jointly designed. The proposed codec uses a source and channel-dependent distortion measure to encode line spectral frequencies derived from segments of a speech signal. Extensive simulation results are provided to demonstrate the consistent reduction in both the mean and the variance of the spectral distortion obtained using the proposed codec relative to the conventional sequentially designed CM-MSVQ. Furthermore, the perceptual quality of the reconstructed speech using the proposed codec was found to be better than that obtained using the sequentially designed CM-MSVQ.	algorithm;channel (communications);codebook;codec;distortion;encode;emoticon;euclidean distance;gene co-expression network;gradient;line spectral pairs;multistage amplifier;multistage interconnection networks;quantization (signal processing);requirement;rewriting;secure digital;simulation;vector quantization	Venkatesh Krishnan;David V. Anderson;Kwan K. Truong	2004	IEEE Transactions on Speech and Audio Processing	10.1109/TSA.2003.819945	linear predictive coding;speech recognition;index term;channel code;telecommunications;computer science;sequential analysis;speech coding;vector quantization;communications system;statistics;channel	Vision	48.68289479989911	-9.771838550180872	33119
40de23fcccb0a41e89b95b5b68ee72aa57ebe960	improved scheme for selection of potentially optimal hyper-rectangles in direct		We consider a box-constrained global optimization problem with a Lipschitz-continuous objective function and an unknown Lipschitz constant. The well known derivative-free global-search DIRECT (DIvide a hyper-RECTangle) algorithm performs well solving such problems. However, the efficiency of the DIRECT algorithmdeteriorates on problemswithmany local optima andwhen the solutionwith high accuracy is required. To overcome these difficulties different regimes of global and local search are introduced or the algorithm is combinedwith local optimization. In this paper we investigate a different direction of improvement of the DIRECT algorithm and propose a new strategy for the selection of potentially optimal rectangles, what does not require any additional parameters or local search subroutines. An extensive experimental investigation reveals the effectiveness of the proposed enhancements.		Linas Stripinis;Remigijus Paulavicius;Julius Zilinskas	2018	Optimization Letters	10.1007/s11590-017-1228-4	local optimum;subroutine;derivative-free optimization;global optimization;mathematical optimization;local search (optimization);mathematics;lipschitz continuity	AI	27.904070715153335	-0.9295554469611181	33218
493f0548d15bec09a74ab2247d94577b5bf652be	gromacs: fast, flexible, and free	molecular dynamics simulations;biophysics;molecular simulation;gromacs;ompf porin;x ray crystallography;boundary conditions;biomolecular simulation;molecular dynamics;particle mesh ewald;molecular dynamic simulation;parallel computation;public domain;theoretical chemistry;free energy differences;teoretisk kemi;biofysik;numerical integration;force field;parallel computer;general public license;liquid water;molecular dynamic;non u s gov t software stochastic processes thermodynamics;source code;chemistry multidisciplinary;shell model;molecular simulation software;free energy;free energy computation;chemical molecular structure protein folding research support;photoactive yellow protein;algorithms cell membrane chemistry computer simulation trends membrane proteins chemistry models;nonequilibrium dynamics	This article describes the software suite GROMACS (Groningen MAchine for Chemical Simulation) that was developed at the University of Groningen, The Netherlands, in the early 1990s. The software, written in ANSI C, originates from a parallel hardware project, and is well suited for parallelization on processor clusters. By careful optimization of neighbor searching and of inner loop performance, GROMACS is a very fast program for molecular dynamics simulation. It does not have a force field of its own, but is compatible with GROMOS, OPLS, AMBER, and ENCAD force fields. In addition, it can handle polarizable shell models and flexible constraints. The program is versatile, as force routines can be added by the user, tabulated functions can be specified, and analyses can be easily customized. Nonequilibrium dynamics and free energy determinations are incorporated. Interfaces with popular quantum-chemical packages (MOPAC, GAMES-UK, GAUSSIAN) are provided to perform mixed MM/QM simulations. The package includes about 100 utility and analysis programs. GROMACS is in the public domain and distributed (with source code and documentation) under the GNU General Public License. It is maintained by a group of developers from the Universities of Groningen, Uppsala, and Stockholm, and the Max Planck Institute for Polymer Research in Mainz. Its Web site is http://www.gromacs.org.	ansi c;adjudication;assisted model building with energy refinement (amber);cardiac catheterization suite;customize;documentation;force field (chemistry);gnu;gromacs;gromos;gaussian (software);inner loop;mopac;mathematical optimization;molecular dynamics;opls;parallel computing;polymer;simulation;software suite;source code;free energy	David van der Spoel;Erik Lindahl;Berk Hess;Gerrit Groenhof;Alan E. Mark;Herman J. C. Berendsen	2005	Journal of computational chemistry	10.1002/jcc.20291	computational science;molecular dynamics;public domain;chemistry;boundary value problem;numerical integration;force field;computational chemistry;physics;quantum mechanics;source code	HPC	41.552080422015536	2.5017226013525184	33268
cef1980f61114145ed1e82fbd5d5f359bdea5c58	an enhanced predictive location tracking scheme with deficient signal sources for wireless networks	kalman filtering techniques;wireless networks;location tracking;geometric dilution of precision signal sources wireless networks kalman filtering techniques mobile devices trajectory tracking enhanced predictive location tracking;mobile device;time measurement;location estimation;wireless network;kalman filters;prediction algorithms;kalman filter;noise measurement;estimation;wireless networks kalman filters filtering trajectory algorithm design and analysis target tracking robustness navigation medical services wireless sensor networks;enhanced predictive location tracking;proceedings paper;position measurement;mobile handsets;mobile handsets kalman filters;signal sources;trajectory tracking;algorithm design;geometric dilution of precision;mobile devices;noise	Location estimation and tracking for the mobile devices have attracted a significant amount of attention in recent years. The location estimators associated with the Kalman filtering techniques are exploited to both acquire location estimation and trajectory tracking for the mobile devices. However, most of the existing schemes become inapplicable for location tracking due to the deficiency of signal sources. In this paper, the enhanced predictive location tracking (EPLT) are proposed to alleviate this problem. The EPLT scheme utilizes the predictive information obtained from the Kalman filter in order to provide the additional signal inputs for the location estimator. Furthermore, the EPLT scheme incorporates the geometric dilution of precision (GDOP) information into the algorithm design. Persistent accuracy for location tracking can be achieved by adopting the proposed EPLT scheme, especially with inadequate signal sources. Numerical results demonstrate that the EPLT algorithm can achieve better precision in comparison with other location tracking schemes.	algorithm design;angular defect;dilution of precision (computer graphics);dilution of precision (navigation);dual total correlation;kalman filter;mobile device;numerical method	Po-Hsuan Tseng;Kai-Ten Feng	2009	2009 IEEE 20th International Symposium on Personal, Indoor and Mobile Radio Communications	10.1109/PIMRC.2009.5449917	kalman filter;computer science;operating system;wireless network;control theory;mobile device;statistics	Mobile	50.292168026115164	1.3085130953353525	33279
8138c3f6fd948f4049e545507c680f2184233990	reliable minimax parameter estimation	parametric model;minimax problem;constraint propagation;problema minimax;exponential model;metodo minimax;minimax method;non linear model;modele non lineaire;probleme minimax;identificacion sistema;resolucion problema;modelo no lineal;system identification;methode minimax;estimacion parametro;aritmetica intervalo;parameter estimation;estimation parametre;interval arithmetic;arithmetique intervalle;local search;identification systeme;recherche locale;problem solving;resolution probleme	This paper deals with the minimax parameter estimation of nonlinear parametric models from experimental data. Taking advantage of the special structure of the minimax problem, a new e¢cient and reliable algorithm based on interval constraint propagation is proposed. As an illustration, the ill-conditioned problem of estimating the parameters of a two-exponential model is considered.	adobe flash builder;algorithm;c++;c++builder;cartesian closed category;condition number;data point;eisenstein's criterion;estimation theory;global optimization;interval arithmetic;library (computing);linear model;linear system;local consistency;matlab;mathematical optimization;minimax;nonlinear system;software propagation;system of linear equations;time complexity	Luc Jaulin	2001	Reliable Computing	10.1023/A:1011451021517	minimax estimator;mathematical optimization;parametric model;system identification;local search;calculus;mathematics;interval arithmetic;estimation theory;minimax approximation algorithm;local consistency;statistics	Vision	33.61810605822277	0.5336724314256253	33283
4ee22fc404e74a4c1730af2eb71ac42dc26afc68	improved cuckoo optimization algorithm for solving systems of nonlinear equations	cuckoo optimization algorithm;evolutionary algorithms;nonlinear equations;optimization	Systems of nonlinear equations come into different range of sciences such as chemistry, economics, medicine, robotics, engineering, and mechanics. There are different methods for solving systems of nonlinear equations such as Newton type methods, imperialist competitive algorithm, particle swarm algorithm, conjugate direction method that each has their own advantages and weaknesses such as low convergence speed and poor quality of solutions. This paper improves cuckoo optimization algorithm for solving systems of nonlinear equations by changing the policy of egg laying radius, and some well-known problems are presented to demonstrate the efficiency and better performance of this new robust optimization algorithm. From obtained results, our approach found more accurate solutions with the lowest number of function evaluations.	convex function;imperialist competitive algorithm;mathematical optimization;newton;nonlinear system;particle swarm optimization;powell's method;robotics;robust optimization	Mahdi Abdollahi;Asgarali Bouyer;Davoud Abdollahi	2016	The Journal of Supercomputing	10.1007/s11227-016-1660-8	mathematical optimization;meta-optimization;nonlinear system;computer science;derivative-free optimization;artificial intelligence;evolutionary algorithm;imperialist competitive algorithm;algorithm	Robotics	28.607334829921687	-3.9733277905993716	33394
0805e0288439b8121ea2cf9fcfd51d5aeeba8c8c	an efficient fast mode decision method for inter prediction in hevc	complexity theory;encoding video coding prediction algorithms distortion correlation algorithm design and analysis complexity theory;prediction algorithms;motion estimation me coding unit cu fast mode decision high efficiency video coding hevc inter prediction;video coding;distortion;motion estimation operations efficient fast mode decision method inter prediction high efficiency video coding hevc flexible combinations bit rate reduction video quality video coding standard h 264 advanced video coding encoding complexity coding units cu computational complexity quad tree structure encoder skip checking;correlation;video coding motion estimation;encoding;algorithm design and analysis	The emerging High Efficiency Video Coding (HEVC) standard adopts many advanced techniques with flexible combinations, which enables HEVC to achieve about 50% bit-rate reduction for similar perceptual video quality relative to the prior video coding standard H.264/Advanced Video Coding. However, the enormously increased encoding complexity of HEVC inevitably becomes one of the greatest challenges for real-time applications. Among all the factors resulting in the increase in encoding complexity of HEVC, the quad-tree structure for coding units (CUs) with different sizes and accordingly a large number of prediction modes is one critical reason. Thus, it is greatly desired to develop a fast mode decision method for HEVC to reduce the computational complexity. In this paper, considering that HEVC employs the quad-tree structure, and the distortion of each sub-CU can indicate whether the current mode is suitable for current CU, we explore the relationship between the impossible modes and the distribution of the distortions to help the encoder skip checking the unnecessary modes. Besides, since the residual values can reflect the prediction result directly, we propose a method to skip some motion estimation operations according to the distribution of the residuals. Experimental results show that the proposed method can save about 77% of encoding time with only about a 4.1% bit-rate increase compared with HM16.4 anchor, while compared with the fast mode decision method adopted in HM16.4, the proposed algorithm can save about 48% of encoding time with only about a 2.9% bit-rate increase.	algorithm;blu-ray;computational complexity theory;data compression;distortion;encoder;experiment;fast fourier transform;h.264/mpeg-4 avc;high efficiency video coding;motion estimation;real-time transcription;selection algorithm;tip (unix utility);tree structure;video coding format	Jinlei Zhang;Bin Li;Houqiang Li	2016	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2015.2461991	algorithm design;computer vision;real-time computing;distortion;prediction;telecommunications;quarter-pixel motion;computer science;theoretical computer science;coding tree unit;mathematics;context-adaptive binary arithmetic coding;motion compensation;h.261;correlation;encoding;statistics;multiview video coding	Visualization	46.86254161052904	-19.048143668512235	33480
95ae2a6ec87bcf9cdcb9dad413fb62d99023fc3d	comparing natural evolution strategies to bipop-cma-es on noiseless and noisy black-box optimization testbeds	benchmarking;natural gradient;adaptive learning rate;evolution strategies;adaptive sampling;evolution strategy;optimal algorithm	Natural Evolution Strategies (NES) are a recent member of the class of preal-valued optimization algorithms that are based on adapting search distributions. Exponential NES (xNES) are the most common instantiation of NES, and particularly appropriate for the BBOB 2012 benchmarks, given that many are non-separable, and their relatively small problem dimensions. Here, we augment xNES with adaptation sampling, which adapts learning rates online, and compare the resulting performance directly to the BIPOP-CMA-ES algorithm, the winner of the 2009 black-box optimization benchmarking competition (BBOB). This report provides an extensive empirical comparison, both on the noise-free and noisy BBOB testbeds.	algorithm;black box;cma-es;colors of noise;mathematical optimization;natural evolution strategy;sampling (signal processing);universal instantiation	Tom Schaul	2012		10.1145/2330784.2330819	mathematical optimization;simulation;computer science;machine learning	ML	25.485688936098562	-14.395600392568078	33503
9fcad80af0e66dd59bca59aefe33ac4fc379800c	audio codingwith power spectral density preserving quantization	audio visual systems;quantization;lattices;backward adaptation;forward adaptation;backward adaptation quantization audio coding predictive coding forward adaptation;speech;other electrical engineering electronic engineering information engineering;quantisation signal;audio coding;psd preserving quantization power spectral density preserving quantization audio visual signal coding distribution preserving quantization paradigm dpq paradigm probability distribution backward adaptive psd estimation forward adaptive psd estimation;estimation;annan elektroteknik och elektronik;quantisation signal adaptive estimation audio coding audio visual systems;adaptation models;predictive coding;quantization audio coding estimation adaptation models lattices speech;adaptive estimation	The coding of audio-visual signals is generally based on different paradigms for high and low rates. At high rates the signal is approximated directly and at low rates only signal features are transmitted. The recently introduced distribution preserving quantization (DPQ) paradigm provides a seamless transition between these two regimes. In this paper we present a simplified scheme that preserves the power spectral density (PSD) rather than the probability distribution. In a practical system the PSD must be estimated. We show that both forward adaptive and backward adaptive PSD estimation are possible. Our experimental results confirm that preservation of PSD at finite precision leads to a unified coding paradigm that provides effective coding at both high and low rates. An audio coding application shows the perceptual benefits of PSD preserving quantization.	approximation algorithm;format-preserving encryption;programming paradigm;seamless3d;spectral density	Minyue Li;Janusz Klejsa;Alexey Ozerov;W. Bastiaan Kleijn	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6287904	estimation;speech recognition;quantization;computer science;speech;machine learning;lattice;mathematics;statistics	Vision	48.14440574052418	-8.973108595223529	33536
89ed7245462004405a296f259d5b2061f58f1b01	a note on teaching-learning-based optimization algorithm	unconstrained optimization problems;teaching learning based optimization;constrained optimization problems;experimental replication	Teaching-Learning-Based Optimization (TLBO) seems to be a rising star from amongst a number of metaheuristics with relatively competitive performances. It is reported that it outperforms some of the well-known metaheuristics regarding constrained benchmark functions, constrained mechanical design, and continuous non-linear numerical optimization problems. Such a breakthrough has steered us towards investigating the secrets of TLBO's dominance. This paper reports our findings on TLBO qualitatively and quantitatively through code-reviews and experiments, respectively. Our findings have revealed three important mistakes regarding TLBO: (1) at least one unreported but important step; (2) incorrect formulae on a number of fitness function evaluations; and (3) misconceptions about parameter-less control. Additionally, unfair experimental settings/conditions were used to conduct experimental comparisons (e.g., different stopping criteria). The experimental results for constrained and unconstrained benchmark functions under fairly equal conditions failed to validate its performance supremacy. The ultimate goal of this paper is to provide reminders for metaheuristics' researchers and practitioners in order to avoid similar mistakes regarding both the qualitative and quantitative aspects, and to allow fair comparisons of the TLBO algorithm to be made with other metaheuristic algorithms.	algorithm;mathematical optimization	Matej Crepinsek;Shih-Hsi Liu;Luka Mernik	2012	Inf. Sci.	10.1016/j.ins.2012.05.009	mathematical optimization;simulation;artificial intelligence;machine learning;mathematics;algorithm;statistics	DB	31.30858847159943	-0.5532440278513324	33660
6e280cbf8feae475109d4187a1ab0c7c2d71b335	a sas macro for estimating transition probabilities in semiparametric models for recurrent events	transition probability;regression model;counting process;transition probabilities;cox model;clinical study;recurrent events;sas macro;semiparametric model;event history analysis	In many clinical studies involving event history analysis, the event of interest is non-fatal and may occur more than once for each subject. Models based on the theory of counting processes have been developed to deal with such data, the recurrences being considered as transitions in a Markovian process. Under this setting, the experimental units can move between states over time, and it is possible to estimate the corresponding transition probabilities employing regression models that incorporate the influence of covariates. Despite of this, most of the softwares are concerned only in the estimation of regression parameters and do not provide transition probabilities estimates. The aim of this paper is to present a SAS macro developed to estimate the transition probabilities, considering three approaches for the regression modeling. The macro is flexible enough to allow the user to select the model to be fit providing, for a given set of covariates, plots of the estimates for the predicted transition probabilities as a function of time.		Ângela Tavares Paes;Antonio Carlos Pedroso de Lima	2004	Computer methods and programs in biomedicine	10.1016/j.cmpb.2003.08.007	econometrics;markov chain;computer science;data mining;mathematics;survival analysis;proportional hazards model;regression analysis;semiparametric model;statistics	ML	25.883451910270285	-22.766877631896033	33680
1d68854aebf8e7427bcb4541c0ade332a275a1ba	wyner-ziv to h.264 video transcoder	rate distortion;wyner ziv;image coding;complexity theory;decoding;decoding mobile communication video coding encoding automatic voltage control transcoding rate distortion personal digital assistants partitioning algorithms proposals;rate distortion loss video transcoder wyner ziv h 264 mobile to mobile video communications;low complexity;avc;video coding;video transcoding;dvc;h 264;transcoding dvc wyner ziv h 264 avc;transcoding;video coding transcoding;mobile video;partitioning algorithms	This paper proposes an improved Wyner-Ziv to H.264 video transcoder as part of a framework for mobile to mobile video communications. In this scheme, the user devices keep the low complexity constraints by using the Wyner-Ziv encoding and H.264 decoding algorithms. They shift their complexities to the network where the proposed transcoder is allocated. The main goal of the transcoder is to convert the bitstream and reduce the delay efficiently. The results show that the proposed transcoder reduces the complexity by a factor of 95% with a negligible rate-distortion loss.	algorithm;bitstream;distortion;h.264/mpeg-4 avc	José Luis Martínez;Hari Kalva;Gerardo Fernández-Escribano;Warnakulasuriya Anil Chandana Fernando;Pedro Cuenca	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5413747	transcoding;telecommunications;computer science;theoretical computer science;multimedia;statistics	Robotics	46.323187876104434	-18.28356066090117	33703
85f862d513c02fd5c3a6972d2d660d2d18703091	hilbert scanning search algorithm for motion estimation	espace hilbert;estimation mouvement;algoritmo busqueda;espacio hilbert;image processing;data compression;forme onde;complexite calcul;image matching;implementation;algorithme recherche;estimacion movimiento;simulacion numerica;search algorithm;procesamiento imagen;motion estimation;video sequences motion estimation hilbert scanning search algorithm block matching algorithms low bit rate video coding optimal block multiple candidate search local monotonicity binary search search accuracy search speed accuracy;indexing terms;traitement image;hilbert space;ejecucion;video coding;complejidad computacion;forma onda;senal video;signal video;computational complexity;simulation numerique;image sequence;correspondencia bloque;video signal;block matching;binary search;secuencia imagen;search problems;waveform;motion estimation image sequences partitioning algorithms computational complexity convergence video coding heuristic algorithms distortion measurement motion measurement two dimensional displays;correspondance bloc;local minima;image sequences motion estimation video coding data compression image matching search problems;block matching algorithm;sequence image;numerical simulation;image sequences	Block-matching algorithms, such as TSS and DSWA/IS, are widely used for motion estimation in low-bit-rate video coding. The assumption behind these algorithms is that when the matching block moves away from the optimal block, the difference between them increases monotonically. Unfortunately, this assumption is often invalid, and therefore leads to a high possibility for the result to be trapped to local minima. In this research, we proposed a new multiple-candidate search scheme, Hilbert scanning search algorithm (HSSA), in which the assumption of global monotonicity is not necessary and the local monotonicity can be effectively explored with binary search around each candidate. In HSSA, the number of initial candidates and a threshold to control the selection of candidates from one stage to the next can be adjusted to meet the required search accuracy and/or speed. With properly chosen parameters, the HSSA converges to their optimal results faster and with better accuracy than the conventional block-matching algorithms.	binary search algorithm;data compression;maxima and minima;motion estimation	Yankang Wang;Hideo Kuroda	1999	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.780357	data compression;computer vision;mathematical optimization;waveform;index term;image processing;computer science;theoretical computer science;maxima and minima;motion estimation;mathematics;block-matching algorithm;computational complexity theory;implementation;algorithm;binary search algorithm;search algorithm;hilbert space	Vision	49.00590145910737	-19.349256695059815	33880
4f5e27e942fca53496e08d8d892c32a9f1d3a520	fitting a mixture model to three-mode three-way data with missing information	classification automatique statistiques;metodo estadistico;ajustamiento modelo;fonction vraisemblance;analyse multivariable;medio ambiente;multivariate analysis;melange loi probabilite;maximum likelihood;dato que falta;implementation;mixed distribution;statistical method;psychology mathematical;funcion verosimilitud;donnee manquante;ajustement modele;discriminant analysis;analyse discriminante;ejecucion;incomplete data;analisis discriminante;mixture model;62h30;clustering;methode statistique;missing at random;environment;model matching;finite mixture models;mezcla ley probabilidad;analisis multivariable;environnement;3 way data;missing data;cluster analysis statistics;62p12;likelihood function;mathematics interdisciplinary applications;imputation	When the data consist of certain attributes measured on the same set of items in different situations, they would be described as a three-mode three-way array. A mixture likelihood approach can be implemented to cluster the items (i.e., one of the modes) on the basis of both of the other modes simultaneously (i.e,, the attributes measured in different situations). In this paper, it is shown that this approach can be extended to handle three-mode three-way arrays where some of the data values are missing at random in the sense of Little and Rubin (1987). The methodology is illustrated by clustering the genotypes in a three-way soybean data set where various attributes were measured on genotypes grown in several environments.	curve fitting;mixture model	Lynette A. Hunt;Kaye E. Basford	2001	J. Classification	10.1007/s00357-001-0016-z	econometrics;missing data;data mining;mathematics;linear discriminant analysis;statistics	ML	33.70356925257401	-23.1649180711378	33940
4891fd0a8ddef8acc67c62e6cbeae0e3811d4f8b	a normal approximation for the chi-square distribution	ji cuadrado;grado aleatorio;error absoluto;grado libertad;linear combination;cumulative distribution function;approximation normale;transformation puissance;degre aleatoire;computacion informatica;analisis datos;random degree;degree of freedom;62e17;khi deux;chi square;maximum absolute error;data analysis;estimation erreur;chi square distribution;combinacion lineal;error estimation;erreur absolue;ciencias basicas y experimentales;matematicas;statistical computation;estimacion error;calculo estadistico;random variable;power transformer;absolute error;analyse donnee;calcul statistique;grupo a;power transformation;60e05;freedom degree;normal approximation;combinaison lineaire;degre liberte	An accurate normal approximation for the cumulative distribution function of the chi-square distribution with  n  degrees of freedom is proposed. This considers a linear combination of appropriate fractional powers of chi-square. Numerical results show that the maximum absolute error associated with the new transformation is substantially lower than that found for other power transformations of a chi-square random variable for all the degrees of freedom considered (1⩽ n ⩽1000).	approximation	Luisa Canal	2005	Computational Statistics & Data Analysis	10.1016/j.csda.2004.04.001	half-normal distribution;random variable;noncentral chi-squared distribution;approximation error;q-function;folded normal distribution;chi distribution;chi-square test;linear combination;cumulative distribution function;scaled inverse chi-squared distribution;calculus;inverse-chi-squared distribution;inverse transform sampling;log-normal distribution;mathematics;geometry;ratio distribution;degrees of freedom;asymptotic distribution;data analysis;chi-squared distribution;welch–satterthwaite equation;transformer;statistics	Theory	33.588860015668914	-21.219070396717264	33942
e5f16945a4d8fad787d5054f8e24083b2906ff2f	resolution dependence of the maximal information coefficient for noiseless relationship	maximal information coefficient;pearson s correlation coefficient;greedy algorithm	Reshef et al. (Science 334:1518–1523, 2011) introduce the maximal information coefficient, or MIC, which captures a wide range of relationships between pairs of variables. We derive a useful property which can be employed either to substantially reduce the computer time to determine MIC, or to obtain a series of MIC values for different resolutions. Through studying the dependence of the MIC scores on the maximal resolution, employed to partition the data, we show that relationships of different natures can be discerned more clearly. We also provide an iterative greedy algorithm, as an alternative to the ApproxMaxMI proposed by Reshef et al., to determine the value of MIC through iterative optimization, which can be conducted parallelly.	greedy algorithm;iterative method;mathematical optimization;maximal information coefficient;maximal set	Shih-Chang Lee;Ning-Ning Pang;Wen-Jer Tzeng	2014	Statistics and Computing	10.1007/s11222-013-9405-5	econometrics;mathematical optimization;greedy algorithm;mathematics;statistics	AI	37.50190676297529	-20.94026258053603	34199
f753528702ad291fa8fec0e603cd8784ad11d944	fractal image compression based on spatial correlation and hybrid genetic algorithm	iterated function system;pifs;premature convergence;simulated annealing;satisfiability;dyadic mutation operator;spatial correlation;compression ratio;genetic algorithm;neighborhood;block codes;hybrid genetic algorithm;fractal image compression;block coding	1047-3203/$ see front matter 2009 Elsevier Inc. A doi:10.1016/j.jvcir.2009.07.002 * Corresponding author. E-mail address: wangxy@dlut.edu.cn (W. Xing-yua In order to solve the high complexity of the conventional encoding scheme for fractal image compression, a spatial correlation hybrid genetic algorithm based on the characteristics of fractal and partitioned iterated function system (PIFS) is proposed in this paper. There are two stages for the algorithm: (1) Make use of spatial correlation in images for both range and domain pool to exploit local optima. (2) Adopt simulated annealing genetic algorithm (SAGA) to explore the global optima if the local optima are not satisfied. In order to avoid premature convergence, the algorithm adopt dyadic mutation operator to take place of the traditional one. Experiment results show that the algorithm convergent rapidly. At the premise of good quality of the reconstructed image, the algorithm saved the encoding time and obtained high compression ratio. 2009 Elsevier Inc. All rights reserved.	central processing unit;data compression ratio;dyadic transformation;fractal compression;genetic algorithm;image compression;iterated function system;iteration;line code;local optimum;memetic algorithm;parallel algorithm;premature convergence;simulated annealing;software release life cycle	Xing-Yuan Wang;Fanping Li;Shuguo Wang	2009	J. Visual Communication and Image Representation	10.1016/j.jvcir.2009.07.002	block code;mathematical optimization;combinatorics;machine learning;mathematics;fractal compression;algorithm;statistics;population-based incremental learning	AI	44.24659709575413	-13.09994230696577	34211
36b8f969467b6ac07c642b7099e7cf9365a03bc2	combined-chain nested sampling for efficient bayesian model comparison		Abstract Model comparison problems arise in many fields of science and engineering, including signal processing. In these problems, we wish to quantify how well each of a set of possible models describes a set of observations. Many numerical techniques exist to perform model comparison, but this paper focuses on nested sampling, which is a numerical integration algorithm for evaluating probabilities of models. The original formulation of nested sampling is a strictly sequential algorithm. Most modern advances in computing are via parallel processing, however, and we therefore present a novel method for parallelizing nested sampling. This paper sets out the mathematical foundation for this parallelization, as well as ideas for implementing it. Three examples demonstrate the effectiveness of the present parallel technique in realistic scientific and engineering data analysis problems.	bayes factor;bayesian network;model selection;nested sampling algorithm;sampling (signal processing)	R. Wesley Henderson;Paul M. Goggans;Lei Cao	2017	Digital Signal Processing	10.1016/j.dsp.2017.07.021	signal processing;theoretical computer science;nested sampling algorithm;numerical integration;sequential algorithm;parallel processing;markov chain monte carlo;bayesian inference;computer science	ML	32.02124116755717	-16.096125962627433	34231
fe9b08372f712cc5b3ed10193462474850cb17f4	construction of multinomial lattice random walks for optimal hedges	statistical moment;volatility;pricing;moment statistique;mean square;parametrization;parametrizacion;haie optimale;higher order;momento estadistico;random walk;portfolio management;implied volatility;volatilite;gestion cartera;marcha aleatoria;gestion portefeuille;cumulant;volatibilidad;fixation prix;marche aleatoire;parametrisation;cumulante;incomplete market;multinomial lattice	"""In this paper, we provide a parameterization of multinomial lattice random walks which take cumulants into account. In the binomial and trinomial lattice cases, it reduces to standard results. Additionally, we show that higher order cumulants may be taken into account by using multinomial lattices with four or more branches. Finally, we outline two synthesis methods which take advantage of the multinomial lattice formulation. One is mean square optimal hedging in an incomplete market and the other involves pricing under """"implied volatility"""" and """"implied kurtosis""""."""	multinomial logistic regression	Yuji Yamada;James A. Primbs	2001		10.1007/3-540-45545-0_67	parametrization;econometrics;mathematics;statistics;project portfolio management	Theory	34.002714241525425	-19.555729908315527	34236
2f4b49edb7b1bd0d6f9f80aeadc8aa01b3aff50d	image information hiding encryption using chaotic sequence	information hiding;secure communication;wavelet transform;encryption algorithm;chaotic sequence;image watermarking;computer simulation;white noise	In this paper, we propose a new image watermark algorithm of information hiding scheme based on chaotic encryption. Chaotic sequence is regarded as a noise-like spread sequence, and the secret signal to be transmitted is modulated and hidden in the carrier image as white noise to implement secure communication. The encryption principle of new algorithm is firstly presented in this paper, and the merits of new scheme are quantitatively analyzed by defining integrality and imperceptibility of image information. The results of computer simulation are described in the end.	algorithm;computer simulation;encryption;modulation;secure communication;white noise	Zhen Liu;Lifeng Xi	2007		10.1007/978-3-540-74827-4_26	multiple encryption;theoretical computer science;mathematics;internet privacy;computer security;encryption;probabilistic encryption	Crypto	38.94140706292667	-8.869570307297176	34255
2cb12c8192ec744baea0fd9edba9a66062179ccc	optimization of heat integration with variable stream data and non-linear process constraints	big m;simultaneous optimization;heat integration;multi m	Two novel formulations for the optimization of heat integration of chemical processes with variable stream data and non-linear process constraints are proposed. An NLP formulation utilizes a concept of pseudo stream temperatures and the Plus/Minus Principles simplifies the formulation with tight constraints. The NLP model is efficiency but the use of bi-linear constraints might sometimes deteriorate the solution quality comparing to the conventional Big-M disjunctive model. To overcome this, a Multi-M model is proposed that applies the same concepts as in the NLP model together with a set of Multi-M constraints. The Multi-M model significantly reduces the number of binary variables and minimizes the imultaneous optimization ig-M ulti-M size of Ms for the use in the Multi-M constraints. Results show that the NLP model spends least time in solution but sometimes converges too soon at near or local optimum. The Multi-M model is the most robust and still maintains a high efficiency in solution quality and speed. For problem with non-linear process constraints, both NLP and Multi-M models perform much better than the traditional Big-M model. © 2014 Elsevier Ltd. All rights reserved.	approximation;converge;disjunctive normal form;display resolution;fermat's principle;han unification;linear programming relaxation;local optimum;mathematical optimization;natural language processing;nonlinear system;sed;smoothing	Chi-Wai Hui	2014	Computers & Chemical Engineering	10.1016/j.compchemeng.2014.03.010	big m method;mathematical optimization;engineering;artificial intelligence;chemical engineering;process integration	AI	32.639673374536315	3.4665441175157428	34256
05164c127c6b369e5f9f4deb7b9c06ee3c3de90f	multi-agent persistent monitoring in stochastic environments with temporal logic constraints	dynamic programming;least squares approximations;probability;temporal logic;robots approximation methods markov processes silicon games clocks monitoring;least square approximation multiagent persistent monitoring stochastic environments temporal logic constraints control policies robots team probabilistic behaviors optimal surveillance mission transition systems markov chains optimal control policy markov decision process approximate dynamic programming framework;optimal control;multi agent systems;temporal logic decision making dynamic programming least squares approximations markov processes multi agent systems multi robot systems optimal control probability;multi robot systems;markov processes	In this paper, we consider the problem of generating control policies for a team of robots moving in an environment containing elements with probabilistic behaviors. The team is required to achieve an optimal surveillance mission, in which a certain proposition needs to be satisfied infinitely often. The goal is to minimize the average time between satisfying instances of the proposition, while ensuring that the mission is accomplished. By modeling the robots as Transition Systems and the environmental elements as Markov Chains, the problem reduces to finding an optimal control policy satisfying a temporal logic specification on a Markov Decision Process. The existing approaches for this problem are computational intensive and therefore not feasible for a large environment or a large number of robots. To address this issue, we propose an approximate dynamic programming framework. Specifically, we choose a set of basis functions to approximate the optimal cost and find the best parameters for these functions based on the least-square approximation. We develop an approximate policy iteration algorithm to implement our framework. We provide illustrative case studies and evaluate our method through simulations.	approximation algorithm;basis function;computation;dynamic programming;iteration;iterative method;markov chain;markov decision process;optimal control;robot;simulation;temporal logic	Yushan Chen;Kun Deng;Calin Belta	2012	2012 IEEE 51st IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2012.6426280	markov decision process;mathematical optimization;simulation;optimal control;partially observable markov decision process;temporal logic;computer science;machine learning;dynamic programming;multi-agent system;probability;control theory;mathematics;markov process;markov model;statistics	Robotics	52.46513787311912	-21.306018024808875	34284
a60a59789075f874942372b4e0f10d37c1fc348f	spatio-temporal scalability using modified mpeg-2 predictive video coding	encoding;spatial resolution;scalability;transform coding	The paper describes spatio-temporally scalable MPEG video coders proposed. Such an encoder produces two bitstreams: base layer bitstream which represents a video sequence with low spatial and temporal resolution and an enhancement layer bitstream which provides additional data needed for reproduction of pictures with full resolution and full temporal frequency. The bitrate overhead measured relative to the single layer MPEG-2 bitstream varies between 2% and 22% for progressive television test sequences. The base layer bitstream constitutes 34–40% of the overall bitstream. The base layer encoder is fully compatible with the MPEG-2 video coding standard. The enhancement layer encoder is a modified version of that used by MPEG-2 for spatial scalability.	bitstream;data compression;encoder;h.262/mpeg-2 part 2;mpeg-2;moving picture experts group;overhead (computing);scalability;video coding format;video compression picture types	Adam Luczak;Slawomir Mackowiak;Marek Domanski	2000	2000 10th European Signal Processing Conference		scalable video coding;computer vision;telecommunications;computer science;bitstream format;computer graphics (images)	Vision	45.04053015052148	-19.04559635887407	34313
4e7984c80270c2c4f9a9e551bedf11dbf14e429b	improved class-based coding of multispectral images with shape-adaptive wavelet transform	1d discrete cosine transform;set partitioning in hierarchical trees;image coding;image segmentation;data compression;shape adaptive wavelet transform sawt compression multispectral segmentation;image classification;segmentation;transform coding;shape adaptive wavelet transform sawt;discrete cosine transform;wavelet transforms;scalar quantization;wavelet transform;image compression;image coding multispectral imaging wavelet transforms image segmentation discrete wavelet transforms discrete cosine transforms statistics rate distortion quantization image storage;discrete cosine transforms;multispectral images;multispectral;shape adaptive wavelet transform;numerical experiment;compression;wavelet transforms data compression discrete cosine transforms image classification image coding;scalar quantization class based coding multispectral images shape adaptive wavelet transform image compression 1d discrete cosine transform;class based coding	In this letter, we improve the class-based transform-coding scheme proposed by Gelli and Poggi for the compression of multispectral images. The original spatial-coding tools, 1-D discrete cosine transform and scalar quantization, are replaced by shape-adaptive wavelet transform and set partitioning in hierarchical trees. Numerical experiments show that the improved technique outperforms the original one for medium- to high-quality compression and is consistently superior to all reference techniques.	discrete cosine transform;experiment;interaction technique;multispectral image;quantization (signal processing);set partitioning in hierarchical trees;transform coding;wavelet transform	Marco Cagnazzo;Sara Parrilli;Giovanni Poggi;Luisa Verdoliva	2007	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2007.900696	multispectral image;computer vision;discrete mathematics;transform coding;s transform;harmonic wavelet transform;second-generation wavelet transform;continuous wavelet transform;computer science;discrete cosine transform;pattern recognition;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;set partitioning in hierarchical trees;wavelet transform	Visualization	43.53854619664953	-15.01297286512992	34320
26b11ee706ec74afcf791588c98953c675ce5c99	analysis of constant creation techniques on the binomial-3 problem with grammatical evolution	fitness landscape;genetic program;evolutionary computation;persistent random constants;probability;search problems genetic algorithms;search space;genetic programming partial response channels sampling methods probability testing polynomials computer applications adaptive systems educational institutions evolutionary computation;probability density function;digit concatenation method;testing;genetic programming constant creation techniques binomial 3 problem grammatical evolution persistent random constants digit concatenation method combinatorial search space;genetic programming;data mining;polynomials;computer applications;adaptive systems;ieee;grammatical evolution;partial response channels;production;genetic algorithms;search problems;sampling methods;constant creation techniques;binomial 3 problem;combinatorial search space;steady state;reactive power	This paper studies the difference between Persistent Random Constants (PRC) and Digit Concatenation as methods for generating constants. It has been shown that certain problems have different fitness landscapes depending on how they are represented, independent of changes to the combinatorial search space, thus changing problem difficulty. In this case we show that the method for generating the constants can also influence how hard the problem is for Genetic Programming.	combinatorial search;concatenation;fitness function;genetic programming;grammatical evolution	Jonathan Byrne;Michael O'Neill;Erik Hemberg;Anthony Brabazon	2009	2009 IEEE Congress on Evolutionary Computation	10.1109/CEC.2009.4982996	genetic programming;sampling;mathematical optimization;probability density function;genetic algorithm;fitness landscape;computer science;artificial intelligence;machine learning;probability;mathematics;software testing;ac power;computer applications;grammatical evolution;steady state;algorithm;statistics;polynomial;evolutionary computation	Theory	27.75351606986648	-8.381877197892793	34322
ceac169be4ca4b7f8be157a0201418629773845d	novel method of hiding information in ip telephony using pitch approximation	pesq steganography ip telephony speech coding speex pitch;steganographic cost information hiding ip telephony pitch approximation steganographic method f0 frequency speaker voice pitch fine pitch parameter approximation speex codec hidden transmission channels mode 5 speex hide f0 method channel hidding steganographic bandwidth;speech coding;steganography;speech codecs bandwidth linear approximation telephony bit rate;pitch;speex;voice communication internet telephony speech coding steganography;pesq;ip telephony	"""In this paper a novel steganographic method, called Hide F0, dedicated to IP telephony is proposed. It is based on the approximation of the parameter that describes the F0 frequency (the pitch) of the speaker's voice. We show that thanks to approximating some fragments of the """"fine pitch"""" parameter in the Speex codec we can create efficient hidden transmission channels. We determined that for Speex working in mode 5 the Hide F0 method can provide a hidden channel with a capacity of ca. 220 bps at the optimal operating point. We also demonstrated that the proposed method offers a significantly more advantageous trade-off between the steganographic bandwidth and steganographic cost than the classic least significant bit (LSB) approach."""	algorithm;approximation;circa;codec;ibm basic programming support;least significant bit;most significant bit;operating point;pesq;pitch (music);requirement;steganography;libspeex	Artur Janicki	2015	2015 10th International Conference on Availability, Reliability and Security	10.1109/ARES.2015.12	speech recognition;telecommunications;computer science;computer network	EDA	47.94739157475981	-8.214167033874784	34328
3c6a2f1c6fa389b6377c8fb3d3f2e16c76cac516	vssi median control chart with estimated parameters and measurement errors				Xiao-Bin Cheng;Fu-Kwun Wang	2018	Quality and Reliability Eng. Int.	10.1002/qre.2297	statistics;observational error;econometrics;engineering;control chart	OS	28.797239633091653	-20.679605036402357	34343
bda62043398d383eb98ac6e820039aca3aff6003	rapidly replanning a*	path planning;distributed computing;rra;conference paper;heuristic search;navigation;image edge detection;heuristic algorithms;replanning;planning;a;search problems;incremental search	In this paper, Rapidly Replanning A* (RRA*) algorithm is proposed for path planning and replanning in partially unknown environments. RRA* uses an effective mechanism to reuse previous search results, which considerably accelerates its replanning process compared to repetitive replanning from scratch. RRA* guarantees to find an optimal path from the current location of an agent to its target location based on the available information. Simulation results verify the optimality of the path generated by RRA* and the superior efficiency of RRA* in path replanning.	a* search algorithm;motion planning;search tree;simulation	Nuwan Ganganath;Chi-Tsun Cheng;C. K. Michael Tse	2016	2016 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery (CyberC)	10.1109/CyberC.2016.81	planning;mathematical optimization;navigation;simulation;heuristic;computer science;machine learning;motion planning	Robotics	52.916872154823764	-23.78456739567772	34362
2203d7ea5af73480af117601d830efaed02bfe68	high positioned brake reminder system based on can bus	road accidents;can bus;road traffic;data processing;controller area networks;pca82c250 high positioned brake reminder system can bus vehicle rear collision reduction dot matrix display technology sensor data acquisition brake position sensor electric signals ad conversion sja1000;pca82c250;rear collision;vehicle driving;vehicle electronics;high positioned brake reminder system;field buses;chip;brake position sensor;vehicle safety brake awaking can bus rear collision vehicle electronics;sja1000;accidents;braking;position control;registers;signal processing;analogue digital conversion;displays;dot matrix display technology;brake awaking;manufacturing;road accidents displays signal processing road transportation data acquisition data processing vehicle driving manufacturing costs vehicle safety;sensor data acquisition;driver circuits;vehicle safety;vehicles;vehicle rear collision reduction;road transportation;electric signals;data acquisition;vehicle dynamics;microcomputers;ad conversion;vehicle dynamics analogue digital conversion braking controller area networks electric sensing devices field buses position control road traffic;electric sensing devices;hardware	In order to reduce the vehicle rear collisions and the damage degrees on passengers, we adopt the dot matrix display technology and the vehicle high positioned brake reminder technology based on CAN bus. The designed high positioned brake reminder system is composed by the nodes based on CAN bus. The first node is the sensor data acquisition node based on SCM. The brake position sensor in the node converts the acquired position signals into electric signals, and transmits them to the SCM for data processing through AD conversion. The display unit is the second node of the system. The CAN bus control part is mainly composed by two core chips, SJA1000 and PCA82C250. It can not only display the reminder information when braking, but remind rear driver aiming at different situations such as displaying brake abruptness, avoiding or reduce the vehicle rear collisions. The manufacturing cost of the system is cheap.	can bus;data acquisition;display device;dot matrix;dot-matrix display;scm	Xiao Chen	2009	2009 Pacific-Asia Conference on Circuits, Communications and Systems	10.1109/PACCS.2009.40	chip;embedded system;vehicle dynamics;data processing;can bus;computer science;engineering;operating system;signal processing;automotive engineering;microcomputer;transport engineering;processor register;manufacturing;data acquisition;brake	Robotics	46.97080303581627	-4.570235082808375	34364
8482e630767e1934cbcdd657db4325bce2fd7a5c	nonparametric tail copula estimation: an application to stock and volatility index returns	macquarie university institutional repository;researchonline;nonparametric estimation;digital repository;volatility indices;macquarie university;copula;62g32;tail dependence;extreme value theory;stock;62p05;60e05	In this study, we measure asymmetric negative tail dependence and discuss their statistical properties. In a simulation study, we show the reliability of nonparametric estimators of tail copula to measure not only the common positive lower and upper tail dependence, but also the negative “lower–upper” and “upper–lower” tail dependence. The use of this new framework is illustrated in an application to financial data. We detect the existence of asymmetric negative tail dependence between stock and volatility indices. Many common parametric copula models used in finance fail to capture this characteristic.	digraphs and trigraphs;nl (complexity);rare events;simulation;time series;volatility	Yuri Salazar;Wing Lon Ng	2013	Communications in Statistics - Simulation and Computation	10.1080/03610918.2011.650256	econometrics;digital library;copula;extreme value theory;mathematics;stock;statistics	HPC	32.28552138989925	-20.191336239009594	34372
269a0801104da83bbe8faf0f08c0a8ced5202a0a	the multistage homotopy analysis method: application to a biochemical reaction model of fractional order	93a30;multistage homotopy method;numerical solution;homotopy analysis method;49mxx;fractional differential equations;34k28;mathematical modelling;33f05;92bxx;enzyme kinetics;runge kutta method;34a30	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	approximation algorithm;enzyme commission number;francis;kinetics internet protocol;multistage amplifier;nonlinear system;numerical analysis;numerical method;primary source;runge–kutta methods	Mohammad Zurigat;Shaher Momani;Ahmed Alawneh	2014	Int. J. Comput. Math.	10.1080/00207160.2013.819088	mathematical optimization;mathematical analysis;homotopy analysis method;runge–kutta methods;homotopy perturbation method;enzyme kinetics;calculus;mathematical model;mathematics	Robotics	49.73329170504998	-2.9932887034639344	34378
22c613f7b3aa291cb49204e1b550e3b21f95243d	overview of cyber-physical temperature estimation in smart buildings: from modeling to measurements	temperature measurement temperature sensors temperature smart buildings matching pursuit algorithms monitoring;sensor measurements cyber physical temperature estimation smart buildings orthogonal matching pursuit algorithm least squares fitting method temperature measurement spatial temperature distribution;temperature measurement bayes methods buildings structures intelligent structures iterative methods least mean squares methods temperature distribution	Smart buildings are playing a more important role in everyday lives of people. One important goal of creating smart buildings is to offer highly comfortable services to the occupants at the lowest cost. In-building temperature modeling and measurement is a critical task to facilitate high-quality service with low cost. In this paper, we give an overview of state-of-the-art cyber-physical temperature estimation methods in smart buildings. For temperature modeling, by adopting an orthogonal matching pursuit algorithm, the modeling cost and accuracy can be both improved compared with the conventional least-squares fitting method. For temperature measurement, by combining temperature modeling with few sensor measurements using a Bayesian model fusion framework, the spatial temperature distribution can be accurately estimated.	algorithm;bayesian network;least squares;matching pursuit;openmp;quality of service	Xiaoming Chen;Xin Li;Sheldon X.-D. Tan	2016	2016 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)	10.1109/INFCOMW.2016.7562081	mathematical optimization	Vision	48.4367030038245	3.9639427813089396	34438
43e12e9f24ae2fae71c3939f5ba6af2e2978207c	harmonic competition: a self-organizing multiple criteria optimization	unsupervised learning;optimal solution;multicriteria analysis;wide dynamic range;optimisation;learning algorithm;costs learning systems space exploration vector quantization testing neurons dynamic range pareto optimization genetic mutations genetic algorithms;competition;duality mathematics;optimizacion;learning;vehicle routing problem;travelling salesman problem;multiple objective optimization;duality;space exploration;traveling salesperson problem;testing;probleme tournee vehicule;costo;pareto optimization;pareto optimal set;optimisation combinatoire;aprendizaje;problema viajante comercio;subcosts;learning systems;learning system;apprentissage;cuantificacion vectorial;vector quantization;multiple criteria optimization;self organising feature maps;probleme commis voyageur;euclidian space;self organising feature maps unsupervised learning constraint theory genetic algorithms travelling salesman problems duality mathematics vector quantisation;harmonic competition;travelling salesman problems;dynamic range;constraint theory;competitive neural net harmonic competition self organizing multicriteria optimization unsupervised learning winner take all subcosts euclidian space vector quantization constraints traveling salesperson problems duality pareto optimal set genetic algorithms;self organization;genetic algorithm;genetic algorithms;optimization;vector quantizer;analisis multicriterio;self organizing multicriteria optimization;genetic mutations;traveling salesperson problems;competitive neural net;neurons;analyse multicritere;reseau neuronal;combinatorial optimization;vector quantisation;winner take all;learning strategies;red neuronal;pareto optimality;constraints;autoorganisation;competencia;neural network;optimizacion combinatoria;cout;quantification vectorielle	Harmonic competition is a learning strategy based upon winner-take-all or winner-take-quota with respect to a composite of heterogeneous subcosts. This learning is unsupervised and organizes itself. The subcosts may conflict with each other. Thus, the total learning system realizes a self-organizing multiple criteria optimization. The subcosts are combined additively and multiplicatively using adjusting parameters. For such a total cost, a general successive learning algorithm is derived first. Then, specific problems in the Euclidian space are addressed. Vector quantization with various constraints and traveling salesperson problems are selected as test problems. The former is a typical class of problems where the number of neurons is less than that of the data. The latter is an opposite case. Duality exists in these two classes. In both cases, the combination parameters of the subcosts show wide dynamic ranges in the course of learning. It is possible, however, to decide the parameter control from the structure of the total cost. This method finds a preferred solution from the Pareto optimal set of the multiple object optimization. Controlled mutations motivated by genetic algorithms are proved to be effective in finding near-optimal solutions.	genetic heterogeneity;genetic algorithm;mathematical optimization;mutation;organizing (structure);pareto efficiency;population parameter;self-organization;unsupervised learning;vector quantization;travel	Yasuo Matsuyama	1996	IEEE transactions on neural networks	10.1109/72.501723	unsupervised learning;mathematical optimization;genetic algorithm;combinatorial optimization;computer science;artificial intelligence;vehicle routing problem;machine learning;mathematics;travelling salesman problem;artificial neural network	ML	26.4466868614901	1.0228638565384234	34456
797597e8206f7f8d86332a80cff2d8e63115a4f5	improved huffman code tables for still and video image encoders	statistics discrete cosine transforms streaming media brightness pixel discrete transforms decorrelation image coding biomedical imaging encoding;image coding;data compression;quality level huffmann code tables compression encoders jpeg baseline algorithm still image compression h 261 based video encoder image encoders input redundancy reduction simulation;h 261 based video encoder;simulation;biomedical imaging;huffman codes;image encoders;brightness;input redundancy reduction;redundancy;image compression;discrete transforms;streaming media;quality level;compression encoders;discrete cosine transforms;huffmann code tables;pixel;statistics;decorrelation;encoding;jpeg baseline algorithm;still image compression;data compression image coding huffman codes redundancy	This note drnls with the Huffman encmling phase of two well-Lnoum com.pmssion rncmlrrs. onr thnt of thr thr tmrlinr algorithm proposed b y .JPEG for rnmpmssion of still ,images. and other of tttr 11.261 Imserl ddeo encmler. It identifirs two tvprs of minor mdundaneies an the input to the enrcnling phanc. and pmscnts alternotii!e Huffman rmlr tables. Simulation results on still pictuw rompwssions inrlieatr thnt the proposrd Trdundanry rrduction lrnds to an impnnxment of the order of 3.35% in the site of thr image rompmsaed at 75% quality leer1 and 5.3% rompmssrrl at thr 50% qunlity lroel on the nwragr.	algorithm;encoder;huffman coding;jpeg;reflow soldering;simulation	Gopal Lakhani;Vamsi Ayyagari	1995		10.1109/CBMS.1995.465432	data compression;medical imaging;computer vision;decorrelation;canonical huffman code;image compression;computer science;theoretical computer science;redundancy;brightness;pixel;encoding;statistics;huffman coding	Vision	43.67290589476134	-16.68305108630141	34480
a88bca712668d8f92452efc7be571dfa2d58cc3b	cuckoo search with varied scaling factor	cuckoo search algorithm uniform distribution random sampling scaling factor function optimization problems;random sampling;function optimization problems;cuckoo search algorithm;scaling factor;lijin wang yilong yin yiwen zhong 比例因子 搜索 杜鹃 随机游走 寄生行为 反复使用 扩展因子 迭代策略 cuckoo search with varied scaling factor;uniform distribution	Cuckoo search (CS), inspired by the obligate brood parasitic behavior of some cuckoo species, iteratively uses Lévy flights random walk (LFRW) and biased/selective random walk (BSRW) to search for new solutions. In this study, we seek a simple strategy to set the scaling factor in LFRW, which can vary the scaling factor to achieve better performance. However, choosing the best scaling factor for each problem is intractable. Thus, we propose a varied scaling factor (VSF) strategy that samples a value from the range [0,1] uniformly at random for each iteration. In addition, we integrate the VSF strategy into several advanced CS variants. Extensive experiments are conducted on three groups of benchmark functions including 18 common test functions, 25 functions proposed in CEC 2005, and 28 functions introduced in CEC 2013. Experimental results demonstrate the effectiveness of the VSF strategy.	algorithm;benchmark (computing);cuckoo search;distribution (mathematics);dixon's factorization method;experiment;image scaling;integrated circuit;iteration;lévy flight;mathematical optimization;optimization problem;performance;powell's method;quartic function;scalability;sid meier's alpha centauri;test suite;veritas cluster server	Lijin Wang;Yilong Yin;Yiwen Zhong	2015	Frontiers of Computer Science	10.1007/s11704-015-4178-y	scale factor;sampling;mathematical optimization;artificial intelligence;uniform distribution;statistics	AI	27.968595911005178	-2.8711677192390272	34499
195856e101acfbf53f84607a141ba646e8d361af	lossy image coding in the pixel domain using a sparse steering kernel synthesis approach	smoothing methods image coding image denoising image reconstruction regression analysis;kernel regression;jpeg coding lossy image coding pixel domain image de noising image deblocking image reconstruction compression scheme sparse steering kernel synthesis coding ssksc jpeg preprocessor jpeg postprocessor nonuniform sampling image smoothness missing pixels adaptive kernel regression blocking artifacts;image coding;reconstruction;sparse steering kernel synthesis image coding compression adaptive sampling kernel regression;sparse steering kernel synthesis;compression standard;regression;technology and engineering;transform coding kernel image coding psnr gain image reconstruction correlation;adaptive sampling;compression	Kernel regression has been proven successful for image de-noising, deblocking and reconstruction. These techniques lay the foundation for new image coding opportunities. In this paper, we introduce a novel compression scheme: Sparse Steering Kernel Synthesis Coding (SSKSC). This pre- and postprocessor for JPEG performs non-uniform sampling based on the smoothness of an image, and reconstructs the missing pixels using adaptive kernel regression. At the same time, the kernel regression reduces the blocking artifacts from the JPEG coding. Crucial to this technique is that non-uniform sampling is performed while maintaining only a small overhead for signalization. Compared to JPEG, SSKSC achieves a compression gain for low bits-per-pixel regions of 50% or more for PSNR and SSIM. A PSNR gain is typically in the 0.0-0.5 bpp range, and an SSIM gain can mostly be achieved in the 0.0-1.0 bpp range.	blocking (computing);color depth;deblocking filter;jpeg;kernel (operating system);lossy compression;noise reduction;nonuniform sampling;overhead (computing);peak signal-to-noise ratio;pixel;sampling (signal processing);sparse matrix;structural similarity	Ruben Verhack;Andreas Krutz;Peter Lambert;Rik Van de Walle;Thomas Sikora	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025974	kernel regression;lossless jpeg;computer vision;regression;computer science;theoretical computer science;machine learning;pattern recognition;jpeg;mathematics;kernel;compression	Robotics	44.7607010513105	-17.118993265674828	34505
13ea315d29cec98b65072f9b0149b29f7390d941	new magneto-elastic sensor signal test and application	biomedical sensor;fpga;magneto-elastic sensor;multi-channel magneto-elastic tester	This paper presents a new kind of magneto-elastic wireless sensor. Its theory background, excitation signal generation, signal measurement method and its application in biochemistry field are introduced. The emphasis focus on the design of 16 channels magneto-elastic sensor signal test device. Based on the Altera Cyclone FPGA, we design a programmable soft processor, 16 channels direct digital synthesizer, 16 channels D/A converter interface, serial communication interface and test system software. The device can test 16 channel sensor signals at the same time and the measurement results are shown on host PC. This proposed system is successfully used in measuring glucose concentration. © 2010 Springer-Verlag.		Lei Chen;Xiangyu Li;Tangsheng Yang	2010		10.1007/978-3-642-16339-5_28	electro-optical sensor	EDA	46.56256571909203	-1.1796773892640355	34540
8c51435ed9e73e7d4b029ef63f9efdbc76048924	energy aware particle swarm optimization as search mechanism for aerial micro-robots	energy consumption;particle swarm optimization;robots;energy states;mathematical model;unmanned aerial vehicles	This paper presents the Energy Aware PSO (EAPSO) as a search mechanism for aerial micro-robots with limited energy capacity. The proposed model is an extension of the search concept of Particle Swarm Optimization (PSO) that additionally considers the energy levels of the individuals for an efficient movement. One major contribution of this paper is that the energy efficiency results from a multi-criteria decision making process performed by the individuals. The energy consumption model in EAPSO is adapted from a real hardware scenario and has been tested on three known landscapes which are very similar to search terrains by the aerial micro-robots. The results show that EAPSO can reduce the total energy consumption of the swarm with negligible degradation of the search results.	aerial photography;elegant degradation;energy level;experiment;mathematical optimization;microbotics;particle swarm optimization;phase-shift oscillator;robot;weight function	Sanaz Mostaghim;Christoph Steup;Fabian Witt	2016	2016 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2016.7850263	mathematical optimization;multi-swarm optimization;simulation;engineering;artificial intelligence;metaheuristic	Robotics	31.581750725852295	-2.7917129732195334	34577
16b0279910505a9bd871938efeb000fc27df27fd	mb energy trend-based intra prediction algorithm for mpeg-2 to h.264/avc transcoding	complexity theory;dct coefficients;decoding;prediction algorithms;h 264 avc transcoding;transform coding;intra prediction;video coding;automatic voltage control;prediction theory;computational complexity;discrete cosine transforms;mpeg 2;prediction algorithms automatic voltage control transcoding computational complexity video coding decoding computer architecture iec standards iso standards discrete cosine transforms;discrete cosine transforms mb energy trend based intra prediction algorithm mpeg 2 h 264 avc transcoding computational complexity dct coefficients;mb energy trend based intra prediction algorithm;video coding discrete cosine transforms prediction theory transcoding;transcoding	In this paper, we proposed a fast Intra prediction method for MPEG-2 to H.264/AVC transcoding. To reduce the computational complexity, we utilized the information of DCT coefficients in MPEG-2 decoder to measure the Intra macroblock (MB) energy trend. Base on the relationship between the energy trend and the intra prediction direction, we can pre-determine the Intra prediction for both luminance (luma) and chrominance (chroma) components. The numbers of the Intra prediction candidates of luma Intra16x16 and chroma Intra8x8 can be reduced from original four to one or two, and the numbers of prediction candidates of luma Intra 4x4 can be reduced from nine to four. With our proposed algorithm, the computational complexity of Intra prediction for MPEG-2 to H.264/AVC transcoding can be reduced about 60% compared with the conventional cascaded transcoding method at the negligible quality loss.	algorithm;chroma subsampling;coefficient;computational complexity theory;discrete cosine transform;h.264/mpeg-4 avc;intra-frame coding;mpeg-2;macroblock	Xingang Liu;Wei Zhu;Kook-Yeol Yoo	2008	2008 IEEE International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2008.74	computer vision;real-time computing;transform coding;transcoding;prediction;computer science;theoretical computer science;mpeg-2;computational complexity theory;statistics	Arch	46.716966852669835	-19.025202618745276	34601
d8b88c958e61eef936ac338cffdb5bf2111999b6	enhanced differential evolution using local lipschitz underestimate strategy for computationally expensive optimization problems	differential evolution;supporting hyperplane;lipschitz underestimate;global optimization;evolutionary algorithm	Graphical abstractDisplay Omitted HighlightsThis study presents a DE with local Lipschitz underestimated strategy to reduce the number of function evaluations.Lipschitz supporting hyperplanes are constructed based on the evolutionary information to guide the DE process.The proposed local Lipschitz underestimate strategy is general and can be applied to other evolutionary algorithms. Differential evolution (DE) has been successfully applied in many scientific and engineering fields. However, one of the main problems in using DE is that the optimization process usually needs a large number of function evaluations to find an acceptable solution, which leads to an increase of computational time, particularly in the case of computationally expensive problems. The Lipschitz underestimate method (LUM), a deterministic global optimization technique, can be used to obtain the underestimate of the objective function by building a sequence of piecewise linear supporting hyperplanes. In this paper, an enhanced differential evolution using local Lipschitz underestimate strategy, called LLUDE, is proposed for computationally expensive optimization problems. LLUDE effectively combines the exploration of DE with the underestimation of LUM to obtain promising solutions with less function evaluations. To verify the performance of LLUDE, 18 well-known benchmark functions and one computationally expensive real-world application problem, namely, the protein structure prediction problem, are employed. Results obtained from these benchmark functions show that LLUDE is significantly better than or at least comparable to the state-of-the-art DE variants and non-DE algorithms. Furthermore, the results obtained from the protein structure prediction problem suggest that LLUDE is effective and efficient for computationally expensive problems.	analysis of algorithms;differential evolution;mathematical optimization	Xiao-gen Zhou;Gui-jun Zhang;Xiao-hu Hao;Dong-wei Xu;Li Yu	2016	Appl. Soft Comput.	10.1016/j.asoc.2016.06.044	differential evolution;mathematical optimization;combinatorics;computer science;machine learning;evolutionary algorithm;mathematics;global optimization	AI	24.633356981195735	-2.879050431197264	34621
45ee982d9a3fc61caf3e73e3dfc2697b58b7ca7a	a multiobjective metaheuristic for a mean-risk static stochastic knapsack problem	random sampling;risk aversion;conditional value at risk;objective function;knapsack problem;expected value;stochastic combinatorial optimisation;risk measure;sample average approximation;local search;mean risk objectives;combinatorial optimisation;multiobjective combinatorial optimisation;stochastic knapsack problem;multiobjective metaheuristics	In this paper we address two major challenges presented by stochastic discrete optimisation problems: the multiobjective nature of the problems, once risk aversion is incorporated, and the frequent difficulties in computing exactly, or even approximately, the objective function. The latter has often been handled with methods involving sample average approximation, where a random sample is generated so that population parameters may be estimated from sample statistics usually the expected value is estimated from the sample average. We propose the use of multiobjective metaheuristics to deal with these difficulties, and apply a multiobjective local search metaheuristic to both exact and sample approximation versions of a mean-risk static stochastic knapsack problem. Variance and conditional value-at-risk are considered as risk measures. Results of a computational study are presented, that indicate the approach is capable of producing high-quality approximations to the efficient sets, with a modest computational effort.	algorithm;approximation;combinatorial optimization;computation;discrete optimization;display resolution;expected shortfall;knapsack problem;local search (optimization);loss function;mathematical optimization;metaheuristic;optimization problem;risk aversion;risk measure;robust optimization;value at risk	João Claro;Jorge Pinho de Sousa	2010	Comp. Opt. and Appl.	10.1007/s10589-008-9197-2	continuous knapsack problem;sampling;mathematical optimization;combinatorics;risk aversion;expected shortfall;local search;machine learning;mathematics;knapsack problem;expected value	AI	30.019008616845536	1.061415577416138	34629
1fea0e325443271d17d7edc0fd5834074d099ec8	variable thresholding based multiple description video coding	multiple description;video streaming;standards;standards video coding video streaming quality of service video codecs;video coding;error resilience;video codecs;quality of service;mds code;video coding redundancy decoding discrete transforms streaming media degradation quality of service video sequences quantization discrete cosine transforms;error resiliency variable thresholding multiple description video coding video streaming packet erasures quality of service h 26 mpeg coding efficiency	Video streaming applications have become increasingly popular in the recent years, and their use will continue to grow in the future. However, variations in channel conditions cause delays and packet erasures resulting in degradation of quality of service (QoS). Multiple description (MD) coding is one method to reduce the detrimental effects caused by these channel variations. In this paper we have presented a novel pre-processing MD approach, which makes use of the redundancies already present in the original sequence to create multiple descriptions. The proposed approach is compatible with the commonly used video coding standards such as the H.26x and MPEG. The proposed scheme improves performance in terms of the coding efficiency and error resiliency compared to the approaches present in the literature.	algorithmic efficiency;data compression;elegant degradation;molecular dynamics;moving picture experts group;network packet;preprocessor;quality of service;thresholding (image processing);video coding format	S. Pavan;G. Sridhar;V. Sridhar	2005	Seventh IEEE International Symposium on Multimedia (ISM'05)	10.1109/ISM.2005.118	video compression picture types;scalable video coding;h.263;quality of service;telecommunications;computer science;theoretical computer science;multiple description coding;video tracking;coding tree unit;block-matching algorithm;multimedia;video processing;smacker video;rate–distortion optimization;context-adaptive binary arithmetic coding;motion compensation;h.261;computer network;multiview video coding	Arch	47.885022711735104	-16.388470906640773	34675
b2d15a6d11504eb5c4d767f85e4fd71d3651179c	full predictivistic modeling of stock market data: application to change point problems	loi student;modelizacion;uncertainty modeling;stock market;ley normal inversa;bolsa valores;flexibilidad;student t distribution;prior distribution;product partition model;stock market returns;time series;ley a priori;yield;stock markets;bourse valeurs;stock exchange;modelisation;marche valeurs;ley student;systeme incertain;particion;inverse normal distribution;serie temporelle;random variable;partition;serie temporal;point changement;normal inverse gamma distribution;gamma distribution;loi normale inverse;flexibilite;rendimiento;student distribution;sistema incierto;modeling;punto cambio;uncertain system;rendement;change point;flexibility;loi a priori	In change point problems in general we should answer three questions: how many changes are there? Where are they? And, what is the distribution of the data within the blocks? In this paper, we develop a new full predictivistic approach for modeling observations within the same block of observation and consider the product partition model (PPM) for treating the change point problem. The PPM brings more flexibility into the change point problem because it considers the number of changes and the instants when the changes occurred as random variables. A full predictivistic characterization of the model can provide a more tractable way to elicit the prior distribution of the parameters of interest, once prior opinions will be required only about observable quantities. We also present an application to the problem of identifying multiple change points in the mean and variance of a stock market return time series.	cobham's thesis;mega man zx;multi-environment real-time;observable;open research;puppy linux;real-time recovery;time series;yao graph	Rosangela Helena Loschi;Pilar Loreto Iglesias;Reinaldo Boris Arellano-Valle;Frederico R. B. Cruz	2007	European Journal of Operational Research	10.1016/j.ejor.2006.04.016	econometrics;student's t-distribution;calculus;mathematics;mathematical economics;statistics	ML	33.738194147606826	-20.676164891945497	34752
fbc3be80c24796c59c0e17490c1809f607e936f9	free energy monte carlo simulations on a distributed network	naturvetenskap;natural sciences	While the use of enhanced sampling techniques and parallel computing to determine potentials of mean force is in widespread use in modern Molecular Dynamics and Monte Carlo simulation studies, there have been few methods that efficiently combine heterogeneous computer resources of varying quality and speeds in realizing a single simulation result on a distributed network. Here, we apply an algorithm based on the Monte Carlo method of Wang and Landau within a client-server framework, in which individual computing nodes report a histogram of regions of phase space visited and corresponding updates to a centralized server at regular intervals entirely asynchronously. The server combines the data and reports the sum to all nodes so that the overall free energy determination scales linearly with the total amount of resources allocated. We discuss our development of this technique and present results for molecular simulations of DNA.	computer simulation;monte carlo method	Luke Czapla;Alexey Siretskiy;John Grime;Malek O. Khan	2010		10.1007/978-3-642-28145-7_1	parallel computing;natural science;simulation;hybrid monte carlo;computer science;theoretical computer science;distributed computing;monte carlo integration;monte carlo method	HPC	42.85304892334784	1.4968729212359169	34766
c309c3e850e8f0105f71e413d1b9b30441ae9d25	image compression using the iteration-tuned and aligned dictionary	matching pursuit algorithms;atomic layer deposited;learned dictionaries;image coding;codecs;structured dictionary;training;rate distortion based sparsity selection criterion;transform coding;matching pursuit image compression sparse representations learned dictionaries structured dictionaries;sparse matrices image coding;iteration tuned and aligned dictionary;image compression;structured dictionaries;dictionaries;block based image codec;codecs dictionaries transform coding image coding atomic layer deposition matching pursuit algorithms training;jpeg2000;matching pursuit;atomic layer deposition;sparse representation;jpeg2000 image compression block based image codec sparse representation structured dictionary iteration tuned and aligned dictionary rate distortion based sparsity selection criterion;sparse matrices;sparse representations	We present a new, block-based image codec based on sparse representations using a learned, structured dictionary called the Iteration-Tuned and Aligned Dictionary (ITAD). The question of selecting the number of atoms used in the representation of each image block is addressed with a new, global (image-wide), rate-distortion-based sparsity selection criterion. We show experimentally that our codec outperforms JPEG2000 in both quantitative evaluations (by 0.9 dB to 4 dB) and qualitative evaluations.	algorithm;codec;data dictionary;distortion;experiment;image compression;iteration;jpeg 2000;sparse approximation;sparse matrix	Joaquin Zepeda;Christine Guillemot;Ewa Kijak	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5946523	codec;transform coding;sparse matrix;k-svd;image compression;computer science;theoretical computer science;machine learning;pattern recognition;sparse approximation;jpeg 2000;mathematics;atomic layer deposition;matching pursuit	Vision	45.103147250653464	-16.120936859362725	34953
1a7a27c1e95be7cad7194071835a1b0b6edba316	a deterministic annealing approach to witsenhausen's counterexample	poor local minima avoidance deterministic annealing approach witsenhausen counterexample optimization method information theoretic idea distributed control problem decentralized control implicit communication zero delay mappings probabilistic mappings shannon entropy constraint;annealing entropy information theory optimization methods decentralized control;simulated annealing constraint theory decentralised control deterministic algorithms distributed control information theory optimisation	This paper proposes an optimization method, based on information theoretic ideas, to a class of distributed control problems. As a particular test case, the well-known and numerically “over-mined” problem of decentralized control and implicit communication, commonly referred to as Witsenhausen's counterexample, is considered. The key idea is to randomize the zero-delay mappings. which become “soft”, probabilistic mappings to be optimized in a deterministic annealing process, by incorporating a Shannon entropy constraint in the problem formulation. The entropy of the mapping is controlled and gradually lowered to zero to obtain deterministic mappings, while avoiding poor local minima. For the particular test case, our approach obtains new mappings that shed light on the structure of the optimal solution, as well as achieving a small improvement in total cost over the state of the art in numerical approaches to this problem. Proposed method is general and applicable to any problem of similar nature.	distributed control system;entropy (information theory);information theory;mathematical optimization;maxima and minima;mined;numerical analysis;shannon (unit);simulated annealing;test case;whole earth 'lectronic link;witsenhausen's counterexample	Mustafa S. Mehmetoglu;Emrah Akyol;Kenneth Rose	2014	2014 IEEE International Symposium on Information Theory	10.1109/ISIT.2014.6875391	mathematical optimization;combinatorics;discrete mathematics;witsenhausen's counterexample;mathematics	Embedded	30.11520439069367	1.5554007927723517	34964
377a2fb682ff1ce001d0ce7142cfc746d9890cd1	modified biogeography-based optimization with local search mechanism		Biogeography-based optimization (BBO) is a new effective population optimization algorithm based on the biogeography theory with inherently insufficient exploration capability. To address this limitation, we proposed a modified BBO with local search mechanism (denoted as MLBBO). In MLBBO, a modified migration operator is integrated into BBO, which can adopt more information from other habitats, to enhance the exploration ability. Then, a local search mechanism is used in BBO to supplement withmodifiedmigration operator. Extensive experimental tests are conducted on 27 benchmark functions to show the effectiveness of the proposed algorithm. The simulation results have been compared with original BBO, DE, improved BBO algorithms, and other evolutionary algorithms. Finally, the performance of the modified migration operator and local search mechanism are also discussed.		Quanxi Feng;Sanyang Liu;Qunying Wu;GuoQiang Tang;Haomin Zhang;Huazhou Chen	2013	J. Applied Mathematics	10.1155/2013/960524	mathematical optimization;algorithm	AI	26.88104294597835	-4.136091525026896	35015
320836e3140a68e9d89a591cd4f4ceae0c6afcf8	evolution of food-foraging strategies for the caribbean anolis lizard using genetic programming	computer program;optimal foraging strategy;genetic program;adaptive control;crossover;lizards;adaptive behavior;genetic programming;genetics;optimal foraging;genetic algorithm;genetic algorithms;behavioral ecology	This paper describes the recently developed genetic programming paradigm which genetically breeds a population of computer programs to solve problems. The paper then shows, step by step, how to apply genetic programming to a problem of behavioral ecology in biology – specifically, two versions of the problem of finding an optimal food foraging strategy for the Caribbean Anolis lizard. A simulation of the adaptive behavior of the lizard is required to evaluate each possible adaptive control strategy considered for the lizard. The foraging strategy produced by genetic programming is close to the mathematical solution for the one version for which the solution is known and appears to be a reasonable approximation to the solution for the second version of the problem.	adaptive behavior;approximation;computer program;control theory;ecology;equation solving;genetic programming;programming paradigm;simulation	John R. Koza;Jonathan Roughgarden;James P. Rice	1992	Adaptive Behaviour	10.1177/105971239200100203	genetic algorithm;adaptive control;behavioral ecology;computer science;artificial intelligence;machine learning;genetic representation	AI	26.35179201527991	-8.234520960993088	35043
6c825ae1c8c308bc5c723944995682e5227ccc40	target tracking in collaborative sensor networks by using a decentralized leader-based scheme	decentralized filtering;estimation theory;extended information filter;collaborative sensor networks;nonlinear motion model;target tracking decentralized filtering information filter sensor networks;information filtering;sensor selection;data fusion;estimation algorithm;sensor network;deif algorithm target tracking collaborative sensor networks data fusion nonlinear motion model fully decentralized estimation extended information filter neighbor selection information selection sensor selection;fully decentralized estimation;estimation;sensor networks;lead;heuristic algorithms;information filter;wireless sensor networks estimation theory information filtering sensor fusion target tracking;information selection;neighbor selection;sensor fusion;target tracking;information filters;target tracking collaboration information filters information filtering motion estimation state estimation vectors mobile communication sensor systems and applications wireless sensor networks;wireless sensor networks;deif algorithm;noise	In this paper, the problem of fully-decentralized data fusion is addressed for tracking a target with nonlinear motion model. The problem is solved by applying a fully-decentralized estimation algorithm based on the extended information filter. We propose the neighbor selection and information selection algorithms for sensor selection based on their closeness to the estimated target position and their information contribution, respectively. Simulation results show that compared to the DEIF algorithm we obtain an analogous response with less consumption of energy and computations.	algorithm;centrality;computation;emoticon;kalman filter;mean squared error;missile guidance;nonlinear system;scalability;simulation;xfig	Mohammad Reza Zoghi;Mohammad Hossein Kahaei	2007	2007 9th International Symposium on Signal Processing and Its Applications	10.1109/ISSPA.2007.4555559	computer vision;wireless sensor network;computer science;machine learning;control theory;sensor fusion;statistics	Embedded	53.3130428792591	2.92608848151	35051
d6b7002b6fcb9e2098b56eb8c381ba326794d9b6	paremd: a parallel program for the evaluation of momentum space properties of atoms and molecules		Abstract The present work describes a code for evaluating the electron momentum density (EMD), its moments and the associated Shannon information entropy for a multi-electron molecular system. The code works specifically for electronic wave functions obtained from traditional electronic structure packages such as GAMESS and GAUSSIAN. For the momentum space orbitals, the general expression for Gaussian basis sets in position space is analytically Fourier transformed to momentum space Gaussian basis functions. The molecular orbital coefficients of the wave function are taken as an input from the output file of the electronic structure calculation. The analytic expressions of EMD are evaluated over a fine grid and the accuracy of the code is verified by a normalization check and a numerical kinetic energy evaluation which is compared with the analytic kinetic energy given by the electronic structure package. Apart from electron momentum density, electron density in position space has also been integrated into this package. The program is written in C++ and is executed through a Shell script. It is also tuned for multicore machines with shared memory through OpenMP. The program has been tested for a variety of molecules and correlated methods such as CISD, Moller–Plesset second order (MP2) theory and density functional methods. For correlated methods, the PAREMD program uses natural spin orbitals as an input. The program has been benchmarked for a variety of Gaussian basis sets for different molecules showing a linear speedup on a parallel architecture. Program summary Program Title: PAREMD Program Files doi: http://dx.doi.org/10.17632/gcr9gmh6zv.1 Licensing provisions: GPLv3 Programming language: C, Csh External routines/libraries: GSL[1], BLAS[2,3], OpenMP[4], GAMESS[5,6] and GAUSSIAN[7]. Nature of problem: Momentum space properties for a multi-electron system. Solution method: Analytic Fourier transformation of Gaussian basis in position space to get momentum space basis followed by EMD evaluation on spherical or Cartesian grids. A numerical integration procedure is implemented for evaluating moments of EMD and Shannon information entropy on a polar grid. [1] Galassi et al, GNU Scientific Library Reference Manual (3rd Ed.), ISBN 0954612078. [2] L. S. Blackford, J. Demmel, J. Dongarra, I. Duff, S. Hammarling, G. Henry, M. Heroux, L. Kaufman, A. Lumsdaine, A. Petitet, R. Pozo, K. Remington, R. C. Whaley, An Updated Set of Basic Linear Algebra Subprograms (BLAS), ACM Trans. Math. Soft., 28-2 (2002), pp. 135–151. [3] J. Dongarra, Basic Linear Algebra Subprograms Technical Forum Standard, International Journal of High Performance Applications and Supercomputing, 16(1) (2002), pp. 1–111, and International Journal of High Performance Applications and Supercomputing, 16(2) (2002), pp. 115–199. [4] OpenMP Architecture Review Board, ”OpenMP Application Program Interface, Version 4.5”, November 2015. [5] ”General Atomic and Molecular Electronic Structure System” M.W. Schmidt, K.K. Baldridge, J.A. Boatz, S.T. Elbert, M.S. Gordon, J.H. Jensen, S. Koseki, N. Matsunaga, K.A. Nguyen, S.J. Su, T.L. Windus, M. Dupuis, J.A. MontgomeryJ. Comput. Chem. 14 1347–1363, 1993. [6] “Advances in electronic structure theory: GAMESS a decade later M.S. Gordon, M.W. Schmidt pp. 1167–1189, in Theory and Applications of Computational Chemistry: the first forty years C.E. Dykstra, G. Frenking, K.S. Kim, G.E. Scuseria (editors), Elsevier, Amsterdam, 2005. [7] Gaussian 09, Revision C.01, M. J. Frisch, G. W. Trucks, H. B. Schlegel, G. E. Scuseria, M. A. Robb, J. R. Cheeseman, G. Scalmani, V. Barone, G. A. Petersson, H. Nakatsuji, X. Li, M. Caricato, A. Marenich, J. Bloino, B. G. Janesko, R. Gomperts, B. Mennucci, H. P. Hratchian, J. V. Ortiz, A. F. Izmaylov, J. L. Sonnenberg, D. Williams-Young, F. Ding, F. Lipparini, F. Egidi, J. Goings, B. Peng, A. Petrone, T. Henderson, D. Ranasinghe, V. G. Zakrzewski, J. Gao, N. Rega, G. Zheng, W. Liang, M. Hada, M. Ehara, K. Toyota, R. Fukuda, J. Hasegawa, M. Ishida, T. Nakajima, Y. Honda, O. Kitao, H. Nakai, T. Vreven, K. Throssell, J. A. Montgomery, Jr., J. E. Peralta, F. Ogliaro, M. Bearpark, J. J. Heyd, E. Brothers, K. N. Kudin, V. N. Staroverov, T. Keith, R. Kobayashi, J. Normand, K. Raghavachari, A. Rendell, J. C. Burant, S. S. Iyengar, J. Tomasi, M. Cossi, J. M. Millam, M. Klene, C. Adamo, R. Cammi, J. W. Ochterski, R. L. Martin, K. Morokuma, O. Farkas, J. B. Foresman, and D. J. Fox, Gaussian, Inc., Wallingford CT, 2016.	position and momentum space	Deep Raj Meena;Shridhar R. Gadre;P. Balanarayan	2018	Computer Physics Communications	10.1016/j.cpc.2017.12.002	entropy (information theory);position and momentum space;architecture;mathematics;mathematical analysis;basic linear algebra subprograms;gamess;spin-½;gaussian;basis function	ML	42.29771065989248	3.189564024234903	35102
e10369348dd425f9ddd86d3c95c3f069b38f25c3	comparison of different methodologies of parameter-estimation from extreme values		This letter deals with the case where parameter estimation is required, but only observations of extreme values (i.e., the minimum observed value and/or the maximum observed value per interval) are available. We describe the theoretical grounds of the three leading methodologies of estimation from extremes, discuss the relations between them, and analyze the tradeoffs of the different methodologies with respect to the performance (accuracy), complexity, and robustness of the estimates. We then demonstrate our evaluations via a specially designed simulation, which validates our results.	complexity;estimation theory;realization (probability);simulation	Jonatan Ostrometzky;Hagit Messer	2017	IEEE Signal Processing Letters	10.1109/LSP.2017.2723544	maximum likelihood;econometrics;mathematical optimization;robustness (computer science);mathematics;extreme value theory;probability density function;statistics;estimation theory	Metrics	28.887095689361644	-21.39628542839924	35110
502e1436e77e22a9334965dd53603b50344053f4	combined source-channel coding for the transmission of still images over a code division multiple access (cdma) channel	orthogonal signaling scheme;joint source channel optimization algorithm;channel coding;still images transmission;optimisation;uplink;error correction codes;image coding;system error correction capability;cellular radio combined source channel coding still images transmission code division multiple access channel discrete cosine transform uplink wireless is 95 cdma channel orthogonal signaling scheme system error correction capability bit rate channel error quantizer array joint source channel optimization algorithm bit allocation problem integer programming technique reconstructed image quality;image communication;wireless is 95 cdma channel;combined source channel coding;cellular radio;bit allocation problem;indexing terms;bit rate;design optimization;discrete cosine transform;joint source channel;code division multiple access channel;quantisation signal;code division multiple access;integer programming technique;image transmission;integer programming;quantizer array;discrete cosine transforms;error correction;image reconstruction;reconstructed image quality;linear programming;bandwidth;image coding bit rate image communication multiaccess communication discrete cosine transforms algorithm design and analysis design optimization linear programming bandwidth image reconstruction;channel error;cellular radio code division multiple access image reconstruction image coding channel coding source coding discrete cosine transforms error correction codes optimisation quantisation signal integer programming;optimal algorithm;algorithm design and analysis;multiaccess communication;source coding	This letter considers a combined source-channel coding scheme for image transmission over the uplink of a wireless IS-95 code division multiple access (CDMA) channel using discrete cosine transform. By adjusting the dimension of the orthogonal signaling scheme, we trade the system error-correction capability for a faster bit rate. The increase in channel error is relieved by employing a set of quantizers which are designed using a joint source-channel optimization algorithm.	algorithm;discrete cosine transform;error detection and correction;forward error correction;mathematical optimization;quantization (signal processing);telecommunications link	E. V. H. Iun;Amir K. Khandani	1997	IEEE Communications Letters	10.1109/ICC.1997.605183	iterative reconstruction;algorithm design;code division multiple access;multidisciplinary design optimization;error detection and correction;integer programming;index term;telecommunications link;channel code;telecommunications;computer science;linear programming;theoretical computer science;discrete cosine transform;bandwidth;source code	Mobile	47.90166085121636	-13.24640840508587	35129
d5e8b82edc7e7bdd575b17cf893730de3d36309a	improved chaotic particle swarm optimization with a perturbation-based chaotic system for a virtual quartic function	metaheuristics;chaotic system;chaos bifurcation standards orbits particle swarm optimization linear programming optimization;cpso ssv improved chaotic particle swarm optimization perturbation based chaotic system virtual quartic function cpso vqo steepest descent method threshold distance parameter values bifurcation structure stochastic selection cpso tsv;perturbation chaotic system particle swarm optimization metaheuristics;particle swarm optimization;perturbation;particle swarm optimisation	In this paper, we discuss the particle swarm optimization method (PSO). In particular, we focus on the CPSO-VQO, a PSO with a perturbation-based chaotic system derived from the steepest descent method for a virtual quartic function having global minima at the lbest and gbest, which selects the updating system of the particle's position from the standard system of the original PSO and the chaotic one on the basis of a threshold distance between two bests. Although the good performance of CPSO-VQO is reported, it is not so easy to select appropriate parameter values of its chaotic system for each problem because the bifurcation structure of the chaotic system depends on the distance of two bests, and, moreover, it is required an appropriate threshold for selecting the updating system. Therefore, we improve the CPSO-VQO by proposing a modified chaotic system having the bifurcation structure irrelevant to the distance of two bests, and a new stochastic selection of the updating system. In addition, we theoretically show the desirable properties of the modified chaotic system and evaluate the improved CPSO-VQOs called CPSO-TSV and CPSO-SSV.	bifurcation diagram;bifurcation theory;chaos theory;experiment;gradient descent;mathematical optimization;maxima and minima;maximal set;norm (social);numerical analysis;particle swarm optimization;quartic function;relevance;through-silicon via	Keiji Tatsumi;Takeru Ibuki;Satoshi Nakashima;Tetsuzo Tanino	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.42	mathematical optimization;multi-swarm optimization;perturbation;computer science;machine learning;control theory;mathematics;particle swarm optimization;metaheuristic	Robotics	28.90719399118151	-4.80659406027254	35182
ec98f717fb60b7416b1a19ccce492e02b54e2368	estimating quality bounds of jpeg 2000 compressed leukocytes images	microscopic images;segmentation;jpeg 2000;compression;leukocytes	Several pathologies are detected by counting different types of leukocytes indigital microscopic images. However, manipulation of these images, i.e. storage and/or transmission, can be complicated by the large sizes of the files containing them. In order to tackle this particular situation, lossy compression codecssuch as JPEG2000 have been employed while preserving the overall perceived image quality. In this paper a strategy based on objective quality metrics and performance of segmentation algorithms is proposed for the estimation of the maximal allowable compression rate (CR) where deterioration introduced in the images by the JPEG 2000 codec does not affect identification of white blood cells. Results indicate that the estimated value lays around CR = 142:1as measured by the metrics employed.	jpeg 2000	Alexander Falcón-Ruiz;Juan Paz-Viera;Hichem Sahli	2010		10.1007/978-3-642-15992-3_12	computer vision;jpeg 2000;multimedia;segmentation;compression;computer graphics (images)	Vision	40.83853658724713	-17.242115355629252	35193
3f8d74ae0c4ec8fbdd42bbce3350ae688a3463b3	low bit-rate compression of omnidirectional images	matching pursuit algorithms;matching pursuit algorithm;quantization;mirrors;geometry atom dictionary;image coding;spherical camera model;data compression;image matching;geometry;computational geometry;vision sensor;omnidirectional images;video compression;jpeg2000 coding;construction industry;image sensors approximation theory computational geometry computer vision data compression image coding image matching image representation;sparse;omnidirectional image;transform coding;image sensors;lts4;bit rate image compression;computer vision;image coding cameras mirrors dictionaries geometry matching pursuit algorithms video compression quantization transform coding design methodology;approximation theory;adaptive quantization;image representation;approximation theory bit rate image compression omnidirectional image representation vision sensor geometry atom dictionary sparse image decomposition image coding matching pursuit algorithm adaptive quantization jpeg2000 coding spherical camera model;field of view;dictionaries;matching pursuit;omnidirectional;sparse image decomposition;sparse omnidirectional images compression;compression;image decomposition;cameras;omnidirectional image representation;design methodology	Omnidirectional images represent a special type of images that are captured by vision sensors with a 360-degree field of view. This work targets the compression of such images by taking into account their particular geometry. We first map omnidirectional images to spherical ones and then perform sparse image decomposition over a dictionary of geometric atoms on the 2D sphere. A coder based on Matching Pursuit and adaptive quantization is finally proposed for efficient compression of omnidirectional images. The experiments demonstrate that the proposed system outperforms JPEG2000 coding of unfolded images. Since most omnidirectional sensors can be parametrized with a spherical camera model, the proposed method is generic with respect to different sensor constructions.	dictionary;experiment;image sensor;jpeg 2000;matching pursuit;quantization (signal processing);sparse matrix	Ivana Tosic;Pascal Frossard	2009	2009 Picture Coding Symposium	10.1109/PCS.2009.5167400	data compression;computer vision;computational geometry;computer science;theoretical computer science;machine learning;mathematics;statistics	Vision	49.55728627617816	-17.476893408013805	35275
c2c7422e5783020dc4844b9f50fabf42758b43a0	feature-assisted threshold selection for all-zero block detection and its application to video coding optimization in h.264	systeme temps reel;quantization;texture;optimisation;image processing;0705p;threshold detection;0130c;transformation cosinus discrete;reference frame;video quality;motion estimation;optimization method;quantification;metodo optimizacion;traitement image;detection seuil;video coding;deteccion umbral;signal quantization;codage video;computational complexity;discrete cosine transforms;quantification signal;methode optimisation;transformation inverse;optimization;video;inverse transformation;4230v;encoding;codage;transformacion inversa;real time systems	All-zero blocks (AZB) denote blocks with all zero DCT coefficients after quantization. Early determination of AZB can avoid unnecessary DCT/Q/IQ/IDCT computation. Existing techniques in the literature primarily address more efficient thresholds for early determination of AZB. This paper deals with the selection of such thresholds based on low level features including motion activity and texture information. This aspect is then utilized to avoid any: (1) unnecessary quarter accuracy motion estimation, (2) unnecessary multiple reference frame motion estimation, and (3) unnecessary DCT/Q/IQ/IDCT computation. The developed approach has been applied to two different format video sequences CIF and QCIF. The results show that the computational complexity is significantly reduced while the video quality is maintained at a tolerable loss level.© (2008) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	data compression;h.264/mpeg-4 avc;mathematical optimization	Jianfeng Ren;Nasser Kehtarnavaz	2008		10.1117/12.767663	reference frame;computer vision;video;quantization;telecommunications;image processing;video quality;theoretical computer science;motion estimation;texture;computational complexity theory;encoding	Vision	46.41715015225349	-14.401777907696372	35305
a4434cd9feb2d399e9d200e06affbe686b829e1a	modelling small area counts in the presence of overdispersion and spatial autocorrelation	autocorrelacion;classification automatique statistiques;association statistique;europa;analyse multivariable;estimator robustness;multivariate analysis;analisis datos;implementation;grande bretagne;62h20;evenement rare;statistical association;great britain;surdispersion;discriminant analysis;analyse discriminante;data analysis;analisis discriminante;asociacion estadistica;robustez estimador;efecto aleatorio;geographic variation;62h30;rare event;random effect;royaume uni;united kingdom;statistical computation;acontecimiento rara;calculo estadistico;reino unido;england;spatial filtering;analisis multivariable;analyse donnee;calcul statistique;inglaterra;angleterre;random effects model;europe;cluster analysis statistics;estimation statistique;implementacion;effet aleatoire;estimacion estadistica;statistical estimation;overdispersion;spatial autocorrelation;autocorrelation;robustesse estimateur;gran bretana	The problems arising when modelling counts of rare events observed in small geographical areas when overdispersion and residual spatial autocorrelation are present or anticipated are considered. Different models are presented for handling inference in this case. The different strategies are implemented using data on offender counts at the enumeration district scale for Sheffield, England and results compared. This example is chosen because previous research suggests that social processes and social composition variables are key to understanding geographical variation in offender counts which will, as a consequence, show evidence of clustering both at the scale of the enumeration district and at larger scales. This in turn leads the analyst to anticipate the presence of overdispersion and spatial autocorrelation. Diagnostic measures are described and different modelling strategies are implemented. The evidence suggests that modelling strategies based on the use of spatial random effects models or models that include spatial filters appear to work well and provide a robust basis for model inference but gaps remain in the methodology that call for further research.	autocorrelation;spatial analysis	Robert Haining;Jane Law;Daniel A. Griffith	2009	Computational Statistics & Data Analysis	10.1016/j.csda.2008.08.014	econometrics;mathematics;linear discriminant analysis;statistics;random effects model	ML	33.43647420120286	-22.76467945954633	35361
a96e7dbdd47a835b5d06a374d8761c1be615510d	multichannel audio signal compression based on tensor decomposition	tensors audio coding;core tensor multichannel audio signal compression tensor decomposition tucker model;tensile stress matrix decomposition encoding decoding discrete cosine transforms radio frequency training;approximation error multichannel audio signal compression method tensor decomposition multichannel audio tensor space tucker model truncated core tensor decoder factor matrices signal distortion;audio coding;tensors	This paper proposes a novel multichannel audio signal compression method based on tensor decomposition. The multichannel audio tensor space is established with three factors (channel, time, and frequency) and is decomposed into the core tensor and three factor matrices based on tucker model. Only the truncated core tensor is transmitted to the decoder which is multiplied by the factor matrices trained before processing. The performance of the proposed method is evaluated with approximation errors, compression degree and listening tests. When the core tensor is smaller, the compression degree will be higher. A very noticeable compression capability will be achieved with an acceptable retrieved quality. The novelty of the proposed method is that it enables both high compression capability and backward compatibility with little signal distortion to the hearing.	approximation;backward compatibility;distortion;signal compression;surround sound;tucker decomposition	Jing Wang;Chundong Xu;Xiang Xie;Jingming Kuang	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6637654	data compression;speech recognition;tensor;theoretical computer science;speech coding;mathematics	Robotics	47.61458268160957	-9.088492050435445	35366
8b51ea6a3952d7a170d3cf2c6375a72114f28238	optimization of parametric companding function for an efficient coding	optimal method;transform coding;scalar quantization;source code;vector quantizer;information theory	Designing a lossy source code remains one of the important topics in information theory, and has a lot of applications. Although plain vector quantization (VQ) can realize any fixed-length lossy source coding, it has a serious drawback in the computation cost. Companding vector quantization (CVQ) reduces the complexity by replacing vector quantization with a set of scalar quantizations. It can represent a wide class of practical VQs, while the structure in CVQ restricts it from representing every lossy source coding. In this article, we propose an optimization method for parametrized CVQ by utilizing a newly derived distortion formula. To test its validity, we applied the method especially to transform coding. We found that our trained CVQ outperforms Karhunen-Loeve transformation (KLT)-based coding not only in the case of linear mixtures of uniform sources, but also in the case of low bit-rate coding of a Gaussian source.	companding	Shin-ichi Maeda;Shin Ishii	2007		10.1007/978-3-540-69158-7_74	mathematical optimization;transform coding;shannon–fano coding;quantization;information theory;variable-length code;computer science;theoretical computer science;mathematics;vector quantization;statistics;source code	Crypto	47.96803658829095	-12.210698550098039	35368
ff6eb71ac646abd73330d7abc3d0fc50aa012f5b	efficient adaptive vector quantization of lpc parameters	code vectors efficient adaptive vector quantization lpc parameters lsf parameters speech coding codebook partition delete operation;speech coding;adaptive codes;adaptive vector quantization;linear predictive coding;vector quantization linear predictive coding speech coding frequency measurement speech analysis bit rate telecommunication standards weight measurement signal analysis proposals;vector quantisation speech coding linear predictive coding adaptive codes;vector quantisation;subjective evaluation	This correspondence presents a new two-stage adaptive vector quantizer of LSF parameters in LPC speech coding. The first codebook is adapted by a partition-delete operation, whereas the code-vectors of the second codebook remain unchanged. The objective and subjective evaluations show that the proposed scheme offers transparent quantization with 22 b/frame. >	vector quantization	Miguel Angel Ferrer-Ballester;Aníbal R. Figueiras-Vidal	1995	IEEE Trans. Speech and Audio Processing	10.1109/89.397097	linear predictive coding;speech recognition;shannon–fano coding;learning vector quantization;vector sum excited linear prediction;harmonic vector excitation coding;variable-length code;computer science;speech coding;pattern recognition;mathematics;linde–buzo–gray algorithm;vector quantization;code-excited linear prediction	Vision	48.29549279049813	-9.082213105227352	35369
1119cbf4248bd844aea6bdec910e463be17d5e31	particle swarm optimization for digital image watermarking	digital watermark;wavelet packet;particle swarm optimizer;digital image watermarking;image watermarking;optimal algorithm	The trade-off between the imperceptibility and robustness is one of the most difficult challenges in digital watermarking system. To solve the problem, an optimal algorithm for image watermarking is proposed. The algorithm embeds the watermark by quantizing the wavelet packets coefficients of the image. In the proposed watermarking system, to protect the originality of the watermark image, a scrambled binary watermark embeds in the host image against intentional and unintentional attacks and each bit of the permuted watermark is embedded optimally into selected sub-bands of the decomposed host image by quantization of sub-bands coefficients. From experimental results, it demonstrates the robustness and the superiority of the proposed hybrid scheme.		Hai Tao;Jasni Mohamad Zain;Ahmed N. Abd Alla;Hongwu Qin	2010		10.1007/978-3-642-17622-7_1	computer vision;theoretical computer science;watermark	EDA	40.90188374100988	-10.094425846577916	35391
92a369db59e1fea93fa0c9fe44a09dfbe80b5b56	fast recursive portfolio optimization	backtesting;portfolio optimization;mathematical programming;quadratic optimization;algorithmic finance;covariance estimation;computational finance	Institutional equity portfolios are typically constructed via taking expected stock returns and then applying the computationally expensive processes of covariance matrix estimation and mean-variance optimization. Unfortunately, these computational costs make it prohibitive to comprehensively backtest and tune higher frequency strategies over long histories. In this paper, we introduce a recursive algorithm which significantly lowers the computational cost of calculating the covariance matrix and its inverse as well as an iterative heuristic which provides a very fast approximation to mean-variance optimization. Together, these techniques cut backtesting time to a fraction of that of standard techniques. Where possible, the additional step of caching pre-calculated covariance matrices, can result in overall backtesting speeds up to orders of magnitude faster than the standard methods. We demonstrate the efficacy of our approach by selecting a prediction strategy in a fraction of the time taken by standard methods.	algorithm;algorithmic efficiency;analysis of algorithms;approximation;backtesting;computation;heuristic;iterative method;mathematical optimization;recursion (computer science)	Laurence Irlicht	2014	Algorithmic Finance	10.3233/AF-140038	financial economics;econometrics;mathematical optimization;mathematics	ML	29.870067687950264	-13.950215260987541	35425
1b4846ab5dfe261c4db70f625dbadb3eb4564184	multi-robot path planning based on cooperative co-evolution and adaptive cga	self adaptive crossover multirobot path planning cooperative coevolution mobile robots multiobjective optimized algorithm shortest path length minimum time cost robot collide free mutation rate;shortest path;evaluation function;mobile robot;path planning;path planning mobile robots multi robot systems;multiobjective optimized algorithm;mutation rate;multi objective optimization;mobile robots;agent communication;path planning robot kinematics mobile robots cost function biological cells orbital robotics artificial intelligence constraint optimization genetic mutations chaos;robot collide free;multirobot path planning;multi agent systems;self adaptive crossover;minimum time cost;multi robot systems;shortest path length;dynamic simulation;cooperative coevolution;teamwork;cooperative co evolution	Multi-robot path planning is a challenge for mobile robots in AI. Multi-objective optimized algorithm based on cooperative co-evolution and CGA is brought up in this paper. Shortest path length, minimum time cost, smoothest and limited speed, obstacle-collide free and robot-collide free are the objectives and constraints to optimize. Linear combination of them is designed as evaluation function for CGA with self-adaptive crossover and mutation rate, combined with chaos disturbs. Finally 2D dynamic simulation has proved the efficiency of the algorithm.	algorithm;color graphics adapter;evaluation function;instruction path length;mobile robot;motion planning;shortest path problem;simulation;smoothing	Dongyong Yang;Jinyin Chen;Naofumi Matsumoto;Yuzo Yamane	2006	2006 IEEE/WIC/ACM International Conference on Intelligent Agent Technology	10.1109/IAT.2006.94	mathematical optimization;simulation;engineering;artificial intelligence	Robotics	30.64351201627993	-2.9743933367870175	35459
03ca4f21d872693a327e8f1cd98bb93136c36d88	incremental map generation (img)	computer aided design;degree of freedom;selective sampling;evaluation criteria;motion planning;probabilistic roadmap method	Automatic motion planning has applications ranging from traditional robotics to computer-aided design to computational biology and chemistry. Probabilistic roadmap methods (prms) have been highly successful in solving many of these high degree of freedom problems. One important practical issue with prms is they do not provide an automated mechanism to determine what size roadmap to construct. Instead, users typically determine an appropriate roadmap size by trial and error and often construct larger maps than needed or build several maps before obtaining one that meets their needs. In this paper, we propose a new prm-based framework called Incremental Map Generation (img) to address this problem. Our strategy is to break the map generation into several independent processes, each of which generates samples and connections independently. img proceeds by adding these collections of samples and connections to an existing roadmap until it satisfies some specified evaluation criteria. We propose some general evaluation criteria and show how they can be used to construct different types of roadmaps, e.g., roadmaps that coarsely or more finely map the space. In addition to addressing the roadmap size question, the fact that each roadmap increment is independently and deterministically seeded has several other benefits such as supporting roadmap reproducibility, the adaptive selection of sampling methods in different roadmap increments, and parallelization. We provide results illustrating the power of img. “Incremental Map Generation (IMG)”, Xie et al. TR06-005, Parasol Lab, Texas A&M, March 2006 1	common criteria;computational biology and chemistry;computer-aided design;deterministic algorithm;img;map;monte carlo method;motion planning;parallel computing;plan;probabilistic roadmap;robotics;sampling (signal processing);x image extension	Dawen Xie;Marco Morales;Roger A. Pearce;Shawna L. Thomas;Jyh-Ming Lien;Nancy M. Amato	2006		10.1007/978-3-540-68405-3_4	computer vision;simulation;computer science;machine learning	Robotics	24.910246227601725	-16.01575853967703	35632
b9a318d190a0ffb5cff1c9f5eb92954de387f424	hybrid biogeography algorithm for reducing power consumption in cloud computing		This paper purposes a hybrid biogeography algorithm that is the combination of biogeography and particle swarm optimization algorithm. A hybrid algorithm performs mutation, based on velocity instead of random mutation. The simulation results show that our proposed scheduling algorithm reduces the energy footprint of cloud data centers and improves a convergence rate of biogeography algorithm.	cloud computing;data center;hybrid algorithm;mathematical optimization;mutation (genetic algorithm);particle swarm optimization;rate of convergence;scheduling (computing);simulation;velocity (software development)	Sunita Singhal;Jyoti Grover	2017	2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2017.8125827	biogeography;computer science;rate of convergence;cluster analysis;algorithm;cloud computing;scheduling (computing);hybrid algorithm;particle swarm optimization	HPC	28.406045503073923	-3.566003424100887	35706
b3a886f5f0601652808b28129bffb3bebcf7c5fe	subdividing labeling genetic algorithm: a new method for solving continuous nonlinear optimization problems		In most global optimization problems, finding a global optimum point in the whole multi-dimensional search space implies a high computational burden. We present a new approach called subdividing labeling genetic algorithm (SLGA) for continuous nonlinear optimization problems. SLGA applies mutation and crossover operators on a subdivided search space where an integer label is defined on a polytope built on a n-dimensional space. After calculating the fitness of each point composing the polytope, SLGA implements a mutation operator to generate offspring and computes an integer label for the population of the polytope. Then, after completely labeling the polytope, a crossover operator is implemented so as to approach the optimum point by reducing the search space. In this regard, new population is generated by subdividing the search space and further implementing the mutation operator. SLGA has been used to optimize the De Jong functions, as well as nonlinear constrained and unconstrained problems with discrete, continuous and mixed variables. It has also been compared with other well-known algorithms. Experimental results show that the SLGA method has good performance and reduces the number of generations within the solution space, which enhances its convergence capability.	computation;crossover (genetic algorithm);feasible region;genetic algorithm;global optimization;mathematical optimization;mutation (genetic algorithm);nonlinear programming;nonlinear system;search algorithm	Majid Esmaelian;Francisco J. Santos-Arteaga;Madjid Tavana;Masoumeh Vali	2017	2017 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2017.7969388	machine learning;polytope;artificial intelligence;mathematical optimization;computer science;operator (computer programming);genetic algorithm;nonlinear programming;global optimization;crossover;convergence (routing);population	ML	24.897244922763125	-1.236826691733543	35719
5f1f85ae7e16da4504ffa5aa8f685734f2512d93	a hierarchical picture coding scheme	quadarbol;storage allocation;representation;hierarchy;image processing;procesamiento imagen;exploracion;pyramide;traitement image;algorithme;algorithm;estructura datos;jerarquia;balayage;transformation walsh hadamard;quad arbre;structure donnee;code;allocation memoire;scanning;quadtree;hierarchie;asignacion memoria;data structure;codigo;representacion;algoritmo	Abstract   The coding of the picture information is an important field for study with a great variety of applications. In this paper we present a hierarchical scheme for coding of a reduced amount of picture information, by combining efficiently two hierarchical data structures. In particular, the pyramid data structure is used to generate the pyramid of the average intensities of a picture  P ( N ,  N ). The regular decomposition method is applied on the above pyramid in order to reduce the picture information hierarchically. On the reduced picture information, an orthogonal transformation coding scheme is applied.		Nikolaos G. Bourbakis;Allen Klinger	1989	Pattern Recognition	10.1016/0031-3203(89)90079-4	combinatorics;data structure;image processing;computer science;quadtree;mathematics;pyramid;representation;code;algorithm;hierarchy	Vision	39.56028286200237	-18.613082384905464	35743
775966ec912032184c45add1cc41799ab18ea2db	hyper-erlang software reliability model	software fault detection patterns;model selection;generic model;delayed s shaped srm;software fault tolerance;hyper erlang software reliability model;non homogeneous poisson process;maximum likelihood estimation;fitting;computational modeling;maximum likelihood estimate;em algorithm software reliability non homogeneous poisson process hyper erlang distribution maximum likelihood estimation;nonhomogeneous poisson process modeling;shape;estimation;stochastic processes;expectation maximization;software reliability iterative algorithms stochastic processes parameter estimation inference algorithms maximum likelihood estimation reliability engineering software algorithms software measurement delay;stochastic processes expectation maximisation algorithm software fault tolerance;fault detection;expectation maximization algorithm;statistical inference;software algorithms;numerical experiment;hersrm;hyper erlang distribution;em algorithm;software reliability;statistical inference hyper erlang software reliability model hersrm nonhomogeneous poisson process modeling goel okumoto srm delayed s shaped srm software fault detection patterns expectation maximization em algorithm;goel okumoto srm;expectation maximisation algorithm	This paper proposes a hyper-Erlang software reliability model (HErSRM) in the framework of non-homogeneous Poisson process (NHPP) modeling. The proposed HErSRM is a generalized model which contains some existing NHPP-based SRMs like Goel-Okumoto SRM and Delayed S-shaped SRM, and can represent a variety of software fault-detection patterns. Such characteristics are useful to solve the model selection problem arising in the practical use of NHPP-based SRMs. More precisely, we discuss the statistical inference of HErSRM based on the EM (expectation-maximization) algorithm. In numerical experiments, we show that the HErSRM outperforms conventional NHPP-based SRMs with respect to fitting ability.	akaike information criterion;computation;erlang (programming language);expectation–maximization algorithm;experiment;model selection;numerical analysis;overfitting;selection algorithm;software quality;software reliability testing;system reference manual	Hiroyuki Okamura;Tadashi Dohi	2008	2008 14th IEEE Pacific Rim International Symposium on Dependable Computing	10.1109/PRDC.2008.20	stochastic process;mathematical optimization;expectation–maximization algorithm;computer science;statistics	Arch	30.403557243162197	-18.596172761175062	35786
4a9acaac39edff0565a8daa51c332b116d0a7d04	lion pride optimizer: an optimization algorithm inspired by lion pride behavior	swarm intelligence;animal behavior evolutionary algorithm lion pride optimization swarm intelligence;animal behavior;optimization;evolutionary algorithm;lion pride	In this paper, we report a novel optimization algorithm, lion pride optimizer (LPO), which is inspired by lion pride behavior. The framework is mainly based on lion prides’ evolution process and group living theory. In a lion pride, brutal competition of individuals happens among male lions both within and among prides; on the other hand, each member plays an important role in the persistence of a lion pride. Based on this framework, concepts from lion prides behavior, e.g., the strongest males occupy nearly all mating resources, and if a new cohort of males is able to take over a pride, they will seek to kill young cubs sired by their predecessors, are employed metaphorically to design optimum searching strategies for solving continuous optimization problems. From the studies of the algorithm property, it is found that the LPO algorithm is not sensitive to most parameters, which shows the robustness of the algorithm and the parameters are not problemdependent. Central tendency of the algorithm is not found. It is found that the pride update strategy and brutal competition of individuals are two main factors that contribute to the performance of LPO. According to the test results on 23 famous benchmark functions, the LPO algorithm has better performance than the other seven state-of-the-art algorithms on both unimodal and multimodal benchmark functions; in the test of high-dimensional multimodal problems, LPO outperforms the other five algorithms on all benchmark functions.	algorithm;benchmark (computing);columbus;complex event processing;continuous optimization;entity–relationship model;expectation propagation;front-end processor;http 404;mathematical optimization;maxima and minima;multimodal interaction;particle swarm optimization;path ordering (term rewriting);persistence (computer science);phase-shift oscillator;shin megami tensei: persona 3;software release life cycle;yao graph;yao's test	Bo Wang;Xiaoping Jin;Bo Cheng	2012	Science China Information Sciences	10.1007/s11432-012-4548-0	mathematical optimization;simulation;swarm intelligence;computer science;artificial intelligence;evolutionary algorithm	ML	25.426097119995323	-5.202287548805208	35808
a8f5c43a88d8b41101602069d6ef5772ccf0e5b1	algorithm 878: exact varma likelihood and its gradient for complete and incomplete data with matlab	verification;modelizacion;distributed system;vector autoregression;varma;fonction vraisemblance;metodo vectorial;systeme reparti;evaluation fonction;exact likelihood function;vector autoregressive moving average model;modelo autorregresivo;dato que falta;informacion incompleta;gradiente;arma model;gradient;time series;program verification;modelo arma;funcion verosimilitud;autoregressive model;donnee manquante;identificacion sistema;modelisation;incomplete information;incomplete data;verificacion programa;moving average;sistema repartido;analisis regresion;distributed lag model;system identification;function evaluation;seasonality;documentacion;vector method;information incomplete;serie temporelle;estimacion parametro;serie temporal;arma;algorithme em;modele arma;analyse regression;algorithms;methode vectorielle;regression analysis;algoritmo em;missing data;missing values;parameter estimation;estimation parametre;verification programme;modele autoregressif;em algorithm;modeling;likelihood function;identification systeme;documentation;time series model	Matlab functions for the evaluation of the exact log-likelihood of VAR and VARMA time series models are presented (vector autoregressive moving average). The functions accept incomplete data, and calculate analytical gradients, which may be used in parameter estimation with numerical likelihood maximization. Allowance is made for possible savings when estimating seasonal, structured or distributed lag models. Also provided is a function for creating simulated VARMA time series that have an accurate distribution from term one (they are spin-up free). The functions are accompanied by a a simple example driver, a program demonstrating their use for real parameter fitting, as well as a test suite for verifying their correctness and aid further development. The article concludes with description of numerical results obtained with the algorithm.	analytical engine;autoregressive model;correctness (computer science);device driver;distributed lag;estimation theory;expectation–maximization algorithm;gradient;matlab;numerical analysis;test suite;time series;verification and validation	Kristjan Jonasson	2008	ACM Trans. Math. Softw.	10.1145/1377603.1377609	econometrics;missing data;calculus;time series;mathematics;statistics	ML	33.68078812059199	-22.32479473872296	35816
74002022e16a82415e9fdc0500f4dcd5c142c1af	risk comparison of improved estimators in a linear regression model with multivariate t errors under balanced loss function		Under a balanced loss function, we derive the explicit formulae of the risk of the Stein-rule (SR) estimator, the#N#positive-part Stein-rule (PSR) estimator, the feasible minimum mean squared error (FMMSE) estimator, and the adjusted feasible#N#minimum mean squared error (AFMMSE) estimator in a linear regression model with multivariate  errors. The results show#N#that the PSR estimator dominates the SR estimator under the balanced loss and multivariate  errors. Also, our numerical results#N#show that these estimators dominate the ordinary least squares (OLS) estimator when the weight of precision of estimation is#N#larger than about half, and vice versa. Furthermore, the AFMMSE estimator dominates the PSR estimator in certain occasions.	loss function	Guikai Hu;Qingguo Li;Shenghua Yu	2014	J. Applied Mathematics	10.1155/2014/129205	efficient estimator;minimum mean square error;minimax estimator;shrinkage estimator;econometrics;mathematical optimization;james–stein estimator;minimum-variance unbiased estimator;estimator;bayes estimator;stein's unbiased risk estimate;ordinary least squares;trimmed estimator;efficiency;mathematics;mean squared error;first-difference estimator;bias of an estimator;orthogonality principle;consistent estimator;invariant estimator;hodges–lehmann estimator;statistics	Vision	29.727568659301504	-23.30107307930665	35821
f7a38f07719e768d096c6ae6eb3fc7487a48bc47	united coding method for compound image compression	dictionary entropy coding;compound image and video;hybrid coding;united coding;lossless coding	This paper proposes a compound image coding method named united coding (UC). In UC, several lossless coding tools such as dictionary-entropy coders, run-length encoding (RLE), Hextile, and a few filters used in portable network graphics (PNG) format are united into H.264 like intraframe hybrid video coding. The basic coding unit (BCU) has a size typically between 16 × 16 pixels to 64 × 64 pixels. All coders in UC are used to code each BCU. Then, the lossless coder that generates minimum bit-rate (R) is chosen as the optimal lossless coder. Finally, the final optimal coder is chosen from the lossy intraframe hybrid coder and the optimal lossless coder using R-D cost based optimization criterion. Moreover, the data coded by one lossless coder can be used as the dictionary of other lossless coders. Experimental results demonstrate that compared with H.264, UC achieves up to 20 dB PSNR improvement and better visual picture quality for compound images with mixed text, graphics and natural picture. Compared with lossless coders such as gzip and PNG, UC can achieve 2–5 times higher compression ratio with just a minor loss and keep partial-lossless picture quality. The partial-lossless nature of UC is indispensable for some typical applications, such as cloud computing and rendering, cloudlet-screen computing and remote desktop, where lossless coding of partial image regions is demanded. On the other hand, the implementation complexity and cost increment of UC is moderate, typically less than 25 % of a traditional hybrid coder such as H.264.	cloud computing;cloudlet;data compression;desktop computer;dictionary;entropy encoding;h.264/mpeg-4 avc;high efficiency video coding;ibm balanced configuration unit;idempotence;image compression;image quality;intra-frame coding;lempel–ziv–markov chain algorithm;lossless compression;lossy compression;mathematical optimization;netbsd gzip / freebsd gzip;peak signal-to-noise ratio;pixel;portable network graphics;remote desktop software;run-length encoding;uc browser	Shuhui Wang;Tao Lin	2012	Multimedia Tools and Applications	10.1007/s11042-012-1274-y	data compression;lossy compression;lossless jpeg;dictionary coder;speech recognition;telecommunications;computer science;entropy encoding;theoretical computer science;context-adaptive variable-length coding;tunstall coding;context-adaptive binary arithmetic coding	Graphics	45.12500369950662	-19.442505261155546	35887
ea70be1e40f5c09c0524ec23a9fa4dcce6faf719	novel up-sampling based watermarking multiple description coding frame	digital watermarking;multiple description coding mdc;数字水印;up sampling method multiple description coding mdc digital watermarking;up sampling method;上采样方法;会议论文;image watermarking discrete wavelet transforms image coding image sampling;watermarking decoding channel coding image coding image reconstruction multimedia communication;多描述编码;discrete wavelet transform upsampling based watermarking multiple description coding frame secret information dwt image	A novel watermarking scheme is proposed for up-sampling based multiple description coding frame in this paper. Secret information is embedded in the DWT image. Up-sampling algorithm is applied on transformed image to introduce some redundancy between different channels. Good performance of the new frame to against noise and to resist the compression attacks is shown in the experimental results.	algorithm;computational complexity theory;digital watermarking;discrete wavelet transform;embedded system;multiple description coding;sampling (signal processing);upsampling	Lin-Lin Tang;Jeng-Shyang Pan	2013	2013 Ninth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2013.20	computer vision;digital watermarking;computer science;electrical engineering;theoretical computer science;mathematics;multimedia	Robotics	40.767194831589876	-11.997164299785371	35941
3cb3e88a31a28c46273c456e7db6a0d9244521ba	minimization of dynamic sensor activation in discrete event systems for the purpose of control	minimisation;observability;modelizacion;metodo polinomial;minimization;largeur bande;reseau capteur;supervisory control centralized control decentralized control discrete event systems sensor activation;decentralized control problems;multiagent system;systeme evenement discret;supervisory control;algorithm complexity;programme commande;temps polynomial;sensors;observabilidad;informacion incompleta;complejidad algoritmo;closed feedback;dynamic sensor activation minimization;commande boucle fermee;polynomial complexity;observabilite;minimizacion;automaton;control centralizado;centralized control problems;partially observed discrete event systems;sensor activation;sistema acontecimiento discreto;decentralized system;identificacion sistema;modelisation;captador medida;incomplete information;automata;system trajectory dynamic sensor activation minimization partially observed discrete event systems centralized control problems decentralized control problems supervisory controllers feedback control law;discrete event system;feedback;measurement sensor;red sensores;capteur mesure;retroaccion;trajectory;decentralised control;complexite algorithme;system identification;position control;retroaction;polynomial method;control program;commande decentralisee;automate;bucle realimentacion cerrada;anchura banda;information incomplete;supervisory controllers;aerospace electronics;polynomial time;discrete event systems;feedback regulation;decentralized control;sensor array;sistema descentralizado;control decentralizado;feedback control law;commande centralisee;bandwidth;programa mando;partial observation;centralized control;sensor systems discrete event systems control systems observability distributed control bandwidth security feedback control automatic control automata;vehicles;systeme decentralise;sistema multiagente;methode polynomiale;system trajectory;modeling;supervision;sensors decentralised control discrete event systems feedback position control;feedback control	This paper considers centralized and decentralized control problems for partially-observed discrete event systems where sensor readings are assumed to be costly for reasons of bandwidth, energy, or security. The supervisory controllers, or agents, dynamically request sensors readings as needed to observe the trajectories of the system and correctly implement the given feedback control law. Thus, each sensor may be turned on/off several times along a given system trajectory. Different policies for dynamic sensor activation can be used by the agents. A set of policies is said to be minimal if any strictly less activation prevents the correct implementation of the control law. A systematic formulation of the dynamic sensor activation problem is proposed and its solution developed when the solution space is restricted to the set of transitions of a given automaton model of the system. Two algorithms that compute minimal sensor activation policies are presented, one for ensuring observability and the other for ensuring coobservability; observability and coobservability are key properties that arise in the solution of supervisory control problems for centralized and decentralized discrete event systems. These algorithms are of polynomial complexity in both the number of states and the number of events of the system.	algorithm;automaton;cardinality (data modeling);centralized computing;computation;distributed control system;feasible region;feedback;finite element method;mathematical optimization;max;optimal control;sensor;state space;subroutine;time complexity	Weilin Wang;Stéphane Lafortune;Feng Ying Lin;Anouck R. Girard	2010	IEEE Transactions on Automatic Control	10.1109/TAC.2010.2046062	control engineering;real-time computing;decentralised system;engineering;control theory;feedback;mathematics;automaton	Embedded	39.89483573901797	2.3571881590340564	35947
ba55562bdcae458c61617e706ebaa59aeee10c28	practical rate control algorithm for temporal scalability in scalable video coding	image coding;complexity theory;scalable video coding;complexity theory bit rate encoding image coding vectors conferences joints;joints;bit rate;video coding computational complexity statistical analysis;rate control;video coding;statistical analysis;vectors;temporal scalability;computational complexity;decision process;scalable video coding rate control temporal scalability;encoding;encoder complexity temporal scalability scalable video coding rate control algorithm hierarchical b picture complex interframe dependency q distance policy decision rule statistical smoothing effect gop based precise bit rate control decision process;decision rule;conferences	A rate control algorithm for hierarchical B-pictures in Scalable Video Coding (SVC) is proposed in this work. The complex inter-frame dependency issue is effectively addressed by the Q-distance policy decision rule while the statistical smoothing effect enables the GOP-based precise bit rate control. The simplicity of the decision processes greatly reduces the encoder complexity providing an efficient and effective rate control algorithm with hierarchical B-pictures. Experimental results verify the significant performance gain by the proposed algorithm.	algorithm;data compression;encoder;group of pictures;scalability;scalable video coding;smoothing	Jiaying Liu;Yongjin Cho;Zongming Guo	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6115767	scalable video coding;real-time computing;computer science;theoretical computer science;machine learning;decision rule;mathematics;computational complexity theory;encoding;statistics	Robotics	47.18679540779526	-18.057096052936274	36039
b8aac49e0a70d11e7fe93bc8e209ab973c7d3c02	the element-wise weighted total least-squares problem	measurement error model;iterative method;analyse multivariable;sample size;re weighted least squares;computacion informatica;variables;measurement error;multivariate analysis;non convex optimization;analisis datos;weighted least square;relacion convergencia;modele lineaire;optimization method;taux convergence;local convergence;convergence rate;modelo lineal;metodo optimizacion;iterative algorithm;unequally sized errors;metodo iterativo;optimization problem;condicion optimalidad;errors in variables model;data analysis;total least square;condition optimalite;grand echantillon;first order;estimation erreur;consistent estimator;ax;error estimation;least squares problem;ciencias basicas y experimentales;methode iterative;matematicas;statistical computation;linear model;estimacion error;calculo estadistico;problema minimos cuadrados;total least squares;estimacion parametro;methode optimisation;error in variable;multivariate errors in variables model;analisis multivariable;analyse donnee;linear convergence;calcul statistique;erreur dans variable;large sample;error;subject;parameter estimation;estimation parametre;grupo a;sista;convergence quadratique;consistency;estimateur convergent;optimality condition;optimisation non convexe;estimador convergente;algorithme iteratif;probleme moindre carre	A new technique is considered for parameter estimation in a linear measurement error model AX ≈ B,A=A0 + Ã,B =B0 + B̃,A0X0 =B0 with row-wise independent and non-identically distributed measurement errors̃ A, B̃. Here,A0 andB0 are the true values of the measurements A andB, andX0 is the true value of the parameter X. The total least-squares method yields an inconsistent estimate of the parameter in this case. Modified total least-squares problem, called element-wise weighted total least-squares, is formulated so that it provides a consistent estimator, i.e., the estimate X̂ converges to the true valueX0 as the number of measurements increases. The new estimator is a solution of an optimization problem with the parameter estimate X̂ and the correction D = [ A B], applied to the measured data D= [A B], as decision variables. An equivalent unconstrained problem is derived by minimizing analytically over the correction D, and an iterative algorithm for its solution, based on the first order optimality condition, is proposed. The algorithm is locally convergent with linear convergence rate. For large sample size the convergence rate tends to quadratic. © 2004 Elsevier B.V. All rights reserved.	algorithm;decision theory;errors-in-variables models;estimation theory;iterative method;local convergence;mathematical optimization;optimization problem;rate of convergence;total least squares	Ivan Markovsky;Maria Luisa Rastello;Amedeo Premoli;Alexander Kukush;Sabine Van Huffel	2006	Computational Statistics & Data Analysis	10.1016/j.csda.2004.07.014	econometrics;calculus;mathematics;iterative method;rate of convergence;statistics	ML	33.34391988875872	-23.661107637163205	36044
88eae8640b3f6861900a2e1446fe1b3be7eaf961	application of discrete ant colony optimization in vrptw		The classical ant colony algorithm for vehicle routing problem with time windows (VRPTW) has problems of low efficiency, slow convergence and prematurity. And the discrete ant colony optimization (DACO) is proposed for these problem. It adopts the one-dimensional discrete coding that can make the data structure simpler and bring in faster convergence speed. In addition, self-convergence mode is used to calculate the optimal vehicle number rather than setting the optimal vehicle number at the beginning, which makes the algorithm more flexible and accelerates the convergence speed effectively. The time window and vehicle load are not considered in the optimization process, when ants complete the whole process and then the path is explained, this move not only expands the ant search scope but also improves the efficiency of the algorithm. The above highlights make the most of the self-adaptation and self-regulating mechanism, which effectively reduces the probability of the local optimal solution at the same time. Experimental results for Solomon benchmark test problems indicate that DACO outperforms both in reducing time and space complexity in the premise of not affecting the accuracy. Thus proves DACO is effective and feasible in solving the VRPTW.		Qinhong Fu;Kang Zhou;Huaqing Qi;Tingfang Wu	2016		10.1007/978-981-10-3614-9_26	mathematical optimization;ant colony optimization algorithms;machine learning;vehicle routing problem;artificial intelligence;ant;computer science;data structure;convergence (routing)	Logic	27.924068129668516	-3.1697702432019463	36061
dc704f1fc52580d2672ebf46ea30d8aa1cfbdedd	low complexity h.264/avc spatial resolution transcoding in the transform domain	spatial resolution reduction;video coding computational complexity data compression transcoding;data compression;automatic voltage control lead prediction algorithms time division multiplexing encoding iir filters;prediction algorithms;video quality;low complexity;mpeg 4 h 264 avc;video coding;video transcoding;automatic voltage control;computational complexity;lead;transcoding architecture;time division multiplexing;optimal full decode full recode approach low complexity h 264 avc spatial resolution transcoding transform domain video transcoder h 264 avc low bit rate compressed images computational complexity transcoding algorithm;encoding;transcoding;iir filters;mpeg 4 h 264 avc video transcoding spatial resolution reduction transcoding architecture;spatial resolution	In this paper, we propose a novel efficient downsizing video transcoder for H.264/AVC low bit-rate compressed images. The algorithm accounts for new coding tools introduced by the H.264/AVC standard and applies directly on the transformed coefficients, thus reducing greatly the computational complexity of the transcoding algorithm. In a first approach, we restrict ourselves to 4×4 blocks intra-coded frames. Simulation results are presented to illustrate the performances of the proposed method. In particular, we show that the computational complexity is greatly reduced compared to the optimal full decode-full recode approach, while giving similar reconstructed video quality at low bit rates.	algorithm;coefficient;computational complexity theory;h.264/mpeg-4 avc;performance;simulation	Anne-Sophie Bacquet;Christophe Deknudt;Patrick Corlay;François-Xavier Coudoux;Marc Gazalet	2010	2010 17th IEEE International Conference on Electronics, Circuits and Systems	10.1109/ICECS.2010.5724556	scalable video coding;computer vision;real-time computing;computer science;theoretical computer science;context-adaptive variable-length coding	EDA	46.06932485823657	-17.796238685750815	36069
1842ac6da2bb7736119310ad8f58820507cf8c86	residual and confidence interval for uncertain regression model with imprecise observations				Waichon Lio;Baoding Liu	2018	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-18353	mathematics;machine learning;statistics;artificial intelligence;residual;regression analysis;confidence interval	Robotics	29.8515293571754	-23.584615590998652	36095
68b127f3e0748f119678638d12ac53e295064b97	improved latency and accuracy for neural branch prediction	eficacia sistema;prediction branchement;computacion informatica;learning;branch prediction;performance systeme;grupo de excelencia;system performance;aprendizaje;computer architecture;apprentissage;architecture ordinateur;machine learning;ciencias basicas y experimentales;arquitectura ordenador;reseau neuronal;red neuronal;industrial design;instructions per cycle;neural network;prediccion enchufe	Microarchitectural prediction based on neural learning has received increasing attention in recent years. However, neural prediction remains impractical because its superior accuracy over conventional predictors is not enough to offset the cost imposed by its high latency. We present a new neural branch predictor that solves the problem from both directions: it is both more accurate and much faster than previous neural predictors. Our predictor improves accuracy by combining path and pattern history to overcome limitations inherent to previous predictors. It also has much lower latency than previous neural predictors. The result is a predictor with accuracy far superior to conventional predictors but with latency comparable to predictors from industrial designs. Our simulations show that a path-based neural predictor improves the instructions-per-cycle (IPC) rate of an aggressively clocked microarchitecture by 16% over the original perceptron predictor. One reason for the improved accuracy is the ability of our new predictor to learn linearly inseparable branches; we show that these branches account for 50% of all branches and almost all branch mispredictions.	artificial neural network;branch predictor;clock rate;instructions per cycle;kerrison predictor;microarchitecture;perceptron;separable polynomial;simulation	Daniel A. Jiménez	2005	ACM Trans. Comput. Syst.	10.1145/1062247.1062250	real-time computing;industrial design;computer science;artificial intelligence;computer performance;algorithm;branch predictor;instructions per cycle	Arch	37.53485094684691	-2.2120463476601606	36113
7194442a60463e652954309d9ba34651349cabf3	progress in motion estimation for consumer video format conversion	tv signals motion estimation consumer video format conversion ic motion compensated video format conversion real time dsp software mc vfc;image matching motion estimation video signal processing consumer electronics television standards real time systems;data compression;motion compensation;video signal processing;image matching;real time;motion estimation digital signal processing pixel predictive models cameras displays laboratories filtering noise reduction interpolation;motion estimation;consumer electronics;motion compensated;motion compensation motion estimation consumer electronics video coding data compression;video coding;block based motion estimators parametric motion models consumer electronics applications application specific ic real time dsp software motion compensated consumer video format conversion video compression pixel based motion estimation;motion estimation iterative algorithms application software digital signal processing vectors design for disassembly convergence senior members laboratories consumer electronics;television standards;real time systems	Two generations of application specific ICs for motion compensated consumer video format conversion (MCVFC) are available, and real time DSP software for MCVFC has recently been demonstrated. The breakthroughs enabling this progress have come from motion estimation. The paper gives an overview of the progress achieved in motion estimation for consumer electronics applications.	algorithm;digital signal processor;motion estimation;object-based language;pixel;recursion (computer science);reed–solomon error correction;video coding format;video processing	Gerard de Haan	2000	IEEE Trans. Consumer Electronics	10.1109/30.883392	data compression;computer vision;electronic engineering;computer science;motion estimation;motion compensation;computer graphics (images)	Vision	44.0270585436853	-20.41982042812827	36184
30a39c8707498a0b33a25bde40539af30d0aed4e	rate distortion optimized slicing over bit error channels	desciframiento;dynamic programming;theorie vitesse distorsion;optimisation;programacion dinamica;optimizacion;decodage;decoding;transmission error;estimation method;information transmission;physical layer;localization;dynamic program;localizacion;segmentation;error transmision;video codec;rate distortion theory;computer programming;distortion;feedback;localisation;senal video;signal video;programmation dynamique;video transmission;video signal;codec;optimization;transmision informacion;video;transmission information;rate distortion optimization;segmentacion;erreur transmission	In this paper, we study the problem of optimizing slice sizes and locations for video transmission over bit error channels. We propose a method that utilizes estimation of slice rate and distortion which is a function of the inter-macroblock dependency as exploited in the video codec. First we experimentally show that. our estimation is effective. Since there are practically numerous possibilities for slice configurations and one must actually check all possibilities for a complete search, we assume segmentation of macroblocks so that a slice can only start at. the segments. Although this results in a slightly suboptimal solution, it reduces the number of possibilities. However there are still practically too many configurations. Then we use the proposed RD estimation method and combine it with a dynamic programming approach to effectively determine the most optimal slice configuration. RD optimization is carried out in terms of minimizing the expected Lagrangian using the expected distortion of a slice. We assume that the physical layer is capable of generating an error report showing which transmission units are received in error. This allows us to use NEWPRED with macroblock level feedback and prevent wrongful decoding of erroneous macroblocks. We compare the proposed system with the two common approaches namely, fixed number of MBs per slice and fixed number of bits per slice and see that the proposed method performs better than both.	bit error rate;distortion	Oztan Harmanci;A. Murat Tekalp	2005		10.1117/12.587445	electronic engineering;telecommunications;computer science;theoretical computer science	Theory	48.88089655708413	-14.993537799539478	36200
9299a13762bb40fe2c26ec0354c8c8aae9d6af15	application of nonlinear mixed-effects modeling approach in tree height prediction	random effects;model calibration;tree height diameter modeling;mixed effects	A nonlinear mixed-effects modeling approach was used to model the individual tree height–diameter relationship based on Chapman-Richards function for dahurian larch (Larix gmelinii. Rupr.) plantations in northeastern China. The study involved the estimation of fixed and random parameters, as well as procedures for determining random effects variance-covariance matrices to reduce the number of the parameters in the model. The mixed-effects model provided better model fitting and more precise estimations than the fixed-effects model. Techniques for calibrating the height-diameter model for a particular plot of interest were also explored. The greatest reductions in bias and root mean square error (RMSE) were obtained when comparing the calibration from one randomly selected tree with the calibration from two randomly selected trees. Substantial reductions were obtained with the inclusion of two randomly selected trees, which could reduce the bias and RMSE of the predictions by almost 73% and 63%, respectively. An important characteristic of mixed-effects models is that they permit both mean response prediction and calibrated prediction. The fixedeffects parameters alone can be used to obtain the mean response prediction. More accurate estimates can be obtained by calibration for individual prediction.	curve fitting;fixed effects model;larch family;mean squared error;mixed model;nonlinear system;random effects model;randomness;tree (data structure)	Lichun Jiang;Yaoxiang Li	2010	JCP	10.4304/jcp.5.10.1575-1581	econometrics;mathematical optimization;mathematics;statistics;random effects model	ML	26.9179827374261	-21.8890370370389	36211
e45a645c62ac004eb9aaa6a1b22c69de2d89da4d	searching for balance: understanding self-adaptation on ridge functions	algoritmo paralelo;adaptability;adaptabilite;parallel algorithm;estrategia optima;heuristic method;error sistematico;metodo heuristico;adaptabilidad;algorithme parallele;funcion escala;optimal strategy;fonction echelon;bias;step function;algorithme evolutionniste;algoritmo evolucionista;methode heuristique;information system;evolutionary algorithm;strategie optimale;systeme information;erreur systematique;adaptive evolution;sistema informacion	The progress rate of a self-adaptive evolution strategy is sub-optimal on ridge functions because the global step-size, denoted σ, becomes too small. On the parabolic ridge we conjecture that σ will stabilize when selection is unbiased towards larger or smaller step-sizes. On the sharp ridge, where the bias in selection is constant, σ will continue to decrease. We show that this is of practical interest because ridges can cause even the best solutions found by self-adaptation to be of little value on ridge problems where spatially close parameters tend to have similar values.		Monte Lunacek;L. Darrell Whitley	2006		10.1007/11844297_9	step function;adaptability;computer science;artificial intelligence;evolutionary algorithm;bias;calculus;parallel algorithm;information system;statistics	EDA	27.466831360879816	2.4547881893738683	36216
29ea6fd795334369b411c7550ea335e88d9d7b75	arm based microcontroller for image capturing in fpga design	semilla;field programmable gate array;diseno circuito;vision ordenador;dark current;detecteur image;image processing;sintesis control;sorting;convolution;semence;circuit design;procesamiento imagen;convolucion;tria;tecnologia mos complementario;red puerta programable;qualite image;reseau porte programmable;traitement image;system on a chip;computer vision;chip;cmos image sensor;sistema sobre pastilla;system on chip;filter;microregisseur;synthese commande;image quality;triage;controller design;cmos imager;filtre;procesador oleoducto;seed;vision ordinateur;conception circuit;calidad imagen;detector imagen;systeme sur puce;enseignement;microcontrolador;processeur pipeline;technologie mos complementaire;filtro;image sensor;control synthesis;pipeline processor;complementary mos technology;microcontroller;teaching;ensenanza	This paper presents how a ARM7 was designed by repeated adding group-instructions and the system was verified in a self-developed FPGA board. This ARM7 was then connected with a CMOS image capturing and processing unit (IPU) implemented in other FPGA board. ARM7 now become a microcontroller for this IPU. IPU integrates image capturing, convolution and sorting in FPGA to perform 3-stage pipelined operations to seed up system operations. Convolution and sorting help further filter out the Fixed Patten Noise (FPN) and dark current noise in CMOS image sensor and result in better image qualities. The FPGA board with ARM7 and IPU could be used for teaching CPU design, controller design and a system-on –chip (SoC) design since all circuits are in a single FPGA chip.	arm7;cmos;convolution;dark current (physics);field-programmable gate array;fixed-pattern noise;image sensor;microcontroller;processor design;sorting	Chi-Jeng Chang;Wu-Ting Wu;Hui-Ching Su;Zen-Yi Huang;Hsin-Yen Li	2005		10.1007/11595755_84	system on a chip;embedded system;computer hardware;image processing;computer science;image sensor;fpga prototype	EDA	41.737762247958	-3.9409724981009213	36226
f43416035b4280f3bd58750c39f0b35320d85737	chaotic grey wolf optimization algorithm for constrained optimization problems	chaotic grey wolf optimization;flower pollination algorithm;firefly algorithm;particle swarm optimization algorithm	The Grey Wolf Optimizer (GWO) algorithm is a novel meta-heuristic, inspired from the social hunting behavior of grey wolves. This paper introduces the chaos theory into the GWO algorithm with the aim of accelerating its global convergence speed. Firstly, detailed studies are carried out on thirteen standard constrained benchmark problems with ten different chaotic maps to find out the most efficient one. Then, the chaotic GWO is compared with the traditional GWO and some other popular meta-heuristics viz. Firefly Algorithm, Flower Pollination Algorithm and Particle Swarm Optimization algorithm. The performance of the CGWO algorithm is also validated using five constrained engineering design problems. The results showed that with an appropriate chaotic map, CGWO can clearly outperform standard GWO, with very good performance in comparison with other algorithms and in application to constrained optimization problems.	algorithm;constrained optimization;mathematical optimization	Mehak Kohli;Sankalap Arora	2018	J. Computational Design and Engineering	10.1016/j.jcde.2017.02.005	mathematical optimization;simulation;computer science;engineering;artificial intelligence;firefly algorithm	Theory	26.855541172416007	-4.0030451803456275	36289
0626fccd83f125e02e857c0567a66a97536bb39b	objective and subjective quality assessment between jpeg xr with overlap and jpeg 2000	overlap operator;stimulus comparison;jpeg xr;coding complexity;digital projection;jpeg 2000;vif;subjective assessment	JPEG XR (eXtended Range) is a recently standardized format for still images compression. It adopts a Lapped Biorthogonal Transform (LBT) that helps in reducing visual artifacts, in particular those due to blocking effects. In this paper, we compare JPEG XR with JPEG 2000, in terms of both objective and subjective visual quality. The adopted objective parameters are the computational complexity and the PSNR. In order to improve the analysis, and to evaluate if JPEG XR can be a feasible alternative to JPEG 2000, subjective tests in a projector-based environment have been set up, from which the benefits of the overlap operator of JPEG XR have been assessed, especially at high compression ratios.	jpeg 2000;jpeg xr	Federico Fiorucci;Giuseppe Baruffa;Fabrizio Frescura	2012	J. Visual Communication and Image Representation	10.1016/j.jvcir.2012.04.011	lossless jpeg;computer vision;computer science;jpeg;jpeg 2000;multimedia;quantization;variance inflation factor;computer graphics (images)	Vision	42.99993308335006	-16.968568260867123	36421
723bdb9bf925a370119d1ce8ba38e497ea37cd6a	cooperative simultaneous localization and synchronization in mobile agent networks	phase measurement;clocks;agent network network synchronization cooperative localization belief propagation message passing factor graph coslas;clocks message passing signal processing algorithms synchronization phase measurement estimation;estimation;synchronization;message passing;signal processing algorithms	Cooperative localization in agent networks based on interagent time-of-flight measurements is closely related to synchronization. To leverage this relation, we propose a Bayesian factor graph framework for cooperative simultaneous localization and synchronization (CoSLAS). This framework is suited to mobile agents and time-varying local clock parameters. Building on the CoSLAS factor graph, we develop a distributed (decentralized) belief propagation algorithm for CoSLAS in the practically important case of an affine clock model and asymmetric time stamping. Our algorithm is compatible with real-time operation and a time-varying network connectivity. To achieve high accuracy at reduced complexity and communication cost, the algorithm combines particle implementations with parametric message representations and takes advantage of a conditional independence property. Simulation results demonstrate the good performance of the proposed algorithm in a challenging scenario with time-varying network connectivity.	algorithm;belief propagation;cooperative mimo;factor graph;mobile agent;real-time clock;real-time computing;simulation;software propagation;time-varying network	Bernhard Etzlinger;Florian Meyer;Franz Hlawatsch;Andreas Springer;Henk Wymeersch	2017	IEEE Transactions on Signal Processing	10.1109/TSP.2017.2691665	synchronization networks;belief propagation;message passing;clock synchronization;synchronization;mobile agent;factor graph;parametric statistics;distributed computing;computer science	Mobile	53.13597491238188	2.204551618361435	36493
009dbdcc56abdefa84233070ddbffb7dc2b0bd06	fast wavelet packet basis selection for block-partitioning image coding	rate distortion basis;rate distortion;image coding;complexity theory;greedy tree growing algorithm;wavelet packets image coding rate distortion wavelet transforms cost function frequency entropy quantization optimization methods chaos;optimal tree pruning algorithm;entropy coding;wavelet transforms computational complexity entropy codes image coding tree searching waveform analysis;basis search procedure wavelet packet block partitioning image coding adaptive waveform analysis basis selection rate distortion basis computational complexity greedy tree growing algorithm optimal tree pruning algorithm quantization method entropy coding coding performance;coding performance;wavelet packet;block partitioning;wavelet transforms;computational complexity;entropy codes;basis selection;tree searching;quantization method;rate distortion optimization;waveform analysis;basis search procedure;adaptive waveform analysis	Wavelet packet provides an effective representative tool for adaptive waveform analysis of a given signal. To construct an appropriate basis, the basis-selection criterion must be carefully designed. A rate-distortion based basis selection method was proposed in recent years, which can produce an optimal basis in the rate-distortion sense. However, this approach is extremely computationally intensive. Other criterions, such as the entropy based method, do not always produce an effective basis. In this work, we propose a distortion based basis-selection criterion with moderate computational complexity. Two basis selection procedures, i.e. greedy tree growing algorithm and optimal tree pruning algorithm are both evaluated. The experimental results show that, when adopting the same quantization and entropy coding methods, the coding performance of the proposed scheme is inferior to that of the rate-distortion optimized basis by only 0.11dB on the average, while the basis search procedure is sped up by a factor of 10.1 averagely	audio signal processing;computational complexity theory;distortion;entropy encoding;greedy algorithm;network packet;quantization (signal processing);wavelet	Yongming Yang;Chao Xu	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1692958	mathematical optimization;computer science;entropy encoding;machine learning;pattern recognition;mathematics;rate–distortion optimization;computational complexity theory;algorithm;wavelet transform	Embedded	44.53460346381285	-13.677818266791908	36510
5bbeecb3cda79947a8fc48696714c22867138eee	on the security of a hybrid svd-dct watermarking method based on lpsnr	watermarking;proof of ownership;singular value decomposition;discrete cosine transform;attacks;ambiguity	Watermarking schemes allow a cover image to be embedded with a watermark, for diverse applications including proof of ownership and covert communication. In this paper, we present attacks on watermarking scheme proposed by Huang and Guan. This scheme is hybrid singular value decomposition (SVD) based scheme in the sense that they employ both SVD and other techniques for watermark embedding and extraction. By attacks, we mean that we show how the designers’ security claim, related to proof of ownership application can be invalidated. Our results are the first known attacks on this hybrid SVD-based watermarking scheme.	digital watermarking;discrete cosine transform;embedded system;scheme;singular value decomposition	Huo-Chong Ling;Raphael C.-W. Phan;Swee-Huay Heng	2011		10.1007/978-3-642-25367-6_23	discrete mathematics;digital watermarking;computer science;theoretical computer science;discrete cosine transform;mathematics;singular value decomposition;computer security	Crypto	40.43109410575326	-9.953808212667337	36599
534f35858c97a06215716d7c64c0a8978dcaa0f0	conditional prediction intervals for linear regression	probability;prediction intervals conditional inference linear regression markov chain monte carlo;training;prediction algorithms;linear regression;data mining;conditional inference;iid errors;coverage probability;natural sigma algebra;markov chain monte carlo;regression analysis;perfect calibration conditional prediction intervals linear regression iid errors natural sigma algebra invariant events;prediction intervals;extraterrestrial measurements;prediction interval;conditional prediction intervals;invariant events;linear regression probability machine learning predictive models statistics testing state estimation application software error analysis distributed computing;noise;perfect calibration	We construct prediction intervals for the linear regression model with IID errors with a known distribution, not necessarily Gaussian. The coverage probability of our prediction intervals is equal to the nominal confidence level not only unconditionally but also conditionally given a natural sigma-algebra of invariant events. This implies, in particular, the perfect calibration of our prediction intervals in the on-line mode of prediction.	online and offline	Peter McCullagh;Vladimir Vovk;Ilia Nouretdinov;Dmitry Devetyarov;Alexander Gammerman	2009	2009 International Conference on Machine Learning and Applications	10.1109/ICMLA.2009.115	econometrics;prediction interval;pattern recognition;mathematics;confidence and prediction bands;statistics	ML	29.891181069334916	-22.323031114416413	36608
33876191a8a565917f8c66bd8173a97184aacb1b	percolation analyses in a swarm based algorithm for shortest-path finding	shortest path;swarm intelligence;ant colony optimization;agents behavior;phase transition;percolation;percolation theory	"""In this paper we show that the convergence in the Ant Colony Optimization (ACO) algorithm can be described as a """"phase- transition"""" phenomenon. The analysis of the ACO with the Percolation Theory approach includes: the pheromone evaporation and the number of agents parameters, so, for a given routing environment, it is possible to select these parameters in order to ensure convergence and to avoid overhead in the algorithm. The objective of this work is to present some experiments that support our hypothesis and to show the methodology used to correlate some algorithm parameters and how they influence in its general performance."""	algorithm;ant colony optimization algorithms;evaporation;experiment;interpolation;overhead (computing);pathfinding;percolation theory;routing;shortest path problem;simulation;swarm	Bruno Panerai Velloso;Mauro Roisemberg	2008		10.1145/1363686.1364136	phase transition;mathematical optimization;ant colony optimization algorithms;swarm intelligence;computer science;artificial intelligence;machine learning;percolation;shortest path problem;percolation theory	ML	27.826372593800045	-1.758864137084699	36613
6e24443f19bb7b70a5271f837a4976d567255cea	lossless compression of color filter array mosaic images with visualization via jpeg 2000	image coding;image color analysis;transform coding;sensor arrays;visualization;interpolation;software	Digital cameras have become ubiquitous for amateur and professional applications. The raw images captured by digital sensors typically take the form of color filter array (CFA) mosaic images which must be “developed” (via digital signal processing) before they can be viewed. Photographers and scientists often repeat the “development process” using different parameters to obtain images suitable for different purposes. Since the development process is generally not invertible it is commonly desirable to store the raw (or undeveloped) mosaic images indefinitely. Uncompressed mosaic image file sizes can be more than 30 times larger than those of developed images stored in JPEG format. Thus data compression is of interest. Several compression methods for mosaic images have been proposed in the literature. However they all require a custom decompressor followed by development-specific software to generate a displayable image. In this paper a novel compression pipeline that removes these requirements is proposed. Specifically mosaic images can be losslessly recovered from the resulting compressed files and more significantly images can be directly viewed (decompressed and developed) using only a JPEG 2000 compliant image viewer. Experiments reveal that the proposed pipeline attains excellent visual quality while providing compression performance competitive to that of state-of-the-art compression algorithms for mosaic images.	algorithm;color filter array;data compression;digital camera;digital signal processing;image file formats;image viewer;jpeg 2000;lossless compression;ncsa mosaic;requirement;sensor	Miguel Hern&#x00E1;ndez-Cabronero;Michael W. Marcellin;Ian Blanes;Joan Serra-Sagrist&#x00E0;	2018	IEEE Transactions on Multimedia	10.1109/TMM.2017.2741426	computer vision;lossy compression;image compression;artificial intelligence;computer science;data compression;computer graphics (images);color cell compression;lossless jpeg;jpeg 2000;lossless compression;jpeg	Visualization	42.458822647354225	-19.409289052783596	36626
87eb5ae2cf2a59e9bd0838ac4a2bc576498cebb0	analog vlsi circuits for primitive sensory attention	very large scale integration image coding analog computers feedback circuits image converters decision making hysteresis motors biological systems eyes tracking;array signal processing;decision circuits analogue processing circuits vlsi circuit feedback array signal processing optical information processing;decision circuits;circuit feedback;optical information processing;nonlinear gain analog vlsi circuits primitive sensory attention feedback winner take all circuit single location selection sensory array local hysteresis visual data processing current mode circuits distributed hysteresis decision making scheme;vlsi;analog vlsi;winner take all;analogue processing circuits	Circuitry that performs primitive attention tasks is presented. With the addition of feedback to a winner-take-all circuit we are able to mediate the selection of a single location within a sensory array through the use of hysteresis.	digital-to-analog converter;electronic circuit;hysteresis	Stephen P. DeWeerth;Tonia G. Morris	1994		10.1109/ISCAS.1994.409637	winner-take-all;control engineering;electronic engineering;computer science;engineering;electrical engineering;control theory;very-large-scale integration	HCI	41.28147247617807	-2.471802129013996	36709
fd048499e51bbe902a03d6ed6054d25ddd73d0b2	nonparametric confidence bands for a quantile comparison function	test kolmogorov smirnov;approximation asymptotique;adaptacion;metodo estadistico;confidence band;kolmogorov smirnov test;nonparametric;copula;statistical method;ley 2 variables;bande confiance non parametrique;quantile matching function;methode statistique;adaptation;bivariate distribution;asymptotic approximation;fonction adaptation quantile;loi 2 variables;aproximacion asintotica	This article develops statistical methods useful for comparing the marginals F and G of a bivariate distribution. Such comparisons are appropriate in the analysis of matched-pair data involving a treatment and a control (or two treatments). The quantile comparison functionq = G−1(F), proposed by Lehmann as a generalized measure of treatment effect in fully randomized experiments, is shown to be applicable and also useful in the analysis of matched-pair data. Nonparametric simultaneous confidence bands for q are constructed using asymptotic and permutation methods. The copula of the bivariate distribution plays a significant role in the methodology. Application of the results is illustrated on a dataset from an industrial experiment.		Fred Lombard	2005	Technometrics	10.1198/004017005000000184	kolmogorov–smirnov test;nonparametric statistics;econometrics;copula;calculus;mathematics;joint probability distribution;confidence and prediction bands;statistics;adaptation	Vision	32.692877624950775	-22.411602178175066	36765
1540992576da1bb01a4ab6cd64dbc4960aefd209	measuring mobility and the performance of global search algorithms	mobility;search algorithm;global search;heuristic search;evolutionary algorithms;evolutionary algorithm;local search	The global search properties of heuristic search algorithms are not well understood. In this paper, we introduce a new metric, mobility, that quantifies the dispersion of local optima visited during a search. This allows us to explore two questions: How disperse are the local optima visited during a search? How does mobility relate to algorithm performance? We compare local search with two evolutionary algorithms, CHC and CMA-ES, on a set of non-separable, non-symmetric, multi-modal test functions. Given our mobility metric, we show that algorithms visiting more disperse local optima tend to be better optimizers.	cma-es;distribution (mathematics);evolution strategy;evolutionary algorithm;file spanning;genetic algorithm;heuristic;image scaling;local optimum;local search (optimization);minimum spanning tree;modal logic;search algorithm	Monte Lunacek;L. Darrell Whitley;James N. Knight	2005		10.1145/1068009.1068208	beam search;mathematical optimization;beam stack search;tabu search;computer science;artificial intelligence;local search;hill climbing;machine learning;evolutionary algorithm;iterated local search;mathematics;incremental heuristic search;iterative deepening depth-first search;best-first search;combinatorial search;fringe search;guided local search;search algorithm	AI	25.02407590011945	-2.2267967083305584	36787
662def8a535525ee0f59cf46043adb53ec2f637e	fast search algorithms for vector quantization and pattern matching	image coding;vector quantization pattern matching testing search methods signal processing algorithms computational complexity area measurement electric variables measurement speech recognition image coding;search algorithm;search methods;low complexity;testing;vector quantization;computational complexity;pattern matching;signal processing;speech recognition;area measurement;vector quantizer;signal processing algorithms;electric variables measurement	Department of Electrical and Computer Engineering University of California Santa Barbara, CA 93106 A fundamental computational task that arises in several areas of signal processing is pattern matching, where a given test pattern is compared with a large set of stored templates. to find the best match that mipjimizes a given measure of dissimilarity. Three different geometrically-oriented methods are proposed for substantially reducing the computational complexity of the search process by reducing the number of multiplies in exchange for additiohal low complexity operations and, in two of the methods, additional memory for storing precomputed tables.	computation;computational complexity theory;computer engineering;pattern matching;precomputation;search algorithm;signal processing;test card;vector quantization	De-Yuan Cheng;Allen Gersho;Bhaskar Ramamurthi;Yair Shoham	1984		10.1109/ICASSP.1984.1172352	computer science;theoretical computer science;machine learning;pattern matching;signal processing;pattern recognition;software testing;computational complexity theory;vector quantization;search algorithm	ML	50.16563190634115	-11.405810589332901	36908
95638b6a5c2cc61bd83d9457c581c66f16644e1c	performing deblocking in video coding based on spatial-domain motion-compensated temporal filtering	estensibilidad;modelizacion;metodo adaptativo;filtering;discontinuity;vision ordenador;filtrage;discontinuite;low bit rate;image coding;scalable video coding;image processing;data compression;motion compensation;filtrado;video compression;procesamiento imagen;overlapped block motion compensation;methode adaptative;intelligence artificielle;visual quality;traitement image;processing time;velocidad de bit debil;computer vision;motion compensated temporal filtering;modelisation;compensation mouvement;codage image;video coding;compression image;image compression;codage video;adaptive method;filtro adaptable;temps traitement;artificial intelligence;vision ordinateur;discontinuidad;extensibilite;scalability;compresion dato;inteligencia artificial;filtre adaptatif;modeling;tiempo proceso;debit binaire faible;adaptive filter;compression donnee;compresion imagen	Employing block-based motion models in scalable video coding based on spatial-domain motion-compensated temporal filtering (SDMCTF) introduces artificial block-boundary discontinuities that adversely affect the video compression performance, particularly at low bit-rates. This paper focuses on the problem of deblocking in the context of SDMCTF-based video coding. One possible solution to this problem is the use of overlapped-block motion compensation (OBMC). An alternative solution is applying an adaptive deblocking filter, similar to the one used in H.264. With this respect, a novel adaptive deblocking filter, tailored to SDMCTF video coding is proposed. In terms of visual-quality, both approaches yield similar performance. However, adaptive deblocking is less complex than OBMC, as it requires up to 34% less processing time. Experimental results show that the two techniques significantly improve the subjective and objective quality of the decoded sequences, confirming the expected benefits brought by deblocking in SDMCTF-based video coding.	deblocking filter	Adrian Munteanu;Joeri Barbarien;Jan Cornelis;Peter Schelkens	2006		10.1007/11864349_33	data compression;computer vision;simulation;image processing;computer science;deblocking filter;computer graphics (images)	Vision	46.471753989644554	-14.914472484135397	36912
bc588c85dd8043f0c40fb564a905906ba04106d6	adaptive and proadaptive image compression	image compression	"""The most of well-known algorithms on the basis of the """"past"""" (processed) samples make the local properties of the image more precise and adjust some own parameters. At decoding of the image on the basis of restored samples the same decisions are taken. It is the classical adaptive scheme. In the offered proadaptive approach for tuning parameters of algorithm the prolongation of a signal is used. For estimate of statistical properties of an encoded (current) sample are used both """"past"""", and """"future"""" samples and even current sample. Certainly, the reliability of the estimation will increase. But it is necessary to pay for information on """"future"""" – to save some additional information. The proadaptive algorithm tunes the parameters only at encoding, and at decoding the saved values of parameters are used. The more size of additional data, the more often and/or precisely are adjusted parameters of algorithm and the better efficiency of encoding of the main data stream. If parameters of the coder are adjusted in each image fragment by size W×W, the entropy of the code words in the main and additional data streams will depend from the size of the image fragment like in figure 1. Under some conditions their sum has an optimum. The experimental probing on the real and artificial images have shown that the optimum is present always, but the conditions for its achievement essentially depend of the image context. I"""	algorithm;code word;denavit–hartenberg parameters;image compression	V. N. Oulianov	2001		10.1109/DCC.2001.10045	data compression;lossy compression;data compression ratio;pyramid;adaptive scalable texture compression;block truncation coding;image compression;computer science;mathematics;fractal transform;texture compression;algorithm	ML	41.39290252397689	-16.595968742409724	36973
e3eb06630547942c72cc518910e095db9cec312d	on the performance of selective adaptation in state lattices for mobile robot motion planning in cluttered environments		Autonomous mobile robots require motion planning algorithms that match limitations of on-board computing resources to safely navigate complex environments. In situations where near-optimality is preferential to runtime performance, search spaces that optimize their local connectivity to improve the global optimality of generated solutions are desirable. However, not all nodes in the search space benefit equally from optimization which can result in an inefficient use of computational resources and unnecessary increase in runtime. To address this limitation, we propose an approach called the Selectively Adaptive State Lattice which uses a heuristic based on the local environment to selectively perform optimization and obtain a balance between runtime performance and relative optimality. We present a statistical evaluation of local connectivity optimization and global search with the State Lattice, Adaptive State Lattice, and Selectively Adaptive State Lattice algorithms in randomly generated obstacle fields. We further highlight the performance of each method on a Clearpath Robotics TurtleBot2 in a qualitative physical experiment.	algorithm;autonomous robot;computational resource;experiment;global optimization;heuristic;mathematical optimization;memory footprint;mobile robot;motion planning;on-board data handling;planetary scanner;procedural generation;recombinant dna;requirement;robotics;run time (program lifecycle phase);sasl;sl (complexity);sampling (signal processing);simulation	Michael E. Napoli;Harel Biggie;Thomas M. Howard	2017	2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2017.8206309	control engineering;computer science;mobile robot;lattice (order);obstacle;distributed computing;heuristic;motion planning;artificial intelligence;robotics	Robotics	52.68517880941512	-23.888592209318336	36999
bf6d37718c13a73bd973d882b51d7d3b95a9dba8	a biophysically based dendrite model using programmable floating-gate devices	silicon;flexible electronics;neural computation;transistor channel models;programmable floating gate devices;neural nets;conventional neurobiological modeling;floating gate;engines silicon;real time computational engine;elemental semiconductors;low power;channel model;engines;transistors;programmable circuits;semiconductor device models;silicon model;low power electronics;biophysically based dendrite model;si;si biophysically based dendrite model programmable floating gate devices transistor channel models real time computational engine neural computation silicon model conventional neurobiological modeling;transistors elemental semiconductors flexible electronics low power electronics neural nets programmable circuits real time systems semiconductor device models silicon;real time computing;real time systems	This paper presents our programmable floating-gate dendrite model based upon our transistor channel models. We are developing a flexible, low-power, real-time computational engine for studying, understanding, and implementing neural computation. Here we present a systematic study of the dynamics seen in this silicon model, emphasizing the connection to conventional neurobiological modeling of dendrites. These results provide a foundation for future work on larger systems.	artificial neural network;computation;low-power broadcasting;real-time clock;real-time computing;transistor	Stephen Brink;Scott Koziol;Shubha Ramakrishnan;Paul E. Hasler	2008	2008 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2008.4541447	electronic engineering;simulation;computer science;engineering;electrical engineering;flexible electronics;silicon;artificial neural network;transistor;low-power electronics;models of neural computation	Embedded	39.99912763132724	-0.9478610801676551	37044
8ae981d917b1513bda67927e2147d66f0e40f799	efficient genetic algorithm for high-dimensional function optimization	timely mutation operator;high dimensional function optimization;genetic algorithm;subspace crossover;genetic algorithms;dual population genetic algorithm based on periodic slow change in radius parameter efficient genetic algorithm high dimensional function optimization ega local search ability subspace crossover operator mutation operator randomization elite solutions analysis solution quality standard ga prpdpga;high dimensional function optimization genetic algorithm subspace crossover timely mutation operator;bismuth computational intelligence security	An Efficient Genetic Algorithm(EGA) proposed in this paper was aiming to high-dimensional function optimization. To generate multiple diverse solutions and to strengthen local search ability, the new subspace crossover and timely mutation operators improved by us will be used in EGA. The combination of the new operators allow the integration of randomization and elite solutions analysis to achieve a balance of stability and diversification to further improve the quality of solutions in the case of high-dimensional functions. Standard GA and PRPDPGA proposed already were compared in simulation. Computational studies of benchmark by testing optimization functions suggest that the proposed algorithm was able to quickly achieve good solutions while avoiding being trapped in premature convergence.	benchmark (computing);computation;diversification (finance);enhanced graphics adapter;genetic algorithm;local search (optimization);mathematical optimization;premature convergence;randomized algorithm;simulation;software release life cycle	Qifeng Lin;Wei Liu;Hongxin Peng;Yuxing Chen	2013	2013 Ninth International Conference on Computational Intelligence and Security	10.1109/CIS.2013.60	mathematical optimization;meta-optimization;genetic algorithm;computer science;artificial intelligence;genetic operator;machine learning;mathematics;algorithm;population-based incremental learning	EDA	25.838061762738608	-3.29252249021711	37050
72484cf39a00ba029a6f41d1c5c018d6a7c7204b	rank-based image watermarking method with high embedding capacity and robustness	engineering;digital watermarking;watermarking;host signal interference;spread spectrum;technology;quantization signal;interference;journal article;discrete cosine transform;quantization based watermarking;science technology;discrete cosine transforms;feature extraction;multimedia communication;high embedding capacity;high embedding capacity image watermarking host signal interference discrete cosine transform;robustness;image watermarking;computer science;computer science information systems;watermarking discrete cosine transforms robustness quantization signal multimedia communication feature extraction interference;engineering electrical electronic;error buffer image watermarking 2d discrete cosine transform dct coefficients secret key image block rank based embedding rule watermark bits detection matrices;index modulation;telecommunications;matrix algebra discrete cosine transforms image watermarking	This paper presents a novel rank-based method for image watermarking. In the watermark embedding process, the host image is divided into blocks, followed by the 2-D discrete cosine transform (DCT). For each image block, a secret key is employed to randomly select a set of DCT coefficients suitable for watermark embedding. Watermark bits are inserted into an image block by modifying the set of DCT coefficients using a rank-based embedding rule. In the watermark detection process, the corresponding detection matrices are formed from the received image using the secret key. Afterward, the watermark bits are extracted by checking the ranks of the detection matrices. Since the proposed watermarking method only uses two DCT coefficients to hide one watermark bit, it can achieve very high embedding capacity. Moreover, our method is free of host signal interference. This desired feature and the usage of an error buffer in watermark embedding result in high robustness against attacks. Theoretical analysis and experimental results demonstrate the effectiveness of the proposed method.	coefficient;digital watermarking;discrete cosine transform;interference (communication);key (cryptography);randomness	Tianrui Zong;Yong Xiang;Song Guo;Yue Rong	2016	IEEE Access	10.1109/ACCESS.2016.2556723	computer vision;discrete mathematics;telecommunications;digital watermarking;computer science;electrical engineering;theoretical computer science;mathematics;watermark	EDA	40.805242198766834	-10.580804297270056	37072
c16298b060c40552b07a17751e1dab034b80ea45	a comparison of normal approximation rules for attribute control charts	l6 industry studies manufacturing;mathematical and simulation modeling;specific statistics;statistical process control;attribute control chart;statistical decision theory;c44 operations research;c63 computational techniques;c46 specific distributions;programming models;binomial distribution;p chart;c6 mathematical methods;simulation modeling;np chart	Control charts, known for more than 80 years, have been important tools for business and industrial manufactures. Among many different types of control charts, the attribute control chart (np-chart or p-chart) is one of the most popular methods to monitor the number of observed defects in products, such as semiconductor chips, automobile engines, and loan applications. The attribute control chart requires that the sample size n is sufficiently large and the defect rate p is not too small so that the normal approximation to the binomial works well. Some rules for the required values for n and p are available in the textbooks of quality control and mathematical statistics. However, these rules are considerably different and hence it is less clear which rule is most appropriate in practical applications. In this paper, we perform a comparison of five frequently used rules for n and p required for the normal approximation to the binomial. Based on this result, we also refine the existing rules to develop a new rule that has a reliable performance. Datasets are analyzed for illustration.	approximation;chart;national supercomputer centre in sweden;semiconductor;software bug	Takeshi Emura;Yi-Shuan Lin	2015	Quality and Reliability Eng. Int.	10.1002/qre.1601	econometrics;western electric rules;np-chart;computer science;binomial distribution;operations management;simulation modeling;data mining;p-chart;mathematics;programming paradigm;shewhart individuals control chart;statistical process control;statistics	DB	29.377845557571057	-18.569087175097696	37174
139a4e224db3b581f502103b9be8dc6f3e49cff5	reliability analysis of the harmonic mean inactivity time order	convolution;weighted distributions harmonic mean inactivity time order reliability analysis nonnegative random variables stochastic order convolution model shock models;weighted distributions convolution mean inactivity time order mixture random sum reversed hazard rate order shock models;reliability theory;stochastic processes;random variables electric shock harmonic analysis distribution functions hazards reliability stochastic processes;stochastic processes convolution harmonic analysis reliability theory;harmonic analysis	Based on the comparison of a certain function of mean inactivity times of two nonnegative random variables, we introduce and study a new stochastic order. Several elementary and then basic preservation properties of the new stochastic order under reliability operations of convolution, mixture, and shock models are discussed. We also derive characterizations of some well-known stochastic orders by the new order, and point out some results related to the weighted distributions. Some examples are included to illustrate the concepts.	consistency model;convolution;reliability engineering	S. Izadkhah;M. Kayid	2013	IEEE Transactions on Reliability	10.1109/TR.2013.2255793	stochastic process;mathematical optimization;mathematical analysis;reliability theory;convolution of probability distributions;stochastic optimization;harmonic analysis;mathematics;convolution;statistics	Vision	31.52650490593133	-18.689849140406107	37223
290c33544ecb28b8e414aa2a0f93a77ff4a1ad17	a comparison of risk difference estimators in multi-center studies under baseline-risk heterogeneity	estimator efficiency;estimacion sesgada;risk difference;theorie echantillonnage;teoria muestreo;sample size;small sample;analyse survie;statistical simulation;erreur moyenne;covariance analysis;pooling of sparse clinical trials;estrategia optima;small sample size;tamano muestra;mataanalisis;simulation;erreur quadratique moyenne;variance analysis;error sistematico;taille echantillon;estimacion promedio;mean error;simulacion;clinical trial;risque;error medio;ecuesta estadistica;homogeneidad;meta analysis;statistical regression;consistencia;estimacion insesgada;optimal strategy;heterogeneity adjusted estimator;metaanalysis;riesgo;sample survey;random effect model;simulacion estadistica;grand echantillon;efecto aleatorio;estimation erreur;analyse covariance;bias;heterogeneidad;random effect;error estimation;risk;simulation statistique;analisis variancia;mean square error;biais asymptotique;regresion estadistica;consistance;62j10;estimacion error;metaanalyse;survival analysis;success rate;stratification;62d05;efficacite estimateur;simulation study;pequena muestra;random effects model;homogeneite;large sample;mean estimation;unbiased estimation;error medio cuadratico;analisis covariancia;regression statistique;estimation moyenne;effet aleatoire;dersimonian laird;biased estimation;strategie optimale;estimation biaisee;sondage statistique;consistency;eficacia estimador;estimation sans biais;petit echantillon;heterogeneity;analyse variance;heterogeneite;erreur systematique;variance;variancia;homogeneity;asymptotic bias;sampling theory	The risk di$erence is frequently used as a measure of the actual gain in the success rate between two treatments within a center (i.e. hospital). Interest is devoted to combining the risk di$erence across several centers under homogeneity but allowing for baseline-risk heterogeneity in each of the treatment arms. The purpose is to compare the e5ciency of six estimators for the common risk di$erence. The six estimators consist of the Pooling method ignoring the strati8cation of centers, several popular sets of di$erent weights, and a new estimator. A simulation study was done to compare bias, variance and mean-square error. The sample sizes in each center varied as 4, 8, 16, 32, 64 and the number of centers as 4, 8, 16, 32, 64. The major result is that the new estimate is an attractive compromise when choosing between the estimators of the set of the center-speci8c sample size weights and the estimators of the set of the inverse-variance weights. It is not an optimal strategy, but it widely extends to cover heterogeneity cases. For small sample size (n6 8), the Cochran and the Mantel–Haenszel estimators are most e5cient because of their smallest mean square errors. Cochran and Mantel–Haenszel estimates are also unbiased and consistent with respect to both sample size and center size. For large sample size (n¿ 32), Lipsitz et al. and Rothman–Boice estimates whose weights are the inverses of variances are the most appropriate. Lipsitz et al. and Rothman–Boice estimates are considerably biased (even if asymptotically unbiased with respect to the sample size). The Pooling estimate is very close and similar to Cochran’s estimate under homogeneity of equal risk di$erence across centers. We recommend to use Cochran, Mantel–Haenszel, or the Pooling estimators when n6 8, to use ∗Corresponding author. E-mail address: phcvw@mahidol.ac.th (C. Viwatwongkasem). 0167-9473/03/$ see front matter c © 2002 Elsevier Science B.V. All rights reserved. PII: S0167 -9473(02)00175 -5 632 C. Viwatwongkasem, W. Bohning / Computational Statistics & Data Analysis 41 (2003) 631–644 Lipsitz et al. and Rothman–Boice estimators when n¿ 32, and to use the new estimator when strong baseline heterogeneity occurs. c © 2002 Elsevier Science B.V. All rights reserved.	baseline (configuration management);coat of arms;computation;computational statistics & data analysis;emoticon;exponent bias;mean squared error;personally identifiable information;simulation;turán's theorem	Chukiat Viwatwongkasem;Walailuck Böhning	2003	Computational Statistics & Data Analysis	10.1016/S0167-9473(02)00175-5	econometrics;meta-analysis;mathematics;asymptotic theory;statistics;random effects model	ML	32.260289842747035	-22.511623841876514	37236
7b43cba053909f32dcea4752d42c6f6151e506d5	spatial error concealment for h.264 using sequential directional interpolation	interpolation;image coding;error concealment;psnr;image resolution;sequential directional interpolation;h 264 spatial error concealment intra prediction modes sequential directional interpolation block loss recovery;psnr spatial error concealment sequential directional interpolation channel errors h 264 coded stream;intra prediction;error analysis;distance measurement;visualization;automatic voltage control;channel errors;image edge detection;interpolation error analysis image coding image resolution;interpolation video compression pixel image restoration streaming media telephony decoding power engineering computing data mining prediction algorithms;pixel;h 264;intra prediction modes;block loss recovery;efficient estimation;h 264 coded stream;encoding;spatial error concealment	Error concealment at the decoder restores erroneous macroblocks (MBs) caused by channel errors. In this paper, we propose a novel spatial error concealment algorithm based on prediction modes of intra-blocks which are included in a H.264-coded stream and highly correlated to the direction of local edge within the block. The key contribution is to sequentially interpolate each pixel in a lost MB by utilizing edge directions and strengths efficiently estimated from the neighboring blocks, preserving local edge continuity for more visually acceptable images. The proposed scheme is simple to implement and more reliably recover high-detailed content in corrupted MBs. The experimental results shows the proposed method achieves reduction in speed by 14%~39% as compared to existing method, and outperforms them in PSNR by 0.5~1 dB as well as in subjective visual evaluation.	algorithm;bitstream;computational complexity theory;data compression;edge detection;error concealment;h.264/mpeg-4 avc;interpolation;intra-frame coding;macroblock;peak signal-to-noise ratio;pixel;scott continuity;video coding format	Myounghoon Kim;Hoonjae Lee;Sanghoon Sull	2008	IEEE Transactions on Consumer Electronics	10.1109/TCE.2008.4711239	computer vision;electronic engineering;visualization;image resolution;peak signal-to-noise ratio;interpolation;computer science;theoretical computer science;pixel;encoding;statistics	Vision	45.78940970955203	-17.5617800825831	37280
96f9320fc9211fa4369022a9b12f89e5e32f5054	application of improved genetic algorithm in optimization computation	numerical analysis genetic algorithms;probability density function;numerical optimization;data mining;constrain condition genetic algorithm improved genetic algorithm optimize computation;genetics;numerical analysis;constrain condition;genetic algorithms optimization methods genetic mutations military computing robustness gradient methods convergence computer applications radar applications algorithm design and analysis;optimization computation;improved genetic algorithm;genetic algorithm;genetic algorithms;optimization;numerical optimization genetic algorithm optimization computation;algorithm design and analysis;optimize computation;gallium	Some practical problems usually contain multiple optima, of which some are local and some global, and traditional method is easily getting trapped at local optimum. To avoid this phenomena, Genetic Algorithm (GA) is introduced, significantly faster & robust at numerical optimization and is more likely to find a function’s true global optimum. In this paper, improved Genetic Algorithm(IGA) is introduced to solve global optima problems. The experiment shows that performance of IGA is better than traditional methods and SGA.	computation;genetic algorithm;global illumination;global optimization;in-game advertising;local optimum;mathematical optimization;numerical analysis;program optimization;synthetic genetic array	Si-ru Zhu	2009	2009 Fifth International Conference on Natural Computation	10.1109/ICNC.2009.393	mathematical optimization;meta-optimization;computer science;theoretical computer science;machine learning	Robotics	28.14161405164566	-5.924233494343248	37306
0b60eb65603aee68a91537590cdf35e4e0ef51da	improving convergence in cartesian genetic programming using adaptive crossover, mutation and selection	convergence;probability;genetic programming;statistics;mathematical model;sociology	Genetic programming (GP) can be described as a paradigm which opens the automatic derivation of programs for problem solving. GP as popularized by Koza uses tree representation. The application of GP takes place on several types of complex problems and became very important for Symbolic Regression. Miller and Thomson introduced a new directed graph representation called Cartesian Genetic Programming (CGP). We use this representation for very complex problems. CGP enables a new application on classification and image processing problems. Previous research showed that CGP has a low convergence rate on complex problems. Like in other approaches of evolutionary computation, premature convergence is also a common issue. Modern GP systems produce population statistics in every iteration. In this paper we introduce a new adaptive strategy which uses population statistics to improve the convergence of CGP. A new metric for CGP is introduced to classify the healthy population diversity. Our strategy maintains population diversity by adapting the probabilities of the genetic operators and selection pressure. We demonstrate our strategy on several regression problems and compare it to the traditional algorithm of CGP. We conclude this paper by giving advices about parameterization of the adaptive strategy.	adaptive grammar;adaptive stepsize;algorithm;cartesian closed category;computer science;directed graph;evolutionary computation;experiment;fingerprint;forking lemma;genetic operator;genetic programming;graph (abstract data type);image processing;iteration;premature convergence;problem solving;programming paradigm;rate of convergence;statistical classification;symbolic regression;vii	Roman Kalkreuth;Günter Rudolph;Jörg Krone	2015	2015 IEEE Symposium Series on Computational Intelligence	10.1109/SSCI.2015.201	mathematical optimization;artificial intelligence;machine learning;mathematics	AI	25.333503047480537	-7.596833702387477	37311
e2bc635ea4ad1929391737cad8cf504a0a91092d	automated antenna design using normalized steady state genetic algorithm	dominance concept;evolved x band antenna;antennas and propagation;microwave antennas;constraint optimization;automated antenna design;antennas genetic algorithms gain wire optimization feeds antennas and propagation;feeds;benchmark problem;gain;evolvable hardware;design optimization;wire;objective function;steady state genetic algorithm;geology;microwave antennas constraint handling genetic algorithms;antennas;constraint handling;genetic algorithms;optimization;st5 antenna design automated antenna design normalized steady state genetic algorithm evolved x band antenna dominance concept constraint handling;space technology;computer science;normalized steady state genetic algorithm;nasa;helical antennas;space technology 5;algorithm design and analysis;st5 antenna design;steady state	A group of NASA had designed an evolved X-band antenna for NASApsilas Space Technology 5 (ST5) spacecraft in 2003 which then became the first evolved hardware in space. However they didn't consider too much about the efficiency of algorithm. In this paper we focus more on the algorithm. Firstly we borrow the concept of dominance to handle constraints and balance different constraints and objectives via normalization. The ultimate comparison is based on the product of these normalized values. Then we test the algorithm carefully using the 24 constrained benchmark problems on CEC2006. The experimental results show it works better than or competitive to a known effective algorithm. Finally we take ST5 antenna design as a real world test problem to verify its ability of solving real world problem. The simulation results are quite promising. Our approach is simple and straightforward, yet effective. Moreover, it has the potential of reducing parameter numbers in objective functions which are quite time consuming to be tuned. In our future work we will apply the algorithm in real antenna design.	benchmark (computing);database normalization;genetic algorithm;simulation;smart antenna;steady state	Zhenhua Cai;Sanyou Zeng;Yang Yang;Lishan Kang	2008	2008 NASA/ESA Conference on Adaptive Hardware and Systems	10.1109/AHS.2008.26	algorithm design;mathematical optimization;constrained optimization;multidisciplinary design optimization;genetic algorithm;gain;computer science;antenna;space technology;steady state	EDA	32.62844492399861	-2.7934587778843714	37317
96cb9ae63ec3c71614ec492537bbee56adb461df	comparison of multi-agent co-operative co-evolutionary and evolutionary algorithms for multi-objective portfolio optimization	agent based;agent based model;multi objective evolutionary algorithm;portfolio optimization;evolutionary algorithm;fitness function	Co-evolutionary techniques makes it possible to apply evol utionary algorithms in the cases when it is not possible to formulate e xplicit fitness function. In the case of social and economic simulations such tec niques provide us tools for modeling interactions between social or economic agents—especially when agent-based models of co-evolution are used. In this pa per gent-based versions of multi-objective co-operative co-evolutionar y lgorithms are applied to portfolio optimization problem. The agent-based algori thms are compared with classical versions of SPEA2 and NSGA2 multi-objective evol uti nary algorithms.	agent-based model;evolutionary algorithm;fitness function;interaction;mathematical optimization;optimization problem;simulation	Rafal Drezewski;Krystian Obrocki;Leszek Siwik	2009		10.1007/978-3-642-01129-0_26	evolutionary programming;mathematical optimization;interactive evolutionary computation;cultural algorithm;machine learning;evolutionary algorithm;mathematics;management science;fitness approximation;memetic algorithm;evolutionary computation	AI	24.611104079388923	-5.644855022901928	37327
eea599bada19fe68f7ee734e0a76cae475d377d3	curvelet-domain image watermarking based on edge-embedding	visual quality;signal processing;image watermarking	A new curvelet-based watermarking technique is presented in this paper, in which watermark signals are selected to be a gray-scale logo image. The curvelet transform was developed in order to represent edges along curves much more efficiently than the traditional transforms. We apply the transform to watermarking and evaluate the effectiveness of the method. Our watermarking algorithm embeds a watermark in curvelet coefficients which are selected by a criterion whether they contain as much edge information as possible. We evaluated the effectiveness of the method against some watermark attacks. Experiment results show that our new method yields quite good visual quality in watermarked images, and is robust to typical signal processing attacks such as compression, cropping, adding noise and filtering.	algorithm;coefficient;curvelet;dictionary attack;digital watermarking;grayscale;logo;signal processing	Thai Duy Hien;Ikeda Kei;Hanane Harak;Yen-Wei Chen;Yasunori Nagata;Zensho Nakao	2007		10.1007/978-3-540-74827-4_40	computer vision;theoretical computer science;curvelet;mathematics;multimedia;watermark	Graphics	40.86684703503454	-10.849201398530983	37344
f248ab59108fb5a3d91737d7e5dcf1e1d8dc2dbf	on the analysis of unbalanced two-level supersaturated designs via generalized linear models	62 07;62j12;factor screening;supersaturated designs;optimality;balance property;62k15;generalized linear model	ABSTRACTSupersaturated designs (SSDs) are factorial designs in which the number of experimental runs is smaller than the number of parameters to be estimated in the model. While most of the literature on SSDs has focused on balanced designs, the construction and analysis of unbalanced designs has not been developed to a great extent. Recent studies discuss the possible advantages of relaxing the balance requirement in construction or data analysis of SSDs, and that unbalanced designs compare favorably to balanced designs for several optimality criteria and for the way in which the data are analyzed. Moreover, the effect analysis framework of unbalanced SSDs until now is restricted to the central assumption that experimental data come from a linear model. In this article, we consider unbalanced SSDs for data analysis under the assumption of generalized linear models (GLMs), revealing that unbalanced SSDs perform well despite the unbalance property. The examination of Type I and Type II error rates through ...	generalized linear model;unbalanced circuit	Kalyan Chatterjee;Christos Koukouvinos;Christina Parpoula	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2015.1089285	econometrics;generalized linear model;mathematics;statistics	EDA	28.93796496348012	-21.772484171264832	37345
c728544de4dcb06b858fa957509ccf2458672d2a	a bivariate failure time model with random shocks and mixed effects	reliability;hazard rate process;bivariate new better than used;62p30;aging properties;bivariate non homogeneous compound poisson process;positive dependence properties;maximum likelihood estimator;stochastic order;60e15;multivariate total positivity;60k10;hazard rate order	Two components are considered, which are subject to common external and possibly fatal shocks. The lifetimes of both components are characterized by their hazard rates. Each shock can cause the immediate failure of either one or both components. Otherwise, the hazard rate of each component is increased by a non fatal shock of a random amount, with possible dependence between the simultaneous increments of the two failure rates. An explicit formula is provided for the joint distribution of the bivariate lifetime. Aging and positive dependence properties are described, thereby showing the adequacy of the model as a bivariate failure time model. The influence of the shock model parameters on the bivariate lifetime is also studied. Numerical experiments illustrate and complete the study. Moreover, an estimation procedure is suggested in a parametric framework, under a specific observation scheme.	bivariate data;experiment;numerical method	Sophie Mercier;Hai Ha Pham	2017	J. Multivariate Analysis	10.1016/j.jmva.2016.09.008	econometrics;mathematical optimization;reliability;mathematics;maximum likelihood;statistics	Metrics	31.34620143160644	-19.062398011283243	37364
af3a5390d19602af3130e1438a78ad1da063bcb8	bayesian estimation of unavailability	informative prior;estimation method;prior information;prior distribution;bayesian method;exponential durations;data aggregation;quantile quantile plots;bayesian estimator;alternating renewal process;noninformative prior;renewal process	This paper gives two Bayesian methods for estimating test-and-maintenance unavailability. Both unplanned and periodic maintenance are considered. One estimation method uses ‘detailed data,’ the individual outage times. The other method uses ‘summary data,’ totals of outage time and exposure time in various time periods such as calendar months. Either method can use either a noninformative or an informative prior distribution. Both methods are illustrated with an example data set, and the results are compared.	unavailability	Corwin L. Atwood;Max Engelhardt	2004	Rel. Eng. & Sys. Safety	10.1016/j.ress.2003.11.010	data aggregator;bayesian average;renewal theory;econometrics;prior probability;bayesian probability;pattern recognition;mathematics;statistics	DB	29.794087727469048	-20.35032693320948	37381
180275c4f0d71155c63b7554d2e7079998c51398	multi-population co-genetic algorithm with double chain-like agents structure for parallel global numerical optimization	numerical optimization;agent;structural dynamics;chain like agent structure;multi population;genetic algorithm	For the low optimization precision and long optimization time of genetic algorithm, this paper proposed a multi-population agent co-genetic algorithm with chain-like agent structure (MPAGA). This algorithm adopted multi-population parallel searching mode, close chain-like agent structure, cycle chain-like agent structure, dynamic neighborhood competition and orthogonal crossover strategy to realize parallel optimization, and has the characteristics of high optimization precision and short optimization time. In order to verify the optimization precision of this algorithm, some popular benchmark test functions were used for comparing this algorithm and a popular agent genetic algorithm (MAGA). The experimental results show that MPAGA has higher optimization precision and shorter optimization time than MAGA.	angular defect;benchmark (computing);data mining;decision support system;distribution (mathematics);evolutionary multimodal optimization;experiment;feature selection;genetic algorithm;mathematical optimization;multimodal interaction;numerical analysis;paradiseo;population	Yongming Li;Xiaoping Zeng	2008	Applied Intelligence	10.1007/s10489-008-0146-7	mathematical optimization;multi-swarm optimization;structural dynamics;simulation;test functions for optimization;meta-optimization;genetic algorithm;computer science;derivative-free optimization;artificial intelligence;machine learning;vector optimization;algorithm;metaheuristic;global optimization;population-based incremental learning	AI	26.176205680031526	-4.828288323802307	37447
609e528bc1fd880ab6122be20c7a0606527a6ac9	optimization of parameterized compactly supported orthogonal wavelets for data compression	data compression;parameterization;genetic algorithm;optimization;entropy;wavelets	In this work we review the parameterization of filter coefficients of compactly supported orthogonal wavelets used to implement the discrete wavelet transform. We also present the design of wavelet based filters as a constrained optimization problem where a genetic algorithm can be used to improve the compression ratio on gray scale images by minimizing their entropy and we develop a quasi-perfect reconstruction scheme for images. Our experimental results report a significant improvement over previous works and they motivate us to explore other kinds of perfect reconstruction filters based on parameterized tight frames.	coefficient;constrained optimization;constraint (mathematics);data compression;discrete wavelet transform;genetic algorithm;grayscale;mathematical optimization;optimization problem;reconstruction filter	Oscar Herrera-Alcántara;Miguel González-Mendoza	2011		10.1007/978-3-642-25330-0_45	data compression;parametrization;wavelet;entropy;mathematical optimization;discrete mathematics;genetic algorithm;computer science;theoretical computer science;mathematics;fast wavelet transform;statistics	Robotics	43.42601400131727	-14.606565400584408	37455
654d7599d554a7878d20801107e90feffeda7789	a novel membrane algorithm based on differential evolution for numerical optimization	differential evolution;membrane algorithm;membrane computing;numerical optimization;p systems;time-frequency atom decomposition	This paper presents a novel membrane algorithm, called DEPS, for numerical optimization. DEPS is an appropriate combination of a differential evolution algorithm, a local search and P systems. In this algorithm, the hierarchical structure of cell-like P systems is used to organize the objects consisting of real-valued strings and the rules which are composed of mutation, crossover and selection operations in elementary membranes, a local search in the skin membrane and transformation/communicationlike rules in P systems. The effectiveness of the algorithm is tested on extensive numerical optimization experiments. In what follows DEPS is applied to solve a real-world problem, time-frequency atom decomposition. Experimental results show that DEPS performs better than its counterpart differential evolution algorithm.	algorithm;differential evolution;experiment;lambert's cosine law;local search (optimization);mathematical optimization;modulation;natural computing;numerical analysis;p system	Jixiang Cheng;Gexiang Zhang;Xiangxiang Zeng	2011	IJUC		differential evolution;mathematical optimization;membrane;computer science	ML	25.23815929945901	-5.683849281774625	37496
ee1c70bda071898a0877e596bdc89486b84db782	chaotic particle swarm optimization for numeric integral	chaos;heuristic algorithms;particle swarm optimization;statistics;optimization;algorithm design and analysis;sociology	Aiming at the shortcoming of easily falling into local optima in solving numeric integral using particle swarm optimization (PSO), this paper proposed a chaotic PSO (CPSO) algorithm to solve numerical integral. The method arbitrarily selects certain nodes in the integral interval and optimizes these nodes through CPSO algorithm, finally obtains more accurate integral results. The proposed algorithm is of high precision and low demands on integrand. Six standard numeric integral examples verify the validity and correctness of the algorithm, thus CPSO algorithm has a certain reference value and application in engineering practice.	algorithm;complex system;computation;computational intelligence;computer simulation;correctness (computer science);local optimum;mathematical optimization;numerical analysis;particle swarm optimization;requirement	Ye Tang;Xu-Yu Peng	2016	2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2016.7603172	algorithm design;mathematical optimization;multi-swarm optimization;computer science;theoretical computer science;mathematics;particle swarm optimization;algorithm	EDA	29.81598916441046	-6.156903608247615	37523
e59f42c28fcc9da4a69607b72927d6d2b6720e6e	a hybrid approach based on evolutionary strategies and interval arithmetic to perform robust designs	hybrid approach;evolutionary strategy;interval arithmetic;robust design	  This paper proposes an approach based on the use of Cellular Evolutionary Strategies (CES) and Interval Arithmetic (IA) as  an alternative technique to obtain robust system design. CES are an approach that combines the Evolution Strategy techniques  with concepts from Cellular Automata in order to optimise a given function, while IA is used as a checking technique that  guarantees the feasibility of the design. IA is able to consider simultaneously the effects of uncertainty of all of the parameters  on a performance function and to provide strict bounds (minimum and maximum values) with only one evaluation. CES and IA are  used to obtain, by an iterative process, a robust design, that is, the maximum size of each variable deviation that allow  to comply with a set of specifications. The proposed approach is an indirect method based on optimisation instead of a direct  method based on mapping from the output into the input space. A numerical example, related to an electronic circuit system  design, illustrates the application of the approach.    	interval arithmetic	Claudio M. Rocco Sanseverino	2005		10.1007/978-3-540-32003-6_66	mathematical optimization;machine learning;mathematics;algorithm	Logic	33.84777407289846	-5.974958961647171	37564
09d22423650c33e032993247442fac22e65715ae	a study on efficient compression of multi-focus images for dense light-field reconstruction	discrete wavelet transforms;light field image coding three dimensional tv multi view multi focus;image coding;data compression;image coding discrete wavelet transforms psnr image reconstruction discrete cosine transforms transform coding robustness;discrete cosine transforms;image reconstruction;disparity compensation dense light field reconstruction 4d light field robust 3d scene estimation 3d information light field compression synthesized multifocus image compression dwt dct based compression psnr ssim;image reconstruction data compression discrete cosine transforms discrete wavelet transforms image coding	Light-Field enables us to observe scenes from free viewpoints. However, it generally consists of 4-D enormous data, that are not suitable for storing or transmitting without effective compression. 4-D Light-Field is very redundant because essentially it includes just 3-D scene information. Actually, although robust 3-D scene estimation such as depth recovery from Light-Field is not so easy, a method of reconstructing Light-Field directly from 3-D information composed of multi-focus images without any scene estimation is successfully derived. Previously, based on the method, Light-Field compression via synthesized multi-focus images as effective representation of 3-D scenes was proposed. In this paper, we study efficient compression of multi-focus images synthesized from dense Light-Field by using DWT instead of DCT-based compression in order to suppress degradation such as block noise. Quality of reconstructed Light-Field is evaluated by PSNR and SSIM for analyzing characteristics of residuals. Experimental results reveal that our method is much superior to Light-Field compression using disparity-compensation at low bit-rate.	binocular disparity;discrete cosine transform;discrete wavelet transform;elegant degradation;light field;peak signal-to-noise ratio;structural similarity;transmitter	Takashi Sakamoto;Kazuya Kodama;Takayuki Hamamoto	2012	2012 Visual Communications and Image Processing	10.1109/VCIP.2012.6410759	data compression;iterative reconstruction;computer vision;transform coding;block truncation coding;image compression;theoretical computer science;mathematics;lossless compression;texture compression;statistics;computer graphics (images)	Vision	43.921597785981724	-18.194462267755803	37575
f3b974e6ccfb73b749a49e003507b15364873176	using a commercial graphical processing unit and the cuda programming language to accelerate scientific image processing applications	evaluation performance;optimisation;gestion memoire;memory management;performance evaluation;image processing;0705p;programming language;video signal processing;0130c;imagerie;optimization method;metodo optimizacion;traitement image;single instruction multiple data;algorithme;identificacion sistema;memory access;computer programming;etat actuel;data storage;visualization;imagery;computer programming languages;system identification;graphics processing units;state of the art;methode optimisation;traitement signal video;langage programmation;graphic processing unit;hough transforms;computing systems;image processing techniques;algorithms;estado actual;scientific communication;optimization;floating point;hough transform;transformation hough;coma flotante;imagineria;video;iris;c;4230v;identification systeme;programming languages;virgule flottante	In the past two years the processing power of video graphics cards has quadrupled and is approaching super computer levels. State-of-the-art graphical processing units (GPU) boast of theoretical computational performance in the range of 1.5 trillion floating point operations per second (1.5 Teraflops). This processing power is readily accessible to the scientific community at a relatively small cost. High level programming languages are now available that give access to the internal architecture of the graphics card allowing greater algorithm optimization. This research takes memory access expensive portions of an image-based iris identification algorithm and hosts it on a GPU using the C++ compatible CUDA language. The selected segmentation algorithm uses basic image processing techniques such as image inversion, value squaring, thresholding, dilation, erosion and memory/computationally intensive calculations such as the circular Hough transform. Portions of the iris segmentation algorithm were accelerated by a factor of 77 over the 2008 GPU results. Some parts of the algorithm ran at speeds that were over 1600 times faster than their CPU counterparts. Strengths and limitations of the GPU Single Instruction Multiple Data architecture are discussed. Memory access times, instruction execution times, programming details and code samples are presented as part of the research.	cuda;graphics processing unit;image processing;programming language	Randy P. Broussard;Robert W. Ives	2011		10.1117/12.872217	cuda pinned memory;hough transform;parallel computing;video;visualization;system identification;telecommunications;image processing;computer science;floating point;theoretical computer science;computer programming;general-purpose computing on graphics processing units;memory management;computer graphics (images)	Arch	39.730313845806755	-20.698716396500828	37583
d76e91fbcdc7f9ce2932de3361d0b603c8264f5a	a color image encryption technique based on a substitution-permutation network		In this paper we have proposed and tested an image encryption technique consisting of matrix transformation, pixel diffusion and a permutation box. Since the matrix transformation, which produces both confusion and diffusion, is linear, the pixel diffusion and permutation have been introduced so as to make the technique nonlinear. This technique is specifically designed for sensitive fields like medicine where misinterpretation could result in loss of life. One apt application of this technique is its use in PACS for secure storage and transfer of medical images. The uniqueness of our technique is that it is ciphertext sensitive so that decryption doesn’t yield a recognizable image in case there is an error during transmission. The proposed technique gives good parametric and sensitivity results proving itself an eligible candidate for image encryption.	color image;encryption;substitution-permutation network	J. Mohamedmoideen Kader Mastan;G. A. Sathish Kumar;K. Bhoopathy Bagan	2011		10.1007/978-3-642-22726-4_54	color depth;color image	Crypto	38.473072543529035	-8.99179398385329	37598
25f46a5e80ffa811b5760828845310024e4d616e	autonomous robot navigation: path planning on a detail-preserving reduced-complexity representation of 3d point clouds	dynamic programming;range data;path planning;spatially important points;autonomous navigation	Determination of a collision free, optimal path achieved by performing complex computations on an accurate representation of the terrain, is essential to the success of autonomous navigation systems. This paper defines and builds upon a technique – Spatially Important Point (SIP) Identification – for reducing the inherent complexity of range data without discarding any essential information. The SIP representation makes onboard autonomous navigation a viable option, since space and time complexity is greatly reduced. A cost based, dynamic navigation analysis is then performed using only the SIPs which culminates into a new time and space efficient Path Planning technique. The terrain is also retained in the form of a graph, with each branch of every node encountered, indexed in a priority sequence given by its cumulative cost. Experiments show the entire dataflow, from input data to path planning, being done in less than 700ms on a modern computer on datasets typically having 10 points.	algorithm;automated planning and scheduling;autonomous robot;computation;dataflow;graph (discrete mathematics);machine learning;motion planning;time complexity	Rohit Sant;Ninad Kulkarni;Ainesh Bakshi;Salil Kapur;Kratarth Goel	2013		10.1007/978-3-642-39402-7_18	computer vision;simulation;computer science;dynamic programming;distributed computing;motion planning;mobile robot navigation	Robotics	53.327484287368726	-23.726669321126924	37618
07a186162f371b17a7297eb339097a55d2543f3f	efficient tracking of moving target based on an improved fast differential evolution algorithm		Computer vision, which is used to detect and track a specific target in image sequences, has drawn great attention in recent years. The process of tracking can be formulated as a dynamic optimization problem that identifies the optimal position of the target in each image. Differential evolution (DE), who owns the advantages of simplicity, parallel computing, and self-adaptive search for global optimization, is envisioned as a promising algorithm to provide effective target tracking. In this paper, several improvements are made in DE for better adaptability in target tracking. Specifically, we introduce two inferior individuals into the mutation stage, which further enriches the diversity of the population and speeds up the offspring’s evolution. We also proceed several image preprocessing and build an adaptive Gaussian mixture model of the target to deal with the complex tracking scenarios. Experimental results show that the designed tracking algorithm based on our improved DE demonstrates a higher tracking accuracy and faster tracking speed in several challenging tracking scenarios.	algorithm;computer vision;differential evolution;dynamic programming;global optimization;mathematical optimization;mixture model;optimization problem;parallel computing;preprocessor;the offspring	Laijie Lin;Min Zhu	2018	IEEE Access	10.1109/ACCESS.2018.2793298	mixture model;global optimization;differential evolution;feature extraction;adaptability;computer science;population;preprocessor;algorithm;optimization problem	Vision	25.773919917673876	-4.82013270808813	37619
d3e53b228ea3ad833ae6b4915edaf37318f1cc30	catalan's equation has no new solution with either exponent less than 10651		Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Maurice Mignotte;Yves Roy	1995	Experimental Mathematics	10.1080/10586458.1995.10504327	combinatorics;mathematical analysis;calculus	Robotics	49.517857503717046	-2.8938138815703796	37634
3fa8b6e2c3e913f2fae17abdd30c5289ea1185b9	high resolution dynamic mri using motion estimated and compensated compressed sensing	image sampling;minimisation;focusing;residual encoding step;high resolution;compressed sensing;image coding;image resolution;data compression;motion compensation;asymptotic optimality;prediction step;mpeg video;motion estimation compensation compressed sensing k t focuss k t blast sense mpeg video;high resolution imaging;model based approach;motion estimation;layout;mr imaging;temporal resolution;sparse residual signal;k t focuss;minimization problem;sparse residual signal high resolution dynamic mri motion field estimation motion compensated compressed sensing mechanism k t blast sense approach spatio temporal resolution k t focuss minimization problem focal underdetermined system solver prediction step residual encoding step temporal redundancies;spatio temporal resolution;heuristic algorithms;image reconstruction;medical image processing;focal underdetermined system solver;magnetic resonance imaging;spatiotemporal phenomena biomedical mri data compression image coding image reconstruction image resolution image sampling medical image processing minimisation motion compensation motion estimation;spatiotemporal phenomena;magnetic resonance imaging motion estimation compressed sensing focusing image resolution high resolution imaging heuristic algorithms layout encoding signal resolution;motion estimation compensation;signal resolution;dynamic mri;k t blast sense;motion compensated compressed sensing mechanism;temporal redundancies;k t blast sense approach;motion field estimation;encoding;high resolution dynamic mri;biomedical mri;dynamic scenes	A model based approach called k-t BLAST/SENSE, has drawn significant attentions from MR imaging community due to its improved spatio-temporal resolution. Recently, we showed that k-t BLAST/SENSE corresponds to the special case of a new dynamic MRI algorithm called k-t FOCUSS that is asymptotically optimal from compressed sensing perspective. The k-t FOCUSS exploits the sparsity of x-f support of dynamic scene and converts imaging problem into an L1 minimization problem that can be solved using FOCal Underdetermined System Solver (FOCUSS). In this paper, we extend the idea of k-t FOCUSS and introduce motion estimation and compensation (ME/MC) based prediction step and residual encoding step. The ME/MC based prediction step exploits the temporal redundancies using the motion field estimation and provides much sparser residual signals. The sparse residual signal can then be effectively encoded using much smaller number of k-t samples. Simulation results demonstrate that high resolution dynamic MR images can be accurately obtained even from very limited data samples.	asymptotically optimal algorithm;blast;compressed sensing;image resolution;motion estimation;motion field;simulation;solver;sparse matrix	Hong Jung;Jong Chul Ye	2008	2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2008.4541322	computer vision;speech recognition;radiology;image resolution;computer science;magnetic resonance imaging	Vision	48.71435160194857	-17.843289868243506	37651
1df6215c39441a8060bc142737f38d72709a9da5	a shock model based approach to network reliability	stochastic processes binomial distribution computer network reliability;electric shock;telecommunication network reliability;hazards;electric shock hazards stochastic processes distribution functions computer network reliability telecommunication network reliability;nonhomogeneous poisson process shock model based approach network reliability t signature based mixture representations reliability function binomial distribution;stochastic processes;two state networks counting process non homogeneous poisson process signature stochastic ordering t signature;distribution functions;computer network reliability	We consider a network consisting of n components and assume that the network has two states up and down. We further suppose that the network is subject to shocks that appear according to a counting process and that each shock may lead to the component failures. Under some assumptions on the shock occurrences, we present a new variant of the notion of signature which we call t-signature. Then, t-signature-based mixture representations for the reliability function of the network are obtained. Several stochastic properties of the network lifetime are investigated. In particular, under the assumption that the number of failures at each shock follows a binomial distribution and the process of shocks is nonhomogeneous Poisson process, explicit form of the network reliability is derived and its aging properties are explored. Several examples are also provided.		Somayeh Zarezadeh;Somayeh Ashrafi;Majid Asadi	2016	IEEE Transactions on Reliability	10.1109/TR.2015.2494366	traffic generation model;reliability engineering;stochastic process;econometrics;hazard;distribution function;mathematics;statistics	Theory	31.593766740688327	-18.822166316553147	37744
66db14a27e4c956f7d73e1ec37932e0ea247fc43	new fast search algorithm for base layer of h.264 scalable video coding extension	scalable video coding;r d optimization;search algorithm;reference frame;motion estimation;video coding testing motion estimation software algorithms automatic voltage control static var compensators displays encoding performance loss helium;video coding;audio coding;temporal decomposition;motion vector;fast search algorithm;h 264 scalable video coding;r d optimization fast search algorithm h 264 scalable video coding motion estimation algorithm audio video coding hierarchical b frame structure temporal decomposition jsvm software block matching coarse search fine search;block matching;audio video coding;search problems;coarse search;motion estimation algorithm;jsvm software;hierarchical b frame structure;video coding audio coding motion estimation search problems;fine search;base layer	In this contribution, a fast search motion estimation algorithm for H.264/AVC SVC (scalable video coding) base layer with hierarchical B-frame structure for temporal decomposition is presented and compared with fast search motion estimation algorithm in JSVM software, that is the reference software for H.264/AVC SVC. The proposed technique is a block-matching based motion estimation algorithm working in two steps, called Coarse search and Fine search. The Coarse search is performed for each frame in display order, and for each 16x16 macroblock chooses the best motion vector at half pel accuracy. Fine search is performed for each frame in encoding order and finds the best prediction for each block type, reference frame and direction, choosing the best motion vector at quarter pel accuracy using R-D optimization. Both Coarse and Fine Search test 3 spatial and 3 temporal predictors, and add to the best one a set of updates.	data compression;h.264/mpeg-4 avc;macroblock;mathematical optimization;motion estimation;pixel;reference frame (video);scalability;scalable video coding;search algorithm;video compression picture types	Livio Lima;Daniele Alfonso;Luca Pezzoni;Riccardo Leonardi	2007	2007 Data Compression Conference (DCC'07)	10.1109/DCC.2007.58	scalable video coding;reference frame;computer vision;real-time computing;quarter-pixel motion;computer science;theoretical computer science;motion estimation;block-matching algorithm;algorithm;search algorithm	Vision	46.78625068917627	-18.458429316251003	37812
d47cbccc8769864dcb72dfcbf79d26d526370073	color pathological image encryption scheme with s-boxes generated by complex chaotic system and environmental noise	complex chaotic system;pathological image encryption;post quantum cryptography;s box;environmental noise	Pathological image encryption can help protect medical privacy. The paper proposes a color pathological image encryption scheme where three S-boxes are utilized to encrypt the red, green and blue components alternately. The S-boxes generated by complex chaotic system are changed dynamically, for the initial values and parameter are dependent on both the plain image and the environmental noise. The S-box switching sequence for each color component is also generated by complex chaotic system. Some effective measures have been taken to speed up the processes of encryption and decryption. Simulation result demonstrates that the scheme is suitable for color pathological image encryption.	bitwise operation;chaos theory;color space;cryptography;cryptosystem;encryption;exclusive or;hash function;lorenz system;medical privacy;noise (electronics);s-box;simulation	Guoyan Liu;Abdurahman Kadir;Hongjun Liu	2015	Neural Computing and Applications	10.1007/s00521-015-1888-x	environmental noise;telecommunications;theoretical computer science;post-quantum cryptography;mathematics;s-box;computer security;probabilistic encryption	Crypto	38.565877973346545	-8.878907773433088	37871
969e601c8ff44420b3c5d018e222c48fc66eff40	multiple rao-blackwellized particle filtering for target tracking in urban environments	rao blackwellized particle filter;estimation theory;urban environment;state space methods;high dimensionality;radar tracking;mathematical model equations vectors radar cross section target tracking radar tracking;radar antennas;particle filter;mrbpf multiple rao blackwellized particle filtering urban environments filtering algorithm joint tracking multiple target states channel state antennas radar network multiple target tracking standard particle filtering spf high dimensional state vector hybrid filter multiple particle filtering mpf state space model;target tracking;state space model;particle filtering numerical methods;numerical simulation;target tracking estimation theory particle filtering numerical methods radar antennas radar tracking state space methods	We propose a new filtering algorithm for joint tracking of multiple target states and the channel state between each pair of antennas in a radar network. The problem of tracking multiple targets in complex scenarios, such as an urban environment, poses a computational challenge as standard particle filtering (SPF) requires large number of particles to obtain an accurate estimate of the high-dimensional state vector. In this paper, we develop a hybrid filter based on the combination of multiple particle filtering (MPF) and Rao-Blackwellized particle filtering (RBPF) by exploiting the structure in the state-space model. Numerical simulations show that the proposed multiple Rao-Blackwellized particle filtering (MRBPF) performs better than the SPF and the RBPF.	algorithm;channel (communications);frequency response;numerical analysis;numerical linear algebra;particle filter;radar;simulation;state space;state-space representation	Phani Chavali;Arye Nehorai	2011	2011 4th IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)	10.1109/CAMSAP.2011.6136039	computer vision;electronic engineering;auxiliary particle filter;geography;control theory	Vision	51.199005913412336	1.5342527268514228	37896
5367453ca5734c461033a4026ab5476e127dd61f	ademo/d: adaptive differential evolution for multiobjective problems	multi objective optimization approaches ademo d adaptive differential evolution for multiobjective problems continuous optimization multiobjective evolutionary algorithms based on decomposition moea d self adaptive differential evolution sade;differential evolution;evolutionary computation;adaptive techniques;multi objective optimization;vectors sociology statistics optimization evolutionary computation indexes biological cells;adaptive techniques multi objective optimization differential evolution	This paper proposes a method for continuous optimization based on Differential Evolution (DE). The approach named Adaptive Differential Evolution for Multiobjective Problems (ADEMO/D) incorporates concepts of Multiobjective Evolutionary Algorithms based on Decomposition (MOEA/D) and mechanisms of mutation strategies adaptation inspired by the adaptive DE named Self-adaptive Differential Evolution (SaDE). Additionally a new mutation strategy, based on MOEA/D neighborhood concept, is proposed to be used in the strategy candidate pool. ADEMO/D is compared with three multi-objective optimization approaches using a set of benchmarks. The preliminary results are very promising and stand the proposed approach as a candidate to the State-of-art for multi-objective optimization.	benchmark (computing);continuous optimization;differential evolution;evolutionary algorithm;experiment;moea framework;mathematical optimization;multi-objective optimization;pareto efficiency;performance	Sandra M. Venske;Richard A. Gonçalves;Myriam Regattieri Delgado	2012	2012 Brazilian Symposium on Neural Networks	10.1109/SBRN.2012.29	differential evolution;mathematical optimization;cma-es;computer science;artificial intelligence;multi-objective optimization;machine learning;mathematics;evolution strategy;evolutionary computation	SE	25.393077195908315	-4.098121445486719	37972
94bfb6417711c78875a3e14df517245ff70a3b4f	evolutionary algorithm based 9dof sensor board calibration	gyroscopes;calibration magnetometers accelerometers gyroscopes iris wireless sensor networks measurement uncertainty;magnetometers;measurement uncertainty;magnetometers accelerometers calibration evolutionary computation gyroscopes;evolutionary algorithm sensor calibration accelerometer gyroscope magnetometer;iris;nonorthogonality corrections evolutionary algorithm 9dof sensor board calibration 9 degree of freedom sensor boards three axis accelerometer gyroscope magnetometer bias values scale factors;accelerometers;calibration;wireless sensor networks	Accelerometers gyroscopes and magnetometers can be used in a large variety of applications, and their calibration is a very actual problem due to high error rates, especially when errors are integrated in time. Systematic errors from the measurement values can be removed by sensor calibration, thus the applicability of the sensor can be increased. In this paper, a new evolutionary algorithm-based, quick and easy-to-use calibration method is presented. The algorithm has been developed and tested with real measurement data of the above-mentioned sensors. During this work, measurement data have been collected with 9 degree of freedom (9DOF) sensor boards, which are built up of three-axis accelerometer, gyroscope and magnetometer. For accelerometer and magnetometer calibration, bias values, scale factors and non-orthogonality corrections have been calculated, while for the gyroscopes only offsets have been determined.	evolutionary algorithm;gyroscope;optic axis of a crystal;sensor	Peter Sarcevic;Szilveszter Pletl;Zoltán Kincses	2014	2014 IEEE 12th International Symposium on Intelligent Systems and Informatics (SISY)	10.1109/SISY.2014.6923583	control engineering;inertial measurement unit;electronic engineering;engineering;control theory	Embedded	49.581607890048666	0.4607606529772958	37986
c00c13215722ab9fabfe11c25ff3ad8b41dd6e8c	neutrality and gradualism: encouraging exploration and exploitation simultaneously with binary decision diagrams	exploitation;search space;search algorithm;mutation rate;search problems binary decision diagrams;data structures boolean functions genetic mutations computer science algorithm design and analysis evolutionary computation genetic programming uncertainty costs testing;binary decision diagrams;gradualism;exploration;neutrality;search problems;multi modal search spaces;search algorithms;multi modal search spaces neutrality gradualism binary decision diagrams search algorithms exploitation exploration;binary decision diagram	Search algorithms are subject to the trappings of local optima. Attempts to address the problem are often framed in the context of needing to balance, or trade-off, exploitation against exploration. Ideally, it is best to maximise both simultaneously, but this is usually seen as infeasible in the presence of multi-modal search spaces. This paper investigates the potential for exploration of both neutrality and mutation rate, and argues that the former is the more important. The most interesting result, however, is that the necessity for a trade-off between exploitation and exploration can be avoided within the context of our algorithm for evolving binary decision diagrams.	algorithm;binary decision diagram;local optimum;modal logic	Richard Mark Downing	2006	2006 IEEE International Conference on Evolutionary Computation	10.1109/CEC.2006.1688367	beam search;mathematical optimization;computer science;machine learning;mathematics;algorithm;search algorithm	Robotics	26.72571848605198	-7.963134815599055	38040
2bad0ccab7f1dd4b5814ea0f2a648c9e6b763fd9	bayesian sequential d-d optimal model-robust designs	plan d optimal;bayes estimation;experimental design;d optimal design;62j12;sample size;estimator robustness;response surface methodology;62k05;statistical simulation;sequential design;62k25;two stage procedures;03cxx;analisis datos;05bxx;d d optimality;two stage procedure;modele lineaire;plan experiencia;posterior probability;modelo lineal;knowledge;data analysis;estimacion bayes;optimal d design;model robustness;robustez estimador;simulacion estadistica;62k99;plan experience;simulation statistique;model uncertainty;probabilite a posteriori;statistical computation;linear model;calculo estadistico;62d05;probabilidad a posteriori;model;optimal design;simulation study;general linear model;analyse donnee;posterior probabilities;calcul statistique;vertical align middle title full size image;binary data;img class imglazyjsb inlineimage height 13 width 15 alt full size image 1 k style margin top 5px;knowledge modeling;applications;optimization model;robust design;estimation bayes;62kxx;optimalite d d;plan sequentiel;plan secuencial;sequential designs;robustesse estimateur	Alphabetic optimal design theory assumes that the model for which the optimal design is derived is known. However in practice, this assumption may not be credible, as models are rarely known in advance. Therefore, optimal designs derived under the classical approach may be the best design but for the wrong assumed model. The Bayesian two-stage approach to design experiments for the general linear model when initial knowledge of the model is poor, is reviewed and extended. A Bayesian optimality procedure that works well under model uncertainty is used in the first stage and the second stage design is then generated from an optimality procedure that incorporates the improved model knowledge from the first stage. In this way, the Bayesian        -        optimal model-robust design is developed. Results show that the Bayesian        -        optimal designs are in general superior in performance to the classical one-stage        -optimal and the one-stage Bayesian        -optimal designs. The ratio of sample sizes for the two stages and the minimum sample size desirable in the first stage is also examined in a simulation study.		Arvind Ruggoo;Martina Vandebroek	2004	Computational Statistics & Data Analysis	10.1016/j.csda.2003.09.014	bayesian average;econometrics;mathematical optimization;bayesian experimental design;optimal design;mathematics;posterior probability;bayesian statistics;information technology;statistics	Theory	25.79226429450521	-19.663558868699052	38084
eb3a7f710ef4a063bd942ecbecf2df30f1a9c417	low-complexity scheme for adaptive interpolation filter based on amplitude characteristic analysis	adaptive interpolation filter;interpolation;image coding;complexity theory;signal estimation;motion compensation;wiener filter encoding adaptive filters interpolation filtering algorithms complexity theory image coding;real time;high compression video coding;signal estimation low complexity scheme adaptive interpolation filter amplitude characteristic analysis motion compensated prediction high compression video coding;low complexity;video coding interpolation motion compensation;motion compensated;filter design;video coding;adaptive filters;filtering algorithms;computational complexity;amplitude characteristic analysis;low complexity scheme;interpolation filter;low bit rate coding;code motion;wiener filter;motion compensated prediction;encoding;low bit rate coding motion compensated prediction interpolation filter uhdtv;uhdtv;compressed video	Motion-compensated (MC) prediction is one of the most effective coding tools for high-compression video coding. In former works, adaptive interpolation filter (AIF) technology, which improves the coding performance of the fractional-pel MC, has been proposed. As for the real-time codec system, the implementation of AIF based on the Wiener filter algorithm is difficult because of its high computational complexity. To overcome this problem, in this paper, we propose a low-complexity AIF scheme, which can maintain the coding performance achieved by the conventional AIF. The key technology is the efficient filter design process based on the amplitude characteristic analysis with the precise estimation of MC error signal. The experimental result confirmed that almost the same coding performance was maintained at only 30% of the complexity compared to the conventional AIF.	codec;computational complexity theory;data compression;filter design;interpolation;peterson's algorithm;pixel;real-time clock;wiener filter	Tomonobu Yoshino;Sei Naito;Shigeyuki Sakazawa;Shuichi Matsumoto	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5653561	adaptive filter;computer vision;interpolation;computer science;theoretical computer science;mathematics;wiener filter;filter design;computational complexity theory;motion compensation;algorithm;encoding;statistics	Robotics	45.86082514172098	-16.840389690563224	38103
e97b79a13a50b1daa0a6c823cc8d33355ca5cbb7	effects of multiplier output offsets on on-chip learning for analog neuro-chips	feed forward;neural networks;on chip learning;chip;analog neuro chip;multiplier offsets;neural network	The effects of multiplier output offsets on on-chip learning are analyzed. Offsets on signal feed-forward multipliers are equivalent to sigmoid biases and do not degrade learning performance. Offsets on multipliers for sigmoid derivatives cause static errors at the outputs, which may be overcome by proper choices of target values. Offsets on weight-adjustment multipliers cause not only output static errors but also weight drifts, which are hard to compensate. Therefore weight-adjustment multipliers are most critical for analog neuro-chips with on-chip learning capability. Simulation results agree well with analytic calculations.	sigmoid function;simulation	Yoon-Kyung Choi;Ki-Hwan Ahn;Soo-Young Lee	1996	Neural Processing Letters	10.1007/BF00454840	chip;real-time computing;computer science;machine learning;control theory;feed forward;artificial neural network	ML	38.19746502390084	-2.413827013204953	38122
d32211b65dce99777b89b765bebeeca033710f93	correlative visualizaton techniques for multidimensional data	tratamiento datos;correlacion;visualizacion;multidimensional system;computer graphics;data management;dimensional analysis;data processing;multidimensional data;traitement donnee;software engineering;imaging techniques;computer programs;algorithme;algorithm;visualization;pipelining computers;visualisation;animation;sistema n dimensiones;data systems;algorithms;visual perception;computer techniques;oleoducto;data reduction;computational grids;systeme n dimensions;correlation;pipeline;graphic arts;algoritmo			Lloyd Treinish;C. Goettsche	1991	IBM Journal of Research and Development	10.1147/rd.351.0184	visualization;data processing;data management;computer science;theoretical computer science;computer graphics (images)	DB	39.03484888833103	-20.074498423987695	38124
0f8fe2576d8cb8858a18a55889fe0cb77d8f0fef	increasing image compression rate using steganography	dwt;steganography;jpeg;dct	The goal of image compression is to remove the redundancies for minimizing the number of bits required to represent an image while steganography works by embedding the secret data in redundancies of the image in invisibility manner. Our focus in this paper is the improvement of image compression through steganography. Even if the purposes of digital steganography and data compression are by definition contradictory, we use these techniques jointly to compress an image. Hence, two schemes exploring this idea are suggested. The first scheme combines a steganographic algorithm with the baseline DCT-based JPEG, while the second one uses this steganographic algorithm with the DWT-based JPEG. In this study data compression is performed twice. First, we take advantage of energy compaction using JPEG to reduce redundant data. Second, we embed some bit blocks within its subsequent blocks of the same image with steganography. The embedded bits not only increase file size of the compressed image, but also decrease the file size further more. Experimental results show for this promising technique to have wide potential in image coding. 2013 Elsevier Ltd. All rights reserved.	algorithm;baseline (configuration management);data compaction;data compression;discrete cosine transform;discrete wavelet transform;display resolution;embedded system;image compression;jpeg;point of view (computer hardware company);steganography	Reza Jafari;Djemel Ziou;Mohammad Mehdi Rashidi	2013	Expert Syst. Appl.	10.1016/j.eswa.2013.06.008	computer vision;steganalysis;steganography tools;image compression;computer science;theoretical computer science;discrete cosine transform;jpeg;steganography;quantization;statistics;computer graphics (images)	EDA	41.34385130022526	-13.53514603898404	38171
2f0201d7d1165ea770b98c74549d6541f4dcaa12	asymptotic distributions for the performance analysis of hypothesis testing of isolated-point-penalization point processes	detection probability approximation;detection probability;forestry;performance evaluation;neural networks;seismology;probability density function;point process;testing;approximation theory maximum likelihood detection poisson distribution;indexing terms;performance analysis testing probability density function signal processing forestry seismology image analysis neural networks performance evaluation approximation methods;markov chain monte carlo simulation;limit theorems;approximation theory;point process model performance analysis log likelihood function hypothesis testing isolated point penalization point processes likelihood ratio test many point interaction point process limit theorems poissonian asymptotic distribution joint probability density function detection probability approximation markov chain monte carlo simulation;markov chain monte carlo;many point interaction point process;signal processing;joint probability density function;likelihood ratio test;hypothesis testing;performance analysis;maximum likelihood detection;poissonian asymptotic distribution;image analysis;approximation methods;asymptotic distribution;point process model;log likelihood function;limit theorem;likelihood function;isolated point penalization point processes;poisson distribution;point interaction;hypothesis test	The performance of the likelihood ratio test is considered for a many-point interaction point process featuring a reduced number of isolated points. Limit theorems are proved that establish the Poissonian asymptotic distribution of the loglikelihood function for point processes with the isolated-pointpenalization joint probability density function. The asymptotic distribution is used to approximate the detection probability associated with the likelihood ratio test. The approximation is compared to empirical results generated using Markov-chain Monte Carlo simulation. The reported results provide an efficient alternative method to simulation in assessing the performance of hypothesis testing for the point-process model considered.	approximation algorithm;classical limit;markov chain monte carlo;monte carlo method;point process;process modeling;profiling (computer programming);simulation	Majeed M. Hayat;John A. Gubner;Sajjad Abdullah	1999	IEEE Trans. Information Theory	10.1109/18.746786	econometrics;mathematical optimization;statistical hypothesis testing;probability density function;p-value;image analysis;asymptotic theory;score test;likelihood-ratio test;signal processing;mathematics;likelihood function;statistics	ML	31.099956951912556	-20.556184146787622	38216
7ea291186440628148fc1d69ae08b1194268d286	induced flocculation of casein micelles: a brownian dynamics simulation on the parsytec gcel mpp	tratamiento paralelo;distributed memory;molecular simulation;fractals;traitement parallele;simulacion numerica;molecular dynamics;confocal scanning laser microscopy;dynamique moleculaire;lennard jones;brownian dynamics;simulation numerique;fractale;low density;dinamica molecular;memoire repartie;fractal structure;parallel processing;numerical simulation;massively parallel processing	Abstract   We perform a Brownian Dynamics simulation of Lennard-Jones particles at low density and temperature. From an initially homogeneous but random configuration the system flocculates into a gel. We study the fractal properties of the gel formed and compare with gels of casein micelles as studied by confocal scanning laser microscopy. In this paper we focus on the aspects of porting the existing code for the simulation program to the Parsytec GCel Massive Parallel Processing computer.	goodyear mpp;simulation	J. H. J. van Opheusden;M. T. A. Bos	1995	Future Generation Comp. Syst.	10.1016/0167-739X(94)00053-H	parallel processing;molecular dynamics;parallel computing;simulation;brownian dynamics;lennard-jones potential;distributed memory;fractal;computer science;massively parallel;computer graphics (images)	Arch	44.50091786055146	3.3186592704278697	38238
bee91db4d73f06d77419659c65d01ea93c857329	image compression predicated on recurrent iterated function systems	fractal image coding;iterated function system ifs;preprint;recurrent iterated function system rifs;fractal image compression	Recurrent iterated function systems (RIFSs) are improvements of iterated function systems (IFSs) using elements of the theory of Marcovian stochastic processes which can produce more natural looking images. We construct new RIFSs consisting substantially of a vertical contraction factor function and nonlinear transformations. These RIFSs are applied to image compression.	image compression;iterated function system;nonlinear system;stochastic process	Chol-Hui Yun;W. Metzler;M. Barski	2008	CoRR		combinatorics;mathematical analysis;discrete mathematics;mathematics;collage theorem;fractal compression;preprint	Logic	39.43977334454767	-6.521611718806649	38408
efe1135d21414e2651fc5f9c1c04470e3da2bbb8	multilocal programming and applications	deterministic method;stochastic method;penalty techniques;engineering process design;multilocal programming	Multilocal programming aims to identify all local maximize rs of unconstrained or constrained nonlinear optimization problems. The multilocal programming theory relies on global optimization strategies combi ned with simple ideas that are inspired in deflection or stretching techniques to a void convergence to the already detected local maximizers. The most used methods to solve this type of problems are based on stochastic procedures. In general, po pulati n-based methods are computationally expensive but rather reliable in ident ifyi g all local solutions. Stochastic methods based on point-to-point strategies are faster to identify the global solution, but sometimes are not able to identify all the opti mal solutions of the problem. To handle the constraints of the problem, some penalty s trategies are proposed. A well-known set of test problems is used to assess the perfor mance of the algorithms. In this chapter, a review on recent techniques for bo th unconstrained and constrained multilocal programming is presented. Some rea l-world multilocal programming problems based on chemical engineering process de sign applications are described. A. I. Pereira Polytechnic Institute of Bragança, Bragança, and Algorit mi R&D Centre, University of Minho, Braga, Portugal, e-mail: apereira@ipb.pt O. Ferreira and S. P. Pinho LSRE/LCM Laboratory of Separation and Reaction Engineerin g, Polytechnic Institute of Bragança, Bragança, Portugal, e-mail:{oferreira,spinho }@ipb.pt E. M. G. P. Fernandes Algoritmi R&D Centre, University of Minho, Braga, Portugal, e-mail: emgpf@dps.uminho.pt	analysis of algorithms;branch and bound;call of duty: black ops;central processing unit;computation;computational complexity theory;constrained optimization;email;experiment;genetic algorithm;global optimization;goodyear mpp;latent class model;mathematical optimization;nonlinear programming;nonlinear system;numerical analysis;penalty method;point-to-point protocol;resources, events, agents (accounting model);simulated annealing;simulation;stationary process;stochastic optimization	Ana I. Pereira;O. Ferreira;S. P. Pinho;Edite M. G. P. Fernandes	2013		10.1007/978-3-642-30504-7_7	mathematical optimization;simulation;mathematics;algorithm	AI	30.398021333016544	0.5690208308973265	38474
5d9a0124c51b569aa382be4eab0e1ed56989492d	a new universal method for solving all problems of operational research	method validation;cybernetics;optimization technique;continuous variable;operations research;optimization techniques;global optimization;design methodology	Purpose – To use a new method based on α‐dense curved for solving problems of operational research.Design/methodology/approach – The method of global optimization (called Alienor) is used for solving problems involving integer or mixed variables. A reducing transformation using α‐dense curves allows to transforms a n‐variables problem into a problem of a single variable.Findings – Extends the basic method valid for continuous variables to problems involving integer, Boolean or mixed variables. All problems of operational research, linear or nonlinear, may be easily solved by or technique based on α‐dense curves (filling a n‐dimensional space). Industrial problems can be quickly solved by this technique obtaining the best solutions.Originality/value – This method is deduced from the original works of Y. Cherruault and colleagues about global optimization and α‐dense curves. It proposes new techniques for solving operational research problems.	operations research	Jean Claude Mazza;Yves Cherruault;Gaspar Mora;Balira O. Konfé;Titem Benneouala	2007	Kybernetes	10.1108/03684920710741161	mathematical optimization;cybernetics;design methods;computer science;artificial intelligence;mathematics;algorithm;global optimization	Logic	32.135630581419974	1.4020429649416482	38477
a7a3ba30fee736aadac483db5cccbddf35138393	solving an arc-routing problem using artificial ants with a graph transformation	traveling salesman problem;graph transformation	  In a recent paper artificial ants were applied to solve the Undirected Rural Postman Problem. The problem was transformed  into a Traveling Salesman Problem and artificial ants were applied to this new problem. This paper applies another transformation,  proposed by Pearn et al, which also transforms the arc-routing problem into a node routing problem.    	artificial ants;graph rewriting;routing	María Luisa Pérez-Delgado	2010		10.1007/978-3-642-12384-9_29	2-opt;mathematical optimization;combinatorics;discrete mathematics;mathematics;3-opt;bottleneck traveling salesman problem	NLP	24.650732750733006	2.2261743367894726	38484
aa325be8e0339cd01984cb91bce47f1833b6d880	an ising computer based on simulated quantum annealing by path integral monte carlo method		In the near future, one of the main processes is solving large combinatorial optimization problems. However, the performance growth of von Neumann architecture will slow due to the end of semiconductor scaling. To resolve this problem, we propose an Ising computer that maps the optimization problems to the ground state search of Ising models. We previously proposed a computer that finds the ground state of Ising models by simulated annealing (SA) approximately. Though the solution quality of the previous prototype is comparable to that of SA, enhancing the solution quality will be required to solve real-world applications. In this paper, we present our FPGA-based Ising computer that executes simulated quantum annealing by using a path integral quantum Monte Carlo method for Ising models on a 48-by-48 king graph with 8-bit couplings. We also propose a shared random number supply, which contributes to decrease the number of random number generators to two. Experimental results indicate that the proposed Ising computer is more than 15 times faster to obtain 99.9%-solution with a probability of 99% than SA running on a state-of-the-art CPU.	8-bit;central processing unit;combinatorial optimization;die shrink;digital electronics;field-programmable gate array;ground state;image scaling;ising model;map;mathematical optimization;monte carlo method;path integral monte carlo;path integral formulation;prototype;quantum monte carlo;quantum annealing;random number generation;semiconductor;simulated annealing;software quality assurance;von neumann architecture	Takuya Okuyama;Masato Hayashi;Masanao Yamaoka	2017	2017 IEEE International Conference on Rebooting Computing (ICRC)	10.1109/ICRC.2017.8123652	quantum monte carlo;quantum annealing;mathematical optimization;path integral monte carlo;quantum computer;simulated annealing;monte carlo method;ising model;mathematics;optimization problem	EDA	33.55119638369498	-0.8024933175923995	38555
54ac162dec2282d61bc5d4f3ac609e23ef441420	quantum-inspired genetic algorithm based on phase encoding	phase encoding;optimization algorithm;quantum computation;optimization biological cells genetic algorithms logic gates convergence encoding quantum computing;quantum genetic algorithm;search problems encoding genetic algorithms quantum gates;optimization algorithm quantum computation quantum genetic algorithm phase encoding;optimization efficiency quantum inspired genetic algorithm phase encoding decoding operations optimization efficiency reduction binary quantum genetic algorithm continuous space optimization chromosome encoding qubit phase quantum rotation gates chromosome mutation quantum pauli z gates function extremum optimization process search capability	Due to frequent decoding operations, the efficiency of optimization is severely reduced when the binary quantum genetic algorithm based on qubits measure is applied to the continuous space optimization. To solve this problem, a quantum genetic algorithm based on phase encoding is proposed in this paper. In this method, the chromosomes are encoded by the phase of qubits, evolved by quantum rotation gates, and mutated by quantum Pauli-Z gates. As the optimization process is performed in [0, 2π], which has nothing to do with specific issues, therefore, the proposed method has good adaptability for a variety of optimization problems. With application of function extremum optimization, the simulation results show that the approach is superior to common quantum genetic algorithm and simple genetic algorithm in both search capability and optimization efficiency.	genetic algorithm;mathematical optimization;maxima and minima;optimization problem;quantum;qubit;simulation	Xiande Liu;Xiaoming Liu	2013	2013 Ninth International Conference on Natural Computation (ICNC)	10.1109/ICNC.2013.6818017	quantum fourier transform;mathematical optimization;meta-optimization;theoretical computer science;mathematics;quantum algorithm for linear systems of equations;quantum algorithm;algorithm;quantum phase estimation algorithm	EDA	27.81564412112048	-5.09550767472131	38577
4590327074cd65dba28bf4882162ba964b3e41c7	a linkage-learning niching in estimation of distribution algorithm	pairwise linkage independent;linkage learning;estimation of distribution algorithm;polynomial time;mutual information;genetic algorithm;niching;hierarchical bayesian optimization algorithm	This work proposes a linkage-learning niching method that improves the capability of estimation of distribution algorithms (EDAs) on reducing spurious linkages which increase problems difficulty. Concatenated parity function (CPF), a class of allelic pairwise independent problems, causes exponential scalability for hierarchical Bayesian optimization algorithm (hBOA), which is one of powerful EDAs. Empirical results show that restricted tournament replacement (RTR) that hBOA employs results in spurious linkages and increases difficulty on solving CPF. Our research consists of these goals: (1) proposing a mutual information matrix to approximate the implicit linkage-information during EDAs' execution, (2) reducing spurious linkages by utilizing new metric of similarity, and (3) maintaining diversity of population. The results show that hBOA with our proposed niching method reduces the spurious linkages and solves CPF in the polynomial time.	approximation algorithm;bayesian optimization;coalition for patent fairness;concatenation;estimation of distribution algorithm;linkage (software);mathematical optimization;mutual information;parity function;real-time recovery;scalability;time complexity	Tsung-Yu Ho;Tian-Li Yu	2012		10.1145/2330784.2330903	time complexity;econometrics;mathematical optimization;genetic algorithm;estimation of distribution algorithm;computer science;artificial intelligence;machine learning;mathematics;mutual information	ML	28.15830674191062	-10.820846779193124	38607
9a08267756a543248f4984d170f16323c10cb345	a hybrid scheme for robust color image watermarking using dswt in dct domain	discrete wavelet transforms;watermarking;visualization;image color analysis;discrete cosine transforms;robustness	This paper proposes a hybrid robust image watermarking scheme based on three levels of Discrete Stationary Wavelet Transform (DSWT) in the Discrete Cosine Transform (DCT) domain. The host image colors (red, green and blue) are separated, and then the DCT is applied on each color after separation. The DSWT is utilized to divide the DCT output into four sub-bands (3 levels). These sub-bands are (A, H, V, D) matrices with the same image size. The watermark is embedded on matrix A. The results of the proposed watermarking scheme are compared with other state-of-the-art schemes. The comparison is based on visualization to detect any degradation of the watermarked image, Peak Signal-to-Noise Ratio (PSNR) of the watermarked image, Normal Correlation (NC) of the extracted watermark after detection, applying attacks, and then calculating the PSNR and NC.	color image;digital watermarking;discrete cosine transform;elegant degradation;embedded system;image resolution;peak signal-to-noise ratio;scheme;stationary process;stationary wavelet transform	Khalid A. Al-Afandy;Osama S. Faragallah;S. El-Rabaie;Fathi E. Abd El-Samie;Ahmed Elmhalawy	2016	2016 4th IEEE International Colloquium on Information Science and Technology (CiSt)	10.1109/CIST.2016.7805088	computer vision;theoretical computer science;mathematics;computer graphics (images)	Vision	41.26623388168332	-10.97057069481659	38608
08d3096eb7d8ca5cde827740860230a9dc5648ec	chaos-enhanced firefly algorithm with automatic parameter tuning		Many metaheuristic algorithms are nature-inspired, and most are population-based. Particle swarm optimization is a good example as an efficient metaheuristic algorithm. Inspired by PSO, many new algorithms have been developed in recent years. For example, firefly algorithm was inspired by the flashing behaviour of fireflies. In this paper, the author extends the standard firefly algorithm further to introduce chaos-enhanced firefly algorithm with automatic parameter tuning, which results in two more variants of FA. The author first compares the performance of these algorithms, and then uses them to solve a benchmark design problem in engineering. Results obtained by other methods will be compared and analyzed. DOI: 10.4018/jsir.2011100101 2 International Journal of Swarm Intelligence Research, 2(4), 1-11, October-December 2011 Copyright © 2011, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. chaotic tunneling is an important phenomenon in complex systems (Tomsovic, 1994; Podolskiy & Narmanov, 2003; Kohler et al., 1998; Delande & Zakrzewski, 2003; Shudo & Ikeda, 1998; Shudo et al., 2009). Traditional wisdom in optimization is to avoid numerical instability and chaos. Contemporary studies suggest that chaos can assist some algorithms such as genetic algorithms (Yang & Chen, 2002). For example, metaheuristic algorithms often use randomization techniques to increase the diversity of the solutions generated during search iterations (Talbi, 2009; Yang, 2010a). The most common randomization techniques are probably local random walks and Lévy flights (Gutowski, 2001; Pavlyukevich, 2007; Yang 2010b). The key challenge for global optimization is that nonlinearity leads to multimodality, which in turns will cause problems to almost all optimization algorithms because the search process may be trapped in any local valley, and thus may cause tremendous difficulty to the search process towards global optimality. Even with most well-established stochastic search algorithms such as simulated annealing (Kirkpatrick et al., 1983), care must be taken to ensure it can escape the local modes/optimality. Premature convergence may occur in many algorithms including simulated annealing and genetic algorithms. The key ability of an efficient global search algorithm is to escape local optima, to visit all modes and to converge subsequently at the global optimality. In this paper, we will first analyze the recently developed firefly algorithm (FA) (Yang, 2008, 2010b). Under the right conditions, FA can have chaotic behaviour, which can be used as an advantage to enhance the search efficiency, because chaos allow fireflies to sample search space more efficiently. In fact, a chaotic tunnelling feature can be observed in FA simulations when a firefly can tunnel through multimodes and jump from one mode to another modes. This enables the algorithm more versatile in escaping the local optima, and thus can guarantee to find the global optimality. Chaotic tunneling is an important phenomenon in complex systems, but this is the first time that a chaotic tunneling is observed in an optimization algorithm. Through analysis and numerical simulations, we will highlight that intrinsic chaotic characteristics in the FA can enhance the search efficiency. Then, we will introduce automatic parameter tuning to the chaotic firefly algorithm and compare its performance against a set of diverse test functions. Finally, we will apply the FA with automatic parameter tuning to solve a design benchmark whose solutions will be compared with other results in the literature. 2. FIREFLY ALGORITHM Firefly Algorithm (FA) was developed by Yang (2008, 2010b), which was based on the flashing patterns and behaviour of fireflies. In essence, each firefly will be attracted to brighter ones, while at the same time, it explores and searches for prey randomly. In addition, the brightness of a firefly is determined by the landscape of the objective function. The movement of a firefly i is attracted to another more attractive (brighter) firefly j is determined by x x e x x i t i t rij j t i t i t + − + − + 1 2 = ( ) , β α ε γ	bios;benchmark (computing);firefly (cache coherence protocol);firefly algorithm;mathematical optimization;metaheuristic;particle swarm optimization	Xin-She Yang	2011	IJSIR	10.4018/jsir.2011100101	mathematical optimization;computer science;artificial intelligence;firefly algorithm;algorithm	AI	27.423469794141827	-4.096547858256583	38627
9f59e3865d80e42e0e7e1cde302487a4c4b00d13	genetic and memetic algorithm with diversity equilibrium based on greedy diversification		The lack of diversity in a genetic algorithm’s population may lead to a bad performance of the genetic operators since there is not an equilibrium between exploration and exploitation. In those cases, genetic algorithms present a fast and unsuitable convergence. In this paper we develop a novel hybrid genetic algorithm which attempts to obtain a balance between exploration and exploitation. It confronts the diversity problem using the named greedy diversification operator. Furthermore, the proposed algorithm applies a competition between parent and children so as to exploit the high quality visited solutions. These operators are complemented by a simple selection mechanism designed to preserve and take advantage of the population diversity. Additionally, we extend our proposal to the field of memetic algorithms, obtaining an improved model with outstanding results in practice. The experimental study shows the validity of the approach as well as how important is taking into account the exploration and exploitation concepts when designing an evolution-	display resolution;diversification (finance);experiment;exploit (computer security);genetic algorithm;genetic operator;greedy algorithm;memetic algorithm	Andrés Herrera-Poyatos;Francisco Herrera	2017	CoRR		genetic algorithm;operator (computer programming);computer science;evolutionary algorithm;mathematical optimization;memetic algorithm;exploit;convergence (routing);diversification (marketing strategy);population	AI	25.355861112279918	-3.773956524604567	38634
4e1bdb8036ee990d113192a2c4b81b20d851706e	ggeffects: tidy data frames of marginal effects from regression models.				Daniel Lüdecke	2018	J. Open Source Software	10.21105/joss.00772	statistics;regression analysis;mathematics	SE	30.503161006127712	-23.36049874747263	38641
1b95419ba0c6cf3fa144352996956206c6766dd5	comparison between multiple description coding and forward error correction for scalable video coding with different burst lengths	video coding forward error correction redundancy;forward error correction redundancy static var compensators bit rate encoding;video coding;forward error correction;redundancy;svc video streaming multiple description coding forward error correction;resilient delivery scalable video coding burst lengths scalable multiple description coding scheme mdc scheme forward error correction coding fec coding raptor code scalable extension h 264 avc svc unequal protection base layer enhancement layer redundancy rate short average burst length	In this paper, a comparison of a scalable multiple description coding (MDC) scheme and the forward error correction (FEC) coding using Raptor code for the scalable extension of H.264/AVC (SVC) is presented. Unequal and equal protection of the base and enhancement layer with different amount of redundancy at the average burst lengths of 2, 5 and 20 are considered. The experimental results show that scalable MDC is generally preferable at the low redundancy rate and long average burst length, while FEC using Raptor code is favorite in case of high redundancy rate and the channel with short average burst length for the resilient delivery of scalable video.	colorburst;data compression;error detection and correction;forward error correction;h.264/mpeg-4 avc;multiple description coding;raptor code;scalability;scalable video coding	Zhijie Zhao;Gunwoo Kim;Doug Young Suh;Jörn Ostermann	2012	2012 IEEE 14th International Workshop on Multimedia Signal Processing (MMSP)	10.1109/MMSP.2012.6343412	scalable video coding;real-time computing;telecommunications;computer science;coding tree unit;forward error correction;redundancy;context-adaptive binary arithmetic coding;computer network	Mobile	47.84611303598026	-16.972109835060834	38714
33847d76c90c46d517c02ad3b86767ec7c1b4e21	on the error analysis associated with the newton-cotes formulae	erreur troncature;quadrature;newton cotes;integracion numerica;methode newton;calcul erreur;cuadratura;truncamiento;error analysis;troncature;numerical integration;calculo error;truncation error;metodo newton;newton method;maple;quadrature formula;truncation;integration numerique;error truncamiento	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;newton;newton–cotes formulas;primary source	Moawwad E. A. El-Mikkawy	2002	Int. J. Comput. Math.	10.1080/00207160212119	newton–cotes formulas;truncation error;quadrature;numerical integration;calculus;truncation;mathematics;newton's method;truncation error;algorithm;statistics	Robotics	49.45485012186242	-2.9410823091541283	38721
6b9ef229c31111a405f23ca840d07c23b69e0d2b	scalable video coding with robust mode selection	transmision paquete;traitement signal;detection erreur;channel coding;evaluation performance;rate distortion;deteccion error;error concealment;performance evaluation;scalable video coding;data compression;source channel coding;correction erreur;signal distortion;packet loss;evaluacion prestacion;optimal estimation;simulation;simulacion;distorsion signal;algorithme;algorithm;video coding;codificacion;enhancement layer;senal video;signal video;error propagation;error correction;signal processing;coding;video signal;error resilience;packet transmission;compresion dato;correccion error;error detection;estimation optimale;motion compensated prediction;transmission paquet;procesamiento senal;compression donnee;scalable coding;codage;estimacion optima;algoritmo;distorsion senal	"""We propose to improve the packet loss resilience of scalable video coding. An algorithm for optimal coding mode selection for the base and enhancement layers is developed, which limits error propagation due to packet loss, while retaining compression e$ciency. We """"rst derive a method to estimate the overall decoder distortion, which includes the e!ects of quantization, packet loss and error concealment employed at the decoder. The estimate accounts for temporal and spatial error propagation due to motion compensated prediction, and computes the expected distortion precisely per pixel. The distortion estimate is incorporated within a rate-distortion framework to optimally select the coding mode as well as quantization step size for the macroblocks in each layer. Simulation results show substantial performance gains for both base and enhancement layers. 2001 Elsevier Science B.V. All rights reserved."""	algorithm;data compression;distortion;error concealment;macroblock;moving picture experts group;network packet;peak signal-to-noise ratio;pixel;propagation of uncertainty;quantization (signal processing);ruby document format;scalability;scalable video coding;simulation;software propagation	Shankar L. Regunathan;Rui Zhang;Kenneth Rose	2001	Sig. Proc.: Image Comm.	10.1016/S0923-5965(01)00003-0	error detection and correction;telecommunications;computer science;theoretical computer science;signal processing;coding tree unit;statistics	Vision	47.22440544257184	-14.494025510249017	38739
bc8f8d102a3f58c839e95abc26c1a928cab8a749	delta algorithms: an empirical analysis	compression algorithm;differencing;empirical analysis;benchmark;delta encoding;compression ratio	Delta algorithms compress data by encoding one file in terms of another. This type of compression is useful in a number of situations: strong multiple versions of data, displaying differences, merging changes, distributing updates, storing backups, transmitting video sequences, and others. This article studies the performance parameters of several delta algorithms, using a benchmark of over 1,300 pairs of files taken from two successive releases of GNU software. Results indicate that modern delta compression algorithms based on Ziv-Lempel techniques significantly outperform diff, a popular but older delta compressor, in terms of compression ratio. The modern compressors also correlate better with the actual difference between files without sacrificing performance.	algorithm;backup;benchmark (computing);data compression;delta encoding;diff utility;gnu;transmitter	James J. Hunt;Kiem-Phong Vo;Walter F. Tichy	1998	ACM Trans. Softw. Eng. Methodol.	10.1145/279310.279321	data compression;real-time computing;benchmark;computer science;theoretical computer science;compression ratio;data mining;lossless compression;incremental encoding;delta encoding	DB	46.93863454409964	-20.708094007089237	38781
179d8e884545e855f07397b9d4561c9e323a1949	any-space probabilistic inference	bayesian network;probabilistic inference	We have recently introduced an any-space algorithm for exact inference in Bayesian networks, called Recursive Conditioning, RC, which allows one to trade space with time at increments of X-bytes, where X is the number of bytes needed to cache a floating point number. In this paper, we present three key extensions of RC. First, we modify the algorithm so it applies to more general factorizations of probability distributions, including (but not limited to ) Bayesian network factorizations. Sec­ ond, we present a forgetting mechanism which reduces the space requirements of RC considerably and then compare such requirements with those of variable elim­ ination on a number of realistic networks, showing orders of magnitude improvements in certain cases. Third, we present a ver­ sion of RC for computing maximum a pos­ teriori hypotheses (MAP}, which turns out to be the first MAP algorithm allowing a smooth time-space tradeoff. A key advan­ tage of the presented MAP algorithm is that it does not have to start from scratch each time a new query is presented, but can reuse some of its computations across mul­ tiple queries, leading to significant savings in certain cases.	algorithm;bayesian network;byte;computation;hp multi-programming executive;map;recursion (computer science);requirement;sion's minimax theorem;space–time tradeoff;symbolic computation;time complexity	Adnan Darwiche	2000			variable elimination;computer science;artificial intelligence;theoretical computer science;machine learning;bayesian network;mathematics;statistics	AI	34.52336125278336	1.9799771669798267	38789
e7b5f57136de743b87d56e5b62927bdc638a19e4	convergent micro-pipelines: a versatile operator for mixed asynchronous-synchronous computations	low power vision convergent micro pipelines mixed asynchronous synchronous computations linear 1d structures retinotopic vlsi vision chips 2d image regions mid level vision computations programmable artificial retinas image sensors pixel digital processor;image sensors;asynchronous communication pixel very large scale integration image converters retina image sensors communication networks clocks computer vision microprocessors;computer vision;chip;low power;asynchronous communication;low power electronics;vlsi;asynchronous circuits;image sensor;pipeline processing;low power electronics computer vision pipeline processing vlsi asynchronous circuits image sensors	Micro-pipelines are linear (1D) structures for asynchronous communications. In retinotopic VLSI vision chips, communicating over 2D image regions is a key to efficient mid-level vision computations. However, micro-pipelines are limited to 1D communications only. In this paper, we introduce an extension of the micro-pipeline, called the convergent micro-pipeline, for implementing mixed asynchronous-synchronous regional computations over arbitrary shaped regions. This operator is exploited in programmable artificial retinas (PAR), that is, image sensors with a digital processor in each pixel, for low power vision applications. To illustrate the behavior and versatility, several regional computations are described.	algorithm;computation;image processing;image sensor;pipeline (computing);pixel;transistor;very-large-scale integration;visual prosthesis	Valentin Gies;Thierry M. Bernard;Alain Mérigot	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1465817	embedded system;computer vision;electronic engineering;computer science;theoretical computer science;image sensor	Arch	41.71352509310732	-2.947304453471356	38813
c92f55d57fbcd923f4961e84e37abb7b03faac2f	"""cluscale (""""clustering and multidimensional scal[e]ing""""): a three-way hybrid model incorporating overlapping clustering and multidimensional scaling structure"""	chevauchement;classification automatique statistiques;discrete parameter modeling;analyse multivariable;echelonnement multidimensionnel;escala multidimensional;analyse variance 3 criteres;metric analysis;covariance analysis;computacion informatica;programacion entera;multivariate analysis;analisis variancia 3 criterios;generic model;analisis datos;multi linear model;three way analysis of variance;modelo hibrido;overlapping clusters;indscal;metric;overlap;modele hybride;imbricacion;programmation en nombres entiers;hybrid model;data analysis;analysis of correlation data;cluster analysis;analyse covariance;integer programming;heterogeneidad;modele discret parametre;clustering;ciencias basicas y experimentales;matematicas;multidimensional scaling;analyse correlation;mixed integer programming;analisis multivariable;analyse donnee;metrico;optimisation non lineaire;mds;parameter estimation;variance decomposition;cluster analysis statistics;analisis covariancia;individual difference;indclus;grupo a;perceptual mapping 3 way three way analysis;nonlinear optimization;tri linear model;metrique;analisis correlacion;heterogeneity;heterogeneite;analysis of covariance data;cluscale;correlation analysis	Traditional techniques of perceptual mapping hypothesize that stimuli are differentiated in a common perceptual space of quantitative attributes. This paper enhances traditional perceptual mapping techniques such as multidimensional scaling (MDS) which assume only continuously valued dimensions by presenting a model and methodology called CLUSCALE for capturing stimulus differentiation due to perceptions that are qualitative, in addition to quantitative or continuously varying perceptual attributes or dimensions. It provides models and OLS parameter estimation procedures for both a two-way and a three-way version of this general model. Since the twoway version of the model and method has already been discussed by Chaturvedi and Carroll (2000), and a stochastic variant discussed by Navarro and Lee (2003), we shall deal in this paper almost entirely with the three-way version of this model. We recommend the use of the three-way approach over the two-way approach, since the three-way approach both accounts for and takes advantage of the heterogeneity in subjects’ perceptions of stimuli to provide maximal information; i.e., it explicitly deals with individual differences among subjects.	carroll morgan (computer scientist);cluster analysis;estimation theory;image scaling;maximal set;multidimensional scaling;ordinary least squares;scalability	Anil K Chaturvedi;J. Douglas Carroll	2006	J. Classification	10.1007/s00357-006-0016-0	econometrics;combinatorics;integer programming;machine learning;mathematics;cluster analysis;statistics	AI	34.05630740001707	-23.386655655128017	38823
01d5dc6376d133e881057f67c7d9d52fa7f35b8f	polynomial-phase signal source tracking using an electromagnetic vector-sensor	azimuth;antenna arrays;radar tracking;polynomials;vectors electromagnetics azimuth radar tracking polynomials signal processing estimation;radar tracking antenna arrays;vectors;estimation;signal processing;fm radar antenna arrays radar tracking direction of arrival estimation;electromagnetics;polynomial phase source scenario polynomial phase signal source tracking electromagnetic vector sensor preprocessing technique single forgetting factor algorithm multiple forgetting factor algorithm;fm radar;direction of arrival estimation	A pre-processing technique is developed to track a polynomial-phase signal using an electromagnetic vector-sensor, which can be collocated or spatially-spread. The performance of the single-forgetting-factor algorithm incorporating the proposed pre-processing approach is improved significantly, and it even surpasses the performance of the multiple-forgetting-factor algorithm in a polynomial-phase source scenario. Simulation results verify the efficacy of the proposed technique.	algorithm;computation;polynomial;preprocessor;sensor;simulation;source tracking;subscriber identity module	Xin Yuan	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288443	computer vision;estimation;continuous-wave radar;radar tracker;radar engineering details;radar lock-on;electromagnetism;fire-control radar;bistatic radar;low probability of intercept radar;signal processing;pulse-doppler radar;mathematics;azimuth;radar imaging;statistics;polynomial	Robotics	49.516941115118144	2.3088233417285577	38840
6825ac626617df365c3735de701936e149b71e12	efficient motion planning for humanoid robots using lazy collision checking and enlarged robot models	humanoid robot;free bubble path validation motion planning humanoid robot lazy collision checking enlarged robot model navigation collision free path collision detection collision avoidance sampling based technique rapidly exploring random trees;degree of freedom;rapidly exploring random trees;mobile robots;trees mathematics;free bubble path validation;enlarged robot model;navigation;collision detection;humanoid robots;rapidly exploring random tree;random processes;motion planning;trees mathematics collision avoidance humanoid robots mobile robots random processes sampling methods;collision free path;collision avoidance;sampling methods;humanoid robots motion planning intelligent robots kinematics runtime trajectory path planning usa councils computer science;lazy collision checking;sampling based technique	Motion planning for humanoid robotic systems with many degrees of freedom is an important and still generally unsolved problem. To give the robot the ability of acting and navigating in complex environments, the motion planner has to find collision-free paths in a robust manner. The runtime of a planning algorithm is critical, since complex tasks require several planning steps where the collision detection and avoidance should be accomplished in reasonable time. In this paper we present an extension of standard sampling-based techniques using Rapidly Exploring Random Trees (RRT). We extend the free-bubble path validation algorithm from Quinlan, which can be used to guarantee the collision-free status of a C-space path between two samples. By using enlarged robot models it is possible to avoid costly distance calculations and therefore to speed up the planning process. We also present a combined approach based on lazy collision checking that brings together the advantages of fast sampling-based and exact path-validated algorithms. The proposed algorithms have been evaluated by experiments on a humanoid robot in a kitchen environment and by a comparison to a validation based on Quinlan's free bubbles approach.	3d modeling;algorithm;automated planning and scheduling;collision detection;critical path method;experiment;humanoid robot;lazy evaluation;level of detail;mathematical model;motion planning;ross quinlan;sampling (signal processing);uniform resource identifier	Nikolaus Vahrenkamp;Tamim Asfour;Rüdiger Dillmann	2007	2007 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2007.4399240	stochastic process;computer vision;simulation;computer science;humanoid robot;artificial intelligence	Robotics	53.12436496814006	-23.741210250090596	38852
7c5c38ab4c54f4f973a7a8c3de8d901f10ad0e81	adaptive watermarking in spatial domain for still image				Somchok Kimpan;Attasit Lasakul;Chom Kimpan	2004			digital watermarking;computer vision;artificial intelligence;computer science	Vision	41.85199225028988	-11.905332069539103	38876
f8d285eaae93971692be2386df1e4b5c9bf17561	multi-layer image compression algorithm for e-commerce	data compression electronic commerce image coding;electronic commerce;image coding business image restoration visualization image quality laboratories image processing military computing computer science usa councils;image coding;data compression;e commerce;low resolution;laplacian pyramid;image compression;image decomposition;minimum mean square error;inverse pyramidal decomposition pyramidal image decomposition multilayer image compression algorithm e commerce low resolution components oriented surfaces difference pyramid minimum mean square error	A new method for pyramidal image decomposition called inverse is presented. This method permits to obtain first low resolution components while mainly concentrating image energy in these first components. Each one of its components is defined on the basis of approximating models representing the so called oriented surfaces of second degree, which constitute the different levels of the corresponding difference pyramid in the image space with minimum mean-square error (MSE). The inverse pyramidal decomposition (IPD) is compared to the pyramidal structures based on the well-known Laplacian pyramid and to the related reduced pyramids.	algorithm;data compression;e-commerce;image compression	Rumen Kounchev;Rumiana Kouncheva;Stuart Harvey Rubin;Mariofanna G. Milanova	2001		10.1109/ICSMC.2001.972945	data compression;e-commerce;image texture;minimum mean square error;computer vision;pyramid;feature detection;image resolution;image compression;computer science;theoretical computer science;machine learning;statistics	ML	46.868150985230706	-12.461452264607695	38924
261c37c45a3a72d1f2145e7d1ead11355710c2fb	hierarchical neural network based compression of ecg signals	hierarchical neural networks;data compression;lossy compression;information content;electrocardiogram	Electrocardiogram (ECG) data compression algorithm is needed that will reduce the amount of data to be transmitted, stored and analyzed, but without losing the clinical information content. An example of application of hierarchical neural network structure is described for compression of ECG signals. Then results of this lossy compression method were compared with two efficient compression methods that are fractal based and wavelet based compressions.	algorithm;artificial neural network;data compression;fractal;lossy compression;self-information;wavelet	Bekir Karlik	2003		10.1007/3-540-44860-8_38	data compression;lossy compression;data compression ratio;self-information;image compression;computer science;theoretical computer science;machine learning;pattern recognition;statistics	ML	42.31610026067545	-15.48225814441217	38928
cdc892c0b869ef9b6926268bf9da1a233df2f78d	the new adaptive etlbo algorithms with k-armed bandit model		TLBO is a novel efficient swarm intelligent algorithm. In this paper, we first analyze TLBO and ETLBO algorithms in detail. Aiming at the disadvantage of ETLBO that it has to adjust the number of elite according to the different problems, we propose an improved adaptive ETLBO algorithm AETLBO-KAB that is based on K-armed bandit model. Experiments are carried out on six popular continuous non-linear test functions, and the results show that AETLBO-KAB algorithm is effective and brings dramatic improvement compared with TLBO and ETLBO. Furthermore, a new perturbation strategy—discussion group strategy is proposed. And the experimental results indicate that the efficiency of AETLBO-KAB with discussion group algorithm exceeds AETLBO-KAB algorithm.		Xitong Wang;Yonggang Zhang;Jiaxu Cui	2018		10.1007/978-3-319-99247-1_3	swarm behaviour;machine learning;artificial intelligence;computer science;algorithm;constrained optimization	ML	27.172854176546785	-4.328820466365372	38965
5ed29a2dccbc537d5fb14d274cf4c1f8ef735399	improved sampling of the pareto-front in multiobjective genetic optimizations by steady-state evolution: a pareto converging genetic algorithm	pareto converging;pareto front;benchmark problem;genetic drift;genetics;stopping criterion;island model genetic algorithms multiobjective optimization steady state pareto converging rank histogram;multiobjective optimization;genetic algorithm;genetic algorithms;island model;rank histogram;local minima;multiobjective genetic algorithm;steady state	Previous work on multiobjective genetic algorithms has been focused on preventing genetic drift and the issue of convergence has been given little attention. In this paper, we present a simple steady-state strategy, Pareto Converging Genetic Algorithm (PCGA), which naturally samples the solution space and ensures population advancement towards the Pareto-front. PCGA eliminates the need for sharing/niching and thus minimizes heuristically chosen parameters and procedures. A systematic approach based on histograms of rank is introduced for assessing convergence to the Pareto-front, which, by definition, is unknown in most real search problems. We argue that there is always a certain inheritance of genetic material belonging to a population, and there is unlikely to be any significant gain beyond some point; a stopping criterion where terminating the computation is suggested. For further encouraging diversity and competition, a nonmigrating island model may optionally be used; this approach is particularly suited to many difficult (real-world) problems, which have a tendency to get stuck at (unknown) local minima. Results on three benchmark problems are presented and compared with those of earlier approaches. PCGA is found to produce diverse sampling of the Pareto-front without niching and with significantly less computational effort	benchmark (computing);biological evolution;computation;convergence (action);feasible region;genetic drift;genetic algorithm;gibbs sampling;heuristic;maxima and minima;newman's lemma;pareto efficiency;sampling (signal processing);sampling - surgical action;steady state	Rajeev Kumar;Peter Rockett	2002	Evolutionary Computation	10.1162/106365602760234117	mathematical optimization;genetic algorithm;computer science;multi-objective optimization;machine learning;genetic representation;mathematics;mathematical economics;genetics	AI	26.360109296076622	-6.645835528194349	39007
3b75793633e3a2e9ff253cd223caef361ea4176f	block encryption using hybrid additive cellular automata	three dimensional imaging;tomography data visualisation image representation;3d imaging;data interpretation;orientation map;fiber reinforced composite;visual representations fiber orientation estimation 3d image data data visualization data interpretation fiber reinforced composites microcomputed tomography;fiber orientation estimation;3d image data;algorithm visualization;linear filtering;data visualisation;visual representations;efficient implementation;visual representation;microcomputed tomography;micro computed tomography;image representation;data visualization;data visualization aerospace materials aerospace industry composite materials automotive engineering microscopy stability computed tomography image segmentation maximum likelihood detection;orientation estimation;tomography;integral geometry;fiber reinforced composites	With the ever-increasing growth of data communication, the need for security and privacy has become a strong necessity. In these conditions, the necessity of new powerful encryption techniques becomes a crucial issue. In this paper Cellular Automata (CA) are applied to construct cryptography algorithms. We present an encryption system implemented on a structure of Hybrid Additive Cellular Automata (HACA) used for securing the medical data sent over the internet. The experimental results prove the power of cellular automata encryption systems. The method supports both software and hardware implementation. In this paper we present a fully functional software application for the data encryption of multimedia medical content.	additive model;algorithm;automata theory;block cipher;cellular automaton;encryption;privacy	Petre Anghelescu;Silviu Ionita;Emil Sofron	2007	7th International Conference on Hybrid Intelligent Systems (HIS 2007)	10.1109/HIS.2007.26	materials science;computer vision;theoretical computer science;engineering drawing	Embedded	38.31928917794458	-15.029632177360174	39013
6b3a3e3a5f5e1ddae7fb92991ffe2ec2ff0d8460	revisiting the memory of evolution	evolution strategy;binary search	A new evolution scheme is presented, memorizing the extreme (best and worst) past individuals through distributions over the binary search space. These distributions are used to bias the mutation operator in a (+) Evolution Strategy, guiding the generation of newborn oospring: diierent mimetic strategies are deened, combining either attraction, indiierence or repulsion with respect to the two distributions. These distributions are then updated from the best and the worse individuals in the current population. Experiments on large size binary problems allow one to delineate the niche of each of these mimetic strategies.	binary search algorithm;evolution strategy;niche blogging	Michèle Sebag;Marc Schoenauer;Mathieu Peyral	1998	Fundam. Inform.	10.3233/FI-1998-35123408	mathematical optimization;computer science;artificial intelligence;mathematics;evolution strategy;programming language;algorithm;statistics;binary search algorithm	ML	26.903229214929343	-7.893957627834908	39072
fd01a4edf022933926e6c6e82eaa5c26bbf350d6	a lossless image coder with context classification, adaptive prediction and adaptive entropy coding	cluster algorithm;image coding;290901;residual pixels image context classification context classification adaptive prediction adaptive entropy coding adaptive lossless image coder local edge texture gradient characteristics local activity optimal predictor clustering algorithm optimal entropy coding prediction residual;predictive value;institute for integrated and intelligent systems;faculty of science environment engineering and technology;entropy coding;image classification;adaptive codes;image texture;adaptive signal processing;prediction theory;entropy codes;pre2009 electrical engineering;image texture image coding image classification adaptive codes adaptive signal processing prediction theory entropy codes;entropy coding image coding image storage microelectronics australia local activities clustering algorithms algorithm design and analysis biomedical imaging satellites	In this paper, we combine a context classification scheme with adaptive prediction and entropy coding to produce an adaptive lossless image code?. In this coder, we maximize the benefits of adaptivity using both adaptive prediction and entropy coding. The adaptive prediction is closely tied with the classification of contexts within the image. These contexts are defined with respect to the local edge, texture or gradient characteristics as well as local activity within small blocks of the image. For each context an optimal predictor is found which is used for the prediction of all pixels belonging to that particular context. Once the predicted values have been removed from the original image, a clustering algorithm is used to design a separate, optimal entropy coding scheme for encoding the prediction residual. Blocks of residual pixels are classified into a finite number of classes and members of each class are encoded using the entropy coder designed for that particular class. The combination of these two powerful techniques produces some of the best lossless coding results reported so far.	algorithm;cluster analysis;comparison and contrast of classification schemes in linguistics and metadata;entropy encoding;gradient;kerrison predictor;lossless compression;pixel	Farshid Golchin;Kuldip K. Paliwal	1998		10.1109/ICASSP.1998.678041	adaptive filter;arithmetic coding;image texture;computer vision;dictionary coder;contextual image classification;computer science;entropy encoding;context-adaptive variable-length coding;machine learning;pattern recognition;mathematics;tunstall coding;adaptive coding	Vision	44.28902930264493	-14.092273533129882	39074
5c7d0d33b19e09465123c77d6bbfbd25d9646590	genetic algorithms for solving the discrete ordered median problem	location problem;probleme localisation;genetic operator;localization;evolutionary computations;heuristic method;metodo heuristico;localizacion;algoritmo genetico;discrete location;optimisation combinatoire;feasibility;localisation;algorithme genetique;algorithme evolutionniste;genetic algorithm;genetic algorithms;algoritmo evolucionista;problema localizacion;methode heuristique;evolutionary algorithm;combinatorial optimization;practicabilidad;faisabilite;optimizacion combinatoria;evolutionary computing	In this paper we present two new heuristic approaches to solve the Discrete Ordered Median Problem (DOMP). Described heuristic methods, named HGA1 and HGA2 are based on a hybrid of genetic algorithms (GA) and a generalization of the well-known Fast Interchange heuristic (GFI). In order to investigate the effect of encoding on GA performance, two different encoding schemes are implemented: binary encoding in HGA1, and integer representation in HGA2. If binary encoding is used (HGA1), new genetic operators that keep the feasibility of individuals are proposed. Integer representation keeps the individuals feasible by default, so HGA2 uses slightly modified standard genetic operators. In both methods, caching GA technique was integrated with the GFI heuristic to improve computational performance. The algorithms are tested on standard ORLIB p-median instances with up to 900 nodes. The obtained results are also compared with the results of existing methods for solving DOMP in order to assess their merits. 2006 Elsevier B.V. All rights reserved.	binary file;binary number;cache (computing);code;computation;ecology;experiment;genetic algorithm;genetic operator;heuristic;heuristic (computer science);norm (social);operations research;parallel computing;performance;software release life cycle;whole earth 'lectronic link	Zorica Stanimirovic;Jozef Kratica;Djordje Dugosija	2007	European Journal of Operational Research	10.1016/j.ejor.2006.09.069	mathematical optimization;combinatorics;genetic algorithm;combinatorial optimization;computer science;artificial intelligence;evolutionary algorithm;mathematics;algorithm	AI	25.335327025332344	1.7530783903274305	39192
43276e88b84e9de4371ec5c6a26caff409cbd67f	ordering genetic algorithm genomes with reconstructability analysis: discrete models	optimal gene ordering genetic algorithm reconstructability analysis discrete model epistatic gene discrete valued problem kauffman nk model information theory;genetic algorithms genomics bioinformatics algorithm design and analysis biological cells educational institutions information analysis sampling methods adaptive systems system testing;occam reconstructability analysis genetic algorithms transposition crossover optimization;information theory genetic algorithms;building block hypothesis;discrete model;gene order;genetic algorithm;genetic algorithms;information theoretic;reconstructability analysis;information theory	The building block hypothesis implies that genetic algorithm effectiveness is influenced by the relative location of epistatic genes on the chromosome. We demonstrate this with a discrete-valued problem, based on Kauffman's NK model, and show that information-theoretic reconstructability analysis can be used to decide on optimal gene ordering.	genetic algorithm;information theory	Stephen Shervais;Martin Zwick	2005	2005 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2005.1571622	genetic algorithm;information theory;computer science;bioinformatics;artificial intelligence;machine learning;mathematics;algorithm;statistics	Robotics	27.01947655565201	-9.424020269518492	39212
a9575bf06e49f80055ce7e5998bd1a9fe7fd75ea	simplified algorithms for rate-distortion optimization in high efficiency video coding	simplified context adaptive binary arithmetic coding;high efficiency video coding;rate distortion optimization;simplified sum of squared error	HEVC is the latest coding standard to improve the coding efficiency by a factor of two over the previous H.264/AVC standard at the cost of the increased complexity of computation rate-distortion optimization (RDO) is one of the computationally demanding operations in HEVC and makes it difficult to process the HEVC compression in real time with a reasonable computing power. This paper aims to present various simplified RDO algorithms with the evaluation of their RD performance and computational complexity. The algorithms for the simplified estimation of the sum of squared error (SSE) and context-adaptive binary arithmetic coding (CABAC) proposed for H.264/AVC are reviewed and then they are applied to the simplification of HEVC RDO. By modifying the previous algorithm for H.264/AVC, a new simplified RDO algorithm is proposed for modifying the previous algorithm for H.264/AVC to be optimized for the hierarchical coding structure of HEVC. Further simplification is attempted to avoid the transforms operations in RDO. The effectiveness of the existing H.264/AVC algorithms as well as the proposed algorithms targeted for HEVC is evaluated and the trade-off relationship between the RD performance and computational complexity is presented for various simplification algorithms. Experimental results show that reasonable combinations of RDO algorithms reduce the computation by 80–85% at the sacrifice of the BD-BR by 3.46–5.93% for low-delay configuration.	algorithm;data compression;distortion;high efficiency video coding;mathematical optimization;rate–distortion optimization	Kyujoong Lee;Tae Sung Kim;Chae-Eun Rhee;Hyuk-Jae Lee	2015	Displays	10.1016/j.displa.2015.06.001	real-time computing;simulation;computer science;theoretical computer science;context-adaptive variable-length coding;rate–distortion optimization;context-adaptive binary arithmetic coding	HCI	47.184648585139215	-18.999393467173572	39253
8d1aa783aa2348a4a1065135fa28ee8cb8953d66	an improved butterfly optimization algorithm with chaos		Butterfly Optimization Algorithm (BOA) is a new comer in the category of nature inspired metaheuristic algorithms, inspired from food foraging behavior of the butterflies. Similar to other metaheuristic algorithms, it encounters two probable problems; (1) entrapment in local optima and (2) slow convergence speed. Chaotic maps are one of the best methods to improve the performance of metaheuristic algorithms. In the present study, chaos is introduced into BOA which increases its performance in terms of both local optima avoidance and convergence speed. Ten chaotic maps are employed to enhance the performance of the BOA. The proposed chaotic BOAs are validated on unimodal and multimodal benchmark test functions as well as on engineering design problems. The results indicate that the chaotic maps are able to significantly boost the performance of BOA.	algorithm;mathematical optimization	Sankalap Arora;Satvir Singh	2017	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-16798	mathematical optimization;simulation;artificial intelligence;mathematics	Robotics	26.9872076073003	-4.170456529378304	39322
3f40d7449db4acca56aaf3e302b49fa3f73e6398	hybrid video coding model of finite ridgelet transform and h.264/avc	finite radon transform hybrid video coding model h 264 avc codec block based finite ridgelet transform ridgelet mode directional transform feature prime number sized block scaling filter;quantization;radon transforms;directional transform feature;finite radon transform;radon transform;directional transform;h 264 avc;ridgelet mode;video coding radon transforms;directional transform h 264 avc ridgelets finite radon transform scaling filter;wavelet transforms;video coding;automatic voltage control;ridgelets;discrete cosine transforms;scaling filter;block based finite ridgelet transform;prime number sized block;prime number;encoding;h 264 avc codec;hybrid video coding model;video coding automatic voltage control discrete wavelet transforms discrete cosine transforms codecs filters discrete transforms wavelet transforms geometry video compression	We developed a new video coding model that combines block-based finite ridgelet transform with H.264/AVC. This model is based on H.264/AVC codec and a new video coding mode called the ridgelet mode is added to it in order to incorporate the directional transform feature. In the ridgelet mode, residuals of each block after intra or inter predictions are transformed into a prime-number-sized block by a scaling filter to fit the finite Radon transform. Then the finite Radon transform and the 1D DCT are applied to the block. The ridgelet mode can adapt to such texture regions that have strong correlation along a specific direction and it can enhance the coding efficiency of the H.264/AVC by its directional feature of the finite Radon transform. We implemented the ridgelet mode onto a H.264/AVC based video encoder. The experimental results show the potential of this method for higher compression efficiency.	algorithmic efficiency;codec;data compression;discrete cosine transform;encoder;experiment;h.264/mpeg-4 avc;image scaling;simulation	Tomokazu Murakami;Masashi Takahashi	2009	2009 Picture Coding Symposium	10.1109/PCS.2009.5167408	computer vision;radon transform;speech recognition;quantization;context-adaptive variable-length coding;mathematics;context-adaptive binary arithmetic coding;prime number;encoding;wavelet transform	AI	44.37427284207204	-15.773662617593835	39325
3abd7ec4ffdf627b6074207a59c4342b06a518cb	toward a coherent monte carlo simulation of cva		This paper is devoted to the simulation of the Credit Valuation Adjustment (CVA) using a pure Monte Carlo technique with Malliavin Calculus (MCM). The procedure presented is based on a general theoretical framework that includes a large number of models as well as various contracts, and allows both the computation of CVA and its sensitivity with respect to the different assets. Moreover, we provide the expression of the backward conditional density of assets vector that can be simulated off-line in order to reduce the variance of the CVA estimator. Using the suitability of MCM to parallel architectures and thus to a Graphic Processing Unit (GPU) implementation, we show that the results obtained are accurate once sufficient number of trajectories are simulated. Both complexity and accuracy are studied for MCM and regression methods and compared to the square Monte Carlo benchmark.	benchmark (computing);coherence (physics);computation;graphics processing unit;line level;malliavin calculus;monte carlo method;multi-chip module;online and offline;simulation	Lokman A. Abbas-Turki;Aych I. Bouselmi;Mohammed A. Mikou	2014	Monte Carlo Meth. and Appl.		econometrics;mathematical optimization	ML	31.62069061479371	-15.900953542088956	39327
c8d4eb0f56b4c10d57b65d13f99267183c19c5e6	implementation and evaluation of nonparametric regression procedures for sensitivity analysis of computationally demanding models	modelizacion;analisis sensibilidad;optimisation;metodo monte carlo;bootstrap;optimizacion;intervalo confianza;confidence intervals;computer model;estimation non parametrique;methode monte carlo;meta analisis;modelisation;non parametric estimation;metaanalysis;systeme incertain;confidence interval;estimation erreur;metamodel;analisis regresion;uncertainty analysis;metamodele;sensitivity index;metamodelo;error estimation;sensitivity analysis;model uncertainty;monte carlo method;intervalle confiance;estimacion error;metaanalyse;nonparametric regression;analyse sensibilite;analyse regression;regression analysis;optimization;estimacion no parametrica;variance decomposition;sistema incierto;modeling;simulation model;surrogate model;uncertain system;monte carlo analysis;meta model	The analysis of many physical and engineering problems involves running complex computational models (simulation models, computer codes). With problems of this type, it is important to understand the relationships between the input variables (whose values are often imprecisely known) and the output. The goal of sensitivity analysis (SA) is to study this relationship and identify the most significant factors or variables affecting the results of the model. In this presentation, an improvement on existing methods for SA of complex computer models is described for use when the model is too computationally expensive for a standard Monte-Carlo analysis. In these situations, a meta-model or surrogate model can be used to estimate the necessary sensitivity index for each input. A sensitivity index is a measure of the variance in the response that is due to the uncertainty in an input. Most existing approaches to this problem either do not work well with a large number of input variables and/or they ignore the error involved in estimating a sensitivity index. Here, a new approach to sensitivity index estimation using meta-models and bootstrap confidence intervals is described that provides solutions to these drawbacks. Further, an efficient yet effective approach to incorporate this methodology into an actual SA is presented. Several simulated and real examples illustrate the utility of this approach. This framework can be extended to uncertainty analysis as well.	additive smoothing;analysis of algorithms;apply;bootstrapping (statistics);code;computation;computational model;computer simulation;feature selection;learning to rank;metamodeling;monte carlo method;sampling (signal processing);sensitivity index;smoothing spline;surrogate model;test case;utility functions on indivisible goods	Curtis B. Storlie;Laura Painton Swiler;Jon C. Helton;Cédric J. Sallaberry	2009	Rel. Eng. & Sys. Safety	10.1016/j.ress.2009.05.007	computer simulation;metamodeling;econometrics;meta-analysis;confidence interval;computer science;artificial intelligence;variance-based sensitivity analysis;mathematics;statistics;monte carlo method	SE	27.672958783676595	-15.482262318261869	39427
411e73528fd6a00f46972f387d9463195525819f	image compression overview.		Compression plays a significant role in a data storage and a transmission. If we speak about a generall data compression, it has to be a lossless one. It means, we are able to recover the original data 1:1 from the compressed file. Multimedia data (images, video, sound...), are a special case. In this area, we can use something called a lossy compression. Our main goal is not to recover data 1:1, but only keep them visually similar. This article is about an image compression, so we will be interested only in image compression. For a human eye, it is not a huge difference, if we recover RGB color with values [150,140,138] instead of original [151,140,137]. The magnitude of a difference determines the loss rate of the compression. The bigger difference usually means a smaller file, but also worse image quality and noticable differences from the original image. We want to cover compression techniques mainly from the last decade. Many of them are variations of existing ones, only some of them uses new principes.	1:1 pixel mapping;computer data storage;data compression;image compression;image quality;lossless compression;lossy compression	Martin Prantl	2014	CoRR		computer science;s3 texture compression;artificial intelligence;lossy compression;image compression;video compression picture types;computer vision;transparency (data compression);data compression;jpeg;lossless compression	ML	41.18842726742294	-16.847338314912584	39433
e9b9922f74e0e57ea759b73af99fe7e4cd8cfbe2	combining forecasts via simulations	combining forecasts;simulation;secondary 62m10;marginal distribution;weighted average;primary 62m20;autocorrelation structure	Combining forecasts can be based on different data or different methods or both. In practice, many different data sets can be difficult to obtain. In this paper we propose a combining forecasts method that uses both simulated and observed time series. Several combining methods have been studied and the combined forecasts have been compared with those obtained from the model fitted to the observed time series only. Simulation studies and applications of the proposed method to real data sets show that on average the combined forecasts are more accurate than those obtained without combining.		Yuzhi Cai;Neville Davies;Wan-Kai Pang	2014	Communications in Statistics - Simulation and Computation	10.1080/03610918.2012.731121	marginal distribution;econometrics;weighted arithmetic mean;data mining;mathematics;statistics	ML	28.81196463038794	-23.32247877338829	39546
bc22ba1f121c3ca843281bb5f30f6f61e6bad4fa	a fast search strategy to optimise path founding in big data graph computing environments		In big data graph computing environments, the shortest path problem is very important and widely applied in various scenarios. But sometimes, in order to find the shortest path, it always entails a lot of cost. So, weighing the various aspects of the problems, itu0027s a good choice to find optimal path. In this paper, an improved A-star algorithm is proposed to define the optimal path. A-star algorithm has two kinds of parameters named actual cost and estimated cost and the second one plays an important part in the algorithm. Based on the traditional A-star algorithm, this paper will propose new parameters and improved heuristic function to enhance the performance of the A-star algorithm. Compared with the traditional A-star algorithm, the simulation experiment shows that the improved A-star algorithm runs at high efficiency. Two factors which affect the best situation are also discussed in the end of this paper.	big data	Wentian Qu;Dawei Sun	2017	IJWMC	10.1504/IJWMC.2017.10009065	suurballe's algorithm;k shortest path routing;widest path problem;shortest path faster algorithm;yen's algorithm;shortest path problem;machine learning;mathematical optimization;computer science;artificial intelligence;fsa-red algorithm;constrained shortest path first	DB	29.98982426928564	-2.575217526223304	39548
2778b410defccfc4c4ef9b810022f4e69d782200	a distribution-free m-out-of-n bootstrap approach to testing symmetry about an unknown median	testing for symmetry;bootstrap;resampling;skewness;mercury bioaccumulation;environmental monitoring	Testing for symmetry about an unknown median is a ubiquitous problem in mathematical statistics, particularly, for nonparametric rank-based methods, and in a broad range of applied studies, from economics and business to biology, ecology, and medicine. However, the challenge still remains on how to derive a symmetry test with a good power performance and at the same time delivering a reliable Type I Error estimate. To overcome this problem, a new data-driven m -out-of- n bootstrap method is introduced for testing symmetry about an unknown median. The asymptotic properties of the developed m -out-of- n bootstrap tests are investigated along with their empirical finite-sample performance. The new tests are illustrated by applications to legal studies and wildlife monitoring.	booting	Vyacheslav Lyubchich;Xingyu Wang;Andrew Heyes;Yulia R. Gel	2016	Computational Statistics & Data Analysis	10.1016/j.csda.2016.05.004	econometrics;mathematical optimization;skewness;resampling;mathematics;environmental monitoring;statistics	Theory	28.40834934949636	-23.325486518093502	39623
25636c9a519e9d09b948297e8aa468274e045749	collusion-resistant intentional de-synchronization for digital video fingerprinting	image sampling;fingerprint recognition sampling methods data security counting circuits redundancy educational institutions bandwidth internet protection degradation;video signal processing;random temporal sampling collusion resistant intentional desynchronization digital video fingerprinting multimedia fingerprinting collusion attacks pseudo random desynchronization techniques spatial sampling;matrix algebra;multimedia systems;synchronisation;matrix algebra multimedia systems synchronisation security of data image sampling video signal processing;digital video;security of data	A powerful class of attacks in multimedia fingerprinting is known as collusion attacks, where a clique of colluders, each having a copy of the same multimedia content with different fingerprint, combine their copies to form a colluded copy. In this paper, we propose a countermeasure against collusion attacks for digital video: pseudo-random intentional de-synchronization techniques. Each user's copy of video is slightly pseudo-randomly changed (de-synchronized) in such a way that these changes will not be noticeable for an individual copy, but will be significant enough to produce perceptual artifacts when multiple copies are combined (e.g., via averaging, replacement attacks, etc.). To achieve this task, we propose several novel effective techniques, including constrained random temporal and spatial sampling. We discuss feasibility issues and limitations of video de-synchronization, and present several examples.	clique (graph theory);digital video fingerprinting;fingerprint (computing);pseudorandomness;randomness;sampling (signal processing)	Yinian Mao;Mehmet Kivanç Mihçak	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1529731	synchronization;computer science;theoretical computer science;multimedia;internet privacy	DB	37.56955913526196	-12.564148687490077	39689
d5541d9dbd12276038a7d32aaba1bde0b4e874d8	computation of optimum reliability acceptance sampling plans in presence of hybrid censoring	type ii hybrid censoring;optimum sampling plan;mil std 105d;producer s risk;consumer s risk;acceptance sampling;type i hybrid censoring;monte carlo simulation	The decision regarding acceptance or rejection of a lot of products may be considered through variables acceptance sampling plans based on suitable quality characteristics. A variables sampling plan to determine the acceptability of a lot of products based on the lifetime of the products is called reliability acceptance sampling plan (RASP). This work considers the determination of optimum RASP under cost constraint in the framework of hybrid censoring. Weibull lifetime models are considered for illustrations; however, the proposedmethodology can be easily extended to any location-scale family of distributions. The proposed method is based on asymptotic results of the estimators of parameters of lifetime distribution. Hence, a Monte Carlo simulation study is conducted in order to show that the sampling plans meet the specified risks for finite sample size. © 2014 Elsevier B.V. All rights reserved.	censoring (statistics);computation;hcs clustering algorithm;monte carlo method;random-access stored-program machine;real life;rejection sampling;reliability engineering;sampling (signal processing);simulation	Ritwik Bhattacharya;Biswabrata Pradhan;Anup Dewanji	2015	Computational Statistics & Data Analysis	10.1016/j.csda.2014.10.002	econometrics;mathematics;acceptance sampling;statistics;monte carlo method	AI	28.87358479821617	-19.1621181402166	39691
f33442727af18e96e7749ff6f434b72fb215bb1e	design of tree filter algorithm for random number generator in crypto module	analisis imagen;modelizacion;filtering;filtrage;random number generator;filtrado;random number generation;generateur nombre aleatoire;modelisation;filter;generation nombre aleatoire;pattern recognition;filtre;image analysis;reconnaissance forme;reconocimiento patron;modeling;analyse image;filtro;random number generators;generacion numero aleatorio	For a hardware random number generator (RNG) in a crypto module, it is important that the RNG hardware offers an output bit stream that is always unbiased. J. H., et al. proposed a combination of the hardware component and a software filter algorithm. However, even though the hardware generating processor generates an output bit stream quickly, if the software filter algorithm is inefficient, the RNG becomes time consuming, thereby restricting the conditions when an RNG can be applied. Accordingly, this paper proposes an effective method of software filtering for an RNG processor in a crypto module. To consistently guarantee the randomness of the output sequence from a RNG, the origin must be stabilized, regardless of any change in circumstances. Therefore, a tree model is proposed to apply the filter algorithm, making it less time consuming than J. H.'s conventional filter algorithm scheme.	peterson's algorithm;random number generation	Jinkeun Hong;Kihong Kim	2005		10.1007/11559573_43	computer vision;discrete mathematics;image analysis;random number generation;computer science;theoretical computer science;operating system;mathematics;algorithm	Crypto	38.5921611609362	-16.647672209710173	39712
a4e67444a1203ae1fe76c07ace9499c35fbb71bb	a new experiential learning electromagnetism-like mechanism for numerical optimization		The Electromagnetism-like Mechanism algorithm (EM) is a population-based search algorithm which has shown good achievements in solving various types of complex numerical optimization problems so far. To date, the study on experience-based local search mechanism is relatively limited, and there is no study in the literature to integrate experience-based features into the EM. This work introduces an experience-learning feature into the EM for the first time. A new Experiential Learning Electromagnetism-like Mechanism algorithm (ELEM) is proposed in this paper. The ELEM is integrated with two new components. The first component is the particle memory concept which allows the particles to remember the details of their past search experience. The second component is the experience analysing and decision making mechanisms which enables the particles to adjust the settings for the coming iterations. Combining the advantages of this strong exploitation strategy and the powerful exploration mechanism of the EM, the proposed ELEM strikes a good balance in providing well diversified solutions with high accuracy. The results from extensive numerical experiments carried out using 21 challenging test functions show that ELEM is able to provide very competitive solutions and significantly outperforms other optimization techniques. It can thus be concluded from the results that the proposed ELEM performs well in solving high dimensional numerical optimization problems.	mathematical optimization	Jian-Ding Tan;Mahidzal B. Dahari;Johnny Siaw Paw Koh;Ying-Ying Koay;Issa-Ahmed Abed	2017	Expert Syst. Appl.	10.1016/j.eswa.2017.06.002	machine learning;electromagnetism;metaheuristic;experiential learning;local search (optimization);computer science;population;search algorithm;optimization problem;artificial intelligence	ML	26.054540065086208	-3.390301118845855	39725
1a86b7e40c856622c4979bf97e7683afeb4b3d31	microphone-based vibration sensor for ugs applications	mathematical model vibrations micromechanical devices transfer functions microphones seismic waves resonant frequency;vibration microphone seismic sensor sound unattended ground sensor ugs	To be an ideal candidate, seismic sensor for wireless unattended ground sensor (UGSs) applications should have lightweight, low noise, high sensitivity, and energy efficiency. Typically, to cover regions far from source, the best choice will be the coil-over-magnet geophone. Because it is usually heavier than batteries, a lighter one should replace it to further cut the weight of sensor nodes. However, currently available seismic sensors, such as micro-electro mechanical systems (MEMS) accelerometers and molecular-electronic transducers, cannot do the job since they usually consume too much energy to achieve low noise level as well as high sensitivity. This work has designed and tested a new kind of vibration sensor, the vibration-to-sound geophone, which can convert seismic waves into sound physically that can then be detected by an MEMS microphone. By using a battery as its proof mass, the vibration-to-sound geophone 1) has better sensibility than the coil-over-magnet geophone from 20 to 500 Hz and is about 58 times more sensitive at 70 Hz which is more than 60 dBV/m; 2) is very light, half the weight of the coil one; and 3) consumes no more than 726 μW which is more energy-efficient than MEMS accelerometers and molecular-electronic transducers.	microelectromechanical systems;microphone;noise (electronics);sensor web;transducer	Qianwei Zhou;Baoqing Li;Huawei Liu;Shengyong Chen;Jingchang Huang	2017	IEEE Transactions on Industrial Electronics	10.1109/TIE.2017.2682015	electronic engineering;speech recognition;acoustics;engineering	Mobile	45.19164660324033	-0.6891815653538635	39739
67dc74714a40fdedc3689bd078b3c2d1db1a782c	a sequence cipher producing method based on two-layer ranking multi-objective evolutionary algorithm	long period;evolutionary computation;random sequences cryptography evolutionary computation;chaos;random sequences;multi objective evolutionary algorithm;two layer ranking multiobjective evolutionary algorithm;mutation operator;cryptosystem;crossover operator;cryptography;chaos sequence cipher producing method two layer ranking multiobjective evolutionary algorithm cryptosystem crossover operator mutation operator key sequences;sequence cipher producing method;random numbers;high efficiency;key sequences	Aiming at designing a high safe and high efficiency cryptosystem, the period of the sequence cipher can not be too long, and the cipher sequence produced should approach random numbers. But the key sequence produced by traditional methods sometimes does not have randomness, which makes insecurity the system using this key sequence. Considering this, in this paper, we take two criteria usually used to evaluate the randomness of a key sequence as two objectives of multi-objective evolutionary algorithm (MOEA), and a new sequence cipher producing method based on two-layer MOEA is proposed (called TLEASCP). Because of TLEASCP is based on the randomness of crossover operator and mutation operator of the high efficient MOEA, the key sequences produced by TLEASCP have the merits of high randomness, chaos and long period.	cipher;cryptosystem;evolutionary algorithm;moea framework;randomness	Kangshun Li;Weifeng Pan;Wensheng Zhang;Zhangxin Chen	2008	2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)	10.1109/CEC.2008.4630794	discrete mathematics;computer science;cryptography;theoretical computer science;machine learning;randomness tests;cryptosystem;mathematics;algorithm;evolutionary computation	Vision	27.969151658827755	-1.3300937060382898	39796
8f967f1ccaa18252e1cfbef77e5e369665c47699	cma-es: evolution strategies and covariance matrix adaptation	cma es;covariance matrix adaptation;non convex optimization;search space;multivariate normal;stochastic optimization;estimation of distribution algorithm;gradient descent;evolution strategy;evolutionary optimization;algorithm design;gaussian distribution	Evolution Strategies (ESs) and many continuous domain Estimation of Distribution Algorithms (EDAs) are stochastic optimization procedures that sample a multivariate normal (Gaussian) distribution in the continuous search space, Rn. Many of them can be formulated in a unified and comparatively simple framework. This introductory tutorial focuses on the most relevant algorithmic question: how should the parameters of the sample distribution be chosen and, in particular, updated in the generation sequence? First, two common approaches for step-size control are reviewed, one-fifth success rule and path length control. Then, Covariance Matrix Adaptation (CMA) is discussed in depth: rank-one update, the evolution path, rank-mu update. Invariance properties and the interpretation as natural gradient descent are touched upon.  In the beginning, general difficulties in solving non-linear, non-convex optimization problems in continuous domain are revealed, for example non-separability, ill-conditioning and ruggedness. Algorithmic design aspects are related to these difficulties. In the end, the performance of the CMA-ES is related to other well-known evolutionary and non-evolutionary optimization algorithms, namely BFGS, DE, PSO,...	broyden–fletcher–goldfarb–shanno algorithm;cma-es;condition number;convex optimization;estimation of distribution algorithm;evolution strategy;gnu octave;gradient descent;information geometry;java;linear separability;matlab;mathematical optimization;nonlinear system;particle swarm optimization;python;scilab;stochastic optimization	Nikolaus Hansen;Anne Auger	2011		10.1145/2001858.2002123	mathematical optimization;cma-es;estimation of distribution algorithm;computer science;stochastic optimization;machine learning;mathematics;random optimization;statistics;natural evolution strategy	ML	31.016267499838612	-1.0785182077283357	39827
28f7fe129f32852e92fde4a0afe42d5b1d2274fb	a unifying framework for analysing common cyclical features in cointegrated time series	autocorrelacion;iterative method;serial correlation;association statistique;analyse multivariable;stochastic process;multivariate analysis;analisis datos;correlation serielle;common features;62m10;62h20;statistical association;time series;polynomial;statistical regression;62jxx;metodo iterativo;data analysis;asociacion estadistica;methode iterative;regresion estadistica;polinomio;statistical computation;serie temporelle;calculo estadistico;analyse correlation;reduced rank regression;settore secs s 03 statistica economica;processus stochastique;serie temporal;statistical inference;analisis multivariable;analyse donnee;calcul statistique;business cycle;estimation statistique;proceso estocastico;regression statistique;polynome;estimacion estadistica;statistical estimation;var model;analisis correlacion;autocorrelation;correlation analysis;common cyclical features	This paper provides a unifying framework in which the coexistence of different form of common cyclical features can be tested and imposed to a cointegrated VAR model. This goal is reached by introducing a new notion of common cyclical features, namely the weak form of polynomial serial correlation common features, which encompasses most of the previous ones. Statistical inference is obtained by means of reduced-rank regression, and alternative forms of common cyclical features are detected by means of tests for over-identifying restrictions on the parameters of the new model. Some iterative estimation procedures are then proposed for simultaneously modelling different forms of common features. Concepts and methods are illustrated by an empirical investigation of the US business cycle indicators. JEL classification: C32	autocorrelation;coexist (image);iteration;iterative method;occam's razor;polynomial;time series;vector autoregression;weak formulation	Gianluca Cubadda	2007	Computational Statistics & Data Analysis	10.1016/j.csda.2007.07.004	stochastic process;econometrics;autocorrelation;calculus;mathematics;statistics	AI	33.4008686988118	-22.736766627860117	39854
01f37f2e11e8b1893d3016d2b1daaae189ee3d4c	differential evolution based soft optimization to attenuate vane-rotor shock interaction in high-pressure turbines	vane rotor shock interaction;design optimization;evolutionary algorithm;turbomachinery	This article presents a soft computing methodology to design turbomachinery components experiencing strong shock interactions. The study targets a reduction of unsteady phenomena using evolutionary optimization with robust, high fidelity, and low computational cost evaluations. A differential evolution (DE) algorithm is applied to optimize the transonic vane of a high-pressure turbine. The vane design candidates are examined by a cost-effective Reynolds-averaged Navier–Stokes (RANS) solver, computing the downstream pressure distortion and aerodynamic efficiency. A reduction up to 55% of the strength of the eywords: esign optimization volutionary algorithm urbomachinery ane–rotor shock interaction shock waves propagating downstream of the stand-alone vane was obtained. Subsequently to the vane optimization, unsteady computations of the vane–rotor interaction were performed using a non-linear harmonic (NLH) method. Attenuation above 60% of the unsteady forcing on the rotor (downstream of the optimal vane) was observed, with no stage-efficiency abatement. These results show the effectiveness of the proposed soft optimization to improve unsteady performance in modern turbomachinery exposed to strong shock interactions.	algorithm;algorithmic efficiency;computation;differential evolution;distortion;downstream (software development);electronic signatures in global and national commerce act;interaction;mathematical optimization;nonlinear system;r.o.t.o.r.;reynolds-averaged navier–stokes equations;soft computing;solver	Michael M. Joly;Tom Verstraete;Guillermo Paniagua	2013	Appl. Soft Comput.	10.1016/j.asoc.2012.12.005	turbomachinery;mathematical optimization;multidisciplinary design optimization;simulation;computer science;machine learning;evolutionary algorithm;control theory	AI	34.025443999192014	-3.8469229392962303	39891
2570de506d89e563922628252334c9afaa848cae	a perceptually-based objective measure for speech coders using abductive network	speech intelligibility;performance evaluation;speech processing;speech processing perceptually based objective measure speech coders reliable performance evaluation telecommunication services telecommunication providers perceptual processing speech sounds human ear speech power spectrum auditory power spectrum bark spectral distance per band coded speech signals abductive networks speech quality coded speech quality correlation human responses;speech coding;correlation methods;power spectrum;vocoders;speech processing speech analysis humans ear signal processing speech coding psychoacoustic models power measurement robustness testing;correlation methods vocoders speech coding speech intelligibility spectral analysis hearing;spectral analysis;measurement technique;hearing	Objective measure techniques that provide reliable performance evaluation of speech coders is indispensable to designers of speech coders, to end users when selecting telecommunication services and to providers in the designing stage of the telecommunications facilities. In this paper, we introduce our objective measure technique that: 1) emulates several known features of perceptual processing of speech sounds by human ear to map the speech power s p e c ” into auditory power spectrum that is used to calculate the bark spectral distance per band (BSDB) between the input and the output coded speech signals. 2) uses the abductive networks to estimate the speech quality from the BSDB. The results indicate that our proposed technique is reliable and robust in evaluating the coded speech quality and it is highly correlated to human responses across a wide range of quality levels and for a wide range of speech processing.	abductive reasoning;bsdb;emulator;performance evaluation;spectral density;speech processing;speech synthesis	Mohamed M. Meky;Tarek N. Saadawi	1996		10.1109/ICASSP.1996.541137	voice activity detection;speech technology;codec2;linear predictive coding;speech recognition;computer science;speech coding;speech processing;acoustic model;psqm;spectral density;intelligibility	ML	48.83567039974887	-7.4522546184233045	39932
1a0e84577a05f7a5c14413530657485849b005a5	the island confinement method for reducing search space in local search methods	search space;satisfiability;constraint satisfaction;optimization problem;sat;constraint satisfaction problem;local minima;local search;penalty function	Typically local search methods for solving constraint satisfaction problems such as GSAT, WalkSAT, DLM, and ESG treat the problem as an optimization problem. Each constraint contributes part of a penalty function in assessing trial valuations. Local search examines the neighbours of the current valuation, using the penalty function to determine a “better” neighbour valuation to move to, until finally a solution which satisfies all constraints is found. In this paper we investigate using some of the constraints as “hard” constraints, that are always satisfied by every trial valuation visited, rather than as part of a penalty function. In this way these constraints reduce the possible neighbours in each move and also the overall search space. The treating of some constraints as hard requires that the space of valuations that are satisfied is “connected” in order to guarantee that a solution can be found from any starting position within the region; thus the concept of islands and the name “island confinement method” arise. Treating some constraints as hard provides new difficulties for the search mechanism since the search space becomes more jagged, and there are more deep local minima. A new escape strategy is needed. To demonstrate the feasibility and generality of our approach, we show how the island confinement method can be incorporated in and significantly improve the search performance of two successful local search procedures, DLM and ESG, on SAT problems arising from binary CSPs.	constrained optimization;constraint satisfaction problem;cryptographic service provider;experiment;lam/mpi;local search (optimization);mathematical optimization;maxima and minima;neighbourhood (graph theory);optimization problem;penalty method;search algorithm;value (ethics);walksat	Hai Fang;Y. Kilani;Jimmy Ho-Man Lee;Peter J. Stuckey	2007	J. Heuristics	10.1007/s10732-007-9020-8	optimization problem;mathematical optimization;combinatorics;constraint satisfaction;computer science;local search;maxima and minima;penalty method;mathematics;constraint satisfaction problem;algorithm;guided local search;satisfiability	AI	25.59439891996324	2.482097292679938	39943
64ca1f2eb167b790487cda918b0a260e28bab482	a new dynamic finite-state vector quantization algorithm for image compression	search problems vector quantisation image coding correlation methods;tabla codificacion;evaluation performance;image coding;performance evaluation;image processing;data compression;imagen fija;visual quality dynamic finite state vector quantization algorithm image compression picture quality supercodebooks dfsvq algorithm global interblock correlation image blocks input block closest block previously encoded data side match technique dynamic codebook codevector space bit rate;evaluacion prestacion;simulation;procesamiento imagen;simulacion;correlation methods;indexing terms;qualite image;visual quality;traitement image;algorithme;algorithm;codificacion;cuantificacion vectorial;vector quantization;fixed image;image compression;codebook;table codage;image quality;coding;image fixe;calidad imagen;search problems;vector quantizer;compresion dato;vector quantization image coding bit rate speech processing rate distortion decoding algorithm design and analysis statistics image reconstruction;vector quantisation;compression donnee;codage;algoritmo;quantification vectorielle	The picture quality of conventional memory vector quantization techniques is limited by their supercodebooks. This paper presents a new dynamic finite-state vector quantization (DFSVQ) algorithm which provides better quality than the best quality that the supercodebook can offer. The new DFSVQ exploits the global interblock correlation of image blocks instead of local correlation in conventional DFSVQs. For an input block, we search the closest block from the previously encoded data using the side-match technique. The closest block is then used as the prediction of the input block, or used to generate a dynamic codebook. The input block is encoded by the closest block, dynamic codebook or supercodebook. Searching for the closest block from the previously encoded data is equivalent to expand the codevector space; thus the picture quality achieved is not limited by the supercodebook. Experimental results reveal that the new DFSVQ reduces bit rate significantly and provides better visual quality, as compared to the basic VQ and other DFSVQs.	algorithm;block size (cryptography);codebook;image compression;image quality;vector quantization	Jyi-Chang Tsai;Chaur-Heh Hsieh;Te-Cheng Hsu	2000	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.877206	data compression;image quality;computer vision;mathematical optimization;index term;image processing;image compression;computer science;theoretical computer science;codebook;mathematics;coding;linde–buzo–gray algorithm;vector quantization;algorithm	Visualization	44.95541604523439	-13.725679776653019	40034
9e6f0f06ca16a68a1e4f1dddb783ed2893818aa9	identification of residential property sub-markets using evolutionary and neural computing techniques	finanza;bâtiment residentiel;edificio residencial;marche biens;finance;property valuation;computational techniques;property market;test gamma;gamma test;estratificacion;residential building;heterogeneidad;identification;evaluation biens;residential property;stratification;portfolio management;algorithme evolutionniste;identificacion;genetic algorithm;algoritmo evolucionista;gestion cartera;evolutionary algorithm;reseau neuronal;gestion portefeuille;economia matematica;mathematical economy;economie mathematique;red neuronal;heterogeneity;heterogeneite;self organising map;neural network	This paper expands on previous work considering methods of stratifying property data in order to enhance its susceptibility to modelling for mortgage value estimation. Previous work [1] considered a clustering approach using a Kohonen Self-Organising Map (SOM) to stratify the training data prior to training a suite of MLPs. Although the results were encouraging, the approach suffers from its estimation of trainability post-clustering. The following method ameliorates the approach by replacing the static clustering step with a dynamic genetic algorithm implementation. The results show a healthy improvement in accuracy over the non-stratified approach, and a more consistent level of accuracy compared with the Kohonen SOM approach. The paper concludes by analysing the underlying content of the derived stratas, thus providing a ‘human readable’ element to the approach that enhances its potential for acceptance by valuation institutions for as a complementary technique to traditional valuation methods.	artificial neural network;cluster analysis;genetic algorithm;self-organizing map;teuvo kohonen;value (ethics)	Owen M. Lewis;J. Andrew Ware;David Harrison Jenkins	2001	Neural Computing & Applications	10.1007/s005210170003	identification;stratification;simulation;genetic algorithm;computer science;artificial intelligence;heterogeneity;machine learning;evolutionary algorithm;goodman and kruskal's gamma;artificial neural network	AI	24.766611012543606	-20.31350220420584	40080
4b91d9198202edeaa0e9c73a1d54cecc74e9c27c	a pyramidal evolutionary algorithm with different inter-agent partnering strategies for scheduling problems	bottom up;evolutionary computing;multiple choice;search space;evolutionary algorithm;scheduling problem	This paper combines the idea of a hierarchical distributed genetic algorithm with different interagent partnering strategies. Cascading clusters of sub-populations are built from bottom up, with higher-level sub-populations optimising larger parts of the problem. Hence higher-level subpopulations search a larger search space with a lower resolution whilst lower-level subpopulations search a smaller search space with a higher resolution. The effects of different partner selection schemes amongst the agents on solution quality are examined for two multiplechoice optimisation problems. It is shown that partnering strategies that exploit problemspecific knowledge are superior and can counter inappropriate (sub-) fitness measurements.	computer cluster;crossover (genetic algorithm);evolutionary algorithm;fitness function;genetic algorithm;hill climbing;mathematical optimization;population;randomness;scheduling (computing);search algorithm;tabu search;top-down and bottom-up design;while	Uwe Aickelin	2002	CoRR		multiple choice;mathematical optimization;computer science;artificial intelligence;machine learning;evolutionary algorithm;top-down and bottom-up design;evolutionary computation	AI	25.027679298910712	-10.934964024813414	40142
93f4ea63538af4a9f8fbf56d23e3ba76849273cc	bayesian density estimation and model selection using nonparametric hierarchical mixtures	test hypothese;non linear functional;processus melangeant;bayes estimation;classification automatique statistiques;60j99;ajustamiento modelo;model selection;analisis numerico;predictive distribution;test statistique;almost sure convergence;chaine markov;cadena markov;metodo monte carlo;stochastic process;theorie approximation;65c05;estimacion densidad;proceso mezclante;analisis datos;melange loi probabilite;stochastic method;loi predictive;proceso markov;convergencia casi segura;melangeage;estimation densite;test hipotesis;test estadistico;aproximacion;62e17;gamma process;funcional no lineal;estimation non parametrique;bayes factor;statistical test;methode monte carlo;mixed distribution;62g07;selection modele;distribucion estadistica;60j05;linear functional;65c40;fonctionnelle non lineaire;analyse numerique;normalized completely random measures;approximation;algorithme;ajustement modele;estimation parametrique;approximation theory;algorithm;non parametric estimation;density estimation;data analysis;estimacion bayes;methode selection;mixing process;numerical analysis;posterior distribution;distribution statistique;seleccion modelo;markov chain monte carlo;processus markov;62f15;monte carlo method;model matching;37a25;62m05;statistical computation;calculo estadistico;bayesian nonparametrics;markov process;processus stochastique;modele hierarchique;ley a posteriori;facteur bayes;methode stochastique;mezcla ley probabilidad;analyse donnee;calcul statistique;estimacion no parametrica;funcional lineal;62m02;cluster analysis statistics;estimation statistique;proceso estocastico;selection method;mixing;mixture models;estimacion estadistica;loi a posteriori;mezclado;statistical estimation;mcmc algorithms;statistical distribution;hierarchical model;historical data;fonctionnelle lineaire;estimation bayes;convergence presque sure;algoritmo;markov chain;hypothesis test;metodo estocastico	We consider mixtures of parametric densities on the positive reals with a normalized generalized gamma process (Brix, 1999) as mixing measure. This class of mixtures encompasses the Dirichlet process mixture (DPM) model, but it is supposedly more flexible in the detection of clusters in the data. With an almost sure approximation of the posterior distribution of the mixing process we can run a Markov chain Monte Carlo algorithm to estimate linear and nonlinear functionals of the predictive distributions. The best-fitting mixing measure is found by minimizing a Bayes factor for parametric against non-parametric alternatives. We illustrate the method with simulated and hystorical data, finding a tradeoff between the best-fitting model and the correct identification of the number of components in the mixture.	approximation;bayes factor;bayesian network;markov chain monte carlo;model selection;monte carlo algorithm;monte carlo method;nonlinear system	Raffaele Argiento;Alessandra Guglielmi;Antonio Pievatolo	2010	Computational Statistics & Data Analysis	10.1016/j.csda.2009.11.002	stochastic process;econometrics;statistical hypothesis testing;calculus;mathematics;statistics	ML	32.87748553888318	-22.853888153064766	40230
d5a22d8fe0b79f5df5fcafec3c7adc6567be7e19	optimal robust estimates using the hellinger distance	robust estimator;negative binomial distribution;62f10;kullback leibler divergence;62f35;optimal estimation;gross error sensitivity;hampel s infinitesimal approach;hellinger distance	Optimal robust M-estimates of a multidimensional parameter are described using Hampel’s infinitesimal approach. The optimal estimates are derived by minimizing a measure of efficiency under the model, subject to a bounded measure of infinitesimal robustness. To this purpose we define measures of efficiency and infinitesimal sensitivity based on the Hellinger distance. We show that these two measures coincide with similar ones defined by Yohai using the Kullback–Leibler divergence, and therefore the corresponding optimal estimates coincide too. We also give an example where we fit a negative binomial distribution to a real dataset of “days of stay in hospital” using the optimal robust estimates.	kullback–leibler divergence	Alfio Marazzi;Victor J. Yohai	2010	Adv. Data Analysis and Classification	10.1007/s11634-010-0061-8	optimal estimation;robust statistics;econometrics;mathematical optimization;mathematics;kullback–leibler divergence;negative binomial distribution;statistics;hellinger distance	Vision	30.41062041425191	-20.66040236512007	40247
35c11194912344c636d3705659449c49a425200a	applying clonal selection theory to data fitting with rational bézier curves	rational bezier curves nature inspired techniques artificial immune systems clonal selection theory data fitting reverse engineering;cloning;polynomials;shape;nonlinear programming artificial immune systems computational geometry data handling;solid modeling;nature inspired techniques;immune system;clonal selection theory;optimization;immune system optimization cloning pathogens shape polynomials solid modeling;rational bezier curves;data fitting;artificial immune systems;computational methodology clonal selection theory data fitting rational bezier curves continuous multivariate nonlinear optimization problem bio inspired paradigm artificial immune systems ais;reverse engineering;pathogens	This paper addresses the problem of obtaining the rational Bezier curve that fits a given set of data points better in the least-squares sense. This is a difficult problem because in addition to compute the control points of the approximating curve, it also requires to obtain their corresponding weights and a suitable parameterization of data points. This leads to a continuous multivariate nonlinear optimization problem that cannot be solved through traditional mathematical optimization techniques. To overcome this limitation, in this work we consider a powerful bio-inspired paradigm called Artificial Immune Systems (AIS), which is receiving increasing attention from the scientific community during the last few years. The AIS is a computational methodology encompassing many different techniques rather than a single method. In this paper we focus on the clonal selection theory principles. The paper describes how they can be effectively applied to solve our problem. The performance of our approach is evaluated through its application to three illustrative examples of freeform shapes. Our experimental results show that our method performs very well, being able to reconstruct the underlying shape of data points with high accuracy.	artificial immune system;british informatics olympiad;bézier curve;computation;computer science;control point (mathematics);curve fitting;data point;fits;information science;least squares;mathematical optimization;nonlinear programming;nonlinear system;optimization problem;programming paradigm;scott continuity	Akemi Gálvez;Andrés Iglesias;Andreina Avila	2014	2014 International Conference on Cyberworlds	10.1109/CW.2014.38	mathematical optimization;immune system;shape;artificial intelligence;machine learning;cloning;mathematics;geometry;solid modeling;algorithm;reverse engineering;statistics;polynomial;curve fitting	Robotics	32.343221119413634	-9.465829121810412	40259
11b12b8f2b5f00dc00f5fd7211258a40c330b055	solving graphical steiner tree problem using parallel genetic algorithm	parallel genetic algorithm;population model;parallel genetic algorithm steiner tree problem;combinatorial optimization problem;global population model;trees mathematics genetic algorithms;trees mathematics;genetic algorithms;distance network heuristic;parallel implementation;steiner tree problem;binary encoding;global population model graphical steiner tree problem parallel genetic algorithm combinatorial optimization problem binary encoding distance network heuristic;graphical steiner tree problem	The graphical Steiner tree problem is a classical combinatorial optimization problem that appears in many practically important applications. This paper presents a new parallel genetic algorithm for solving the problem. The presented algorithm is based on binary encoding, used the Distance Network Heuristic for evaluating fitness of individuals and is implemented in parallel using global population model. The results of experiments on the OR-Library tests are reported to show the algorithmpsilas performance in comparison with other metaheuristics for the given problem. The speed-up of the parallel implementation is also discussed.	binary file;combinatorial optimization;experiment;genetic algorithm;heuristic;mathematical optimization;metaheuristic;optimization problem;population model;steiner tree problem	Nguyen Viet Huy;Nguyen Duc Nghia	2008	2008 IEEE International Conference on Research, Innovation and Vision for the Future in Computing and Communication Technologies	10.1109/RIVF.2008.4586329	optimization problem;mathematical optimization;combinatorics;discrete mathematics;steiner tree problem;mathematics	Robotics	25.037357751978387	-0.47407272233948855	40268
277086581d8dbb5d71ca9ad0481b62b91f41fc03	an adaptive hvs based video watermarking scheme for multiple watermarks using bam neural networks and fuzzy inference system	dwt;hvs;multiple watermarks;imperceptibility;fuzzy inference system;robustness;payload;video watermarking;bidirectional associative memory;adaptive embedding strength	Multi-BAM-FUZ scheme, achieves multiple watermarking systems by embedding the BAM weight matrix.The adaptive nature of α computes using Fuzzy for every frame improves imperceptibility.This gives the NCC value of 1.00 and BCR value of 0.99, which is suitable for the content sensitive applications.In addition to fidelity and robustness, this also achieves a higher payload capacity of 2000 images. An efficient reversible adaptive video watermarking scheme for multiple watermarks based on Bi-directional Associative Memory (BAM) Neural Networks and Fuzzy Inference System namely, Multi-BAM-FUZ scheme is proposed in this paper. The main goal of this paper is to design a robust video watermarking system which facilitates secure video transmission over a communication channel by maintaining a trade-off among imperceptibility, robustness and watermark capacity or payload. The BAM neural network supports creation of weight matrix (formed out of multiple images) and this matrix is embedded into the DWT uncorrelated mid frequency coefficients of all the components (Y, Cb, Cr) of every frames of the video with varying embedding strength 'α'. This adaptive embedding strength is generated using the Fuzzy Inference System which takes HVS characteristics such as luminance, texture and edge of each frame as an input in the DWT transform. The simulations performed on various test videos demonstrate that the proposed Multi-BAM-FUZ not only outperforms other existing methods with respect to various video degradation processes, but also maintains a satisfactory image quality, robustness and payload. It is noted that, the implementation of the novel adaptive process enhances the visual quality of about 60.97dB in terms of PSNR and 0.9998 in terms of SSIM, robustness of about nearly 1.0000 and 0.9999 in terms of Normalized Cross Correlation (NCC) value and Bit Correction Rate (BCR) respectively against various attacks. Moreover, the proposed scheme facilitates high level of payload without affecting the imperceptibility and robustness level.	artificial neural network;human visual system model;inference engine	Loganathan Agilandeeswari;Ganesan Kaliyaperumal	2016	Expert Syst. Appl.	10.1016/j.eswa.2016.05.019	computer vision;payload;real-time computing;computer science;theoretical computer science;machine learning;mathematics;bidirectional associative memory;robustness	ML	40.67844258238081	-11.650031592034647	40378
b9bc11365e37d8f8a500779b50306f380f8bce3d	in vivo validation of quasi-static field model in intra-body communication	quasi static	Intra-Body communication (IBC) is a short-range body based communication using the human body as the transmission medium in biomedical application. Quasi-static electric field modeling of the galvanic coupling IBC channel is analyzed; Analytical solution of voltage distribution and signal attenuation in the forearm of human body from the model are investigated; Effects of human limb’s properties in the model are studied. Finally, in vivo validation and calculation results are presented. Especially, the gain is greatly affected by the thickness of muscle both in calculation result and in vivo experiment.	galvanic isolation;information-based complexity;thickness (graph theory);video-in video-out	Xi Mei Chen;Peng Un Mak;Mang I Vai	2011			electronic engineering;telecommunications;engineering;biological engineering	HCI	46.139657998705104	0.7077216939741696	40430
446c4bc5327181bcdcbf25b29a3938a6cfbda15c	reduction of the effect of estimation error on in-control performance for risk-adjusted bernoulli cusum chart with dynamic probability control limits	statistical process control;risk adjustment;control limits;estimation error;average run length arl	The in-control performance of any control chart is highly associated with the accuracy of estimation for the in-control parameter(s). For the risk-adjusted Bernoulli cumulative sum (CUSUM) chart with a constant control limit, it had been shown that the estimation error could have a substantial effect on the in-control performance. In our study, we examine the effect of estimation error on the in-control performance of the risk-adjusted Bernoulli CUSUM chart with dynamic probability control limits (DPCLs). Our simulation results show that the in-control performance of risk-adjusted Bernoulli CUSUM chart with DPCLs is also affected by the estimation error. The most important factors affecting estimation error are the specified desired in-control average run length, the Phase I sample size, and the adverse event rate. However, the effect of estimation error is uniformly smaller for the risk-adjusted Bernoulli CUSUM chart with DPCLs than for the corresponding chart with a constant control limit under various realistic scenarios. In addition, we found a substantial reduction in the mean and variation of the standard deviation of the in-control run length when DPCLs are used. Therefore, use of DPCLs has yet another advantage when designing a risk-adjusted Bernoulli CUSUM chart. Copyright © 2016 John Wiley u0026 Sons, Ltd.	bernoulli polynomials	Xiang Zhang;William H. Woodall	2017	Quality and Reliability Eng. Int.	10.1002/qre.2014	reliability engineering;econometrics;control limits;computer science;operations management;mathematics;statistical process control;statistics	DB	28.206554562217406	-19.21342040824207	40458
b778b83508cc0406ef56275b06df56e153ce8f00	matching pursuit-based region-of-interest image coding	estensibilidad;sistema interactivo;iterative method;theorie vitesse distorsion;traitement signal;rate distortion;compresion con perdida;embedded coding;image coding;algorithms data compression image enhancement image interpretation computer assisted numerical analysis computer assisted signal processing computer assisted;image processing;image resolution;data compression;lossy compression;image matching;persecusion adaptativa;signal analysis;multiresolution signal analysis method;poursuite adaptative;procesamiento imagen;analisis de senal;region of interest roi;analyse multiresolution;region interes;traitement image;matching pursuit algorithms image coding dictionaries streaming media iterative algorithms image resolution signal resolution image analysis scalability bit rate;systeme conversationnel;metodo iterativo;rate distortion theory;signal processing computer assisted;refinement method;codage image;image enhancement;dictionnaire;numerical analysis computer assisted;compression image;image compression;image interpretation computer assisted;interactive system;computational complexity;methode iterative;signal processing;region of interest;image resolution computational complexity image coding image matching;dictionaries;matching pursuit;algorithms;region of interest image coding;extensibilite;scalability;compresion dato;region interet;methode raffinement;multiresolution analysis;procesamiento senal;metodo afinamiento;diccionario;multiresolution signal analysis method matching pursuit region of interest image coding;analyse signal;matching pursuit mp;analisis multiresolucion;compression donnee;compression degradante;interest region;compresion imagen;scalability embedded coding image compression lossy compression matching pursuit mp region of interest roi	Matching pursuit (MP) is a multiresolution signal analysis method and can be used to render a selected region of an image with a specific quality. A novel, scalable, and progressive MP-based region-of-interest image-coding scheme is presented. The method is capable of providing a trade off between rate, distortion, and complexity. The method also provides an interactive way of information refinement for regions of an image with higher receiver's priority. By selecting a proper subset of the huge initial MP dictionary, using the method described in this paper, the complexity burden of MP analysis can be adapted to the computational power of the image encoder	complexity;computation;data dictionary;decoder device component;dictionary [publication type];distortion;encoder device component;interactive media;matching pursuit;population parameter;refinement (computing);region of interest;scalability;signal processing;subgroup;transmitter device component;unbalanced circuit	Abbas Ebrahimi-Moghadam;Shahram Shirani	2007	IEEE Transactions on Image Processing	10.1109/TIP.2006.888333	data compression;lossy compression;multiresolution analysis;computer vision;scalability;image resolution;rate–distortion theory;image compression;computer science;theoretical computer science;signal processing;iterative method;computational complexity theory;algorithm;matching pursuit;computer graphics (images);region of interest	Visualization	46.61353134327202	-13.318624565019759	40489
121bc5ecbbf74ce9a417605bce31cc8ffcb81e2a	small-world particle swarm optimization with topology adaptation	particle swarm optimization;topology adaptation;global optimization;small world network	Traditional particle swarm optimization (PSO) algorithms adopt completely regular network as topologies, which may encounter the problems of premature convergence and insufficient efficiency. In order to improve the performance of PSO, this paper proposes a novel topology based on small-world network. Each particle in the swarm interacts with its cohesive neighbors and by chance to communicate with some distant particles via small-world randomization. In order to improve search diversity, each dimension of the swarm is assigned with a specific network, and the particle is allowed to follow the historical information of different neighbors on different dimensions. Moreover, in the proposed small-world topology, the neighborhood size and the randomization probability are adaptively adjusted based on the convergence state of the swarm. By applying the topology adaptation mechanism, the particle swarm is able to balance its exploitation and exploration abilities during the search process. Experiments were conducted on a set of classical benchmark functions. The results verify the effectiveness and high efficiency of the proposed PSO algorithm with adaptive small-world topology when compared with some other PSO variants.	algorithm;benchmark (computing);experiment;mathematical optimization;particle swarm optimization;premature convergence;randomness	Yue-jiao Gong;Jun Zhang	2013		10.1145/2463372.2463381	mathematical optimization;multi-swarm optimization;computer science;artificial intelligence;machine learning;mathematics;small-world network;particle swarm optimization;metaheuristic;global optimization	AI	27.10762752715587	-3.3773450796555835	40550
f8eeeca90360dd2e788f454afaa699211bda7aa7	a fast dea-based intra-coding algorithm for hevc	hevc;intra-coding;dominant edge assent;block partitioning;intra-prediction mode selection;eo class decision	As the newest video coding standard, High Efficiency Video Coding (HEVC) greatly enhances the encoding performance of H.264/AVC. However, HEVC also has high computational complexity, which limits application of this new standard. In this paper, we propose a fast DEA-based intra-coding algorithm, including block partitioning; prediction mode selection and edge offset (EO) class decision algorithms. The idea behind the proposed algorithm is to utilize the texture characteristics of the encoding image, which are quantified by dominant edge assent (DEA) and its distribution, to reduce the decision space. Specifically, for block partitioning, we propose the most possible depth range (MPDR) and employ DEA to determine whether the current coding block can use the MPDR to predict the partitioning depth or not; for intra-prediction mode selection, we use DEA and its distribution to reduce the range of prediction direction; for the EO class decision, we use DEA to determine the EO class of the sample adaptive offset. We integrate the proposed algorithm into the test model HM 13.0 and present a detailed comparative analysis. Experimental results show that the proposed fast DEA-based intra-coding algorithm reduces the computational complexity of HM 13.0 to about 46% in encoding time with 2.08% increases in the Biontegaard-Delta bitrate (BD-rate). Moreover, the proposed algorithm also demonstrates better performance over other state-of-the-art work.	algorithm;blu-ray;computational complexity theory;data compression;h.264/mpeg-4 avc;high efficiency video coding;qualitative comparative analysis;video coding format	Yingbiao Yao;Tianjie Jia;Xiaojuan Li;Yu Lu	2017	Multimedia Tools and Applications	10.1007/s11042-017-4372-z	artificial intelligence;computer science;coding (social sciences);computational complexity theory;pattern recognition;algorithm;offset (computer science)	AI	46.95357790550878	-19.602834124833183	40626
a9f9c3e1b55625304002ec4557047c789ea6ae30	efficient vq-based image coding scheme using inverse function and lossless index coding	lossless index coding;vector quantization;image compression;inverse function	A novel image coding scheme based on vector quantization (VQ) is proposed in this paper. The goal of the proposed scheme is to provide better image qualities of the compressed grayscale images while keeping low bit rates. To achieve the goal, the encoding of the inverse image block is cooperated with the VQ scheme. In addition, the lossless index coding process is employed in the proposed scheme to cut down the required bit rates. From the results, it is shown that the proposed scheme achieves better image qualities than VQ while keeping low bit rates. & 2013 Elsevier B.V. All rights reserved.	block code;codebook;decibel;grayscale;image quality;lossless compression;peak signal-to-noise ratio;vector quantization	Yu-Chen Hu;Wu-Lin Chen;Chun-Chi Lo;Chang-Ming Wu;Chia-Hsien Wen	2013	Signal Processing	10.1016/j.sigpro.2013.03.034	mathematical optimization;discrete mathematics;harmonic vector excitation coding;image compression;computer science;theoretical computer science;machine learning;mathematics;context-adaptive binary arithmetic coding;inverse function;vector quantization	Vision	43.099785745499105	-14.18813964914615	40671
2df47f6abfccad98da841472a411c7bd273fd11e	on predictive distributions and bayesian networks	bayesian framework;parametric model;bayesian network;predictive distribution;marginal likelihood;naive bayes;prior distribution;fisher information matrix;public domain;predictive inference;probability distribution;tree structure;decision theoretic;stochastic complexity;information theoretic;jeffreys prior;mml;bayesian networks;mdl	P. KONTKANEN , P. MYLLYM ÄKI , T. SILANDER , H. TIRRI and P. GR̈UNWALD† Complex Systems Computation Group (CoSCo), P.O.Box 26, Dep artment of Computer Science, FIN-00014 University of Helsinki, Finland (http://www.cs Helsinki.FI/research/cosco/) (pkontkan@cs.helsinki.fi)(myllymak@cs.helsinki.fi)(ts ilande@cs.helsinki.fi)(tirri@cs.helsinki.fi) †Department of Computer Science, Stanford University, Stan ford, CA 94306, USA (grunwald@cs.stanford.edu)	bayesian network;complex systems;computation;computer science;stan	Petri Kontkanen;Petri Myllymäki;Tomi Silander;Henry Tirri;Peter Grünwald	2000	Statistics and Computing	10.1023/A:1008984400380	dirichlet distribution;econometrics;posterior predictive distribution;categorical distribution;machine learning;pattern recognition;bayesian network;mathematics;bayesian linear regression;bayesian hierarchical modeling;statistics	Theory	26.308743706937207	-23.678492677490837	40824
680dfa09d24d6db94660d8969dcef8a165543586	genetic engineering versus natural evolution: genetic algorithms with deterministic operators	genetic engineering;automated design problem solving;automated design;synthese circuit;approche heuristique;algoritmo genetico;heuristic search;minimisation input support problem;ingenieria genetica;input support minimization;genie genetique;algorithme genetique;enfoque heuristico;evolutionary algorithms;sintesis circuito;algorithme evolutionniste;genetic algorithm;genetic algorithms;algoritmo evolucionista;heuristic approach;evolutionary algorithm;circuit synthesis;problem solving	Genetic algorithms (GA) have several important features that predestine them to solve design problems. Their main disadvantage however is the excessively long run-time that is needed to deliver satisfactory results for large instances of complex design problems. The main aims of this paper are (1) to demonstrate that the effective and efficient application of the GA concept to design problem solving requires substitution of the basic GAs natural evolution by genetic engineering (GE), (2) to propose and discuss the concept of a genetic engineering algorithm (GEA), and (3) to show how to apply the GEA to solve synthesis problems. In this paper, an effective and efficient GE scheme is proposed and applied to solve an important design problem: the minimal input support problem. In almost all cases, our GEA produces strictly optimal results and realizes a very good trade-off between effectiveness and efficiency. The experimental results clearly demonstrate that the proposed GE scheme is suitable for solving design problems and its application results in very effective and efficient GEAs.		Lech Józwiak;Adam Postula	1999		10.1016/S1383-7621(02)00094-2	mathematical optimization;genetic algorithm;computer science;artificial intelligence;evolutionary algorithm;algorithm	DB	25.195487893160912	2.463298515836699	40825
a63cdf3ff5e9bf97fc2ef592356772567d94ada9	genetically-engineered whole-cell bioreporters on integrated circuits for environmental monitoring	signal processing bioluminescence biosensors cellular biophysics cmos integrated circuits microorganisms photodiodes;genetic engineering;cmos integrated circuits;integrated photodiodes;fluorescence;integrated circuit;microluminometer;bioluminescence;luminescence;cmos process;genetics;pseudomonas fluorescens;photodiodes;signal processing circuits;monitoring;monitoring bioluminescence biosensors microorganisms photodiodes biomedical signal processing circuits cmos process luminescence fluorescence;genetically engineered whole cell bioreporters;signal processing;biomedical signal processing;circuits;pseudomonas fluorescene 5rl bacteria;bioluminescent bioreporter integrated circuit biosensor;pseudomonas fluorescene 5rl bacteria genetically engineered whole cell bioreporters environmental monitoring bioluminescent bioreporter integrated circuit biosensor bioluminescence microluminometer integrated photodiodes signal processing circuits standard cmos process;standard cmos process;microorganisms;cellular biophysics;biosensors;environmental monitoring	A bioluminescent bioreporter integrated circuit (BBIC) biosensor for environmental monitoring is presented. The bioluminescent bioreporters are bacteria that can be genetically altered to achieve bioluminescence when in contact with a targeted substance. The bioreporters are placed on a microluminometer. The microluminometer includes integrated photodiodes and signal processing circuits and is realized in a standard CMOS process. The BBIC can detect luminescence from as few as 5000 fully induced Pseudomonas fluorescene 5RL bacteria and provides a integrated biosensor platform for environmental and food/water safety monitoring.	cmos;integrated circuit;signal processing	Nora D. Bull;Syed Kamrul Islam;Benjamin J. Blalock;Steven Ripp;Scott Moser;Gary S. Sayler	2008	2008 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2008.4541838	genetic engineering;electronic circuit;photodiode;electronic engineering;bioluminescence;fluorescence;computer science;electrical engineering;integrated circuit;signal processing;environmental monitoring;microorganism;cmos;luminescence;biosensor	EDA	53.06370882161193	-0.37634727122443984	40829
d10c3bfb12c195d1eb3a7bc40bbd1f66009a9401	multi-agent path finding with kinematic constraints		Multi-Agent Path Finding (MAPF) is well studied in both AI and robotics. Given a discretized environment and agents with assigned start and goal locations, MAPF solvers from AI find collision-free paths for hundreds of agents with userprovided sub-optimality guarantees. However, they ignore that actual robots are subject to kinematic constraints (such as finite maximum velocity limits) and suffer from imperfect plan-execution capabilities. We therefore introduce MAPFPOST, a novel approach that makes use of a simple temporal network to postprocess the output of a MAPF solver in polynomial time to create a plan-execution schedule that can be executed on robots. This schedule works on non-holonomic robots, takes their maximum translational and rotational velocities into account, provides a guaranteed safety distance between them, and exploits slack to absorb imperfect plan executions and avoid time-intensive replanning in many cases. We evaluate MAPF-POST in simulation and on differentialdrive robots, showcasing the practicality of our approach.	artificial intelligence;discretization;pathfinding;robot;robotics;simulation;slack variable;solver;time complexity;velocity (software development)	Wolfgang Hönig;T. K. Satish Kumar;Liron Cohen;Hang Ma;Hong Xu;Nora Ayanian;Sven Koenig	2016			mathematical optimization;real-time computing;simulation;computer science;artificial intelligence;machine learning;algorithm	AI	53.169471366983025	-22.42989685713962	40832
290f78279e99822ac582c28a8957248423d8ce49	primal-dual contraint aggregation method in multistage stochastic programming	multistage stochastic programming		multistage amplifier;stochastic programming	Matthew J Davidson	1997	Universität Trier, Mathematik/Informatik, Forschungsbericht		mathematical optimization;stochastic programming;mathematics	Robotics	33.66343834996567	3.480758797960832	40838
6604dc3af7d9b6246915185d7b93cc3ab245321e	performance of modified differential evolution for optimal design of complex and non-linear chemical processes	differential evolution;nonlinear programming problems;nonlinear programming;chemical process;numerical optimization;convergence rate;modified differential evolution;objective function;empirical evidence;optimal design;optimization;evolutionary algorithm;evolutionary optimization	Differential evolution (DE) is an evolutionary optimization technique that is exceptionally simple, fast, and robust at numerical optimization. However, the convergence rate of DE in optimizing a computationally expensive objective function still does not meet our requirements, and an attempt to speed up DE is considered necessary. This article introduces a modified differential evolution (MDE) that enhances the convergence rate without compromising the robustness. MDE algorithm utilizes only one set of population array as against two sets in original DE at any given generation. This modification improves the convergence rate of DE and at the same time maintains the robustness. The performance of MDE is evaluated on two benchmark test functions followed by nonlinear chemical processes. The simulation results show empirical evidences on the efficiency and effectiveness of the proposed MDE.	algorithm;analysis of algorithms;benchmark (computing);central processing unit;differential evolution;distribution (mathematics);global optimization;loss function;mathematical optimization;model-driven engineering;nonlinear system;numerical analysis;optimal design;optimization problem;problem domain;rate of convergence;requirement;simulation	Rakesh Angira;B. V. Babu	2006	J. Exp. Theor. Artif. Intell.	10.1080/09528130600975717	differential evolution;mathematical optimization;empirical evidence;nonlinear programming;computer science;optimal design;machine learning;evolutionary algorithm;chemical process;evolution strategy;rate of convergence;algorithm	ML	29.80465671717056	-0.7880013025094688	40847
453c78497767252955b19e7f49dde0114c7e8a1c	heuristic learning based on genetic programming	bdd minimization;genetic program;multi objective optimization;genetic programming;heuristic learning;variable re ordering	In this paper we present an approach to learning heuristics based on Genetic Programming (GP) which can be applied to problems in the VLSI CAD area. GP is used to develop a heuristic that is applied to the problem instance instead of directly solving the problem by application of GP. The GP-based heuristic learning method is applied to one concrete field from the area of VLSI CAD, i.e. minimization of Binary Decision Diagrams (BDDs). Experimental results are given in order to demonstrate that the GP-based method leads to high quality results that outperform previous methods while the run-times of the resulting heuristics do not increase. Furthermore, we show that by clever adjustment of parameters, further improvements such as the saving of about 50% of the run-time for the learning phase can be achieved.	binary decision diagram;computer-aided design;decision problem;display resolution;genetic programming;heuristic (computer science);mathematical optimization;multi-objective optimization;software release life cycle;test set;very-large-scale integration	Frank Schmiedle;Nicole Drechsler;Daniel Große;Rolf Drechsler	2002	Genetic Programming and Evolvable Machines	10.1023/A:1020988925923	genetic programming;mathematical optimization;computer science;artificial intelligence;multi-objective optimization;machine learning;algorithm	AI	26.911012625242673	4.0009699175193605	40849
f5589d44f9c020446252604070249f37e100bf2d	research on adaptive target detection based on improved genetic algorithm from infrared images	morphology filter target detection genetic algorithm infrared image;adaptive target detection;morphology filter adaptive target detection genetic algorithm infrared image dynamic clustering method crossover probability mutation probability;object detection morphology noise heuristic algorithms genetics encoding optimization;genetics;crossover probability;dynamic clustering;morphology;parallel evolution;infrared image;infrared imaging;heuristic algorithms;mutation probability;improved genetic algorithm;genetic algorithm;genetic algorithms;optimization;infrared;object detection genetic algorithms infrared imaging;encoding;target detection;local search;morphology filter;object detection;dynamic clustering method;noise	In this paper, a novel optimized genetic algorithm based on morphology for target detection from infrared images is proposed. In our improved algorithm, a new fitness measurement method based on target characteristics value is introduced to meet specific target detection needs. Male and female parent dynamic clustering methods are put forward to make crossover operator more reasonable. Besides, multi-population parallel evolution and optimum individual transplant strategy are adopted to merge optimum individual keeping and gene keeping effectively. Crossover probability and mutation probability are adjusted adaptively according to population diversity and more reasonable target characteristics variable is designed according to the features of infrared images. Morphology filter based on genetic optimization for infrared target detection is given to recognize structural information and target background information. Experimental results demonstrate that the convergence speed can be controlled and local search ability is increased as well by using trained structural elements based on improved genetic optimum. In addition, the efficiency and accuracy is boosted evidently and noise can be restrained to a great extent.	cluster analysis;fitness function;genetic algorithm;image noise;local search (optimization);mathematical morphology;mathematical optimization;signal-to-noise ratio	Zhenfeng Shao;Guangxi Zhu;Jun Liu	2010	2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2010.5569266	computer vision;genetic algorithm;morphology;computer science;machine learning;pattern recognition	Robotics	29.607553392881847	-5.867245772938678	40851
8f88540b0d147a470a71205afc856cc5d2617124	image compression via adaptive self-quantization of wavelet subtrees	wavelet analysis;fractal compression;quantization;fractals;iterated function system;convergence;image coding;haar wavelet;data compression;decoding;transform coders;transform coding;tree data structures;bit rate;adaptive self quantization;quantisation signal;wavelet transforms;iterated function systems;image compression;iterated function systems image compression adaptive self quantization wavelet subtrees fractal block coders haar wavelet subtree quantization schemes transform coders fractal compression sqs decoding scheme;fractal block coders;compression ratio;sqs decoding scheme;image coding fractals wavelet analysis decoding wavelet transforms convergence transform coding quantization bit rate block codes;tree data structures image coding data compression decoding fractals wavelet transforms transform coding quantisation signal;block codes;wavelet subtrees;haar wavelet subtree quantization schemes	Fractal compression schemes do not t into the standard transform coder paradigm and have proven diicult to analyze. The theory of iterated function systems motivates a broad class of fractal schemes but does not give much guidance for their implementation. We introduce a wavelet-based framework for analyzing fractal block coders which simpliies these schemes considerably. We show that fractal block coders are Haar wavelet subtree quan-tization schemes, and we thereby place fractal schemes in the context of conventional transform coders. We derive a wavelet-based analog of fractal compression, the self-quantization of subtrees (SQS) scheme. SQS compression outperforms the best fractal schemes in the literature by roughly 1 dB in PSNR across a broad range of compression ratios and has performance comparable to some of the best conventional wavelet schemes. We describe a fast SQS decoding scheme which suuers from none of the convergence problems of fractal decoders.	amazon simple queue service;fractal compression;haar wavelet;image compression;iterated function system;iteration;peak signal-to-noise ratio;programming paradigm;transform coding;tree (data structure)	Geoffrey M. Davis	1996		10.1109/ICASSP.1996.547756	combinatorics;discrete mathematics;computer science;theoretical computer science;mathematics;fractal transform;fractal compression;iterated function system;statistics	Crypto	44.42975666829786	-14.789581502127062	40857
d40d183f7e1813cb259ed078422a282fec4be41d	ga-based applications for routing with an upper bound constraint	genetic algorithms network routing;network routing;routing upper bound biological cells genetic algorithms information science electronic mail algorithm design and analysis vehicles navigation multimedia systems;upper bound;optimization probability upper bound constrained routing ga based routing shortest route search genetic algorithms chromosome fitness function;genetic algorithm;genetic algorithms;fitness function	This paper presents a method of searching for the shortest route via the most designated points among the routes whose lengths are less than the upper bound using a genetic algorithm (GA). If chromosomes whose route lengths exceed the upper bound are simply screened out in the GA process, the optimization probability gets worse. For the purpose of solving this problem, this paper proposes a new fitness function including an upper bound constraint which can be flexibly changed during the searching process. By using this function, the optimum is efficiently obtained and the optimization probability can be raised. Furthermore, the effectiveness of the proposed method is verified by experiments, applying it to the actual map data.	experiment;fitness function;genetic algorithm;mathematical optimization;projection screen;requirement;routing;software release life cycle	Jun Inagaki;Miki Haseyama	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1465068	mathematical optimization;routing;genetic algorithm;computer science;multipath routing;destination-sequenced distance vector routing;machine learning;distributed computing;link-state routing protocol	Embedded	30.552947428730125	-2.6016341730356163	40881
3ecd1ada912e8fa6f830c6d389ae23b535479c02	fractal image compression by the classification in the wavelet transform domain	libraries;discrete wavelet transforms;fractals;image coding;discrete wavelet transform;nondecimated separable discrete wavelet transform;data compression;power fractal image compression image classification wavelet transform domain domain range comparison domain range blocks nondecimated separable discrete wavelet transform average variance image intensities;wavelet transform domain;image classification;transform coding;transform coding fractals data compression image coding image classification discrete wavelet transforms;wavelet transforms;brightness;wavelet transform;fractals image coding wavelet domain wavelet transforms discrete wavelet transforms low pass filters brightness image reconstruction libraries;image reconstruction;image quality;average;image intensities;low pass filters;wavelet domain;domain range blocks;power;domain range comparison;variance;fractal image compression	In the fractal image compression the domain-range comparison step of the encoding is very computationally intensive. Therefore in order to minimize the number of domains compared with a range a classification scheme is used. We propose a new theory for classification of domain-range blocks. The classification uses non-decimated separable discrete wavelet transform. The encoding using the proposed classification is compared with that by Y. Fisher (1995), which uses the average and the variance as features of images and classifies domain-range blocks into 72 classes. The Y. Fisher's classification uses the variance which represents only a messy degree of image intensities. The new classification proposed in this paper represents more effective features of images and classifies domain-range blocks into 432 classes by using the average and the power. With this classification we are able to encode faster and realize a high image quality in fractal image compression.	fractal compression;image compression;wavelet transform	Daiki Endo;Tsuyoshi Hiyane;Kiyoaki Atsuta;Shozo Kondo	1998		10.1109/ICIP.1998.723619	computer vision;discrete mathematics;pattern recognition;mathematics;fractal transform;statistics;wavelet transform	Vision	43.37698865630634	-14.89559542068646	40911
21fffe24fb157b8946276f2872668cd12183acf6	a robust motion estimation with center-biased diamond search and its parallel architecture for motion-compensated de-interlace	parallelisme;estimacion sesgada;fiabilidad;reliability;estimation mouvement;de interlace architecture;motion compensation;traitement flux donnee;reutilizacion;correction erreur;estimacion movimiento;circuit vlsi;desentrelacement;motion estimation;flux donnee;flujo datos;reuse;compensation mouvement;parallelism;vlsi circuit;paralelismo;deinterlacing;error correction;fiabilite;robustesse;data flow processing;television haute resolution;motion compensated interpolation;robustness;de interlace;correccion error;circuito vlsi;television alta definicion;data flow;desentrelazado;architecture;biased estimation;estimation biaisee;high definition television;reutilisation;robustez	For motion compensated de-interlace, the accuracy and reliability of the motion vectors have a significant impact on the performance of the motion compensated interpolation. In order to improve the robustness of motion vector, a novel motion estimation algorithm with center-biased diamond search and its parallel VLSI architecture are proposed in this paper. Experiments show that it works better than conventional motion estimation algorithms in terms of motion compensation error and robustness, and its architecture overcomes the irregular data flow and achieves high efficiency. It also efficiently reuses data and reduces the control overhead. So, it is highly suitable for HDTV applications.	algorithm;computation;dataflow;deinterlacing;interlaced video;interpolation;motion compensation;motion estimation;overhead (computing);parallel computing;requirement;very-large-scale integration	Yong Ding;Xiaolang Yan	2010	The Journal of Supercomputing	10.1007/s11227-010-0527-7	simulation;quarter-pixel motion;computer science;architecture;deinterlacing;motion estimation;reliability;reuse;motion compensation;statistics;robustness;computer graphics (images)	Robotics	46.66459430035685	-14.779097330131801	40923
ea68497ecd0461e854ec7ea63e412e3412a2c730	a general purpose approximate goodness-of-fit test for progressively type-ii censored data	kolmogorov smirnov;exponential distribution;cramer von mises statistic;distribution functions gaussian distribution testing monte carlo methods shape maximum likelihood estimation exponential distribution;normal distribution;progressive type ii censoring anderson darling statistic cramer von mises statistic empirical distribution function hypothesis testing kolmogorov smirnov statistic monte carlo simulation;empirical distribution function;testing;maximum likelihood estimation;normal distribution general purpose approximate goodness of fit test progressive type ii censored data empirical distribution function edf monte carlo simulation log normal distribution gumbel distribution;statistical testing log normal distribution monte carlo methods normal distribution;distribution function;maximum likelihood estimate;censored data;shape;monte carlo method;hypothesis testing;goodness of fit test;statistical testing;kolmogorov smirnov statistic;distribution functions;monte carlo simulation;critical value;log normal distribution;anderson darling statistic;gaussian distribution;monte carlo methods;progressive type ii censoring;hypothesis test	We propose a general purpose approximate goodness-of-fit test that covers several families of distributions under progressive Type-II censored data. The test procedure is based on the empirical distribution function (EDF), and generalizes the goodness-of-fit test proposed by Chen and Balakrishnan [11] to progressively Type-II censored data. The new method requires some tables for critical values, which are constructed by Monte Carlo simulation. The power of the proposed tests are then assessed for several alternative distributions, while testing for normal, Gumbel, and log-normal distributions, through Monte Carlo simulations. It is observed that the proposed tests are quite powerful when compared to an existing goodness-of-fit test proposed for progressively Type-II censored data due to Balakrishnan et al. . The proposed goodness-of-fit test is then illustrated with two real data sets.	approximation algorithm;censoring (statistics);earliest deadline first scheduling;entity–relationship model;monte carlo method;simulation;type inference	Reza Pakyari;N. Balakrishnan	2012	IEEE Transactions on Reliability	10.1109/TR.2012.2182811	econometrics;mathematical optimization;statistical hypothesis testing;mathematics;statistics;monte carlo method	ML	30.845991451737397	-20.4065148908017	40926
068603c5efb562c8476e897d87d7da5c9f9b1609	robust audio watermarking using frequency-selective spread spectrum	watermarking;audio signal processing;normalised correlation;spread spectrum;human auditory system;robust audio watermarking;correlation methods;band dependent frequency masking characteristics;host audio signal;watermarking audio signal processing correlation methods;estimation correlation;estimation correlation robust audio watermarking frequency selective spread spectrum technique host audio signal band dependent frequency masking characteristics human auditory system blind watermark detection methods normalised correlation;frequency selective spread spectrum technique;blind watermark detection methods;audio watermarking	A novel audio watermarking scheme based on frequency-selective spread spectrum (FSSS) technique is presented. Unlike most of the existing spread spectrum (SS) watermarking schemes that use the entire audible frequency range for watermark embedding, the proposed scheme randomly selects subband(s) signal(s) of the host audio signal for watermark embedding. The proposed FSSS scheme provides a natural mechanism to exploit the band-dependent frequency-masking characteristics of the human auditory system to ensure the fidelity of the host audio signal and the robustness of the embedded information. Key attributes of the proposed scheme include reduced host interference in watermark detection, better fidelity, secure embedding and improved multiple watermark embedding capability. To detect the embedded watermark, two blind watermark detection methods are examined, one based on normalised correlation and the other based on estimation correlation. Extensive simulation results are presented to analyse the performance of the proposed scheme for various signal manipulations and standard benchmark attacks. A comparison with the existing full-band SS-based schemes is also provided to show the improved performance of the proposed scheme.		Hafiz Malik;Rashid Ansari;Ashfaq A. Khokhar	2008	IET Information Security	10.1049/iet-ifs:20070145	speech recognition;telecommunications;audio signal processing;digital watermarking;computer science;watermark;spread spectrum	Crypto	41.95103164616288	-8.857802019284433	40928
423c5b934ea1648cfadc8cfd729702d51f604195	a fast multi-resolution block matching algorithm for multiple-frame motion estimation	tecnologia electronica telecomunicaciones;estimation mouvement;algoritmo busqueda;image processing;multiple frame;algorithme recherche;estimacion movimiento;search algorithm;procesamiento imagen;motion estimation;analyse multiresolution;traitement image;multi resolution structure;analyse performance;performance analysis;correspondencia bloque;complexity motion estimation;block matching;correspondance bloc;tecnologias;multi resolution;grupo a;fast search;multiresolution analysis;block matching algorithm;analisis multiresolucion;analisis eficacia	이 논문에서는 H.264 를 통해 그 성능이 입증된 다중 프레임 움직임 추정에서의 계산량을 줄이기 위해, 3 개의 탐색 레벨을 가지는 고속 다해상도 블록 정합 알고리즘을 제안하였다. 제안된 방법은 효율적인 다해상도 탐색과 움직임 벡터의 공간적 상관성을 이용하여, 전역 탐색 기법(full search algorithm)에 가까운 PSNR 성능을 유지하는 고속 탐색 방법이다. 또한, 두 가지의 시간축 방향의 계산량 감소 방법을 제안하였다. 첫째 시공간적으로 인접한 매크로블록(MB; macroblock)과 첫번째 기준 프레임 (reference frame)의 탐색 결과로부터 얻어지는 정보를 이용하여 상위 및 중간 레벨에서 계산량을 감소시킨다. 그리고 두번째로는 통계적 정보를 이용하여 하위 레벨에서 계산량을 감소시킨다. 하위 레벨에 적용되는 예측 움직임 벡터(PMV; predicted motion vector) 결정 시 성능 감소를 막기위해, 다해상도 구조를 변경하여 예측 움직임 벡터가 중간 레벨에서 얻어지도록 하였다. 실험결과를 통해, 제안된 방법이 전역 탐색 기법에 비해 아주 적은 계산량으로 비슷한 PSNR 성능을 보임을 입증하였다. 특히, 제안된 방법은 빠른 움직임을 가지는 비디오 시퀀스(video sequences)에 대해 기존의 고속 기법과 비교할 때, 비슷한 계산량으로 눈에 띄게 향상된 PSNR 성능을 보인다. 다중 프레임 움직임 추정을 위한 고속 다해상도 블록 정합 알고리즘 김명준, 권성민, 나종범 한국과학기술원 전자전산학과 A Fast Multi-Resolution Block Matching Algorithm for Multiple Frame Motion Estimation Myung Jun Kim, Sung Min Kwon, and Jong Beom Ra Dept. of Electrical Engineering and Computer Science, KAIST mjkim@issserver.kaist.ac.kr, smkwon@issserver.kaist.ac.kr, jbra@ee.kaist.ac.kr	block-matching algorithm;computational complexity theory;jumbo frame;macroblock;motion estimation;peak signal-to-noise ratio;reference frame (video)	Myung Jun Kim;Yun-Gu Lee;Jong Beom Ra	2005	IEICE Transactions	10.1093/ietisy/e88-d.12.2819	multiresolution analysis;computer vision;mathematical optimization;image processing;computer science;motion estimation;mathematics;geometry;block-matching algorithm;algorithm;search algorithm	Vision	46.59893778355173	-14.26218503263147	41020
81fe1b3fbfd12c6770b32aae1f46191e9a336bb5	a 9.6 kb/s speech coder using the bell laboratories dsp integrated circuit	processing element;nonlinear filters;detectors;digital signal processing;integrated circuit;decoding;logic design;speech;logic circuits;influenza;acoustic signal detection;digital signal processor;time domain;speech digital signal processing detectors autocorrelation acoustic signal detection nonlinear filters logic design logic circuits decoding influenza;hardware implementation;autocorrelation	A digital speech coder has been designed for real-time operation for a data rate of 9.6 kb/s. The design is based on a combination of two speech compression techniques: Time-Domain Harmonic Scaling (TDHS) and Sub-Band Coding (SBC). It is a highly modularized hardware implementation using five Bell Laboratories Digital Signal Processor (DSP) integrated circuits as the key processing elements. Three DSPs are used in the encoder for pitch detection, TDHS compression and sub-band encoding. Another two DSPs are used in the receiver for sub-band decoding and TDHS expansion.	data rate units;digital signal processor;integrated circuit;kilobyte;speech coding	Ronald E. Crochiere;Richard V. Cox;James D. Johnston;L. Seltzer	1982		10.1109/ICASSP.1982.1171414	digital signal processor;detector;logic synthesis;real-time computing;speech recognition;autocorrelation;logic gate;time domain;computer science;speech;integrated circuit;digital signal processing;statistics	EDA	47.535322706834826	-7.613124427697397	41059
10a0d93b717e4791c847fc2d05bf1803e0dc218e	an entropy coding method for floating-point texture coordinates of 3d mesh	entropy coding method;3d mesh compression;texture coordinates 3d mesh compression entropy coding;floating point texture coordinates;arithmetic coding algorithm;image coding;mesh compression;entropy coding geometry image coding data compression topology graphics couplings arithmetic bit rate design engineering;arithmetic coding;data compression;geometry;entropy coding;image texture;statistical properties;arithmetic coding algorithm entropy coding method floating point texture coordinates 3d mesh compression graphics;3d model;arithmetic codes;three dimensional displays;entropy codes;mesh generation arithmetic codes data compression entropy codes floating point arithmetic image texture;solid modeling;floating point;floating point arithmetic;compression;mesh generation;texture coordinates;graphics;3d mesh	3D mesh compression has been studies for compact storage and fast transmission of large 3D meshes for a variety of graphics applications. However, there is still much redundancy not well exploited by the previous works, e.g., the redundancy lying in the floating-point data texture coordinates. This paper investigates the statistical property of texture coordinates of a couple of 3D models and proposes a new entropy coding method to compress the floating-point texture coordinates of 3D mesh. It uses some reference buffers and proposes an updating scheme as well as a context based arithmetic coding algorithm. Experiments show that the proposed method can averagely reduce about 40% of bitrate compared with the state-of-art prior work.	3d computer graphics;algorithm;arithmetic coding;entropy encoding;lossless compression;texture mapping	Tong Zhou;Yong Liu;Quqing Chen;Kangying Cai;Jun Teng;Zhibo Chen	2010	Proceedings of 2010 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2010.5537779	computer vision;computer science;floating point;theoretical computer science;operating system;mathematics;texture compression;engineering drawing;statistics	Visualization	43.09347277590468	-18.472792454311726	41091
1cf2991b56bc490eee900436989663d3d7917a5f	cellular automaton model with non-hypothetical congested steady state reproducing the three-phase traffic flow theory		A new assumption is assumed to explain the mechanisms of traffic flow that in the noiseless limit, vehicles' space gap will oscillate around the desired space gap, rather than keep the desired space gap, in the homogeneous congested traffic flow. It means there are no steady states of congested traffic and contradicts with the fundamental diagram approach and three-phase traffic flow theory both of which admit the existence of steady states of congested traffic. In order to verify this assumption, a cellular automaton model with non-hypothetical congested steady state is proposed, which is based on the Nagel-Schreckenberg model with additional slow-to-start and the effective desired space gap. Simulations show that this new model can produce the synchronized flow, the transitions from free flow to synchronized flow to wide moving jams, and multiple congested patterns observed by the three-phase theory.	cellular automaton;steady state	Junfang Tian;Martin Treiber;Chenqiang Zhu;Bin Jia;HuiXuan Li	2014		10.1007/978-3-319-11520-7_65	discrete mathematics;simulation;three-phase traffic theory;newell's car-following model;mathematics	Logic	38.28210432915843	1.3955049326197384	41100
af6c7c0b1c9f6815efd65ea3e070616516e73eb0	an effective cu size decision method for hevc encoders	algorithme rapide;modelizacion;theorie vitesse distorsion;codificacion de video;estimation mouvement;image coding;quad tree;correlation encoding materials standards algorithm design and analysis video coding prediction algorithms;complexite calcul;codificacion de imagenes;quad arbol;efficiency;estimacion movimiento;depth of field;motion estimation;cu size decision;lagrange multiplier;homogeneidad;probleme terminaison;profondeur champ;hevc;journal;skip mode checking effective cu size decision method hevc encoder high efficiency video coding standard quadtree structured coding unit recursive splitting motion estimation prediction mode least rate distortion cost lagrange multiplier coding efficiency computational complexity fast cu size decision algorithm optimal depth level early termination method motion homogeneity checking rd cost checking;rate distortion theory;modelisation;codage image;video coding;eficacia;complejidad computacion;codage video;computational complexity;fast algorithm;multiplicateur lagrange;efficacite;profundidad campo;multiplicador lagrange;quad arbre;termination problem;motion estimation cu size decision hevc;homogeneite;modeling;quadtrees;algoritmo rapido;problema terminacion;video coding computational complexity motion estimation quadtrees rate distortion theory;homogeneity	The emerging high efficiency video coding standard (HEVC) adopts the quadtree-structured coding unit (CU). Each CU allows recursive splitting into four equal sub-CUs. At each depth level (CU size), the test model of HEVC (HM) performs motion estimation (ME) with different sizes including 2N × 2N, 2N × N, N × 2N and N × N. ME process in HM is performed using all the possible depth levels and prediction modes to find the one with the least rate distortion (RD) cost using Lagrange multiplier. This achieves the highest coding efficiency but requires a very high computational complexity. In this paper, we propose a fast CU size decision algorithm for HM. Since the optimal depth level is highly content-dependent, it is not efficient to use all levels. We can determine CU depth range (including the minimum depth level and the maximum depth level) and skip some specific depth levels rarely used in the previous frame and neighboring CUs. Besides, the proposed algorithm also introduces early termination methods based on motion homogeneity checking, RD cost checking and SKIP mode checking to skip ME on unnecessary CU sizes. Experimental results demonstrate that the proposed algorithm can significantly reduce computational complexity while maintaining almost the same RD performance as the original HEVC encoder.	algorithm;algorithmic efficiency;cuda;computational complexity theory;data compression;distortion;encoder;high efficiency video coding;lagrange multiplier;motion estimation;performance;quadtree;rate–distortion theory;recursion;ruby document format;tip (unix utility);video coding format	Liquan Shen;Zhi Liu;Xinpeng Zhang;Wenqiang Zhao;Zhaoyang Zhang	2013	IEEE Transactions on Multimedia	10.1109/TMM.2012.2231060	homogeneity;simulation;systems modeling;rate–distortion theory;computer science;theoretical computer science;quadtree;motion estimation;depth of field;mathematics;efficiency;lagrange multiplier;computational complexity theory;algorithm;statistics	AI	47.983190868827876	-18.29801693669785	41104
d5d7345441b819a8072a849a2435d682c148a784	bio-inspired analog parallel array processor chip with programmable spatio-temporal dynamics	electronic mail;cmos technology;early vision tasks;temporal dynamics;building block;very large scale integration;cellular neural nets analogue processing circuits parallel processing programmable circuits vlsi cmos analogue integrated circuits;prototypes;programmable spatio temporal dynamics;biological system modeling;0 5 micron;cellular neural nets;chip;cmos analogue integrated circuits;complex system;retina cellular neural networks semiconductor device modeling prototypes photodetectors electronic mail very large scale integration rendering computer graphics cmos technology biological system modeling;retina;semiconductor device modeling;high complexity system;programmable circuits;photodetectors;vlsi;visual pathway;cellular neural networks;rendering computer graphics;analogue processing circuits;0 5 micron bio inspired analog parallel array processor chip programmable spatio temporal dynamics apap vlsi early vision tasks cmos high complexity system;bio inspired analog parallel array processor chip;cmos;parallel processing;apap	A bio-inspired model for an analog parallel array processor (APAP), based on studies on the vertebrate retina, permits the realization of complex spatio-temporal dynamics in VLSI. This model mimics the way in which images are processed in the natural visual pathway which renders a feasible alternative for the implementation of early vision tasks in standard technologies. A prototype chip has been designed and fabricated in 0.5 /spl mu/m CMOS. Design challenges, trade-offs and the building blocks of such a high-complexity system (0.5/spl times/10/sup 6/ transistors, most of them operating in analog mode) are presented in this paper.	array processing;parallel array;vector processor	Ricardo Carmona-Galán;Francisco Jiménez-Garrido;Rafael Domínguez-Castro;Servando Espejo-Meana;Ángel Rodríguez-Vázquez	2002		10.1109/ISCAS.2002.1010778	embedded system;parallel processing;electronic engineering;computer hardware;telecommunications;computer science;electrical engineering;very-large-scale integration;cmos	Arch	41.2373645052491	-2.5014796281183305	41106
e5c93cbd8e4a8ce3e2c8c19d4abeef78c000adae	re-em trees: a data mining approach for longitudinal and clustered data	panel data;cart;regression tree;random effects;mixed effects model;longitudinal data;clustered data	Longitudinal data refer to the situation where repeated observations are available for each sampled object. Clustered data, where observations are nested in a hierarchical structure within objects (without time necessarily being involved) represent a similar type of situation. Methodologies that take this structure into account allow for the possibilities of systematic differences between objects that are not related to attributes and autocorrelation within objects across time periods. A standard methodology in the statistics literature for this type of data is the mixed effects model, where these differences between objects are represented by so-called “random effects” that are estimated from the data (population-level relationships are termed “fixed effects,” together resulting in a mixed effects model). This paper presents a methodology that combines the structure of mixed effects models for longitudinal and clustered data with the flexibility of tree-based estimation methods. We apply the resulting estimation method, called the RE-EM tree, to pricing in online transactions, showing that the RE-EM tree is less sensitive to parametric assumptions and provides improved predictive power compared to linear models with random effects and regression trees without random effects. We also apply it to a smaller data set examining accident fatalities, and show that the RE-EM tree strongly outperforms a tree without random effects while performing comparably to a linear model with random effects. We also perform extensive simulation experiments to show that the estimator improves predictive performance relative to regression trees without random effects and is comparable or superior to using linear models with random effects in more general situations.	autocorrelation;challenge-handshake authentication protocol;cluster analysis;data mining;decision tree;e-commerce;experiment;fixed effects model;general linear model;higher-order function;mixed model;r language;random effects model;simulation;tree structure;unbalanced circuit	Rebecca J. Sela;Jeffrey S. Simonoff	2011	Machine Learning	10.1007/s10994-011-5258-3	random binary tree;mixed model;econometrics;generalized linear mixed model;computer science;fixed effects model;machine learning;decision tree;data mining;panel data;statistics;random effects model	ML	30.626465232085426	-23.069971184167315	41141
5b0db70b246eed16c2d17bc3e520ad353f55dea9	an adaptive multi-swarm optimizer for dynamic optimization problems	conference;multipopulation adaptation;dynamic optimization problems;particle swarm optimization;particle swarm optimization multipopulation adaptation dynamic optimization problems;multi population adaptation	The multipopulation method has been widely used to solve dynamic optimization problems (DOPs) with the aim of maintaining multiple populations on different peaks to locate and track multiple changing optima simultaneously. However, to make this approach effective for solving DOPs, two challenging issues need to be addressed. They are how to adapt the number of populations to changes and how to adaptively maintain the population diversity in a situation where changes are complicated or hard to detect or predict. Tracking the changing global optimum in dynamic environments is difficult because we cannot know when and where changes occur and what the characteristics of changes would be. Therefore, it is necessary to take these challenging issues into account in designing such adaptive algorithms. To address the issues when multipopulation methods are applied for solving DOPs, this paper proposes an adaptive multi-swarm algorithm, where the populations are enabled to be adaptive in dynamic environments without change detection. An experimental study is conducted based on the moving peaks problem to investigate the behavior of the proposed method. The performance of the proposed algorithm is also compared with a set of algorithms that are based on multipopulation methods from different research areas in the literature of evolutionary computation.	adaptive algorithm;dynamic programming;evolutionary computation;experiment;global optimization;mathematical optimization;numerous;population;swarm	Changhe Li;Shengxiang Yang;Ming Yang	2014	Evolutionary Computation	10.1162/EVCO_a_00117	mathematical optimization;multi-swarm optimization;computer science;artificial intelligence;machine learning;particle swarm optimization	AI	25.922190196266374	-4.986599509511823	41143
568623f30f2a70f996a360189ca548f1c68f4a0f	pavlov's dog associative learning demonstrated on synaptic-like organic transistors	time dependent;associative learning;low power;field effect transistor;temporal coding;physical properties;neural network	In this letter, we present an original demonstration of an associative learning neural network inspired by the famous Pavlov's dogs experiment. A single nanoparticle organic memory field effect transistor (NOMFET) is used to implement each synapse. We show how the physical properties of this dynamic memristive device can be used to perform low-power write operations for the learning and implement short-term association using temporal coding and spike-timing-dependent plasticity–based learning. An electronic circuit was built to validate the proposed learning scheme with packaged devices, with good reproducibility despite the complex synaptic-like dynamic of the NOMFET in pulse regime.	artificial nanoparticles;artificial neural network;biological neural networks;canis familiaris;cochlear structure;computation (action);content-addressable memory;elementary;electronic circuit;exhibits as topic;experiment;field effect (semiconductor);hl7publishingsubsection <operations>;inspiration function;learning disorders;low-power broadcasting;memory disorders;nomfet;nanodevice;neural network simulation;neural coding;noosphere;physical phenomenon or property;relevance;retina;scalability;sensor;synapse;transistor;volatility	Olivier Bichler;Fabien Alibart;Stéphane Pleutin;Stéphane Lenfant;Dominique Vuillaume;Christian Gamrat	2013	Neural Computation	10.1162/NECO_a_00377	field-effect transistor;computer science;artificial intelligence;machine learning;physical property;artificial neural network	ML	39.20746108854323	-0.701799607271951	41166
382270c7bf00882e40eb41ad3113fec359a1487f	identifying structural mechanisms in standard genetic programming	genetic program	This paper presents a hypothesis about an undiscovered class of mechanisms that exist in standard GP. Rather than being intentionally designed, these mechanisms would be an unintended consequence of using trees as information structures. A model is described that predicts outcomes in GP that would arise solely from such mechanisms. Comparisons with empirical results from GP lend support to the existence of these mechanisms.	genetic programming;tree (data structure);unintended consequences	Jason M. Daida;Adam M. Hilss	2003		10.1007/3-540-45110-2_58	computer science;bioinformatics;artificial intelligence;algorithm	AI	25.098908555764396	-9.680598588097	41198
8032520e3f9fa4e850b3c2a2601a0ef4ee64e83a	a self-adaptive video dual watermarking based on the motion characteristic and geometric invariant for ubiquitous multimedia	discrete wavelet transforms;watermarking;video dual watermarking;会议论文;geometric invariant of sift;visualization;streaming media;video dual watermarking motion characteristics geometric invariant of sift;feature extraction;robustness;zero watermarking self adaptive video dual watermarking geometric invariant ubiquitous multimedia digital copyright motion characteristics detection scale invariant feature transform sift embedding strength discrete wavelet transform dwt ipr database;watermarking visualization feature extraction streaming media discrete wavelet transforms robustness;video watermarking copyright discrete wavelet transforms geometry multimedia communication ubiquitous computing;motion characteristics	In ubiquitous multimedia applications, how to protect the digital copyright for multimedia data has been a difficult task. In this paper, a novel self-adaptive video dual watermarking, which is combined the motion characteristics detection with the geometric invariant of Scale-invariant feature transform (SIFT) is proposed. For each frame, the motion characteristics are calculated as the maximum of embedding strength. Moreover, the maximum of embedding strength are firstly embedded into the middle and high frequency bands in discrete wavelet transform (DWT) domain as the first watermarking signals. Then the watermarked frame is used to obtain the feature points with the invariant performance of SIFT descriptors. These feature points, as the other watermark, are registered and stored in IPR database as zero-watermarking. In the process of watermarking extraction, to each frame, the SIFT feature points are acquired through zero-watermarking. By using the scale feature and coordinate relationship of the SIFT feature points, the video frames are detected and corrected to makes the extraction of the watermark information is synchronized. Experiments indicate that using Motion Characteristic and geometric invariant of SIFT can significantly improve the watermarking imperceptibility, effectively resist common geometrical attacks for video watermarking and consequently achieve higher robustness.	blocking (computing);digital watermarking;discrete wavelet transform;embedded system;frequency band;human visual system model;interpro;nonlinear system;scale-invariant feature transform;watermark (data file)	Zhi Li;Ling-Feng Liu;Chuan-Xian Jiang	2015	2015 IEEE International Conference on Smart City/SocialCom/SustainCom (SmartCity)	10.1109/SmartCity.2015.42	computer vision;theoretical computer science;mathematics;multimedia	Robotics	39.6409070407877	-13.446951369107412	41236
a703229da6216650e73af4ad72098883e2855b1d	an aco-rfd hybrid method to solve np-complete problems	river formation dynamics ant colony optimization heuristic algorithms np hard problems;ant colony optimization;np hard problems;heuristic algorithms;pablo rabanal ismael rodriguez fernando rubio 混合动力 np完全问题 最佳路径 rfd 梯度方向 混合方法 np难问题 旅行商问题 an aco rfd hybrid method to solve np complete problems;river formation dynamics	In this paper we hybridize ant colony optimization (ACO) and river formation dynamics (RFD), two related swarm intelligence methods. In ACO, ants form paths (problem solutions) by following each other’s pheromone trails and reinforcing trails at best paths until eventually a single path is followed. On the other hand, RFD is based on copying how drops form rivers by eroding the ground and depositing sediments. In a rough sense, RFD can be seen as a gradient-oriented version of ACO. Several previous experiments have shown that the gradient orientation of RFD makes this method solve problems in a different way as ACO. In particular, RFD typically performs deeper searches, which in turn makes it find worse solutions than ACO in the first execution steps in general, though RFD solutions surpass ACO solutions after some more time passes. In this paper we try to get the best features of both worlds by hybridizing RFD and ACO. We use a kind of ant-drop hybrid and consider both pheromone trails and altitudes in the environment. We apply the hybrid method, as well as ACO and RFD, to solve two NP-hard problems where ACO and RFD fit in a different manner: the traveling salesman problem (TSP) and the problem of the minimum distances tree in a variable-cost graph (MDV). We compare the results of each method and we analyze the advantages of using the hybrid approach in each case.	ant colony optimization algorithms;benchmark (computing);entity;experiment;genetic algorithm;gradient;karp's 21 np-complete problems;mathematical optimization;np-completeness;np-hardness;polynomial;polynomial-time reduction;propositional calculus;swarm intelligence;traverse;travelling salesman problem	Pablo Rabanal;Ismael Rodríguez;Fernando Rubio	2013	Frontiers of Computer Science	10.1007/s11704-013-2302-4	mathematical optimization;ant colony optimization algorithms;computer science;artificial intelligence;np-hard;operations research;algorithm	AI	24.897152829374466	1.4793464518268256	41244
a20f113c690a2e58761c92f848573cbe4b0a9a8f	neurogenetic approach for solving dynamic programming problems	dynamic programming;search spaces dynamic programming problems shortest path problem combinatorial optimization problem modified hopfield neural network neurogenetic approach genetic algorithm;neurogenetic approach;dynamic programming genetic algorithms neurons shortest path problem hopfield neural networks constraint optimization artificial neural networks neurofeedback equations design optimization;search space;computer model;combinatorial optimization problem;dynamic programming problems;dynamic program;equilibrium point;hopfield neural network;optimization problem;modified hopfield neural network;biological cells;hopfield neural networks;search spaces;genetic algorithm;genetic algorithms;optimization;search problems;neurons;genetic algorithm shortest path problem hopfield neural network;combinatorial mathematics;dynamic optimization problem;large classes;shortest path problem;gallium;search problems combinatorial mathematics dynamic programming genetic algorithms	The shortest path problem is the classical combinatorial optimization problem arising in numerous planning and designing contexts. This paper presents a association of a modified Hopfield neural network, which is a computing model capable of solving a large class of optimization problems, with a genetic algorithm, that to make possible cover nonlinear and extensive search spaces, which guarantees the convergence of the system to the equilibrium points that represent solutions for the dynamic optimization problems. Experimental results are presented and discussed.	artificial neural network;combinatorial optimization;cybernetics;dynamic programming;emoticon;genetic algorithm;hopfield network;mathematical optimization;nonlinear system;optimization problem;shortest path problem	Matheus Giovanni Pires;Ivan Nunes da Silva;Fabiana Cristina Bertoni	2008	2008 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2008.4811609	computer simulation;optimization problem;mathematical optimization;genetic algorithm;combinatorial optimization;computer science;artificial intelligence;machine learning;mathematics;3-opt	Robotics	26.950093712791052	-7.276524419135343	41272
12234f05c9090deebfae8219b41614df0c979b07	calibration of distributed sound acquisition systems using toa measurements from a moving acoustic source	time of arrival estimation calibration equalisers microphone arrays;array calibration microphone localization;microphones calibration acoustics delays arrays noise acoustic measurements;distributed microphone array microphone localization gain differences calibration signal moving acoustic source spatially distributed acoustic events gain equalization toa measurements time of arrival measurements	We present a method for calibrating a distributed microphone array using time-of-arrival (TOA) measurements. The calibration encompasses localization and gain equalization of the microphones, which are both important in applications such as beamforming. The availability of accurate TOA measurements between the microphones and a set of spatially distributed acoustic events is pivotal to the calibration task. We propose to use a moving acoustic source emitting a calibration signal at known intervals. We then show that the TOAs and the observed signals can be used to estimate the gain differences between microphones in addition to the more established microphone localization. Finally, we provide experimental results with simulated and real measured data to demonstrate that our approach facilitates accurate TOA measurements and hence, accurate localization and gain equalization, even in reverberant and noisy conditions.	acoustic cryptanalysis;beamforming;internationalization and localization;microphone;time of arrival	Nikolay D. Gaubitch;W. Bastiaan Kleijn;Richard Heusdens	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6855049	speech recognition;noise-canceling microphone	Robotics	50.2396800501874	3.6382558025220995	41286
748452ffdbe3d5c9bdcaff260cc3b2e878e8d1ef	real royal road functions for constant population size	uniform crossover;selection problem;problema seleccion;difference operator;temps polynomial;004;loi probabilite;ley probabilidad;search space;population size;optimization method;royal road function;one point crossover;algoritmo genetico;metodo optimizacion;mutacion;49xx;informatique theorique;real function;random function;probability distribution;fonction reelle;polynomial time;methode optimisation;algorithme genetique;timing analysis;evolution strategy;algorithme evolutionniste;genetic algorithm;regime permanent;algoritmo evolucionista;run time analysis;regimen permanente;evolutionary algorithm;26xx;funcion real;mutation;computer theory;steady state;tiempo polinomial;probleme selection;informatica teorica	Evolutionary and genetic algorithms (EAs and GAs) are quite successful randomized function optimizers. This success is mainly based on the interaction of different operators like selection, mutation, and crossover. Since this interaction is still not well understood, one is interested in the analysis of the single operators. Jansen and Wegener (2001a) have described so-called real royal road functions where simple steady-state GAs have a polynomial expected optimization time while the success probability of mutation-based EAs is exponentially small even after an exponential number of steps. This success of the GA is based on the crossover operator and a population whose size is moderately increasing with the dimension of the search space. Here new real royal road functions are presented where crossover leads to a small optimization time, although the GA works with the smallest possible population	crossover (genetic algorithm);genetic algorithm;ingo wegener;mathematical optimization;mutation (genetic algorithm);polynomial;population;randomized algorithm;selection (genetic algorithm);software release life cycle;steady state;time complexity	Tobias Storch;Ingo Wegener	2004	Theor. Comput. Sci.	10.1016/j.tcs.2004.03.047	mutation;probability distribution;time complexity;population size;genetic algorithm;computer science;artificial intelligence;evolutionary algorithm;random function;calculus;mathematics;evolution strategy;steady state;static timing analysis;algorithm;statistics	Theory	28.03255258715228	2.4206202021497263	41438
a7e62ee46e288158eabcc9c44464313d2439e4b4	fpsoc using xilinx zynq for medical image coding based on the quaternionic paraunitary filter banks	fpga;lossless image coding fpga;low latency separable image processing xilinx zynq medical image coding quaternionic paraunitary filter banks low cost fpsoc telemedicine applications generalized block lifting structure 2d cordic algorithm 8 band linear phase paraunitary filter bank 4 band linear phase paraunitary filter bank one regularity constraints hypercomplex coefficients lossy to lossless image coding integer to integer transform parallel pipelined efficient architecture;image coding computer architecture transforms biomedical imaging quaternions filter banks;lossless image coding;transforms channel bank filters image coding image filtering medical image processing pipeline processing telemedicine	In this paper, we have introduced a low-cost FPSoC for medical image coding and implemented to telemedicine applications based on the Xilinx Zynq. We have recently introduced a generalized block-lifting structure using the 2-D CORDIC algorithm as a block of 4- and 8-band linear phase paraunitary filter banks (LP PUFB) based on the quaternionic algebra (Q-PUFB) with one-regularity constraints on hypercomplex coefficients of the schemes for the lossy-to-lossless image coding. Its structure can implement the integer-to-integer transform (I-Q-PUFB). The parallel-pipelined efficient architecture (P2E_Q-PUFB) has been proposed. The low latency separable image processing is implemented in the given architecture.	algorithm;cordic;coefficient;filter bank;image processing;lifting scheme;linear phase;lossless compression;lossy compression	Nick Petrovsky;Andrew Stankevich;Alexandr Petrovsky	2015	2015 17th International Conference on E-health Networking, Application & Services (HealthCom)	10.1109/HealthCom.2015.7454573	computer vision;electronic engineering;theoretical computer science;mathematics	Robotics	45.19382198991961	-9.691671030640004	41440
8537c21b4ede82a3a19642c66e7ba1ea424a8632	particle swarm optimization in stationary and dynamic environments	thesis	Inspired by social behavior of bird flocking or fish schooling, Eberhart and Kennedy first developed the particle swarm optimization (PSO) algorithm in 1995. PSO, as a branch of evolutionary computation, has been successfully applied in many research and application areas in the past several years, e.g., global optimization, artificial neural network training, and fuzzy system control, etc.. Especially, for global optimization, PSO has shown its superior advantages and effectiveness. Although PSO is an effective tool for global optimization problems, it shows weakness while solving complex problems (e.g., shifted, rotated, and compositional problems) or dynamic problems (e.g., the moving peak problem and the DF1 function). This is especially true for the original PSO algorithm. In order to improve the performance of PSO to solve complex problems, we present a novel algorithm, called self-learning PSO (SLPSO). In SLPSO, each particle has four different learning strategies to deal with different situations in the search space. The cooperation of the four learning strategies is implemented by an adaptive framework at the individual level, which can enable each particle to choose the optimal learning strategy according to the properties of its own local fitness landscape. This flexible learning mechanism is able to automatically balance the behavior of exploration and exploitation for each particle in the entire search space during the whole running process. Another major contribution of this work is to adapt PSO to dynamic environments, we propose an idea that applies hierarchical clustering techniques to generate multiple populations. This idea is the first attempt to solve some open issues when using multiple population methods in dynamic environments, such as, how to define the size of search region of a sub-population, how many individuals are needed in each sub-population, and how many sub-populations are needed, etc.. Experimental study has shown that this idea is effective to locate and track multiple peaks in dynamic environments.	algorithm;artificial neural network;cluster analysis;evolutionary computation;exploit (computer security);fuzzy control system;global optimization;hierarchical clustering;mathematical optimization;particle swarm optimization;phase-shift oscillator;population;russell c. eberhart;stationary process	Changhe Li	2011			mathematical optimization;multi-swarm optimization	AI	27.02372899654193	-4.860586293569765	41443
2385b40f78314ff1c6ad9a10d5d7fd464544f2ed	robust specification of the roughness penalty prior distribution in spatially adaptive bayesian p-splines models	institutional repositories;bayes estimation;prior specification;sampler;model specification;computacion informatica;fedora;41a15;analisis datos;melange loi probabilite;bayesian p splines;echantillonnage gibbs;fonction repartition;gibbs sampling;prior distribution;metodo penalidad;mixed distribution;ley a priori;vital;funcion distribucion;data analysis;estimacion bayes;aproximacion esplin;distribution function;penalty method;adaptive penalties;methode penalite;posterior distribution;65d07;specification modele;markov chain monte carlo;especificacion modelo;ciencias basicas y experimentales;spline approximation;approximation spline;matematicas;statistical computation;calculo estadistico;ley a posteriori;mezcla ley probabilidad;analyse donnee;muestreador;calcul statistique;b spline;vtls;grupo a;muestreo gibbs;60e05;echantillonneur;loi a posteriori;ils;b splin;loi a priori;estimation bayes	The potential important role of the prior distribution of the roughness penalty parameter in the resulting smoothness of Bayesian P-splines models is considered. The recommended specification for that distribution yields models that can lack flexibility in specific circumstances. In such instances, these are shown to correspond to a frequentist P-splines model with a predefined and severe roughness penalty parameter, an obviously undesirable feature. It is shown that the specification of a hyperprior distribution for one parameter of that prior distribution provides the desired flexibility. Alternatively, a mixture prior can also be used. An extension of these two models by enabling adaptive penalties is provided. The posterior of all the proposed models can be quickly explored using the convenient Gibbs sampler. ∗Correspondence to: Philippe Lambert, Université catholique de Louvain, Institut de Statistique, Voie du Roman Pays 20, B-1348 Louvain-la-Neuve (Belgium). E-mail: lambert@stat.ucl.ac.be Phone: +32-10-47.28.01 Fax: +32-10-47.30.32	fax;gibbs sampling;linear algebra;louvain modularity;robustness (computer science);sampling (signal processing);spline (mathematics)	Astrid Jullion;Philippe Lambert	2007	Computational Statistics & Data Analysis	10.1016/j.csda.2006.09.027	b-spline;econometrics;mathematical optimization;prior probability;gibbs sampling;markov chain monte carlo;distribution function;penalty method;mathematics;posterior probability;bayesian hierarchical modeling;data analysis;specification;statistics	ML	32.8084313190975	-23.434474289395602	41467
44f8e2654bc06eda50501feea898f36de18987cb	adaptive data compression for efficient sequential transmission and change updating of remote sensing images	remote sensing image;geophysical image processing;data transmission;compression algorithm;change detection;image coding;huffman coding change detection pca linear predictive model;data compression;huffman coding;linear predictive model;compression algorithms;huffman codes;data communication;linear predictive;image compression;three step pre processing;principal component analysis;remote sensing;indexation;different compression algorithms;huffman coding adaptive data compression sequential data transmission remote sensing images three step pre processing binary index image image compression different compression algorithms pca linear predictive model;sequential data transmission;remote sensing data compression geophysical image processing geophysical techniques huffman codes image coding;predictive models;image analysis;binary index image;data compression remote sensing image coding principal component analysis compression algorithms predictive models image analysis australia data communication multispectral imaging;adaptive data compression;remote sensing images;pca;geophysical techniques;australia;multispectral imaging	In this paper, a selective data compression scheme is developed to combine the need for efficient data transmission and the receivers' interest in changes presented in the data by taking the advantage of the fact that receivers hold a copy of previous data. Three-step pre-processing is introduced in this paper. Firstly we separate the unchanged areas (the majority) of the image from the changed areas between successive images of the same area. Secondly, the bands which are sensitive to the changes are identified with the aid of statistical measures. Finally, a binary index image of each band is generated to indicate the two categories. Following this pre-processing, compression of the unchanged areas and changed areas is conducted separately. In this way, different compression algorithms can be applied to each case. As the majority data will be unchanged and only a subset of bands reflects the changes, high compression rate is achievable.	algorithm;bitmap index;data compression;preprocessor	Md. Abdullah Al Mamun;Xiuping Jia;Michael Ryan	2009	2009 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2009.5417422	data compression;computer vision;image analysis;image compression;computer science;pattern recognition;statistics;remote sensing	Mobile	43.880401247212426	-14.945426361878468	41501
3f7facce5da2bf363208bbfcecf485df232992ea	a knowledge-based initialization technique of genetic algorithm for the travelling salesman problem	travelling salesman problem;heuristic technique genetic algorithm travelling salesman problem initial population;sociology statistics genetic algorithms cities and towns biological cells optimization heuristic algorithms;initial population genetic algorithm travelling salesman problem slow convergence local optima optimization knowledge based initialization technique tsplib;travelling salesman problems genetic algorithms;heuristic technique;genetic algorithm;initial population	Genetic Algorithm (GA) is efficient for the travelling salesman problem, but it has the defect of slow convergence and is easily trapped in local optima. Because the initialization has a profound impact on the optimization, this study proposed to improve the performance of GA by applying a knowledge-based initialization technique (KI). KI learns the features of evolved population and uses them to guide the generation of initial population. Advanced initial solution without path crossover can be fast generated with this method. Instances in TSPLIB were used to test different initialization methods. The results proved that this proposed technique helped GA get better initial population and performance.	binary prefix;computer performance;genetic algorithm;local optimum;mathematical optimization;software bug;software release life cycle;travelling salesman problem	Chao Li;Xiaogeng Chu;Yingwu Chen;Lining Xing	2015	2015 11th International Conference on Natural Computation (ICNC)	10.1109/ICNC.2015.7377988	nearest neighbour algorithm;extremal optimization;2-opt;mathematical optimization;christofides algorithm;lin–kernighan heuristic;artificial intelligence;machine learning;mathematics;chromosome;3-opt	EDA	26.190217159277694	-2.1589527739449283	41541
ad7bc835764544c0aecb6d0df14e3ab1704965f1	self-embedding fragile watermarking scheme combined average with vq encoding	fragile watermarking;vq encoding;self embedding;compression code	Combined average encoding with vector quantization (VQ) encoding, a new self-embedding fragile watermarking scheme is proposed. To take into account watermark payload, localization accuracy and recovery quality, the 6-bit average-watermark of a 2×2 original block and the 8-bit VQ-watermark of a 4×4 block of image high-frequency component are generated and hidden in the corresponding mapping blocks of them based on secret key, respectively. To improve the tamper detection performance, the validity of a 2×2 block is determined by combining the average-watermark with the VQ-watermark. The average, VQ and inpainting recovery operations are executed in sequence to improve the recovery quality especially for a larger tampering ratio. Simulation results demonstrate that the proposed scheme not only provides a better invisibility and security against the known counterfeiting attacks, but also allows image recovery with an acceptable visual quality up to 70% tampering ratios.	vector quantization	HongJie He;Fan Chen;Yaoran Huo	2012		10.1007/978-3-642-40099-5_11	computer vision;theoretical computer science;mathematics;computer security	EDA	39.7480680017645	-11.804496791612124	41558
b31be6879a351dab41f586a3f1b24148180904e6	transform coding of stereo image residuals	transformation cosinus;transformacion discreta;image coding;image processing;data compression;redundancia;procesamiento imagen;stereoscopy;imagen nivel gris;transform coding;indexing terms;traitement image;discrete cosine transform;wavelet transforms;codificacion;8 bit stereo image residuals transform coding image compression telepresence cross view redundancy post compensation coding process specialized coding methods residual image disparity compensated stereo residual quantization methods mean squared error stereo unique metric image registration directional characteristics discrete cosine transform dct performance gray scale imagery color imagery wavelet algorithm bit rate vertical channel 24 bit;codage residuel;compensation;arithmetic codes;redundancy;image compression;error cuantificuacion;discrete cosine transforms;image colour analysis;mean square error;image registration;image niveau gris;stereo image processing;transformacion coseno;coding;discrete transformation;stereoscopie;estereoscopia;compresion dato;cosine transform;grey level image;transformation discrete;erreur quantification;compression donnee;redondance;wavelet transforms transform coding image coding data compression image registration image colour analysis discrete cosine transforms stereo image processing arithmetic codes compensation;codage;quantization error;transform coding image coding discrete cosine transforms pixel displays redundancy discrete transforms quantization image registration gray scale	Stereo image compression is of growing interest because of new display technologies and the needs of telepresence systems. Compared to monoscopic image compression, stereo image compression has received much less attention. A variety of algorithms have appeared in the literature that make use of the cross-view redundancy in the stereo pair. Many of these use the framework of disparity-compensated residual coding, but concentrate on the disparity compensation process rather than the post compensation coding process. This paper studies specialized coding methods for the residual image produced by disparity compensation. The algorithms make use of theoretically expected and experimentally observed characteristics of the disparity-compensated stereo residual to select transforms and quantization methods. Performance is evaluated on mean squared error (MSE) and a stereo-unique metric based on image registration. Exploiting the directional characteristics in a discrete cosine transform (DCT) framework provides its best performance below 0.75 b/pixel for 8-b gray-scale imagery and below 2 b/pixel for 24-b color imagery, In the wavelet algorithm, roughly a 50% reduction in bit rate is possible by encoding only the vertical channel, where much of the stereo information is contained. The proposed algorithms do not incur substantial computational burden beyond that needed for any disparity-compensated residual algorithm.	algorithm;binocular disparity;computation;contain (action);discrete cosine transform;display device;experiment;grayscale;guided imagery;image compression;image registration;mean squared error;pixel;transform coding;wavelet;registration - actclass	Mark S. Moellenhoff;Mark W. Maier	1998	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.679421	computer vision;speech recognition;image processing;computer science;discrete cosine transform;mathematics;algorithm;computer graphics (images)	Vision	43.99749865476913	-16.12540257918421	41570
6a59e0a6e6f757b24af8aa2586fdec7b50afa275	learning based fast h.264/avc to hevc intra video transcoding for cloud media computing		Cloud video transcoding enable to convert the video standards and properties from one to another so as to adapt to different user end devices and network capacity, especially in sharing massive video contents in cloud environment. High Efficiency Video Coding (HEVC) and H.264/Advanced Video Coding are two recent high performance video coding standards that are widely used and co-existing in video industry. Video transcoding is desirable to bridge the standard gap. To effectively transcode video stream from H.264/AVC to HEVC for higher compression efficiency and meanwhile maintaining low computational complexity, a learning based fast H.264/AVC to HEVC transcoder is proposed for cloud media computing. We firstly analyze the correlation of block partition sizes between these two standards and then present a fast Coding Unit (CU) decision algorithm, in which three levels of binary classifiers are used to predict different CU sizes in HEVC intra coding and the optimal parameters are determined by statistical experiments. The experimental results show that the proposed transcoder achieves 44.3% time saving on average with only negligible quality degradation when compared with the original cascaded transcoder and is also superior than the state-of-the-art benchmarks in terms of complexity reduction and rate-distortion performance.	h.264/mpeg-4 avc;high efficiency video coding	Yun Zhang;Na Li;Zongju Peng	2017		10.1007/978-3-319-68542-7_32	computer science;real-time computing;reduction (complexity);parallel computing;computational complexity theory;cloud computing;transcoding	HPC	46.54416478085464	-19.48689189426229	41589
e3cea4abde1d882a9eed196244ff9e3c4b0c9b47	balancing exploration and exploitation in memetic algorithms: a learning automata approach		Correspondence Mehdi Rezapoor Mirsaleh, Department of Computer Engineering and Information Technology, Payame Noor University (PNU), PO Box 19395-3697, Tehran, Iran. Email: mrezapoorm@aut.ac.ir Abstract One of the problems with traditional genetic algorithms (GAs) is premature convergence, which makes them incapable of finding good solutions to the problem. The memetic algorithm (MA) is an extension of the GA. It uses a local search method to either accelerate the discovery of good solutions, for which evolution alone would take too long to discover, or reach solutions that would otherwise be unreachable by evolution or a local search method alone. In this paper, we introduce a new algorithm based on learning automata (LAs) and an MA, and we refer to it as LA-MA. This algorithm is composed of 2 parts: a genetic section and a memetic section. Evolution is performed in the genetic section, and local search is performed in the memetic section. The basic idea of LA-MA is to use LAs during the process of searching for solutions in order to create a balance between exploration performed by evolution and exploitation performed by local search. For this purpose, we present a criterion for the estimation of success of the local search at each generation. This criterion is used to calculate the probability of applying the local search to each chromosome. We show that in practice, the proposed probabilistic measure can be estimated reliably. On the basis of the relationship between the genetic section and the memetic section, 3 versions of LA-MA are introduced. LLA-MA behaves according to the Lamarckian learning model, BLA-MA behaves according to the Baldwinian learning model, and HLA-MA behaves according to both	automata theory;computer engineering;email;genetic algorithm;graph isomorphism;learning automata;link-local address;local search (optimization);mathematical optimization;memetic algorithm;memetics;premature convergence;rate of convergence;shortest path problem;software release life cycle;travelling salesman problem;unreachable memory	Mehdi Rezapoor Mirsaleh;Mohammad Reza Meybodi	2018	Computational Intelligence	10.1111/coin.12148	machine learning;computer science;local search (optimization);memetic algorithm;artificial intelligence;learning automata	AI	27.348955457716617	-1.856929244379785	41617
42f833811fd59b16efa44cfaccb61f0e6bb82f89	fast prediction unit selection and mode selection for hevc intra prediction	h.264/avc;high efficiency video coding (hevc);intra prediction;mode selection;prediction unit size selection;video coding	As a next-generation video compression standard, High Efficiency Video Coding (HEVC) achieves enhanced coding performance relative to prior standards such as H.264/AVC. In the new standard, the improved intra prediction plays an important role in bit rate saving. Meanwhile, it also involves significantly increased complexity, due to the adoption of a highly flexible coding unit structure and a large number of angular prediction modes. In this paper, we present a low-complexity intra prediction algorithm for HEVC. We first propose a fast preprocessing stage based on a simplified cost model. Based on its results, a fast prediction unit selection scheme reduces the number of prediction unit (PU) levels that requires fine processing from 5 to 2. To supply PU size decision with appropriate thresholds, a fast training method is also designed. Still based on the preprocessing results, an efficient mode selection scheme reduces the maximum number of angular modes to evaluate from 35 to 8. This achieves further algorithm acceleration by eliminating the necessity to perform fine Hadamard cost calculation. We also propose a 32 × 32 PU compensation scheme to alleviate the mismatch of cost functions for large transform units, which effectively improves coding performance for high-resolution sequences. In comparison with HM 7.0, the proposed algorithm achieves over 50% complexity reduction in terms of encoding time, with the corresponding bit rate increase lower than 2.0%. Moreover, the achieved complexity reduction is relatively stable and independent to sequence characteristics. key words: intra prediction, high efficiency video coding (HEVC), H.264/AVC, video coding, prediction unit size selection, mode selection	algorithm;analysis of algorithms;angularjs;data compression;fast fourier transform;h.264/mpeg-4 avc;hadamard transform;high efficiency video coding;ibm systems network architecture;image resolution;intra-frame coding;one-class classification;preprocessor;reduction (complexity);teaching method;video coding format	Heming Sun;Dajiang Zhou;Peilin Liu;Satoshi Goto	2014	IEICE Transactions		coding tree unit	AI	46.51251008067189	-19.747671503421824	41646
d3771b95fafbb2494b38a7f29b7ab5f550ced0b3	new frame rate up-conversion algorithms with low computational complexity	new frame rate up conversion algorithms motion estimation h 264 avc hole interpolation interpolated frames blocking artifacts pamc partial average based motion compensation pmos prediction based motion vector smoothing psnr performance peak signal to noise ratio fruc algorithm low computational complexity;motion estimation frame interpolation frame rate up conversion h264 avc high efficiency video coding hevc motion compensation;interpolation;image coding;motion compensation;smoothing methods prediction algorithms computational complexity algorithm design and analysis vectors psnr motion compensation;computational complexity;motion compensation computational complexity image coding interpolation	This paper proposes a new frame rate up-conversion (FRUC) algorithm to reduce the computational complexity and to improve the peak signal-to-noise ratio (PSNR) performance. The proposed FRUC algorithm includes prediction-based motion vector smoothing (PMOS), partial average-based motion compensation (PAMC), and intrapredicted hole interpolation (IPHI). PMVS can efficiently remove outliers using motion vectors of neighboring blocks and PAMC performs motion compensation with the region-based partial average to reduce blocking artifacts of the interpolated frames. For hole interpolation, IPHI uses intraprediction of H.264/AVC to eliminate blurring and also uses the fixed weights implemented using only shift operations, which result in low computational complexity. Compared to the existing algorithms, which use bilateral motion estimation, the proposed algorithm improves the average PSNR of the interpolated frames by 3.44 dB and lowers PSNR performance only by 0.13 dB than the existing algorithm that employs unilateral ME; however, it can significantly reduce the computational complexity of FRUC about 89.3% based on the absolute difference.	algorithm;analysis of algorithms;bilateral filter;blocking (computing);computation;computational complexity theory;decibel;h.264/mpeg-4 avc;interpolation;motion compensation;motion estimation;pmos logic;peak signal-to-noise ratio;pixel;smoothing	Un Seob Kim;Myung Hoon Sunwoo	2014	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2013.2278142	computer vision;mathematical optimization;interpolation;quarter-pixel motion;computer science;theoretical computer science;motion estimation;mathematics;computational complexity theory;motion compensation;algorithm	Vision	45.83662862337915	-17.95763090636766	41647
37a700eb433ad29f262f09537d15a729e6062b59	effects of spatial pattern persistence on the performance of sampling designs for regional trend monitoring analyzed by simulation of space-time fields	sum metric space time variogram;pure panel;supplemented panel;hybrid sampling approach;probability sampling	The effect of the persistence of spatial patterns on the performance of space-time sampling designs is explored by a simulation study. The performance is evaluated on the basis of the covariance matrix of the two parameters (intercept and slope) of a linear model for the change over time of the spatial means or totals. The evaluated sampling approach is hybrid, i.e. design-based estimation of spatial means from spatial probability samples is combined with time-series modelling of the spatial means. A simulation algorithm is presented for approximating the covariance matrix of the time-series model parameters from a full space-time model. Designs were evaluated on the basis of the determinant of this matrix and the variance of the estimated trend parameter. As a space-time model a sum-metric space-time variogram is used, the parameters of which are chosen such that the persistence of spatial patterns varies from nearly absent to very strong. Based on the extensive simulations, recommendations on the type of space-time design can most easily be made for situations with either very strong or no persistence of spatial patterns. With strong persistence the supplemented panel (SuP) design is recommendable. With no persistence the independent-synchronous (IS) and serially alternating (SA) designs are the best choice. These designs performed well with regard to both quality criteria. With moderate persistence of spatial patterns the choice of design type is more complicated. The IS and static-synchronous (SS) design performed best on one quality criterion, but worst on the other. Therefore, with moderate pattern persistence, the compromise designs, either SuP or SA, can be a good choice, unless one of the two quality criteria has priority. An R script is provided for ex ante evaluation of space-time designs in real-world applications. HighlightsSpatial pattern persistence effects quality of trend model for spatial means.With strong and weak persistence there is clearly a best design.Supplemented panel is best choice with strong persistence.With moderate persistence choice of quality criterion determines which design is best.R script is provided for ex ante evaluation of space-time designs	persistence (computer science);sampling (signal processing);simulation;spatiotemporal pattern	D. J. Brus;J. J. de Gruijter	2013	Computers & Geosciences	10.1016/j.cageo.2013.09.001	econometrics;simulation;mathematics;statistics	EDA	27.18945145864511	-22.062190914919615	41659
22685e4c332cfc8006f785038f3a789e1306c05e	citation age data and the obsolescence function: fits and explanations	test kolmogorov smirnov;infometrie;citation analysis;informetrics;envejecimiento;journal contribution;kolmogorov smirnov test;obsolescencia;lognormal distribution;analisis cita;age;loi lognormale;citation;ley weibull;distribucion estadistica;litterature scientifique;scholarly journals;weibull distribution;analyse citation;statistical distributions;ley lognormal;distribution statistique;mathematical formulas;literatura cientifica;tables data;ageing;vieillissement;obsolescence;citacion;infometria;scientific literature;loi weibull;statistical distribution;modele avramescu;citations references;edad	The paper deals with the shape of the obsolescence function, which one can construct, based on the age data of reference lists. This paper shows that the obsolescence factor (aging factor) a is not a constant but merely a function of time. This jeopardizes this factor as a useful measure. We show (by experiment and also mathematically) that the function a has a minimum, which is obtained at a time t later than the time at which the maximum of the number of citations is reached. We then fit sets of citation data by using the lognormal distribution (other distributions do not fit well). Analytical calculations with this function a are indeed valid for these data. These arguments also yield a description of the utility function 1( and the total utility U. The latter can be used in comparing the “total lives” of the literature in various subjects.	c date and time functions;experiment;fits;reference implementation;utility	Leo Egghe;I. K. Ravichandra Rao	1992	Inf. Process. Manage.	10.1016/0306-4573(92)90046-3	ageing;probability distribution;demography;citation analysis;statistics	ML	34.90657917989202	-20.19246671839815	41678
c598222ccb66a0db65712566c52314556f961adc	an energy-scalable margin propagation-based analog vlsi support vector machine	cmos integrated circuits;degradation;support vector machines;software prototyping;mosfets;floating gate transistors;floating gate;very large scale integration;prototypes;very large scale integration support vector machines mosfets support vector machine classification degradation software prototyping prototypes cmos process circuit simulation power dissipation;0 5 micron;cmos process;support vector;chip;feature vector;circuit simulation;translinear response;power dissipation;mos transistors;vlsi;analog vlsi;circuit level simulations;vlsi circuit simulation cmos integrated circuits mosfet support vector machines;support vector machine classification;mosfet;equivalent software;support vector machine;energy scalable margin propagation;0 5 micron energy scalable margin propagation analog vlsi support vector machine translinear response mos transistors cmos process floating gate transistors circuit level simulations equivalent software	This paper presents a novel approach for designing energy-scalable analog VLSI recognizers. Unlike conventional designs that rely on the translinear response of MOS transistors biased in weak inversion, the proposed approach uses margin propagation, enabling system operation independent of MOS transistor biasing conditions. In this paper margin propagation has been used for designing energy-scalable support vector machines (SVM) whose power and speed requirements can be configured dynamically without any degradation in performance. A prototype SVM operating with 14 dimensional feature vectors and 28 support vectors has been designed and fabricated in a 0.5mum CMOS process. The chip integrates an array of floating gate transistors that serve as storage for SVM parameters. Circuit level simulations demonstrate near identical performance to an equivalent software-based SVM with power dissipation less than 1muW at a rate of 100 classifications per second.	algorithm;analog-to-digital converter;array data structure;biasing;cmos;central processing unit;elegant degradation;feature vector;finite-state machine;interval propagation;prototype;rectifier;requirement;scalability;simulation;software propagation;support vector machine;transistor;very-large-scale integration	Paul Kucher;Shantanu Chakrabartty	2007	2007 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2007.378407	embedded system;support vector machine;electronic engineering;computer science;engineering;electrical engineering;machine learning	Arch	38.51554936158977	-1.9536405743863805	41763
90ae1c09276f10c4f7e20e0e75321a1a4b470f82	a new particle swarm optimisation based on matlab for portfolio selection problem	ajustamiento modelo;selection problem;model selection;swarm intelligence;tratamiento transaccion;problema seleccion;improved particle swarm optimisation;algoritmo busqueda;intelligence en essaim;algorithme recherche;portfolio selection;heuristic method;search algorithm;seleccion cartera;metodo heuristico;intelligence artificielle;multidimensional analysis;selection modele;selection portefeuille;ajustement modele;analyse n dimensionnelle;seleccion modelo;model matching;analisis n dimensional;portfolio management;artificial intelligence;algorithme evolutionniste;algoritmo evolucionista;gestion cartera;methode heuristique;inteligencia artificial;evolutionary algorithm;reseau neuronal;gestion portefeuille;transaction processing;inteligencia de enjambre;improved pso;particle swarm optimisation;red neuronal;ipso;traitement transaction;neural network;probleme selection	This paper focuses on the constrained portfolio selection problem and develops an improved particle swarm optimisation (IPSO) algorithm to solve it. As an alternative and extension to the standard Markowitz model, a constrained portfolio selection model with transaction costs and quantity limit is formulated for selecting portfolios. Due to these complex constraints, the process becomes a high-dimensional constrained optimisation problem. Traditional optimisation algorithms fail to work efficiently and heuristic algorithms with effective searching ability can be the best choice for the problem, so we design an IPSO to solve our problem. In order to prevent premature convergence to local minima, we design a new definition for global point. Finally, a numerical example of a portfolio selection problem is given to illustrate our proposed method; the simulation results demonstrate good performance of the IPSO in solving the complex constrained portfolio selection problem.	matlab;mathematical optimization;particle swarm optimization;selection algorithm	Jianwei Gao;Zhonghua Chu	2010	IJMIC	10.1504/IJMIC.2010.032380	multidimensional analysis;mathematical optimization;transaction processing;swarm intelligence;computer science;engineering;artificial intelligence;evolutionary algorithm;operations research;artificial neural network;model selection;search algorithm	ML	30.141078242320646	-10.932721811928884	41764
a1953e64ee8aa42c3034f9f73d4b30f564f84cab	lossless image coding by cellular neural networks with backward error propagation learning	image coding;learning artificial intelligence cellular neural nets image coding;cellular neural nets;image coding standards context context modeling entropy coding prediction algorithms;learning artificial intelligence;grayscale test images cellular neural networks backward error propagation learning novel hierarchical lossless image coding scheme cnn entropy coding predict coding split coding adaptive arithmetic coder minimum coding rate learning	This paper proposes a novel hierarchical lossless image coding scheme using cellular neural network (CNN). The coding architecture of proposed method is composed of three steps: split, predict, and entropy coding. The coding performance of proposed method highly depends on that of CNN predictors. The resulting prediction errors are encoded by the adaptive arithmetic coder. To achieve the high coding efficiency, the type of space-variant CNN templates and their parameters are optimized to minimize the actual coding bits of prediction residuals by the minimum coding rate learning with backward error propagation. Experimental results in 21 kinds of standard grayscale test images show that the average coding rates of the proposed scheme is better than that of the conventional schemes.	algorithmic efficiency;arithmetic coding;artificial neural network;branch predictor;cellular neural network;entropy encoding;grayscale;jpeg;kerrison predictor;lossless compression;propagation of uncertainty;scalability;software propagation	Keisuke Takizawa;Seiya Takenouchi;Hisashi Aomori;Tsuyoshi Otake;Mamoru Tanaka;Ichiro Matsuda;Susumu Itoh	2012	The 2012 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2012.6252404	computer vision;linear network coding;shannon–fano coding;vector sum excited linear prediction;harmonic vector excitation coding;variable-length code;computer science;entropy encoding;artificial intelligence;theoretical computer science;context-adaptive variable-length coding;machine learning;coding tree unit;tunstall coding;context-adaptive binary arithmetic coding;huffman coding	Robotics	45.348023000045266	-16.303661431378814	41775
687ede3b94bfc503f9400da271affff753ac3c6b	a hybrid particle swarm algorithm for function optimization	particle swarm;biology computing;swarm intelligence;particle swarm optimisation biology computing evolutionary computation learning artificial intelligence;evolutionary computation;learning experience;data mining;function optimization;optimization problem;particle swarm optimizer;particle experience;particle experience hybrid particle swarm optimization function optimization evolutionary techniques swarm intelligence learning strategy;learning strategy;particle swarm optimization;mathematical model;optimization;evolutionary techniques;learning artificial intelligence;particle swarm optimisation;learning strategies;particle swarm optimization equations benchmark testing random number generation genetic mutations performance evaluation evolutionary computation birds marine animals educational institutions;benchmark testing;hybrid particle swarm optimization	Particle swarm optimization (PSO) is one of the evolutionary techniques based on swarm intelligence, which has show good performance in many optimization problems. This paper proposes a new learning strategy to help particles learn experiences from other previous best particles. In order to verify the proposed approach (HPSO), this paper investigates the effects of learning factor on six well-known benchmark functions. Additionally, comparison of HPSO with standard PSO and comprehensive learning PSO shows that HPSO outperforms them on most test functions. Keywords-particle swarm optimization; learning strategy; function optimization	algorithm;benchmark (computing);distribution (mathematics);mathematical optimization;particle swarm optimization;swarm intelligence	Jie Yang;Jiahua Xie	2009	2009 2nd International Conference on Biomedical Engineering and Informatics	10.1109/BMEI.2009.5305534	mathematical optimization;multi-swarm optimization;meta-optimization;swarm intelligence;computer science;derivative-free optimization;artificial intelligence;firefly algorithm;machine learning;imperialist competitive algorithm;particle swarm optimization;metaheuristic;evolutionary computation	Robotics	27.5002967251386	-5.5214182765616915	41813
207bcf05ace997a46d308bd67a2f573b52cef18b	analysis of packet loss for compressed video: effect of burst losses and correlation between error frames	modelizacion;theorie vitesse distorsion;data transmission;evaluation performance;video compression propagation losses predictive models decoding internet automatic voltage control streaming media performance loss resilience rate distortion;estimation theory;rate distortion;effet special;optimisation;propagation losses;streaming;low bit rate;video streaming;image coding;performance evaluation;image processing;data compression;optimizacion;decoding;video signal processing;debit information;videoconference;h 264 avc coded video;information transmission;packet loss;evaluacion prestacion;h 264 avc;erreur quadratique moyenne;video compression;video streaming packet loss video compression burst losses video communication internet mean squared error distortion estimation inter frame error propagation h 264 avc coded video video telephony;procesamiento imagen;packet switching;video telephony;conmutacion por paquete;indice informacion;perdida transmision;traitement image;data communication;telephony;special effect;inter frame error propagation;velocidad de bit debil;packet loss rate;perte transmission;propagation erreur;rate distortion optimization distortion modeling error propagation error resilience h 264 avc packet loss;rate distortion theory;error re silience;modelisation;codage image;video coding;accuracy;transmission en continu;model error;compression image;precision;automatic voltage control;internet;senal video;signal video;resilience;image compression;codage video;error propagation;streaming media;scheduling;mean square error;transmission donnee;distortion modeling;transmission loss;traitement signal video;information rate;video communication data communication estimation theory internet telephony video coding;video signal;error resilience;predictive models;burst losses;videoconferencia;optimization;mean squared error distortion estimation;compresion dato;transmision fluyente;transmision informacion;propagacion error;error medio cuadratico;growth of error	Video communication is often afflicted by various forms of losses, such as packet loss over the Internet. This paper examines the question of whether the packet loss pattern, and in particular, the burst length, is important for accurately estimating the expected mean-squared error distortion resulting from packet loss of compressed video. We focus on the challenging case of low-bit-rate video where each P-frame typically fits within a single packet. Specifically, we: 1) verify that the loss pattern does have a significant effect on the resulting distortion; 2) explain why a loss pattern, for example a burst loss, generally produces a larger distortion than an equal number of isolated losses; and 3) propose a model that accurately estimates the expected distortion by explicitly accounting for the loss pattern, inter-frame error propagation, and the correlation between error frames. The accuracy of the proposed model is validated with H.264/AVC coded video and previous frame concealment, where for most sequences the total distortion is predicted to within plusmn0.3 dB for burst loss of length two packets, as compared to prior models which underestimate the distortion by about 1.5 dB. Furthermore, as the burst length increases, our prediction is within plusmn0.7 dB, while prior models degrade and underestimate the distortion by over 3 dB. The proposed model works well for video-telephony-type of sequences with low to medium motion. We also present a simple illustrative example, of how knowledge of the effect of burst loss can be used to adapt the schedule of video streaming to provide improved performance for a burst loss channel, without requiring an increase in bit rate.	additive model;can bus;cognitive dimensions of notations;data compression;digital video;experiment;fits;h.264/mpeg-4 avc;internet;linear system;mean squared error;network packet;propagation of uncertainty;software propagation;streaming media;total distortion;videotelephony	Yi J. Liang;John G. Apostolopoulos;Bernd Girod	2008	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2008.923139	data compression;real-time computing;telecommunications;image processing;computer science;accuracy and precision;statistics	Vision	48.342133295105505	-15.050880859826856	41853
af39dd67252544d058dd6c2fbc02360042ad422d	mathltwa: multiple lapse time window analysis using wolfram mathematica 7	computadora;tratamiento datos;computers;replique sismique;uttar pradesh;sismo;uttar pradesh india;ordinateur;seisme;time window;seisme chamoli 1999;data processing;traitement donnee;attenuation;earthquakes;windows;modelo;asie;mltwa;fenetre;atenuacion;aftershocks;replica sismica;timing analysis;ventana;modele;mathematica7;seismic attenuation and scattering;models;india;asia;inde	The MATHCAD 2000 professional code to perform the Multiple Lapse Time Analysis (MLTWA) has been revised and rewritten in MATHEMATICA 7. The new code contains two new procedures to find the minimum of the misfit function between observation and model and a new example of application to real data from Chamoli earthquake aftershock sequence. & 2010 Elsevier Ltd. All rights reserved.	mathcad;wolfram mathematica	Edoardo Del Pezzo;Francesca Bianco	2010	Computers & Geosciences	10.1016/j.cageo.2010.03.017	attenuation;seismology;data processing;computer science;static timing analysis;aftershock	AI	38.194857855730135	-21.56282214157558	41914
ef127bba326b0d6d7661c0bbed0e9b977461bafd	fast inter-prediction mode decision algorithm for hevc		This paper proposes a novel fast intra-prediction algorithm that exploits the Sobel operator to replace the Hadamard transform used in the Rough Mode Decision (RMD) process of intra prediction in High Efficiency Video Coding (HEVC). First, the Sobel operator is used to calculate the vector direction of each pixel. A judgment is then made on which prediction mode the vector belongs to, and a histogram is applied in the scheme to generate the statistics of prediction modes for each prediction unit. Finally, the prediction mode candidates are placed in the candidate list for the rate-distortion optimization process. Experimental results show that our proposed algorithm for RMD significantly reduces the complexity of the encoder with an acceptable degradation of quality and BD-rate compared with HM7.0.	algorithm;blu-ray;distortion;elegant degradation;encoder;hadamard transform;high efficiency video coding;international standard serial number;intra-frame coding;mathematical optimization;pixel;rate–distortion optimization;remote data objects;sherlock holmes: consulting detective vol. ii;sobel operator	Xinpeng Huang;Qiuwen Zhang;Xiaoxin Zhao;Weiwei Zhang;Yan Zhang;Yong Gan	2017	Signal, Image and Video Processing	10.1007/s11760-016-0887-4	real-time computing;computer science;theoretical computer science	AI	47.0644321755154	-19.470521017206575	41943
52dc73736b56891b58fa368aa97b1e3b89d931d5	evaluation methodology and control strategies for improving reliability of hev power electronic system	reliability batteries vehicles inverters traction motors insulated gate bipolar transistors stress;reliability;voltage control hybrid electric vehicles pwm power convertors reliability thermal stresses;electric drivetrain hev power electronic system reliability prediction hybrid electric vehicles vehicle operation management vehicle control strategy vehicle design vehicle planning objective criterion mission profile dependent simulation model matlab thermal stresses variable driving scenarios dormant mode electrical stresses reliability assessment mean time to failure hev powertrains variable dc link voltage control hybrid discontinuous pulsewidth modulation scheme power losses power converters numerical simulation;simulation;discontinuous modulation;hybrid electric vehicle hev;power trains;powertrain;electric vehicles;matlab computer program;failure rate;methodology;hybrid vehicles	The reliability prediction of hybrid electric vehicles (HEVs) is of paramount importance for planning, design, control, and operation management of vehicles, since it can provide an objective criterion for comparative evaluation of various configurations and topologies and can be used as an effective tool to improve the design and control of the overall system. This paper presents a mission-profile-dependent simulation model based on MATLAB for quantitatively assessing the reliability of the electric drivetrain of HEVs. This model takes into consideration the variable driving scenarios, dormant mode, electrical stresses, and thermal stresses. Therefore, more reliable and accurate prediction of system reliability has been achieved. The methodology is explained in detail, and the results of reliability assessment based on a series HEV are presented. Based on reliability analysis, two control strategies are proposed to increase the mean time to failure of HEV powertrains: 1) variable dc-link voltage control and 2) hybrid discontinuous pulsewidth modulation scheme. These novel control schemes reduce the power losses and thermal stresses of power converters, and consequently, enhance system reliability. Numerical simulation results verify the benefits of two proposed control strategies in terms of power losses and reliability.	matlab;mean time between failures;pulse-width modulation;semiconductor device;simulation;switched-mode power supply	Yantao Song;Bingsen Wang	2014	IEEE Transactions on Vehicular Technology	10.1109/TVT.2014.2306093	control engineering;electronic engineering;powertrain;engineering;failure rate;automotive engineering;methodology;reliability;statistics	Robotics	36.001660432735164	-3.3633106847287055	41956
f917f35fee65c163952c8be4c18df1eb2b4aa603	runtime analysis of a binary particle swarm optimizer	runtime analysis;upper bound;particle swarm optimizer;particle swarm optimization;evolutionary algorithm;pseudo boolean;lower bound	We investigate the runtime of a Binary Particle Swarm Optimizer (PSO) for optimizing pseudo-Boolean functions f : {0, 1}n → R. The Binary PSO maintains a swarm of particles searching for good solutions. Each particle consists of a current position from {0, 1}n, an own best position and a velocity vector used in a probabilistic process to update its current position. The velocities for a particle are then updated in the direction of its own best position and the position of the best particle in the swarm. We present a lower bound for the time needed to optimize any function with unique optimum. To prove upper bounds, we transfer a fitness-level argument well-established for evolutionary algorithms (EAs) to PSO. This method is applied to estimate the expected runtime on the class of unimodal functions. A simple variant of the Binary PSO is considered in more detail on the test function OneMax, showing that there the Binary PSO is competitive to EAs. An additional experimental comparison reveals further insights.	distribution (mathematics);evolutionary algorithm;global optimization;like button;mathematical optimization;particle swarm optimization;phase-shift oscillator;runtime system;russell c. eberhart;velocity (software development)	Dirk Sudholt;Carsten Witt	2010	Theor. Comput. Sci.	10.1016/j.tcs.2010.03.002	mathematical optimization;multi-swarm optimization;computer science;artificial intelligence;machine learning;evolutionary algorithm;mathematics;upper and lower bounds;particle swarm optimization	ML	27.71554354666006	-3.548103494035269	42082
b62c19bd817c5432af7473e57510c5213854988a	reference line guided pareto local search for bi-objective traveling salesman problem		In this paper, a reference line guided Pareto local search (RLG-PLS) is proposed for combinatorial bi-objective optimization problems (CBOPs). RLG-PLS uses a set of predefined reference lines to guide the search direction and maintain the diversity of the population. Two populations are evolving in RLG-PLS, i.e., 1) the external population (EP) maintains the nondominated solutions that are closest to the reference lines; and 2) a starting population (SP) stores all the starting solutions for Pareto local search. At each generation, Pareto local search is applied to search the neighborhood of each solution in SP and these neighborhood solutions are also used to update EP and then, SP is updated with the newly added solutions from EP. When no nondominated solutions can be found (i.e., SP is empty), new reference lines are inserted to guide the Pareto local search for more new nondominated solutions. In the experimental studies, RLG-PLS is compared with MOEA/D-LS (WS, TCH, PBI), NSGA-II-LS and MOMAD on bi-objective travelling salesman problem (BOTSP). The experimental results show that RLG-PLS outperforms all the compared algorithms.	algorithm;combinatorial optimization;contour line;effective method;expectation propagation;local search (optimization);moea framework;mathematical optimization;multi-objective optimization;pls (complexity);pareto efficiency;partial least squares regression;population;travelling salesman problem	Chao Xia;Xinye Cai;Zhun Fan;Muhammad Sulaman	2017	22017 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC)	10.1109/CSE-EUC.2017.20	guided local search;travelling salesman problem;pareto principle;local search (optimization);mathematical optimization;population;computer science;optimization problem	Robotics	25.775794259298436	-1.7271295072051376	42243
8932d4760728228255f17e12b7b213a78913b723	encoder-driven rate control and mode decision for distributed video coding	signal image and speech processing;quantum information technology spintronics	To provide low-complexity encoding for video in unidirectional or offline compression scenarios, this paper proposes an efficient feedback-channel-free distributed video coding architecture featuring a novel encoder-driven rate control scheme in tandem with a designated mode selection process. To this end, the encoder features a novel low-complexity motion estimation technique to approximate the side-information (SI) available at the decoder. Then, a SI-dependent correlation channel estimation between the approximated SI and the original frames is used to derive the theoretically required rate for successful Slepian-Wolf (SW) decoding. Based on the evaluation of the expected trade-off between the estimated required coding rate and the estimated distortion outcome, a novel encoder-side mode decision module assigns a different coding mode to distinct portions of the coded frames. In this context, skip, intra and SW coding modes are supported. To reduce the effect of underestimation, the final SW rate is adjusted upwards using a novel rate formula. Additionally, a successive SI refinement technique is exploited at the decoder to decrease the number of SW decoding failures. Experimental results illustrate the benefit of the different coding options and show similar or superior compression performance with respect to the feedback-based DISCOVER benchmark system. Finally, the low-complexity encoding characteristics of the proposed system are confirmed, as well as the beneficial impact of the proposed scheme on the decoding complexity.	approximation algorithm;benchmark (computing);channel state information;data compression;distortion;encoder;motion estimation;online and offline;refinement (computing);shattered world;slepian–wolf coding	Frederik Verbist;Nikos Deligiannis;Shahid M. Satti;Peter Schelkens;Adrian Munteanu	2013	EURASIP J. Adv. Sig. Proc.	10.1186/1687-6180-2013-156	real-time computing;telecommunications;computer science;theoretical computer science;context-adaptive binary arithmetic coding	ML	47.91628039935955	-18.419172301195548	42249
72a5de7d96124bfd9d2713e0318bd62e2267aacb	selecting an adaptive sequence for computing recursive m-estimators in multivariate linear regression models	miao baiqi tong qian wu yuehua jin baisuo 多元线性回归模型 自适应 m估计 计算 序列 递推 递归算法 算法选择 selecting an adaptive sequence for computing recursive m estimators in multivariate linear regression models	In this paper, the authors consider an adaptive recursive algorithm by selecting an adaptive sequence for computing M-estimators in multivariate linear regression models. Its asymptotic property is investigated. The recursive algorithm given by Miao and Wu (1996) is modified accordingly. Simulation studies of the algorithm is also provided. In addition, the Newton-Raphson iterative algorithm is considered for the purpose of comparison.	algorithm;converge;general linear model;iterative method;newton's method;recursion (computer science);recursive acronym;simulation;strong consistency	Baiqi Miao;Qian Tong;Yuehua Wu;Baisuo Jin	2013	J. Systems Science & Complexity	10.1007/s11424-013-1113-x	econometrics;multivariate adaptive regression splines;computer science;mathematics;algorithm;statistics	HPC	29.384067363219017	-22.970756625505963	42269
7167d4817cb06bb87cd67c4dc0df4d2ae6ba4f80	optimal viterbi based total variation sequence detection (tvsd) for robust image/video decoding in wireless sensor networks	image coding;combined source channel coding;viterbi detection;wireless sensor networks combined source channel coding image coding image reconstruction maximum likelihood decoding viterbi decoding viterbi detection;jpeg encoded image transmission multimedia wireless sensor networks optimal viterbi detection total variation sequence detection tvsd robust image video decoding robust reconstruction total variation regularization image video communication optimal joint source channel decoder maximum likelihood cost function anisotropic total variation norm regularization factor bounded variation trellis based viterbi decoder robust image reconstruction btc encoded image transmission;wireless sensor networks image reconstruction wireless communication tv decoding vectors streaming media;image reconstruction;maximum likelihood decoding;viterbi decoding;wireless sensor networks;wireless sensor networks image video reconstruction total variation regularization viterbi decoder	In this letter, we propose a novel scheme for robust reconstruction, based on total variation regularization, towards image/video communication in multimedia wireless sensor networks. We derive the optimal joint source channel decoder as the combination of a maximum likelihood cost function and an anisotropic total variation norm based regularization factor. The proposed scheme exploits the bounded variation (BV) property of images and is thus ideally suited for reconstruction. Subsequently, it is demonstrated that the trellis based Viterbi decoder can be employed for robust image reconstruction using modified total variation state and branch metrics. Simulation results for BTC and JPEG encoded image transmission demonstrate a superior reconstruction performance for the proposed scheme in comparison to conventional methods.	block truncation coding;bounded variation;image quality;iterative reconstruction;jpeg;loss function;norm (social);peak signal-to-noise ratio;simulation;streaming media;total variation denoising;trellis quantization;video decoder;viterbi algorithm;viterbi decoder	Ankit Kudeshia;Aditya K. Jagannatham	2014	IEEE Signal Processing Letters	10.1109/LSP.2014.2313887	iterative reconstruction;soft output viterbi algorithm;wireless sensor network;telecommunications;viterbi algorithm;computer science;theoretical computer science;mathematics;viterbi decoder;iterative viterbi decoding;statistics	Vision	49.19020523175005	-16.87901278899826	42320
95eecfd28ea0f3441d838a50c5bfbedd3550070a	particle swarm optimization: hybridization perspectives and experimental illustrations	equation non lineaire;calculo de variaciones;65h20;ecuacion no lineal;global solution;analisis numerico;differential evolution;ecuacion trascendente;matematicas aplicadas;mathematiques appliquees;optimization technique;benchmark problem;ecuacion algebraica;65kxx;search method;equation transcendante;optimization method;etude methode;estudio metodo;algoritmo genetico;65k10;classification;metodo optimizacion;analyse numerique;49xx;resolucion problema;calcul variationnel;numerical analysis;particle swarm optimizer;or algorithm;mathematical programming;transcendental equation;particle swarm optimization;hybridization;algorithme qr;methode optimisation;algorithme genetique;genetic algorithm;equation algebrique;genetic algorithms;solution globale;method study;non linear equation;applied mathematics;optimal algorithm;programmation mathematique;algebraic equation;solucion global;programacion matematica;clasificacion;variational calculus;problem solving;resolution probleme;hybrid genetic algorithm	Metaheuristic optimization algorithms have become popular choice for solving complex and intricate problems which are otherwise difficult to solve by traditional methods. In the present study an attempt is made to review the hybrid optimization techniques in which one main algorithm is a well known metaheuristic; particle swarm optimization or PSO. Hybridization is a method of combining two (or more) techniques in a judicious manner such that the resulting algorithm contains the positive features of both (or all) the algorithms. Depending on the algorithm/s used we made three classifications as (i) Hybridization of PSO and genetic algorithms (ii) Hybridization of PSO with differential evolution and (iii) Hybridization of PSO with other techniques. Where, other techniques include various local and global search methods. Besides giving the review we also show a comparison of three hybrid PSO algorithms; hybrid differential evolution particle swarm optimization (DE-PSO), adaptive mutation particle swarm optimization (AMPSO) and hybrid genetic algorithm particle swarm optimization (GA-PSO) on a test suite of nine conventional benchmark problems. 2010 Elsevier Inc. All rights reserved.	benchmark (computing);differential evolution;evolutionary algorithm;genetic algorithm;mathematical optimization;memetic algorithm;metaheuristic;optimization problem;particle swarm optimization;test suite	Radha Thangaraj;Millie Pant;Ajith Abraham;Pascal Bouvry	2011	Applied Mathematics and Computation	10.1016/j.amc.2010.12.053	mathematical optimization;multi-swarm optimization;meta-optimization;genetic algorithm;parallel metaheuristic;derivative-free optimization;firefly algorithm;calculus;mathematics;imperialist competitive algorithm;particle swarm optimization;algorithm;metaheuristic;algebra	AI	27.743653250206947	1.1922388620061837	42421
2b7898989e8ae3d4d8fa25a1b07274396e9ab897	varying quality function in genetic algorithms and the cutting problem	optimisation;simulation results dynamic quality function genetic algorithms cutting problem search evolution continuously changing search space;genetic algorithms waste materials shape measurement testing space exploration traveling salesman problems needles genetic mutations area measurement;waste materials;search space;dynamic quality function;space exploration;testing;shape measurement;continuously changing search space;search evolution;search problems genetic algorithms optimisation;cutting problem;area measurement;genetic algorithm;genetic algorithms;search problems;traveling salesman problems;genetic mutations;simulation results;needles	In this paper, an implementation of a genetic algorithm (GA) is presented, using a quality function that is not unaltered but changes according to the search evolution. This means that the GA 'sees' a continuously changing search space, throughout one run. The example chosen to test the effect of a varying quality function is the cutting problem. Simulation results show that the dynamic quality function performs much better than its static counterpart. >	genetic algorithm	Vassilios Petridis;Spiridon A. Kazarlis	1994		10.1109/ICEC.1994.350022	mathematical optimization;simulation;genetic algorithm;computer science;artificial intelligence;machine learning	Theory	28.13458878609741	-6.027034332133804	42462
178e2da95bdd9e3c4c196146d1244a69a33d7c77	adaptive weighted prediction in video coding	prediction algorithms;layout;motion estimation;histograms;brightness;coding efficiency	An adaptive weighted prediction algorithm is proposed to improve the coding efficiency for video scenes that contain global brightness variations caused by fade in/out or local brightness variations caused by camera flashes. A two brightness-variation parameters model is used, which represent multiplier and offset components of the brightness variation. Brightness variations at frame and macroblock level, respectively, are detected; if there are brightness variations, we use weighted prediction to compensate these brightness variations; otherwise the conventional prediction method is used. Simulation results show that the proposed algorithm can improve the coding efficiency sufficiently when the coded scenes contain brightness variations. This technology is adopted by AVS.	advanced visualization studio;algorithm;algorithmic efficiency;data compression;macroblock;simulation	Yanfei Shen;Dongming Zhang;Chao Huang;Jintao Li	2004	2004 IEEE International Conference on Multimedia and Expo (ICME) (IEEE Cat. No.04TH8763)		layout;computer vision;simulation;prediction;computer science;motion estimation;histogram;algorithmic efficiency;brightness;statistics;computer graphics (images)	Vision	44.54516183809179	-18.23982088159232	42541
90b1d64413f37daca6e0cda3f971a0e35a60cfbf	computing the distribution function of a conditional expectation via monte carlo: discrete conditioning spaces	rate of convergence;distributed computing distribution functions monte carlo methods portfolios uncertainty mean square error methods sampling methods convergence reactive power finance;asymptotic optimality;optimal rate of convergence;convergence of numerical methods;value at risk;conditional expectation;confidence interval;distribution function;statistical analysis;central limit theorem;stochastic processes;stochastic processes monte carlo methods statistical analysis digital simulation convergence of numerical methods;mean square error;numerical computation;performance analysis;mathematical finance;monte carlo;monte carlo simulation;bayesian performance analysis distribution function conditional expectation discrete conditioning spaces conditional expectations conditioning element countably infinite outcome space monte carlo simulation computer budget mean square error budget allocation conditioning variable asymptotically optimal rates central limit theorems confidence intervals value at risk mathematical finance;monte carlo methods;digital simulation	We examine different ways of numerically computing the distribution function of conditional expectations where the conditioning element takes values in a finite or countably infinite outcome space. Both the conditional expectation and the distribution function itself are computed via Monte Carlo simulation. Given a limited (and fixed) computer budget, the quality of the estimator is gauged by the inverse of its mean square error. It is a function of the fraction of the budget allocated to estimating the conditional expectation versus the amount of sampling done relative to the “conditioning variable.” We will present the asymptotically optimal rates of convergence for different estimators and resolve the trade-off between the bias and variance of the estimators. Moreover, central limit theorems are established for some of the estimators proposed. We will also provide algorithms for the practical implementation of two of the estimators and illustrate how confidence intervals can be formed in each case. Major potential application areas include calculation of Value at Risk (VaR) in the field of mathematical finance and Bayesian performance analysis.	asymptotically optimal algorithm;bayesian network;mean squared error;monte carlo method;numerical analysis;sampling (signal processing);simulation;value at risk	Shing-Hoi Lee;Peter W. Glynn	1999		10.1145/324898.325356	conditional probability distribution;stochastic process;econometrics;mathematical optimization;conditional variance;mathematics;mathematical finance;statistics;monte carlo method	ML	31.117638573684246	-17.624369376317407	42650
6fdd300dbed539d1b659092b3bd5732832652197	keep–best reproduction: a local family competition selection strategy and the environment it flourishes in	optimisation sous contrainte;constrained optimization;evaluation performance;replacement;remplacement;performance evaluation;relacion orden;evaluacion prestacion;population size;ordering;calcul evolutionniste;algoritmo genetico;constraint satisfaction;optimizacion con restriccion;empirical evidence;search;satisfaction contrainte;relation ordre;recombinacion portador carga;constrained ordering problems;algorithme genetique;genetic algorithm;recombinaison porteur charge;genetic algorithms;reemplazo;satisfaccion restriccion;charge carrier recombination;selection strategies;evolutionary computing	This paper presents a comparison of two genetic algorithms (GAs) for constrained ordering problems. The first GA uses the standard selection strategy of roulette wheel selection and generational replacement (STDS), while the second GA uses an intermediate selection strategy in addition to STDS. This intermediate selection strategy keeps only the superior offspring and replaces the inferior offspring with the superior parent. We call this selection strategy Keep–Best Reproduction (KBR). The effect of recombination alone, mutation alone and both together are studied. We compare the performance of the different selection strategies and discuss the environment that each selection strategy needs to flourish in. Overall, KBR is found to be the selection strategy of choice. We also present empirical evidence that suggests that KBR is more robust than STDS with regard to operator probabilities and works well with smaller population sizes.	crossover (genetic algorithm);fitness proportionate selection;genetic algorithm;key-based routing;selection bias;software release life cycle	Kay C. Wiese;Scott D. Goodwin	2001	Constraints	10.1023/A:1011409029226	mathematical optimization;constrained optimization;genetic algorithm;computer science;artificial intelligence;operations research	HCI	26.58648705259415	0.9816129574762674	42752
411351a200afe87ebe2d420de5da991b9f852d5c	heteroscedasticity and distributional assumptions in the censored regression model	censored regression;non normality;partially adaptive estimators;tobit;heteroskedasticity;62j99;nonnormality	Heteroscedasticity and Distributional Assumptions in the Censored Regression Model James B. McDonald & Hieu Nguyen To cite this article: James B. McDonald & Hieu Nguyen (2015) Heteroscedasticity and Distributional Assumptions in the Censored Regression Model, Communications in Statistics Simulation and Computation, 44:8, 2151-2168, DOI: 10.1080/03610918.2013.851217 To link to this article: http://dx.doi.org/10.1080/03610918.2013.851217	artificial intelligence: a modern approach;boyce–codd normal form;censored regression model;censoring (statistics);communications in statistics – simulation and computation;granular computing;least absolute deviations;mason;management science;monte carlo method;nl (complexity);numerical aperture;ordinary least squares;p (complexity);performance rating;powell's method;principle of maximum entropy;south central library system;tobit model	James B. McDonald;Hieu Nguyen	2015	Communications in Statistics - Simulation and Computation	10.1080/03610918.2013.851217	econometrics;tobit model;mathematics;heteroscedasticity;censored regression model;statistics	NLP	28.138334670307355	-22.643850314213	42782
9f982482fb01bb1b2039a7765b23db8841e13210	data hiding using vq index file	computers;binary secret data hiding;quantization;data hiding;private key cryptography;art;image coding;standards;image processing;compression algorithms;host image recovery;gain;resistance;government;data hiding method;image restoration;materials;gray scale;system on a chip;search order coding;data encapsulation;computer vision;computer security;media;image vector quantization index file;indexes;internet;vector quantisation data encapsulation image coding image restoration private key cryptography;vector quantization;lead;image color analysis;cryptography;search order coding technique;indexation;web sites;proceedings paper;pattern recognition;mutual information;artificial intelligence;genetic algorithms;vector quantizer;binary data;computer science;search order coding data hiding vector quantization;correlation;data encapsulation image coding cryptography internet vector quantization discussion forums protection risk analysis artificial intelligence pattern analysis;vector quantisation;reviews;security;encoding;information theory;decompression procedure;boats;decompression procedure data hiding method image vector quantization index file search order coding technique binary secret data hiding host image recovery;genetic communication;knowledge engineering	This paper modifies an elegant data hiding method of Chang et alpsilas [Pattern Recog. Lett. 2004, vol. 25, no. 11, 1253-1261]. The embedding employs a modified search-order coding (SOC) technique to hide binary secret data in the vector quantization (VQ) index file of a host image. The recovered host image is identical to the one recovered by traditional VQ; the compression rate is competitive to traditional VQpsilas (often better than traditional VQpsilas if we use t = 1 mode); while the hidden binary data is revealed in the decompression procedure as a bonus. The compression rate and hiding capacity are also better than those in Chang et alpsilas.	binary data;data compression;database index;vector quantization	Yu-Jie Chang;Ja-Chen Lin	2008	2008 IEEE International Conference on Intelligence and Security Informatics	10.1109/ISI.2008.4565064	system on a chip;image restoration;lead;media;quantization;image processing;information theory;gain;computer science;cryptography;information security;theoretical computer science;knowledge engineering;data mining;mutual information;resistance;world wide web;correlation;vector quantization;government;encoding;statistics	Robotics	41.65243429559468	-11.741247991520035	42790
47b1ec754f4466518b4427858f490e337ed41419	applying population based aco to dynamic optimization problems	dynamic programming;utilisation information;ajustamiento modelo;programacion dinamica;ant colony optimization;information use;travelling salesman problem;dinamica poblacion;optimisation combinatoire;problema viajante comercio;ajustement modele;combinatorial problem;probleme combinatoire;problema combinatorio;probleme commis voyageur;model matching;programmation dynamique;population dynamics;dynamique population;combinatorial optimization;utilizacion informacion;dynamic optimization problem;optimizacion combinatoria	Population based ACO algorithms for dynamic optimization problems are studied in this paper. In the population based approach a set of solutions is transferred from one iteration of the algorithm to the next instead of transferring pheromone information as in most ACO algorithms. The set of solutions is then used to compute the pheromone information for the ants of the next iteration. The population based approach can be used to solve dynamic optimization problems when a good solution of the old instance can be modified after a change of the problem instance so that it represents a reasonable solution for the new problem instance. This is tested experimentally for a dynamic TSP and dynamic QAP problem. Moreover the behavior of different strategies for updating the population of solutions are compared.	algorithm;dynamic problem (algorithms);dynamic programming;experiment;fitness function;heuristic;iteration;mathematical optimization;population;program optimization;quadratic assignment problem	Michael Guntsch;Martin Middendorf	2002		10.1007/3-540-45724-0_10	mathematical optimization;ant colony optimization algorithms;combinatorial optimization;computer science;artificial intelligence;dynamic programming;mathematics;population dynamics;travelling salesman problem;algorithm	AI	25.794925630210447	1.5577765118355111	42800
fac55ede6644da4ebec6726fe8af4889ea0a3b5a	overlapped multiresolution motion compensation technique for wavelet video compression	transformation ondelette;chevauchement;mesure deplacement;video techniques;estimation mouvement;transformacion discreta;data compression;motion compensation;etude experimentale;video compression;technique video;motion estimation;overlapped block motion compensation;overlap;analyse multiresolution;imbricacion;motion compensated;algorithme;wavelet transforms;compensation mouvement;displacement measurement;signal video;discrete transformation;algorithms;video signals;displaced frame difference;multi resolution;encoding;multiresolution analysis;transformation discrete;wavelets;analisis multiresolucion;compression donnee;codage	A new block based motion estimation and compensation technique named overlapped multi-resolution motion compensation (OMRMC) is proposed. The algorithm employs the overlapped block motion compensation (OBMC) algorithm in wavelet domain to reduce the blocky artifacts in the predicted frames using the multi-resolution motion estimation (MRME) technique. Simulation results showed the use of OMRMC reduced up to 26% of the displaced frame difference (DFD) energy compared with MRME.		Yufei Yuan;Choong Wah Chan	2000		10.1117/12.386711	data compression;computer vision;quarter-pixel motion;computer science;motion estimation;mathematics;motion compensation;algorithm;computer graphics (images)	Vision	46.27595301058151	-14.553088073509606	42825
332112199981b54d23cf52a9a65c49603b26fb0c	parallel particle swarm optimization with adaptive asynchronous migration strategy	search space;solution accuracy;particle swarm optimization pso;convergence speed;particle swarm optimizer;problem complexity;adaptive asynchronous migration strategy;experience base;large scale problem;parallel particle swarm optimization ppso	This paper proposes a parallel particle swarm optimization (PPSO) by dividing the search space into sub-spaces and using different swarms to optimize different parts of the space. In the PPSO framework, the search space is regarded as a solution vector and is divided into two sub-vectors. Two cooperative swarms work in parallel and each swarm only optimizes one of the sub-vectors. An adaptive asynchronous migration strategy (AAMS) is designed for the swarms to communicate with each other. The PPSO benefits from the following two aspects. First, the PPSO divides the search space and each swarm can focus on optimizing a smaller scale problem. This reduces the problem complexity and makes the algorithm promising in dealing with large scale problems. Second, the AAMS makes the migration adapt to the search environment and results in a very timing and efficient communication fashion. Experiments based on benchmark functions have demonstrated the good performance of the PPSO with AAMS on both solution accuracy and convergence speed when compared with the traditional serial PSO (SPSO) and the PPSO with fixed migration frequency.		Zhi-hui Zhan;Jun Zhang	2009		10.1007/978-3-642-03095-6_47	mathematical optimization;multi-swarm optimization;simulation;searching the conformational space for docking;machine learning	EDA	26.39968724220713	-2.6700570684193456	42846
ec471692d647adb5fc0a9ee89d639a267fd0af87	simultaneous encoder for high-dynamic-range and low-dynamic-range video	hdr technology simultaneous video encoder high efficiency video coding encoding acceleration;ibcn;video coding data compression video codecs;encoding acceleration dynamic range tv standards computational complexity;computational cost simultaneous encoder high dynamic range video low dynamic range video hdr technology video technology human eye luminance values content providers high efficiency video coding coding information compression ldr technology encoding algorithms	High-dynamic-range (HDR) technology is an emerging video technology that allows displays to produce a higher range of luminance to better approximate the range of brightness perceived by the human eye. However, during the transition to this new technology, not all consumer devices will support the full range of luminance values offered by HDR. In order to also support these devices with lower dynamic ranges, content providers have to supply multiple dynamic range versions to provide the best experience to all viewers. This means that the processing cost to compress these versions will be multiplied by the number of versions. As a solution, this paper proposes a simultaneous encoder based on high efficiency video coding. This encoder reuses parts of the coding information generated during compression of an HDR video to accelerate the encoding of a low-dynamicrange (LDR) version of the same video. The proposed method speeds up the encoder 299 times with a bit rate increase of 12.4% compared to a non-accelerated encode of the LDR version. This is more than 90 times faster compared to stateof- the-art fast encoding algorithms and allows simultaneous encoding of the two versions for approximately the computational cost of a single encoder.	algorithmic efficiency;approximation algorithm;data compression;dynamic range;encode;encoder;high efficiency video coding;high-dynamic-range imaging;high-dynamic-range rendering;ldraw;overhead (computing);television;tip (unix utility)	Johan De Praeter;Antonio Jesús Díaz-Honrubia;Tom Paridaens;Glenn Van Wallendael;Peter Lambert	2016	IEEE Transactions on Consumer Electronics	10.1109/TCE.2016.7838095	video compression picture types;scalable video coding;real-time computing;h.263;computer science;video capture;video tracking;coding tree unit;multimedia;video processing;smacker video;context-adaptive binary arithmetic coding;motion compensation;h.261;multiview video coding;computer graphics (images)	Mobile	43.96997096618204	-20.47731924020722	42853
c57ffe42c221df1e3522bf6f030c8fd55aef754e	temporal forensics and anti-forensics for motion compensated video	digital forensics;theoretical model;frame deletion;game theory;theoretical framework;computer forensics;motion compensation;forgery;video coding computer forensics game theory message authentication motion compensation;video compression;game theory digital forensics anti forensics frame deletion video compression;motion compensated;video coding;forgery video compression digital forensics multimedia communication mathematical model heuristic algorithms;heuristic algorithms;multimedia communication;mathematical model;probability of false alarm;digital video;message authentication;game theoretic framework temporal forensics antiforensic manipulation motion compensated video digital forensic technique multimedia content digital forgeries digital manipulation false alarm probability digital video authentication video frame deletion addition detection technique video forensic technique;heuristic algorithm;anti forensics	Due to the ease with which digital information can be altered, many digital forensic techniques have been developed to authenticate multimedia content. Similarly, a number of anti-forensic operations have recently been designed to make digital forgeries undetectable by forensic techniques. However, like the digital manipulations they are designed to hide, many anti-forensic operations leave behind their own forensically detectable traces. As a result, a digital forger must balance the trade-off between completely erasing evidence of their forgery and introducing new evidence of anti-forensic manipulation. Because a forensic investigator is typically bound by a constraint on their probability of false alarm (P_fa), they must also balance a trade-off between the accuracy with which they detect forgeries and the accuracy with which they detect the use of anti-forensics. In this paper, we analyze the interaction between a forger and a forensic investigator by examining the problem of authenticating digital videos. Specifically, we study the problem of adding or deleting a sequence of frames from a digital video. We begin by developing a theoretical model of the forensically detectable fingerprints that frame deletion or addition leaves behind, then use this model to improve upon the video frame deletion or addition detection technique proposed by Wang and Farid. Next, we propose an anti-forensic technique designed to fool video forensic techniques and develop a method for detecting the use of anti-forensics. We introduce a new set of techniques for evaluating the performance of anti-forensic operations and develop a game theoretic framework for analyzing the interplay between a forensic investigator and a forger. We use these new techniques to evaluate the performance of each of our proposed forensic and anti-forensic techniques, and identify the optimal actions of both the forger and forensic investigator.	anti-computer forensics;authentication;claire;data compression;database;digital data;digital video;experiment;fingerprint;game theory;scalable video coding;sensor;simulation;tracing (software);video clip;video compression picture types	Matthew C. Stamm;Wan-Yi Sabrina Lin;K. J. Ray Liu	2012	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2012.2205568	data compression;message authentication code;heuristic;game theory;computer science;digital forensics;mathematical model;multimedia;internet privacy;motion compensation;computer security;computer forensics;statistics	Security	37.73948906637454	-12.603517936174484	42865
01df005542c75a63c7b27287ef0863346cbf7d9b	evaluating the squared-exponential covariance function in gaussian processes with integral observations		This paper deals with the evaluation of double line integrals of the squared exponential covariance function. We propose a new approach in which the double integral is reduced to a single integral using the error function. This single integral is then computed with efficiently implemented numerical techniques. The performance is compared against existing state of the art methods and the results show superior properties in numerical robustness and accuracy per computation time.		Johnny Hendriks;Carl Jidling;Adrian Wills;Thomas B. Schon	2018	CoRR			Robotics	32.47794351439884	-15.757319252479943	42975
0c8948c464f8c250b799af62bcdb15122182ed35	parallel in-loop filtering in hevc encoder on gpu		In-loop filtering is an important part of high efficiency video coding (HEVC), which consists of deblocking filter and sample adaptive offset (SAO) filter. It can not only improve the compression efficiency of HEVC, but also improve the visual quality of the reconstructed videos significantly. However, the high computational complexity hampers its applications for real-time encoding scenarios. In this paper, we propose a parallel strategy for in-loop filtering in HEVC encoder on graphics processing unit (GPU). In the proposed strategy, the pipeline structure for HEVC encoding by parallel processing deblocking filter and SAO on GPU is described first. Then, the joint optimization for deblocking filter and SAO on GPU is detailed by parallel processing of deblocking filter and parallel processing of SAO separately. The joint optimization can improve the degree of parallelism and ease the computational burden of the CPU. Experimental results demonstrate that the proposed method can achieve about 47% (up to 67%) time saving on average for test sequences.	central processing unit;computational complexity theory;computer graphics;data compression;deblocking filter;degree of parallelism;encoder;graphics processing unit;high efficiency video coding;mathematical optimization;parallel computing;real-time clock;sword art online: progressive	Yang Wang;Xun Guo;Xiaopeng Fan;Yan Lu;Debin Zhao;Wen Gao	2018	IEEE Transactions on Consumer Electronics	10.1109/TCE.2018.2867812	computer science;computer vision;artificial intelligence;deblocking filter;encoder;parallel computing;graphics processing unit;filter (signal processing);degree of parallelism;decoding methods;offset (computer science);central processing unit	Visualization	45.851505519012846	-20.18012825101608	42985
2a6666c74b8de36a2ad83dc2b89896ff9d3d065a	interval multi-objective quantum-inspired cultural algorithms		It had been proved that the knowledge may promote more efficient evolution. Considering the knowledge defined in different form, we present interval multi-objective quantum-inspired cultural algorithms so as to effectively utilize the implicit information embodied in the evolution to promote more efficient search. It adopts the dual structure derived from cultural algorithm. In population space, the rectangle’s height of each allele in real-encoding quantum individuals is calculated in terms of the possibility dominant rank, instead of the relative fitness values. Three kinds of crowding operators are defined, including the crowding distance of hypercube, the harmonic distance of hypercube and the coverage rate of hypercube to grids, to measure the crowding degree among evolutionary individuals. In belief space, the knowledge is used to guide selection and mutation operations of evolutionary individuals and the update operation of quantum individuals. The statistic simulation results for four benchmark functions indicate that the solutions obtained from the proposed algorithms more close to the true Pareto front uniformly and the uncertainty of non-dominant solutions is less. Furthermore, the knowledge extracted from the evolution plays a positive role in improving the convergence and distribution.	benchmark (computing);computation;converge;crowding;cultural algorithm;mathematical optimization;multi-objective optimization;optimization problem;pareto efficiency;simulation;topography	Yi-Nan Guo;Pei Zhang;Jian Cheng;Chun Wang;Dun-Wei Gong	2016	Neural Computing and Applications	10.1007/s00521-016-2572-5	mathematical optimization;combinatorics;discrete mathematics;machine learning;mathematics;statistics	AI	27.83495575295636	-4.56225461535027	42997
eb06f39e8e3fd174a7cc7306040b5ac1ee656772	exploring the accuracy of a parallel cooperative model for trajectory-based metaheuristics	recent new parallel model;classical cooperative parallel model;trajectory-based metaheuristics;local solution;parallel model;classical cooperative method;new model;single solution;parallel cooperative model;search method;important information;underlying search method	Classical cooperative parallel models for metaheuristics have one major issue when the underlying search method is based on the exploration of the neighborhood of one single solution, i.e., a trajectory-based metaheuristic. Whenever a cooperation step takes place by exchanging solutions, either the incoming or the local solution has to be discarded because the subalgorithm does only work with one single solutions. Therefore, important information may be lost. A recent new parallel model for trajectory-based metaheuristics has faced this issue by adding a crossover operator that is aimed at combining valuable information from both the incoming and the local solution. This work is targeted to further evaluate this parallel model by addressing two well-known, hard optimization problems (MAXSAT and RND) using Simulated Annealing as the search method in each subalgorithm. The results have shown that the new model is able to outperform the classical cooperative method under the experimental conditions used.	mathematical optimization;maximum satisfiability problem;metaheuristic;numerical analysis;randomness;simulated annealing	Gabriel Luque;Francisco Murilo Tavares Luna;Enrique Alba;Sergio Nesmachnow	2011		10.1007/978-3-642-27549-4_41	mathematical optimization;simulation;parallel metaheuristic;computer science;artificial intelligence;machine learning	AI	26.51400889993582	-2.562183322933698	43008
e0b5971cb86ba01b37d87d51a7f03908c139f2d1	digital watermarking-based dct and jpeg model	watermarking;image segmentation;digital watermark;computational geometry;wiener filters;computational geometry watermarking discrete cosine transforms multimedia systems image segmentation median filters wiener filters;multimedia systems;discrete cosine transform;discrete cosine transforms;feature extraction;multimedia data;watermarking discrete cosine transforms robustness image segmentation image coding protection feature extraction random sequences distortion transform coding;joint photographic expert group;pseudorandom sequence;compression ratio;wiener filter;image size digital watermarking techniques dct model jpeg model copyright multimedia data discrete cosine transform image segmentation voronoi diagram extraction points pseudorandom sequence signal distortions geometric manipulations compression ratio wiener filters median filters;median filters;voronoi diagram	In recent years, digital watermarking techniques have been proposed to protect the copyright of multimedia data. Different watermarking schemes have been suggested for images. The goal of this paper is to develop a watermarking algorithm based on the discrete cosine transform (DCT) and image segmentation. The image is first segmented in different portions based on the Voronoi diagram and features extraction points. Then, a pseudorandom sequence of real numbers is embedded in the DCT domain of each image segment. Different experiments are conducted to show the performance of the scheme under different types of attacks. The results show that our proposed watermark scheme is robust to common signal distortions, including geometric manipulations. The robustness against Joint Photographic Experts Group (JPEG) compression is achieved for a compression ratio of up to 45, and robustness against average, median, and Wiener filters is shown for the 3/spl times/3 up to 9/spl times/9 pixel neighborhood. It is observed that robustness against scaling was achieved when the watermarked image size is scaled down to 0.4% of its original size.	digital watermarking;discrete cosine transform;jpeg	Mohamed A. Suhail;Mohammad S. Obaidat	2003	IEEE Trans. Instrumentation and Measurement	10.1109/TIM.2003.817155	computer vision;speech recognition;digital watermarking;computational geometry;computer science;theoretical computer science;mathematics;quantization	Embedded	41.2141211358951	-11.314494115112174	43048
d50edd473b6ee381ee7c50e3e95b145e87b75809	real-time compression of intra-cerebral eeg using eigendecomposition with dynamic dictionary	eigendecomposition;eigenvalues and eigenfunctions;jpeg2000 real time compression intracerebral eeg eigendecomposition dynamic dictionary ieeg compression eeg channel eigenchannel low bit rate br;vectors dictionaries real time systems transform coding electroencephalography covariance matrices;jpeg2000 eeg compression eigendecomposition;distortion;jpeg2000;eeg;electroencephalography;compression;medical signal processing distortion eigenvalues and eigenfunctions electroencephalography encoding;encoding;medical signal processing	A novel technique for Intra-cerebral Electroencephalogram (iEEG) compression in real-time is proposed in this article. This technique uses eigendecomposition and dynamic dictionary update to reduce the EEG channels to only one decor related channel or eigenchannel. Experimental results show that this technique is able to provide low distortion values at very low bit rates (BRs). In addition, performance results of this method show to be better and more stable than JPEG2000. Results do not vary a lot both in time and between different patients which proves the stability of the method.	dictionary;distortion;electroencephalography;jpeg 2000;numerical stability;real-time clock	Hoda Daou;Fabrice Labeau	2013	2013 Data Compression Conference	10.1109/DCC.2013.68	speech recognition;electroencephalography;mathematics	EDA	44.44348948511756	-16.286354596056146	43341
92fa0caed91a4258c808ce270624eab4969420fc	computational cost reduction of h.264/avc video coding standard for videoconferencing applications	videoconferencing;spline;teleconferencing;interpolation;psnr;motion compensation;estimation method;cost reduction;video sequences;temporal spline interpolation;video coding computational complexity cost reduction interpolation motion compensation splines mathematics teleconferencing;splines mathematics;motion compensated;motion compensated estimation;video coding;h 264 avc standard;estimation;computational complexity;pixel;computational complexity computational cost reduction video coding h 264 avc standard videoconferencing motion compensated estimation video coders temporal spline interpolation;video coders;computational cost reduction;spline interpolation;computational efficiency automatic voltage control video coding teleconferencing decoding costs motion estimation spline interpolation video sequences	The performance of the most recent video coding standards is obtained at the price of a high computational cost. This cost is essentially due to the sophisticated motion-compensated estimation methods developed during these last years. In videoconferencing applications, we show that it is possible to replace the motion-compensated estimation of the B and/or P frames in the video coder H.264/AVC by temporal spline interpolation. The P-frames (respectively B-frames) are predicted with nearest close past and future reference decoded I-frames (respectively P or I and P or I). Simulation results show that, for a given bit-rate, the quality of the decoded video sequence using spline interpolation is equivalent to the H.264/AVC decoded video sequence with a very low cost in term of computational complexity.	algorithmic efficiency;analysis of algorithms;codec;computation;computational complexity theory;data compression;distortion;h.264/mpeg-4 avc;jumbo frame;moving picture experts group;performance;simulation;spline (mathematics);spline interpolation;video coding format	Vianney Muñoz-Jiménez;Anissa Zergaïnoh-Mokraoui	2007	2007 9th International Symposium on Signal Processing and Its Applications	10.1109/ISSPA.2007.4555508	scalable video coding;spline interpolation;spline;computer vision;estimation;teleconference;peak signal-to-noise ratio;interpolation;computer science;context-adaptive variable-length coding;mathematics;multimedia;videoconferencing;context-adaptive binary arithmetic coding;computational complexity theory;motion compensation;pixel;statistics;computer graphics (images)	Arch	46.166928709299704	-18.608522181378916	43361
404592b37cede8ba4d91f5f8398bd623b044c30c	mapping motion vectors for awyner-ziv video transcoder	silicon;distributed video coding wyner ziv coding transcoder h 263;wyner ziv;complexity theory;codecs;decoding;motion estimation;motion estimation motion vectors mapping awyner ziv video transcoder wyner ziv video coding encoders highly complex decoders wz codecs dpcm dct codec standard h 263;proof of concept;video coding;video transcoding;motion vector;pixel;distributed video coding;wyner ziv coding;codecs decoding discrete cosine transforms code standards motion estimation video coding feedback mobile communication laboratories solids;h 263;transcoding;transcoder;video coding codecs motion estimation transcoding	Wyner-Ziv (WZ) coding of video utilizes simple encoders and highly complex decoders. A transcoder from a WZ codec to a traditional codec can potentially increase the range of applications for WZ codecs. We present a transcoder scheme from the most popular WZ codec architecture to a DPCM/DCT codec. As a proof of concept, we implemented this transcoder using a simple pixel domain WZ codec and the standard H.263+. The transcoder design aims at reducing complexity, since the transcoder has to perform both WZ decoding and DPCM/DCT encoding, including motion estimation. New approaches are used to map motion vectors for such a transcoder. Results are presented to demonstrate the transcoder performance.	codec;discrete cosine transform;encoder;motion estimation;pixel;winzip	Eduardo Peixoto;Ricardo L. de Queiroz;Debargha Mukherjee	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414231	transcoding;telecommunications;computer science;theoretical computer science;multimedia	Vision	45.15048911325181	-18.830869270040765	43407
3193e0c0d753a4762f3a6ee506b5683baf4796e5	a bayesian nonparametric study of a dynamic nonlinear model	bayes estimation;modelo dinamico;analisis datos;melange loi probabilite;echantillonnage;dynamic model;estimation non parametrique;logistic model;mixed distribution;distribucion logistica;non linear model;modele non lineaire;sampling;non parametric estimation;data analysis;estimacion bayes;modelo no lineal;estimation erreur;mixture model;error estimation;modele dynamique;modele logistique;nonlinear dynamics;statistical computation;estimacion error;calculo estadistico;bayesian nonparametrics;distribution logistique;qa273 probabilities;modelo logistico;mezcla ley probabilidad;analyse donnee;calcul statistique;estimacion no parametrica;estimation statistique;muestreo;estimacion estadistica;statistical estimation;logistic distribution;estimation bayes;nonlinear model	A Bayesian nonparametric approach to modeling a nonlinear dynamic model is presented. New techniques for sampling infinite mixture models are used. The inference procedure specifically in the case of the logistic model and when the nonparametric component is applied to the additive errors is demonstrated.	nonlinear system	Spyridon J. Hatjispyros;Theodoros Nicoleris;Stephen G. Walker	2009	Computational Statistics & Data Analysis	10.1016/j.csda.2009.05.009	logistic distribution;sampling;econometrics;nonlinear system;calculus;mixture model;mathematics;logistic regression;data analysis;nonparametric regression;statistics	ML	33.03303922183935	-22.589601289798697	43459
23e22328eff407652234a771bfe59730b887c65d	adaptive tuning of the sampling domain for dynamic-domain rrts	path planning;computational geometry;trees mathematics;rapidly exploring random trees adaptive tuning sampling domain dynamic domain sampling based planners motion planning single query planners voronoi region path planning;tuning;adaptive systems;rapidly exploring random tree;sampling methods robustness iterative algorithms motion planning robots geometry computer science urban planning application software testing;sampled data systems;random processes;motion planning;rrts motion planning voronoi bias;sampling methods;rrts;tuning path planning sampled data systems sampling methods random processes adaptive systems trees mathematics computational geometry;voronoi bias	Sampling based planners have become increasingly efficient in solving the problems of classical motion planning and its applications. In particular, techniques based on the rapidly-exploring random trees (RRTs) have generated highly successful single-query planners. Recently, a variant of this planner called dynamic-domain RRT was introduced by Yershova et al. (2005). It relies on a new sampling scheme that improves the performance of the RRT approach on many motion planning problems. One of the drawbacks of this method is that it introduces a new parameter that requires careful tuning. In this paper we analyze the influence of this parameter and propose a new variant of the dynamic-domain RRT, which iteratively adapts the sampling domain for the Voronoi region of each node during the search process. This allows automatic tuning of the parameter and significantly increases the robustness of the algorithm. The resulting variant of the algorithm has been tested on several path planning problems.	adaptive grammar;algorithm;automatic summarization;computation;dd (unix);gibbs sampling;iteration;motion planning;performance tuning;sampling (signal processing);voronoi diagram	Léonard Jaillet;Anna Yershova;Steven M. LaValle;Thierry Siméon	2005	2005 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2005.1545607	mathematical optimization;computational geometry;computer science;artificial intelligence;theoretical computer science;machine learning;motion planning	Robotics	51.746850283391815	-23.893936656965305	43513
8abc8eddaabf1bbc1ebb12c9e08850f65e060bd9	application of basic and logarithmic poisson execution time models in software reliability measurement	software reliability measurement;logarithmic poisson execution time;software reliability;predictive validity;parameter estimation	Two software reliability models that share the advantageous property of being based on execution time are presented. They are compared with a number of other published models. Predictive validity is established using sets of failure data from a varied group of software projects and two different parameter estimation methods. The characteristics and advantages and disadvantages of the two with respect to each other are discussed.	software quality;software reliability testing	John D. Musa;Kazuhira Okumoto	1987		10.1007/BFb0034287	predictive validity;computer science;software reliability testing;estimation theory;software quality	Metrics	28.516593775284072	-17.243319140059008	43567
df01b1e4f2d981394432fa9792709b626b657fcd	modeling, simulation and inference for multivariate time series of counts using trawl processes		Abstract This article presents a new continuous-time modeling framework for multivariate time series of counts which have an infinitely divisible marginal distribution. The model is based on a mixed moving average process driven by Levy noise, called a trawl process, where the serial correlation and the cross-sectional dependence are modeled independently of each other. Such processes can exhibit short or long memory. We derive a stochastic simulation algorithm and a statistical inference method for such processes. The new methodology is then applied to high frequency financial data, where we investigate the relationship between the number of limit order submissions and deletions in a limit order book.		Almut E. D. Veraart	2019	J. Multivariate Analysis	10.1016/j.jmva.2018.08.012	mathematics;statistics;count data;autocorrelation;marginal distribution;multivariate statistics;moving-average model;inference;statistical inference;algorithm;stochastic simulation	PL	31.734443725798872	-20.084589809588348	43739
a0bba406290c45762839b6df9794ed4cf67f2b05	the role of long memory in hedging effectiveness	methode jackknife;commodities;62f40;optimal hedge ratio;hg finance;variation journaliere;analyse multivariable;garch model;covariance analysis;modele empirique;multivariate analysis;bootstrap;analisis datos;fractional cointegration;long memory process;multivariate garch;correction erreur;metodo jackknife;variance analysis;statistical significance;long term variation;satisfiability;statistical regression;processus memoire longue;hedge ratio;estimation parametrique;variacion largo plazo;data analysis;modele garch;estimation erreur;analyse covariance;error estimation;analisis variancia;error correction;regresion estadistica;62j10;long memory;statistical computation;estimacion error;calculo estadistico;reduccion variancia;empirical model;fractional integral;analisis multivariable;analyse donnee;modelo empirico;reduction variance;calcul statistique;methode reechantillonnage;variacion diaria;correccion error;resampling method;analisis covariancia;jackknife method;regression statistique;variance reduction;variation long terme;analyse variance;daily variation;variance;variancia;fiec bekk	A joint fractionally integrated, error-correction and multivariate GARCH (FIEC-BEKK) approach is applied to investigate hedging effectiveness using daily data 1995–2005. The findings reveal the proxied error-correction term has a long memory component that theoretically should affect hedging effectiveness. When the FIEC model empirical conditions are satisfied, the FIEC-BEKK hedging strategy outperforms the OLS benchmark out of sample in terms of both variance reduction and hedger utility. A bootstrap exercise indicates that the variance reduction is statistically significant. © 2007 Elsevier B.V. All rights reserved.	benchmark (computing);computation;computational statistics & data analysis;conditional entropy;error detection and correction;foreign exchange service (telecommunications);futures and promises;jian xu;ordinary least squares;variance reduction	Jerry Coakley;Jian Dollery;Neil Kellard	2008	Computational Statistics & Data Analysis	10.1016/j.csda.2007.10.019	autoregressive conditional heteroskedasticity;econometrics;error detection and correction;analysis of variance;analysis of covariance;mathematics;statistical significance;variance;multivariate analysis;mathematical economics;data analysis;empirical modelling;regression analysis;statistics;variance reduction;satisfiability	AI	33.44575054335695	-22.422612427452165	43755
c0e141addd17bf237bd0725346ad0d6c38f80c97	positional effect of crossover and mutation in grammatical evolution	position;crossover;bias;grammatical evolution;mutation	An often-mentioned issue with Grammatical Evolution is that a small change in the genotype, through mutation or crossover, may completely change the meaning of all of the following genes. This paper analyses the crossover and mutation operations in GE, in particular examining the constructive or destructive nature of these operations when occurring at points throughout a genotype. The results we present show some strong support for the idea that events occurring at the first positions of a genotype are indeed more destructive, but also indicate that they may be the most constructive crossover and mutation points too. We also demonstrate the sensitivity of this work to the precise definition of what is constructive/destructive.	crossover (genetic algorithm);grammatical evolution	Tom Castle;Colin G. Johnson	2010		10.1007/978-3-642-12148-7_3	mutation;crossover;mutation;computer science;position;artificial intelligence;bias;grammatical evolution;genetics;algorithm	ML	25.055138381989217	-9.333631233653103	43876
249a6e0f97824d21b3cffb1a9868680918fa17d5	statistical optimization of switched two-level quantizers with application to dpcm encoding of color video	block adaptive quantization;optimisation;lossy source coding;prediction error;statistical optimization;prediction theory video coding differential pulse code modulation optimisation source coding vector quantisation statistical analysis;probability density function;signal design;color video;adpcm;low complexity;differential pulse code modulation;encoding pulse modulation signal design vector quantization modulation coding video coding probability density function telecommunication switching source coding error analysis;dpcm encoding;error analysis;video coding;adpcm dpcm encoding color video video coding statistical optimization switched two level quantizers lossy source coding prediction error signal block adaptive quantization block truncation coding vector quantization vq adaptive differential pulse code modulation peak signal to noise ratio;vq;vector quantization;statistical analysis;prediction theory;telecommunication switching;switched two level quantizers;block truncation coding;peak signal to noise ratio;modulation coding;source code;vector quantizer;vector quantisation;encoding;pulse modulation;prediction error signal;source coding;adaptive differential pulse code modulation	Quantization is the core part of all lossy source coding schemes. We present a design scheme for statistical optimization of a set of one-bit (i.e., two-level) quantizers, given the signal statistics of an arbitrary signal, for example a prediction error signal. The set of quantizers can then be used for block-adaptive quantization of the signal. The quantizer design criterion combines ideas from block truncation coding (BTC) and vector quantization (VQ). As an example application of our design scheme we apply a set of two-level quantizers to adaptive differential pulse code modulation of color video where the quantizers are switched adaptively on a block-by-block basis. Coding results show that this scheme has a better performance than other low-complexity video coding schemes. At 2 bit/pel, it performs more than 2 dB better in peak signal-to-noise ratio than DPCM (Differential Pulse Code Modulation) and up to 3.5 dB better than BTC. 1. STATISTICAL DESIGN OF SWITCHED We present a design scheme for a set of two-level (one-bit) quantizers for an arbitrary signal which is based on statistical optimization and does not assume a special signal model. Thus, the scheme is very flexible. The design procedure consists of the following steps: TWO-LEVEL QUANTIZERS 1. A representative set of signals is grouped into blocks using the same blocksize M x N that shall be used for quantization in the actual application. 2. For each block, the minimum mean squared error (MMSE) two-level quantizer is given by the two representative levels [I] A4 x N n1. If nz = 0 (all samples have the same amplitude), 7-2 is defined as 7-2 := 7-1. 3. The pairs (7-1,r~) are calculated for all blocks and collected in a (discrete) two-dimensional probability density function P(r1,rz). Figure 1 shows an example. The signal used is a prediction error signal from a DPCM scheme for video. The distribution is peaky, as one would expect for a prediction error signal.	adaptive differential pulse-code modulation;block truncation coding;data compression;lossy compression;mathematical optimization;mean squared error;peak signal-to-noise ratio;pixel;quantization (signal processing);vector quantization	Frank Hartung;Bernd Girod	1996		10.1109/ICASSP.1996.544840	speech recognition;computer science;theoretical computer science;mathematics;statistics;source code	ML	48.33985071374346	-12.085174774641738	44060
d50601e0b300e9f6ad49f2ae83bc2e565656bff1	dynamic computational complexity and bit allocation for optimizing h.264/avc video compression	computational complexity allocation;data compression;h 264 avc;rate quantization step size computational complexity;video compression;rate quantization step size computational complexity bit allocation h 264 avc video compression dynamic allocation mean absolute difference computational complexity rate distortion;computational complexity rate distortion;rate distortion theory;quantisation signal;mean absolute difference;video coding;dynamic allocation;computational complexity;bit allocation;computational complexity bit rate automatic voltage control video compression quantization clocks encoding video sequences rate distortion control systems;coding modes selection;h 264 avc computational complexity allocation bit allocation coding modes selection video compression;video coding computational complexity data compression image sequences quantisation signal rate distortion theory;h 264 avc video compression;image sequences	In this work we present a novel approach for optimizing H.264/AVC video compression by dynamically allocating computational complexity (such as a number of CPU clocks) and bits for encoding each coding element (basic unit) within a video sequence, according to its predicted MAD (mean absolute difference). Our approach is based on a computational complexity-rate-distortion (C-R-D) analysis, which adds a complexity dimension to the conventional rate-distortion (R-D) analysis. Both theoretically and experimentally, we prove that by implementing the proposed approach better results are achieved. In addition, we present a method and system for implementing the proposed approach, and for controlling computational complexity and bit allocation in real-time and off-line video coding. For allocating a corresponding group of coding modes and the quantization step-size, we develop computational complexity - complexity step - rate (C-I-R) and rate - quantization step-size - computational complexity (R-Q-C) models.	computational complexity theory;data compression;h.264/mpeg-4 avc;optimizing compiler	Evgeny Kaminsky;Dan Grois;Ofer Hadar	2006		10.1109/ITRE.2006.381556	parameterized complexity;computer vision;real-time computing;average-case complexity;decision tree model;computer science;theoretical computer science;computational resource;worst-case complexity;context-adaptive binary arithmetic coding;asymptotic computational complexity	Theory	47.634069228879255	-19.00549185330458	44118
a503db5e026c79aca8d34a0830f9f5ff842848e9	ifs matlab generator: a computer tool for displaying ifs fractals	fractals;computer displays fractals chaos image coding algorithm design and analysis optimization methods weather forecasting economic forecasting mathematical model rendering computer graphics;iterated function system;mathematics computing;critical point;dimension;probabilistic algorithm;attractor;deterministic algorithms;iterated function systems;chaos game algorithm ifs matlab generator ifs fractals mathematical objects iterated function systems rendering algorithms deterministic algorithm probabilistic algorithm;rendering computer graphics;computational efficiency;rendering computer graphics deterministic algorithms fractals mathematics computing;dimension fractals iterated function systems attractor	"""Fractals are among the most exciting and intriguing mathematical objects ever discovered. A particular type of fractals, the Iterated Function Systems (IFS), has received a lot of attention due to its appealing combination of conceptual simplicity, computational efficiency and great ability to reproduce natural formations and complex phenomena. This paper introduces a new Matlab program, called """"IFS Matlab Generator"""", for generating and rendering IFS fractals. In addition to providing a gentle introduction to the mathematical basis of IFS, two of the most important rendering algorithms, the deterministic algorithm and the probabilistic algorithm (also called """"chaos game"""" algorithm), are briefly outlined. A critical point of chaos game is the choice of the set of probabilities associated with the iterated functions. This issue will be briefly discussed in this paper: we analyze the efficiency of the chaos game algorithm, comparing the standard method for choosing the probabilities proposed by Michael Barnsley with another method based on a new multifractal technique. The latter method optimizes the rendering process by obtaining the most efficient set of probabilities. Some examples aimed at illustrating this technique along with a gallery of beautiful two-dimensional fractal objects are also given."""	fractal;matlab	Akemi Gálvez	2009		10.1109/ICCSA.2009.10	mathematical optimization;discrete mathematics;computer science;artificial intelligence;theoretical computer science;machine learning;mathematics;iterated function system;algorithm	Logic	33.47506255846367	-7.552957062223542	44222
b8b7bb0b02623fbd4c02fdd93824faabe26a424d	a dct-domain system for hiding fractal compressed images	data hiding;fractals;image coding;data compression;data encapsulation;steganography;discrete cosine transforms;cryptography;cryptography image coding data encapsulation fractals discrete cosine transforms data compression;fractals image coding steganography frequency domain analysis discrete cosine transforms computer science cryptography data encapsulation robustness computer science education;frequency domain;dct steganography data hiding fractal image compression;dct;middle frequency domain steganography data hiding fractal image compression dct domain system fractal compressed images secret image encrypted;fractal image compression	In this paper, we propose an approach for hiding a secret image in a cover image. In the beginning, the fractal image compression method is used to compress the secret image, and then we encrypt this compressed data by DES. Finally, we embed the encrypted data into the middle-frequency domain of DCT. After embedding the secret image, the goal of steganography can be successfully achieved.	code;coefficient;data compression;discrete cosine transform;embedded system;encryption;fractal compression;image compression;image quality;iteration;steganography	Chin-Chen Chang;Chi-Lung Chiang;Ju Yuan Hsiao	2005	19th International Conference on Advanced Information Networking and Applications (AINA'05) Volume 1 (AINA papers)	10.1109/AINA.2005.17	data compression;computer vision;steganography tools;image compression;computer science;cryptography;theoretical computer science;fractal transform;steganography;fractal compression;frequency domain;statistics;computer graphics (images)	Robotics	41.24726398733591	-11.925627461312112	44284
a85e83e706b9a455939c32f9eabfbd4952229a74	cell attribute-based algorithm for crack visualization	cracks;visualization;distributed software vispartdem;discrete element method;cell atribute	The paper presents the development of the visualization algorithm for propagating cracks. The novel algorithm is based on the cell attribute obtained from the damaged lattice connections employed for discrete element computations of mono-dispersed particulate media. Generation of the cells is efficiently performed by using the positions of particles and the lattice connections. The developed visualization algorithm is implemented in the distributed visualization software VisPartDEM. The efficiency of the software is tested visualizing the datasets resulting from computations of the lattice-based discrete element method. The performance of the developed algorithm is compared with that of the visualization algorithms based on the Voronoi diagrams and the inscribed spheres. DOI:  http://dx.doi.org/10.5755/j01.itc.42.3.2575	algorithm	Ruslan Pacevic;Darius Markauskas;Lukas Radvilavicius;Arnas Kaceniauskas;Remigijus Kutas	2013	ITC	10.5755/j01.itc.42.3.2575	visualization;computer science;theoretical computer science;distributed computing;discrete element method	Visualization	52.83709399316859	-14.143807174682891	44386
64f94e30fe16641ce78996716b00927b9478901d	a robust watermarking scheme using selective curvelet coefficients	watermarking;robust watermarking;curvelet	In this paper, a digital watermarking algorithm used in the curvelet domain is proposed. Wrapping of specially selected Fourier samples is employed to implement fast discrete curvelet transforms (FDCT) to transform the digital image to the curvelet domain. Based on the capacity of the watermark and the impact of the variation of the curvelet coefficients, the proper positions to embed the watermark were chosen. The experimental results show that watermark is robust to most of the signal processing operations, including Common Image Processing Attacks like Histogram Equalization, Brighter Filter, Darker Filter, Increase Contrast, Decrease Contrast, Median Filtering, JPEG Compression attack, Gaussian Noise attack and Laplacian Filtering.	algorithm;coefficient;curvelet;digital image;digital watermarking;fast fourier transform;histogram equalization;image processing;median filter;signal processing;wrapping (graphics)	H. Y. Leung;Lee-Ming Cheng;L. L. Cheng	2008	2008 International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1142/S0219691309002830	arithmetic;computer vision;digital watermarking;theoretical computer science;curvelet;mathematics	EDA	41.2128956287537	-10.646327682110446	44409
5d3393f23c4b505f35b18652d86695b2e6d46c3c	a discrete tchebichef transform approximation for image and video coding	approximation methods signal processing algorithms discrete cosine transforms approximation algorithms image coding complexity theory;image coding;complexity theory;video coding approximation theory discrete transforms field programmable gate arrays inverse transforms numerical analysis;image and video coding approximate dtt fast algorithms;approximation algorithms;dynamic power consumption discrete tchebichef transform approximation video coding image coding dtt inverse transforms forward transforms bit shifting operations numerical compression simulations xilinx virtex 6 fpga;discrete cosine transforms;approximation methods;signal processing algorithms	In this letter, we introduce a low-complexity approximation for the discrete Tchebichef transform (DTT). The proposed forward and inverse transforms are multiplication-free and require a reduced number of additions and bit-shifting operations. Numerical compression simulations demonstrate the efficiency of the proposed transform for image and video coding. Furthermore, Xilinx Virtex-6 FPGA based hardware realization shows 44.9% reduction in dynamic power consumption and 64.7% lower area when compared to the literature.	approximation;bitwise operation;data compression;field-programmable gate array;numerical linear algebra;simulation	Paulo A. M. Oliveira;Renato J. Cintra;Fábio M. Bayer;Sunera Kulasekera;Arjuna Madanayake	2015	IEEE Signal Processing Letters	10.1109/LSP.2015.2389899	mathematical optimization;discrete mathematics;transform coding;computer science;theoretical computer science;mathematics;approximation algorithm	Vision	45.56387611925831	-10.638092799060834	44424
1fa4d12d602af2fc1e35ed946622a42baf598ef6	a computer program to improve lr tests for generalized linear models	modelo lineal generalizado;62j12;computer program;metodo estadistico;ajustamiento modelo;likelihood ratio;exponential distribution;bartlett correction;aplicacion;ley exponencial;modele lineaire generalise;test lineaire;exponential model;fonction repartition;loi exponentielle;modele exponentiel;ajustement bartlett;linear test;62f05;simulacion numerica;modele lineaire;exponential dispersion distribution;statistical method;link function;modelo lineal;statistical regression;programa informatico;ajustement modele;bartlett adjustment;funcion distribucion;distribution function;dispersion parameter;methode statistique;regresion estadistica;correction bartlett;model matching;variance function;simulation numerique;linear model;likelihood ratio test;general linear model;generalized linear model;regression statistique;rapport vraisemblance;test razon verosimilitud;application;deviance;test rapport vraisemblance;60e05;programme ordinateur;62 04;numerical simulation;relacion verosimilitud;r software	Generalized linear models enable the fitting of models to a wide range of data types. These models are based on exponential dispersion distributions. Improved likelihood ratio tests for these models were developed by Cordeiro (1983)Cordeiro (1987). We present a simple R program source for calculating Bartlett corrections to improve likelihood ratio tests in these models. The program was tested on some special models, confirming all of the previously reported numerical results for the Bartlett corrections.	computer program;generalized linear model;lr parser	D. N. da Silva;G. M. Cordeiro	2009	Communications in Statistics - Simulation and Computation	10.1080/03610910903268825	computer simulation;econometrics;likelihood-ratio test;calculus;mathematics;statistics	AI	33.062569789532745	-22.45085005470521	44550
3030fb7ab0fcac32d08899ae9fca5eab0e1b72c4	poster: randomized algorithms for planted motif search	planted motif search l;biology computing;probability randomized algorithms planted motif search computational biology;d motif search;probability;randomized motif search;approximation algorithms search problems approximation methods algorithm design and analysis bioinformatics biology silicon;randomised algorithms;randomized motif search planted motif search l d motif search;randomized algorithm;randomised algorithms biology computing probability;computational biology	Planted (l, d) Motif Search is one of the well studied problems in computational biology. Two kinds of algorithms for solving this problem can be found in the literature, namely, exact and approximate. We introduce a third kind, namely, randomized algorithms with an estimate on the probability of identifying the planted motif. We present one such algorithm. An experimental study of this algorithm has yielded promising results.	approximation algorithm;computation;computational biology;experiment;planted motif search;randomized algorithm;whole earth 'lectronic link	Jerlin C. Merlin;Hieu Dinh	2012	2012 IEEE 2nd International Conference on Computational Advances in Bio and medical Sciences (ICCABS)	10.1109/ICCABS.2012.6182654	computer science;bioinformatics;theoretical computer science;machine learning;probability;planted motif search;randomized algorithm;statistics	Robotics	30.413260848809994	-9.174143862506709	44648
e4d1d3b9c3290e62fbab47352d1cf7202ae3f200	error propagation control in laplacian mesh compression.				Libor Vása;Jan Dvorák	2018	Comput. Graph. Forum	10.1111/cgf.13491	computer vision;theoretical computer science;computer science;artificial intelligence;propagation of uncertainty;compression (physics);laplace operator	Robotics	41.901662610170035	-15.862472244930087	44727
4b2b2f4f9e462a506adae2aa3b3dfbafae9a79d8	approaches to adaptive stochastic search based on the nonextensive q-distribution	particle swarm;nonextensive statistics;adaptive stochastic search;stochastic search;simulated annealing diffusion	This paper explores the use of the nonextensive q-distribution in the context of adaptive stochastic searching. The proposed approach consists of generating the “probability” of moving from one point of the search space to another through a probability distribution characterized by the q entropic index of the nonextensive entropy. The potential benefits of this technique are investigated by incorporating it in two different adaptive search algorithmic models to create new modifications of the diffusion method and the particle swarm optimizer. The performance of the modified search algorithms is evaluated in a number of nonlinear optimization and neural network training benchmark problems.	artificial neural network;benchmark (computing);inverted index;mathematical optimization;nonextensive entropy;nonlinear programming;nonlinear system;search algorithm;stochastic optimization;swarm	George D. Magoulas;Aristoklis D. Anastasiadis	2006	I. J. Bifurcation and Chaos	10.1142/S0218127406015921	statistical physics;mathematical optimization;machine learning;mathematics;particle swarm optimization;physics	AI	27.557001035050412	-5.44876521820612	44732
7e2f8cd51d454543d2d896f2ab6f12f018ec75e3	fuzzy dynamic parameters adaptation in the cuckoo search algorithm using fuzzy logic	search problems fuzzy logic fuzzy set theory;heuristic algorithms fuzzy systems optimization birds mathematical model benchmark testing algorithm design and analysis;birds;mathematical functions fuzzy dynamic parameters adaptation fuzzy logic levy flights fuzzy cuckoo search fcs;heuristic algorithms;mathematical model;optimization;fuzzy systems cuckoo search algorithm levy flights;algorithm design and analysis;fuzzy systems;benchmark testing	The proposed method in this paper describes the enhancement of the Cuckoo Search (CS) Algorithm via Lévy flights using a fuzzy system to dynamically adapt its parameters. The original CS method is compared with the proposed method called Fuzzy Cuckoo Search (FCS) on a set of benchmark mathematical functions. In this case we consider a fuzzy system to dynamically change parameters during the execution of the algorithm. Simulation results on a set of mathematical functions show that the FCS outperforms the traditional CS. In addition, we demonstrate through statistical tests that the proposed method is better than the original CS algorithm.	benchmark (computing);cuckoo search;fuzzy control system;fuzzy logic;genetic algorithm;lévy flight;search algorithm;simulation	Maribel Guerrero;Oscar Castillo;Mario García Valdez	2015	2015 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2015.7256923	algorithm design;benchmark;mathematical optimization;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;mathematical model;mathematics;fuzzy set operations;algorithm;fuzzy control system	Robotics	25.871964484365563	-7.012830981415059	44756
a76014d507709f2b0a6603f02f682e9e9c5322fc	fast multi reference frame motion estimation for high efficiency video coding	motion complexity high efficiency video coding motion estimation reference frame set;low delay testing configuration high efficiency video coding fast multireference frame motion estimation algorithm computational complexity hevc reference frame distribution motion complexity initial reference frame set rfs prediction unit motion vector difference mvd motion estimation process;motion estimation;video coding;computational complexity;video coding computational complexity motion estimation	In this paper, a fast multi reference frame motion estimation algorithm is proposed to reduce the computational complexity for high efficiency video coding (HEVC). Firstly, according to the reference frame distribution, we define the motion complexity for a frame. Based on the motion complexity, an initial reference frame set (RFS) is constructed to reduce the number of the reference frame for motion estimation. Then for each prediction unit (PU), the average distortion per pixel together with the motion vector difference (MVD) in the first reference frame is employed to shrink the RFS in order to early terminate the motion estimation process. In addition, to enhance the robustness of the proposed scheme, an expansion method for RFS is proposed to guarantee the video quality. Experimental results demonstrate that the proposed scheme can significantly save the encoding time. For Low Delay testing configuration, over 30% of time saving can be achieved on average with ignorable performance loss.	algorithm;computational complexity theory;data compression;distortion;high efficiency video coding;mathematical optimization;motion estimation;one-class classification;pixel;reference frame (video);remote file sharing;terminate (software);web server	Shanshe Wang;Siwei Ma;Shiqi Wang;Debin Zhao;Wen Gao	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738413	reference frame;inter frame;residual frame;computer vision;real-time computing;quarter-pixel motion;computer science;theoretical computer science;motion estimation;mathematics;block-matching algorithm;computational complexity theory;motion compensation	Vision	46.77039054869125	-18.856401068747367	44765
54a5cf52caeaed4cd0716b39a2abafcbbb989073	improved artificial fish algorithm for parameters optimization of pid neural network	parameters optimization;control effect;pid neural network;improved artificial fish algorithm	In order to solve problems such as initial weights are difficult to be determined, training results are easy to trap in local minima in optimization process of PID neural network parameters by traditional BP algorithm, this paper proposed a new method based on improved artificial fish algorithm for parameters optimization of PID neural network. This improved artificial fish algorithm uses a composite adaptive artificial fish algorithm based on optimal artificial fish and nearest artificial fish to train network weights parameters of PID neural network. By comparing food consistence in preying behavior to adaptively select vision and step of artificial fish, this method overcomes shortcomings such as slow convergence speed, low optimization accuracy of basic artificial fish algorithm. Simulations of PID neural network system whose parameters are trained respectively through BP algorithm and improved artificial fish algorithm are conducted respectively in the MATLAB environment. The simulation result shows that the PID neural network control system whose parameters are trained by the improved artificial fish algorithm has a better control effect, especially for nonlinear systems	algorithm;artificial neural network;pid	Jing Wang;Yourui Huang	2013	JNW	10.4304/jnw.8.8.1726-1733	artificial intelligence;machine learning	ML	29.693940495670315	-5.40569591138456	44817
bacc81b8b622b23f70c48c27e90813c4b240f245	influence analyses of skew-normal/independent linear mixed models	test hypothese;linear mixed model;error aleatorio;mesure influence;ciencias exatas e da terra;dato observacion;fonction vraisemblance;analyse multivariable;log likelihood;test statistique;skewness measure;theorie approximation;multivariate analysis;algorithm analysis;analisis datos;ciencia da computacao;random effects;log vraisemblance;ley n variables;technique diagnostic;test hipotesis;test estadistico;62h10;62e17;modele lineaire;modele mixte;statistical test;modelo lineal;curva gauss;distribucion estadistica;erreur aleatoire;funcion verosimilitud;estimation parametrique;approximation theory;data analysis;perturbacion;efecto aleatorio;distribution statistique;mixed model;diagnostic techniques;random effect;influence measure;mesure asymetrie;statistical computation;62f03;linear model;06axx;calculo estadistico;local influence;loi normale;algorithme em;influence locale;invariante;analisis multivariable;multivariate distribution;analyse donnee;random error;calcul statistique;analyse algorithme;algoritmo em;donnee observation;perturbation;modelo mixto;effet aleatoire;em algorithm;loi n variables;statistical distribution;likelihood function;observation data;gaussian distribution;invariant;analisis algoritmo;hypothesis test	A extension of some diagnostic procedures to skew-normal/independent linear mixed models is discussed. This class provides a useful generalization of normal (and skew-normal) linear mixed models since it is assumed that the random effects and the random error terms follow jointly a multivariate skew-normal/independent distribution. Inspired by the EM algorithm, a local influence analysis for linear mixed models, following Zhu and Lee's approach is developed. This is because the observed data log-likelihood function associated with the proposed model is somewhat complex and Cook's well-known approach can be very difficult for obtaining measures of local influence. Moreover, the local influence measures obtained under this approach are invariant under reparameterization. Four specific perturbation schemes are also discussed. Finally, a real data set is analyzed in order to illustrate the usefulness of the proposed methodology.	estimation theory;expectation–maximization algorithm;hessian;mixed model	Camila Borelli Zeller;Filidor V. Labra;Victor H. Lachos;Narayanaswamy Balakrishnan	2010	Computational Statistics & Data Analysis	10.1016/j.csda.2009.11.008	mixed model;econometrics;generalized linear mixed model;statistical hypothesis testing;calculus;mathematics;statistics;random effects model	ML	32.96150899937675	-22.93293651129606	44870
4f92e659d3cdce9b9f668f03ca2f4943d7492349	when long memory meets the kalman filter: a comparative study	arfima models;state space methods;measurement error;kalman filter;level shifts;hb economic theory;time series;state space;long memory;comparative study;missing observations;finite sample properties;missing values;monte carlo simulation;data generation process	The finite sample properties of the state space methods applied to long memory time series are analyzed through Monte Carlo simulations. The state space setup allows to introduce a novel modeling approach in the long memory framework, which directly tackles measurement errors and random level shifts. Missing values and several alternative sources of misspecification are also considered. It emerges that the state space methodology provides a valuable alternative for the estimation of the long memory models, under different data generating processes, which are common in financial and economic series. Two empirical applications highlight the practical usefulness of the proposed state space methods.		Stefano Grassi;Paolo Santucci de Magistris	2014	Computational Statistics & Data Analysis	10.1016/j.csda.2012.10.018	kalman filter;econometrics;simulation;state space;comparative research;time series;mathematics;statistics;monte carlo method;observational error	ML	28.323605509839506	-21.144509546656067	44934
66c8ff1a00d57293ca48a2167b6348585a215242	hierarchical search for parsing	search error;relative strength;parsing use;certain threshold;hierarchical a;simple grammar;hierarchical search;multi-level hierarchy;coarse-to-fine case;agenda-based framework;moderate level	Both coarse-to-fine and A∗ parsing use simple grammars to guide search in complex ones. We compare the two approaches in a common, agenda-based framework, demonstrating the tradeoffs and relative strengths of each method. Overall, coarse-to-fine is much faster for moderate levels of search errors, but below a certain threshold A∗ is superior. In addition, we present the first experiments on hierarchical A∗ parsing, in which computation of heuristics is itself guided by meta-heuristics. Multi-level hierarchies are helpful in both approaches, but are more effective in the coarseto-fine case because of accumulated slack in A∗ heuristics.	charge trap flash;computation;experiment;heuristic (computer science);parsing;slack variable	Adam Pauls;Dan Klein	2009			computer science;bottom-up parsing;theoretical computer science;machine learning;algorithm	NLP	31.646109870117574	0.45496002319067286	44935
9cf611996f4547b524f193092b2db3fa6d299185	complexity reduction methods for fast motion estimation in hevc	early termination;motion estimation;hevc;dynamic search range;hexagon search pattern;fine refinement	Motion estimation is one of the most demanding and complex tools in block based video encoders. Variable block size motion estimation (ME) and multiple reference frames in H.264/AVC make motion estimation even more complex and time consuming. In HEVC, the complexity is even higher since there are more block sizes. This paper presents an analysis of various tools involved in some fast ME algorithms and proposes some improvements to them in order to achieve a novel fast hybrid algorithm. The proposed algorithm has been tested with HEVC reference software. Simulation results show that the algorithm achieves up to 44.7% decrease in ME complexity when compared to the fast ME algorithm (Test Zone Search or TZSearch) and up to 99% reduction in ME complexity compared to full search algorithm with negligible loss in PSNR and bitrate. Motion estimation is one of the most complex tools in block based video encoders.In HEVC, the motion estimation complexity is even higher since there are more block sizes.TZSearch algorithm is the fast motion estimation algorithm in HEVC reference software.This paper reduces the complexity of TZSearch ME algorithm upto 44.7%.The overall RD performance loss is negligible despite decrease in the complexity.		Purnachand Nalluri;Luis Nero Alves;Antonio Navarro	2015	Sig. Proc.: Image Comm.	10.1016/j.image.2015.09.015	computer vision;real-time computing;simulation;quarter-pixel motion;computer science;theoretical computer science;motion estimation	Vision	47.44916952404273	-19.318393300306163	44951
1574ebe523d46d59e9d7f7205930b5fec98337c2	incorporating directional information within a differential evolution algorithm for multi-objective optimization	differential evolution;multi objective optimization;non dominated sorting genetic algorithm;evolutionary computing	The field of Differential Evolution (DE) has demonstrated important advantages in single objective optimization. To date, no previous research has explored how the unique characteristics of DE can be applied to multi-objective optimization. This paper explains and demonstrates how DE can provide advantages in multi-objective optimization using directional information. We present three novel DE variants for multi-objective optimization, and a report of their performance on four multi-objective problems with different characteristics. The DE variants are compared with the NSGA-II (Nondominated Sorting Genetic Algorithm). The results suggest that directional information yields improvements in convergence speed and spread of solutions.	differential evolution;genetic algorithm;mathematical optimization;multi-objective optimization;sorting	Antony W. Iorio;Xiaodong Li	2006		10.1145/1143997.1144119	differential evolution;mathematical optimization;multi-swarm optimization;test functions for optimization;meta-optimization;computer science;derivative-free optimization;artificial intelligence;multi-objective optimization;machine learning;mathematics;evolutionary computation	Visualization	25.060840632310857	-4.343692045350291	44992
0120b8d1c3c29af1878b4cc53f0d65ea402b5520	study of growth pattern of cattle under different error structures		Logistic and Gompertz growth models are fitted in growth data for Friesian × Sahiwal (F × S) and Friesian × Sahiwal × Haryana (F × S × H) breed at Agra station, Gompertz model gives better fit than Logistic model. The Generalized Least Squares (GLS) estimates are found to be more precise than Ordinary Least Squares (OLS) estimates for both Logistic as well as Gompertz model under heteroscedastic error condition for both breeds. Growth is found better for F × S × H breed, therefore, triple cross may have increased maturing rate while asymptotic weight (mature weight) is found better for F × S breed. Breed type significantly affects weight at maturity.		Surendra Singh;A. K. Paul;Ranjit Kumar Paul;L. M. Bhar;Ashok Kumar;Wasi Alam	2015	MASA	10.3233/MAS-140318	econometrics;geography;operations management;genealogy	ML	25.622793428940014	-21.437517177139775	44997
dc47b5902acbb44ce9033b71e5b4c3d4e0a5bfec	a bivariate gompertz regression model with shared gamma frailty for censored data		The Gompertz distribution has many applications, particularly in medical and actuarial studies. However, there has been little recent work on the Gompertz in comparison with its early investigation. The problem of analyzing and estimating parameters of bivariate Gompertz distribution with shared frailty is of interest and the focus of this paper. We propose maximum likelihood estimation procedure for a bivariate survival model in which dependence is generated by a gamma distribution. We assume that the bivariate survival times follow bivariate Gompertz distribution and distribution of censoring variable is independent of the two life times. A search of the literature suggests there is currently no bivariate Gompertz distribution with shared frailty.	bivariate data;censoring (statistics);estimation theory;gamma correction;gompertz function	David D. Hanagal;Richa Sharma	2012	MASA	10.3233/MAS-2011-0220	econometrics;demography;gompertz function;gompertz distribution;statistics	Metrics	30.61687200307215	-22.4949518441711	45072
92a518525a7e4b990143501be15000740211a5d4	the importance of being (a little bit) discrete	stochastic noise;concurrent constraint programming;biological modeling;hybrid system;biological systems;circadian clock;robustness;hybrid automata;discreteness;hybrid systems	We compare the hybrid, stochastic, and differential semantics for stochastic Concurrent Constraint Programming, focussing on the exhibited behavior of models and their robustness. By investigating in detail two case studies, a circadian clock model and the Repressilator, we comment on the effect of the introduction of a limited amount of discreteness in the description of biological systems with hybrid automata. Experimental evidence suggests that discreteness increases robustness of the models.	automata theory;biological system;concurrent constraint logic programming;constraint programming;hybrid automaton;repressilator;stochastic process	Luca Bortolussi;Alberto Policriti	2009	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2009.02.006	theoretical computer science;control theory;mathematics;algorithm;hybrid system	AI	33.503539603200174	-12.388572315247707	45151
ca29b63af9629c7d68ded0bb29597481ba477705	transcoding resilient video watermarking scheme based on spatio-temporal hvs and dct	visual attention region;video transcoding;human visual system;video watermarking;motion distortion threshold	Video transcoding is a legitimate operation widely used to modify video format in order to access the video content in the end-user's devices, which may have some limitations in the spatial and temporal resolutions, bit-rate and video coding standards. In many previous watermarking algorithms the embedded watermark is not able to survive video transcoding, because this operation is a combination of some aggressive attacks, especially when lower bit-rate coding is required in the target device. As a consequence of the transcoding operation, the embedded watermark may be lost. This paper proposes a robust video watermarking scheme against video transcoding performed on baseband domain. In order to obtain the watermark robustness against video transcoding, four criteria based on Human Visual System (HVS) are employed to embed a sufficiently robust watermark while preserving its imperceptibility. The quantization index modulation (QIM) algorithm is used to embed and detect the watermark in 2D-Discrete Cosine Transform (2D-DCT) domain. The watermark imperceptibility is evaluated by conventional peak signal to noise ratio (PSNR) and structural similarity index (SSIM), obtaining sufficiently good visual quality. Computer simulation results show the watermark robustness against video transcoding as well as common signal processing operations and intentional attacks for video sequences. & 2013 The Authors. Published by Elsevier B.V. All rights reserved. Published by Elsevier B.V. A er the terms of the	adaptive stepsize;algorithm;baseband;computer simulation;data compression;digital video;digital watermarking;discrete cosine transform;distortion;elegant degradation;embedded system;frame language;h.264/mpeg-4 avc;human visual system model;mpeg-2;modulation;neural coding;peak signal-to-noise ratio;signal processing;structural similarity;video coding format	Antonio Cedillo-Hernandez;Manuel Cedillo-Hernandez;Mireya S. García-Vázquez;Mariko Nakano-Miyatake;Héctor M. Pérez Meana;Alejandro Alvaro Ramírez-Acosta	2014	Signal Processing	10.1016/j.sigpro.2013.08.019	video compression picture types;computer vision;transcoding;computer science;video tracking;multimedia;human visual system model;video post-processing;video denoising;multiview video coding;computer graphics (images)	Graphics	43.99672809529626	-17.492095290484	45154
52c2f9b0212cfcad2a0bb450c4564e91ffd1fb1b	irrigation control based on model predictive control (mpc): formulation of theory and validation using weather forecast data and aquacrop model	irrigation;simple model;uncertainty;weather forecasts;model predictive control mpc;automation	This research proposes A THEORETICAL FRAMEWORK based on model predictive control (MPC) for irrigation control to minimize both root zone soil moisture deficit (RZSMD) and irrigation amount under a limited water supply. We (i) investigate means to incorporate direct measurements to MPC (ii) introduce two Robust MPC techniques e Certainty Equivalence control (CE) and Disturbance Affine Feedback Control (DA) e to mitigate the uncertainty of weather forecasts, and (iii) provide conditions to obtain two important theoretical aspects of MPC e feasibility and stability e in the context of irrigation control. Our results show that system identification enables automation while incorporating direct measurements. Both DA and CE minimize RZSMD and irrigation amount under uncertain weather forecasts and always maintain soil moisture above wilting point subject to water availability. The theoretical results are compared against the model AQUACROP, weather data and forecasts from Shepparton, Australia. We also discuss the performance of Robust MPC under different water availability, soil, crop conditions. In general, MPC shows to be a promising tool for irrigation control. © 2015 Elsevier Ltd. All rights reserved.	decision support system;numerical analysis;numerical weather prediction;optimal control;optimizing compiler;robustness (computer science);simulation;system identification;turing completeness;victoria (3d figure)	Dilini Delgoda;Hector M. Malano;Syed Khusro Saleem;Malka N. Halgamuge	2016	Environmental Modelling and Software	10.1016/j.envsoft.2015.12.012	control engineering;uncertainty;geology;hydrology;engineering;automation;control theory;irrigation;mathematics;statistics	Robotics	36.703379805428966	-6.121942491552017	45160
a6261789ad256a217c5c5ef53ed53e92ec2e494f	dirichlet mixture modeling to estimate an empirical lower bound for lsf quantization	performance bound;line spectral frequency;signalbehandling;vector quantization;signal processing;dirichlet mixture model	The line spectral frequencies (LSFs) are commonly used for the linear predictive/autoregressive model in speech and audio coding. Recently, probability density function (PDF)-optimized vector quantization (VQ) has been studied intensively for quantization of LSF parameters. In this paper, we study the VQ performance bound of the LSF parameters. The LSF parameters are transformed to the @D LSF domain and the underlying distribution of the @DLSF parameters is modeled by a Dirichlet mixture model (DMM) with a finite number of mixture components. The quantization distortion, in terms of the mean squared error (MSE), is calculated with high rate theory. For LSF quantization, the mapping relation between the perceptually motivated log spectral distortion (LSD) and the MSE is empirically approximated by a polynomial. With this mapping function, the minimum required bit rate (an empirical lower bound) for transparent coding of the LSF under DMM modeling is derived.	lsf;quantization (signal processing)	Zhanyu Ma;Saikat Chatterjee;W. Bastiaan Kleijn;Jun Guo	2014	Signal Processing	10.1016/j.sigpro.2014.04.023	speech recognition;computer science;machine learning;signal processing;pattern recognition;mathematics;vector quantization;statistics	ML	49.060863959613016	-10.263793578850196	45206
3c237d5a6b006413f591fec235159b9ae4e7cfec	probabilistic clustering via pareto solutions and significance tests		The present paper proposes a new strategy for probabilistic (often called model-based) clustering. It is well known that local maxima of mixture likelihoods can be used to partition an underlying data set. However, local maxima are rarely unique. Therefore, it remains to select the reasonable solutions, and in particular the desired one. Credible partitions are usually recognized by separation (and cohesion) of their clusters. We use here the p values provided by the classical tests of Wilks, Hotelling, and Behrens–Fisher to single out those solutions that are well separated by location. It has been shown that reasonable solutions to a clustering problem are related to Pareto points in a plot of scale balance vs. model fit of all local maxima. We briefly review this theory and propose as solutions all well-fitting Pareto points in the set of local maxima separated by location in the above sense. We also design a new iterative, parameter-free cutting plane algorithm for the multivariate Behrens–Fisher problem.	algorithm;cluster analysis;cohesion (computer science);cutting-plane method;integer programming;iterative method;maxima and minima;pareto efficiency	María Teresa Gallegos;Gunter Ritter	2018	Adv. Data Analysis and Classification	10.1007/s11634-016-0278-2	econometrics;mathematical optimization;mathematics;statistics	ML	29.37187629400171	-12.39790357860549	45281
ae0a57ba7038b7948b1823532f39a4b665cfa200	joint doa estimation and source signal tracking with kalman filtering and regularized qrd rls algorithm	recursive estimation;least squares approximations;nonlinear programming;kalman filters;recursive least square rls autoregressive ar model direction of arrival doa estimation and tracking kalman filter kf qr decomposition;direction of arrival estimation estimation arrays sensors vectors target tracking kalman filters;autoregressive processes;computational complexity;recursive estimation autoregressive processes computational complexity direction of arrival estimation kalman filters least squares approximations nonlinear programming object tracking;object tracking;computational complexity joint doa estimation source signal tracking regularized qrd rls algorithm signal direction of arrival tracking signal direction of arrival estimation sensor array autoregressive processes ar process celebrated kalman filter qr decomposition based recursive least square technique ar coefficients observed time interval ar modeled sources antennas symmetric array complex valued nonlinear problem;direction of arrival estimation	In this brief, we present a nontraditional approach for estimating and tracking signal direction-of-arrival (DOA) using an array of sensors. The proposed method consists of two stages: in the first stage, the sources modeled by autoregressive (AR) processes are estimated by the celebrated Kalman filter, and in the second stage, the efficient QR-decomposition-based recursive least square (QRD-RLS) technique is employed to estimate the DOAs and AR coefficients in each observed time interval. The AR-modeled sources can provide useful temporal information to handle cases such as the number of sources being larger than the number of antennas. In addition, the symmetric array enables one to transfer a complex-valued nonlinear problem to a real-valued linear one, which can reduce the computational complexity. Simulation results demonstrate the superior performance of the algorithm for estimating and tracking DOA under different scenarios.	algorithm;autoregressive model;coefficient;computational complexity theory;direction of arrival;kalman filter;kernel density estimation;nonlinear system;qr decomposition;recursion;recursive least squares filter;sensor;simulation	Jian-Feng Gu;Shing-Chow Chan;Jun Yan;M. N. Shanmukha Swamy	2013	IEEE Transactions on Circuits and Systems II: Express Briefs	10.1109/TCSII.2012.2234874	kalman filter;mathematical optimization;nonlinear programming;computer science;video tracking;control theory;mathematics;moving horizon estimation;computational complexity theory;statistics;recursive least squares filter	Robotics	52.13227586636301	3.0977034260056304	45331
3a14c83047d361c950e3cad69b387ef79a4e0d2b	a novel video encryption technique based on secret sharing	discrete cosine transform dct;secret sharing;video encryption algorithm;mpeg video;transform coding;indexing terms;discrete cosine transform;video coding;internet;video data protection;streaming media;digital content;digitized content;discrete cosine transforms;cryptography;video distribution;multimedia communication;multimedia security;shamir s secret sharing scheme video encryption technique internet digitized content video distribution video data protection video encryption algorithm mpeg video;mpeg;shamir s secret sharing scheme;security;discrete cosine transform dct multimedia security mpeg secret sharing cryptography;video coding cryptography internet;cryptography video sharing video compression discrete cosine transforms internet multimedia communication information technology protection data security codecs;video encryption technique;real time systems	The rapid growth of Internet and digitized content has made video distribution easy. Hence the need for video data protection is on the rise. In this paper, we propose a secure and computationally feasible video encryption algorithm based on the method of Secret Sharing. In an MPEG video, the strength of the DC is distributed among the AC values based on Shamir's Secret Sharing (SSS) scheme. The proposed algorithm guarantees security, speed and error tolerance with a small increase in video size.	algorithm;encryption;error-tolerant design;information privacy;internet;moving picture experts group;shamir's secret sharing	Chigullapally Narsimha Raju;Ganugula Umadevi;K. Srinathan;C. V. Jawahar	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4712460	transform coding;computer science;cryptography;information security;theoretical computer science;discrete cosine transform;homomorphic secret sharing;multimedia;secure multi-party computation;internet privacy;proactive secret sharing;secret sharing;algorithm;statistics	EDA	37.92075962632487	-12.674615706267783	45363
8ac3b04b3dbbeb69c3f1111475a59c16c512f04e	a fast motion estimation algorithm based on adaptive pattern and search priority	motion estimation;video coding;block matching;search priority;adaptive pattern	Block matching algorithm (BMA) has been widely used in motion estimation for various video coding standards since it can remove temporal redundancy effectively. However, motion estimation is the key problem in realizing real-time video coding due to the high computation complexity of BMA. In this manuscript, we present a fast motion estimation algorithm according to the adaptive pattern and search priority (APSP). Based on the distribution characteristics of motion vector (MV) that achieved by a series of experiments, the improved algorithm defines different efficient patterns and adopts the appropriate pattern adaptively. Firstly, the search can be stopped after checking one point by the features of the current block. And then the starting pattern is determined based on the motion vectors from the neighboring blocks. The subsequent pattern can be further adjusted according to the current best matching point. Furthermore, the proposed method assigns search priority to each point of every pattern. Therefore, the search is performed under the guidance of the search priority, with the result that each pattern can be interrupted in any position by using priority and threshold. Compared to conventional fast algorithms, the experimental results demonstrate that the proposed algorithm improves the performance of the search algorithm with significant reduction in computational complexity on the premise of ensuring the image quality and searching precision.	block-matching algorithm;computation;computational complexity theory;data compression;experiment;fast fourier transform;image quality;interrupt;motion estimation;pattern matching;real-time clock;search algorithm;shortest path problem;time complexity;video coding format	Jun Luo;Xiaohua Yang;Liheng Liu	2014	Multimedia Tools and Applications	10.1007/s11042-014-2280-z	computer vision;mathematical optimization;real-time computing;quarter-pixel motion;computer science;theoretical computer science;motion estimation;block-matching algorithm;best-first search	Vision	48.249100116192864	-19.310743451726523	45380
6d8ab6951203dc851e2a1a156b6eae4bd38b082c	robust optimization of noisy blackbox problems using the mesh adaptive direct search algorithm		Blackbox optimization problems are often contaminated with numerical noise, and direct search methods such as the Mesh Adaptive Direct Search (MADS) algorithm may get stuck at solutions artificially created by the noise. We propose a way to smooth out the objective function of an unconstrained problem using previously evaluated function evaluations, rather than resampling points. The new algorithm, called Robust-MADS is applied to noisy problems from the literature.	mathematical optimization;numerical analysis;optimization problem;robust optimization;search algorithm	Charles Audet;Amina Ihaddadene;Sébastien Le Digabel;Christophe Tribes	2018	Optimization Letters	10.1007/s11590-017-1226-6	mathematical optimization;theoretical computer science;machine learning	Vision	29.986265362026547	-0.3747383992178415	45469
15e7f233088f52dffadc0b07b5ff90ad69a71323	reducing metric sensitivity in randomized trajectory design	optimisation;state space methods;high dimensionality;path planning;global constraint;nonlinear systems randomized trajectory trajectory planning state space randomized planning rapidly exploring random trees heuristics greedy exploration strategy spacecraft navigation vehicle dynamics;performance improvement;nonlinear systems;dynamics;rapidly exploring random tree;sensitivity analysis;state space;nonlinear equation;optimisation path planning dynamics state space methods nonlinear systems road vehicles space vehicles sensitivity analysis;state space methods algorithm design and analysis remotely operated vehicles vehicle dynamics mobile robots nonlinear systems orbital robotics cities and towns dynamic programming computer science;vehicle dynamics;space vehicles;road vehicles	This paper addresses trajectory design for generic problems that involve: 1) complicated global constraints that include nonconvex obstacles, 2) nonlinear equations of motion that involve substantial drift due to momentum, 3) a high-dimensional state space. Our approach to this challenging problems is to develop randomized planning algorithms on the basis of Rapidlyexploring Random Trees (RRTs). RRTs use metricinduced heuristics to conduct a greedy exploration of the state space; however, performance substantially degrades when the chosen metric does not adequately reflect the true cost-to-go. In this paper, we present an adaptive version of the RRT that is capable of refining its exploration strategy in the presence of a poor metric. Initial experiments on problems in vehicle dynamics and spacecraft navigation indicate substantial performance improvement over existing techniques.	experiment;greedy algorithm;heuristic (computer science);nonlinear system;randomized algorithm;state space	Peng Cheng;Steven M. LaValle	2001		10.1109/IROS.2001.973334	control engineering;rapidly exploring random tree;mathematical optimization;dynamics;vehicle dynamics;simulation;nonlinear system;computer science;state space;artificial intelligence;control theory;mathematics;motion planning;sensitivity analysis	Robotics	52.06244015906893	-22.775893901168743	45481
7fe01bee67b0fadfd840cb1697382d9212d90b2c	shape-adaptive dct with block-based dc separation and δdc correction	metodo adaptativo;transformation cosinus;error correction codes;transformacion discreta;image segmentation;image processing;coding errors;procesamiento imagen;methode adaptative;discrete cosine transforms mpeg 4 standard image coding iso standards iec standards video coding image segmentation shape quantization noise shaping;code standards;object based video coding;transform coding;indexing terms;traitement image;discrete cosine transform;algorithme;standardisation;algorithm;video coding;codificacion;adaptive signal processing;adaptive signal processing video coding discrete cosine transforms transform coding code standards telecommunication standards coding errors error correction codes standardisation image segmentation;object oriented;discrete cosine transforms;telecommunication standards;segmented video spl delta dc correction block based dc separation mpeg 4 standardization shape adaptive dct algorithm video verification model mpeg 4 performance limitation spl delta dc sa dct video coding;adaptive method;transformacion coseno;coding;discrete transformation;oriente objet;cosine transform;orientado objeto;transformation discrete;verification model;codage;algoritmo	This paper refers to a shape-adaptive DCT algorithm (SA-DCT) originally proposed by Sikora and Makai (see ibid., vol.5, p.59-62, 1995). The SA-DCT has been developed in the framework of the ongoing MPEG-4 standardization phase of ISO/IEC, and has been included in the video verification model of MPEG-4. In this context, the focus of the paper is to emphasize a systematic performance limitation of conventional SA-DCT, and to propose an extended version (/spl Delta/DC-SA-DCT) which avoids this restriction in general. This modification considerably improves the efficiency of SA-DCT, and can easily be implemented in existing SA-DCT tools.	discrete cosine transform	Peter Kauff;Klaas Schüür	1998	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.678616	speech recognition;image processing;computer science;theoretical computer science;discrete cosine transform;algorithm	Arch	45.91144407358349	-14.190462876408635	45590
37eabf19be0c4f1b536aad379048f9e76ce95a19	an analysis of perceptual artifacts in mpeg scalable audio coding	transform weighted interleaved vector quantization perceptual artifacts mpeg scalable audio coding mpeg advanced audio coder bit slice scalable arithmetic coding human subjective testing comparison category rating mid range tonal signal reng probing time frequency decomposition linear time invariant component nonlinear time varying component time frequency analysis suboptimal bit allocation;multirate system;time varying;arithmetic coding;vector quantisation audio coding arithmetic codes time frequency analysis;time frequency;audio coding bit rate performance evaluation testing time frequency analysis arithmetic mpeg 4 standard humans nonlinear distortion time varying systems;human subjects;audio coding;arithmetic codes;linear time invariant;bit allocation;vector quantisation;time frequency analysis	In this paper we study coding artifacts in MPEG-compressed scalable audio. Specifically, we consider the MPEG advanced audio coder (AAC) using bit slice scalable arithmetic coding (BSAC) as implemented in the MPEG 4 reference software. First, we perform human subjective testing using the comparison category rating (CCR) approach, quantitatively comparing the performance of scalable BSAC with the nonscalable TwinVQ and AAC algorithms. This testing indicates that scalable BSAC performs very poorly relative to TwinVQ at the lowest bitrate considered (16 kb/s) largely because of an annoying and seemingly random mid-range tonal signal that is superimposed onto the desired output. In order to better understand and perceptually quantify the various forms of distortion introduced into compressed audio at low bit rates, we apply two analysis techniques: Reng probing and time-frequency decomposition. The Reng probing technique is capable of separating the linear time-invariant component of a multirate system from its nonlinear and periodically time-varying components. Using this technique, we conclude that aliasing is probably not the cause of the annoying tonal signal; instead, time-frequency analysis indicates that its cause is most likely suboptimal bit allocation.	advanced audio coding;algorithm;aliasing;arithmetic coding;bit slicing;data rate units;distortion;frequency analysis;linear time-invariant theory;moving picture experts group;nonlinear system;scalability;time complexity;time-invariant system;time–frequency analysis	Charles D. Creusere	2002		10.1109/DCC.2002.999953	sub-band coding;speech recognition;time–frequency analysis;computer science;theoretical computer science;mathematics	SE	47.49839339670877	-9.504285496046315	45635
64a655921aa336bb615d4387d9355548ba1bdcdd	on the interval estimation of stress-strength reliability for exponentiated scale family of distributions	exponentiated exponential distribution;generalized pivot quantity;warranty period;maximal scale invariant likelihood estimator			K. P. Patil;H. V. Kulkarni	2017	Quality and Reliability Eng. Int.	10.1002/qre.2117	econometrics;mathematical optimization;exponentiated weibull distribution;mathematics;statistics	DB	31.42406869812473	-21.47035621650869	45681
d5749a231c79651c4036bed468dd186f94b31dd6	a new core and delta detection for fingerprints using the extended relation graph	tecnologia electronica telecomunicaciones;delta;core;extended relational graph;directional image;fingerprint;tecnologias;grupo a	A new detection methodology for both of the core and the delta of the fingerprint using the extended relational graph is presented. This paper shows the way to detect both of the core loop and the delta loop from the extended relational graph, which we proposed in order to summarize the global feature of the fingerprint ridge pattern distribution. The experimental results for 180 fingerprint samples show that the processing time is ranging from 0.34 [sec] to 0.44 [sec] for each fingerprint image by using Pentium 4 1.8 GHz Processor. In our experiments, the core and the delta were successfully extracted in 94.4% of the 180 samples.	fingerprint	Tomohiko Ohtsuka;Akiyoshi Kondo	2005	IEICE Transactions	10.1093/ietfec/e88-a.10.2587	embedded system;core;fingerprint;real-time computing;delta;computer science;distributed computing	Robotics	42.7510519097818	-1.9747194932124001	45693
66e6ed651aa2bd796016b1d3d740689bb20618a1	digital forensic technique for double compression based jpeg image forgery detection		In today’s cyber world images and videos are the major sources of information exchange. The authenticity of digital images and videos is extremely crucial in the legal industry, media world and broadcast industry. However, with huge proliferation of low-cost, easy–to–use image manipulating software the fidelity of digital images is at stake. In this paper we propose a technique to detect digital forgery in JPEG images, based on ”double–compression”. We deal with JPEG images because JPEG is the standard storage format used in almost all present day digital cameras and other image acquisition devices. JPEG compresses an image to optimize the storage space requirement. When an attacker or criminal alters some part of a JPEG image by any image–editing tool and rewrites it to memory, the forged or modified part gets doubly–compressed. In this paper, we exploit this double–compression in JPEG images to identify digital forgery.	authentication	Pankaj Malviya;Ruchira Naskar	2014		10.1007/978-3-319-13841-1_25	computer vision;jpeg;internet privacy;computer security	Vision	38.596986007072815	-12.578959720365662	45737
612ae804474362484fd2591917f8954f4a6b65f8	polynomial regression and measurement error: implications for is research			polynomial	Miguel I. Aguirre-Urreta;Jiang Hu;Mikko Rönkkö	2017			observational error;computer science;econometrics;statistics;polynomial regression	Metrics	28.7020045557945	-21.182702581751673	45761
7a0d01d9c8e5e4f9e1ae0f82ce2f93a4fde24dd4	digital watermarking in joint time-frequency domain	digital watermarking;watermarking;spectral domain;filtering;inverse dct;image coding;spatial domain;data compression;inverse wigner problem;digital watermark;frequency domain analysis;signal analysis;time frequency;time frequency domain;watermarking time frequency analysis discrete cosine transforms signal synthesis image quality frequency domain analysis filtering resilience security signal analysis;transform coding;spatial domain digital watermarking time frequency domain signal synthesis filtering time frequency distributions wigner distribution image quality jpeg inverse dct security spectral domain inverse wigner problem;resilience;jpeg;wigner distribution;discrete cosine transforms;image quality;time frequency distributions;image watermarking;signal synthesis;security;security of data wigner distribution watermarking time frequency analysis data compression image coding transform coding inverse problems;time frequency analysis;security of data;time frequency distribution;inverse problems	1. NSF CAREER Award 2008. 2. Withrow Teaching Award, Department of Electrical and Computer Engineering, Michigan State University, 03/05. 3. Distinguished Achievement Award, College of Engineering, University of Michigan, 03/01. 4. Barbour Scholar, Rackham School of Graduate Studies, University of Michigan, 09/0005/01. 5. Miller Fellowship, Department of Electrical Engineering and Computer Science, University of Michigan, 1/99-12/99. 6. Sloan Summer Research Fellowship for Women in Engineering and Sciences, 05/9809/98.	computer engineering;computer science;digital watermarking;ibm notes;time–frequency analysis	Bijan G. Mobasseri	2002		10.1109/ICIP.2002.1039012	computer vision;digital watermarking;computer science;information security;theoretical computer science;signal processing;mathematics;psychological resilience	DB	41.60087747680298	-8.45606967776022	45762
cbc96070e0e89b99f6950349844698c273c758aa	analysis of split-plot designs: an overview and comparison of methods	split plot designs;fractional factorial designs;response surface models;mixed models;anova	Abstract#R##N##R##N#Split-plot designs are frequently needed in practice because of practical limitations and issues related to cost. This imposes extra challenges on the experimenter, both when designing the experiment and when analysing the data, in particular for non-replicated cases. This paper is an overview and discussion of some of the most important methods for analysing split-plot data. The focus is on estimation, testing and model validation. Two examples from an industrial context are given to illustrate the most important techniques. Copyright © 2006 John Wiley & Sons, Ltd.		T. Næs;Are Halvor Aastveit;N. S. Sahni	2007	Quality and Reliability Eng. Int.	10.1002/qre.841	mixed model;econometrics;fractional factorial design;analysis of variance;computer science;mathematics;single-subject research;statistics	EDA	27.90392737356589	-21.28334332839786	45805
08e6e5d3c0937cca06413cb9138d6d995f6c1681	performance evaluation of different cost functions in motion vector estimation		Video is an important medium in terms of information sharing in this present era. The tremendous growth of video use can be seen in the traditional multimedia application as well as in many other applications like medical videos, surveillance video etc. Raw video data is usually large in size, which demands for video compression. In different video compressing schemes, motion vector is a very important step to remove the temporal redundancy. A frame is first divided into small blocks and then motion vector for each block is computed. The difference between two blocks is evaluated by different cost functions (i.e. mean absolute difference (MAD), mean square error (MSE) etc).In this paper the performance of different cost functions was evaluated and also the most suitable cost function for motion vector estimation was found. Performance Evaluation of Different Cost Functions in Motion Vector Estimation	closed-circuit television;data compression;loss function;mad;mean squared error;performance evaluation;uncompressed video	Suvojit Acharjee;Sayan Chakraborty;Wahiba Ben Abdessalem Karaa;Ahmad Taher Azar;Nilanjan Dey	2014	IJSSMET	10.4018/ijssmet.2014010103	mathematical optimization	Vision	48.01176047862259	-18.697095280287066	45935
dc99ac7bc9f558deb173d10d63a209dcf83a098f	adaptive fast intra mode decision of depth map coding by low complexity rd-cost in 3d-hevc	3d hevc;depth map coding;video coding probability rate distortion theory;intra prediction;encoding complexity theory videos three dimensional displays prediction algorithms video coding image coding;depth map coding multi view videos plus depth 3d hevc intra prediction rough mode decision;multi view videos plus depth;rough mode decision;depth coding time intra mode decision depth map coding low complexity rate distortion cost information low complexity rd 3d hevc coding efficiency intraprediction modes depth modelling modes dmm intramode decision algorithm rough mode decision	The coding efficiency of 3D-HEVC can be improved significantly by adopting more intra modes in depth map coding, including up to 35 conventional HEVC intra prediction modes and 4 depth modelling modes (DMMs). However, traversing these modes dramatically results in unendurable high complexity. In this paper, we propose an efficient intra mode decision algorithm by taking advantage of the Low Complexity Rate-Distortion Cost information in the rough mode decision. Only one conventional intra mode and several most probable modes are then selected for most cases adaptively based on the threshold derived from the prior probability. Experimental results show that the proposed algorithm provides about 34% saving in terms of the total depth coding time with little drop of the coding performance compared with the state-of-the-art algorithm.	algorithm;algorithmic efficiency;depth map;distortion;high efficiency video coding;intra-frame coding;marginal model;overhead (computing);rough set;ruby document format	Hongbin Zhang;Chang-Hong Fu;Weimin Su;Sik-Ho Tsang;Yui-Lam Chan	2015	2015 IEEE International Conference on Digital Signal Processing (DSP)	10.1109/ICDSP.2015.7251920	computer vision;computer science;theoretical computer science;machine learning;coding tree unit	Robotics	46.579464146961584	-19.23599297758063	46041
42eb55abecd7afbd191608d6633cd8dcf85d4daa	two group inspection-based control charts for dispersion matrix	synthetic control chart;generalized variance s;vmax;zero state;steady state	ABSTRACTIn this article, we propose the two control charts, i.e. the ‘VMAX Group Runs’ (VMAX-GR) and ‘VMAX Modified Group Runs’ (VMAX-MGR) control charts based on the bivariate normal processes, for monitoring the covariance matrix. The proposed charts give the faster detection of a process change and have better diagnostic feature. It is verified that the VMAX-GR and the VMAX-MGR charts give a significant reduction in the out-of-control ‘Average Run Length’(ARL) in the zero state, as well as in the steady state, as compared to the synthetic control chart based on the VMAX statistic and the generalized variance |S| chart.	chart	M. P. Gadre;V. C. Kakade	2018	Communications in Statistics - Simulation and Computation	10.1080/03610918.2017.1321120	econometrics;zero state response;steady state;statistics	Networks	28.634398246452314	-20.384694187192025	46048
0ff540de4b50b032f79d5b85579149c001113270	afmpb: an adaptive fast multipole poisson-boltzmann solver for calculating electrostatics in biomolecular systems	health research;uk clinical guidelines;iterative solver;biological patents;multipoles;electrostatics;general and miscellaneous mathematics computing and information science;ucsd;numerical solution;node patch method;system modeling;europe pubmed central;integral equations;fast multipole method;krylov subspace method;boundary integral equation;citation search;krylov subspace methods;mathematical methods and computing;poisson boltzmann;uk phd theses thesis;diagonal translations;electrostatic potential;life sciences;communication protocol;biological materials;algorithms;fortran;krylov subspace;fast multipole methods;uk research reports;medical journals;a codes;europe pmc;poisson boltzmann equation;biomedical research;bioinformatics	A Fortran program package is introduced for rapid evaluation of the electrostatic potentials and forces in biomolecular systems modeled by the linearized Poisson-Boltzmann equation. The numerical solver utilizes a well-conditioned boundary integral equation (BIE) formulation, a node-patch discretization scheme, a Krylov subspace iterative solver package with reverse communication protocols, and an adaptive new version of fast multipole method in which the exponential expansions are used to diagonalize the multipole to local translations. The program and its full description, as well as several closely related libraries and utility tools are available at http://lsec.cc.ac.cn/lubz/afmpb.html and a mirror site at http://mccammon.ucsd.edu/. This paper is a brief summary of the program: the algorithms, the implementation and the usage.	academy;algorithm;appendix;boundary element method;code;computation (action);computers;condition number;dspace;desktop computer;digit structure;discretization;disk mirroring;drug labeling;error message;experiment;fast multipole method;fortran;header of a document;ibm notes;imagery;include directive;input/output;integer (number);interaction;iterative method;krt10 wt allele;krylov subspace;language translations;libraries;limewire;mesh generation;multi-core processor;multiprocessing;node - plant part;numerical analysis;numerical method;numerous;octal;octree;open-source software;parallel computing;pointer (computer programming);pointer <dog>;poisson–boltzmann equation;population parameter;precomputation;protocols documentation;silicon;simulation;singular;solver;subroutine;systems science;tree structure;united states national institutes of health;vmd;yousef saad;exponential	Benzhuo Lu;Xiaolin Cheng;Jingfang Huang;James Andrew McCammon	2010	Computer physics communications	10.1016/j.cpc.2010.02.015	computational science;mathematical optimization;computer science;theoretical computer science;mathematics;physics;quantum mechanics;poisson–boltzmann equation;electrostatics	HPC	41.5698379576547	2.498290777922596	46055
47bff978e941c420f80ce6aa6056ccabe74705e7	spread spectrum watermark for color image based on wavelet tree structure	gaussian noise;spread spectrum communication watermarking color tree data structures frequency noise robustness discrete wavelet transforms noise reduction data mining filtering algorithms;watermarking;contrast enhanced;spread spectrum;image processing;random sequences;tree data structures;wavelet transforms;contrast enhancement spread spectrum watermark color image wavelet tree structure pseudo random sequence image denoising image watermarking salt and pepper noise image processing attacks jpeg compression gaussian noise;image colour analysis;tree structure;random sequence;watermark;spread spectrum watermarking;image denoising;spread spectrum watermark wavelet;wavelet transforms image colour analysis image denoising random sequences tree data structures watermarking;pepper;high frequency;wavelet;color image	This paper proposes a spread spectrum watermark for color image based on wavelet tree structure. It embeds a pseudo-random sequence representing one bit of the original watermark into a four-fork tree in the DWT of Y component of YUV. This distributes the information of one bit into several frequency sub-bands, such as low, intermediate and high frequency sub-band, which improves the robustness of watermark against different attacks. In addition, we apply denoising method to watermark image extracted from the attacked watermarked image to remove the salt-and-pepper noise introduced by the attacks to host image, which improves the NC of watermark. Experimental results prove that the algorithm is very robust to image processing attacks such as JPEG compression, Gaussian noise, filtering, cropping and contrast enhancement, and it has a good transparency.	algorithm;color image;digital watermarking;discrete wavelet transform;image processing;jpeg;noise reduction;pseudorandomness;salt-and-pepper noise;tree structure;wavelet tree	Qiujuan Liang;Zhizhong Ding	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.958	gaussian noise;wavelet;computer vision;color image;image processing;digital watermarking;computer science;theoretical computer science;random sequence;high frequency;pattern recognition;mathematics;tree structure;tree;watermark;spread spectrum;statistics;wavelet transform	Vision	41.412181254865935	-10.891994285592018	46102
06fddcf70a7802f4ed364a993df22c7b4aa9260b	high-efficiency mode decision procedure for h.264/avc under cloud computing environments	video coding encoding prediction algorithms computational complexity standards cloud computing;rate distortion cost rdcost cloud computing h 264 avc intraframe mode decision;rate distortion theory;video coding;computational complexity;mode decision procedure multimedia communication systems coding efficiency intraprediction block size intramode type selection algorithm computational stress optimal rate distortion cost computational complexity rate distortion optimization technique encoding performance video coding standard cloud computing environments h 264 avc;video coding cloud computing computational complexity rate distortion theory;cloud computing	The latest video coding standard, H.264/AVC, could achieve higher encoding performance compared with previous versions since it employs many tools to increase the coding accuracy, particularly the rate-distortion optimization technique for mode decision. However, enormous computational complexity is produced because all possible combinations of modes are calculated in order to find the optimal Rate-Distortion Cost, which will bring huge computational stress, particularly under cloud computing environments. To conquer this problem, a high-efficiency mode decision procedure for H.264/AVC under cloud computing environment is proposed in this paper. First, a fast intramode-type selection algorithm is presented to determine the intraprediction block size in advance. Then, the original multiple mode candidates for each prediction type are reduced at least by half with the information abstracted from macroblock and its 4×4 subblocks. The simulation results show that the proposed algorithm could reduce around 84% encoding time in average while maintaining the encoding performance efficiently with slight decrease in peak signal-to-noise ratio and increase in bit rate compared with H.264 reference software. It also produces better coding efficiency compared with the existing novel algorithms and could be widely used for multimedia communication systems with H.264/AVC standard under cloud computing environments.	algorithmic efficiency;block size (cryptography);cloud computing;color depth;computation;computational complexity theory;data compression;decision problem;distortion;encoder;h.264/mpeg-4 avc;mad;macroblock;mathematical optimization;mobile device;peak signal-to-noise ratio;rate–distortion optimization;real-time clock;selection algorithm;simulation;video coding format	Xingang Liu;Yinbo Liu;Wenjie Yang;Laurence Tianruo Yang	2014	IEEE Systems Journal	10.1109/JSYST.2013.2260642	scalable video coding;real-time computing;simulation;rate–distortion theory;cloud computing;computer science;theoretical computer science;operating system;context-adaptive variable-length coding;context-adaptive binary arithmetic coding;computational complexity theory	HPC	46.757052601054546	-19.049514434759864	46125
696f0c87f161eebef6835c55b5657e6473eee54f	16x16 macroblock partition size prediction for h.264 p slices	decision models;rate distortion;motion estimation;indexing terms;visual quality;variable block size;video coding;statistical analysis;motion estimation encoding bit rate rate distortion video coding automatic voltage control quantization statistical analysis partitioning algorithms degradation;computational complexity;decision model;intra coding modes;video coding computational complexity motion estimation statistical analysis;inter coding modes;decision model macroblock partition size prediction h 264 p slices intra coding modes inter coding modes reduce complexity motion estimation statistical analysis;mode decision;reduce complexity;macroblock partition size prediction;h 264 p slices	H.264 uses variable block sizes for macroblock partition. The H.264 encoder allows 2 intra coding modes and 5 inter coding modes for a macroblock in a P slice. This is the most essential part for high compression efficiency but requires many computations to decide the most suitable coding mode. In this paper, we present a fast mode decision algorithm for H.264 P slices. To reduce complexity, we analyze the results of P16times16 ME (motion estimation) of a macroblock according to the proposed decision model, and if this macroblock is considered to be coded with a larger mode, we prevent motion estimations for smaller modes. From statistical analysis, we define a decision model which estimates whether the macroblock partition size is further divided or not. Our experimental results show that the proposed algorithm can achieve 11-63% computational savings without significant degradation of visual quality and increase in bit rate	algorithm;archive;computation;elegant degradation;encoder;h.264/mpeg-4 avc;intra-frame coding;macroblock;motion estimation;raster document object	Jongmin You;Wonkyun Kim;Jechang Jeong	2006	IEEE Transactions on Consumer Electronics	10.1109/TCE.2006.273159	decision model;real-time computing;computer science;theoretical computer science;machine learning;coding tree unit;macroblock	Robotics	46.88335278034961	-18.827017135383723	46128
2cc7a7fa541b92e166f44836e4cd7f6b77e98a77	improving software size estimates by using probabilistic pairwise comparison matrices	size distribution;estimation uncertainty;improving software size estimates;pairwise comparison technique;probabilistic pairwise comparison technique;probabilistic pairwise comparison matrices;paired ratio comparison matrices;original software size;actual delivery size;case study;general purpose estimation approach;monte carlo methods;monte carlo method;statistical distributions	The pairwise comparison technique is a general purpose estimation approach for capturing expert judgment. This approach can be generalized to a probabilistic version using Monte Carlo methods to produce estimates of size distributions. The probabilistic pairwise comparison technique enables the estimator to systematically incorporate both estimation uncertainty as well as any uncertainty that arises from using multiple historical analogies as reference modules. In addition to describing the methodology, the results of the case study are also included. This paper is an extension of the work presented in [Lum, K et al., (2003)] and shows how the original software size estimates compared to the actual delivery size. It also describes the techniques used to modify the approach based on lessons learned. The results because they are based on only one case do not validate the effectiveness of the proposed approach but are suggestive that the technique can be effective and support the conclusion that further research is worth pursuing.	angular resolution (graph drawing);monte carlo method;reference architecture;software sizing	Jairus Hihn;Karen T. Lum	2004	10th International Symposium on Software Metrics, 2004. Proceedings.	10.1109/METRIC.2004.1357898	econometrics;computer science;data mining;statistics;monte carlo method	SE	27.73224812193972	-18.257786280750853	46157
4ab569a372c8e3b455da990e46275a5d27ec0c7a	landscapes, embedded paths and evolutionary scheduling	search space;combinatorial problems;search strategy;scheduling problem;genetic algorithm	Summary. been applied to scheduling problems. Whenever we apply such an algorithm, we implicitly construct a landscape over which the search traverses. The nature of this landscape is not an invariant of the problem instance, but depends on a number of algorithmic choices—most obviously, the type of neighbourhood operator used as a means of exploring the search space. In this chapter, after discussing the basic ideas of a landscape, we show how this variation in the landscape of a particular instance manifests itself in terms of an important property—the number of local optima—and discuss ways in which local optima can be avoided. We then review evidence for a particular conformation of local optima in a variety of scheduling and other combinatorial problems—the ‘big valley’ property. We then turn to the question of how we can exploit such effects in terms of a fruitful search strategy—embedded path tracing—for flowshop scheduling problems. While many approaches could be taken, the one described here embeds the path tracing strategy within a genetic algorithm, and experimental evidence is presented that shows this approach to be capable of generating very high-quality solutions to different versions of the permutation flowshop scheduling problem. Finally, some recent research is reported into the use of data from the search trace that provides some clues as to the quality of the results found. In particular, it is possible to use data on the re-occurrence of previous local optima to estimate the total number of optima and, indirectly, to quantify the probability that a global optimum has been reached.	embedded system;scheduling (computing)	Colin R. Reeves	2007		10.1007/978-3-540-48584-1_2	nurse scheduling problem;mathematical optimization;combinatorics;dynamic priority scheduling;genetic algorithm scheduling;two-level scheduling;mathematics;distributed computing;best-first search;combinatorial search;search algorithm	Embedded	27.847054072108254	2.5168798327957873	46245
2ac87caacfd66f0694e0203a2ced9d9e3dcf57aa	a heuristic filter based on firefly algorithm for nonlinear state estimation	nonlinear filters;state estimation;filtering algorithms;heuristic algorithms;maximum likelihood detection;optimization	A new heuristic filter, called firefly filter, is proposed for state estimation of nonlinear stochastic systems. The new filter formulates the state estimation problem as a stochastic dynamic optimization and utilizes the firefly optimization algorithm to find and track the best estimation. The fireflies search the state space dynamically and are attracted to one other based on the perceived brightness. The performance of the proposed filter is evaluated for a set of benchmarks and the results are compared with the well-known filters like extended Kalman filter and particle filter, showing improvements in terms of estimation accuracy.	benchmark (computing);dynamic programming;extended kalman filter;firefly (cache coherence protocol);firefly algorithm;heuristic (computer science);mathematical optimization;nonlinear system;optimization problem;pf (firewall);particle filter;state space;stochastic process;whole earth 'lectronic link	Hadi Nobahari;Mohsen Raoufi;Alireza Sharifi	2016	2016 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2016.7850275	adaptive filter;mathematical optimization;invariant extended kalman filter;ensemble kalman filter;kernel adaptive filter;machine learning;filtering problem;control theory;mathematics;extended kalman filter;moving horizon estimation;filter design;alpha beta filter	EDA	31.845304882190216	-5.2848216704653455	46247
2ab37fe8e56ac44bca06433fdca85b98517004c6	image fusion based visible watermarking using dual-tree complex wavelet transform	transformation ondelette;filigranage numerique;digital watermarking;texture;steganographie;droit auteur;filigrana;aumentacion;digital watermark;espectro visible;image fusion;duality;low pass;copyright;augmentation;data fusion;filigrane;securite donnee;classification;qualite image;complex wavelet transform;visible spectrum;copyright protection;notices;dualite;steganography;esteganografia;subbanda;increase;fusion donnee;subband;robustesse;image quality;filigrana digital;textura;watermark;notification;robustness;dualidad;calidad imagen;image watermarking;transformacion ondita;spectre visible;fusion datos;sous bande;clasificacion;security of data;wavelet transformation;robustez;derecho autor;data security	Digital watermarking has been researched extensively due to its potential use for data security and copyright protection. Much of the literature has focused on developing invisible watermarking algorithms. However, not much has been done on visible watermarking. A visible watermark is apparently needed for copyright notification. This work proposes a new framework of visible watermarking based on image fusion, a common technique used in combining images acquired from different modalities. To better protect the host features and increase the robustness of the watermark, the dual-tree complex wavelet transform (DT-CWT) is used. A new classification strategy is proposed to classify complex wavelet coefficients into 6 classes with different perceptual significance. The embedding decision can be made based on the classification information. Small watermark coefficients are prohibited from embedding. In the host edges, the insertion of watermark energy is controlled by using the inversely proportional embedding scheme to better preserve the sensitive region, while in other regions, the embedding strength becomes stronger as texture activity increases. This work also addresses the problem of low-pass subband watermark embedding, which is a special issue of visible watermarking. Experimental results show that the proposed algorithm yields significantly superior image quality than the current DCT-based method.	complex wavelet transform;digital watermarking;image fusion	Yongjian Hu;Jiwu Huang;Sam Kwong;Yiu-Keung Chan	2003		10.1007/978-3-540-24624-4_7	computer vision;telecommunications;digital watermarking;computer science;electrical engineering;watermark;computer security	Robotics	42.14816227207577	-8.545081653859619	46347
91ac1e986984c6f70dba760c82fc390b0c1c0ae0	intra-daily information of range-based volatility for mem-garch	garch;garch model;volatility forecasting;moving average;indexation;multiplicative error model;conditional variance	Conventional GARCH modeling formulates an additive-error mean equation for daily return and an autoregressive moving-average specification for its conditional variance, without much consideration on the effects of intra-daily data. Using Engle's multiplicative-error model (MEM) formulation, range-based volatility is proposed as an intraday proxy for several GARCH frameworks. The performances of these different approaches for two 8-year market data sets: the S&P 500 and the NASDAQ composite index, are studied and compared. The impact of significant changes in intraday data has been found to reflect in the MEM-GARCH volatility. For some frameworks it is also possible to use lagged values of range-based volatility to delay the intraday effects in the conditional variance estimation.	volatility	Kai-Pui Lam;H. S. Ng	2009	Mathematics and Computers in Simulation	10.1016/j.matcom.2008.12.007	forward volatility;autoregressive conditional heteroskedasticity;econometrics;implied volatility;variance swap;volatility;conditional variance;mathematics;stochastic volatility;statistics	ECom	26.45100499479121	-20.901774573588444	46349
002b28c9577f08613103cc584827fa8adfed5361	a new method for tuning mutation and crossover rate in genetic algorithm		In this paper, an innovative way to solve the Travelling Salesman Problem is proposed. This method is based on Genetic Algorithms (GA) tuned with a fuzzy controller. A fuzzy controller is used to determine the rate of two main operators of GA. In other words, crossover and mutation rates in reproduction processes are determined by the mentioned fuzzy controller. The effectiveness of our method is demonstrated by comparing of our results with other methods.	genetic algorithm;travelling salesman problem	Giti Javadi;Ehsan Aminian	2017		10.1145/3055635.3056622	fuzzy logic;genetic algorithm;artificial intelligence;operator (computer programming);machine learning;control theory;computer science;mutation rate;travelling salesman problem;crossover;fuzzy control system;mathematical optimization	AI	25.667883166633857	-6.47965605310573	46371
955c65967fff236f088a402fd2613cdfb7c892cb	"""comments on """"notes on bit allocation in the time and frequency domains"""""""	frequency domain analysis;speech processing;speech coding;bit rate;chromium;signal processing;bandwidth;radio spectrum management;entropy;bit allocation;bit rate frequency domain analysis radio spectrum management speech processing entropy speech coding signal processing bandwidth chromium signal to noise ratio;signal to noise ratio;frequency domain	"""In """"Notes on Bit Allocation in the Time and Frequency Domains"""" [1], Gibson presents a historical survey of previous work on bit allocation in the time and frequency domains. In this comment, we would like to mention the relationship of the idea involved in our paper [2] to earlier papers on this subject and our research history concerning it."""		Masaaki Honda;Fumitada Itakura	1985	IEEE Trans. Acoustics, Speech, and Signal Processing	10.1109/TASSP.1985.1164745	computer vision;electronic engineering;speech recognition;telecommunications;harmonic vector excitation coding;computer science;signal processing;speech processing;frequency domain	Arch	49.02079660287163	-7.132040404321982	46382
023933a8bc9378c94c715dcf7338430c0eb1e934	direct search methods on parallel machines		This paper describes an approach to constructing derivative-free algorithms for unconstrained optimization that are easy to implement on parallel machines. A special feature of this approach is the ease with which algorithms can be generated to take advantage of any number of processors and to adapt to any cost ratio of communication to function evaluation. Numerical tests show speed-ups on two fronts. The cost of synchronization being minimal, the speed-up is almost linear with the addition of more processors, i.e., given a problem and a search strategy, the decrease in execution time is proportional to the number of processors added. Even more encouraging, however, is that different search strategies, devised to take advantage of additional (or more powerful) processors, may actually lead to dramatic improvements in the performance of the basic algorithm. Thus search strategies intended for many processors actually may generate algorithms that are better even when implemented sequentially. The key difference is that the additional processors are not used simply to enhance the performance of an inherently sequential algorithm; they are used to spur the design of ever more ambitious-and effective-search strategies. The algorithms given here are supported by a strong convergence theorem, promising computational results on a variety of problems, and an intuitively appealing interpretation as multidirectional line search methods.		John E. Dennis;Virginia Torczon	1991	SIAM Journal on Optimization	10.1137/0801027	mathematical optimization;theorem;convergence;numerical analysis;strategy;theoretical computer science;computation;sequence;mathematics;ratio;algorithm	Theory	29.04453841533261	-0.023625120483664608	46402
578eb798b7017581a273f03ee25f9c3f7c9b3e76	robust labeling methods for copy protection of images	storage system;multimedia;real time;copy protection;quality factor;data storage;structure determination;real time application;high frequency;color image	In the European project SMASH a mass multimedia storage device for home usage is being developed. The success of such a storage system depends not only on technical advances, but also on the existence of an adequate copy protection method. Copy protection for visual data requires fast and robust labeling techniques. In this paper, two new labeling techniques are proposed. The first method extends an existing spatial labeling technique. This technique divides the image into blocks and searches an optimal label- embedding level for each block instead of using a fixed embedding-level for the complete image. The embedding-level for each block is dependent on a lower quality JPEG compressed version of the labeled block. The second method removes high frequency DCT-coefficients in some areas to embed a label. A JPEG quality factor and the local image structure determine how many coefficients are discarded during the labeling process. Using both methods a perceptually invisible label of a few hundred bits was embedded in a set of true color images. The label added by the spatial method is very robust against JPEG compression. However, this method is not suitable for real-time applications. Although the second DCT-based method is slightly less resistant to JPEG compression, it is more resistant to line-shifting and cropping than the first one and is suitable for real-time labeling.© (1997) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	copy protection	Gerhard C. Langelaar;Jan C. A. van der Lubbe;Reginald L. Lagendijk	1997		10.1117/12.263418	telecommunications;computer science;world wide web;computer graphics (images)	Vision	40.07153593172111	-14.336313102169864	46407
366e8e0ed815c77628f52e3fc71792f82aec79af	an enhanced bspline based zernike moment evaluations with watermarking applications	pseudo zernike moments;watermarking;image recognition;least squares approximations;interpolation;psnr;image classification;psnr watermarking image recognition interpolation robustness transforms visualization;splines mathematics;visualization;moments;noise attack b spline based zernike moment evaluations watermarking applications zm pattern recognition image classification moment order image processing applications b spline interpolation image pixel interpolation low frequency concentration feature least squares b spline decimated images rotation invariant feature rotation attacks natural preserving transform npt cropping attack compression attack;transforms;watermarking moments pseudo zernike moments;robustness;image watermarking;zernike polynomials;zernike polynomials image classification image watermarking interpolation least squares approximations splines mathematics	Zernike Moments (ZM), are extensively used in pattern recognition and image classification. However, they suffer from severe computational errors especially as the moment order increases. In this paper, in order to reduce the effects of these errors and make ZM suitable for image processing applications, it is proposed to compute ZM for a decimated image, and use Bspline interpolation to interpolate between image pixels. This significantly improves performance, due to its superior low frequency concentration feature and robust performance. Further improvements are also possible, by basing our computations on least squares Bspline decimated images, as well as optimizing some of ZM coefficients. It is also proposed to use the rotation invariant feature of ZM, in modifying a watermarking schemes to be insensitive to rotations or rotation attacks. In this respect, a watermarking technique based on using Natural Preserving Transform NPT, known for its robustness against cropping, compression and noise attacks is modified to be insensitive to rotation attacks. Several simulation experiments are given to verify these results.	b-spline;coefficient;computation;computer vision;decimation (signal processing);digital watermarking;experiment;image processing;interpolation;least squares;pattern recognition;pixel;simulation;watermark (data file)	Mamdouh F. Fahmy;M. A. Thabet	2013	IEEE International Symposium on Signal Processing and Information Technology	10.1109/ISSPIT.2013.6781877	computer vision;mathematical optimization;contextual image classification;discrete mathematics;visualization;peak signal-to-noise ratio;digital watermarking;interpolation;computer science;zernike polynomials;mathematics;moment;statistics;robustness	Vision	41.71262471735901	-11.209757855663288	46409
7dcc316c343a03858cc0d573e25ce119bf5558a7	wavelet based watermarking method for digital images using the human visual system	energy resolution;discrete wavelet transforms;gaussian noise;watermarking;image coding;discrete wavelet transform;jpeg compression;smoothing cropping collusion;decoding;digital watermark;copy protection;attack robustness wavelet based watermarking method digital images human visual system multiresolution watermarking method three level wavelet based method jpeg compression smoothing cropping collusion multiple watermarking;copyright;attack robustness;multimedia systems;random coding;power engineering and energy;wavelet transforms;human visual system;robustness;humans;three level wavelet based method;digital image;watermarking digital images humans visual system discrete wavelet transforms energy resolution image coding gaussian noise power engineering and energy robustness;multiresolution watermarking method;decoding copy protection copyright security of data wavelet transforms image coding;visual system;digital images;security of data;multiple watermarking;wavelet based watermarking method	Lm A wavelet based multiresolution watermarking method using the human visual system ( H V S ) is proposed, in which a different number of watermarks, in proportion to the energy contained in each band, are embedded into each band. Experiments show that the proposed three-level wavelet based method is robust to various attacks such as joint photographc experts group (PEG) compression, smoothing, cropping, collusion, multiple watermarking, and so on.	digital image;digital watermarking;embedded system;human visual system model;moving picture experts group;parsing expression grammar;smoothing;wavelet	Young-Sik Kim;O-Hyung Kwon;Rae-Hong Park	1999		10.1109/ISCAS.1999.779947	computer vision;digital watermarking;computer science;theoretical computer science;mathematics;digital image;computer graphics (images)	Graphics	41.766623807134046	-11.11965924206881	46458
e408680b0edfe31335e15853286ceaa701eecc84	memristor circuits and systems for future computing and bio-inspired information processing	memristors;memristor logic;future computing;bio inspired pattern processing;cellular nanoscale network;neuromorphic memristor crossbar	Memristors can be used in mimicking synaptic plasticity of biological neuronal systems. In addition, memristor crossbars can be realized in 3-dimensional architecture like human brain. This possibility of 3-dimensional integration is crucial in implementing the full-scale electronic neuron-synapse system in future. One more thing to note here is that memristor-based neuromorphic systems can be more energy-efficient than the conventional Von Neumann ones in some applications such as bio-inspired pattern processing. This is because they are more suitable to brain-like parallel processing. Based on these advantages of memristor-based neuromorphic systems, this paper reviews the memristor logics, where the computation and memory can be merged together. Then, we introduce neuromorphic memristor crossbars which can mimic the brain's pattern recognition of speech and image. The simulation results of neuromorphic crossbars strongly highlight the future possibility of memristor circuits in brain-mimicking pattern processing. In Cellular Nanoscale Network (CNN), memristors can be used in analog multiplication that is essential to perform CNN pixel calculation with low power consumption and high-area density.	british informatics olympiad;computation;full scale;information processing;memristor;nanonetwork;neuromorphic engineering;neuron;parallel computing;pattern recognition;pixel;simulation;synapse;synaptic package manager	Son Ngoc Truong;Khoa Van Pham;Wonsun Yang;Kyeong-Sik Min	2016	2016 IEEE Biomedical Circuits and Systems Conference (BioCAS)	10.1109/BioCAS.2016.7833830	electronic engineering;computer science;artificial intelligence;memistor;theoretical computer science;neuromorphic engineering	EDA	39.297223980728674	-0.8393563589235042	46483
0cc37453fbc8297cc797bba5fb79abb2f0d1eded	novel progressive region of interest image coding based on matching pursuits	image resolution computational complexity image coding image matching image reconstruction;available bit rate;image coding;image resolution;image matching;signal analysis;multiresolutional signal analysis tool;visual quality;roi image coding;computational complexity;image reconstruction;region of interest;indexation;image reconstruction region of interest roi image coding matching pursuit multiresolutional signal analysis tool computational complexity;image browsing;matching pursuit;image coding matching pursuit algorithms dictionaries signal analysis pursuit algorithms decoding image reconstruction image resolution computational complexity transmitters;high resolution imager;multi resolution	A progressive and scalable, region of interest (ROl) image coding scheme based on matching pursuits (MP) is presented. Matching pursuit is a multi-resolutional signal analysis tool and can be employed in order to progressively refine the quality of a set of selected regions of an image up to a specific grade. The computational complexity of this analysis method can be reduced by decreasing the size of MP dictionary. Thus, the proposed method provides a trade off between complexity, rate, and quality. By the suggested scheme, regions of an image with higher receiver's priority are refined in an interactive manner. The transmitter sends an initial coarse version of the image. Then, the receiver transmits its preferred ROI parameters. Afterwards, the reconstructed image is refined according to the ROl parameters, in a progressive way	computation;computational complexity theory;dictionary;encoder;interactive media;matching pursuit;progressive scan;region of interest;scalability;signal processing;transmitter;unbalanced circuit	Abbas Ebrahimi-Moghadam;Shahram Shirani	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262404	iterative reconstruction;computer vision;image resolution;computer science;signal processing;pattern recognition;computational complexity theory;matching pursuit;computer graphics (images);region of interest	Vision	45.61532921643293	-16.445839079170778	46608
068a5b0ec4824bb777225dfbcaafa3949ff83dd6	adaptive dual-cross search algorithm for block-matching motion estimation	diamond search;comparative analysis;data compression;motion compensation;pattern search;search algorithm;video compression;three step search;video peak signal to noise ratio adaptive dual cross search algorithm block matching algorithm motion estimation motion compensated video compression orthogonal search diamond search hexagonal search adaptive rood pattern search;motion estimation;indexing terms;motion compensated;video coding;fast motion estimation;peak signal to noise ratio;block matching;video coding motion estimation motion compensation data compression;block matching algorithm;motion estimation video compression redundancy distortion measurement analytical models pattern analysis signal analysis algorithm design and analysis psnr partitioning algorithms	In this paper, we present two new and efficient block-matching algorithm (BMAs) for fast motion estimation used in motion-compensated video compression. The proposed algorithms are improvement of existing BMAs and consist of three effective steps: 1) Initial search center prediction, 2) early search termination, and 3) dual-cross pattern search. Extensive simulation results and comparative analysis have shown that the proposed algorithms outperform conventional algorithms, like the three-step search, orthogonal search and diamond search, as well as the newly proposed algorithms, like the hexagonal search and adaptive rood pattern search, in terms of both video peak signal-to-noise ratio and the number of checking points are evaluated.	block-matching algorithm;checking (action);data compression;dual;matching;motion estimation;pattern search (optimization);peak signal-to-noise ratio;qualitative comparative analysis;search algorithm;simulation	Xuan-Quang Banh;Yap-Peng Tan	2004	IEEE Transactions on Consumer Electronics	10.1109/TCE.2004.1309460	data compression;beam search;computer vision;mathematical optimization;beam stack search;computer science;theoretical computer science;incremental heuristic search;iterative deepening depth-first search;best-first search;statistics;guided local search;binary search algorithm;search algorithm	Vision	48.48665111936213	-19.236481168998456	46618
444e82fbcb7979006351848245cca87d63dcd43e	parameter dependence in cumulative selection		Cumulative selection is a powerful process in which small changes accumulate over time because of their selective advantage. It is central to a gradualist approach to evolution, the validity of which has been called into question by proponents of alternative approaches to evolution. An important question in this context concerns how the efficiency of cumulative selection depends on various parameters. This dependence is investigated as parameters are varied in a simple problem where the goal is to find a target string starting with a randomly generated guess. The efficiency is found to be extremely sensitive to values of population size, mutation rate and string length. Unless the mutation rate is sufficiently close to a value where the number of generations is a minimum, the number of generations required to reach the target is much higher if it can be reached at all.	procedural generation;string (computer science)	David H. Glass	2014		10.1007/978-3-319-10840-7_26	econometrics;mathematical optimization;mathematics;statistics	Theory	26.649995341457295	-9.19834739982083	46655
038f33133b6116e4f21acbb9c84bb0ef70ff4a9c	a joint source-channel distortion model for jpeg compressed images	cuantificacion senal;systeme temps reel;modelizacion;quantization;optimisation;pulse code modulation;data interpretation statistical;image coding;modulation impulsion;psnr;image processing;computer communication networks;data compression;optimizacion;huffman coding;video signal processing;complexite calcul;telecommunication sans fil;unequal power allocation upa distortion model joint source channel coding jscc jpeg;differential pulse coding modulation;computer graphics;codage modulation;bit error rate;wireless communication systems;combined source channel coding;visual communication;procesamiento imagen;entropy coding;snr;bit error rates joint source channel distortion model jpeg compressed images joint source channel coding multimedia services wireless communication system quantization signal to noise ratio snr statistical model huffman coding differential pulse coding modulation run length coding;power allocation;3g mobile communication image coding combined source channel coding multimedia communication visual communication huffman codes pulse code modulation error statistics;transform coding;modulacion por impulsos;huffman codes;jpeg compressed images;differential pulse code modulation;traitement image;statistical model;multimedia systems;joint source channel;bit error rates;artifacts;signal processing computer assisted;modelisation;wireless communication;modulation differentielle;codage image;video coding;distortion;huffman code;joint source channel coding;image enhancement;complejidad computacion;compression image;3g mobile communication;estimation erreur;signal quantization;image compression;codage video;image interpretation computer assisted;codigo huffman;error cuantificuacion;jpeg;run length coding;error estimation;computational complexity	The need for efficient joint source-channel coding is growing as new multimedia services are introduced in commercial wireless communication systems. An important component of practical joint source-channel coding schemes is a distortion model to measure the quality of compressed digital multimedia such as images and videos. Unfortunately, models for estimating the distortion due to quantization and channel bit errors in a combined fashion do not appear to be available for practical image or video coding standards. This paper presents a statistical model for estimating the distortion introduced in progressive JPEG compressed images due to both quantization and channel bit errors. Important compression techniques such as Huffman coding, DPCM coding, and run-length coding are included in the model. Examples show that the distortion in terms of peak signal to noise ratio can be predicted within a 2 dB maximum error.	arithmetic coding;channel capacity;data compression;distortion;estimated;forward error correction;huffman coding;image;jpeg;multimedia;quantization (signal processing);run-length encoding;signal-to-noise ratio;statistical model;video coding format;standards characteristics;videocassette	Muhammad F. Sabir;Hamid R. Sheikh;Robert W. Heath;Alan C. Bovik	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/TIP.2006.871118	computer simulation;telecommunications;image processing;computer science;theoretical computer science;algorithm;statistics;huffman coding	Vision	48.48379413954081	-14.432761311995229	46666
d92288d35b604ff25e3f68b6ae7624f05493d680	sample efficiency analysis of neuroevolution algorithms on a quadruped robot	evolutionary computation;probability;sample efficiency analysis gait generation task continuous state action parameterized policy search covariance matrix adaptive evolutionary strategy neuroevolution through augmenting topology self adaptive factors probability weighted averaging characteristic optimization method neat cma neuroes ne algorithms artificial neural network ann reinforcement learning tasks quadruped robot neuroevolution algorithms;legged locomotion;neural nets;covariance matrices;gait analysis;probability covariance matrices evolutionary computation gait analysis learning artificial intelligence legged locomotion neural nets;learning artificial intelligence;topology optimization network topology legged locomotion sociology statistics;cma neuroes neural network neuroevolution neat evolution	In reinforcement learning tasks with continuous state-action, parameterized policy search has been known to be a powerful method. Applying NeuroEvolution (NE) to optimizing the policy represented by artificial neural network (ANN) is a particularly active research field. In most cases, NE algorithms cost a large amount of trial-and-error (episode) to optimize policies. However, due to time and cost constraints, researchers and practitioners cannot repeat a number of episodes on physical robots. Thus, choosing an efficient NE algorithm is a key to optimize policies with limited time and cost. In this work, our goal is to help users to choose an efficient NE algorithm. We compare and analyze sample efficiency of two successful state-of-the-art NE algorithms: CMA-NeuroES and NEAT in a gait generation task of a quadruped robot. Moreover, we run both algorithms with various initial topologies in order to analyze the performance difference between each topology. From experimental results, we show CMA-NeuroES outperforms NEAT regardless of initial topologies when the limited number of episodes can be executed. Additional experiments conclude that the optimization method for connection weights in NEAT results in its inferior performance to CMA-NeuroES, while a probability-weighted averaging characteristic and self-adaptive factors make CMA-NeuroES to be advantageous.	algorithm;artificial neural network;cma-es;comment (computer programming);executable;exemplification;experiment;mathematical optimization;neat chipset;neuroevolution;reinforcement learning;robot;vii	Shengbo Xu;Hirotaka Moriguchi;Shinichi Honiden	2013	2013 IEEE Congress on Evolutionary Computation	10.1109/CEC.2013.6557826	mathematical optimization;simulation;gait analysis;neuroevolution of augmenting topologies;computer science;artificial intelligence;machine learning;evolutionary acquisition of neural topologies;probability;evolutionary computation	AI	51.303285549800606	-23.339105034625504	46677
481a151aecc76c37ff0ca99aecd3426edaca2ab4	an image coder for the presentation of products with multiple color choices	histograms;image quality degradation image coder product presentation e commerce websites jpeg images file format single color products image multiple color choices decoder file size reductions;data compression;quantization signal;footwear;image color analysis;discrete cosine transforms;image color analysis encoding quantization signal data compression discrete cosine transforms footwear histograms;multiple color choices image coding product presentation jpeg jfif;encoding;web sites data compression decoding electronic commerce image coding image colour analysis	E-commerce websites rely heavily on JPEG images for product presentation. We describe a new coding scheme and file format that is tailored to the presentation of single-color products with multiple color choices: Instead of using multiple files of the same product at different colors, the new file format allows the transmission of all color choices in one file. On the decoder side images of the same product at different colors can be conveniently recovered from the same file. We demonstrate that file size reductions above 90% are achievable, with little quality degradation.	codec;color;e-commerce;elegant degradation;jpeg	Wai C. Chu	2014	2014 Data Compression Conference	10.1109/DCC.2014.10	data compression;color histogram;rgb color model;computer vision;color quantization;color image;computer science;jpeg;histogram;mathematics;image file formats;multimedia;jpeg file interchange format;quantization;encoding;statistics;computer graphics (images)	DB	43.39329940507688	-18.6437761400313	46688
fe7fc61101dcc49d18f5fdbdf8214bd6278b247f	inferring fitness in finite, variably-sized, and dynamically-structured populations		Biological fitness is not an observable quantity and must be inferred from population dynamics. Bayesian inference applied to the Moran process and variants yields a robust inference method that can infer fitness in finite, variably-sized, and dynamicallystructured populations. Information about fitness is derived solely from birth-events in birth-death and death-birth processes in which selection acts proportionally to fitness, which allows the method to be applied to populations on a network where the network itself may be changing in time. Populations may also be allowed to change size while still allowing estimates for fitness to be inferred.	fitness function;observable variable;population dynamics	Marc Harper	2013	CoRR		fitness proportionate selection;fitness landscape;statistics	ML	31.808584104096177	-12.704393941149638	46751
9ca516ca153895d7c69091c1ed9bdffb4dac8975	new watermarking algorithm with coding efficiency improvement and authentication in video surveillance	digital watermarking;watermarking;video coding efficiency improvement;psnr cwa compression watermarking algorithm video coding efficiency improvement authentication function video surveillance system mjpeg watermark information extraction encoding process decoding process;coding efficiency;video surveillance;psnr;data compression;decoding;surveillance system;digital watermark;watermarking authentication video surveillance signal processing algorithms transform coding video sequences data mining protection video compression multimedia systems;authentication;mjpeg;watermark information extraction;video sequences;data mining;authentication function;cwa;video coding;motion jpeg;digital watermarking motion jpeg mjpeg surveillance system coding efficiency authentication;feature extraction;storage capacity;multimedia communication;video surveillance system;message authentication;process simulation;encoding;algorithm design and analysis;motion jpeg mjpeg;watermarking data compression decoding feature extraction message authentication video coding video surveillance;encoding process;compression watermarking algorithm;decoding process	This paper proposes a fragile watermarking algorithm named compression watermarking algorithm (CWA) which provides compression and authentication functions to apply in MJPEG of video surveillance system. The watermark information is embedded into every 8 by 8 block in the coefficient domain in encoding process and extracted watermark from each 8 by 8 block in the coefficient domain in decoding process. Simulation results show that the PSNR of embedded video is lower than non-embedded video, however, the loss of quality is hardly discernible. Furthermore, the results show that the scheme improves the coding efficiency and therefore reduces both the transmission time and the storage capacity.	algorithm;algorithmic efficiency;authentication;closed-circuit television;coefficient;cognitive work analysis;digital watermarking;elegant degradation;embedded system;peak signal-to-noise ratio;simulation	Chuen-Ching Wang;Ming-Jun Chi;Yao-Tang Chang	2009	2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2009.163	computer vision;process simulation;digital watermarking;computer science;multimedia;internet privacy	EDA	40.0905917679966	-12.636145182841336	46804
56f26e23a79a0fbed1c8d948b2772b5a161f9fa3	a class of new knn methods for low sample problems	densite probabilite;estimation theory;estimateur knn;moyenne ponderee;small sample;probability;probability density;entropy nearest neighbor searches image segmentation image restoration speech processing multidimensional systems statistical analysis pixel lattices h infinity control;weighted averaging;probability density function;rule based;estimation non parametrique;clasificador;small samples;small sample classification problems estimation theory k nearest neighbor probability density function random variable observations;densidad probabilidad;non parametric estimation;vecino mas cercano;classifier;statistical analysis;consistent estimator;weighted average;nearest neighbor;random variable;classificateur;plus proche voisin;nearest neighbour;k nearest neighbor;estimacion no parametrica;petit echantillon;statistical analysis estimation theory probability	The K-nearest neighbor (KNN) estimates proposed by Loftsgaarden and Quesenbery D l give unbiased and consistent estimates of p ( X ) when K , the number of nearest neighbors considered, and N , the total number of observations available, tend to infinity such that K / N + 0. Hence excellent results may be obtained in large sample problems by using the KNN method for either density estimation or classification. A class of new KNN estimates is proposed as weighted averages of K KNN estimates, and it is shown that in small sample problems they give closer estimates to the true probability density than the traditional KNN estimates. Further, on the basis of some experimental results, we demonstrate that the KNN rules based on these estimates are suitable for small sample classification problems. I. INTRODUC~ION Loftsgaarden and Quesenbery [l] proposed a very useful and simple method for nonparametric estimation of the probability density function p ( X ) of a random variable X from N observations of X . This method is known as the K” method. The density estimate according to this method is given by where V ( K , N , X ) is the smallest hypervolume enclosing all the Manuscript received December 22, 198X; revised July I , 19x9 and November 6, 1989. The authors are with the Department of Electronics and Electrical Communication Engineering, Indian Institute of Technology. Kharagpur-721 302, India. A preliminary version of this paper was presented at the International Conference on Computers, Systems, and Signal Processing, Bangalore, India, Dec. 9-12, 1984 (sponsored by IEEE Computer Society, IEEE Circuits and Systems Society and IISc). IEEE Log Number 8933544. 715 points at least as near to X as the Kth nearest neighbor of X . On application of this method to the classification problem, we get the KNN rule which classifies an observation with unknown classification by assigning it to the class most heavily represcntcd among its K nearest neighbors. When K and N such that K / N + 0 , the KNN estimates can be shown to be unbiased and consistent estimates of p ( X ) . Hence, in large sample problems, the K” estimates or the K” majority rules for classification give excellent results. But, for small sample problems, a K” classification rule with the facility to weight the evidence of samples nearer to the unknown observation more heavily is intuitively more appealing. Using this idea, Dudani [2] developed a K” rule called the distance-weighted K” rule and tried to establish the merit of his rule on the basis of some experimental results. Later, however, some researchers [3], [4] pointed out that Dudani made an unfair comparison of his rule to a traditional K” rule in which all ties are reckoned as errors. Furthermore they attempted to establish by means of some experimental results that the distanceweighted K” rule does not offer any advantage over the traditional KNN majority rules with facility to resolve ties judiciously. Yet, since the sample size to dimensionality ratios for the problems they have chosen are large, their conclusions may not be valid for small sample problems. In this paper, we show that K” estimates formed as weighted averages of K K” estimates give, under some assumptions, closer estimates to the true probability density than the usual K” estimates. Again, by applying these estimates to the classification problem, we arrive at some distance-weighted K” rules that have more of a theoretical basis than the one proposed by Dudani. The relatively superior performance of these K” rules (cis-a-cis, the traditional ones) in small sample problems is demonstrated by means of some experimental results. 11. AVERAGE K” METHOD FOR DENSITY ESTIMATION A N D CLASSIFICATION Let us consider the K” estimate of p ( X ) given by the (1). By substituting in this equation the values 1 , 2 , . . . etc. for K , we get the INN, 2NN, etc. estimates. The equation thus formed for the 1NN estimate has a term involving the volume enclosing the first neighbor and hence has the information about the distance of the first neighbor. Similarly, the other estimates have the information about the distances of different neighbors. So, for incorporating this information on a statistical basis, a new estimate of p ( X ) may be formulated as an average of the different nearest-neighbor estimates. When Euclidean metric is employed for measuring distances, it is given by	dimensionality reduction;euclidean distance;ieee circuits and systems society;k-nearest neighbors algorithm;nearest-neighbor interpolation;signal processing	Parthasarathy Guturu;Biswanath N. Chatterji	1990	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.57285	rule-based system;probability density function;machine learning;pattern recognition;mathematics;k-nearest neighbors algorithm;statistics	ML	25.334836718441952	-23.210960365950356	46805
7244f0afa70889ccbb74d0f60053447d2287c99b	royal road functions and the (1 + λ) evolutionary algorithm: almost no speed-up from larger offspring populations	evolutionary computation;ω 2 d d c lower bound 1 λ evolutionary algorithm classic royal road test function class bit strings block sized offspring population o 2 d d c n d log n d generations;computational complexity;evolutionary computation computational complexity;optimization roads runtime sociology statistics markov processes evolutionary computation	We analyze the runtime of the (1 + λ) evolutionary algorithm (EA) on the classic royal road test function class. For a royal road function defined on bit-strings of length n having block sized ≥ log n + (c + 1 + ε) log d, we prove that the (1 + λ) EA with λ = Θ(n<sup>c</sup>) finds the optimum in an expected number of O(2<sup>d</sup>/d<sup>c</sup> · n/d log n/d) generations. Together with our lower bound of Ω(2<sup>d</sup>/d<sup>c</sup>), this shows that for royal road functions even very large offspring populations do not reduce the runtime significantly.	analysis of algorithms;distribution (mathematics);eulerian path;evolutionary algorithm;expect;experiment;polynomial;population;speedup	Benjamin Doerr;Marvin Künnemann	2013	2013 IEEE Congress on Evolutionary Computation	10.1109/CEC.2013.6557600	mathematical optimization;computer science;artificial intelligence;theoretical computer science;machine learning;mathematics;computational complexity theory;algorithm;evolutionary computation	Theory	28.875445664636565	2.4088818572004436	46812
a598325500f1cd40f5b9182bb12adcd16d9d5e0f	sparse source recovery with graph in network coding	compressed sensing;image coding;laplace equations;network coding;discrete cosine transforms;image reconstruction;sparse matrices	In network coding, the successive original video frame data can be transmitted at once. However, if insufficient number of innovative packets are transmitted due to the packet loss or delay, network coding system is to be underdetermined. Thus, since network coding matrix (random coefficient matrix) is not invertible, original data cannot be recovered by matrix inversion. To solve this problem, in this paper, a new compressive sensing method with graph Laplacian regularizer is proposed, which exploits correlation between successive original video frame data. Experimental results demonstrate the effectiveness of proposed algorithm, implemented by the alternative direction multiplier method (ADMM) and show that PSNR values from reconstructed images are above 33dB with coding matrix Φ ∊ CM×N of which measurement M = 0.75N and 22dB with measurement M = 0.66N, respectively.	algorithm;coefficient;compressed sensing;discrete cosine transform;emoticon;fast fourier transform;image compression;laplacian matrix;linear network coding;network packet;peak signal-to-noise ratio;sparse matrix	Sung Bok Yu;Yoonsik Choe	2016	2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)	10.1109/APSIPA.2016.7820844	mathematical optimization;linear network coding;variable-length code;theoretical computer science;machine learning;coding tree unit;mathematics	AI	44.73307139272958	-16.584664044678206	46918
aaf568649bc46f52ea9e67b3b1a774dde8c707ba	kalman-extended genetic algorithm for search in nonstationary environments with noisy fitness evaluations	process noise;optimal solution;optimisation;kga kalman extended genetic algorithm nonstationary environment search noisy fitness evaluations ga process noise environmental changes noisy evaluations observation noise uncertainty;propagation losses;algoritmo busqueda;performance evaluation;filtro kalman;optimizacion;incertidumbre;ga;algoritmo adaptativo;uncertainty;noisy evaluations;algorithme recherche;working environment noise;kalman extended genetic algorithm;filtre kalman;adaptive control;kalman filters;search algorithm;kalman filters search problems noise genetic algorithms;kalman filter;nonstationary environment search;indexing terms;algoritmo genetico;observation noise;genetic algorithms working environment noise uncertainty kalman filters noise reduction biological cells space stations performance evaluation stochastic processes propagation losses;environmental changes;adaptive algorithm;systeme incertain;biological cells;noisy fitness evaluations;algorithme adaptatif;stochastic processes;noise reduction;network configuration;non stationary condition;space stations;algorithme genetique;network optimization;genetic algorithm;genetic algorithms;condition non stationnaire;optimization;search problems;kga;incertitude;condicion no estacionaria;environmental change;sistema incierto;uncertain system;noise	In basic genetic algorithm (GA) applications, the fitness of a solution takes a value that is certain and unchanging. There are two classes of problem for which this formulation is insufficient. The first consists of ongoing searches for better solutions in a nonstationary environment, where the expected fitness of a solution changes with time in unpredictable ways. The second class consists of applications in which fitness evaluations are corrupted by noise. For problems belonging to either or both of these classes, the estimated fitness of a solution will have an associated uncertainty. Both the uncertainty due to environmental changes (process noise) and the uncertainty due to noisy evaluations (observation noise) can be reduced, at least temporarily, by re-evaluating existing solutions. The Kalman formulation provides a well-developed formal mechanism for treating uncertainty within the GA framework. It provides the mechanics for determining the estimated fitness and uncertainty when a new solution is generated and evaluated for the first time. It also provides the mechanics for updating the estimated fitness and uncertainty after an existing solution is re-evaluated, and for increasing the uncertainty with the passage of time. A Kalman-extended genetic algorithm (KGA) is developed to determine when to generate a new individual, when to re-evaluate an existing individual, and which one to re-evaluate. This KGA is applied to the problem of maintaining a network configuration with minimized message loss, in which the nodes are mobile, and the transmission over a link is stochastic. As the nodes move, the optimal network changes, but information contained within the population of solutions allows efficient discovery of better-adapted solutions. The ability of the KGA to continually find near-optimal solutions is demonstrated at several levels of process and observation noise. The sensitivity of the KGA performance to several control parameters is explored. Index terms – Genetic algorithm, Kalman filter, adaptive control, network optimization.	british undergraduate degree classification;fitness function;flow network;genetic algorithm;kalman filter;mathematical optimization;software release life cycle	Phillip D. Stroud	2001	IEEE Trans. Evolutionary Computation	10.1109/4235.910466	kalman filter;mathematical optimization;simulation;genetic algorithm;environmental change;adaptive control;computer science;artificial intelligence;machine learning;fitness approximation;statistics	AI	31.029064068438867	-10.423043912458933	47006
15efc4b4be4c3a086294f1f072f3c14705018bfc	joint multi-video compression for robot explorations	data compression;bandwidth allocation;robotic explorations network bandwidth encoding quality joint rate control approach target bit allocation encoding parameters flexible priority adjustment encoding rates channel bandwidths joint multivideo compression human shared control compression rate multiple robot videos joint compression scheme multiple video data human control stations wheel land interactions monitoring;video coding;robot vision;priority adjustment robots robotic videos joint rate control bit allocation;videos encoding robots bit rate psnr complexity theory joints;video coding bandwidth allocation data compression robot vision	To assist human shared control for robotic explorations in complicated environments, it is essential to transmit multiple videos of monitoring wheel-land interactions to human control stations. Since multiple video data is very huge, in this research, we have developed an effective joint compression scheme for multiple robot videos to assist human shared control. The proposed scheme is flexible which can work for multiple videos with both the same and different resolutions. To jointly regulate the overall compression rate to meet the available network bandwidth while obtaining optimal encoding quality, we propose a new joint rate control approach which can intelligently perform target bit allocation among multiple videos and dynamically determine encoding parameters. Further, our scheme also allows flexible priority adjustment among multiple videos to meet diverse needs from human controllers. Our experimental results have demonstrated that the proposed scheme can effectively compress multiple robot videos and regulate overall encoding rates meet diverse channel bandwidths.	data compression;interaction;robot control	Yu Sun;Michael Turney	2014	2014 International Conference on Computing, Networking and Communications (ICNC)	10.1109/ICCNC.2014.6785360	computer vision;real-time computing;simulation;computer science	Robotics	47.84126481547809	-21.704431121836024	47008
e6a23cc3e70282ebb71147436bef5ce611cd4ae7	fast cu size decision and mode decision algorithm for hevc intra coding	prediction modes fast cu size decision mode decision algorithm hevc intra coding high efficiency video coding h 264 avc tree structured coding unit mode decision process least rate distortion cost lagrange multiplier computational complexity optimal cu depth level;trees mathematics;video coding;computational complexity;intra prediction hevc mode decision;video coding computational complexity trees mathematics;encoding prediction algorithms correlation video coding algorithm design and analysis accuracy computational complexity	The emerging international standard of High Efficiency Video Coding (HEVC) is a successor to H.264/AVC. In the joint model of HEVC, the tree structured coding unit (CU) is adopted, which allows recursive splitting into four equally sized blocks. At each depth level, it enables up to 34 intra prediction modes. The intra mode decision process in HEVC is performed using all the possible depth levels and prediction modes to find the one with the least rate distortion (RD) cost using Lagrange multiplier. This achieves the highest coding efficiency but requires a very high computational complexity. In this paper, we propose a fast CU size decision and mode decision algorithm for HEVC intra coding. Since the optimal CU depth level is highly content-dependent, it is not efficient to use a fixed CU depth range for a whole image. Therefore, we can skip some specific depth levels rarely used in spatially nearby CUs. Meanwhile, there are RD cost and prediction mode correlations among different depth levels or spatially nearby CUs. By fully exploiting these correlations, we can skip some prediction modes which are rarely used in the parent CUs in the upper depth levels or spatially nearby CUs. Experimental results demonstrate that the proposed algorithm can save 21% computational complexity on average with negligible loss of coding efficiency.	algorithm;algorithmic efficiency;cuda;computational complexity theory;distortion;encoder;fast fourier transform;h.264/mpeg-4 avc;high efficiency video coding;intra-frame coding;lagrange multiplier;performance;rate–distortion theory;real-time clock;recursion;ruby document format;time complexity;tip (unix utility)	Liquan Shen;Zhaoyang Zhang;Ping An	2013	IEEE Transactions on Consumer Electronics	10.1109/TCE.2013.6490261	real-time computing;simulation;computer science;theoretical computer science;coding tree unit;mathematics;computational complexity theory;algorithm	AI	46.94147060323192	-19.408290088661435	47014
3c97bfe3c4771772eb5c230b2b2f1010ad970684	image coding using high-order conditional entropy-constrained residual vq	image coding vector quantization entropy bit rate digital signal processing laboratories algorithm design and analysis signal processing algorithms optimization methods probability;image coding;memory requirements high order conditional entropy constrained residual vq vector quantization inter vector dependencies entropy conditioning strategy local information neighboring vectors complexity design algorithm multistage structure searching techniques conditioning spatial stage region;computational complexity;entropy codes;conditional entropy;search problems;vector quantizer;vector quantisation;search problems image coding entropy codes computational complexity vector quantisation;entropy condition	An extension of entropy-constrained residual vector quantization is presented where inter-vector dependencies are exploited. The method, which the authors call conditional entropy-constrained residual vector quantization, employs a high-order entropy conditioning strategy that captures local information in the neighboring vectors. The complexity of the proposed design algorithm is relatively low, due mainly to the efficiency of the multistage structure of the residual vector quantizer, but also to the effectiveness of the searching techniques used to locate the best conditioning spatial-stage region of support. Experimental results show that the new method outperforms standard entropy-constrained residual vector quantization while also requiring lower encoding complexity and memory requirements. >	conditional entropy;vector quantization	Faouzi Kossentini;Wilson C. Chung;Mark J. T. Smith	1994		10.1109/ICIP.1994.413387	mathematical optimization;learning vector quantization;quantization;computer science;entropy encoding;machine learning;pattern recognition;mathematics;linde–buzo–gray algorithm;computational complexity theory;vector quantization;conditional entropy;statistics	Vision	44.5018642509179	-13.718156790048186	47066
b5914bfb69f5c02084f9bd0cf710dfd01c1d2781	the ld-celp gain filter based on bp neural network	cuantificacion senal;optimisation;mise a jour;optimizacion;behavioral analysis;quantifier;recommandation;actualizacion;signal quantization;backpropagation algorithm;quantification signal;quantificateur;analyse comportementale;recomendacion;algorithme retropropagation;recommendation;analisis conductual;optimization;reseau neuronal;cuantificador;red neuronal;updating;neural network;algoritmo retropropagacion	The recommendation G.728 depends on the Levinson-Durbin (L-D) algorithm to update its gain filter coefficients. In this paper, it is contrasted with BP neural network method. Because quantizer has not existed at optimizing gain filter, the quantization SNR can not be used to evaluate its performance. This paper proposes a scheme to estimate SNR so that gain predictor can be separately optimized with quantizer. Using BP neural network filter, the calculation quantity is only 6.7 percent of L-D method's and its average segment SNR is about 0.156dB higher than G.728. It is also used to evaluate the case that excitation vector is 16 or 20 samples, respectively, the BP neural network algorithm has similarly good result.	aac-ld;artificial neural network;code-excited linear prediction	Gang Zhang;Keming Xie;Zhefeng Zhao;Chunyu Xue	2006		10.1007/11760191_22	computer science;artificial intelligence;backpropagation;machine learning;artificial neural network;algorithm	ML	45.75278770651554	-11.317932965728737	47097
cc64e701085009e755bdd898bcd640576a38d271	adaptive downsampling for high-definition video coding	image sampling;video coding decoding high definition video image sampling performance evaluation rate distortion theory;performance evaluation;decoding;up sampling adaptive downsampling high definition video coding decoding encoding rate distortion performance improvement optimal downsampling ratio optimal r d performance;rate distortion theory;video coding;high definition video;bit rate encoding video coding filtering algorithms algorithm design and analysis high definition video psnr	Previous research has shown that downsampling prior to encoding and upsampling after decoding can improve the rate-distortion (R-D) performance compared with directly coding the original video using standard coding technologies, e.g., JPEG and H.264/AVC, especially at low bit rates. This paper proposes a practical algorithm to find the optimal downsampling ratio that balances the distortions caused by downsampling and coding, thus achieving the overall optimal R-D performance. Given the optimal sampling ratio, dedicated filters for down- and up-sampling are also designed. Simulations show this algorithm improves the R-D performance over a wide range of bit rates, e.g., from 1.0 dB at high bit rates to 2.5 dB at low bit rates.	algorithm;computer simulation;data compression;decibel;decimation (signal processing);distortion;h.264/mpeg-4 avc;hdmi;jpeg;sampling (signal processing);upsampling	Jie Dong;Yan Ye	2012	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/ICIP.2012.6467512	computer vision;rate–distortion theory;harmonic vector excitation coding;computer science;theoretical computer science;coding tree unit;mathematics;context-adaptive binary arithmetic coding;h.261;statistics;multiview video coding	Metrics	45.42912941504142	-17.12766455305445	47111
1e54429773fbea9ded7fd9995c1d239218f1ce49	parallel programmable asynchronous neighborhood mechanism for kohonen som implemented in cmos technology	kohonen self organizing map;software;cmos integrated circuits;topology;complementary metal oxide semiconductor;optimisation;self organizing maps;complementary metal oxide semiconductor implementation;low energy;low energy consumption;computing methodologies equipment design equipment failure analysis neural networks computer signal processing computer assisted transistors electronic;circuit complexity;chip;artificial neural networks;self organising feature maps asynchronous circuits cmos integrated circuits optimisation;neighborhood mechanism;self organising feature maps;sensor networks;circuit complexity parallel programmable asynchronous neighborhood mechanism kohonen som cmos technology kohonen self organizing maps asynchronous architecture complementary metal oxide semiconductor technology map topology optimization;transistors;neighborhood mechanism asynchronous and parallel circuits complementary metal oxide semiconductor implementation kohonen self organizing map low energy consumption;neurons hardware artificial neural networks parallel programming topology cmos technology self organizing feature maps;power dissipation;asynchronous and parallel circuits;software algorithms;asynchronous circuits;coverage;neurons;point of view;artificial neural network;hardware	We present a new programmable neighborhood mechanism for hardware implemented Kohonen self-organizing maps (SOMs) with three different map topologies realized on a single chip. The proposed circuit comes as a fully parallel and asynchronous architecture. The mechanism is very fast. In a medium sized map with several hundreds neurons implemented in the complementary metal-oxide semiconductor 0.18 μm technology, all neurons start adapting the weights after no more than 11 ns. The adaptation is then carried out in parallel. This is an evident advantage in comparison with the commonly used software-realized SOMs. The circuit is robust against the process, supply voltage and environment temperature variations. Due to a simple structure, it features low energy consumption of a few pJ per neuron per a single learning pattern. In this paper, we discuss different aspects of hardware realization, such as a suitable selection of the map topology and the initial neighborhood range, as the optimization of these parameters is essential when looking from the circuit complexity point of view. For the optimal values of these parameters, the chip area and the power dissipation can be reduced even by 60% and 80%, respectively, without affecting the quality of learning.	acclimatization;anatomy, regional;cmos;cpu power dissipation;circa;circuit complexity;data rate units;eighty;greater than;kilohertz;large;matching;map;mathematical optimization;neuron;neurons;online and offline;organizing (structure);parallel computing;point of view (computer hardware company);prospective search;robustness (computer science);sampling (signal processing);self-organization;semiconductor;simulation;trim31 gene;teuvo kohonen;transistor;transistors;weight;orders - hl7publishingdomain;square millimeter (qualifier value);voltage	Rafal Dlugosz;Marta Kolasa;Witold Pedrycz;Michal Szulc	2011	IEEE Transactions on Neural Networks	10.1109/TNN.2011.2169809	computer science;theoretical computer science;machine learning;cmos;artificial neural network	ML	38.760226126247765	-1.434209433115369	47128
782dbbdd93b23831819404eb4fbdb3a250bd10ff	the joint causal effect in linear structural equation model and its application to process analysis	structural equation model	"""& * ! ! ! ) """" ) """" """" ( ! ! : """" ) """" 9 ) * $ * ; * < """" ! ! : ) """" ! ! ! ! ) """" 6 C ! ! ) """" ) """" """" ( ! 3 ! * * """" ( ! D @ ! $ * D . E 0 F . E F G H H : ) ) """" < ! """" D @ """" D . E / G H I """" """" D . E 1 E H H 6 J ) """" ( K """" D 0 L L L H * : """" 3 3 ' ) """" """" ! $ ; ) """" ! $ * ) ! $ , ! ) ) """" < ! """" 6 * ! , 3 ' * 3 ' ! ) """" """" A ! ) ! ! """" """" ! 3 * ! ) * * : """" : ! ; """" * """" ) $ """" ( : """" ) : ! B * * ! ) ! ! """" """" ! 3 * ! ) * : """" : ! ; """" 7 ; """" ) ) ! $ * : """" ) : ! 6"""	causal filter;structural equation modeling	Manabu Kuroki;Zhihong Cai	2003			mathematical optimization;econometrics;computer science;structural equation modeling	Vision	27.65947969207978	-23.291451522246803	47142
546c19343f4e0fc8e75b56abc02227eb772da0b3	wavelet-based self-similar watermarking for still images	discrete wavelet transforms;watermarking robustness wavelet domain spatial resolution wavelet transforms informatics protection ear information filtering information filters;image coding;copy protection;copyright;nonlinear filter;rotation wavelet based watermarking self similar watermarking still images copyright protection cartesian grid watermark embedding watermark detection multiresolution detection quasi scale invariant watermark geometric transformation robustness scaling robustness linear filtering nonlinear filtering jpeg compression cropping;copyright protection;message authentication discrete wavelet transforms copy protection copyright image coding;image watermarking;message authentication;scale invariance	This paper presents a wavelet-based watermarking method for copyright protection of still images. Watermarks are structured in such a way to attain spatial self-similarity with respect to a carte sian grid. Embedding and detection are performed in the wavelet domain allowing thus multiresolution detection. The novelty of the current approach is the use of self-similar watermarks (quasi scaleinvariant), which are expected to be robust against geometric transformations, especially scaling. The approach proves rather efficient against many kinds of distortions, such as linear and nonlinear filtering, JPEG compression, scaling, cropping and rotation.	digital watermarking;distortion;image scaling;jpeg;nonlinear system;self-similarity;wavelet	Sofia Tsekeridou;Ioannis Pitas	2000		10.1109/ISCAS.2000.857067	message authentication code;nonlinear filter;wavelet;computer vision;discrete mathematics;computer science;electrical engineering;theoretical computer science;scale invariance;mathematics;wavelet packet decomposition;discrete wavelet transform;statistics	Graphics	41.8067428543039	-10.58835190171075	47252
77daa0251649903abd374df507cf5613801da1fe	firefly algorithm with dynamic attractiveness model and its application on wireless sensor networks		Firefly algorithm (FA) is a new population-based meta-heuristic algorithm which has outstanding performance on many optimisation problems. However, in standard FA, the attractiveness quickly approaches a constant in the middle period of the iterations. It may be very detrimental to the search ability of the algorithm. So we propose a new variant FA (DFA) with a dynamic attractiveness model which can linearly adjust the rate of change of attractiveness as the number of iterations grows. Thirteen well-known benchmark functions are used to verify the performance of our proposed method; the computational results show that DFA is more efficient than many other FA algorithms. We also successfully used DFA to solve the wireless sensor network node distribution optimisation problem; the results of the coverage statistics further validate the effectiveness of the proposed algorithm.	firefly algorithm	Jing Wang	2017	IJWMC	10.1504/IJWMC.2017.10009497	wireless sensor network;firefly algorithm;computer engineering;distributed computing;attractiveness;computer science;population	Mobile	27.725731672275494	-2.740613032139101	47267
25a177191b17d5314044b4282f9c58346a206487	experimentation of motion estimation algorithms in gpu	gpu;motion estimation;video coding	Video encoder motion estimation algorithms allow a great level of parallelism exploitation, since the same arithmetic operations are repeated over near amounts of pixel data. This paper analyses the use of modern general purpose graphical processing units (GPGPU), such as the NVIDIA CUDA® as an effective acceleration engine to improve motion estimation algorithms overall performance. The results of our analysis include practical evaluations performed on different ME methods using CUDA platform. The evaluations show the impacts of the method, window search size, and ME thread mapping onto the GPGPU in the speed up that can be achieved in such parallel platform.	algorithm;cuda;encoder;general-purpose computing on graphics processing units;graphical user interface;graphics processing unit;motion estimation;parallel computing;pixel;speedup;window function	Ronaldo Husemann;José Valdeni de Lima;Valter Roesler	2015		10.1145/2820426.2820454	parallel computing;quarter-pixel motion;computer science;theoretical computer science;motion estimation;computer graphics (images)	Vision	40.339782930125295	-20.793675392268497	47337
00a015e210f556367f426e490976f0abe31101b2	estimating reliability in proportional odds ratio models	methode jackknife;survival function;engineering;62f40;metodo estadistico;fiabilidad;reliability;analyse survie;bootstrap;intervalo confianza;analisis datos;fonction repartition;62p30;industrie;implementation;metodo jackknife;62f25;simulation;estimation non parametrique;sobrevivencia;industria;donnee censuree;simulacion;statistical method;exponential family;prueba duracion;famille exponentielle;funcion sobrevivencia;ingenierie;estimation parametrique;life test;non parametric estimation;funcion distribucion;data analysis;confidence interval;distribution function;industry;62n02;fonction survie;censored data;62g15;methode statistique;fiabilite;intervalle confiance;statistical computation;calculo estadistico;survival analysis;62n99;survie;analyse donnee;ingenieria;calcul statistique;odd ratio;methode reechantillonnage;familia exponencial;62nxx;estimacion no parametrica;resampling method;estimation statistique;survival;implementacion;60k10;jackknife method;62n05;estimacion estadistica;60e05;statistical estimation;60k20;essai endurance	In this paper, we are mainly interested in inference on the reliability coefficient, R = P(X < Y ), in proportional odds ratio models based on the new family of tilted survival functions introduced by Marshall and Olkin [Marshall, A.W., Olkin, I., 1997. A new method for adding a parameter to a family of distributions with application to the exponential and Weibull families. Biometrika 84 (3), 641–652]. We also present some results on stochastic comparison between the survival distribution functions. Asymptotic and various bootstrap confidence intervals of R are investigated. The performance of asymptotic and bootstrap confidence intervals is studied through a simulation. A numerical example based on reallife data is presented to illustrate the implementation of the proposed procedure. © 2008 Elsevier B.V. All rights reserved.	baseline (configuration management);booting;bootstrapping (statistics);coefficient;experiment;numerical analysis;ordered logit;performance;population;real life;resampling (statistics);sampling (signal processing);simulation;time complexity	Ramesh C. Gupta;Cheng Peng	2009	Computational Statistics & Data Analysis	10.1016/j.csda.2008.10.014	econometrics;exponential family;confidence interval;distribution function;reliability;mathematics;survival analysis;data analysis;implementation;censoring;survival function;statistics;odds ratio	AI	32.77701031711518	-22.410203260237292	47367
5ab22fd79dde70d7cd439a903f9da91111fb2536	yield optimization for nondifferentiable density functions using convolution techniques	computer program;metodo estadistico;optimisation;perturbation theory filters optimisation;microelectronic fabrication;fabricacion microelectrica;productivite;high dimensionality;aplicacion;optimizacion;convolution yield estimation circuit testing optimization methods density functional theory probability density function approximation algorithms filters circuit synthesis production;integrated circuit;convolution;perturbation theory;probability density function;aproximacion;performance;filters;circuito integrado;statistical method;convolucion;analog filter;productividad;filtre analogique;yield;approximation;algorithme;algorithm;methode statistique;analog filters nondifferentiable density functions convolution techniques yield derivative estimation truncated probability density functions yield optimization perturbation approach convergence dimensionality;gradient estimate;optimization;fortran;productivity;rendimiento;application;programa computador;optimal algorithm;density functional;rendement;filtro analogico;circuit integre;programme ordinateur;fabrication microelectronique;algoritmo	A method of yield derivative estimation for nondifferentiable or truncated probability-density functions (PDFs) is proposed and applied to yield optimization. The method applies convolution techniques and is based on the recently introduced perturbation approach. It constructs some approximation to the original PDF and requires a small number of samples per yield-optimization-algorithm step. The method is efficient and provides fast convergence in the solution, especially for problems of high dimensionality. Several yield-gradient estimation formulas are given. Some theoretical and practical aspects of the proposed method are discussed. Practical applications are demonstrated on several analog filters, and the method is compared with some other existing methods. >	convolution;mathematical optimization	Tian-Shen Tang;M. A. Styblinski	1988	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.7805	econometrics;mathematical optimization;yield;probability density function;productivity;performance;integrated circuit;approximation;perturbation theory;mathematics;convolution;analogue filter;algorithm;quantum mechanics;statistics	EDA	30.90901194589119	-11.957604711782173	47675
adefd282de23ca9dfb7b11330974997215d47446	using intelligent agents to build navigation meshes	navigation mesh agent path planning;path planning;intelligent agent;mesh generation	We present a novel algorithm that allows agents to discover a navigation mesh for an environment as they move through the environment. The Navigation-Mesh Automated Discovery (NMAD) algorithm works by constructing its best guess for the navigation-mesh of a game level and then refines it when the agents moving through the world using the navigation mesh encounter unexpected or unknown obstacles. Using this algorithm, agents can enter a world in which they know nothing about, while still enjoying all of the advantages of a navigation mesh for path planning. We validated the effectiveness of this technique by showing that for both random and deliberative searches through multiple game worlds the error present in the best guess approximation the navigation mesh generated and maintained by NMAD converges to zero.	algorithm;approximation;intelligent agent;motion planning;navigation mesh;procedural generation;simulation;virtual world	D. Hunter Hale;Gregory Michael Youngblood;Nikhil S. Ketkar	2010			navigation mesh;mesh generation;computer vision;simulation;computer science;artificial intelligence;motion planning;intelligent agent;mobile robot navigation	Robotics	53.280588284977256	-23.19108765387814	47737
63ca958767515abfc8c47911d1eb845babd136d4	analysis of coverage and performance of the variable sized replications simulation method in parallel	simulation methods	Parallel simulation methods can be used to reduce the execution time of the simulations of complex systems. In this context, we apply the method of the replications in parallel in order to reduce the execution time of the models. The particular technique consists of starting a fixed number of replications that are maintained in execution until a stop criterion is achieved. Different stopping criteria can be used in order to finish the simulation. These criteria must be robust in order to provide results according the applied statistical theory. The results obtained with two habitual criteria applied to a simple simulation model show that it is necessary to provide more elaborated stopping criteria of the simulation in order to obtain the adequate coverage. In order to reduce the detected problems we propose a new stopping criterion that increments the coverage of the true mean. The analysis of performance shows that, obviously, this better coverage has the cost of more simulation time, but it is important to get results statistically correct.	complex systems;early stopping;parallel computing;run time (program lifecycle phase);simulation	Angel Perles Ivars;Xavier Molero;Antonio Martí Campoy;Juan José Serrano	2002			parallel computing;simulation;computer graphics (images)	HPC	28.18805807998834	-16.764701587389368	47739
915a431198f268f4a911f7f9313c4b39a68172d9	evolutionary programming with only using exponential mutation	strategy parameter;gaussian processes evolutionary computation;evolutionary computation;gaussian processes;exponential evolutionary programming;cauchy random number;evolutionary programming;gaussian random number exponential mutation objective variable strategy parameter cauchy random number exponential evolutionary programming;genetic programming genetic mutations evolution biology probability distribution distributed computing exponential distribution evolutionary computation artificial intelligence genetic algorithms functional programming;objective variable;gaussian random number;random numbers;exponential mutation	The individual of population in standard self-adaptive evolutionary programming (EP) is composed as a pair of objective variable and strategy parameter. Therefore, EP must evolve both objective variable and strategy parameter. In standard evolutionary programming (CEP), these evolutions are implemented by mutation based on only Gaussian random number. On the other hand, fast evolutionary programming (FEP) uses Cauchy random number as evolution of objective variable and exponential evolutionary programming (EEP) uses exponential random number as evolution of objective variable. However, all of these EP (CEP, FEP and EEP) commonly uses Gaussian random number as evolution of strategy parameter. In this paper, we propose new EEP algorithm (NEP) which uses double exponential random number for both evolution of objective variable and strategy parameter. The experimental results show that this new algorithm (NEP) outperforms the existing CEP and FEP.	algorithm;complex event processing;evolution;evolutionary programming;expectation propagation;noise-equivalent power;random number generation;time complexity;uk educational evidence portal	Hiroyuki Narihisa;Keiko Kohmoto;Takahiro Taniguchi;Mayumi Ohta;Kengo Katayama	2006	2006 IEEE International Conference on Evolutionary Computation	10.1109/CEC.2006.1688358	evolutionary programming;mathematical optimization;interactive evolutionary computation;human-based evolutionary computation;computer science;artificial intelligence;machine learning;genetic representation;gaussian process;mathematics;statistics;evolutionary computation	Robotics	27.525468716726042	-7.246780414105568	47774
cf4636123289f7b9cd56e3548a6e7f0c16243763	an application of general maximum entropy to utility	utility;linear function;generalised maximum entropy;gme;power function;logarithmic function	Methodologies related to information theory have been increasingly used in studies in economics and management. In this paper, we use generalised maximum entropy as an alternative to ordinary least squares in the estimation of utility functions. Generalised maximum entropy has some advantages: it does not need such restrictive assumptions and could be used with both well and ill-posed problems, for example, when we have small samples, which is the case when estimating utility functions. Using linear, logarithmic and power utility functions, we estimate those functions and confidence intervals and perform hypothesis tests. Results point to the greater accuracy of generalised maximum entropy, showing its efficiency in estimation.	information theory;linear function;ordinary least squares;principle of maximum entropy	Paulo Ferreira;Andreia Dionísio	2013	IJADS	10.1504/IJADS.2013.054943	linear function;econometrics;mathematical optimization;logarithm;joint entropy;power function;binary entropy function;transfer entropy;maximum entropy probability distribution;principle of maximum entropy;mathematics;maximum entropy thermodynamics;maximum entropy spectral estimation;utility;min entropy;statistics	ML	27.547224268484328	-22.182192327291812	47792
453228ce26f1df8c00bced4b96ae6484879ef536	video security protection technology based on shot segmentation and bit commitment	watermarking;digital signatures;data mining;discrete cosine transforms;feature extraction;cameras	In order to make sure the video publishing in the public network is transmitted safely and recognized with liability, this study aims to provide a video feature extraction method based on shot segmentation, moreover video coding and tamper feature have been fully taking into consideration. Besides, the extraction data will be researched with bit commitment, and the tampered part of the video will be figured out through verifying the data of bit commitment. The sender can adjudge the liability of illegal video transmission through verify the bit commitment. Experimental results show the complexity of this computation method is lower and it has a good safety and reliability.	algorithm;authentication;commitment scheme;computation;computational complexity theory;content security policy;data compression;digital video;feature extraction;public-key cryptography;xslt/muenchian grouping	Peng Yu;Yongbin Wang;Fengfeng Duan;Jing An	2016	2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)	10.1109/CISP-BMEI.2016.7852690	computer vision;digital signature;feature extraction;digital watermarking;computer science;machine learning;video tracking;internet privacy;computer security	Security	38.814008144069085	-12.686821646013739	47817
d9f1ac12d3eaf809b15f7076916aea22a2068425	automaton-based stochastic control for navigation of emergency rescuers in buildings	kernel;hazards;automata;navigation;computational modeling;stochastic processes;buildings	In this paper we consider the problem of navigating emergency rescuers in buildings. The objective of the rescuer is to reach several critical regions in a building endangered by a spreading hazard and leave safely. We consider stochastic finite state models to capture the dynamics of the rescuer and the hazardous environment. We encode the objective of the rescuer as an automaton specification satisfaction problem. We synthesise a navigation policy for the rescuer that maximises the predicted probability of safely fulfilling the rescue mission. We finally test the proposed method in a simulation environment.	algorithm;automaton;encode;hazard analysis;high- and low-level;mathematical optimization;predictive modelling;reachability;recursion;simulation;stochastic control	Tony A. Wood;Maryam Kamgarpour	2016	2016 IEEE Conference on Control Applications (CCA)	10.1109/CCA.2016.7587893	structural engineering;simulation;engineering;artificial intelligence	Robotics	52.555706883605325	-21.168590726553898	47862
1b3721c5636858b7302a5aea888de0b57ad5148d	a novel fast intra prediction scheme for depth-map in 3d high efficiency video coding	depth map		high efficiency video coding	Mengmeng Zhang;Shenghui Qiu;Huihui Bai	2014	IEICE Transactions		depth map	Vision	45.24604690615548	-19.78847520246129	47928
46489ada7eb4891334a3bda0915a5e9769c2cbe8	control strategies for removing nitrogen compounds in wastewater treatment plants	carbon controllers nitrogen compound removal wastewater treatment plants wwtp benchmark simulation model no 2 bsm2 evaluation criteria ammonia cascade controller oxygen controller ammonia controller effluent quality operational costs aeration energy savings;inductors carbon biological system modeling nitrogen indexes benchmark testing layout;ammonia;costing;effluents;cascade control;quality control;wastewater treatment;wastewater treatment ammonia cascade control costing effluents quality control	This paper presents the application of different control strategies applied for wastewater treatment plants (WWTP). The main purpose is to evaluate its effectiveness/cost trade-off attending a set of criteria. The study has been performed by using the Benchmark Simulation Model No. 2 (BSM2) and the evaluation criteria have been selected taking into account the most relevant indicators provided by this benchmark scenario. More specifically, five strategies have been implemented and compared with the default closed loop control strategy (DCL) of BSM2 and between them. Results show that the control strategies that use the ammonia cascade controller provide better results in most of the evaluated criteria than the control strategies implemented by using only the oxygen controller. Ammonia controller improves the results in terms of effluent quality and operational costs due to the reported aeration energy savings. As a conclusion of this study the control strategy that uses the ammonia and carbon controllers has been selected as the most recommended for implementation.	benchmark (computing);control theory;maxima and minima;memory controller;offset binary;oracle call interface;simulation	Henry R. Concepción;Darko Vrecko;Montserrat Meneses;Ramon Vilanova	2013	2013 9th Asian Control Conference (ASCC)	10.1109/ASCC.2013.6606191	environmental engineering;engineering;operations management;waste management	Robotics	36.22174556008803	-4.520539695267476	47942
e5a81a38130c320df09fad336d53fc6038e1ea47	data hiding based on image texture classification		In this paper, a new pattern-based fragile, semiblind, spatial domain data hiding scheme is proposed. The Local Binary Pattern texture classification approach is used, in order to transparently and securely embed secret data into an image. Pixel values are modified in such a way that the texture satisfies the message requirements. The method is thoroughly studied and compared to other techniques in spatial domain in terms of capacity and image quality. The scheme performs well in images with smooth areas and can be used for authentication, tamper proofing, and secret communications.	8-bit;authentication;belief propagation;embedded system;image quality;image texture;pixel;requirement	Eleni E. Varsaki;Vassilis E. Fotopoulos;Athanassios N. Skodras	2013	Signal, Image and Video Processing	10.1007/s11760-011-0229-5	image texture;computer vision;data mining;mathematics;internet privacy	Vision	39.3233317104645	-11.385164110978009	48158
fcaae721b0ea770246dd9938f5d806ec84f61317	frequency-domain data hiding based on the matryoshka principle	data hiding;matryoshka principle;steganographie;multimedia;image based steganography;frequency domain image hiding;image;steganography;esteganografia;methode domaine frequence;image in image;frequency domain method;image in;l a b colour space;nested doll principle;spectral properties;metodo dominio frecuencia;frequency domain;imagen color;image couleur;colour image hiding;propriete spectrale;propiedad espectral;color image;hidden messages;covert communication	"""This paper introduces a novel steganography paradigm using the 'Matryoshka principle', a design paradigm, also known as 'nested doll principle', that denotes a recognisable relationship of """"similar object-within-similar object"""" that appears in the design of many natural and man made entities. This new steganography framework for image-within-image hiding exploits spectral properties of the Fourier magnitude and phase of natural images. The theory is that, as long as the Fourier phase of an image is maintained intact, the overall appearance of an image remains unchanged if the Fourier magnitude of the image is slightly modified."""		Tamer F. Rabie	2007	IJAMC	10.1504/IJAMC.2007.013952	computer vision;color image;computer science;image;steganography;information hiding;frequency domain;algorithm;statistics	NLP	42.58657661325963	-10.220495322281778	48160
4955a209c1d26d6208b23e11d296140bddb0c9bd	on the performance of linear phase wavelet transforms in low bit-rate image coding	image reconstruction linear phase wavelet transforms wavelet transforms performance low bit rate image coding vanishing moments filter length coding gain frequency selectivity wavelets shape coding performance first order markov source image quality subjective tests test image product code lattice quantizer synthesis wavelet shape coding errors visibility;transformation ondelette;linear phase;image numerique;image coding;filter bank;image processing;markov source;banc filtre;product code;coding errors;wavelet transforms image coding shape wavelet analysis image quality testing filters performance gain frequency performance analysis;procesamiento imagen;transform coding;qualite image;traitement image;coding gain;quantisation signal;wavelet transforms;codificacion;evaluation subjective;first order;wavelet transform;phase lineaire;vanishing moment;image reconstruction;banco filtro;image quality;imagen numerica;coding;error statistics;calidad imagen;digital image;transformacion ondita;markov processes;fase lineal;quantisation signal image coding image reconstruction wavelet transforms transform coding coding errors error statistics markov processes;subjective evaluation;wavelet transformation;codage;evaluacion subjetiva	The behavior of linear phase wavelet transforms in low bit-rate image coding is investigated. The influence of certain characteristics of these transforms such as regularity, number of vanishing moments, filter length, coding gain, frequency selectivity, and the shape of the wavelets on the coding performance is analyzed. The wavelet transforms performance is assessed based on a first-order Markov source and on the image quality, using subjective tests. More than 20 wavelet transforms of a test image were coded with a product code lattice quantizer with the image quality rated by different viewers. The results show that, as long as the wavelet transforms perform reasonably well, features like regularity and number of vanishing moments do not have any important impact on final image quality. The influence of the coding gain by itself is also small. On the other hand, the shape of the synthesis wavelet, which determines the visibility of coding errors on reconstructed images, is very important. Analysis of the data obtained strongly suggests that the design of good wavelet transforms for low bit-rate image coding should take into account chiefly the shape of the synthesis wavelet and, to a lesser extent, the coding.	coding gain;first-order predicate;image quality;linear phase;markov chain;markov information source;mike lesser;quantization (signal processing);rating (action);selectivity (electronic);standard test image;universal product code;wavelet transform	Eduardo A. B. da Silva;Mohammed Ghanbari	1996	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.495953	wavelet;computer vision;speech recognition;second-generation wavelet transform;image processing;computer science;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;gabor wavelet;statistics;wavelet transform	Vision	47.28729572373856	-12.502577526924243	48192
0c1bcb9073ece317177ca5da5c13214efb7c90b5	a parameter control method of evolutionary algorithms using exploration and exploitation measures with a practical application for fitting sovova's mass transfer model	sovova model;parameter control;exploitation;exploration	Exploration and exploitation are omnipresent terms in evolutionary computation community that have been broadly utilized to explain how evolutionary algorithms perform search. However, only recently exploration and exploitation measures were presented in a quantitative way enabling to measure amounts of exploration and exploitation. To move a step further, this paper introduces a parameter control approach that utilizes such measures as feedback to adaptively control evolution processes. The paper shows that with new exploration and exploitation measures, the evolution process generates relatively well results in terms of fitness and/or convergence rate when applying to a practical chemical engineering problem of fitting Sovova’s model. We also conducted an objective statistical analysis using Bonferroni–Dunn test and sensitivity analysis on the experimental results. The statistical analysis results again proved that the parameter control strategy using exploration and exploitation measures is competitive to the other approaches presented in the paper. The sensitivity analysis results also showed that different initial values may affect output in different magnitude. © 2013 Elsevier B.V. All rights reserved.	control theory;evolutionary algorithm;evolutionary computation;exploit (computer security);rate of convergence	Shih-Hsi Liu;Marjan Mernik;Dejan Hrncic;Matej Crepinsek	2013	Appl. Soft Comput.	10.1016/j.asoc.2013.05.010	econometrics;mathematical optimization;simulation;exploration;machine learning	AI	31.03869990695051	-6.961268351662791	48212
7f8493d9cdaea1b65fc2389849cac62bd058e467	a new improved fruit fly optimization algorithm iafoa and its application to solve engineering optimization problems		Nature-inspired algorithms are widely used in mathematical and engineering optimization. As one of the latest swarm intelligence-based methods, fruit fly optimization algorithm (FOA) was proposed inspired by the foraging behavior of fruit fly. In order to overcome the shortcomings of original FOA, a new improved fruit fly optimization algorithm called IAFOA is presented in this paper. Compared with original FOA, IAFOA includes four extra mechanisms: 1) adaptive selection mechanism for the search direction, 2) adaptive adjustment mechanism for the iteration step value, 3) adaptive crossover and mutation mechanism, and 4) multi-sub-swarm mechanism. The adaptive selection mechanism for the search direction allows the individuals to search for global optimum based on the experience of the previous iteration generations. According to the adaptive adjustment mechanism, the iteration step value can change automatically based on the iteration number and the best smell concentrations of different generations. Besides, the adaptive crossover and mutation mechanism introduces crossover and mutation operations into IAFOA, and advises that the individuals with different fitness values should be operated with different crossover and mutation probabilities. The multi-sub-swarm mechanism can spread optimization information among the individuals of the two sub-swarms, and quicken the convergence speed. In order to take an insight into the proposed IAFOA, computational complexity analysis and convergence analysis are given. Experiment results based on a group of 29 benchmark functions show that IAFOA has the best performance among several intelligent algorithms, which include five variants of FOA and five advanced intelligent optimization algorithms. Then, IAFOA is used to solve three engineering optimization problems for the purpose of verifying its practicability, and experiment results show that IAFOA can generate the best solutions compared with other ten algorithms. © 2017 Elsevier B.V. All rights reserved.		Lei Wu;Qi Liu;Xue Tian;Jixu Zhang;Wensheng Xiao	2018	Knowl.-Based Syst.	10.1016/j.knosys.2017.12.031	engineering optimization;swarm intelligence;artificial intelligence;machine learning;global optimum;computer science;crossover;computational complexity theory;algorithm;convergence (routing)	AI	27.352708691027317	-4.142090866760523	48299
373ffc9cec29f83b2fc4dc2af8cd97a58cae16b5	fast calculation of ifs parameters for fractal image coding	affine transformation;computer experiment;data compression;fractal analysis;image compression;iterated function systems;iterated function system	Fractal image coding based on Iterated Function System (IFS) has been attracting much interest because of possibilities of drastic data compression. It achieves compression by using the self-similarity in an image. It is one of the weak points on IFS that the calculation time is huge. Especially, the amount of calculation on scaling parameter and rmse is very huge. In this paper, we propose two schemes to reduce the calculation time while the quality of the image is kept. The first one reduces calculation time of parameters, affine transform and rmse by using the maximum amplitude ratio which is a ratio between the maximum amplitude range of range block and that of domain block. By using the maximum amplitude radio, domain block which does not seem to choose is excluded before calculating parameters. The second one reduces calculation time of scaling parameters by using the ratio between variance of range block and that of domain block. The variance ratio is used instead of the scaling parameter. We perform the fractal compression experiments based on the proposed scheme to verify the effectiveness of these schemes. Computational experiments show that about 50% of calculation time is reduced by using both of two schemes.	fractal	Masaki Harada;Tadahiko Kimoto;Toshiaki Fujii;Masayuki Tanimoto	2000			combinatorics;image processing;mathematics;geometry;iterated function system;algorithm	Vision	42.98959905331318	-13.753576115302055	48320
495de68c3e54dcaea91bb5d7a8b18add58f92fd8	easy predictions for the easy-hard-easy transition	local search algorithm;learning;latent class models;sequential search;satisfiability;variable ratio;decision problem;phase transition;local dependence;finite size scaling;power law;model based clustering;bayesian networks	"""We study the scaling properties of sequential and parallel versions of a local search algorithm, WalkSAT, in the easy regions of the easy-hard-easy phase transition (PT) in Random 3-SAT.In the underconstrained region, we study scaling of the sequential version of WalkSAT. We find linear scaling at fixed clause/variable ratio. We also study the case in which a parameter inspired by """"finite-size scaling"""" is held constant. The scaling then also appears to be a simple power law. Combining these results gives a simple prediction for the performance of WalkSAT over most of the easy region. The experimental results suggest that WalkSAT is acting as a threshold algorithm, but with threshold below the satisfiability threshold.Performance of a parallel version of WalkSAT is studied in the over-constrained region. This is more difficult because it is an opumization rather than decision problem. We use the solution quality, the number of unsatisfied clauses, obtained by the sequential algorithm to set a target for its parallel version. We find that qualities obtained by the sequential search with O(<i>n</i>) steps, are achievable by the parallel version in O(<i>log(n)</i>) steps. Thus, the parallelization is efficient for these """"easy MAXSAT"""" problems."""	ab initio quantum chemistry methods;decision problem;image scaling;linear search;local search (optimization);maximum satisfiability problem;parallel computing;search algorithm;sequential algorithm;walksat	Andrew J. Parkes	2002			phase transition;linear search;reinforcement;mathematical optimization;power law;computer science;local search;machine learning;decision problem;bayesian network;statistics;satisfiability	AI	28.43137933238479	3.3318601804523653	48337
fde359ea1793e29769f93aee37ec019684b7bb72	20 years of progress in video compression - from mpeg-1 to mpeg-h hevc. general view on the path of video coding development		Compression of moving images has opened unprecedented opportunities of transmission and storage of digital video. Extraordinary performance of todays video codecs is a result of tens of years of work on the development of methods of data encoding. This paper is an attempt to show this history of development. It highlights the history of individual algorithms of data encoding as well as the evolution of video compression technologies as a whole. With the development of successive technologies also functionalities of codecs were evolving, which make also the topic of the paper. The paper ends the attempt of authors’ forecasting about the future evolution of video compression technologies.	algorithm;code;codec;data compression;digital video;high efficiency video coding;mpeg-1;mpeg-h;moving picture experts group	Damian Karwowski;Tomasz Grajek;Krzysztof Klimaszewski;Olgierd Stankiewicz;Jakub Stankowski;Krzysztof Wegner	2016		10.1007/978-3-319-47274-4_1	video compression picture types;computer vision;simulation;h.263;multimedia;h.261;multiview video coding	ML	43.06054538648654	-19.964959653022813	48386
1810fda821008ffd06479cab395dcdd890f7d2b0	robuste optimierung mit quantilmaßen auf globalen metamodellen	mathematics;general statistics	Robust optimization integrates the input variables’ uncertainty in the optimization process and determines solutions, which are insensitive to such variations. It has a high practical orientation, because scattering parameters like production tolerances or variations in the material properties play a role in many industrial applications. In this thesis a methodology for robust optimization with quantile measures on global metamodels is developed. It is tailored to the situation, that only a few function evaluations are available. In practical applications this is the case, whenever a complex system or process behaviour can only be modeled by expensive computer simulations or experiments. The focus of the thesis is on characterizing the system’s robustness in an appropriate way and on calculating it efficiently. It is demonstrated, that quantile measures meet the industry’s requirements to robustness measures in a better way than the standard measures used up to now. They find more appropriate robust areas and give more accurate lower and upper limits for a certain percentage of the centred output distribution, which can be fixed according to the needs of the user. The methodology approximates the system’s behaviour with global metamodels. The robustness behaviour is modeled with metamodels for quantiles of the output distributions. The efficient quantile measure computation is done with a new algorithm, which combines Halton sequence and Harrell-Davis estimator and contains an internal error estimation. A numerical study demonstrates its efficiency. A measure for the tolerance of the quantile metamodels is developed, which derives its limits from the original metamodel. Finally a new iterative method supports the user in selecting the robust optimum. The methodology can be accelerated with fast multipole procedures. This way is described and the effort estimated. The validity and efficiency of the methodology is demonstrated with several real application examples from automotive industry.		Beate Rhein	2014			humanities;philosophy	Visualization	27.094643126670398	-15.785827035535439	48423
fcb51c6438d63d31e4bf24c44c9e2c987d0b1e2f	selecting the don't care bits in jpeg2000 roi coding	least significant bit;region of interest;jpeg2000;wavelets	"""Region of interest coding is an important feature in JPEG2000 and it is accomplished by de-emphasizing the wavelet coefficients associated with the non-ROI regions of the image. In the general scaling-based method, a number of extra bits appear below the least significant bit of the ROI samples after the shifting process. These bits need be coded at the time of bit-plane coding, but they are discarded by the decoder. The usual procedure is for the encoder to set these """"don't care"""" bits to zero. In this paper, we propose a method that exploits the state of the JPEG2000 entropy coder to set the values of these bits in a more intelligent way. The method has been observed to reduce the number of bits required to represent ROI code-block by up to 7%, with the bit-stream remaining JPEG 2000 Region-of-interest (ROI) coding allows user-defined parts of an image to be coded with a higher quality than other regions. In JPEG2000 (1-7), the wavelet transform coefficients are partitioned into code-blocks of size 32×32 or 64×64, with the possible exception of boundary regions. The coefficients in each code-block are quantized and entropy coded independently of each other, with entropy coding done bit-plane by bit-plane starting from the most significant plane. The ability to truncate the resulting code-block bit-streams part way through coding is the key to the quality progressive feature of the JPEG2000 standard. JPEG2000 implements ROI coding by down-shifting the """"background"""" (BG) coefficients in the direction of the least significant bits prior to bit-plane coding. This manipulation results in the ROI coefficients being isolated in the most significant bit-planes. As a result, if decoding is terminated at any point in the middle of the procedure, a higher number of bits from the ROI coefficients, comparing to those of the BG, will be available for the decoder and the quality of ROI will be comparatively higher. The decoder can easily reverse the scaling provided that it knows which coefficients are in the ROI and the amount of scaling that has been done. After the shifting process a number of """"extra bits"""" appear below the LSB in the bit-plane representations of the ROI coefficients and for simplicity, these are often set to zero. In this paper we propose a method for selecting these bits in order to reduce the coding cost of the code-blocks. The benefits of this are two fold: the bit rate is reduced for a given set of block truncation points and, more subtlety, a rate-distortion optimized search for the best set of code- block truncation points may now be able to find a significantly better operating point. The paper is organized as follows: first the two ROI coding techniques used in JPEG2000 are introduced and the JPEG2000 entropy coding method is briefly reviewed. We then present our proposed method for selecting the """"don't care"""" bits values and follow with experimental results."""	don't-care term;jpeg 2000;region of interest	Jamshid Ameli;Jacques Vaisey;Tong Jin	2004		10.1117/12.529453	arithmetic coding;computer vision;4b5b;effective number of bits;theoretical computer science;mathematics;bit field;engineering drawing	Theory	43.07546160244046	-14.355296722171614	48447
cd8eb143e50a32cee947f58a5405d70c5c8a15ec	a modified group search optimizer algorithm for high dimensional function optimization		This paper presents a modified group search optimizer algorithm for high dimensional function optimization, which is based on levy flight strategy, self-adaptive joining strategy, and chaotic mutation strategy. The levy flight strategy is employed for the producer to simplify the computation and improve efficiency in the exploring space. The self-adaptive joining strategy is used for the scroungers walking towards the producer to promote convergence speed. The chaotic mutation strategy is designed for the rangers to strengthen diversification. Using those strategies, the modified algorithm can get better balance between intensification and diversification. The simulation experiments, which were carried on benchmark functions, show that those strategies are effective, and they improve the global optimization ability and convergence speed of modified group search optimizer for high dimensional function optimization.	algorithm;mathematical optimization;program optimization	Lijin Wang;Xinxin Hu;Jing Ning;Lin Jing	2012		10.1007/978-3-642-34041-3_32	global optimization;computation;lévy flight;algorithm;computer science;convergence (routing);diversification (marketing strategy)	DB	26.644806627139424	-4.064337351805767	48451
b26c4a5512e3ac7fed1347ca0b9fec4443e9dfc4	meta-heuristic algorithms applied to the optimization of type-1 and type 2 tsk fuzzy logic systems for sea water level prediction	seawater fuzzy logic fuzzy set theory fuzzy systems genetic algorithms geophysics computing particle swarm optimisation sea level;seawater;metaheuristic algorithms;type 1 tsk fuzzy logic system;optimization algorithm;type 2 tsk fuzzy logic system optimization;fuzzy logic optimization genetic algorithms prediction algorithms particle swarm optimization artificial neural networks sociology;prediction algorithms;particle swarm optimization metaheuristic algorithms type 1 tsk fuzzy logic system type 2 tsk fuzzy logic system optimization sea water level prediction firefly algorithm optimization algorithm optimal fuzzy logic system takagi sugeno kang fuzzy logic system parameter optimization genetic algorithm;fuzzy set theory;interval type 2 tsk fuzzy logic system;fuzzy logic;artificial neural networks;geophysics computing;firefly algorithm;particle swarm optimization;genetic algorithm;genetic algorithms;optimization;sea water level prediction firefly algorithm particle swarm optimization genetic algorithm type 1 tsk fuzzy logic system interval type 2 tsk fuzzy logic system;takagi sugeno kang fuzzy logic system;particle swarm optimisation;sociology;sea level;fuzzy systems;parameter optimization;sea water level prediction;optimal fuzzy logic system	This paper describes an approach using Firefly Algorithm, Particle Swarm Optimization and Genetic Algorithm to optimize the parameters of Takagi-Sugeno-Kang (TSK) fuzzy logic system (both type-1 and type-2) in order to find the optimal fuzzy logic system for sea water level prediction. The obtained results of the simulations performed are compared among these optimization algorithms in order to find which one is the best optimization algorithm for sea water level prediction.	adaptive neuro fuzzy inference system;firefly algorithm;formal system;free library of springfield township;fuzzy logic;genetic algorithm;heuristic (computer science);mathematical optimization;nsa product types;particle swarm optimization;simulation;software release life cycle	Nguyen Cong Long;Phayung Meesad	2013	2013 IEEE 6th International Workshop on Computational Intelligence and Applications (IWCIA)	10.1109/IWCIA.2013.6624787	multi-swarm optimization;meta-optimization;genetic algorithm;computer science;artificial intelligence;machine learning;mathematics;artificial neural network;fuzzy control system	EDA	26.355587185953073	-10.164235862019883	48635
33b7c4276f70bdbdffa630dbbdcb67566fda5f5f	region based compression of multispectral images by classified klt	rate distortion image segmentation image coding abstracts classification algorithms encoding vectors;karhunen loeve transforms data compression discrete wavelet transforms image coding image segmentation;energy compaction region based algorithm multispectral images compression homogeneous regions spectral klt spatial shape adaptive dwt spiht encoding dedicated klt classified klt	A new region-based algorithm is proposed for the compression of multispectral images. The image is segmented in homogeneous regions, each of which is subject to spectral KLT, spatial shape-adaptive DWT, and SPIHT encoding. We propose to use a dedicated KLT for each region or for each class rather than a single global KLT. Experiments show that the classified KLT guarantees a significant increase in energy compaction, and hence, despite the need to transmit more side information, it provides a valuable performance gain over reference techniques.	algorithm;data compaction;discrete wavelet transform;distortion;encode;experiment;interaction technique;multispectral image;set partitioning in hierarchical trees	Marco Cagnazzo;Raffaele Gaetano;Sara Parrilli;Luisa Verdoliva	2006	2006 14th European Signal Processing Conference		computer vision;speech recognition;pattern recognition;mathematics	Vision	43.552975578185375	-15.029365774727971	48666
f9bb581c2b879055c73510a2791333302ce4d943	transfer entropy in mdps with temporal logic specifications		Emerging applications in autonomy require control techniques that take into account uncertain environments, communication and sensing constraints, while satisfying high-level mission specifications. Motivated by this need, we consider a class of Markov decision processes (MDPs), along with a transfer entropy cost function. In this context, we study high-level mission specifications as co-safe linear temporal logic (LTL) formulae. We provide a method to synthesize a policy that minimizes the weighted sum of the transfer entropy and the probability of failure to satisfy the specification. We derive a set of coupled non-linear equations that an optimal policy must satisfy. We then use a modified Arimoto-Blahut algorithm to solve the non-linear equations. Finally, we demonstrated the proposed method on a navigation and path planning scenario of a Mars rover.		Suda Bharadwaj;Mohamadreza Ahmadi;Takashi Tanaka;Ufuk Topcu	2018	2018 IEEE Conference on Decision and Control (CDC)		mathematical optimization;transfer entropy;motion planning;computer science;linear temporal logic;temporal logic;markov decision process;mars rover	Robotics	52.617122925628614	-21.218784989393992	48813
657f42c6e526475568c701ee476c243180d65ffe	a high-density and low-power charge-based hamming network	silicon;processing element;feedforward layer;cmos prototype chip;cmos integrated circuits;neural chips cmos integrated circuits;2 micron chip area winner take all network charge based hamming network programmable hamming neural network circuit capacitive comparators processing elements feedforward layer multiport charge sensing amplifier maxnet wta cmos prototype chip interconnected processing elements low power dissipation experimental results robust retrieval classification properties modularity methodology vlsi system level;chip area;high density;neural networks;maxnet;programmable hamming neural network circuit;2 micron;prototypes;robust retrieval;cmos process;capacitive comparators;prototypes neural networks integrated circuit interconnections artificial neural networks silicon neural network hardware pattern matching cmos process robustness;multiport charge sensing amplifier;processing elements;chip;neural chips;artificial neural networks;low power;vlsi system level;classification properties;low power dissipation;pattern matching;integrated circuit interconnections;wta;neural network hardware;interconnected processing elements;robustness;charge based hamming network;experimental results;winner take all;winner take all network;neural network;modularity methodology	A charge-based programmable Hamming neural network circuit is proposed. It utilizes capacitive comparators as processing elements in the feedforward layer, and a multiport charge-sensing amplifier as the MAXNET (or winner-take-all (WTA)) circuit. The CMOS prototype chip contains 10*10 fully interconnected processing elements with the capability of encoding 10 exemplar patterns. The whole circuit occupies a silicon area of 0.414 mm/sup 2/ fabricated in a 2- mu m CMOS technology. The low-silicon area and low-power dissipation are the fundamental properties of the proposed implementation. The experimental results from a prototype chip show robust retrieval and excellent classification properties as theoretically predicted. A modularity methodology and how to extend the prototype chip to VLSI system level integration are examined. >	low-power broadcasting	Yuping He;Ugur Çilingiroglu;Edgar Sánchez-Sinencio	1993	IEEE Trans. VLSI Syst.	10.1109/92.219907	chip;winner-take-all;embedded system;electronic engineering;computer science;engineering;electrical engineering;pattern matching;prototype;silicon;cmos;artificial neural network;robustness	Embedded	38.89214816667585	-2.2142116901524673	48832
9b016a7c2cb16d131b01ae5c8ab6232fa6777a0c	multi-touch points detection for capacitive touch panel	gaussian noise;signal detection;tactile sensors vectors signal to noise ratio noise measurement shape;statistical analysis;haptic interfaces;statistical analysis capacitive sensors gaussian noise haptic interfaces signal detection;capacitive sensors;multi touch points detection real touch coordinates weighting average technique touch locations touch signal features likelihood ratio test partial touch signal information noise induced detection error gaussian noise capacitive touch panel systems	We consider the detection of multiple touch points for capacitive touch panel systems under Gaussian noise. We propose an algorithm that reduces the noise-induced detection error and improves the detection accuracy with partial touch signal information. The proposed algorithm is based on the likelihood ratio test, and utilizes the touch signal features, such as the local maximum, the range of touch magnitude, and the consecutive occurrence of touch locations, to first detect touch points and then calculates the real touch coordinates based on the weighting average technique. Simulation demonstrates the improved performance of the proposed algorithm.	algorithm;maxima and minima;multi-touch;signal-to-noise ratio;simulation;touchscreen	Chien-Hsien Wu;Ronald Y. Chang;Wei-Ho Chung	2013	2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference	10.1109/APSIPA.2013.6694269	embedded system;computer vision;electronic engineering;computer science	EDA	50.54811087686331	2.5040803951283577	48833
10df69b314da118918bb94d8d4ed1b8d8b97dcc5	automated tracking approach with ant colonies for different cell population density distribution		Visual tracking of cells has many challenges due to complex situations such as varying cell population densities, intricate motion patterns and sophisticated interactions with other cells. This paper focuses on an efficient and effective ant-based method with working modes updated to track multiple cells over varying densities in the presence of occlusion or clustering situations. To overcome these challenges, the proposed Ant Colony Optimization (ACO) algorithm models two types of ant working modes, namely, cooperation mode and interactive competition mode, whereas the classical ACO algorithms use only one type of mode which performs poor in clutter situations. Moreover, mode update strategies based on ant colony pheromone is used to adjust pheromone field to obtain accurate state vector of cells. Experimental results demonstrate that the proposed method robustly tracks multiple cells in various scenarios. The averaged LSR, LTR and FTR of our method can be only 1.43, 1.71 and 1.37 %, respectively. Our experimental results also show that our tracking method is competitive with state-of-the-art multi-cell tracking methods.	ant colony	Mingli Lu;Benlian Xu;Zhengqiang Jiang;Andong Sheng;Peiyi Zhu;Jian Shi	2017	Soft Comput.	10.1007/s00500-016-2048-7	machine learning;computer science;artificial intelligence;ant colony optimization algorithms;ant colony;motion analysis;ant;cluster analysis;state vector	Metrics	27.107417574377855	-12.511758417870844	48951
15841b4e59bf1d52d19c0d305b8d0892b3e7fa47	a hybrid algorithm based on tabu search and immune algorithm for k-cardinality tree problems	population diversity tabu search immune algorithm k cardinality tree problem hybrid algorithm kctp weighted graph np hard combinatorial optimization problem tsia algorithm immune system inspired operator;trees mathematics artificial immune systems computational complexity search problems	A k-cardinality tree problem (KCTP) is to find a subtree with exactly k edges in an undirected, connected, weighted graph, such that the sum of weights is minimal. KCTP has been proved to be an NP-hard combinatorial optimization problem. In this paper, a hybrid algorithm (TSIA) based on Tabu Search (TS) and Immune Algorithm (IA) is presented for solving KCTP. The immune system inspired operator enforces diversity in the population of solutions. Numerical results demonstrate that the proposed algorithm is competitive to existing state-of-art algorithms in both precision and computing time.	benchmark (computing);combinatorial optimization;graph (discrete mathematics);hybrid algorithm;mathematical optimization;np-hardness;numerical method;optimization problem;tabu search;tree (data structure)	Qingqiang Guo;Hideki Katagiri	2012	The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems	10.1109/SCIS-ISIS.2012.6505009	mathematical optimization;combinatorics;tabu search;computer science;machine learning;mathematics;best-first search;search algorithm	Robotics	24.761709649690893	1.3634119200179775	48991
32da4485a18a817565328b54a62b5dd03d50da08	a note on fisher's correlation coefficient	matematicas aplicadas;coeficiente correlacion;mathematiques appliquees;geometric approach;applied mathematics;correlation coefficient;coefficient ficher;coefficient correlation;fisher ciefficient	This note uses a geometric approach to discuss connections between different definitions of the correlation coefficient.	coefficient	Sergio Plata	2006	Appl. Math. Lett.	10.1016/j.aml.2005.02.036	pearson product-moment correlation coefficient;fisher transformation;association;econometrics;rv coefficient;correlation ratio;applied mathematics;calculus;mathematics;intraclass correlation;spearman's rank correlation coefficient;statistics	NLP	34.233517311232	-20.242395601847416	49047
172284b8da18e710af8d58b0f2f289fe5ecd6a2c	a digital blind watermarking system using adaptive quantization		estimation in image watermarking using Genetic Programming (GP). The distortion is estimated by considering the problem of obtaining a distorted watermarked signal from the original watermarked signal as a function regression problem. This function regression problem is solved using GP, where the original watermarked signal is considered as an independent variable. GP-based distortion estimation scheme is checked for Gaussian attack and Jpeg compression attack. We have used Gaussian attacks of different strengths by changing the standard deviation. JPEG compression attack is also varied by adding various distortions. Experimental results demonstrate that the proposed technique is able to detect the watermark even in the case of strong distortions and is more robust against attacks.	digital watermarking;distortion;gaussian blur;genetic programming;jpeg;quantization (signal processing)	Ching-Tang Hsieh;Yeh-Kuang Wu	2004			digital watermarking;computer vision;quantization (signal processing);artificial intelligence;computer science	Vision	41.216676570527355	-10.225331774496905	49065
488abf2bedd00774ed82435839eeb837de6f4825	a video compression scheme with optimal bit allocation between displacement vector field and displaced frame difference	dynamic programming;video sequence;rate distortion theory video coding data compression dynamic programming image sequences;rate distortion;data compression;video compression;video sequences;dynamic program;bit rate;rate distortion video compression optimal bit allocation displacement vector field displaced frame difference video sequence lossless motion compensated video coder dynamic programming lossy mcvc lagrangian relaxation lossless mcvc h 263 based mcvc h 263 coder;design for disassembly;motion compensated;rate distortion theory;video coding;h 263 coder;redundancy;video compression bit rate design for disassembly video sequences redundancy rate distortion hafnium rate distortion theory lagrangian functions video coding;displacement vector field;lossy mcvc;lossless motion compensated video coder;h 263 based mcvc;bit allocation;displaced frame difference;optimal bit allocation;lossless mcvc;lagrangian functions;lagrangian relaxation;image sequences;hafnium	In this paper, we address the fundamental problem of optimally splitting a video sequence into two sources of information, the displaced frame di erence (DFD) and the displacement vector eld (DVF). We rst consider the case of a lossless motion compensated video coder (MCVC) and derive a general Dynamic Programming (DP) formulation which results in an optimal tradeo between the DVF and the DFD. We then consider the more important case of a lossy MCVC and present an algorithm which solves optimally the bit allocation between the rate and the distortion. This algorithm is based on Lagrangian relaxation and the DP approach introduced for the lossless MCVC. We then present an H.263-based MCVC which uses the proposed optimal bit allocation scheme and compare its results to H.263. As expected, the proposed coder is superior in the rate-distortion sense.	algorithm;data compression;data flow diagram;displacement mapping;distortion;dynamic programming;lagrangian relaxation;linear programming relaxation;lossless compression;lossy compression	Guido M. Schuster;Aggelos K. Katsaggelos	1996		10.1109/ICASSP.1996.544838	data compression;computer vision;computer science;theoretical computer science;mathematics;statistics	Theory	47.30248931598876	-16.340606823551013	49078
656920feee02e5cd5c2e154a3b201a8a65ae9f42	image encryption using 2d hénon-sine map and dna approach		Abstract Chaotic systems have been widely applied in digital image encryption due to their complex properties such as ergodicity, pseudo randomness and extreme sensitivity to their initial values and parameters. This paper first proposes a two-dimensional Henon-Sine map (2D-HSM). The new map possesses better ergodicity and pseudo randomness, and its parameters have a wider chaotic range, compared with many existing chaotic systems. Then a DNA encoding and a DNA exclusive-or (XOR) operation rule are defined because DNA approach applying in image encryption can greatly improve the efficiency of image permutation and diffusion. Furthermore, a novel image encryption scheme whose image pixels are diffused by the DNA approach and permutated by 2D-HSM, is proposed to protect image content while an image is transferred over the Internet. Some experimental analyses such as statistical attack analysis, differential attack analysis, exhaustive attack analysis, robustness against noise and computational complexity have been applied to measure the new scheme, and the experimental results illustrate the scheme possesses better encryption performances than that of other references, and therefore, is secure in real-world communication.	encryption;hénon map	Jiahui Wu;Xiaofeng Liao;Bo Yang	2018	Signal Processing	10.1016/j.sigpro.2018.06.008	mathematical optimization;ergodicity;mathematics;randomness;robustness (computer science);pixel;permutation;theoretical computer science;digital image;encryption;computational complexity theory	ML	38.579633666981984	-8.63378957285709	49336
be924c755b0d3065d7ca4d7f5e46ea7d0bbe48e9	inference of stress-strength for the type-ii generalized logistic distribution under progressively type-ii censored samples	bayes estimation;credible interval;maximum likelihood estimator;monte carlo simulations;progressive type ii censoring	AbstractThis paper studies the estimation of the reliability R = P[Y < X] when X and Y come from two independent generalized logistic distributions of Type-II with different parameters, based on progressively Type-II censored samples. When the common scale parameter is unknown, the maximum likelihood estimator and its asymptotic distribution are proposed. The asymptotic distribution is used to construct an asymptotic confidence interval of R. Bayes estimator of R and the corresponding credible interval using the Gibbs sampling technique have been proposed too. Assuming that the common scale parameter is known, the maximum likelihood estimator, uniformly minimum variance unbiased estimator, Bayes estimation and confidence interval of R are extracted. Monte Carlo simulations are performed to compare the different proposed methods. Analysis of a real data set is given for illustrative purposes. Finally, methods are extended for proportional hazard rate models.		Salman Babayi;Esmaile Khorram	2018	Communications in Statistics - Simulation and Computation	10.1080/03610918.2017.1332214	efficient estimator;minimax estimator;econometrics;minimum-variance unbiased estimator;estimator;confidence interval;nelson–aalen estimator;bayes estimator;trimmed estimator;maximum a posteriori estimation;mathematics;maximum likelihood;bias of an estimator;estimation theory;credible interval;statistics;monte carlo method	ECom	29.885468008519695	-22.431751418012933	49360
057fbca303ccfc7f0e3a90dbb8db1ae09b9a3e0a	distributed, anytime optimization in power-generator networks for economic dispatch	penalty functions distributed anytime optimization power generator network economic dispatch strongly connected digraph weight balanced digraph total generation cost convex function distributed coordination algorithm laplacian set valued dynamics asymptotic convergence algebraic graph theory nonsmooth analysis set valued dynamical systems;heuristic algorithms generators optimization convergence economics convex functions algorithm design and analysis;power systems cooperative control optimization algorithms;set theory convex programming distributed power generation graph theory power generation dispatch power generation economics	This paper considers the economic dispatch problem for a group of power generating units communicating over an arbitrary strongly connected, weight-balanced digraph. The goal of the group is to collectively meet a specified load while respecting individual generation bounds and minimizing the total generation cost, which corresponds to the sum of individual arbitrary convex functions. We introduce a distributed coordination algorithm, termed Laplacian-set-valued dynamics, and establish its asymptotic convergence to the solutions of the economic dispatch problem. In addition, we show that the algorithm is anytime, meaning that its executions are feasible solutions at all times and the total cost monotonically decreases as time elapses. The technical approach combines notions and tools from algebraic graph theory, nonsmooth analysis, set-valued dynamical systems, and penalty functions. Several simulations illustrate our results.	algebraic graph theory;anytime algorithm;connectivity (graph theory);convex function;directed graph;dynamic dispatch;dynamical system;mathematical optimization;ramp simulation software for modelling reliability, availability and maintainability;rate of convergence;simulation;strongly connected component;subderivative;transmission line	Ashish Cherukuri;Sonia Martínez;Jorge Cortés	2014	2014 American Control Conference	10.1109/ACC.2014.6859195	mathematical optimization;combinatorics;discrete mathematics;mathematics	ML	33.699279893251	2.0005187021768687	49392
9ccb8067ba987bd80a06dcaa566ec1df19bbd649	speech watermarking scheme based on singular-spectrum analysis for tampering detection and identification		This paper proposes a speech watermarking scheme for tampering detection and identification, which is based on the singular-spectrum analysis (SSA). Tampered signals can be detected and identified by analyzing signature information embedded into the speech signals. A required property of such hidden information is that it should be fragile to any malicious signal-processing operation. In this work, we use the SSA to analyze signals in embedding and extraction processes. The signature information is embedded into those signals by modifying a less-significant part of singular spectra. The evaluation results show that the proposed scheme is fragile to several malicious attacks but robust against non-malicious signal- processing operations. The sound quality of the watermarked signals also meets the inaudibility criteria. Besides detecting and locating the tampered locations, our proposed scheme can make a prediction about types and degrees of attacks. In addition, its new embedding rule enables one to fine-tune some parameters in order to improve the overall performance further.	digital watermarking;embedded system;malware;sensor;signal processing;sound quality;watermark (data file)	Jessada Karnjana;Kasorn Galajit;Pakinee Aimmanee;Chai Wutiwiwatchai;Masashi Unoki	2017	2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)	10.1109/APSIPA.2017.8282027	robustness (computer science);digital watermarking;required property;sound quality;singular spectrum analysis;embedding;trajectory;matrix decomposition;pattern recognition;computer science;artificial intelligence	Security	41.3135516207489	-9.190414349156558	49420
3a4416e27be3a724aea0335aa41ca882449ebdc0	conditional ordering of order statistics	right tail increasing;random distribution;order statistics;metodo estadistico;analyse multivariable;usual multivariate stochastic order;nombre entier;order statistic;multivariate analysis;variable independante;variable aleatoire;fonction repartition;relacion orden;statistique ordre;estimation non parametrique;left tail decreasing;variable aleatoria;distribution aleatoire;ordering;loi conditionnelle;ordre stochastique;statistical method;ley condicional;non parametric estimation;integer;funcion distribucion;relation ordre;distribution function;methode statistique;entero;independent random variables;estadistica orden;random variable;stochastic order;fonction repartition empirique;60e15;analisis multivariable;variable independiente;62g30;estimacion no parametrica;estimation statistique;60k10;estimacion estadistica;distribucion aleatoria;statistical estimation;conditional distribution;independent variable;left tail decreasing right tail increasing order statistics usual multivariate stochastic order	"""For any positive integers m and n, let X""""1,X""""2,...,X""""m""""@?""""n be independent random variables with possibly nonidentical distributions. Let X""""1"""":""""n@?X""""2"""":""""n@?...@?X""""n"""":""""n be order statistics of random variables X""""1,X""""2,...,X""""n, and let X""""1"""":""""m@?X""""2"""":""""m@?...@?X""""m"""":""""m be order statistics of random variables X""""1,X""""2,...,X""""m. It is shown that (X""""j"""":""""n,X""""j""""+""""1"""":""""n,...,X""""n"""":""""n) given X""""i"""":""""m>y for j-i>=max{n-m,0}, and (X""""1"""":""""n,X""""2"""":""""n,...,X""""j"""":""""n) given X""""i"""":""""m@?y for j-i@?min{n-m,0} are all increasing in y with respect to the usual multivariate stochastic order. We thus extend the main results in Dubhashi and Haggstrom (2008) [1] and Hu and Chen (2008) [2]."""		Weiwei Zhuang;Junchao Yao;Taizhong Hu	2010	J. Multivariate Analysis	10.1016/j.jmva.2009.11.007	combinatorics;order statistic;calculus;mathematics;statistics	Logic	33.19698313236051	-20.34289049368635	49421
88f8d10ebad320a26790112f7e7efbf5a2c7df66	enhanced comprehensive learning particle swarm optimization with exemplar evolution		Enhanced comprehensive learning particle swarm optimization (ECLPSO) is a metaheuristic recently proposed by us for global optimization. ECLPSO is balanced in exploration and exploitation; however, it still cannot satisfactorily address some complex multimodal problems. In this paper, we investigate further improving the exploration performance of ECLPSO through exemplar evolution (EE). EE encourages information exchange among different dimensions of the search space and performs mutation and selection on personal best positions that are exemplars guiding the flight of particles. EE is able to prevent the dimensions from getting stuck in stagnancy. Experimental results on various benchmark functions demonstrate that the EE strategy significantly improves the exploration performance of ECLPSO and helps ECLPSO to locate the global optimum region on all of the functions.	particle swarm optimization	Xiang Yu;Yunan Liu;Xiangsheng Feng;Genhua Chen	2017		10.1007/978-3-319-68759-9_76	mathematical optimization;machine learning;global optimization;metaheuristic;information exchange;global optimum;particle swarm optimization;artificial intelligence;computer science	Vision	26.60703050310646	-3.7390808295751157	49508
675ab800715151ccc66a52d70fb3039a7bc70455	tests for skewness and kurtosis in the one-way error component model	panel data;kurtosis;62h15;qa mathematics;62f05;normality;error components;hb economic theory;62f03;skewness	This paper derives tests for skewness and kurtosis for the panel data one-way error components model. The test statistics are based on the between and within transformations of the pooled OLS residuals, and are derived in a moment conditions framework. We establish the limiting distribution of the test statistics for panels with large crosssection and fixed time-series dimension. The tests are implemented in practice using the bootstrap. The proposed methods are able to detect departures away from normality in the form of skewness and kurtosis, and to identify whether these occur at the individual, remainder, or both error components. The finite sample properties of the tests are studied through extensive Monte Carlo simulations, and the results show evidence of good finite sample performance.	booting;bootstrapping (statistics);component-based software engineering;monte carlo method;one-way function;ordinary least squares;panel data;simulation;time series	Antonio F. Galvao;Gabriel Montes-Rojas;Walter Sosa-Escudero;Liang Wang	2013	J. Multivariate Analysis	10.1016/j.jmva.2013.07.002	shape of the distribution;econometrics;skewness;jarque–bera test;kurtosis;panel data;mathematics;normality test;d'agostino's k-squared test;shape parameter;pearson distribution;normal probability plot;statistics	ML	30.704259162478326	-21.819510231136093	49546
dc439ca8ffcfddcf1865f33884fb2a53d89088e9	genetic algorithms for protein structure prediction	protein structure prediction	Genetic algorithms are a general class of search methods that mimic natural gene-based optimization mechanisms. Mutation, cross-over and replication operations are performed on strings. When applied to structure prediction, each string describes a particular conformation of a protein molecule. There are many ways in which such search methods may be implemented. Recent results show potential for helping with protein structure prediction, but more data are needed before a complete assessment can be made.	genetic algorithm;protein structure prediction	Richard S. Judson	2009		10.1007/978-0-387-74759-0_219	molecular dynamics;genetic algorithm;bioinformatics;protein structure prediction;monte carlo method	Theory	29.770741981774385	-8.528903034071739	49611
a792f50e6fc044e509afbbb81f073328951b2318	nichingeda: utilizing the diversity inside a population of edas for continuous optimization	machine learning algorithms;estimation theory;optimisation;evolution biology distance measurement probabilistic logic probability density function computational modeling gallium optimization;recombination operators;evolutionary computation;high dimensionality;probability density function;global optimization problems;nichingeda;mathematical operators;evolution biology;electronic design automation and methodology;distance measurement;estimation of distribution algorithm;gaussian mixture model;computational modeling;shape;mixture model;continuous optimization;probability distribution;multimodal functions;robustness;global optimization;optimization;space technology;genetic mutations;probabilistic logic;optimisation estimation theory gaussian distribution mathematical operators;computational efficiency;global optimization problems nichingeda continuous optimization estimation of distribution algorithms gaussian distribution unimodal functions multimodal functions gaussian mixture model recombination operators;estimation of distribution algorithms;gaussian distribution;unimodal functions;gallium	Since the estimation of distribution algorithms (EDAs) have been introduced, several single model based EDAs and mixture model based EDAs have been developed. Take Gaussian models as an example, EDAs based on single Gaussian distribution have good performance on solving simple unimodal functions and multimodal functions whose landscape has an obvious trend towards the global optimum. But they have difficulties in solving multimodal functions with irregular landscapes, such as wide basins, flat plateaus and deep valleys. Gaussian mixture model based EDAs have been developed to remedy this disadvantage of single Gaussian based EDAs. A general framework NichingEDA is presented in this paper from a new perspective to boost single model based EDAspsila performance. Through adopting a niching method and recombination operators in a population of EDAs, NichingEDA significantly boosts the traditional single model based EDAspsila performance by making use of the diversity inside the EDA population on hard problems without estimating a precise distribution. Our experimental studies have shown that NichingEDA is very effective for some hard global optimization problems, although its scalability to high dimensional functions needs improving. Analyses and discussions are presented to explain why NichingEDA performed well/poorly on certain benchmark functions.	benchmark (computing);continuous optimization;crossover (genetic algorithm);ensemble learning;estimation of distribution algorithm;global optimization;ieee congress on evolutionary computation;mathematical optimization;mixture model;multimodal interaction;scalability	Weishan Dong;Xin Yao	2008	2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)	10.1109/CEC.2008.4630958	mathematical optimization;estimation of distribution algorithm;computer science;machine learning;mixture model;mathematics;continuous optimization;statistics;global optimization;evolutionary computation	Vision	28.20200975392122	-7.955459441730306	49661
2a9c8a2d79ab51db02c2bfe9b594fef0ec9a5906	improvement of algorithm on the track recognition	bayesian theory;simulation ordinateur;test statistics;metodo estadistico;fuzzy fusion algorithm;analisis numerico;matematicas aplicadas;algorithm performance;mathematiques appliquees;statistical algorithm;reconocimiento;probabilistic algorithm;statistical method;threshold value;analyse numerique;algorithme;algorithm;recognition;numerical analysis;bayesian fusion algorithm;resultado algoritmo;methode statistique;fuzzy algorithm;track recognition;performance algorithme;navigation system;simulacion computadora;applied mathematics;reconnaissance;computer simulation;algoritmo	For more exact navigation and positioning on the navigation systems, this paper discusses the improvement of statistical algorithms based on fuzzy knowledge with a brand-new theory. The improved fusion algorithms based on Bayesian theory and fuzzy methods on the track recognition are also given, respectively. However, this paper mainly makes a comparison of the improved new algorithms and two old algorithms for track recognition. The computer simulation shows the validity and feasibility of two improved algorithms. At the same time, the simulation results show that the overall performance of two improved algorithms for the track recognition all is better than that of the probabilistic algorithms. Based on the experiments in the dense target environment, the correct average recognition rate of statistical algorithms is 66.96% at the highest; that of the improved new algorithm based on fuzzy knowledge is 77.98%, that of the Bayesian fusion algorithm is 86.75%, however, that of the fuzzy fusion algorithm can be 90.14%. These results support the usefulness of the two improved algorithms. 2002 Elsevier Science. All rights reserved. 2008 Elsevier Inc. All rights reserved.	computer simulation;experiment;randomized algorithm	Qinge Wu;Tuo Wang;YongXuan Huang;JiSheng Li	2008	Applied Mathematics and Computation	10.1016/j.amc.2008.05.060	computer simulation;statistical hypothesis testing;probabilistic analysis of algorithms;threshold limit value;numerical analysis;bayesian probability;computer science;artificial intelligence;machine learning;mathematics;randomized algorithm;algorithm	AI	24.97605963431979	-23.14509268426674	49759
daa6586f4cba61ccf3c957a1d7b54d9a9dbde003	abnormal event detection and localization in wireless sensor network using compressed sensing	wireless sensor network event detection event localization compressed sensing;wireless sensor networks compressed sensing signal detection;compressed sensing;abnormal event detection transmission signal basis pursuit algorithm alternating direction method compressed sensing recovery algorithms sparse signal compressed sensing theory wireless sensor network abnormal event localization;sensors;bit error rate;event detection;会议论文;sensors signal processing algorithms event detection accuracy compressed sensing bit error rate;wireless sensor network;event localization;accuracy;signal processing algorithms	The main challenge in wireless sensor network is the accuracy and timeliness of event detection and localization. However, for a wireless sensor network, abnormal events are relatively sparse compared with all the events in monitoring area. Compressed sensing theory proposed the idea to recover a sparse signal from a few measurements. In this paper abnormal event detection and localization in wireless sensor network are formulated as a compressed sensing problem. On the assumption that the transmission signal of sensors is binary, the performance of the basis pursuit algorithm combined with alternating direction method of multipliers and other compressed sensing recovery algorithms are analyzed. Simulation results verify the preponderance of the basis pursuit algorithm on accurate detection and localization.	algorithm;augmented lagrangian method;basis pursuit;compressed sensing;overhead (computing);sensor;simulation;sparse matrix;x.690	Quan Wang;Yu Liu;Lin Zhang;Xiaofei Wu	2013	2013 15th IEEE International Conference on Communication Technology	10.1109/ICCT.2013.6820384	electronic engineering;computer science;machine learning;pattern recognition	Mobile	52.06131965437428	2.5827574589696063	49812
5a27bfd13864a6cebe9c7c15343f4f6689e2bbed	computing confidence intervals for log-concave densities	nonparametric density estimation;maximum likelihood;log concave;confidence interval	In Balabdaoui, Rufibach, and Wellner (Annals of Statistics, 37, pages 1299-1331, 2009), pointwise asymptotic theory was developed for the nonparametric maximum likelihood estimator of a log-concave density. Here, the practical aspects of their results are explored. Namely, the theory is used to develop pointwise confidence intervals for the true log-concave density. To do this, the quantiles of the limiting process are estimated and various ways of estimating the nuisance parameter appearing in the limit are studied. The finite sample size behaviour of these estimated confidence intervals is then studied via a simulation study of the empirical coverage probabilities.		Mahdis Azadbakhsh;Hanna K. Jankowski;Xin Gao	2014	Computational Statistics & Data Analysis	10.1016/j.csda.2014.01.020	econometrics;confidence interval;confidence distribution;pattern recognition;mathematics;maximum likelihood;likelihood function;cdf-based nonparametric confidence interval;robust confidence intervals;statistics	ML	30.78339286905945	-21.82653053544411	49835
90dfb1e51c3e72c53510b38e8d7764c40457d8e3	a sensor network for particulate distribution estimation		This paper describes the development of a sensor network designed to estimate the spatial and temporal distribution of particulate in the air. The network employs sensor nodes which are based on an optical solution and are capable of estimating the particulate size distribution. The sensor nodes employ a commercial fiberglass filter through which the air is forced to pass by means of a small pump. A video camera coupled to an inexpensive RaspberryPI Zero W is used to acquire and process the filter image. A small LoRa wireless module is coupled to the the RaspberryPI in order to transmit the acquired data over a range exceeding 10 km. The nodes can measure reliably particles down to sizes of 10 μm, usually refereed to as PM10 and a solution down to 2.5 μm, (PM2.5) is being tested. The fiberglass filter is in form of a strip and a small motor is used to move the strip and to start a new measurement when the filter gets covered in dust. The overall node cost is of less than 100$.	raspberry pi 3 model b (latest version);sensor node	Luca Lombardo;Marco Parvis;Francesco Vitiello;Emma Angelini;Sabrina Grassini	2018	2018 IEEE International Symposium on Medical Measurements and Applications (MeMeA)	10.1109/MeMeA.2018.8438701	wireless sensor network;particulates;video camera;computer science;electronic engineering	Mobile	45.23810762473994	-0.5683659894764562	49921
2dae78e829c83359ad07dad8cf6277b95e4eb4ef	otsu's criterion-based multilevel thresholding by a nature-inspired metaheuristic called galaxy-based search algorithm	silicon;chaos image segmentation thresholding metaheuristic optimization otsu;optimisation;otsu;search problems image segmentation optimisation;image segmentation;chaos;search algorithm;space exploration;biology;silicon image segmentation spirals biology space exploration logistics optimization;logistics;spirals;optimization;hill climbing;thresholding;search problems;hill climbing algorithm criterion based multilevel thresholding nature inspired metaheuristic galaxy based search algorithm image segmentation gray level images gbsa;metaheuristic;local search	In this paper, image segmentation of gray-level images is performed by multilevel thresholding. The optimal thresholds for this purpose are found by maximizing the between-class variance (the Otsu's criterion). The optimization is conducted by a newly-developed nature-inspired metaheuristic called “Galaxy-based Search Algorithm” or the GbSA. The proposed GbSA resembles the spiral arms of some galaxies to search for the optimal thresholds. The GbSA also uses a modified Hill Climbing algorithm as a local search. The experimental results show that the GbSA finds the optimal or very near optimal thresholds in all runs of the algorithm.	akaike information criterion;coat of arms;hill climbing;image segmentation;local search (optimization);mathematical optimization;metaheuristic;otsu's method;search algorithm;thresholding (image processing);wild arms 3	Hamed Shah-Hosseini	2011	2011 Third World Congress on Nature and Biologically Inspired Computing	10.1109/NaBIC.2011.6089621	logistics;computer vision;mathematical optimization;computer science;local search;hill climbing;space exploration;machine learning;mathematics;thresholding;image segmentation;silicon;metaheuristic;spiral;search algorithm	AI	28.86925714417179	-5.790162361975773	49949
d250cc6a419edfd25858d2fe28c3ed13eee4a3c3	modélisation statistique et probabiliste du temps inter-vehiculaire aux différents niveaux de trafic		"""Time Headway (TH) is a microscopic variable in traffic flow theories that has been studied since the 1930s. Distribution of this fundamental variable describes the arrival pattern of vehicles in traffic flow, so probabilistic modeling is the main approach to study TH and represents driving behavior. The applications of the variable in traffic engineering are varied ; include capacity calculation, microscopic simulation, traffic safety analysis, etc. This dissertation aims at modeling the TH distribution in different contexts. Firstly, the short-time sampling method and long-time sampling method are applied to obtain TH samples from the two data bases (the RN118 national roadway and the A6 motorway). Then, three probabilistic TH model types are analyzed and classified. An exhaustive comparison between the existing models and between the corresponding estimation methods leads to consider that the gamma-GQM is the best TH model in the literature. An estimation process is also proposed in order to obtain good and stable estimated results of the parameters. After that, the TH probabilistic modeling is developed by six new models. Except for the two ones which are worse, the four other models are statistically equivalent and/or better than the gamma-GQM. For practical reason, the Double Gamma model is selected, as a comparison model, with the gamma-GQM to calibrate all TH samples. Three traffic levels are considered : macroscopic, mesoscopic and microscopic. The effects of exogenous factors are also examined. Examining this factor in each macroscopic variable level leads to distinguish two following factor types : """"impeding"""" factor and """"propulsive"""" factor. Finally, different approaches for TH validation are tested. The proposed approach of """"envelope of distributions"""" seems to be promising for future applications."""	database;gqm;gamma correction;mesoscopic physics;sampling (signal processing);simulation;theory	Duy-Hung Ha	2011				ML	27.43826516062297	-16.501246196202455	49970
43528250ab3fed9079da222bd701a2fb79d6f2e7	doppler-only tracking in gsm-based passive radar	passive radar radar tracking vectors velocity measurement accuracy standards;tracking system simulations doppler only tracking gsm noncooperating transmitters fm radio passive radar receiver velocity estimation doppler frequency estimation passive radar velocity measurement doppler only localization;tracking bistatic radar passive radar localization;velocity measurement cellular radio doppler radar passive radar radar tracking radio receivers radio transmitters target tracking	A passive radar is a radar which uses external noncooperating transmitters (e.g. FM radio or GSM) to illuminate the target. Typical passive radar receiver benefits from its long integration time with a good velocity (Doppler frequency) estimation accuracy, while - especially for a GSM-based radar - the range measurement accuracy may be poor. This paper presents a study of Doppler-only tracking idea, where the strength of a passive radar velocity measurement is exploited. Moreover, Doppler-only localization is also presented. Tracking system simulations are shown as a proof-of-concept, where a significant improvement of tracking accuracy was achieved.	algorithm;fm broadcasting;radar;simulation;spectral density estimation;systems architecture;tracking system;transmitter;velocity (software development)	Piotr Krysik;Maciej Wielgo;Jacek Misiurewicz;Anna Kurowska	2014	17th International Conference on Information Fusion (FUSION)		early-warning radar;man-portable radar;continuous-wave radar;electronic engineering;radar tracker;radar engineering details;radar lock-on;radar configurations and types;semi-active radar homing;telecommunications;fire-control radar;passive radar;radar horizon;bistatic radar;low probability of intercept radar;pulse-doppler radar;3d radar;radar imaging;radar;remote sensing	Mobile	49.413354505634274	2.100005946313045	49974
7cd5ef3a0918737d7354a8ea4de3097cf173a3e3	in-network view synthesis for interactive multiview video systems	virtual view synthesis interactive multiview video systems in network reference view synthesis distributed cloud network architecture cloudlets resource rich proxies 3d scene reference view selection problem distortion minimization view navigation window np hard problem polynomial time algorithm dynamic programming;video signal processing cloud computing computational complexity computer graphics dynamic programming interactive systems optical distortion;distortion;navigation;interactive systems depth image based rendering network processing cloud assisted applications;depth image based rendering dibr;three dimensional displays;cloud computing cameras navigation distortion bandwidth three dimensional displays delays;cloud assisted applications;bandwidth;interactive systems;network processing;cameras;delays;cloud computing	In multiview applications, camera views can be used as reference views to synthesize additional virtual viewpoints, allowing users to freely navigate within a 3D scene. However, bandwidth constraints may restrict the number of reference views sent to clients, limiting the quality of the synthesized viewpoints. In this work, we study the problem of in-network reference view synthesis aimed at improving the navigation quality at the clients. We consider a distributed cloud network architecture, where data stored in a main cloud is delivered to end users with the help of cloudlets, i.e., resource-rich proxies close to the users. We argue that, in case of limited bandwidth from the cloudlet to the users, re-sampling at the couldlet the viewpoints of the 3D scene (i.e., synthesizing novel virtual views in the cloudlets to be used as new references to the decoder) is beneficial compared to mere subsampling of the original set of camera views. We therefore cast a new reference view selection problem that seeks the subset of views minimizing the distortion over a view navigation window defined by the user under bandwidth constraints. We prove that the problem is NP-hard, and we propose an effective polynomial time algorithm using dynamic programming to solve the optimization problem under general assumptions that cover most of the multiview scenarios in practice. Simulation results confirm the performance gain offered by virtual view synthesis in the network.	chroma subsampling;cloudlet;distortion;dynamic programming;internet access;mathematical optimization;np-hardness;network architecture;optimization problem;p (complexity);sampling (signal processing);selection algorithm;simulation;view synthesis	Laura Toni;Gene Cheung;Pascal Frossard	2016	IEEE Transactions on Multimedia	10.1109/TMM.2016.2537207	computer vision;navigation;simulation;distortion;cloud computing;telecommunications;computer science;theoretical computer science;operating system;world wide web;bandwidth	Visualization	46.61584016676403	-22.790653543566453	50009
304a55e8915a2d32473e5b9718509211ee173336	a modified differential evolution with a random disturbance mechanism for global optimization	economic dispatch problem differential evolution benchmark functions;statistics;optimization;economics;next generation networking;algorithm design and analysis;sociology;benchmark testing;optimization benchmark testing sociology statistics economics algorithm design and analysis next generation networking	In this paper, a new modified Differential Evolution (DE) algorithm is proposed. A new mutation operation is used in this algorithm, besides, a random disturbance mechanism is introduced before selection operation. The proposed DE is used to solve ten typical benchmark functions and economic dispatch problem with valve-point effect. The economic dispatch problem is tested on 13-unit system and 40-unit system respectively, each case will be run 50 times. The results have been compared with the results of other algorithms. The efficiency and effectiveness of the proposed DE in global optimization is demonstrated via the simulation results.	algorithm;benchmark (computing);converge;convex optimization;differential evolution;dynamic dispatch;global optimization;mathematical optimization;simulation	Jia-Huan He;Xiangdong Wang	2015	2015 International Conference on Control, Automation and Information Sciences (ICCAIS)	10.1109/ICCAIS.2015.7338700	algorithm design;benchmark;mathematical optimization;simulation;next-generation network;computer science;engineering;mathematical economics;statistics	EDA	27.44221091216656	-5.931564291953248	50019
1896b1ed92280b913b808d69c6eb64653a67981a	variance analysis and adaptive control in intelligent system based on gaussian model	eda;estimation of distribution algorithm;intelligent system;principle component analysis;adaptive variance control;pca	Estimation of distribution algorithms (EDAs) are intelligent systems that take advantage of statistical learning techniques. The distribution of promising regions in the search space is estimated and probabilistically guide the new particles’ searching towards them in EDAs. But EDAs cannot solve complex optimisation problems reliably and efficiently because of premature convergence and difficulty of complex probabilistic model learning. This paper presents a PCA-EDA algorithm which introduces principal component analysis (PCA) into simple Gaussian model-based EDA. PCA-EDA is aimed to keep the balance of accuracy and efficiency of probabilistic model learning, as well as to avoid premature convergence by PCA’s ability of analysis of Gaussian model’s variance. From the results of numerical experiments, it is showed that the proposed method is feasible and effective for complex optimisation problems.	artificial intelligence;electronic design automation;estimation of distribution algorithm;evolutionary computation;experiment;machine learning;mathematical optimization;mixture model;nonlinear system;numerical analysis;premature convergence;principal component analysis;regular expression;statistical model	Jun Liu;Yishou Wang;Hongfei Teng	2013	IJMIC	10.1504/IJMIC.2013.051930	econometrics;electronic design automation;computer science;machine learning;statistics;principal component analysis	ML	29.027608857771032	-10.448622065526157	50021
15d160176fdd17610b2c01b3f665ec521fbd1277	neural network predictive control for vanadium redox flow battery		The vanadium redox flow battery (VRB) is a nonlinear system with unknown dynamics and disturbances. The flowrate of the electrolyte is an important control mechanism in the operation of a VRB system. Too low or too high flowrate is unfavorable for the safety and performance of VRB.This paper presents a neural network predictive control scheme to enhance the overall performance of the battery. A radial basis function (RBF) network is employed to approximate the dynamics of the VRB system. The genetic algorithm (GA) is used to obtain the optimum initial values of the RBF network parameters.The gradient descent algorithm is used to optimize the objective function of the predictive controller. Compared with the constant flowrate, the simulation results show that the flowrate optimized by neural network predictive controller can increase the power delivered by the battery during the discharge and decrease the power consumed during the charge.	approximation algorithm;artificial neural network;control theory;discharger;genetic algorithm;gradient descent;loss function;mathematical optimization;network model;nonlinear system;optimization problem;radial (radio);radial basis function network;simulation;software release life cycle	Haifeng Shen;Xin-Jian Zhu;Meng Shao;Hongfei Cao	2013	J. Applied Mathematics	10.1155/2013/538237	control theory	AI	35.667716336875536	-5.885730796376408	50086
2575d72c5f3e311ea05773f629484ffbcc432673	monotonicity of failure rate and mean residual life function of a gamma-type model	monotonic and non monotonic failure rates;fiabilidad;reliability;matematicas aplicadas;mathematiques appliquees;porcentaje falla;fonction repartition;fonction monotone;relacion orden;loi gamma;ordering;taux defaillance;fonction transformation;gamma function;funcion monotona;monotonie;funcion distribucion;ley gama;relation ordre;residual mean life;distribution function;order relations;location of turning points;fiabilite;monotonicity;hazard rate;monotonic function;turning point;fonction generalisee;gamma distribution;failure rate;generalized function;monotonia;fonction gamma;applied mathematics;funcion generalizada;funcion gama;transform function;vie residuelle moyenne	In reliability studies, the sense of variation of the hazard rate is of major concern since it indicates system wear out or burn in or in some cases a non-monotonic hazard rate. In this paper, we consider a generalized gamma distribution and study, in detail, the monotonicity of its failure rate and the mean residual life function. This involves the investigation of the monotonic structure of a transform function and then using this structure, we study the monotonic structure of the failure rate and the mean residual life function. Some of the known results follow as special cases of our results.	failure rate	Ramesh C. Gupta;Sergey Lvin	2005	Applied Mathematics and Computation	10.1016/j.amc.2004.04.121	mathematical analysis;monotonic function;calculus;mathematics;statistics	ECom	33.43717693741448	-19.77706867776276	50163
9d10a2380405f29b0da02c631fcd8300bbd97cea	simplified 2dpalmhash code for secure palmprint verification		2DPalmHash Code (2DPHC) was proposed as a cancelable code for secure palmprint verification. In order to relieve the vertical and horizontal dislocation problems, palmprint codes, including 2DPHC, need to be shifted both in horizontal and vertical directions and matched repeatedly, which leads to high computational complexity. However, according to our analysis, horizontal-shift matching can be ignored. Therefore, the multiple-shift matching of 2DPHC can be greatly simplified. Simplified 2DPHC (S2DPHC) has three-fold advantages: (1) reduces matching complexity; (2) enhances changeability performance; (3) improves verification performance. Furthermore, the superiorities of S2DPHC over 2DPHC in terms of changeability and verification performances are validated via rigorously analysis and extensive experimentation.	code;computational complexity theory;experiment;fingerprint;performance	Lu Leng;Andrew Beng-Jin Teoh;Ming Li	2016	Multimedia Tools and Applications	10.1007/s11042-016-3458-3	computer science;theoretical computer science;computer security	Security	36.06911249198482	-10.507253850603561	50199
c6231eee1d909a48cc69464f24bb91f1b5f07a9a	a new genetic algorithm using large mutation rates and population-elitist selection (galme)	simulated annealing genetic algorithm large mutation rates population elitist selection galme function optimization local search global search global optimum performance;mutation rate;simulated annealing;function optimization;focus of attention;genetic algorithm;genetic algorithms;search problems;search problems genetic algorithms simulated annealing;high performance;local search;genetic algorithms genetic mutations simulated annealing optimization methods testing stochastic processes search methods skeleton	Genetic algorithms (GAs) are promising for function optimization. Methods for function optimization are required to perform local search as well as global search in a balanced way. It is recognized that the traditional GA is not well suited to local search. I have tested algorithms combining various ideas to develop a new genetic algorithm to obtain the global optimum effectively. The results show that the performance of a genetic algorithm using large mutation rates and population-elitist selection (GALME) is superior. This paper describes the GALME and its theoretical justification, and presents the results of experiments, compared to the traditional GA. Within the range of the experiments, it turns out that the performance of GALME is remarkably superior to that of the traditional GA.	genetic algorithm;selection (genetic algorithm)	Hisashi Shimodaira	1996		10.1109/TAI.1996.560396	quality control and genetic algorithms;mathematical optimization;meta-optimization;genetic algorithm;simulated annealing;cultural algorithm;tabu search;computer science;bioinformatics;derivative-free optimization;local search;hill climbing;genetic operator;machine learning;genetic representation;line search;adaptive simulated annealing;metaheuristic;guided local search	NLP	26.596874606888182	-5.56384230645121	50302
f742185edb12c0d046165383b11ae2e79b216b16	a new construction algorithm for symmetrical reversible variable-length codes from the huffman code	construction process;caracteristica funcionamiento;reversible variable length codes;video streaming;analisis estadistico;transmission error;optimal code;code optimal;codigo longitud variable;variable length code;error transmision;probabilistic approach;huffman codes;huffman code;design method;senal video;statistical analysis;signal video;codigo huffman;code longueur variable;enfoque probabilista;approche probabiliste;code huffman;caracteristique fonctionnement;analyse statistique;video signal;error resilience;codigo optimal;performance characteristic;erreur transmission	Variable-length codes (VLCs) improve coding performance using statistical characteristics of source symbols; however, VLCs have disastrous effects from bit errors in noisy transmission environments. In order to overcome problems with VLCs, reversible variable-length codes (RVLCs) have been introduced as one of the error resilience tools due to their error recovering capability for corrupted video streams. Still, existing RVLCs are complicated in the design and have some rooms for improvement in coding efficiency. In this paper, we propose a new design method for a symmetrical RVLC from the optimal Huffman code table. The proposed algorithm has a simpler construction process and also demonstrates an improved performance in terms of the average codeword length than other symmetrical RVLC algorithms.	algorithm;algorithmic efficiency;code (cryptography);code word;gene expression programming;huffman coding;streaming media;variable-length code	Wook-Hyun Jeong;Yo-Sung Ho	2003		10.1007/978-3-540-39737-3_84	telecommunications;computer science;theoretical computer science;algorithm;statistics;huffman coding	EDA	45.62599752531485	-12.272451582931028	50332
3e1dd9fb09c6cddc6bf9d38686bea81986f9dabe	a multi-objective artificial bee colony algorithm	artificial bee colony;multi objective optimization;multi objective artificial bee colony	This work presents a multi-objective optimization method based on the artificial bee colony, called the MOABC, for optimizing problems with multiple objectives. The MOABC uses a grid-based approach to adaptively assess the Pareto frontmaintained in an external archive. The external archive is used to control the flying behaviours of the individuals and structuring the bee colony. The employed bees adjust their trajectories based on the non-dominated solutionsmaintained in the external archive. On the other hand, the onlooker bees select the food sources advertised by the employed bees to update their positions. The qualities of these food sources are computed based on the Pareto dominance notion. The scout bees are used by the MOABC to get rid of food sources with poor qualities. The proposed algorithm was evaluated on a set of standard test problems in comparison with other state-of-the-art algorithms. Experimental results indicate that the proposed approach is competitive compared to other algorithms considered in this work. © 2011 Elsevier B.V. All rights reserved.	archive;artificial bee colony algorithm;exploit (computer security);mathematical optimization;multi-objective optimization;pareto efficiency	Reza Akbari;Ramin Hedayatzadeh;Koorush Ziarati;Bahareh Hassanizadeh	2012	Swarm and Evolutionary Computation	10.1016/j.swevo.2011.08.001	engineering;artificial intelligence;operations management;bees algorithm;artificial bee colony algorithm;ecology	AI	26.05428550838178	-3.9801524321728965	50336
7b15b92489cec32983b39c2a618331bb9565707f	efficient reversible data hiding in encrypted image with public key cryptosystem	reversible data hiding;image encryption;public key cryptosystem;homomorphic property;probabilistic property;paillier cryptosystem	This paper proposes a new reversible data hiding scheme for encrypted images by using homomorphic and probabilistic properties of Paillier cryptosystem. The proposed method can embed additional data directly into encrypted image without any preprocessing operations on original image. By selecting two pixels as a group for encryption, data hider can retrieve the absolute differences of groups of two pixels by employing a modular multiplicative inverse method. Additional data can be embedded into encrypted image by shifting histogram of the absolute differences by using the homomorphic property in encrypted domain. On the receiver side, legal user can extract the marked histogram in encrypted domain in the same way as data hiding procedure. Then, the hidden data can be extracted from the marked histogram and the encrypted version of original image can be restored by using inverse histogram shifting operations. Besides, the marked absolute differences can be computed after decryption for extraction of additional data and restoration of original image. Compared with previous state-of-the-art works, the proposed scheme can effectively avoid preprocessing operations before encryption and can efficiently embed and extract data in encrypted domain. The experiments on the standard image files also certify the effectiveness of the proposed scheme.	circuit restoration;cryptosystem;embedded system;encryption;experiment;hidden surface determination;pixel;preprocessor;public-key cryptography	Shijun Xiang;Xinrong Luo	2017	EURASIP J. Adv. Sig. Proc.	10.1186/s13634-017-0496-6	encryption;theoretical computer science;image file formats;cryptosystem;computer science;public-key cryptography;histogram;homomorphic encryption;paillier cryptosystem;modular multiplicative inverse	Graphics	38.95789299586265	-10.704002106019637	50406
7da895533ae545e1b78bf6704271b198dca72a99	on the capacity of evolution strategies to statistically learn the landscape	limit distributions of order statistics;extreme value distributions;covariance matrix adaptation;theory of evolution strategies;landscape hessian;statistical learning	We investigate the covariance matrix when constructed by Evolution Strategies (ESs) operating with the selection operator alone. We model continuous generation of candidate solutions about quadratic basins of attraction, with deterministic selection of the decision vectors that minimize the objective function values. Our goal is to rigorously show that accumulation of winning individuals carries the potential to reveal valuable information about the search landscape. We first show that the statistically-constructed covariance matrix over such winning decision vectors shares the same eigenvectors with the Hessian matrix about the optimum. We then provide an analytic approximation of this covariance matrix for a non-elitist multi-child (1,\lambda)-strategy, which holds for a large population size \lambda.	approximation;evolution strategy;hessian;loss function;optimization problem;tree accumulation	Ofer M. Shir;Jonathan Roslund;Amir Yehudayoff	2016		10.1145/2908961.2909057	estimation of covariance matrices;mathematical optimization;combinatorics;cma-es;computer science;covariance;machine learning;mathematics;statistics;covariance function	ML	30.55022532340885	-12.238195338832508	50421
5fda3cd03e97b5b9d690913a6c1bae0da59ac08c	digital image watermarking using balanced multiwavelets	transformation ondelette;filigranage numerique;protection information;digital watermarking;traitement signal;teledetection;watermarking;metodo adaptativo;data hiding;orthogonality;metodo analitico;optical memory;image numerique;synchronisation image coding data compression watermarking wavelet transforms adaptive codes spread spectrum communication gaussian distribution;game theory;image coding;dvd;scalar wavelets balanced multiwavelets data hiding embedding capacity game theory image watermarking information theory;spread spectrum;robust watermarking;image processing;memoria optica;scalar wavelets;data compression;televigilancia;complexite calcul;espectro ensanchado;methode echelle multiple;implementation;real time;procesamiento imagen;technique video;real time processing;metodo escala multiple;methode adaptative;adaptive codes;curva gauss;tecnica video;computer;traitement image;statistical model;embedding capacity;data recovery;algorithme;synchronisation;wavelet transforms;algorithm;tratamiento tiempo real;remote supervision;complejidad computacion;traitement temps reel;spread spectrum communication;g600 software engineering;digital images watermarking robustness computational complexity broadcasting monitoring dvd spread spectrum communication gaussian distribution data encapsulation;spectre etale;proteccion informacion;computational complexity;telesurveillance;synchronization;signal processing;information protection;analytical method;remote sensing;robustesse;filigrana digital;adaptive method;electrical;imagen numerica;teledeteccion;modele statistique;loi normale;generalized gaussian distribution;memoire optique;methode analytique;video technique;robustness;modelo estadistico;multiscale method;digital image watermarking;sincronizacion;image watermarking;digital image;transformacion ondita;implementacion;video watermarking;recuperation donnee;procesamiento senal;article;balanced multiwavelets;gaussian distribution	In this paper, a robust watermarking algorithm using balanced multiwavelet transform is proposed. The latter transform achieves simultaneous orthogonality and symmetry without requiring any input prefiltering. Therefore, considerable reduction in computational complexity is possible, making this transform a good candidate for real-time watermarking implementations such as audio broadcast monitoring and DVD video watermarking. The embedding scheme is image adaptive using a modified version of a well-established perceptual model. Therefore, the strength of the embedded watermark is controlled according to the local properties of the host image. This has been achieved by the proposed perceptual model, which is only dependent on the image activity and is not dependent on the multifilter sets used, unlike those developed for scalar wavelets. This adaptivity is a key factor for achieving the imperceptibility requirement often encountered in watermarking applications. In addition, the watermark embedding scheme is based on the principles of spread-spectrum communications to achieve higher watermark robustness. The optimal bounds for the embedding capacity are derived using a statistical model for balanced multiwavelet coefficients of the host image. The statistical model is based on a generalized Gaussian distribution. Limits of data hiding capacity clearly show that balanced multiwavelets provide higher watermarking rates. This increase could also be exploited as a side channel for embedding watermark synchronization recovery data. Finally, the analytical expressions are contrasted with experimental results where the robustness of the proposed watermarking system is evaluated against standard watermarking attacks.	additive white gaussian noise;algorithm;bch code;coefficient;computational complexity theory;data compression;digital image;digital watermarking;embedded system;gaussian blur;hamming code;jpeg;real-time clock;repetition code;side-channel attack;statistical model;watermarking attack;wavelet;wiener filter	Lahouari Ghouti;Ahmed Bouridane;Mohammad K. Ibrahim;Said Boussakta	2006	IEEE Transactions on Signal Processing	10.1109/TSP.2006.870624	game theory;synchronization;telecommunications;image processing;information theory;digital watermarking;computer science;theoretical computer science;signal processing;mathematics;watermark;spread spectrum;algorithm;statistics	Vision	43.935448822354225	-11.150636113358681	50428
462f0984703a16a9beff2ee88674ddea705dea60	image compression based on side-match vq and soc	image coding;data compression;decoding;performance comparison;system on a chip;search order coding;indexes;vector quantisation data compression image coding;vector quantization;image compression;indexation;computational efficiency image compression side match vector quantization search order coding algorithm index table smvq compression technique compression efficiency left upper coding;image coding vector quantization algorithm design and analysis bit rate image storage clustering algorithms partitioning algorithms iterative algorithms digital images computer applications;vector quantizer;vector quantisation;computational efficiency;computer simulation;algorithm design and analysis;smvq;search order coding image compression smvq	A novel image compression scheme that takes advantages of side-match vector quantization (SMVQ) and search-order-coding (SOC) algorithm is proposed in this article. In the proposed scheme, the image to be compressed is firstly encoded into an index table by applying the traditional SMVQ compression technique. Then, the index table of image is further compressed based on the ordinary SOC algorithm. To improve the compression efficiency of the proposed scheme, a modified search-order-coding algorithm, called left-upper-coding (LUC), is designed. The performance comparison between the two SOC algorithms has been conducted in our computer simulation. Experimental results show that the SOC algorithm functions very well with SMVQ, and the LUC algorithm is more feasible for compressing the SMVQ indexes of image when the computational efficiency is concerned.	computation;computer simulation;genetic algorithm;image compression;vector quantization	Shih-Chieh Shie;Long-Tai Chen	2009	2009 Digital Image Computing: Techniques and Applications	10.1109/DICTA.2009.68	data compression;computer simulation;system on a chip;database index;algorithm design;computer vision;image compression;computer science;theoretical computer science;machine learning;mathematics;vector quantization	Vision	43.62554409036864	-13.486814297026173	50517
3d338226e367f9551529ab6eedff2033b3c674df	finite variance unbiased estimation of stochastic differential equations		We develop a new unbiased estimation method for Lipschitz continuous functions of multi-dimensional stochastic differential equations with Lipschitz continuous coefficients. This method provides a finite variance estimator based on a probabilistic representation which is similar to the recent representations obtained through the parametrix method and recursive application of the automatic differentiation formula. Our approach relies on appropriate change of variables to carefully handle the singular integrands appearing in the iterated integrals of the probabilistic representation. It results in a scheme with randomized intermediate times where the number of intermediate times has a Pareto distribution.	algorithmic efficiency;automatic differentiation;coefficient;contraction mapping;iteration;loss function;optimization problem;pareto efficiency;randomized algorithm;recursion;technological singularity	Ankush Agarwal;Emmanuel Gobet	2017	2017 Winter Simulation Conference (WSC)	10.1109/WSC.2017.8247930	estimator;iterated function;parametrix;differential equation;stochastic differential equation;probabilistic logic;lipschitz continuity;change of variables;mathematical analysis;mathematics	ML	32.240077391419824	-15.897387251298124	50624
c6bc6c87f2995e82ce7b00735a76da3f13e9a5d1	finite markov chain analysis of classical differential evolution algorithm	computacion informatica;journal;ciencias basicas y experimentales;continuous optimization;期刊论文;matematicas;convergence in probability;grupo a;differential evolution algorithm;markov chain	Theoretical analyses of algorithms are important to understand their search behaviors and develop more efficient algorithms. Compared with the plethora of works concerning the empirical study of the differential evolution (DE), little theoretical research has been done to investigate the convergence properties of DE so far. This paper focuses on theoretical researches on the convergence of DE and presents a convergent DE algorithm. First of all, it is proved that the classical DE cannot converge to the global optimal set with probability 1 by using the property that it cannot escape from a local optimal set. Inspired by the characteristics of the elitist genetic algorithm, this paper proposed a modified DE to overcome the disadvantage. The proposed algorithm employs two operators that assist it in escaping from a local optimal set and enhance the diversity of the population. And it is then verified that the proposed algorithm is capable of converging to global optima with probability 1. The theoretical research of this paper is undertaken in a finite discrete set, and the analysis tool used is the Markov chain. The numerical experiments are conducted on a deceptive function and a set of benchmark functions. The experimental results support the theoretical analyses on the convergence performances of the classical and modified DE algorithm.	algorithm;differential evolution;markov chain	Zongbo Hu;Shengwu Xiong;Qinghua Su;Zhixiang Fang	2014	J. Computational Applied Mathematics	10.1016/j.cam.2014.02.034	markov chain;mathematical optimization;mathematical analysis;convergence of random variables;calculus;mathematics;continuous optimization;algorithm;statistics	Theory	28.057190808217445	-4.652377481844355	50659
32611cd9eaa917d5bdcccd688799768afbd63579	improving estimation of distribution algorithm on multimodal problems by detecting promising areas	gaussian processes evolutionary computation;histograms;sociology covariance matrices histograms estimation clustering algorithms joints;joints;promising areas estimation of distribution algorithms edas multimodal problems;estimation;covariance matrices;期刊论文;clustering algorithms;evolutionary algorithms estimation of distribution algorithm multiple submodels maintenance technique maintaining and processing submodels maps technique promising areas detection optimization speed gaussian model;sociology	In this paper, a novel multiple sub-models maintenance technique, named maintaining and processing sub-models (MAPS), is proposed. MAPS aims to enhance the ability of estimation of distribution algorithms (EDAs) on multimodal problems. The advantages of MAPS over the existing multiple sub-models based EDAs stem from the explicit detection of the promising areas, which can save many function evaluations for exploration and thus accelerate the optimization speed. MAPS can be combined with any EDA that adopts a single Gaussian model. The performance of MAPS has been assessed through empirical studies where MAPS is integrated with three different types of EDAs. The experimental results show that MAPS can lead to much faster convergence speed and obtain more stable solutions than the compared algorithms on 12 benchmark problems.	benchmark (computing);convergence (action);entity name part qualifier - adopted;estimation of distribution algorithm;evaluation;f-18 16 alpha-fluoroestradiol;local optimum;map;mathematical optimization;modal logic;multimodal interaction;normal statistical distribution;performance;sensor;solutions	Peng Yang;Ke Tang;Xiaofen Lu	2015	IEEE Transactions on Cybernetics	10.1109/TCYB.2014.2352411	econometrics;mathematical optimization;estimation;machine learning;histogram;mathematics;cluster analysis;statistics	ML	28.69548159554607	-7.12597099749112	50664
6eab5f0e170dd5aada58ed4530e54e6efa0c719c	a hybrid method for estimating the process change point using support vector machine and fuzzy statistical clustering	statistical process control;support vector machine;fuzzy statistical clustering;change point	Graphical abstractDisplay Omitted HighlightsA new hybrid method is developed for estimating the process change point in x ? control chart.An effective sets of features were extracted and the control chart patterns (CCP) are classified using support vector machines (SVM).Change point estimation methods were implemented for detecting the process change point.The proposed hybrid method provides a more accurate estimate of the process change point. Control charts are the most popular process monitoring techniques designed to determine whether a process is in a state of statistical control or not. When a process change occurs, the control chart exhibits an out-of-control signal. In most cases, the signal is followed by a substantial amount of delay. To address this drawback, supplementary techniques have been considered to be employed along with the control charts to identify the exact time of the process change. This paper presents a hybrid method for estimating the change point on x ? chart when neither the change type nor its magnitude are known. For this purpose, two sets of features with the most discriminatory power between x ? chart patterns are selected. Then the feature vectors are extracted from the control chart patterns (CCPs) and served as an input to a classification scheme which is comprised of several support vector machine (SVM) classifiers. After parameters tuning, the classifiers are built and used for classifying the CCPs and identifying the change type. Once the change type is determined, the fuzzy statistical clustering (FSC) and the maximum likelihood (ML) estimators are employed to identify the process change point. The performance of selected features, the classification scheme, and the change point estimators are evaluated by conducting several simulation studies. Empirical results show that in identifying the change types, the proposed classification scheme is more accurate than two recent CCP classification methods. The results also confirm that the proposed hybrid method offers an accurate estimate of the process change point, as compared to the most recent methods developed for the change point estimation.	cluster analysis;support vector machine	M. S. Kazemi;K. Kazemi;M. A. Yaghoobi;H. Bazargan	2016	Appl. Soft Comput.	10.1016/j.asoc.2015.11.021	support vector machine;computer science;machine learning;pattern recognition;data mining;mathematics;statistical process control;statistics	ML	28.03043002686726	-19.98326572819592	50688
0337cf8212f22bfa8e2742681ee3c76f6b09e361	bayesian nonparametric inference of switching dynamic linear models	complex dynamical phenomena;sistema lineal;bayes estimation;modelo dinamico;unsupervised learning;linear systems;traitement signal;switching dynamic linear model;vector autoregression;problema valor limite;complex dynamics;dirichlet process;state space methods;electronic mail;hidden markov models switches bayesian methods superluminescent diodes biological system modeling electronic mail signal processing;dancing honey bee switching dynamic linear model bayesian nonparametric inference complex dynamical phenomena conditionally linear dynamical mode vector autoregressive process hierarchical dirichlet process ibovespa stock index target tracking sampling algorithm state sequence;nonparametric statistics;modelo autorregresivo;time varying systems autoregressive processes bayes methods inference mechanisms linear systems nonparametric statistics sampling methods target tracking;hidden markov model;flexibilidad;bayes methods;methode bayes;echantillonnage;boundary value problem;honey bee;dynamic model;modele markov variable cachee;modele lineaire;vector autoregressive process;biological system modeling;time varying systems;dirichlet problem;bayesian methods;processus autoregressif;sparse set;inference mechanisms;autoregressive process;modelo lineal;probabilistic approach;blanco movil;probleme dirichlet;linear system;angular distribution;bayesian nonparametric inference;bayesian method;autoregressive model;dynamical system;sampling;algorithme;systeme dynamique;algorithm;estimacion bayes;hidden markov models;analyse serie temporelle;state space method;autoregressive processes;time series analysis;methode espace etat;switching system;enfoque probabilista;approche probabiliste;signal processing;conditionally linear dynamical mode;problema dirichlet;linear dynamical system;modele dynamique;superluminescent diodes;linear model;poursuite cible;bayesian nonparametrics;autoconmutador;distribucion angular;autocommutateur;cible mobile;automatic relevance determination;flexibilite	Many complex dynamical phenomena can be effectively modeled by a system that switches among a set of conditionally linear dynamical modes. We consider two such models: the switching linear dynamical system (SLDS) and the switching vector autoregressive (VAR) process. Our Bayesian nonparametric approach utilizes a hierarchical Dirichlet process prior to learn an unknown number of persistent, smooth dynamical modes. We additionally employ automatic relevance determination to infer a sparse set of dynamic dependencies allowing us to learn SLDS with varying state dimension or switching VAR processes with varying autoregressive order. We develop a sampling algorithm that combines a truncated approximation to the Dirichlet process with efficient joint sampling of the mode and state sequences. The utility and flexibility of our model are demonstrated on synthetic data, sequences of dancing honey bees, the IBOVESPA stock index and a maneuvering target tracking application.	algorithm;approximation;approximation algorithm;autoregressive model;batch processing;dynamical system;experiment;finite difference;gibbs sampling;hidden markov model;iw engine;image scaling;information;linear model;markov chain monte carlo;mixture model;network switch;online and offline;particle filter;recursion;relevance;sampling (signal processing);scalability;sparse language;sparse matrix;spatial variability;synthetic data;time series;unsupervised learning;vector autoregression;volatility	Emily B. Fox;Erik B. Sudderth;Michael I. Jordan;Alan S. Willsky	2011	IEEE Transactions on Signal Processing	10.1109/TSP.2010.2102756	sampling;econometrics;bayesian probability;machine learning;mathematics;autoregressive model;linear system;hidden markov model;statistics	ML	34.89076777155074	-23.341502773113895	50690
44d940cd32cfa61956ea48e1c78e006ad399cb5e	optimizing linear functions with randomized search heuristics - the robustness of mutation	004;randomized search heuristics evolutionary algorithms linear functions running time analysis	The analysis of randomized search heuristics on classes of functions is fundamental for the understanding of the underlying stochastic process and the development of suitable proof techniques. Recently, remarkable progress has been made in bounding the expected optimization time of the simple (1+1) EA on the class of linear functions. We improve the best known bound in this setting from (1.39 + o(1))en lnn to en lnn+O(n) in expectation and with high probability, which is tight up to lower-order terms. Moreover, upper and lower bounds for arbitrary mutations probabilities p are derived, which imply expected polynomial optimization time as long as p = O((lnn)/n) and which are tight if p = c/n for a constant c. As a consequence, the standard mutation probability p = 1/n is optimal for all linear functions, and the (1+1) EA is found to be an optimal mutation-based algorithm. Furthermore, the algorithm turns out to be surprisingly robust since large neighborhood explored by the mutation operator does not disrupt the search. 1998 ACM Subject Classification F.2 [Analysis of algorithms and problem complexity]	analysis of algorithms;carrier-to-noise ratio;heuristic (computer science);like button;linear function;mathematical optimization;optimizing compiler;polynomial;randomized algorithm;stochastic process;with high probability	Carsten Witt	2012		10.4230/LIPIcs.STACS.2012.420	mathematical optimization;combinatorics;computer science;mathematics;algorithm	Theory	28.997406514765444	2.399705307628392	50824
b4dd683be041b94d7ceed687dac1a504051b76d4	optimized transmission of h.26l/jvt coded video over packet-lossy networks	optimisation;coding standard optimized transmission h 26l jvt coded video packet lossy networks hybrid coded video motion compensation spatial prediction error prone channels spatio temporal error propagation decoded video rate distortion optimization coding mode reference frame macroblock channel statistics decoder pixel distortion;motion compensation;decoding;visual communication video coding optimisation code standards motion compensation rate distortion theory statistics decoding;decoding image coding propagation losses rate distortion computer errors robustness shape distortion image processing operating systems;visual communication;reference frame;code standards;spatial prediction;rate distortion theory;video coding;error propagation;statistics;rate distortion optimization	Transmission of hybrid coded video including motion compensation and spatial prediction over error-prone channels results in the well-known problem of spatio-temporal error propagation at the decoder. A widely accepted standard-compliant technique to enhance the quality of the decoded video significantly is the more frequent introduction of intra-coded macroblocks. However, intra-coded information generally requires more bit rate. Therefore, a careful selection of intra-updates in terms of rate and distortion is necessary. A flexible and robust rate-distortion optimization technique is presented to select coding mode and reference frame for each macroblock. The channel statistics are included in the optimization process. We derive a method to obtain an estimate of the decoder pixel distortion at the encoder. The presented techniques are verified within the new H.26L/JVT video coding standard based on common test conditions.	code;cognitive dimensions of notations;data compression;distortion;encoder;h.264/mpeg-4 avc;lossy compression;macroblock;mathematical optimization;motion compensation;network packet;pixel;propagation of uncertainty;rate–distortion optimization;reference frame (video);software propagation;video coding format;whole earth 'lectronic link	Thomas Stockhammer;Thomas Wiegand;Stephan Wenger	2002		10.1109/ICIP.2002.1039915	reference frame;computer vision;rate–distortion theory;computer science;propagation of uncertainty;theoretical computer science;coding tree unit;mathematics;block-matching algorithm;rate–distortion optimization;motion compensation;statistics;visual communication;multiview video coding	AI	48.068308964013	-16.30064483202064	50848
490f43dcb4d875cf4170a0f454df9a29d4a1bd20	evolving neural networks in compressed weight space	indirect encoding;neuroevolution;search space;benchmark problem;evolving neural networks;image compression;fourier coefficient;weighted space;evolutionary algorithms;recurrent neural networks;recurrent neural network;evolutionary algorithm;frequency domain;high frequency;neural network	"""We propose a new indirect encoding scheme for neural networks in which the weight matrices are represented in the frequency domain by sets Fourier coefficients. This scheme exploits spatial regularities in the matrix to reduce the dimensionality of the representation by ignoring high-frequency coefficients, as is done in lossy image compression. We compare the efficiency of searching in this """"compressed"""" network space to searching in the space of directly encoded networks, using the CoSyNE neuroevolution algorithm on three benchmark problems: pole-balancing, ball throwing and octopus arm control. The results show that this encoding can dramatically reduce the search space dimensionality such that solutions can be found in significantly fewer evaluations"""	algorithm;artificial neural network;benchmark (computing);coefficient;image compression;line code;lossy compression;neuroevolution;the matrix	Jan Koutník;Faustino J. Gomez;Jürgen Schmidhuber	2010		10.1145/1830483.1830596	mathematical optimization;computer science;artificial intelligence;recurrent neural network;theoretical computer science;machine learning;evolutionary algorithm;time delay neural network;mathematics	ML	39.1679800929727	-16.231754170095545	50964
b890fa6b226fb4a94ed8725a1562f5950a768fd2	an efficient nearest neighbor search method	traitement signal;quantization;cuantificacion;full search;information compression;simulation;estimation non parametrique;search methods;simulacion;procesamiento de senales;recherche voisinage le plus proche;vecteur;quantification;compresion informacion;classification;nearest neighbor searches vector quantization testing search methods sorting performance evaluation encoding image communication communications society councils;recherche;algorithme;algorithm;non parametric estimation;algorritmo;signal processing;search methods quantization;compression information;pattern recognition;vector;vector quantizer;nearest neighbor search;reconnaissance forme;estimacion no parametrica;reconocimiento patron;investigacion;clasificacion	A simple, but efficient, nearest neighbor search algorithm is proposed and simulation results demonstrating its effectiveness in the case of vector quantization for a given source are presented. The simulation results indicate that use of this approach reduces the number of multiplications and additions to as low as 9 percent of those required for the conventional full search method. The reduction in the number of subtractions is also considerable. The increase in the number of comparisons is moderate, and therefore, the total number of operations can be as low as 28 percent of those required by the full search method. An additional advantage of the described algorithm is the fact that it requires no precomputations and/or extra memory.	nearest neighbor search	M. Reza Soleymani;Salvatore D. Morgera	1987	IEEE Trans. Communications	10.1109/TCOM.1987.1096830	best bin first;quantization;vector;biological classification;computer science;artificial intelligence;machine learning;signal processing;mathematics;nearest neighbor search;algorithm	Vision	45.318262053435326	-11.92461010294138	51034
147131d8311d06b0fea3e1b726d1b790ec3c9273	improved identification of hammerstein plants using new cpso and ipso algorithms	response matching;hammerstein model;artificial immune system;ga;clonal;control design;pso;convergence speed;cpso;ais;stability analysis;simulation study;flann;ipso;hybrid algorithm;evolutionary computing	Identification of Hammerstein plants finds extensive applications in stability analysis and control design. For identification of such complex plants, the recent trend of research is to employ nonlinear network and to train their weights by evolutionary computing tools. In recent years the area of Artificial Immune System (AIS) has drawn attention of many researchers due to its broad applicability to different fields. In this paper by combining the principles of AIS and PSO, we propose two new but simple hybrid algorithms called Clonal PSO (CPSO) and Immunized PSO (IPSO) which involve less complexity and offers better identification performance. Identification of few benchmark Hammerstein models is carried out through simulation study and the results obtained are compared with those obtained by standard PSO, Clonal and GA based methods. Various simulation results demonstrate that IPSO algorithm offers best identification performance compared to the other algorithms. Out of the two algorithms proposed, the CPSO is computationally simpler but offers identification performance nearly similar to its PSO counterpart.	algorithm;ipso alliance	Satyasai Jagannath Nanda;Ganapati Panda;Babita Majhi	2010	Expert Syst. Appl.	10.1016/j.eswa.2010.03.043	von neumann stability analysis;hybrid algorithm;automatic identification system;computer science;artificial intelligence;machine learning;artificial immune system;evolutionary computation	Crypto	31.14204911755918	-6.236410052595545	51063
2c44066cb95c4176ec839a127219ba99b70e9b15	orthogonal dynamic hill-climbing algorithm for dynamic optimization problems	dynamic optimization problem orthogonal dynamic hill climbing algorithm;optimisation;continuous variable;heuristic algorithms space technology design methodology design optimization algorithm design and analysis evolutionary computation computer science organizing optimization methods displays;orthogonal design;adaptive computing;self organization;hill climbing;numerical experiment;orthogonal design climbing algorithm dynamic optimization;dynamic optimization problem;dynamic optimization	"""An orthogonal hill-climbing algorithm for dynamic optimization problems with continuous variables (labeled ODHC ) is proposed in present paper. The local peak climber is not a solution x, but rather a """"niche"""", a small hyperrectangle. An orthogonal design method is employed on the niches for the niche to climb a potentially peak fast. An archive is used to store the latest found higher peaks for the ODHC algorithm learning from the past search. The randomly creating niches implement the global search. Numerical experiments show that the ODHC algorithm performs a lot better than the SOS (self organizing scouts) algorithm [J. Branke, T. Kaufler, C. Schmidt, and H. Schmeck. A multipopulation approach to dynamic optimization problems. Adaptive Computing in Design and Manufacturing. Springer, 2000.]."""	algorithm;archive;climber (beam);dynamic programming;experiment;fast fourier transform;hill climbing;mathematical optimization;niche blogging;numerical method;organizing (structure);randomness;reconfigurable computing;schmidt decomposition;self-organization;springer (tank)	Sanyou Zeng;Hui Shi;Guang Chen;Hugo de Garis;Lishan Kang;Lixin X. Ding	2006	2006 IEEE International Conference on Evolutionary Computation	10.1109/CEC.2006.1688463	mathematical optimization;self-organization;simulation;principle of orthogonal design;computer science;artificial intelligence;hill climbing;continuous optimization	Robotics	25.45621449996992	-2.088137069367684	51076
68b319b5311cc473d3d035ac7f50fcba5708afd1	dichotomous search in abc and its application in parameter estimation of software reliability growth models	artificial bee colony;optimisation;exponential model;efficient algorithm;computer model;benchmark problem;delayed s shaped model dichotomous search artificial bee colony parameter estimation software reliability growth models nature inspired algorithm swarming metaphor global optimization problems continuous space exponential model power model;software engineering;computational modeling;mathematical model software algorithms computational modeling optimization software reliability algorithm design and analysis benchmark testing;software reliability optimisation parameter estimation search problems;mathematical model;software algorithms;global optimization;software reliability artificial bee colony bidirectional optimization software engineering;optimization;search problems;shape modeling;parameter estimation;software reliability growth model;software reliability;bidirectional optimization;algorithm design;power modeling;algorithm design and analysis;benchmark testing	ABC (Artificial Bee Colony) is one of the most recent nature inspired algorithm (NIA) based on swarming metaphor. Proposed by Karaboga in 2005, ABC has proven to be a robust and efficient algorithm for solving global optimization problems over continuous space. In this paper, we propose a modified version of the ABC to improve its performance, in terms of converging to individual optimal point and to compensate the limited amount of search moves of original ABC. In modified version called Dichotomous ABC (DABC), the idea is to move dichotomously in both directions to generate a new trial point. The performance of the proposed algorithm is analyzed on five standard benchmark problems and also we explored the applicability of the proposed algorithm to estimate the parameters of software reliability growth models (SRGM). The proposed algorithm presents significant advantages in handling variety of modeling problems such as the exponential model, power model and Delayed S Shaped model.	abc;algorithm;benchmark (computing);estimation theory;global optimization;list of software reliability models;mathematical optimization;neural impulse actuator;software quality;software reliability testing;software system;time complexity	Tarun Kumar Sharma;Millie Pant;Ajith Abraham	2011	2011 Third World Congress on Nature and Biologically Inspired Computing	10.1109/NaBIC.2011.6089460	algorithm design;mathematical optimization;computer science;artificial intelligence;machine learning;statistics;global optimization	AI	27.856100695514833	-7.307955517234117	51172
7f3b4676a50adb8b44e5a70ed63ea13b2302e3f5	distributed video coding with adaptive selection of hash functions	wyner ziv;xin hao chen lu yu distributed video coding with adaptive selection of hash functions;hash;distributed video coding;hash function;collision;rate distortion optimization	We address the compression efficiency of feedback-free and hash-check distributed video coding, which generates and transmits a hash code of a source information sequence. The hash code helps the decoder perform a motion search. A hash collision is a special case in which the hash codes of wrongly reconstructed information sequences occasionally match the hash code of the source information sequence. This deteriorates the quality of the decoded image greatly. In this paper, the statistics of hash collision are analyzed to help the codec select the optimal trade-off between the probability of hash collision and the length of the hash code, according to the principle of rate-distortion optimization. Furthermore, two novel algorithms are proposed: (1) the nonzero prefix of coefficients (NPC), which indicates the count of nonzero coefficients of each block for the second algorithm, and also saves 8.4% bitrate independently; (2) the adaptive selection of hash functions (AHF), which is based on the NPC and saves a further 2%–6% bitrate on average. The detailed optimization of the parameters of AHF is also presented.	algorithm;code;codec;coefficient;collision (computer science);cryptographic hash function;data compression;distortion;file verification;mathematical optimization;np-completeness;rate–distortion optimization	Xin-Hao Chen;Lu Yu	2011	Journal of Zhejiang University SCIENCE C	10.1631/jzus.C1000198	double hashing;hash function;perfect hash function;merkle tree;primary clustering;quadratic probing;telecommunications;sha-2;collision resistance;computer science;theoretical computer science;hash chain;hash buster;rolling hash;algorithm;cryptographic hash function;fowler–noll–vo hash function;mdc-2;swifft;hash tree;hash filter	ML	47.947418895168575	-16.83264133505668	51227
995b4b43fac374ab3ccfd33fe65cea5f088f206a	validation of a parallel genetic algorithm for image reconstruction from projections	parallel genetic algorithm;algoritmo paralelo;parallel algorithm;image processing;reconstruction;distributed computing;procesamiento imagen;ds parallel genetic algorithm;langage java;algoritmo genetico;iterative algorithm;traitement image;genetics;algorithme parallele;reconstruction image;medical image;electron microscope;reconstruccion imagen;image reconstruction;algorithme genetique;calculo repartido;genetic algorithm;lenguaje java;reconstruction algorithm;javaspaces;calcul reparti;java language;x rays;molecular structure	The problem of accurate image reconstruction from projections has repeatedly arisen over the last decades in a large number of scientific, medical and technical fields. Reconstruction algorithms use data from electron microscopes to reconstruct molecular structures or X-ray projection data to compute medical images. Usually, the applied projection data are noisy and therefore iterative algorithms are used to solve numerically a number of equations. Theory and empirical results demonstrate that genetic algorithms (GA) can accurately solve a broad class of problems, especially if noisy input data are used. GA are based on the evolution of random tries by individuals, and therefore the time to find an appropriate solution is rather long. In this work, we use a parallel approach using JavaSpaces to speed up a genetic reconstruction algorithm. r 2003 Elsevier Science (USA). All rights reserved.	electron;genetic algorithm;iterative method;iterative reconstruction;numerical analysis;randomness;reconstruction from projections;software release life cycle;tuple space	Peter Knoll;Siroos Mirzaei	2003	J. Parallel Distrib. Comput.	10.1016/S0743-7315(03)00019-4	iterative reconstruction;parallel computing;genetic algorithm;molecule;image processing;computer science;artificial intelligence;theoretical computer science;mathematics;distributed computing;parallel algorithm;iterative method;algorithm;electron microscope	AI	44.026473306912834	3.307079620125504	51452
f9b50633952bfe794f5e6e2b14bcccf0bf3207f4	lut filters for quantized processing of signals	traitement signal;nonlinear filters;look up table;image coding;non linear filtering;image processing;hierarchical vector quantization;aproximacion;filtrado no lineal;procesamiento imagen;nonlinear filter;traitement image;approximation;approximation theory;cuantificacion vectorial;vector quantization;image processing lut filters look up table quantized signal processing input signal approximation nonlinear filters overlapped hierarchical vector quantization switching;table lookup signal processing nonlinear filters filtering vector quantization costs image analysis image processing digital filters;signal processing;vector quantizer;vector quantisation;table lookup;procesamiento senal;nonlinear filtering;filtrage non lineaire;image coding table lookup nonlinear filters approximation theory vector quantisation;look up table processing;quantification vectorielle	We introduce a method to perform filtering on approximations (quantized versions) of the input signal, which lends itself to a practical implementation solely based on look-up tables (LUTs). The LUT filter approximates the performance of some traditional nonlinear filters at a fraction of the cost. The filter is divided into an approximation stage that is constant for all filters and a filtering stage, which is one LUT that can change in order to implement different filters. We introduce an overlapped hierarchical vector quantization (OHVQ) scheme that is used as the approximation stage. The output is produced by mapping the OHVQ codes to filtered data. Hence, all processing is done via LUTs, even though the filter size needs to be small because of typical OHVQ contraints. Switching among filters demands changing pointers to only one small LUT. Preliminary analysis and image processing examples are shown, demonstrating the efficacy of the proposed method.	approximation;code;codebook;complexity;computer vision;filter (signal processing);image processing;lookup table;nonlinear system;particle filter;pixel;quantization (signal processing);vector quantization	Ricardo L. de Queiroz;Patrick A. Stein	2004	IEEE Transactions on Signal Processing	10.1109/TSP.2003.822357	network synthesis filters;nonlinear filter;computer vision;lookup table;image processing;computer science;theoretical computer science;approximation;signal processing;control theory;mathematics;prototype filter;vector quantization;approximation theory	Vision	50.659962627568774	-11.836944778807418	51473
056173cdd9dcdf1dcba8eb2ba8f220cc3c93e722	dct-domain coder for digital video applications	discrete cosine transform dct;low resolution;video compression;reference frame;motion estimation;dct domain motion estimation;complexity reduction;computational complexity;dct domain video encoder;mpeg 2;block matching;digital video;cosine transform	In this paper, we present an effective DCT-domain video encoder architecture that decreases the computational complexity of conventional hybrid video encoders by reducing the number of transform operations between the pixel and the DCT domains. The fixed video encoder architecture (such as a fixed DCT block of 8 × 8 size) and a huge number of DCT/IDCT transforms performed during the video encoding process limit the minimum possible computational load of conventional video encoders. In this study, we solve this problem by developing a flexible video encoder architecture, which reduces video encoder computational complexity by performing low-resolution coarse-step motion estimation operations in the DCT domain. When a high level of motion activity is detected, the video encoder slightly increases the computational complexity of the motion estimation operation by computing fine-search block matching for a small-size search window in a reference frame. The proposed DCT-domain video encoder architecture is based on the conventional hybrid coder and on a set of fast integer composition and decomposition DCT transforms. The set of transforms implements a technique for estimation of DCT coefficients of a block that is partitioned by the sub-blocks. Experimental results of this method were compared with the results of the conventional hybrid coder in terms of PSNR quality and computational complexity. This comparison shows that the computational complexity of the proposed encoder is lower by 26.8% with respect to the conventional hybrid video coder for the same objective PSNR quality.	algorithm;coefficient;computational complexity theory;data compression;digital video;discrete cosine transform;distortion;encoder;future internet;h.264/mpeg-4 avc;high-level programming language;level of detail;mpeg-2;motion compensation;motion estimation;peak signal-to-noise ratio;pixel;reference frame (video);streaming media;the matrix;video decoder	Evgeny Kaminsky;Alex Ginzburg;Ofer Hadar	2006	2006 International Conference on Information Technology: Research and Education	10.1007/s11554-010-0166-5	video compression picture types;data compression;reference frame;computer vision;image resolution;trellis quantization;computer science;theoretical computer science;discrete cosine transform;motion estimation;block-matching algorithm;mpeg-2;rate–distortion optimization;computational complexity theory;motion compensation;algorithm;reduction;computer graphics (images)	Vision	46.16506663468487	-17.90521526070983	51495
7d70099e6339e5d38ce7ee3d3e2ce3b590f24768	a design of htm spatial pooler for face recognition using memristor-cmos hybrid circuits	hierarchical temporal memory;memristor;pattern recognition hierarchical temporal memory memristor neuromorphic design machine learning feature extraction;visual databases cmos integrated circuits face recognition integrated circuit design learning artificial intelligence memristor circuits;memristors;neuromorphic design;training;prediction algorithms;face recognition accuracy htm spatial pooler design memristor cmos hybrid circuit design hierarchical temporal memory machine learning algorithm neocortex bit encoded inputs synapses ar database;face recognition;machine learning;feature extraction;memristors face recognition algorithm design and analysis feature extraction face training prediction algorithms;pattern recognition;face;algorithm design and analysis	Hierarchical Temporal Memory (HTM) is a machine learning algorithm that is inspired from the working principles of the neocortex, capable of learning, inference, and prediction for bit-encoded inputs. Spatial pooler is an integral part of HTM that is capable of learning and classifying visual data such as objects in images. In this paper, we propose a memristor-CMOS circuit design of spatial pooler and exploit memristors capabilities for emulating the synapses, where the strength of the weights is represented by the state of the memristor. The proposed design is validated on a challenging application of single image per person face recognition problem using AR database resulting in a recognition accuracy of 80%.	algorithm;autostereogram;cmos;circuit design;emulator;facial recognition system;floor and ceiling functions;html;hierarchical temporal memory;hybrid integrated circuit;machine learning;memristor;real-time computing;real-time transcription;scalability;spectral leakage;very-large-scale integration	Timur Ibrayev;Alex Pappachen James;Cory E. Merkel;Dhireesha Kudithipudi	2016	2016 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2016.7527475	facial recognition system;computer vision;memristor;computer science;artificial intelligence;machine learning	Vision	38.528715152332914	-1.0048550816824369	51506
14f6e3bf9a4ba9bad3f7c6c9114d68f3fc0ea74f	multidimensional rotations for quantization	multidimensional rotations;quantization;random noise analogue digital conversion data compression image coding speech coding telecommunication channels encoding;image coding;data compression;lattices;filters;speech coding;random variables;walsh hadamard transform multidimensional rotations laplacian data generalized gaussian data speech coding image coding independent identically distributed data overload characteristics granular characteristics lattice quantization image compression noisy channels scalar quantization vector quantisation;lattice quantization;random noise;laplace equations;walsh hadamard transform;scalar quantization;image compression;analogue digital conversion;generalized gaussian data;generalized gaussian;independent identically distributed;granular characteristics;multidimensional systems quantization laplace equations lattices image coding random variables filters reflection speech coding gaussian distribution;independent identically distributed data;telecommunication channels;vector quantisation;noisy channels;encoding;reflection;gaussian distribution;laplacian data;multidimensional systems;overload characteristics	Laplacian and generalized Gaussian data arise in image and speech coding. Simple rotations of independent, identically distributed Laplacian and generalized Gaussian data in multiple dimensions can improve the granular and overload characteristics for quantization. In this papel; we describe the practical properties of multidimensional rotations for both scalar and lattice quantization and then apply them to image compression over noisy channels.	image compression;quantization (signal processing);speech coding	Andy C. Hung;Teresa H. Y. Meng	1994		10.1109/DCC.1994.305910	normal distribution;data compression;random variable;discrete mathematics;reflection;quantization;multidimensional systems;image compression;computer science;theoretical computer science;speech coding;lattice;mathematics;algorithm;encoding;statistics	ML	48.21566062704943	-11.440179860795904	51525
ed883f85c67d9dd7c9ecb95bc17cb758316877fb	prediction in trend-renewal processes for repairable systems	weibull power law trend renewal process;trend renewal process;non homogeneous poisson process;future failure time prediction;power law process;prediction interval	Some problems of point and interval prediction in a trend-renewal process (TRP) are considered. TRP’s, whose realizations depend on a renewal distribution as well as on a trend function, comprise the non-homogeneous Poisson and renewal processes and serve as useful reliability models for repairable systems. For these processes, some possible ideas and methods for constructing the predicted next failure time and the prediction interval for the next failure time are presented. A method of constructing the predictors is also presented in the case when the renewal distribution of a TRP is unknown (and consequently, the likelihood function of this process is unknown). Using the prediction methods proposed, simulations are conducted to compare the predicted times and prediction intervals for a TRP with completely unknown renewal distribution with the corresponding results for the TRP with a Weibull renewal distribution and power law type trend function. The prediction methods are also applied to some real data.	simulation	Jürgen Franz;Alicja Jokiel-Rokita;Ryszard Magiera	2014	Statistics and Computing	10.1007/s11222-013-9393-5	econometrics;prediction interval;residual time;mathematics;inhomogeneous poisson process;statistics	HPC	30.556264676163817	-19.238032595736563	51585
bb4f2fd447c6178a68ae573a40688ae93a8d6aa0	application of copula and copula-cvar in the multivariate portfolio optimization	garch;empirical study;archimedean copula;copula;three dimensional;portfolio optimization;objective function;multi dimensional;monte carlo method;copula function;portfolio management;cvar	In this article we resort to the copula theory and CVaR measures in the portfolio management, using copula function and copulaCVaR to design the portfolio optimization. We initially apply the threedimensional Archimedean copula in the empirical study. After estimating the multi-dimensional copula, we use Monte Carlo method to generate the scenarios for the calculation of portfolio’s variance and CVaR. Then we apply the minimum of copula based standard variance and CVaR as the objective function of the portfolio programming. The multivariate demonstration indicates that the copula theory and copula based CVaR method does better in the portfolio management than the normal hypothesis.	cvar;loss function;mathematical optimization;modern portfolio theory;monte carlo method;optimization problem;simulation	Manying Bai;Lujie Sun	2007		10.1007/978-3-540-74450-4_21	financial economics;econometrics;economics;finance;portfolio optimization	ML	26.980903706880397	-20.72001770313331	51587
5bef23361edd85739be25995fed58efbf714f35c	an moea-based method to tune ea parameters on multiple objective functions	multiple objectives	In this paper, we demonstrate the benefits of using a multi-objective approach when tuning the parameters of an Evolutionary Algorithm. To overcome the specific challenges that arise when using a meta-algorithm for parameter tuning on multiple functions, we introduce a new algorithm called the Multi-Function Evolutionary Tuning Algorithm (M-FETA) that is able to approximate the parameter Pareto front effectively. The results of the experiments illustrate how the approximated Parameter Pareto front can be used to gain insights, identify ‘generalists’, and study the robustness of the algorithm to be tuned.	approximation algorithm;cluster analysis;distribution (mathematics);evolutionary algorithm;experiment;fitness function;interaction;moea framework;metaheuristic;multi-function printer;pareto efficiency;performance tuning;test case;test suite;thinking outside the box	Selmar K. Smit;A. E. Eiben;Zoltán Szlávik	2010			mathematical optimization;robustness (computer science);evolutionary algorithm;multi-objective optimization;mathematics	ML	25.0146396861465	-4.277985294533672	51615
0ff191aa212a36d0d3afb7b6ead6cebf519b5ee4	quantile estimation: a minimalist approach	relevant data;data analysis;minimalist approach;statistical data;fixed-point algorithm;quantile estimate;large amount;time quantiles;large data stream;management action;quantile estimation;fixed point;sequential estimation;standard deviation;exponential smoothing;fixed point arithmetic;statistical analysis;probability density function	Managing telecommunication networks involves collecting and analyzing large amounts of statistical data. The standard approach to estimating quantiles involves capturing all the relevant data (what may require significant storage/processing capacities), and performing an off-line analysis (what may delay management actions). It is often essential to estimate quantiles as the data are collected, and to take management actions promptly. Towards this goal, we present a minimalist approach to sequentially estimating constant/changing over time quantiles. We follow prior work and devise a fixed-point algorithm, which does not estimate the unknown probability density function at the quantile. Instead, our algorithm uses the log-odds transformation of the observed fractions, and the exponentially smoothed estimates of the standard deviation to update the quantile estimate. For large data streams, this algorithm can significantly reduce the amount of collected data and the complexity of data analysis.	algorithm;fixed-point iteration;minimalism (computing);online and offline;smoothing	Yury Bakshi;David A. Hoeflin	2006	Proceedings of the 2006 Winter Simulation Conference		sequential estimation;exponential smoothing;econometrics;probability density function;data mining;mathematics;fixed point;fixed-point arithmetic;data analysis;standard deviation;statistics	DB	26.10519929752298	-20.38642687109655	51634
f311ee943b38f6b0045d0f381aed421919581c15	quantum inspired genetic algorithms	complexity classes quantum inspired genetic algorithms evolutionary computing quantum mechanics terminology genetic algorithm travelling salesperson problem performance;performance;electrons;computational complexity genetic algorithms quantum theory travelling salesman problems;orbits;computational modeling;genetic algorithms quantum computing orbits electrons energy states quantum mechanics vectors wave functions computational modeling computer science;vectors;complexity class;quantum mechanics;computational complexity;quantum theory;travelling salesman problems;energy states;complexity classes;genetic algorithm;terminology;travelling salesperson problem;genetic algorithms;computer science;wave functions;quantum computing;quantum inspired genetic algorithms;evolutionary computing	|A novel evolutionary computing method | quantum inspired genetic algorithms | is introduced, where concepts and principles of quantum mechanics are used to inform and inspire more eecient evolutionary computing methods. The basic terminology of quantum mechanics is introduced before a comparison is made between a classical genetic algorithm and a quantum inspired method for the travelling salesperson problem. It is informally shown that the quantum inspired genetic algorithm performs better than the classical counterpart for a small domain. The paper concludes with some speculative comments concerning the relationship between quantum inspired genetic algorithms and various complexity classes.	complexity class;evolutionary computation;genetic algorithm;speculative execution;the principles of quantum mechanics;travelling salesman problem	Ajit Narayanan;Mark Moore	1996		10.1109/ICEC.1996.542334	genetic programming;quantum complexity theory;theoretical computer science;genetic representation;mathematics;algorithm	AI	28.13000162053389	-8.643870294808744	51638
31e8e152dd3455a315200135cc7fd1e382e34e1d	principal component analysis-based control charts for multivariate nonnormal distributions	bootstrap;kernel density estimation;multivariate control charts;average run length;principal component analysis;期刊论文	Multivariate control charts have been widely used in many industries to monitor and diagnose processes characterized by a large number of quality characteristics. Usually, these characteristics are highly correlated with each other. The direct use of conventional multivariate control charts for situations with highly correlated characteristics may lead to increased rates of false alarms. Principal component analysis (PCA) control charts have been widely used to address problems posed by such high correlations by transforming the set of correlated variables to an uncorrelated set of variables and then identifying the PCs with highest contribution which then allows one to reduce dimensionality. However, an assumption that the data are normally distributed underlies the construction of the control limits of traditional PCA control charts. This assumption has limited the use of PCA control charts in nonnormal situations found in many modern systems. This study presents the development of nonparametric PCA control charts that do not require any distributional assumptions for their construction. We propose to use nonparametric techniques, kernel density estimation, and bootstrapping to establish the control limits of these charts. A simulation study was conducted to evaluate the performance of the proposed charts and compare them with traditional PCA control charts. The comparative performance in terms of average run length showed that the proposed nonparametric PCA control charts performed better than the parametric PCA control charts in nonnormal situations.	chart;principal component analysis	Poovich Phaladiganon;Seoung Bum Kim;Victoria C. P. Chen;Wei Jiang	2013	Expert Syst. Appl.	10.1016/j.eswa.2012.12.020	kernel density estimation;econometrics;computer science;machine learning;data mining;statistics;principal component analysis	Mobile	28.156285189441547	-20.02969100369595	51725
70f2183b4b6dfad336ef622a47f075f654f74c11	multitrajectory simulation performance for varying scenario sizes	rate of convergence;assessment methods	"""Multitrajectory Simulation allows random events in simulation to generate multiple trajectories, a techniq called """"splitting"""", with explicit management of the set trajectories. The goal is to gain a better understanding the possible outcome set of the simulation and scena This has been applied to a prototype combat simulati """"eaglet"""" which was designed to have similar, but simpl representations of the features of the """"Eagle"""" simulat used for Army analyses. The study compared the num of multitrajectory simulation trajectories with numbers stochastic replications to experimentally determining t rate of convergence to a definitive outcome set. T definitive set was determined using very large numbers replications to develop a plot of loss exchange ratio ver losses of one side. This was repeated with scenario from 40 to 320 units. While the multitrajectory techniqu gave superior results in general as expected, there w some anomalies, particularly in the smallest scenario, illustrate limitations of the technique and the assessm method used."""	experiment;prototype;rate of convergence;simulation;ver (command)	John B. Gilmer;Frederick J. Sullivan	1999		10.1145/324898.325018	econometrics;simulation;computer science;mathematics;rate of convergence;statistics	ML	29.79555039208544	-16.205782028329654	51794
273884bcefb5f5a507c22624439b7a37f8806ff3	a modified and efficient shuffled frog leaping algorithm (msfla) for unsupervised data clustering		Shuffled frog leaping Algorithm (SFLA) is a new memetic, population based, meta-heuristic algorithm, has emerged as one of the fast, robust with efficient global search capability. In order to enhance the algorithm’s stability and the ability to search the global optimum, the conventional SFL Algorithm has been modified in our work by using the local best value of each memeplex instead of generating a new frog, to enhance the effectiveness of the SFLA. This paper implements the application of Modified SFLA in Partitional clustering of the unlabelled data. This algorithm is applied on various classification problems and the simulated results demonstrate that, this modified SFLA has outperformed the conventional SFL Algorithm.		Suresh Chittineni;Dinesh Godavarthi;A. N. S. Pradeep;Suresh Chandra Satapathy;P. V. G. D. Prasad Reddy	2011		10.1007/978-3-642-22720-2_57	control theory;computer science;cluster analysis;algorithm;population;global optimum;shuffled frog leaping algorithm	ML	27.03372330998669	-4.870645083024265	51807
754335856120273eb896d7b37bbf3266a473f5db	closed-loop mpeg video rendering	jpeg bit streams;video streaming;image coding;materials processing;software based real time video rendering system closed loop mpeg video rendering compressed video stream generation jpeg bit streams mpeg bit streams storage requirements memory disk swapping time desktop editing processing speeds data size image quality;neural networks;data compression;mpeg video;video compression;transform coding;multimedia systems;video coding;compressed video stream generation;streaming media;video compression image coding streaming media transform coding rendering computer graphics material storage neural networks computer science laboratories milling machines;memory disk swapping time;storage requirements;data size;material storage;image quality;milling machines;multimedia systems video coding data compression real time systems image sequences;mpeg bit streams;processing speeds;computer science;rendering computer graphics;software based real time video rendering system;desktop editing;closed loop mpeg video rendering;real time systems;image sequences	1.1 Abstract Video Player Internals Embedded platforms put demands on latency and memory use. Video playback makes these difficult to guarantee. This presentation discusses the architecture of video players, and the problems imposed on them by the design of video codecs and their containers. To explain these problems we look at both proprietary and open source formats (MPEG, Ogg, Theora, Dirac, etc.) and evaluate open source video players in this context. We particularly examine xine and GStreamer, and introduce the minimal architecture of OggPlay.	codec;embedded system;gstreamer;moving picture experts group;open-source software;xine project	Bo Shen;Ishwar K. Sethi;Vasudev Bhaskaran	1997		10.1109/MMCS.1997.609604	video compression picture types;data compression;uncompressed video;computer hardware;rendering;computer science;video tracking;block-matching algorithm;multimedia;video processing;smacker video;real-time rendering;motion compensation;alternate frame rendering;video post-processing;artificial neural network;statistics;multiview video coding;computer graphics (images)	Embedded	43.20323691098671	-20.23939561723028	51819
2c628b2cbd9ddd416a29dfb326110b2046e7dd2b	quadra-embedding: binary code embedding with low quantization error	code bit;encoding scheme;state-of-the-art embedding method;quantization region;binary code;quantization error;low quantization error;high quantization error;novel binary code distance;accurate embedding;novel binary code	Thanks to compact data representations and fast similarity computation, many binary code embedding techniques have been proposed for large-scale similarity search used in many computer vision applications including image retrieval. Most prior techniques have centered around optimizing a set of projections for accurate embedding. In spite of active research efforts, existing solutions suffer from diminishing marginal efficiency and high quantization errors as more code bits are used. To reduce both quantization error and diminishing efficiency we propose a novel binary code embedding scheme, Quadra-Embedding, that assigns two bits for each projection to define four quantization regions, and a binary code distance function tailored to our method. Our method is directly applicable to most binary code embedding methods. Our scheme combined with four state-of-the-art embedding methods has been evaluated and achieves meaningful accuracy improvement in most experimental configurations. ∗Corresponding author. Tel.: +82 42 350 3531. Fax: +82 42 350 3510 Email addresses: lywoon89@gmail.com (Youngwoon Lee), jaepilheo@gmail.com (Jae-Pil Heo), sungeui@gmail.com (Sung-Eui Yoon) Preprint submitted to Computer Vision and Image Understanding April 24, 2014	binary code;computation;computer vision;email;fax;image retrieval;marginal model;quantization (signal processing);similarity search	Youngwoon Lee;Jae-Pil Heo;Sung-Eui Yoon	2012		10.1007/978-3-642-37444-9_17	combinatorics;discrete mathematics;constant-weight code;theoretical computer science;mathematics	Vision	40.509999246001954	-15.683983163734505	51901
5f29199db60893d25fed0197df19e6aa3f4ff1ac	implementation of zero tree wavelet coders in dsp processor	ezw coding;wavelet transform;spiht coding;pzw coding	With the fast evolution of Multimedia systems, Image compression algorithms are very much needed to achieve effective transmission and compact storage by removing the redundant information of the image data. Wavelet transforms have received significant attention, recently, due to their suitability for a number of important signal and image compression applications and the lapped nature of this transform and the computational simplicity, which comes in the form of filter bank implementations. In this paper, the implementation of image compression algorithms based on discrete wavelet transform such as embedded zero tree wavelet (EZW) coder, set partitioning in hierarchical trees coder without lists (SPIHT — No List) and packetizable zero tree wavelet (PZW) coder in DSP processor is dealt in detail and their performance analysis is carried out in terms of different compression ratios, execution timing and for different packet losses. PSNR is used as the criteria for the measurement of reconstructed image quality.	algorithm;data compression;discrete wavelet transform;embedded zerotrees of wavelet transforms;embedded system;filter bank;image compression;image quality;network packet;peak signal-to-noise ratio;set partitioning in hierarchical trees	S. Arivazhagan;D. Gnanadurai;J. R. Antony Vance;K. M. Sarojini;L. Ganesan	2004	IJWMIP	10.1142/S0219691304000366	wavelet;computer vision;mathematical analysis;speech recognition;second-generation wavelet transform;computer science;theoretical computer science;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;set partitioning in hierarchical trees;wavelet transform	EDA	42.62612857044447	-15.467047262176324	51963
3e2257f7bca9c639dd544d27402c77b041a0e443	cooperation evolution in structured populations by using discrete pso algorithm		Social dilemma is a challenge to many scientists. The Prisoner’s Dilemma and Snowdrift game were the most used social dilemma models in the cooperation evolution. A particularly effect to the evolutionary process comes from population structure. By comparing population structures that amplify selection with other population structures, both analytically and numerically, we show that evolution also affected by the cost to benefit ratio and neighbor number.	algorithm;particle swarm optimization;population	Xiaoyang Wang;Xiaorong Du;Yunlin Sun	2015		10.1007/978-3-319-27400-3_6	mathematical optimization;artificial intelligence;machine learning	Vision	25.470499065797	-8.7777754205058	51976
605e3415d6375a07c2e73463b218f39cf123e47b	enhancing divergent search through extinction events	neuroevolution;extinction events;evolutionary robotics;novelty search;evolvability;divergent search	A challenge in evolutionary computation is to create representations as evolvable as those in natural evolution. This paper hypothesizes that extinction events, i.e. mass extinctions, can significantly increase evolvability, but only when combined with a divergent search algorithm, i.e. a search driven towards diversity (instead of optimality). Extinctions amplify diversity-generation by creating unpredictable evolutionary bottlenecks. Persisting through multiple such bottlenecks is more likely for lineages that diversify across many niches, resulting in indirect selection pressure for the capacity to evolve. This hypothesis is tested through experiments in two evolutionary robotics domains. The results show that combining extinction events with divergent search increases evolvability, while combining them with convergent search offers no similar benefit. The conclusion is that extinction events may provide a simple and effective mechanism to enhance performance of divergent search algorithms.	bottleneck (software);evolutionary computation;evolutionary robotics;experiment;mass effect trilogy;search algorithm	Joel Lehman;Risto Miikkulainen	2015		10.1145/2739480.2754668	neuroevolution;computer science;machine learning;evolutionary robotics;evolvability;extinction event	AI	25.540123568606116	-8.834471914128352	51988
305fea3e67997d45d4fbf26f460e253d75cfd706	analysis of sample-path optimization	minimisation;common random numbers;steady state solution;steady state function;minimization;optimisation;solucion estacionaria;convergence;retrospective optimization;optimizacion;strong stochastic convexity;simulation optimization;minimizacion;sample path optimization;random number;convergencia;solution stationnaire;mathematical programming;nombre aleatoire;optimization;stochastic model;numero aleatorio;programmation mathematique;m estimation;modelo estocastico;programacion matematica;modele stochastique	Sample-path optimization is a method for optimizing limit functions occurring in stochastic modeling problems, such as steady-state functions in discrete-event dynamic systems. It is closely related to retrospective optimization techniques and to M-estimation. The method has been computationally tested elsewhere on problems arising in production and in project planning, with apparent success. In this paper we provide a mathematical justification for sample-path optimization by showing that under certain assumptions---which hold for the problems just mentioned---the method will almost surely find a point that is, in a specified sense, sufficiently close to the set of optimizers of the limit function.	program optimization	Stephen M. Robinson	1996	Math. Oper. Res.	10.1287/moor.21.3.513	stochastic programming;probabilistic-based design optimization;minimisation;mathematical optimization;multi-swarm optimization;test functions for optimization;convergence;stochastic modelling;stochastic optimization;multi-objective optimization;calculus;mathematics;mathematical economics;vector optimization;l-reduction	Theory	31.02790539694399	1.156541573512354	52004
03b23f499f95936cea2db54c292d68173826601e	visual quality and file size prediction of h.264 videos and its application to video transcoding for the multimedia messaging service and video on demand	image motion analysis;video on demand visual quality assessment predictive models h 264 video transcoding multimedia messaging service;visual quality assessment;video coding;video transcoding;video on demand image motion analysis multimedia communication transcoding video coding;video on demand;multimedia communication;h 264;predictive models;multimedia messaging service;transcoding;videos visualization estimation quantization signal adaptation models computational modeling training;netflix visual quality maximization file size prediction h 264 videos video transcoding multimedia messaging service video on demand terminal file size resolution constraint quality estimation model resolution function quantization step size frame rate parameters video motion video file size estimation model pearson correlation coefficient pcc mean opinion score generic quality model motion conscious model parameter combination estimation average quality difference theoretical transcoding youtube	In this paper, we address the problem of adapting video files to meet terminal file size and resolution constraints while maximizing visual quality. First, two new quality estimation models are proposed, which predict quality as function of resolution, quantization step size, and frame rate parameters. The first model is generic and the second takes video motion into account. Then, we propose a video file size estimation model. Simulation results show a Pearson correlation coefficient (PCC) of 0.956 between the mean opinion score and our generic quality model (0.959 for the motion-conscious model). We obtain a PCC of 0.98 between actual and estimated file sizes. Using these models, we estimate the combination of parameters that yields the best video quality while meeting the target terminal's constraints. We obtain an average quality difference of 4.39% (generic model) and of 3.22% (motion-conscious model) when compared with the best theoretical transcoding possible. The proposed models can be applied to video transcoding for the Multimedia Messaging Service and for video on demand services such as YouTube and Netflix.	coefficient;computation;digital video;h.264/mpeg-4 avc;portable c compiler;quantization (signal processing);simulation;video file format	Didier Joset;Stéphane Coulombe	2013	2013 IEEE International Symposium on Multimedia	10.1109/ISM.2013.62	video compression picture types;subjective video quality;simulation;transcoding;h.263;computer science;video quality;operating system;machine learning;video tracking;block-matching algorithm;multimedia;video processing;smacker video;rate–distortion optimization;motion compensation;video post-processing;world wide web;pevq;statistics;multiview video coding	Arch	45.1862789753983	-20.65861568986434	52080
42774c93ea13cd94d9ea179ef190269b44084852	utilization of discrete transforms to conquer the problems of multi-tone systems		This paper presents a new implementation of discrete multi-tone (DMT) systems based on different discrete transforms that include the discrete sine transform (DST), discrete cosine transform (DCT), and discrete wavelet transform (DWT). The implementation also considers time-domain equalization to mitigate channel distortion. Compared to the fast Fourier transform discrete multi-tone (FFT-DMT) system, the proposed implementations have an advantage in that their energy-compaction property helps in reducing the channel effects. The performance of the DST-DMT, DCT-DMT, DWT-DMT, and FFT-DMT systems, employing a time-domain equalizer (TEQ), is investigated in the paper. It has been www.elsevier.com/locate/jfranklin 0016-0032/$32.00 & 2013 The Franklin Institute. Published by Elsevier Ltd. All rights reserved. http://dx.doi.org/10.1016/j.jfranklin.2013.11.003 Corresponding author. E-mail addresses: samir.ghaffer@yahoo.com (S.A. Elghafar), dr_salah_diab@yahoo.com (S.M. Diab), b_m_salam@hotmail.com (B.M. Sallam), eng_emadash@yahoo.com (E.S. Hassan), i_shokair@yahoo.com (M. Shokair), wax@liv.ac.uk (W. Al-Nauimy), dr_moawad@yahoo.com (M.I. Dessouky), srabie1@yahoo.com (E.-S. El-Rabaie), dsaleh@ksu.edu.sa (S. Alshebeili), fathi_sayed@yahoo.com (F.E. Abd El-Samie). Author's personal copy demonstrated by computer simulations that the proposed implementations outperform the FFT-DMT system and that the utilization of the TEQ can lead to higher bit rates & 2013 The Franklin Institute. Published by Elsevier Ltd. All rights reserved.	computer simulation;data compaction;digital monetary trust;discrete cosine transform;discrete sine transform;discrete transform;discrete wavelet transform;distortion;equalization (communications);experiment;fast fourier transform;filter bank;franklin electronic publishers;information sciences institute;marc (archive);signal-to-noise ratio	Samir Abd Elghafar;Salah Eldeen M. Diab;Bassioni M. Salam;Emad S. Hassan;Mona Shokair;Waleed Al-Nuaimy;Moawad I. Dessouky;S. El-Rabaie;Saleh A. Alshebeili;Fathi E. Abd El-Samie	2014	J. Franklin Institute	10.1016/j.jfranklin.2013.11.003	discrete hartley transform;electronic engineering;real-time computing;lapped transform;telecommunications;computer science;discrete cosine transform;discrete wavelet transform	AI	41.62003079937409	-7.959519203019603	52098
867fbcb153580f64b5a60820999da47895023c40	lie algebra solution of population models based on time-inhomogeneous markov chains	stochastic process;population model;natural population;markov chain;birth death process;lie algebra	Many natural populations are well modelled through time-inhomogeneous stochastic processes. Such processes have been analysed in the physical sciences using a method based on Lie algebras, but this methodology is not widely used for models with ecological, medical and social applications. This paper presents the Lie algebraic method, and applies it to three biologically well motivated examples. The result of this is a solution form that is often highly computationally advantageous.	linear algebra;markov chain;population;stochastic process	Thomas House	2012	J. Applied Probability	10.1017/S0021900200009219	stochastic process;lie algebra;markov chain;combinatorics;discrete mathematics;population model;natural population growth;mathematics;birth–death process;statistics	AI	34.32694496665046	-13.88385330304463	52152
ce1025947c05d6f40d57af882e7b76ac91136c1c	hyperbolic positioning with antenna arrays and multi-channel pseudolite for indoor localization	biological patents;biomedical journals;text mining;europe pubmed central;citation search;hyperbolic positioning;citation networks;gps;research articles;abstracts;open access;life sciences;clinical guidelines;full text;pseudolite;indoor positioning;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	A hyperbolic positioning method with antenna arrays consisting of proximately-located antennas and a multi-channel pseudolite is proposed in order to overcome the problems of indoor positioning with conventional pseudolites (ground-based GPS transmitters). A two-dimensional positioning experiment using actual devices is conducted. The experimental result shows that the positioning accuracy varies centimeter- to meter-level according to the geometric relation between the pseudolite antennas and the receiver. It also shows that the bias error of the carrier-phase difference observables is more serious than their random error. Based on the size of the bias error of carrier-phase difference that is inverse-calculated from the experimental result, three-dimensional positioning performance is evaluated by computer simulation. In addition, in the three-dimensional positioning scenario, an initial value convergence analysis of the non-linear least squares is conducted. Its result shows that initial values that can converge to a right position exist at least under the proposed antenna setup. The simulated values and evaluation methods introduced in this work can be applied to various antenna setups; therefore, by using them, positioning performance can be predicted in advance of installing an actual system.	antenna device component;collaborative product development;computer simulation;converge;evaluation method;global positioning system;gray platelet syndrome;linear least squares (mathematics);multilateration;non-linear least squares;numerous;observable;preparation;transmitter;penciclovir	Kenjiro Fujii;Yoshihiro Sakamoto;Wei Wang;Hiroaki Arie;Alexander Schmitz;Shigeki Sugano	2015		10.3390/s151025157	embedded system;text mining;simulation;global positioning system;telecommunications;computer science;bioinformatics;engineering;electrical engineering;precise point positioning;data mining	Mobile	49.43651219511846	0.5438873113251913	52352
f16ac03f9588e7a278c6ae29053b1b95bf712160	lossless image compression with projection-based and adaptive reversible integer wavelet transforms	transformation ondelette;modelizacion;wavelet transform coefficient;traitement signal;metodo adaptativo;linear combination;evaluation performance;image coding;performance evaluation;image processing;data compression;ondelette;evaluacion prestacion;signal analysis;integer wavelet transform;lossless image compression;ordre 1;projection method;procesamiento imagen;projection based wavelet transforms;lossless compression;analisis de senal;methode adaptative;transform coding;adaptive reversible integer wavelet transforms;indexing terms;bit rate;traitement image;modelisation;wavelet transforms;codage image;reversible integer transforms;standards development;compression image;image coding wavelet transforms performance loss transform coding bit rate standards development entropy context modeling predictive models wavelet domain;first order;wavelet transform;prediction theory;methode projection;image compression;projection technique;combinacion lineal;computational complexity;signal processing;entropy codes;adaptive method;metodo proyeccion;jpeg2000 image coding standard;predictive models;first order entropy;entropy;adaptive prediction scheme;optimal fixed prediction steps;compresion dato;transformacion ondita;modeling context;transform coefficients;wavelet domain;prediction theory wavelet transforms entropy codes image coding data compression computational complexity;procesamiento senal;modeling;context modeling;performance loss;orden 1;wavelets;analyse signal;wavelet transformation;combinaison lineaire;compression donnee;projection technique lossless image compression projection based wavelet transforms adaptive reversible integer wavelet transforms jpeg2000 image coding standard first order entropy transform coefficients wavelet transform coefficient optimal fixed prediction steps adaptive prediction scheme modeling context;compresion imagen;lifting	Reversible integer wavelet transforms are increasingly popular in lossless image compression, as evidenced by their use in the recently developed JPEG2000 image coding standard. In this paper, a projection-based technique is presented for decreasing the first-order entropy of transform coefficients and improving the lossless compression performance of reversible integer wavelet transforms. The projection technique is developed and used to predict a wavelet transform coefficient as a linear combination of other wavelet transform coefficients. It yields optimal fixed prediction steps for lifting-based wavelet transforms and unifies many wavelet-based lossless image compression results found in the literature. Additionally, the projection technique is used in an adaptive prediction scheme that varies the final prediction step of the lifting-based transform based on a modeling context. Compared to current fixed and adaptive lifting-based transforms, the projection technique produces improved reversible integer wavelet transforms with superior lossless compression performance. It also provides a generalized framework that explains and unifies many previous results in wavelet-based lossless image compression.	coefficient;first-order predicate;image compression;integer (number);jpeg 2000;lambda lifting;lossless compression;wavelet transform	Aaron Deever;Sheila S. Hemami	2003	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/TIP.2003.812374	data compression;lossy compression;wavelet;lossless jpeg;mathematical optimization;discrete mathematics;harmonic wavelet transform;second-generation wavelet transform;continuous wavelet transform;image compression;computer science;theoretical computer science;signal processing;cascade algorithm;mathematics;lossless compression;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;algorithm;statistics;wavelet transform	Visualization	45.68346359824151	-14.613775383819675	52429
8bca3537bbd9bc9992634698001b52513420e899	modified firefly algorithm using quaternion representation	firefly algorithm;representation of individuals;optimization;quaternions	Quaternions are a number system, which extends complex numbers. They are especially useful in areas where fast rotation calculations are needed, e.g., programming video games or controllers of spacecraft. This paper proposes to use quaternion for the representation of individuals in firefly algorithm so as to enhance the performance of the firefly algorithm and to avoid any stagnation. The preliminary results of our experiments after optimizing a test-suite consisting of ten standard functions, showed that the proposed firefly algorithms using quaternion’s representation improved the results of the original firefly algorithm.		Iztok Fister;Xin-She Yang;Janez Brest;Iztok Fister	2013	Expert Syst. Appl.	10.1016/j.eswa.2013.06.070	mathematical optimization;simulation;computer science;firefly algorithm;control theory;mathematics;quaternion	NLP	28.506024874206286	-3.758877910074569	52453
e5c3a0ae802aa208f7dc0cc1c7dee92725e60108	exact two-sided statistical tolerance limits for sample variances				Hatice O Ozguldez;S. Chakraborti;Eugenio K. Epprecht	2018	Quality and Reliability Eng. Int.	10.1002/qre.2321		DB	32.264470440386575	-20.176536766259503	52534
c789110991add751102b71b82510771aee9aee79	unconstrained optimal control of regular languages	transition state;linear algebra;modelizacion;politica optima;langage commande;control optimo;matriz transicion;systeme evenement discret;lenguaje control;sintesis control;measurement;complexite calcul;polynomial control;automata estado finito;ecuacion algebraica;control polinomio;regular language;ecuacion lineal;language measure;optimal policy;approche deterministe;transition matrix;sistema acontecimiento discreto;lenguaje racional;deterministic approach;optimal control;modelisation;complejidad computacion;discrete event system;medida;computational complexity;estado transitorio;finite state automata;synthese commande;commande optimale;commande polynomiale;enfoque determinista;discrete event systems;langage rationnel;unconstrained optimization;equation algebrique;finite automaton;mesure;automate fini;linear equation;politique optimale;modeling;control language;algebraic equation;etat transition;control synthesis;state transition;equation lineaire;matrice transition	This paper formulates an unconstrained optimal policy for control of regular languages realized as deterministic 2nite state automata (DFSA). A signed real measure quanti2es the behavior of controlled sublanguages based on a state transition cost matrix and a characteristic vector as reported in an earlier publication. The state-based optimal control policy is obtained by selectively disabling controllable events to maximize the measure of the controlled plant language without any further constraints. Synthesis of the optimal control policy requires at most n iterations, where n is the number of states of the DFSA model. Each iteration solves a set of n simultaneous linear algebraic equations. As such, computational complexity of the control synthesis is polynomial in n. ? 2003 Elsevier Ltd. All rights reserved.	algebraic equation;algorithm;automata theory;computational complexity theory;controlled natural language;deterministic finite automaton;finite-state machine;formal language;iteration;maximal set;optimal control;polynomial;regular language;state transition table	Jinbo Fu;Asok Ray;Constantino M. Lagoa	2004	Automatica	10.1016/j.automatica.2003.11.011	algebraic equation;systems modeling;optimal control;regular language;calculus;stochastic matrix;mathematics;transition state;linear equation;finite-state machine;deterministic system;computational complexity theory;algorithm;measurement	AI	39.86732634383582	2.474585147751871	52539
5afcceb805030af20b5f55f17a88ce0861ace257	a hybrid particle swarm optimization for numerical optimization	machine learning algorithms;marine animals;convergence;evolutionary computation;premature convergence;ant colony optimization;modified velocity model;numerical function problem;particle swarm optimization benchmark testing evolutionary computation machine learning machine learning algorithms ant colony optimization educational technology heuristic algorithms birds marine animals;particle swarm optimisation learning artificial intelligence;numerical optimization;data mining;numerical optimization particle swarm optimization function optimization;function optimization;particle swarm optimizer;birds;machine learning;heuristic algorithms;benchmark function hybrid particle swarm optimization numerical optimization numerical function problem opposition based learning modified velocity model nonzero velocity;particle swarm optimization;benchmark function;mathematical model;optimization;educational technology;learning artificial intelligence;local minima;nonzero velocity;particle swarm optimisation;benchmark testing;opposition based learning;hybrid particle swarm optimization	Particle Swarm Optimization (PSO) has shown its good performance on numerical function problems. However, on some multimodal functions the PSO easily suffers from premature convergence because of the rapid decline in velocity. This paper presents a hybrid PSO for numerical optimization, namely HPSO, which employs opposition-based learning (OBL) and a modified velocity model. The OBL provides more chances to find solutions more closely to the global optimum. And the modified velocity model guarantees a non-zero velocity to help trapped particles jump out local minima. Experimental results on 6 benchmark functions show that the HPSO outperforms the standard PSO and opposition-based PSO in all test cases.	benchmark (computing);function problem;global optimization;mathematical optimization;maxima and minima;multimodal interaction;numerical analysis;particle swarm optimization;premature convergence;program optimization;test case;velocity (software development)	Zhengang Ning;Liyan Ma;Zhenping Li;Wenjian Xing	2009	2009 International Conference on Business Intelligence and Financial Engineering	10.1109/BIFE.2009.31	mathematical optimization;engineering;artificial intelligence;machine learning	Robotics	28.06382150418851	-5.336672398228797	52541
952073afea0ea6dfeb60e84bf43e86377d11862c	defects in parallel monte carlo and quasi-monte carlo integration using the leap-frog technique	processing element;parallel algorithm;high dimensionality;numerical technique;parallel systems;numerical integration;quasi monte carlo methods;quasi monte carlo;parallel computer;quasi monte carlo method;parallel programming model;monte carlo;random numbers;low discrepancy sequences;pseudo random numbers;parallel algorithms	Currently, the most efficient numerical techniques for evaluating high-dimensional integrals are based on Monte Carlo and quasi-Monte Carlo techniques. These tasks require a significant amount of computation and are therefore often executed on parallel computer systems. In order to keep the communication amount within a parallel system to a minimum, each processing element (PE) requires its own source of integration nodes. Therefore, techniques for using separately initialized and disjoint portions of a given point set on a single PE are classically employed. Using the so-called substreams may lead to dramatic errors in the results under certain circumstances. In this work, we compare the possible defects employing leaped quasi-Monte Carlo and Monte Carlo substreams. Apart from comparing the magnitude of the observed integration errors we give an overview under which circumstances (i.e. parallel programming models) such errors can occur.	monte carlo integration;monte carlo method;quasi-monte carlo method	Karl Entacher;Thomas Schell;Wolfgang Ch. Schmid;Andreas Uhl	2003	Parallel Algorithms Appl.	10.1080/1063719031000088021	quasi-monte carlo method;mathematical optimization;parallel computing;hybrid monte carlo;markov chain monte carlo;computer science;theoretical computer science;monte carlo molecular modeling;parallel algorithm;monte carlo integration;monte carlo method	HPC	44.01203490367622	2.46460822508312	52548
ece3464281a5c0fd3bec078a47b7cd790bac386d	improved evolutionary design for rule-changing cellular automata based on the difficulty of problems	concurrent computing performance evaluation lattices genetic algorithms biological cells automatic testing boundary conditions automata design methodology application software;lambda parameter;rule changing cellular automata;probability;evolutionary design;density classification task;genetic algorithm evolutionary design rule changing cellular automata lambda parameter probability density classification task;probability cellular automata genetic algorithms pattern classification;pattern classification;genetic algorithm;genetic algorithms;evolutionary process;cellular automata;cellular automaton	This paper describes a method to promote the evolution of the transition rules of cellular automata using a genetic algorithm. We previously proposed the evolutionary design of a cellular automaton in which an applied rule changes with time. This method encodes a rule and the number of times the rule is applied as a chromosome. In this paper, we describe the improvement of the method and analyze rules obtained using the Lambda parameter defined by Langton. The difficulty of test problems in an evolutionary process is adjusted so as to obtain a rule which performs the density classification task with high probability. Experiments using ten-thousand randomly generated tasks have shown that the proposed method performs better than the previous method.	automata theory;cellular automaton;continuous design;experiment;genetic algorithm;langton's ant;majority problem (cellular automaton);procedural generation;production (computer science);with high probability	Hitoshi Kanoh;Shohei Sato	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4413753	stochastic cellular automaton;cellular automaton;genetic algorithm;continuous spatial automaton;growcut algorithm;quantum finite automata;computer science;theoretical computer science;asynchronous cellular automaton;machine learning;rule 184;algorithm	Robotics	27.60870411898304	-7.64356857099293	52556
33463637f5ca864ef5000c77ade498e60a61cdca	multilayered contourlet based image compression	transformation ondelette;information structure;texture;image coding;multimedia;image processing;structure information;data compression;piecewise smooth;competitive algorithms;estructura informacion;procesamiento imagen;analyse multiresolution;traitement image;codage image;multilayered representation;algorithme competitif;superposicion;compression image;image compression;image representation;textura;spiht;contourlet transform;superposition;compresion dato;transformacion ondita;multiresolution analysis;perfect reconstruction;wavelet transformation;analisis multiresolucion;compression donnee;compresion imagen	After studying contourlet transform and multilayered image representation, we present a Multilayered Contourlet Based Image Compression algorithm (MCBIC) in this paper. It decomposes image into a superposition of coherent layers: piecewise smooth regions layer, directional information layer, e.g., textures and edges. MCBIC uses the best basis to deal with different layers of multilayered representation, and retains the most significant structures of the image. In MCBIC, the first layer of the image is coded in wavelet, which acquires information of piecewise smooth regions; because the contourlet transform is an efficient directional multiresolution image representation, so the second layer of the image is coded in contourlet, which captures directional structure information of the image. Furthermore, each layer is encoded independently with a different transform and the combination of the compressed layers can always be perfect reconstructed. Our experiments demonstrate that MCBIC is efficient in coding images that possess mostly textures and contours. Our experimental results also show that MCBIC is competitive to the contourlet algorithm, SPIHT algorithm and the multilayered image compression approach in terms of the PSNR-rate curves, and is visually superior to these algorithms for the mentioned images.	contourlet;image compression	Fang Liu;Yanli Liu	2007		10.1007/978-3-540-69423-6_30	data compression;multiresolution analysis;superposition principle;computer vision;contourlet;image processing;image compression;computer science;theoretical computer science;pattern recognition;mathematics;texture;set partitioning in hierarchical trees;algorithm	Vision	45.55196415420583	-13.122082874962773	52624
29293d487bd6e9b9be5e6454c783412db8d4de5f	using distributed source coding and depth image based rendering to improve interactive multiview video access	3d tv;video retrieval hidden feature removal image texture interactive video rendering computer graphics source coding video coding;wyner ziv;video streaming;inpainting distributed video coding interactive tv 3d tv multiview video plus depth depth map depth image based rendering;hidden feature removal;decoding;interactive video;dibr distributed source coding depth image based rendering interactive multiview video access interactivity requirement redundant predictive frames wyner ziv frames distributed video coding interactive multiview video plus depth depth aided inpainting occlusion interactive mvd coding texture video;video retrieval;multiview video plus depth;streaming media switches decoding servers cameras tv video coding;multiple views;interactive tv;image texture;distributed source coding;group of picture;video coding;servers;inpainting;streaming media;distributed video coding;tv;depth image based rendering;depth map;video communication;rendering computer graphics;switches;cameras;source coding	Multiple-views video is commonly believed to be the next significant achievement in video communications, since it enables new exciting interactive services such as free viewpoint television and immersive teleconferencing. However the interactivity requirement (i.e. allowing the user to change the viewpoint during video streaming) involves a trade-off between storage and bandwidth costs. Several solutions have been proposed in the literature, using redundant predictive frames, Wyner-Ziv frames, or a combination of them. In this paper, we adopt distributed video coding for interactive multiview video plus depth (MVD), taking advantage of depth image based rendering (DIBR) and depth-aided inpainting to fill the occlusion areas. To the authors' best knowledge, very few works in interactive MVD consider the problem of continuity of the playback during the switching among streams. Therefore we survey the existing solutions, we propose a set of techniques for MVD coding and we compare them. As main results, we observe that DIBR can help in rate reduction (up to 13.36% for the texture video and up to 8.67% for the depth map, wrt the case where DIBR is not used), and we also note that the optimal strategy to combine DIBR and distributed video coding depends on the position of the switching time into the group of pictures. Choosing the best technique on a frame-to-frame basis can further reduce the rate from 1% to 6%.	data compression;depth map;distributed source coding;free viewpoint television;group of pictures;inpainting;interactivity;jumbo frame;scott continuity;streaming media;switching time	Giovanni Petrazzuoli;Marco Cagnazzo;Frédéric Dufaux;Béatrice Pesquet-Popescu	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6116620	image texture;computer vision;distributed source coding;network switch;computer science;video quality;video tracking;multimedia;video post-processing;server;multiview video coding;depth map;source code;inpainting;computer graphics (images)	Robotics	43.83210684451444	-19.714579450693936	52687
82179600d693f38352f15d282761c6f830b9758b	macroblock skipping algorithms for high definition h.264/avc video coding in the baseline profile		This paper discusses different macroblock skipping algorithms to be applied in the H.264/AVC Baseline profile, in order to facilitate the adoption of High Definition video coding in real time applications. Moving from Standard to High Definition video coding, there is six times as much data to process: this motivates the search for suited Mode Decision strategies, to reduce complexity while preserving an acceptable video quality for the final user. The proposed schemes permit to speed up significantly the Mode Decision procedure, by forcing the selection of the SKIP mode over each frame, without affecting significantly the final quality.	algorithm;armstrong's axioms;baseline (configuration management);block size (cryptography);circa;data compression;decision problem;encoder;h.264/mpeg-4 avc;image processing;macroblock;richardson number;selection algorithm;selective area epitaxy;signal processing;video coding format;weatherstar	Susanna Spinsante;Ennio Gambi;Damiano Falcone	2007			scalable video coding;context-adaptive variable-length coding;coding tree unit;context-adaptive binary arithmetic coding;macroblock	AI	45.990361949439034	-18.944490012649325	52752
6b5cd61b2ebc1a64d1e61479b72a3a508f422c73	an efficient parallel collision detection algorithm for virtual prototype environments	shared memory;virtualprototype environments;time complexity;virtual prototype environments;resource allocation;cad;computational geometry;parallel programming;production engineering computing;virtual prototyping;parallel application parallel collision detection automatic recognition geometric constraints virtual assembly intersecting surfaces virtual prototype applications virtual prototyping environment overlapping axis aligned bounding box parallel computing openmp shared memory multiple processors overall time complexity parallel simulation load balancing code parallelisation work distribution real industrial applications;shared memory systems;collision detection;detection algorithms virtual prototyping assembly software prototyping design automation conference management virtual environment filters parallel processing computational modeling;shared memory multiprocessors;parallelising compilers;parallel computer;openmp;industrial application;load balance;parallelising compilers parallel algorithms collision avoidance virtual prototyping computational geometry open systems shared memory systems resource allocation digital simulation production engineering computing cad parallel programming;collision avoidance;geometric constraints;open systems;parallel applications;parallel simulation;digital simulation;shared memory multiprocessor;parallel algorithms	The automatic recognition of geometric constraints in virtual assembly and maintenance operations relies in the determination of intersecting surfaces between virtual prototypes. This is a key challenge in many virtual prototype applications, where it is necessary to find collisions precisely and interactively. This paper presents an algorithm to determine intersecting surfaces at interactive speed in a virtual prototyping environment. The proposed algorithm is based on the overlapping axis-aligned bounding box (OAABB). The OAABB concept is used effectively to eliminate the number of surfaces that cannot intersect and improve performance. The algorithm also facilitates the implementation using parallel computing methods. OpenMP is used, taking advantage of shared memory multiple processors and reducing the overall time complexity of the collision detection algorithm. To achieve an efficient parallel simulation, it is necessary to provide an efficient load balancing scheme. Our experiences in parallelising the code to achieve a better work distribution are also described. Results show that the proposed collision detection achieves interactive rates in real industrial applications as desired.	algorithm;apache axis;central processing unit;collision detection;interactivity;load balancing (computing);minimum bounding box;openmp;parallel computing;prototype;shared memory;simulation;time complexity	Mauro Figueiredo;Terrence Fernando	2004	Proceedings. Tenth International Conference on Parallel and Distributed Systems, 2004. ICPADS 2004.	10.1109/ICPADS.2004.19	time complexity;shared memory;computer architecture;parallel computing;computational geometry;resource allocation;computer science;load balancing;operating system;cad;distributed computing;parallel algorithm;open system;collision detection	HPC	44.90371841561105	-4.830666775596433	52782
1a25edaaf1ee59c1bc26617feca42c33375e2f2b	coupled chaotic tent map lattices system with uniform distribution	tent map;chaos;coupled map lattices;cryptography;uniform distribution	A coupled chaotic map lattices system with uniform distribution (CML-UD) consisting of tent maps is presented based on the security from the viewpoint of cryptography. The system inherited the coupled diffusion and parallel iteration mechanism of coupled map lattices (CML). Through the dual non-linear effect of the rolled-out and folded-over of local lattices tent map and modular algorithms, CML-UD allows the system to enter into an ergodic state, and to rapidly generate uniform distributed multi-dimensional pseudo-random sequences concurrently. The experimental results show that, the time sequences generated by the system has the same differential distribution character with the real random sequence of which each element has equal chance to appear, and with this property, the system effectively restrains the short-period phenomenon which is easy to occur in digital chaotic system. In addition, it had many special properties such as zero correlation in total field, uniform invariable distribution and the maximum Lyapunov exponent is much bigger and steady. All of the properties suggest that the CML-UD possesses the potential application in encryption.	algorithm;chaos theory;coupled map lattice;cryptography;encryption;ergodicity;iteration;lyapunov fractal;nonlinear system;pseudorandomness;tent map;urban dictionary	Jiandong Liu;Kai Yang	2010	2010 2nd International Conference on E-business and Information System Security	10.4304/jcp.6.2.190-199	tent map;coupled map lattice;combinatorics;discrete mathematics;topology;cryptography;mathematics;uniform distribution;statistics	Robotics	38.56961965419329	-8.187279080621064	52784
767e052218163fc487dfdea5fd5803b4b7f86d0c	improved guotao algorithm for unconstrained optimization problems	optimal solution;high dimensionality;unconstrained optimization;global optimization;numerical experiment;local search;evolutionary computing	This paper presents an improved GuoTao algorithm for solving unconstrained optimization problems. The algorithm combines the multi-parent crossover operator of GuoTao algorithm with two local search operators. The first local search operator is a simplified Hooke-Jeeves algorithm which can find near-optimal solutions near the start point within reasonable computational time. And the second local search operator is a powell algorithm which can find the local optimal solution near the start point. The multi-parent crossover operator can enables individual to draw closer to each local optimal solution, thus the population will be divided into subpopulations automatically, meanwhile, the first local search operator is adopted to gain near-optimal solution in each subpopulation. As a result, the promising areas are gained quickly which decrease useless search greatly. At last, the second local search operator is adopted to find the global optimal solution only from those promising areas.Numerical experiments using a suite of test functions, which are widely studied in the field of evolutionary computation, show that the proposed algorithm is superior to GuoTao algorithm for solving unconstrained optimization problems with high dimensionality.	algorithm;program optimization	Ziyi Chen;Lishan Kang;Lijun Liu	2008		10.1007/978-3-540-92137-0_5	beam search;local optimum;mathematical optimization;local search;hill climbing;machine learning;iterated local search;mathematics;best-first search;algorithm;guided local search;search algorithm	Theory	25.942852073179086	-3.1642402511992276	52822
f3fe92b5785a9e3b79975ae7589c3b036d9e7936	a neural approach to topological optimization of communication networks, with reliability constraints	optimal solution;system reliability;optimisation;network design;telecommunication network reliability;network unreliability communication networks topological optimization reliability constraints all terminal network reliability artificial neural networks opti net optimization ann minimization process hysteresis mcculloch pitts neuron model iterative behavior all terminal reliability bounds computer simulation computation time;neural nets;search space;artificial neural networks telecommunication network reliability computer network reliability neural networks costs testing constraint optimization hysteresis neurons computer simulation;telecommunication computing;indexing terms;energy function;failure analysis;network topology;community networks;size effect;optimisation telecommunication network reliability telecommunication computing neural nets failure analysis network topology;upper and lower bounds;network reliability;computer simulation;artificial neural network;neural network	Consider network topological optimization under a reliability constraint. The objective is to find the topological layout of links, at minimal cost, under the constraint: all-terminal network reliability is not less than a given level of system reliability. The all-terminal reliability is Pr every pair of nodes in the network can communicate with each other . This paper presents a new approach based on Artificial Neural Networks (ANN) for solving the problem. The problem is mapped onto an optimization ANN (OPTI-net) by constructing an energy function whose minimization process drives the neural network into one of its stable states. This stable state corresponds to a solution for the network design problem. The OPTI-net favors states that correspond to a selection of links with an overall reliability greater than or equal to a threshold value. Among these states it also favors the one which has the lowest total cost. Hysteresis McCulloch–Pitts neuron model is used in the solution, due to its performance and fast convergence. Considering the NP-hard complexity of the exact reliability calculation, together with the iterative behavior of the neural networks, bounds for the all-terminal reliability are used. This paper introduces new upper and lower bounds that are functions of the link selection and uses them to represent the network reliability. The neural network is tested via computer simulation using three problem sets. The first two sets are used to compare the results obtained by this method to those obtained by previous heuristics. The third test set contains five networks of larger sizes for which no results have been reported by previous methods. This paper finds the optimal or near-optimal solutions for most of the problems in a relatively short time. The OPTI-net found many good solutions for a 50-vertex 1225-arc network in 1/2 CPU hour. For each problem instance, many solutions are found at each run of the simulator. The strengths of this neural network approach are very slowly increasing computation time with respect to network size, effective optimization, and flexibility. The OPTI-net is very effective in identifying optimal, or suboptimal, solutions even in search spaces up to 10 for a fully connected network with 50 vertexes. The OPTI-net is the first approach to be applied on such large networks. The simulation results show that the neural approach is more efficient in designing networks of large sizes compared to other heuristic techniques.	artificial neural network;artificial neuron;biological neuron model;central processing unit;computation;computer simulation;heuristic (computer science);hysteresis;iterative method;mathematical optimization;np-hardness;network planning and design;network topology;telecommunications network;terminal emulator;test set;time complexity;walter pitts	Hosam M. F. AboElFotoh;Loulwa S. Al-Sumait	2001	IEEE Trans. Reliability	10.1109/24.983401	stochastic neural network;mathematical optimization;failure analysis;network planning and design;index term;random neural network;computer science;artificial intelligence;machine learning;network simulation;reliability;upper and lower bounds;artificial neural network;network topology	Metrics	30.786906403756387	4.099394910979405	52851
9b2a730086bfe5e24486dc1c05c05245d9f784df	rd-optimization of hierarchical structured adaptive vector quantization for video coding	performance measure;hierarchical structure;rate distortion;optimisation;hierarchical organization;quadtree structures;image coding;three way coding mode decision;building block;product code;frequency domain analysis;avq;entropy coding;low complexity;transform coding;adaptive codes;runtime;transform coding video coding vector quantisation optimisation rate distortion theory adaptive codes image sequences statistical analysis video codecs quadtrees wavelet transforms;video codec;motion compensated;rate distortion theory;wavelet transforms;video coding;adaptive vector quantization;video coding vector quantization image coding video codecs wavelet coefficients rate distortion lagrangian functions runtime entropy coding frequency domain analysis;block based coding;vector quantization;statistical analysis;research evaluation;tree structure;image sequence;image sequence statistics;video codecs;three way coding mode decision very low bitrate video coding rate distortion optimization generalized bfos algorithm adaptive vector quantization avq codebooks image sequence statistics video codec block based coding wavelet coefficients hierarchical organization quadtree structures;very low bitrate video coding;vector quantizer;frequency domain;vector quantisation;rate distortion optimization;codebooks;mode decision;quadtrees;generalized bfos algorithm;block codes;lagrangian functions;wavelet coefficients;image sequences	This poster contains two contributions to very-low-bitrate video coding. First, we show that in contrast to common practice incremental techniques for rate-distortion optimization such as the generalized BFOS algorithm may clearly outperform the standard technique based on Lagrangian multipliers. This is relevant in cases where the computation of RDpoints has a low complexity. An implementation independent performance measure is used for comparison and run-time experiments are provided. Second, we report on recent progress of our ongoing research evaluating the prospects of adaptive vector quantization (AVQ) [1] for very-low-bitrate video coding. In contrast to conventional state-of-the-art video coding based on entropy coding of motion compensated residual frames in the frequency domain, adaptive vector quantization offers the potential to adapt its codebooks to the changing statistics of image sequences. The basic building blocks of our current AVQ video codec are (1) block-based coding in the wavelet domain where wavelet coefficients correspond to (overlapping) spatial regions, (2) hierarchical organization of the wavelet coefficients using quad-tree structures, (3) three way coding mode decision for each block (block replenishment, product code vector quantization, new VQ block with codebook update), and (4) rigorous rate/distortion optimization for all coding choices (image partition and block coding mode). This video codec does not apply motion compensation, however. A comparison with standard transform coding (H.263) shows that inspite of the improvements of our coder over previously published AVQ video coders [2, 3] it still shows a performance gap of about 1 dB for some test sequences. We conclude that motion compensation is essential also for codecs based on AVQ. First preliminary tests show that AVQ coders that incorporate motion compensation can become competitive with standard transform coding.	algorithm;codebook;codec;coefficient;computation;data compression;decibel;distortion;entropy encoding;experiment;lagrange multiplier;lagrangian (field theory);mathematical optimization;motion compensation;rate–distortion optimization;ruby document format;transform coding;universal product code;vector quantization;wavelet	Marcel Wagner;Dietmar Saupe	2000		10.1109/DCC.2000.838223	sub-band coding;computer vision;speech recognition;shannon–fano coding;harmonic vector excitation coding;variable-length code;computer science;theoretical computer science;context-adaptive variable-length coding;coding tree unit;mathematics;context-adaptive binary arithmetic coding;motion compensation;h.261;frequency domain;statistics	Vision	46.40264172524193	-16.859826592466405	52907
070ba38439e796da3e146bff82ec0ce04a5455f0	subjective and objective video codec evaluation	codecs;psnr;video quality;video sequences;code standards;transform coding;adaptive codes;bit rate;vqm;video coding;visualization;codecs transform coding bit rate psnr video sequences visualization;dscqs;avc h 264;dsis;mpeg 4;video codecs;subjective and objective evaluation;avc objective video codec subjective video codec h 264 vc 1 mpeg 4 visual mpeg 2 video sequences benchmarking;video coding adaptive codes code standards image sequences video codecs;vc 1;structural similarity;vqm subjective and objective evaluation video quality avc h 264 mpeg 4 vc 1 dscqs dsis psnr structural similarity;image sequences	This paper describes the methodology and the results of the objective and subjective evaluation of four codecs: AVC/H.264, VC-1, MPEG-4 Visual, and MPEG-2. The standard -- based evaluation procedure has been applied on three video sequences and produced 576 file versions and interesting benchmarking results.	codec;h.264/mpeg-4 avc;mpeg-2	Andreas Papadakis;Konstantinos Zachos	2011	2011 15th Panhellenic Conference on Informatics	10.1109/PCI.2011.61	subjective video quality;computer vision;codec;transform coding;visualization;peak signal-to-noise ratio;computer science;video quality;structural similarity;machine learning;multimedia;mpeg-4;statistics;computer graphics (images)	Vision	44.5172201138905	-19.690197331539395	52916
4bf17c40f225de63bf1c5abd1d684be60660aead	optimization for workspace volume of 3r robot manipulator using modified differential evolution	differential evolution;soft computing;robot manipulator;objective function;industrial robots;constrained optimization problem	  Robotic manipulators with three-revolute (3R) family of positional configurations are very common in the industrial robots  (IRs). The manipulator capability of a robot largely depends on the workspace (WS) of the manipulator apart from other parameters.  With the constraints in mind, the optimization of the workspace is of prime importance in designing the manipulator. The workspace  of manipulator is formulated as a constrained optimization problem with workspace volume as objective function. It is observed  that the previous literature is confined to use of conventional soft computing algorithms only, while a new search modified  algorithm is conceptualized and proposed here to improve the computational time. The proposed algorithm gives a good set of  geometric parameters of manipulator within the applied constrained limits. The availability of such an algorithm for optimizing  the workspace is important, especially for highly constrained environments. The efficiency of the proposed approach to optimize  the workspace of 3R manipulators is exhibited through two cases.    	differential evolution;shadow volume;workspace	Bibhuti Bhusan Biswal;Sumanta Panda;Debadutta Mishra	2010		10.1007/978-3-642-17563-3_14	differential evolution;mathematical optimization;parallel manipulator;computer science;mobile manipulator;control theory;robot control;soft computing	Robotics	31.12933048861959	-2.953678780223075	52922
d2af18c2a0fe948f385c91a6b25e4a6a4f4586ae	clustering strategy of wireless sensor networks based on improved discrete particle swarm optimization	energy efficiency;network lifetime;stagnation;dpso;low energy adaptive clustering hierarchy;wireless sensor networks particle swarm optimisation;leach;mixed dynamic inertia;inertia weight adjustment;energy imbalance;wireless sensor network;indexes;improved discrete particle swarm optimization;wsns;heuristic algorithms;particle swarm optimization;uneven clustering;mutant strategy;clustering algorithms;clustering algorithms wireless sensor networks heuristic algorithms particle swarm optimization algorithm design and analysis indexes energy efficiency;cluster head;leach clustering strategy wireless sensor networks improved discrete particle swarm optimization uneven clustering energy imbalance network lifetime mutant strategy mixed dynamic inertia heuristic algorithms low energy adaptive clustering hierarchy;mutant;discrete particle swarm optimization;leach dpso wsns stagnation mutant inertia weight adjustment heuristic algorithm remaining energy;remaining energy;particle swarm optimisation;clustering strategy;algorithm design and analysis;wireless sensor networks;heuristic algorithm;elementary particles	In this paper, an improved Discrete Particle Swarm Optimization (DPSO) algorithm is proposed to solve the uneven clustering problem, a difficult problem in Wireless Sensor Networks (WSNs) area. An uneven clustering result leads to severe energy imbalance in WSNs, which shortens the life time of networks. This paper presents a strategy to obtain good clustering result by directly using an improved DPSO algorithm. The phenomenon of premature stagnation of elementary Particle Swarm Optimization (PSO) is avoided by proposing mutant strategy and mixed dynamic inertia weight adjustment strategy, heuristic algorithms are also presented to help find global best solution efficiently in this paper. Remaining energy of cluster heads (CHs) is taken into consideration in the process of clustering too. Experiments demonstrate the proposed algorithm obtains much better clustering result in comparison with other classic clustering strategies such as Low Energy Adaptive Clustering Hierarchy (LEACH) and its improved strategy.	algorithm;cluster analysis;elementary particle;experiment;heuristic;particle swarm optimization	Jiabin Hou;Xinggang Fan;Wanliang Wang;Jing Jie;Yi Wang	2010	2010 Sixth International Conference on Natural Computation	10.1109/ICNC.2010.5582664	correlation clustering;constrained clustering;mathematical optimization;artificial intelligence;canopy clustering algorithm;machine learning;cure data clustering algorithm;mathematics	Robotics	28.736799820133164	-3.411241479309086	52974
4baa65e07ce3d046348414b862a5c36877354328	a genetic algorithm using priority-based encoding with new operators for fixed charge transportation problems	spanning tree based genetic algorithm;fixed charge transportation problem;priority based genetic algorithm	In this paper, we propose a genetic algorithm using priority-based encoding (pb-GA) for linear and nonlinear fixed charge transportation problems (fcTP) in which new operators for more exploration are proposed. We modify a priority-based decoding procedure proposed by Gen et al. [1] to adapt with the fcTP structure. After comparing well-known representation methods for a transportation problem, we explain our proposed pb-GA. We compare the performance of the pb-GA with the recently used spanning tree-based genetic algorithm (st-GA) using numerous examples of linear and nonlinear fcTPs. Finally, computational results show that the proposed pb-GA gives better results than the st-GA both in terms of the solution quality and computation time, especially for medium- and large-sized problems. Numerical experiments show that the proposed pb-GA better absorbs the characteristics of the nonlinear fcTPs.	genetic algorithm	M. M. Lotfi;Reza Tavakkoli-Moghaddam	2013	Appl. Soft Comput.	10.1016/j.asoc.2012.11.016	mathematical optimization;combinatorics;mathematics;algorithm	Logic	25.631874236928816	-0.48604921576654325	53004
b1826b080bd234e751d6b43010e1b6b60346bd48	stochastic analysis and behavior modeling of errors associated with global positioning sensor	gps position fix;global positioning sensor;dilution of precision;fgdc federal geographic data committee;stochastic analysis;mobile robot localization;standard positioning service;automated systems;selective addressability	The aim of this work is to analyze the behavior of errors associated with a commercial grade Global Positioning Sensor (GPS receiver) and thereafter, to characterize the error pattern of the sensor based on a mathematical analytic model. The measurement errors associated with a GPS receiver are generally perceived to be random. Thus for manual decision-making purposes, the inferences are drawn qualitatively by human operator with respect to a relative quality factor referred to as Dilution of Precision (DOP) as per NMEA 0183 standard. However with an appropriate mathematical modeling it can be inferred that these positioning errors behave in a definable way. The current work encompasses comparative testing of two commercial grade GPS receivers as per National Standard for Spatial Data Accuracy (NSSDA, Part 3 of FGDC-STD-007.3-1998) with Standard Positioning Service (non-differential, horizontal positioning with Selective Addressability off). The error patterns associated with these receivers are then statistically analyzed with standard statistical tools e.g. mean & standard deviation. Thereafter the Cumulative Distribution Function (CDF) and Probability Density Function (PDF) of these experimental data are compared with those of theoretical stochastic models. The corresponding stochastic models are selected which fits best with the experimental results and thus the error patterns are identified mathematically. This characterization helps in analyzing the spatial accuracy of a positioning sensor by obtaining the parameters like Circular Error Probability and R95. This characterization will further help in fusing the position information with other similar sensors and thereby increasing the confidence of position estimate. The methodology for stochastic error characterization suggested in this paper can be applied to any measurement sensor for improving the decision-making capabilities of an automated system.	fits;global positioning system;glossary of computer graphics;mathematical model;nmea 0183;portable document format;sensor;stochastic process	Vishal Veer Singh;Durga Prasad Idiwal	2013		10.1145/2506095.2506098	dilution of precision;stochastic process;simulation;data mining	Mobile	36.84927092374993	-20.396496367663477	53023
e430161466260c740a79ee45da01841e85a71ccb	mathematical analysis of the cumulative effect of novel ternary crossover operator and mutation on probability of survival of a schema	schema;ternary crossover;schema survival;genetic algorithms;mutation	The concept of schema plays a vital role in the study of genetic algorithms. The effect of selection, simple crossover and mutation on schemata has already been studied rigorously by several researchers. In this paper a novel ternary crossover operator is introduced and its effects on the probability of survival of a schema are meticulously analyzed. A theorem regarding the effect of novel crossover operator on survival of schemata is proved and based on that the combined effect of novel crossover and mutation on probability of survival of a schema is established mathematically.		Apoorva Mishra;Anupam Shukla	2017	Theor. Comput. Sci.	10.1016/j.tcs.2016.07.035	mutation;genetic algorithm;computer science;artificial intelligence;theoretical computer science;schema;mathematics;algorithm	ECom	26.856293324912244	-8.531053070312312	53040
1634eaa5434994a1a7dee93cbdb976059d534227	dynamic simulation metamodeling using mars: a case of radar simulation	mars;dynamic simulation model;simulation;statistical design of experiments;metamodeling	Dynamic system simulations require relating the inputs to the multivariate output which can be a function of time-space coordinates. In this work, we propose a methodology for the metamodeling of dynamic simulation models via Multivariate Adaptive Regression Splines (MARS). To handle incomplete output processes, where the simulation model does not produce an output in some steps due to missing inputs, we have devised a two-stage metamodeling scheme. The methodology is demonstrated on a dynamic radar simulation model. The prediction performance of the resulting metamodel is tested with four different sampling techniques (i.e., designs) and 16 sample sizes. We also investigate the effect of alternative coordinate system representations on the metamodeling performance. The results suggest that MARS is an effective method for metamodeling dynamic simulations, particularly, when expert judgment is not readily available. Results also show that there are interactions between the coordinate systems and sampling techniques, and some design-representation-size combinations are very promising in the metamodeling of radar simulations.	dynamic simulation;metamodeling;radar	Doruk Bozagaç;Inci Batmaz;Halit Oguztüzün	2016	Mathematics and Computers in Simulation	10.1016/j.matcom.2016.01.005	metamodeling;econometrics;mathematical optimization;mars exploration program;simulation	Robotics	35.56046212653729	-16.402909489679892	53065
56a0849936d6a4d8c954a7d8ca86a906c9c087e4	on estimating the size and confidence of a statistical audit	statistical sampling;statistical audit;auditing elections;working paper	We consider the problem of statistical sampling for auditing elections, and we develop a remarkably simple and easily-calculated upper bound for the sample size necessary for determining with probability at least c if a given set of n objects contains fewer than b “bad” objects. While the size of the optimal sample drawn without replacement can be determined with a computer program, our goal is to derive a highly accurate and simple formula that can be used by election officials equipped with only a hand-held calculator. We actually develop several formulae, but the one we recommend for use in practice is:	computer program;mobile device;sampling (signal processing);statistical model	Javed A. Aslam;Raluca A. Popa;Ronald L. Rivest	2007			econometrics;computer science;data mining;statistics	ML	29.906551238242287	-20.19644621364979	53069
4358dfaec663ac0e27c9928e5c10f6a28a302812	automated transient input stimuli generation for analog circuits	nonlinear analog circuits;rapidly exploring random trees;state space automated directed random input stimulus generation algorithm nonlinear analog circuits user defined goal regions monte carlo based methods multiobjective rapidly exploring random trees morrt feedback loop random trees algorithm statistical inference algorithm;directed input stimulus generation;analog circuits;trees mathematics analogue circuits inference mechanisms monte carlo methods nonlinear network synthesis statistical analysis;trajectory;integrated circuit modeling;mathematical model;clustering algorithms;inference algorithms;inference algorithms analog circuits monte carlo methods integrated circuit modeling mathematical model trajectory clustering algorithms;monte carlo methods;rapidly exploring random trees rrts directed input stimulus generation nonlinear analog circuits	We present an automated directed random input stimulus generation algorithm with high coverage for nonlinear analog circuits. Our methodology is able to generate input stimuli to meet two kinds of objectives: 1) to reach user-defined goal regions and 2) increased coverage of state space. The principal benefit of our approach is that it can provide directed input stimulus generation, as opposed to the randomly generated input stimulus by Monte Carlo-based methods. The methodology introduces multiobjective rapidly-exploring random trees (MORRTs), which add a bias and a feedback loop to the standard rapidly-exploring random trees algorithm. The biasing is provided by a statistical inference algorithm. Simultaneous biasing toward goal regions and coverage is possible in MORRT to a user-defined extent. Our methodology generates several input stimuli that are concentrated in the goals or relevant operating regions, while providing high coverage of the state space. We demonstrate the efficiency and scalability of our approach on high-dimensional analog case studies.	algorithm;analogue electronics;biasing;feedback;monte carlo method;nonlinear system;procedural generation;scalability;state space	Seyed Nematollah Adel Ahmadyan;Shobha Vasudevan	2016	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2015.2488482	mathematical optimization;analogue electronics;computer science;trajectory;theoretical computer science;machine learning;mathematical model;mathematics;cluster analysis;statistics;monte carlo method	EDA	29.193864788844817	-11.22713432401282	53156
f39e69c45c533b5d9a1a7217a83c4d43fb436d51	new proposal for a multi-objective technique using tribes and tabu search	tabu search	The aim of this paper is to present a new multi-objective technique which consists on a hybridization between a particle swarm optimization approach (Tribes) and tabu search technique. The main idea of the approach is to combine the high convergence rate of Tribes with a local search technique based on Tabu Search. Besides, in our study, we proposed different places to apply local search: the archive, the best particle among each tribe and each particle of the swarm. As a result of our study, we present three versions of our hybridized algorithm. The mechanisms proposed are validated using twelve different functions from specialized literature of multi-objective optimization. The obtained results show that using this kind of hybridization is justified as it is able to improve the quality of the solutions in the majority of cases.	algorithm;archive;distribution (mathematics);evolutionary algorithm;global distance test;local search (optimization);mathematical optimization;multi-objective optimization;particle swarm optimization;rate of convergence;remote desktop services;tabu search	Nadia Smairi;Sadok Bouamama;Khaled Ghédira;Patrick Siarry	2010			tabu search;computer science;artificial intelligence;guided local search	AI	25.416626425585964	-3.352587778220461	53207
0a56a5d4db0e414c6059852e9abf1c36d206bb31	ant colony optimization with partial-complete searching for attribute reduction	ant colony optimization;attribute reduction;cost;partial complete searching;heuristic algorithm	The time-cost-sensitive attribute reduction problem is more challenging than the classical reduct problem since the optimal solution is sparser. Ant colony optimization (ACO) is an effective approach to this problem. However, the efficiency is unsatisfactory since each ant needs to search for a complete solution. In this paper, we propose a partial-complete searching technique for ACO and design the APC algorithm. Partial searching is undertaken by pioneer ants through selecting only a few attributes to save time, while complete searching is undertaken by harvester ants for complete solutions. Experiments are undertaken on seven real-world and a set of artificial datasets with various settings of costs. Compared with two bio-inspired and two greedy algorithms, APC is more efficient while obtaining the same level of quality metrics. The APC algorithm can be also extended for other combinatorial optimization problems.	ant colony optimization algorithms;british informatics olympiad;combinatorial optimization;experiment;greedy algorithm;mathematical optimization	Fan Min;Zhi-Heng Zhang;Ji Dong	2018	J. Comput. Science	10.1016/j.jocs.2017.05.007	heuristic;mathematical optimization;ant colony optimization algorithms;computer science;machine learning;data mining;mathematics;metaheuristic	AI	25.096358762594015	0.499077349914555	53215
3e2e75216fc63c2e0dbe8eece220a811121dc71d	a new emigrant creation strategy based on local best sources for parallel artificial bee colony algorithm	optimisation evolutionary computation;standards;computer architecture;emigrant creation artificial bee colony algorithm distributed systems;particle swarm optimization;standards particle swarm optimization optimization sociology statistics computer architecture;statistics;parallelized artificial bee colony algorithm emigrant creation strategy local best sources swarm intelligence based optimization algorithms foraging behavior distributed architectures independent compute nodes objective function values convergence performance;optimization;sociology	Artificial Bee Colony algorithm, inspired by the foraging behavior of real honey bees, is one of the most important swarm intelligence based optimization algorithms. Like other population based evolutionary computation techniques, Artificial Bee Colony algorithm is suitable for parallelization on distributed architectures. In this paper, we presented a new emigrant creation strategy that is being distributed between subcolonies running simultaneously on the independent compute nodes. The running times and objective function values obtained by the parallelized Artificial Bee Colony algorithm with the proposed model on different number of compute nodes are compared with the sequential counterpart of the algorithm and it is seen that convergence performance of the parallelized Artificial Bee Colony algorithm is significantly improved with the proposed emigrant creation strategy.	artificial bee colony algorithm;evolutionary computation;loss function;mathematical optimization;optimization problem;parallel computing;swarm intelligence	Dervis Karaboga;Selcuk Aslan	2016	2016 24th Signal Processing and Communication Application Conference (SIU)	10.1109/SIU.2016.7495886	mathematical optimization;swarm intelligence;computer science;artificial intelligence;machine learning;artificial bee colony algorithm;metaheuristic	HPC	26.19421999444631	-2.905329613021502	53236
99c474a68e33f2a07451117f86c858f71a4e6ca0	an archive maintenance scheme for finding robust solutions	objective function;local search	This paper presents an archive maintenance scheme that can be used within an evaluation scheme for finding robust optima when dealing with expensive objective functions. This archive maintenance scheme aims to select the additional sampling points such that locally well-spread distributions of archive points will be generated. By doing so, the archive will contain better predictive information about the robustness of candidate solutions. Experiments on 10D test problems show that this scheme can be used for accurate local search for robust solutions.	archive	Johannes W. Kruisselbrink;Michael T. M. Emmerich;Thomas Bäck	2010		10.1007/978-3-642-15844-5_22	mathematical optimization;simulation;computer science;local search;data mining;mathematics	Vision	25.28476321696009	-1.945461187147127	53276
18a3795201fd2d810dfc781ce81163d708afd465	adaptive fractional-pixel motion estimation skipped algorithm for efficient hevc motion estimation		High-Efficiency Video Coding (HEVC) efficiently addresses the storage and transmit problems of high-definition videos, especially for 4K videos. The variable-size Prediction Units (PUs)--based Motion Estimation (ME) contributes a significant compression rate to the HEVC encoder and also generates a huge computation load. Meanwhile, high-level encoding complexity prevents widespread adoption of the HEVC encoder in multimedia systems. In this article, an adaptive fractional-pixel ME skipped scheme is proposed for low-complexity HEVC ME. First, based on the property of the variable-size PUs--based ME process and the video content partition relationship among variable-size PUs, all inter-PU modes during a coding unit encoding process are classified into root-type PU mode and children-type PU modes. Then, according to the ME result of the root-type PU mode, the fractional-pixel ME of its children-type PU modes is adaptively skipped. Simulation results show that, compared to the original ME in HEVC reference software, the proposed algorithm reduces ME encoding time by an average of 63.22% while encoding efficiency performance is maintained.	4k resolution;algorithm;computation;computational complexity theory;digital video;encoder;hdmi;high efficiency video coding;high- and low-level;ibm systems network architecture;motion estimation;one-class classification;pixel;simulation	Zhaoqing Pan;Jianjun Lei;Yajuan Zhang;Fu Lee Wang	2018	TOMCCAP	10.1145/3159170	reference software;encoder;motion estimation;pixel;computation;computer science;algorithm;data compression ratio	EDA	46.692433028806555	-19.178726019950435	53290
914bfdbe977119c57e2b06426ba05e0f21b20618	improved mixture representation in real-time particle filters for robot localization		Monte Carlo methods have been successfully adopted for robot localization thanks to their flexibility in distribution representation. However, these techniques are computationally expensive and can hardly perform at the incoming sensor data rate, when computation resources are limited. The Real-Time Particle Filter (RTPF) is an algorithmic solution conceived to make execution of a particle filter iteration feasible within time constraints by means of a mixture representation for the set of samples. RTPF requires an optimal balance of the contribution of each set to the mixture, whose computation, unfortunately, is quite difficult. In this paper, we provide a formal discussion of mixture representation by considering the weight mixture. We illustrate a novel solution for computing the mixture parameters based on the notion of effective sample size. This solution is less prone to numerical instability. Finally, we compare the proposed approach with the original RTPF algorithm through simulation tests and experiments.	algorithm;analysis of algorithms;computation;data rate units;experiment;gradient descent;instability;internationalization and localization;iteration;mobile robot;monte carlo method;numerical stability;particle filter;real-time clock;real-time transcription;robotic mapping;simulation;window function	Dario Lodi Rizzini;Stefano Caselli	2007			mathematical optimization;simultaneous localization and mapping;computation;monte carlo method;particle filter;mathematics;numerical stability;effective sample size;monte carlo localization;control engineering	Robotics	52.425710350036695	3.4957912211523707	53359
3f81ed2921f4928c0dde67bcf8ed455854dd9d23	60-channel high-resolution counter array for high-speed continuous long-term data acquisition	60 channel high resolution counter array;single photon avalanche diode array sensor;single photon avalanche diode;high speed continuous long term data acquisition;high resolution;fpga;tcp ip server 60 channel high resolution counter array high speed continuous long term data acquisition single photon avalanche diode array sensor state of art astronomical application commercial off the shelf fpga dsp ieee 1394 high speed serial link linux box;data acquisition system;proof of concept;avalanche photodiodes;commercial off the shelf;ieee 1394 high speed serial link;photodetectors;state of art astronomical application;sensor arrays avalanche photodiodes digital signal processing chips field programmable gate arrays photodetectors;linux box;digital signal processing chips;field programmable gate arrays;tcp ip server;data acquisition;high speed;sensor arrays;dsp	We present a fast and highly compact data acquisition system suitable for interfacing with photon-counter SPADA (single-photon avalanche diode array) sensors, which are used in state-of-art astronomical applications. The acquisition system features a 60-channel counter array, which is able to capture events with a rate of up to 20 MHz, and allows a continuous long-term data acquisition, lasting up to several hours, with a time resolution downto few tenths of microseconds at the same time. The system is based on a commercial-off-the-shelf (COTS) single board, allocating an FPGA and a DSP, connected by means of an IEEE 1394 high-speed serial link to a Linux box, which implements a TCP/IP server. Therefore, the acquisition parameters, as well as the collected data, can remotely be accessed through a LabView interface running on a different PC. A proof-of-concept demonstration has been realized and the experimental results are presented.	data acquisition;digital signal processor;diode;field-programmable gate array;high speed serial link;ieee 1394;image resolution;internet protocol suite;labview;linux;sensor;serial communication;server (computing)	Federico Baronti;Diego Lunardini;Roberto Roncella;Roberto Saletti;Franco Zappa	2005	2005 12th IEEE International Conference on Electronics, Circuits and Systems	10.1109/ICECS.2005.4633495	embedded system;electronic engineering;computer hardware;engineering;data acquisition	Robotics	44.193971051064	-2.4057552080425557	53399
5a47162db7b74cf301209e0faccc348acf1ca472	low power motion estimation algorithm based on temporal correlation and its architecture	temporal correlation;processing element;low power motion estimation algorithm;teleconferencing;motion estimation signal processing algorithms psnr search methods computer architecture video compression energy consumption electronics industry industrial electronics power engineering and energy;processing element architecture;psnr;fast search algorithms;fast motion estimation algorithm;search algorithm;search methods;video compression;search method;motion estimation;radiotelephony;industrial electronics;correlation methods;mobile phone;power engineering and energy;video coding;computer architecture;fast motion estimation;performance improvement;center focused search;low power;land mobile radio;video conferencing;energy consumption;macro block;image sequence low power motion estimation algorithm temporal correlation fast macro block mode normal macro block motion vector fast motion estimation algorithm fast search algorithms four step search center focused search psnr level processing element architecture mobile phone video conferencing video compression;motion vector;fast macro block mode;electronics industry;normal macro block;image sequence;digital signal processing chips;search problems;four step search;signal processing algorithms;parallel processing;digital signal processing chips correlation methods motion estimation video coding teleconferencing radiotelephony land mobile radio search problems image sequences parallel processing;psnr level;image sequences	We propose architecture of low power motion estimation algorithm. There are two types of MB (macro block) modes in the proposed algorithm (fast MB mode and normal MB mode). In the fast MB mode, the motion vector found from the previous frame is utilized in the next frame. This mode can be adopted in the conventional fast motion estimation algorithm, and as a result, the computational power is reduced by 40%. In normal the MB mode, among the conventional fast search algorithms, we take the 4SS (four step search), and introduce an additional search method called a center-focused search in the step to increase the PSNR level. We then, implement the corresponding PE (processing element) architecture that gives the hardware performance improvement. The new motion estimation architecture is especially efficient for mobile phone and video conferencing applications in which there is not much motion.	algorithm;motion estimation	Sun-Hyoung Han;Sung-Woo Kwon;Tae-Young Lee;Moon-Key Lee	2001		10.1109/ISSPA.2001.950228	parallel processing;computer vision;real-time computing;electronics;quarter-pixel motion;computer science;motion estimation;search algorithm	Vision	48.67191384930989	-19.540429136515467	53436
1f8272bc96e0b734b75932fe92703a136a504c4e	evolutionary antenna design via modified normalized gt algorithm	genetic operator;evolutionary antenna design;evolvable hardware;gt algorithm;constrained multi objective optimization;ames research center;evolutionary algorithm;space technology 5	Jason Lohn and his workmates in NASA Ames Research Center have evolved an X-band antenna for NASA's Space Technology 5 (ST5) mission which is the first evolved hardware in space. However their approach did not put too much attention on the efficiency of the evolutionary algorithm. Owing to the flaw we employ GT Algorithm to tackle constraints and balance multi-objective via normalization which makes evolution more efficient. Moreover we adopt a kind of linear real-values code to describe the structure of antenna so that it is easier to carry out genetic operations and control the size of the antenna. We have evolved a wire antenna successfully via this approach and all the targets have been meted.	algorithm	Yuanyuan Fan;Qingzhong Liang;Zhenhua Cai;Hui Li	2008		10.1007/978-3-540-92137-0_14	simulation;engineering;artificial intelligence;evolved antenna	ML	32.67318211280502	-2.790155791146512	53448
c44b649dda0f544a245290e244ebf5f9233b28a4	image encryption based on a novel reality-preserving fractional fourier transform	image encryption;image coding;digital image encryption;spatial domain;fractional fourier transform;information security;statistical analysis cryptography fourier transforms image coding;information security digital image encryption reality preserving fractional fourier transform decorrelation property spatial domain statistic analysis;cryptography fourier transforms image storage information security digital images decorrelation resists statistical analysis displays robustness;statistical analysis;cryptography;fourier transforms;statistic analysis;digital image;reality preserving fractional fourier transform;decorrelation property	A novel method of digital image encryption is presented. Image encryption and decryption are performed based on the continuously increasing decorrelation property and the real-valuedness of the reality-preserving fractional Fourier transform. The input and encrypted data are respectively in the spatial domain and the reality-preserving fractional Fourier transformed domain determined by encryption keys. It resists statistic analyses effectively. In addition, the encrypted image is real-valued, so it's convenient for display and storage. The simulation result shows that the parameters of the reality-preserving fractional Fourier transform enhance the space of keys. As a method of encryption, it won't bring data expanding, and is sensitive to parameters, with considerable robustness, security, and feasible implementation in practice, which has a good prospect and practicability in information security field	cryptography;decorrelation;digital image;encryption;fractional fourier transform;information security;simulation	Yi Xin;Ran Tao;Yue Wang	2006	First International Conference on Innovative Computing, Information and Control - Volume I (ICICIC'06)	10.1109/ICICIC.2006.461	computer vision;discrete mathematics;computer science;cryptography;information security;theoretical computer science;fractional fourier transform;mathematics;digital image;probabilistic encryption;statistics	Visualization	39.388003856206964	-9.516867105278271	53511
bff83ac9d2626ee9146d67f9b24c7b1663bca512	efficient schemes for compressed-domain image resizing	upsampling;computational complexity;downsampling;filtering;convolution;jpeg;dct	Fast schemes for compressed-domain image size change, are proposed. Fast Winograd DCTs are applied to resizing images by a factor of two to one. First, we speed up the DCT domain downsampling scheme which uses the bilinear interpolation. Then, we speed up other image resizing schemes which use DCT lowpass truncated approximations. The schemes proposed here reduce the computational complexities significantly, while there is no difference in the overall quality of the images compared to previous works. key words: compressed domain processing, DCT, JPEG, MPEG, convolution, downsampling, filtering, upsampling	analysis of algorithms;approximation;bilinear filtering;convolution;coppersmith–winograd algorithm;decimation (signal processing);discrete cosine transform;fast fourier transform;image resolution;image scaling;interpolation;jpeg;low-pass filter;upsampling	Do Nyeon Kim;Yoonsik Choe;Kamisetty R. Rao	2009	IEICE Transactions		upsampling;computer vision;computer science;theoretical computer science;mathematics;algorithm;computer graphics (images)	Vision	44.14666919358653	-16.575338864618974	53588
dabc28a15281dca74fdb4ca6ac142b7c26ac58bf	reversible video data hiding using zero qdct coefficient-pairs		H.264/Advanced Video Coding (AVC) is one of the most commonly used video compression standard currently. In this paper, we propose a Reversible Data Hiding (RDH) method based on H.264/AVC videos. In the proposed method, the macroblocks with intra-frame 4 × 4 prediction modes in intra frames are first selected as embeddable blocks. Then, the last zero Quantized Discrete Cosine Transform (QDCT) coefficients in all 4 × 4 blocks of the embeddable macroblocks are paired. In the following, a modification mapping rule based on making full use of modification directions are given. Finally, each zero coefficient-pair is changed by combining the given mapping rule with the to-be-embedded information bits. Since most of last QDCT coefficients in all 4 × 4 blocks are zero and they are located in high frequency area. Therefore, the proposed method can obtain high embedding capacity and low distortion.	coefficient;discrete cosine transform;distortion;embedded system;h.264/mpeg-4 avc;intra-frame coding;noise (electronics);peak signal-to-noise ratio;video coding format	Yi Chen;Hongxia Wang;Hanzhou Wu;Yong Liu	2018	CoRR		information hiding;rule-based system;coding (social sciences);computer science;artificial intelligence;quantization (physics);data compression;discrete cosine transform;distortion;pattern recognition;embedding	EDA	44.39331771090123	-15.951012314831278	53639
ebd1fe4471195be81728f0bee9186074dded940c	a current-mode programmable and expandable hamming neural network	cmos integrated circuits;time sharing;current mode;maximum matching;circuit complexity;chip;integrated circuit design;neural networks circuits cmos technology sorting time sharing computer systems prototypes pattern matching microelectronics virtual prototyping artificial neural networks;neural chips;programmable circuits;switched current circuits;current mode circuits;vlsi;time sharing systems;integrated circuit design neural chips current mode circuits programmable circuits switched current circuits time sharing systems vlsi cmos integrated circuits circuit complexity;neural network;1 2 mum current mode programmable expandable hamming neural network maximum matching currents sorting order programmable binary template learnable binary template switched current technique time sharing mode complexity hspice simulation	A current-mode programmable and expandable Hamming neural network with the ability to output the first K maximum matching currents from M ones in sorting order is put forward in this papel: The binary template can be programmable or learnable if needed in this network and the K maximum matching currents can be output in sorting order based on the switched-current technique, and its corresponding lubels are also output in time sharing mode in the same time. The complexity of this network is just O(N) and its scale can be easily expanded. This network has been fabricated in a 1.2CMOS technology. Both Hspice simulation and experimental results of the prototype chip show good performance.	artificial neural network;matching (graph theory);prototype;spice 2;simulation;sorting;time-sharing	Guoxing Li;Bingxue Shi	1999		10.1109/IJCNN.1999.833450	chip;circuit complexity;embedded system;real-time computing;computer science;machine learning;very-large-scale integration;cmos;time-sharing;artificial neural network;matching;integrated circuit design	AI	38.98600939414713	-2.182251013102625	53695
ce6784cb8deaf3a05d3395e2624c02e8796a30e5	definition of the opus audio codec	category	This document describes the Opus codec, designed for interactivenspeech and audio transmission over the Internet.	codec;libopus	Jean-Marc Valin;Koen Vos;Timothy B. Terriberry	2012	RFC	10.17487/RFC6716	adaptive multi-rate audio codec;codec;real-time computing;speech recognition;computer science;celt;multimedia	HCI	46.71161962642056	-8.044976503285135	53707
42bfe996c72b26da8d9cacd08eb529de124948ac	predator-prey behavior firefly algorithm for solving 2-chlorophenol reaction kinetics equation		2-Chlorophenol is a kind of representative organic waste water. With the environmental pollution becoming increasingly serious, and the large amount of waste discharged and the increasing difficulty of treatment, the research on the kinetics of the oxidation of supercritical water of 2-chlorophenol has important significant. Aiming at the phenomenon that the Glowworm Swarm Optimization (GSO) algorithm has slow convergence, low precision and easy to get trapped into local optimum, this paper presents an improved version of the GSO based on the behavior of predator-prey and biological predator, and we call it dual population Glowworm Swarm Optimization (GSOPP). The algorithm accelerates the convergence speed by introducing strategies such as chase and escape and variation among populations, and can obtain a more accurate solution. Tested by three standard test functions, the results showed that the improved GSOPP algorithm had better performance than the basic GSO algorithm. Finally, the algorithm was applied to estimate the parameter estimation of the supercritical water oxidation kinetics of 2-chlorophenol, and satisfactory results were obtained.	firefly algorithm;kinesiology;lotka–volterra equations;prey	Yuanbin Mo;Yanyue Lu;Fuyong Liu	2018		10.1007/978-3-319-93815-8_43	local optimum;mathematical optimization;firefly algorithm;supercritical water oxidation;glowworm swarm optimization;environmental pollution;estimation theory;population;computer science;convergence (routing)	Robotics	29.102703320158763	-4.700612109679229	53721
618413e93eaf54c98d6825978a016315fe2d51e1	a trustworthy computing of adapt principle guaranteeing genuine medical image	mammography trustworthy computing adapt principle forensic analysis medical malpractice;forensic analysis;medical malpractice;image coding;cancer;medical diagnostic imaging breast cancer hospitals;information security;adapt principle;trusted computing;medical image;medical image processing;trusted computing cancer image coding medical image processing;image diagnoses trustworthy computing adapt principle guaranteeing genuine medical image cancer chemotherapy radiation medical malpractice breast cancer trustworthy image steganography concepts medical image diagnosis patients death information security mammography image;mammography;data confidentiality;breast cancer;trustworthy computing	Cancer is usually treated with surgery and probably with chemotherapy or radiation. A medical malpractice of breast cancer has become an urgent need to clear the mess based on technical, workable strategy. The paper tries to: (i) understand a general diagnosis, (ii) pay attentions on how to improve the right judgments of possible misinterpretation, (iii) focus on breast cancer malpractice combined with digital medical image, (iv) present a trustworthy image based upon ADAPT principle, (v) utilize steganography concepts to assure data confidentiality, integrity, and authenticity. The physician's failure to diagnose medical images can result a patient's death, or cause enormous medical bills. A workable strategy of trustworthy computing in APAPT principle is proposed to improve the information security issue of mammography image, and establish the trustworthy computing of image diagnoses.	confidentiality;emoticon;information security;medical imaging;steganography;trustworthy computing	Da-Yu Kao;Shiuh-Jeng Wang;Dushyant Goyal;Jonathan C. L. Liu	2011	2011 IEEE 17th International Conference on Parallel and Distributed Systems	10.1109/ICPADS.2011.20	computer science;information security;trustworthy computing;computer security	Vision	38.369621881752515	-13.321663797889624	53750
976cf45e6b9c2b9e3d2e69079f6824bb61e641ad	rate-distortion based optimization for zerotree entropy wavelet coding	rate control algorithm entropy wavelet coding bit allocation mpeg 4 multiscale zerotree entropy coding visual texture coding tool iterative quadratic r d model algorithm image quality fast convergence rate distortion based optimization image coding;quadratic programming;quantization;rate distortion;optimisation;rate distortion entropy quantization image coding encoding bit rate mpeg 4 standard image quality wavelet coefficients quadratic programming;image coding;optimization technique;rate distortion mpeg 4 standard bit rate image coding entropy coding image quality video coding quantization wavelet coefficients scalability;rate distortion based optimization;accurate quadratic modeling;iterative methods trees mathematics entropy codes wavelet transforms transform coding rate distortion theory optimisation code standards telecommunication standards image coding image texture;visual texture coding tool;entropy coding;mpeg 4 visual texture coding;code standards;transform coding;trees mathematics;bit rate;efficient iterative technique;image texture;rate distortion theory;iterative methods entropy trees mathematics wavelet transforms transform coding rate distortion theory optimisation image texture code standards telecommunication standards;wavelet transforms;iterative methods;algorithm;video coding;mpeg 4 standard;multiscale zerotree entropy coding;iterative quadratic r d model;entropy codes;telecommunication standards;image quality;rate distortion function;fast iterative technique;entropy wavelet coding;mpeg 4;fast convergence;rate control algorithm;entropy;scalability;bit allocation;efficient iterative technique rate distortion based optimization zerotree entropy wavelet coding mpeg 4 visual texture coding accurate quadratic modeling rate distortion function bit rate image quality fast iterative technique;encoding;zerotree entropy wavelet coding;wavelet coefficients	We present a rate-distortion (R-D) based bit allocation scheme for MPEG-4 zerotree entropy based wavelet coding, specifically, the multiscale zerotree entropy coding mode (MQ) of the MPEG-4 visual texture coding tool. With an iterative quadratic R-D model, our algorithm achieves accurate target bit rate and better image quality with fast convergence.	distortion;entropy (information theory);mathematical optimization;wavelet transform	Hung-Ju Lee;Tihao Chiang;Ya-Qin Zhang	1999	IEEE Trans. Consumer Electronics	10.1109/30.793559	image quality;image texture;computer vision;entropy;mathematical optimization;scalability;transform coding;rate–distortion theory;quantization;harmonic vector excitation coding;computer science;entropy encoding;theoretical computer science;mathematics;tunstall coding;iterative method;mpeg-4;quadratic programming;algorithm;encoding;wavelet transform	EDA	46.7490617599391	-16.600231285913335	53751
f36ac114264cfda610e52987fb1d775f57006354	monte carlo algorithms for elliptic differential equations. data parallel functional approach	linear algebra;distributed memory;data parallel;partial differential equation;numerical solution;boundary value problem;data parallelism;differential equation;f 2 1;elliptic differential equation;numerical algorithm;monte carlo algorithm;c 1 2;monte carlo algorithms;monte carlo;integral representation;functional language;g 3	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	algorithm;data parallelism;francis;functional approach;monte carlo method;primary source	Ivan Todor Dimov;Aneta Karaivanova;Herbert Kuchen;Holger Stoltze	1996	Parallel Algorithms Appl.	10.1080/10637199608915563	quantum monte carlo;quasi-monte carlo method;mathematical optimization;diffusion monte carlo;discrete mathematics;parallel computing;distributed memory;hybrid monte carlo;markov chain monte carlo;boundary value problem;computer science;theoretical computer science;linear algebra;monte carlo molecular modeling;mathematics;data parallelism;monte carlo integration;differential equation;partial differential equation;monte carlo algorithm;monte carlo method	Robotics	48.93133723231789	-2.944147171655621	53768
0abd10ea347adc9aa23aeacf2daa5675d45a5182	elitist generational genetic chromodynamics - a new radii-based evolutionary algorithm for multimodal optimization	parameter tuning eltist generational genetic chromodynamics radii based evolutionary algorithm multimodal optimization species formation parallel search convergence generational selection replacement scheme function optimization function classification;genetics;function optimization;optimization problem;parameter tuning;genetics evolutionary computation biological cells computer science mathematics algorithm design and analysis merging medical tests design optimization convergence;genetic algorithms;search problems;local interaction;evolutionary algorithm;search problems genetic algorithms parallel algorithms;parallel algorithms	A new radii-based evolutionary algorithm (EA) designed for multimodal optimization problems is proposed. The approach can be placed within the genetic chromodynamics framework and related to other EAs with local interaction, e.g. using species formation or clearing procedures. The underlying motivation for modifying the original algorithm was to preserve its ability to search for many optima in parallel while increasing convergence speed, especially for complex problems, through generational selection and different replacement schemes. The algorithm is applied to function optimization and classification; obtained experimental results, in part improved immensely by state-of-the-art parameter tuning (SPO), and encouraged further investigation.	evolutionary algorithm;evolutionary multimodal optimization;mathematical optimization;multimodal interaction;pareto efficiency	Catalin Stoean;Mike Preuss;Ruxandra Gorunescu;Dumitru Dumitrescu	2005	2005 IEEE Congress on Evolutionary Computation	10.1109/CEC.2005.1554911	optimization problem;mathematical optimization;meta-optimization;genetic algorithm;interactive evolutionary computation;cultural algorithm;computer science;derivative-free optimization;artificial intelligence;theoretical computer science;machine learning;evolutionary algorithm;mathematics;parallel algorithm;imperialist competitive algorithm;metaheuristic;evolutionary computation	Comp.	27.205103657725452	-5.568069998559138	53849
eb83465125b6238803bcb9a920515a6b36c25e6d	provably secure authentication of digital media through invertible watermarks	digital image;provable security;digital media;signal processing	The recent advances in multimedia technology have made the manipulation of digital images, videos or audio files easy. On the one hand the broad availability of these new capabilities enabled numerous new applications. On the other hand, for the same reasons, digital media can easily be forged by almost anyone. To counteract this risk, fragile watermarks were proposed to protect the integrity and authenticity of digital multimedia objects. Traditional watermarking schemes employ non-cryptographic and signal processing oriented techniques, which fail to provide any provable security guarantee against malicious modification attempts. In this paper, we give for the first time a provably secure authentication mechanism for digital multimedia files that is based on both cryptographic signatures and invertible watermarks. While traditional watermarking schemes introduce some small irreversible distortion in the digital content, invertible watermarks can be completely removed from a watermarked work.	authentication;cryptography;data integrity;digital image;digital media;digital recording;digital signature;digital watermarking;distortion;malware;message authentication;online and offline;provable security;signal processing;streaming media;type signature;watermark (data file)	Jana Dittmann;Stefan Katzenbeisser;Christian Schallhart;Helmut Veith	2004	IACR Cryptology ePrint Archive			Security	37.757295789130346	-12.372861581824454	53895
df9a15c40e3f0aeff31dfe609523b9ce04ac5b6a	numerical solution of dirichlet boundary value problems for partial differential equations using quantum-behaved particle swarm optimization with random gaussian function	quantum behaved particle swarm optimization partial differential equation artificial intelligence computational intelligence evolutionary computation;partial differential equation;evolutionary computation;gaussian processes;quantum behaved particle swarm optimization;computational intelligence;convergence of numerical methods;nonlinear single pde numerical solution dirichlet boundary value problems partial differential equations quantum behaved particle swarm optimization random gaussian function mesh based algorithm random median filter mutation operator local minima qpso convergence linear single pde;partial differential equations;computational complexity;random processes;random processes boundary value problems computational complexity convergence of numerical methods gaussian processes median filters mesh generation partial differential equations particle swarm optimisation;artificial intelligence;boundary value problems;sociology statistics particle swarm optimization genetic algorithms random variables boundary conditions;mesh generation;particle swarm optimisation;median filters	A new mesh-based algorithm to solve partial differential equations (PDEs) using quantum-behaved particle swarm optimization (QPSO) with random Gaussian function and random median filter is proposed in this paper. The random Gaussian function behaves as a mutation operator of QPSO to escape from local minima, and the random median filter accelerates the convergence of QPSO. It provides accurate results for Dirichlet boundary value problems of both linear and nonlinear single PDEs in two space dimensions.	algorithm;mathematical optimization;maxima and minima;median filter;nonlinear system;numerical partial differential equations;particle swarm optimization;quantum	Youngmin Ha	2012	2012 11th International Conference on Machine Learning and Applications	10.1109/ICMLA.2012.126	gaussian random field;mathematical optimization;combinatorics;computer science;artificial intelligence;machine learning;computational intelligence;mathematics;partial differential equation;evolutionary computation	Robotics	29.634776467790946	-6.461493014934855	53923
19e13eeabbd73c671300aa8da4ccd3bf1d8b4765	the use of simulation techniques for hybrid software cost estimation and risk analysis	empirical study;point estimation;software cost estimation;risk analysis;random sampling;project manager;software effort estimation;selective sampling;latin hypercube sampling;experimental software engineering;simulation technique;probability distribution;cost estimation;sampling methods;monte carlo simulation;analytic solution	Cost estimation is a crucial field for companies developing software or software-intensive systems. Besides point estimates, effective project management also requires information about cost-related project risks, e.g., a probability distribution of project costs. One possibility to provide such information is the application of Monte Carlo simulation. However, it is not clear whether other simulation techniques exist that are more accurate or efficient when applied in this context. We investigate this question with CoBRA1, a cost estimation method that applies simulation, i.e., random sampling, for cost estimation. This chapter presents an empirical study, which evaluates selected sampling techniques employed within the CoBRA method. One result of this study is that the usage of Latin Hypercube sampling can improve average simulation accuracy by 60% and efficiency by 77%. Moreover, analytical solutions are compared with sampling methods, and related work, limitations of the study, and future research directions are described. In addition, the chapter presents a comprehensive overview and comparison of existing software effort estimation methods.	cost estimation in software engineering;monte carlo method;sampling (signal processing);simulation;software development effort estimation	Michael Kläs;Adam Trendowicz;Axel Wickenkamp;Jürgen Münch;Nahomi Kikuchi;Yasushi Ishigai	2008	Advances in Computers	10.1016/S0065-2458(08)00604-9	sampling;simulation;computer science	SE	28.32919978012598	-17.208502302773216	53943
421216069a8281f8c34e45dc81b9229ff2471b58	generic path planning for real-time applications	data visualisation;path planning;terrain mapping;tree searching;a* algorithm;generic path planning;path planning algorithm;real-time application	We present a fast and robust path planning algorithm for generic static terrains with polygonal obstacles. Our algorithm finds shorter, and therefore more intuitive paths than a traditional A* approach with a similar underlying graph. The presented algorithm is derived from A* and is modified to circumvent undecidable situations and unintuitive results. Additionally, we present two post-processing steps to enhance the quality and visual appearance of the resulting paths. The first method minimizes the number of waypoints in a path while the second method takes the slope of the terrain into account in order to generate visually more pleasing paths. We show that our algorithm is fast and, therefore, well suited for realtime applications, such as games or virtual environments	a* search algorithm;automated planning and scheduling;autonomous robot;computation;directed graph;entity;graph (discrete mathematics);greedy algorithm;lazy evaluation;motion planning;real-time clock;real-time computing;simulation;undecidable problem;video post-processing;virtual reality;waypoint	Christoph Niederberger;Dejan Radovic;Markus H. Gross	2004	Proceedings Computer Graphics International, 2004.	10.1109/CGI.2004.1309225	mathematical optimization;suurballe's algorithm;fast path;any-angle path planning;computer science;artificial intelligence;theoretical computer science;machine learning;motion planning;data visualization	Graphics	52.339921598762416	-23.12101175413431	53980
5e3add0f95b9284feae14d36de3f72924f8f21ed	adaptive color-space transform in hevc screen content coding	color;video coding;high efficiency video coding;redundancy;image color analysis;transforms;algorithm design and analysis	The screen content coding (SCC) Extensions of high efficiency video coding (HEVC) employs in-loop adaptive color-space transform (ACT) technique to explore the inter-color-component redundancy, i.e., statistical redundancy among different color components. In ACT, the prediction residual signal is adaptively converted into a different color space, i.e., YCgCo. The rate-distortion criteria is employed to decide whether to code the residual signal in the original color space or YCgCo color space. Typically, the inter-color-component correlation could be reduced when ACT is enabled. The residual signal after possible color-space conversion is then coded, following the existing HEVC framework, i.e., transform if necessary, quantization and entropy coded. This paper describes the design of ACT from several points of view, from theoretical analysis to implementation details. Experimental results are also provided to demonstrate the significant coding gains of ACT in the HEVC SCC Extensions.	color space;data compression;distortion;entropy encoding;high efficiency video coding;konami scc;redundancy (information theory)	Xiang Lin;Xiaoyu Xiu;Jianle Chen;Marta Karczewicz;Yunwen He;Yan Ye;Ji-Zheng Xu;Joel Solé;Woo-Shik Kim	2016	IEEE Journal on Emerging and Selected Topics in Circuits and Systems	10.1109/JETCAS.2016.2599860	sub-band coding;algorithm design;computer vision;speech recognition;color depth;computer science;coding tree unit;redundancy;h.261;computer graphics (images)	ML	43.63131614618408	-15.349217840696001	54007
13dd9fa83bf082c4a3cd2ba484743321b71ff1e2	a unified authentication framework for jpeg2000	robustness;digital signatures;authentication;transcoding;sun;public key;scalability;image resolution;data compression	This paper proposes a unified authentication framework for JPEG2000 images, which consists of fragile, lossy and lossless authentication for different applications. The authentication strength can be specified using only one parameter called lowest authentication bit-rate (LABR), bringing much convenience to users. The lossy and lossless authentication could survive various incidental distortions while being able to allocate malicious attacks. In addition, with lossless authentication, the original image can be recovered after verification if no incidental distortion is introduced	distortion;jpeg 2000;lossless compression;lossy compression;message authentication code;software brittleness	Zhishou Zhang;Gang Qiu;Qibin Sun;Xiao Lin;Zhicheng Ni;Yun Q. Shi	2004	2004 IEEE International Conference on Multimedia and Expo (ICME) (IEEE Cat. No.04TH8763)		data compression;data authentication algorithm;digital signature;challenge–response authentication;computer science;authentication protocol;lightweight extensible authentication protocol;internet privacy;computer security;statistics;computer network	Visualization	39.02589225149375	-11.863704863726626	54049
