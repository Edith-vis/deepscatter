id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
198c38b7ea5bd7110fbd1ae7faeb8e916b134d09	heart sound acquisition based on pda and bluetooth	instruments;heart;medical administrative data processing;personal digital assistant;heart sound;medical signal detection;cardiology;real time;labview heart sound pda bluetooth auscultation;low pass filter;data acquisition system;data communication;signal playback heart sound acquisition pda bluetooth personal digital assistant data acquisition terminal equipment dspic chip heart sound transmission heart sound auscultation data communication virtual serial port heart sound data heart sound waveform display heart sound signal preservation;pda;personal digital assistants;chip;information storage;notebook computers biomedical communication bluetooth cardiology data acquisition data communication information retrieval systems information storage medical administrative data processing medical signal detection;auscultation;information retrieval systems;notebook computers;low pass filters;bluetooth;heart bluetooth personal digital assistants data acquisition low pass filters instruments;data acquisition;labview;biomedical communication	This article introduces a heart sound data acquisition system based on PDA and Bluetooth. The system mainly includes PDA(Personal Digital Assistant) and the data acquisition terminal equipment. The data acquisition terminal equipment is controlled by dsPIC chip and has the following functions: heart sound acquisition, heart sound transmission and heart sound auscultation. The data communication between PDA and data acquisition terminal equipment is achieved via Bluetooth virtual serial port. The PDA can receive heart sound data, display heart sound waveform real-time, preserve heart sound signal and playback heart sound signal. The software and hardware are also given in detail.	bluetooth;com port redirector;data acquisition;pic microcontroller;personal digital assistant;real-time locating system;serial port;waveform	Tian Xian-ting;Zhao Zhi-dong	2011	2011 4th International Conference on Biomedical Engineering and Informatics (BMEI)	10.1109/BMEI.2011.6098495	embedded system;speech recognition;low-pass filter;computer hardware;computer science;data acquisition	Visualization	43.915415620761685	-29.908931654498218	13109
2454fab96b25df8372f2487f6309797e91bbbfcb	monte carlo train derailment model for risk assessment	axiomatic safety critical assessment process;ascap;train derailment;wheel lateral force ratio;wheel vertical force ratio;derailment probability;probability of derailment;risk assessment;genetic algorithm;genetic algorithms;monte carlo;derailment coefficient;monte carlo simulation;railroad risk assessment	A Monte Carlo train over-speed derailment model for risk assessment is developed based on the derailment coefficient (wheel lateral and vertical force ratio). The model considers a one lump train negotiating on curved tracks or tangent tracks. The derailment coefficients for these two situations are calculated and the probabilities of train derailment are obtained to enhance a large scale Monte Carlo railroad risk assessment simulator called the axiomatic safety critical assessment process (ASCAP). A genetic algorithm based approach is taken to performing the validation of this Monte Carlo train derailment model. From the perspective of railroad risk assessment, the model is analysed and compared with an empirical formula, which is currently used in ASCAP. The Monte Carlo train over-speed derailment model will replace the current empirical formula in ASCAP simulator and will be able to provide a more realistic determination of the probability of train derailment.	coefficient;genetic algorithm;lateral thinking;lumped element model;monte carlo method;risk assessment;simulation	Weidong Ruan;Zongli Lin;Ted C. Giras	2008	IJMIC	10.1504/IJMIC.2008.021091	simulation;genetic algorithm;computer science;engineering;automotive engineering;mathematics;forensic engineering;statistics;monte carlo method	AI	44.90699434693006	-25.776462889759976	13149
719196f29621377137f7c79cf6473d397849c792	tracking concept drifting with an online-optimized incremental learning framework	concept drift;long period;video streaming;online optimization;video retrieval;time series;trec video retrieval evaluation;concept drifting;learning system;gaussian mixture model;incremental learning;machine learning;video content analysis;model updating	Concept drifting is an important and challenging research issue in the field of machine learning. This paper mainly addresses the issue of semantic concept drifting in time series such as video streams over a relatively long period of time. An Online-Optimized Incremental Learning framework is proposed as an example learning system for tracking the drifting concepts. Furthermore, a set of measures are defined to track the process of concept drifting in the learning system. These tracking measures are also applied to determine the corresponding parameters used for model updating in order to obtain the optimal up-to-date classifiers. Experiments on the data set of TREC Video Retrieval Evaluation 2004 not only demonstrate the inside concept drifting process of the learning system, but also prove that the proposed learning framework is promising for tackling the issue of concept drifting.	algorithm;boosting (machine learning);discriminative model;generative model;machine learning;streaming media;time series	Jun Wu;Dayong Ding;Xian-Sheng Hua;Bo Zhang	2005		10.1145/1101826.1101834	computer science;artificial intelligence;concept drift;machine learning;time series;mixture model;data mining	AI	32.00395774655948	-46.826468124978916	13170
0ff2288d1fba242c0302704da580c26add326b0f	three nested kalman filters-based algorithm for real-time estimation of optical flow, uav motion and obstacles detection	monocular vision;3 nested kalman filters;obstacle detection;image motion analysis;vision based autopilot small flying robots optical flow computation structure from motion;motion control;robust estimator;optical filters;real time;scene structure;kalman filters;aircraft motion;robot vision aerospace control aerospace robotics collision avoidance mobile robots motion control remotely operated vehicles;kalman filter;motion estimation;mobile robots;remotely operated vehicles;layout;optical filters kalman filters motion estimation image motion analysis unmanned aerial vehicles motion detection remotely operated vehicles mobile robots aircraft layout;aerospace control;robot vision;realtime estimation;vision based autopilot;3 nested kalman filters realtime estimation optical flow uav motion obstacle detection vision based autopilot autonomous small aerial vehicle aircraft motion scene structure range map monocular vision;aerospace robotics;uav motion;range map;optical flow;collision avoidance;small flying robots;autonomous small aerial vehicle;unmanned aerial vehicles;structure from motion;motion detection;optical flow computation;aircraft	We aim at developing a vision-based autopilot for autonomous small aerial vehicle applications. This paper presents a new approach for the estimation of optical flow, aircraft motion and scene structure (range map), using monocular vision and inertial data. The proposed algorithm is based on 3 nested Kalman filters (3NKF) and results in an efficient and robust estimation process. The 3NKF-based algorithm was tested extensively in simulation using synthetic images, and in real-time experiments.	aerial photography;algorithm;autonomous robot;autopilot;experiment;kalman filter;optical flow;real-time clock;simulation;synthetic intelligence;unmanned aerial vehicle	Farid Kendoul;Isabelle Fantoni;Gérald Dherbomez	2007	Proceedings 2007 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2007.364210	kalman filter;control engineering;computer vision;simulation;computer science;engineering	Robotics	53.04445802146986	-38.20784829703664	13181
52475db551dfb5537a2df7d3baa02d7e7caa1f24	learning to navigate through crowded environments	robot navigation mobile robots human like motion behavior inverse reinforcement learning realistic crowd flow simulator;robot sensing systems;shortest path;inverse reinforcement learning;path planning gaussian processes learning artificial intelligence mobile robots;learning;gaussian processes;human like motion behavior;mobile robot;path planning;training;robot navigation;mobile robots;navigation learning cost function mobile robots airports robot sensing systems gaussian processes robotics and automation usa councils computer science;navigation;feature extraction;partial observation;flow simulation;learning artificial intelligence;realistic crowd flow simulator	The goal of this research is to enable mobile robots to navigate through crowded environments such as indoor shopping malls, airports, or downtown side walks. The key research question addressed in this paper is how to learn planners that generate human-like motion behavior. Our approach uses inverse reinforcement learning (IRL) to learn human-like navigation behavior based on example paths. Since robots have only limited sensing, we extend existing IRL methods to the case of partially observable environments. We demonstrate the capabilities of our approach using a realistic crowd flow simulator in which we modeled multiple scenarios in crowded environments. We show that our planner learned to guide the robot along the flow of people when the environment is crowded, and along the shortest path if no people are around.	crowdsourcing;gaussian process;ibm notes;mobile robot;motion planning;partially observable system;process modeling;reinforcement learning;shortest path problem;simulation	Peter Henry;Christian Vollmer;Brian Ferris;Dieter Fox	2010	2010 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2010.5509772	mobile robot;computer vision;simulation;computer science;engineering;artificial intelligence	Robotics	50.739454875118454	-27.807199968873398	13200
486a9fa0a608d1af768704949e8c2a9b2230c71e	a computer vision based whiteboard capture system	autonomous whiteboard scan;video signal processing;pan tilt zoom;static camera;capture prototype system;whiteboard capture system;computer vision;video cameras;camera calibration method computer vision whiteboard capture system whiteboard video capture static camera autonomous whiteboard scan capture prototype system pan tilt zoom cameras static cameras;camera calibration method;pan tilt zoom cameras;image processing methods;computer vision cameras writing image edge detection image processing prototypes calibration hardware robustness education;static cameras;video signal processing computer vision video cameras;camera calibration;whiteboard video capture	Conventional whiteboard video capture using a static camera usually results in a poor quality. In this paper, we present an autonomous whiteboard scan and capture prototype system, which consist a pair of static and pan-tilt-zoom (PTZ) cameras. The PTZ camera is used to scan the newly-updated whiteboard regions without interrupting the instructor. We will illustrate several computer vision techniques used in our system: Firstly, we present our unique camera calibration method using rough hand-drawn gridlines. Secondly, we present the image processing methods used to determine where the newly updated whiteboard region to be scanned is. Our method also accounts for the whiteboard region occlusion from the instructor.	autonomous robot;camera resectioning;computer vision;image processing;interrupt;pan–tilt–zoom camera;prototype	Richard Y. D. Xu	2008	2008 IEEE Workshop on Applications of Computer Vision	10.1109/WACV.2008.4544028	computer vision;camera resectioning;computer science;multimedia;computer graphics (images)	Vision	46.20713871911124	-44.839387749516064	13203
193fe76f49fa39cf593f3f258a6cdf16a2b824a2	let's take a walk on superpixels graphs: deformable linear objects segmentation and model estimation		While robotic manipulation of rigid objects is quite straightforward, coping with deformable objects is an open issue. More specifically, tasks like tying a knot, wiring a connector or even surgical suturing deal with the domain of Deformable Linear Objects (DLOs). In particular the detection of a DLO is a non-trivial problem especially under clutter and occlusions (as well as self-occlusions). The pose estimation of a DLO results into the identification of its parameters related to a designed model, e.g. a basis spline. It follows that the stand-alone segmentation of a DLO might not be sufficient to conduct a full manipulation task. This is why we propose a novel framework able to perform both a semantic segmentation and b-spline modeling of multiple deformable linear objects simultaneously without strict requirements about environment (i.e. the background). The core algorithm is based on biased random walks over the Region Adiacency Graph built on a superpixel oversegmentation of the source image. The algorithm is initialized by a Convolutional Neural Networks that detects the DLO’s endcaps. An open source implementation of the proposed approach is also provided to easy the reproduction ar X iv :1 81 0. 04 46 1v 1 [ cs .C V ] 1 0 O ct 2 01 8 2 De Gregorio et al. of the whole detection pipeline along with a novel cables dataset in order to encourage further experiments.	algorithm;b-spline;clutter;convolutional neural network;discontinuity layout optimization;experiment;open-source software;requirement;robot;wiring	Daniele De Gregorio;Gianluca Palli;Luigi di Stefano	2018	CoRR		pattern recognition;random walk;convolutional neural network;artificial intelligence;computer science;pose;segmentation;spline (mathematics);graph	Vision	47.08904895135658	-51.972258168516696	13237
f2e38e619cf4d0f626059dc12b6201d6fad04e7c	a graph theoretic approach to simulation and classification	graph theory;optical character recognition;simulation;spatial correlation;random field	A new class of discrete random fields designed for quick simulation and covariance inference under inhomogenous conditions is introduced and studied. Simulation of these correlated fields can be done in a single pass instead of relying on multi-pass convergent methods like the Gibbs Sampler or other Markov Chain Monte Carlo algorithms. The fields are constructed directly from an undirected graph with specified marginal probability mass functions and covariances between nearby vertices in a manner that makes simulation quite feasible yet maintains the desired properties. Special cases of these correlated fields have been deployed successfully in data authentication, object detection and CAPTCHA generation. Further applications in maximum likelihood estimation and classification such as optical character recognition are now given within.	algorithm;captcha;circuit restoration;gibbs sampling;graph (discrete mathematics);graph theory;image restoration;marginal model;markov chain monte carlo;message authentication;monte carlo method;object detection;optical character recognition;sampling (signal processing);simulation;speech recognition;statistical classification	Michael A. Kouritzin;Fraser Newton;Biao Wu	2014	Computational Statistics & Data Analysis	10.1016/j.csda.2013.09.026	combinatorics;random field;spatial correlation;graph theory;machine learning;mathematics;optical character recognition;statistics	ML	45.84147670757444	-50.34833436858061	13295
3312eb79e025b885afe986be8189446ba356a507	moon: a mixed objective optimization network for the recognition of facial attributes		Overview Facial attributes have implicit latent correlations – which are naturally learnt through a multi-label approach. Due to demographic correlations, it is difficult to obtain a balanced multi-label facial attribute training set; thus a multi-label classifier trained for one demographic might perform poorly on another due to training set biases. CelebA, for example, contains images of celebrities where Young is over-represented and anti-correlated attributes like Bald and Gray Hair are under-represented. Label Bias of the CelebA Dataset	moon;multi-label classification;test set	Ethan M. Rudd;Manuel Günther;Terrance E. Boult	2016		10.1007/978-3-319-46454-1_2	computer science;artificial intelligence;machine learning;pattern recognition	Vision	24.667883508184346	-46.089044176921746	13364
3de3ad0f6ae59ff579dc32856740f9f9b801eadd	methodology for improving detection speed of pedestrians in autonomous vehicle by image class classification		We propose a pedestrian detection method to minimize the amount of computation for classifying and candidate region detection in autonomous vehicles. The minimization of the computational complexity is a crucial factor for commercial products with a limited computational power. In conventional pedestrian detection methods, the number of candidate regions is 300 to 2,000 even if there is no pedestrian in an image. Therefore, the unnecessary computation is significant to classify each falsely decided candidate region. Moreover, it leads to false detection. In this paper, we propose a new methodology for solving this problem, and show through experiments that the processing speed can be improved by the proposed methodology.	autonomous robot;computation;computational complexity theory;experiment;pedestrian detection	Junkwang Kim;Woo Young Jung;Heechul Jung;Dong Seog Han	2018	2018 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2018.8326252	artificial intelligence;computer vision;object detection;feature extraction;computation;minification;computational complexity theory;computer science;pedestrian detection	Robotics	41.066781229515115	-43.615401460998044	13365
eebc4a70509aca8491c2302c8a4241fd82c9007f	face-tld: tracking-learning-detection applied to faces	verification;detectors;tracking learning detection;real time face tracking resistent;přispěvek z konference elektronický;unlabeled video;occlusion;learning;video signal processing;generic detector;online trained validator;offline trained detector;real time;offline trained detector face tld tracking learning detection long term tracking human face unconstrained video generic detector real time face tracking resistent occlusion online trained validator multiview model unlabeled video;video signal processing face recognition object tracking;detection;face tracking;long term face tracking;visualization;long term tracking;human face;face recognition;trajectory;unconstrained video;object tracking;face tld;detectors videos trajectory real time systems target tracking visualization;real time long term face tracking learning detection verification;target tracking;quantitative evaluation;multiview model;videos;real time systems	A novel system for long-term tracking of a human face in unconstrained videos is built on Tracking-Learning-Detection (TLD) approach. The system extends TLD with the concept of a generic detector and a validator which is designed for real-time face tracking resistent to occlusions and appearance changes. The off-line trained detector localizes frontal faces and the online trained validator decides which faces correspond to the tracked subject. Several strategies for building the validator during tracking are quantitatively evaluated. The system is validated on a sitcom episode (23 min.) and a surveillance (8 min.) video. In both cases the system detects-tracks the face and automatically learns a multi-view model from a single frontal example and an unlabeled video.	maxima and minima;online and offline;real-time clock;validator;view model	Zdenek Kalal;Krystian Mikolajczyk;Jiri Matas	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5653525	facial recognition system;computer vision;detector;facial motion capture;verification;speech recognition;visualization;computer science;trajectory;video tracking;multimedia	Vision	41.57473732310434	-47.90023242282015	13393
58f72662ae80fb11dc65325267fef1878bf7aebc	detection and tracking of boundary of unmarked roads	roads image color analysis vehicles shape kalman filters noise measurement computational modeling;traffic engineering computing bayes methods edge detection feature extraction object detection object tracking roads;bayes filter unmarked road boundary tracking unmarked road boundary detection drivable region boundaries road markings residential places public roads self driving cars image region ego vehicle image acquisition condition image features	This paper presents a new method of detecting and tracking the boundaries of drivable regions in road without road-markings. As unmarked roads connect residential places to public roads, the capability of autonomously driving on such a roadway is important to truly realize self-driving cars in daily driving scenarios. To detect the left and right boundaries of drivable regions, our method first examines the image region at the front of ego-vehicle and then uses the appearance information of that region to identify the boundary of the drivable region from input images. Due to variation in the image acquisition condition, the image features necessary for boundary detection may not be present. When this happens, a boundary detection algorithm working frame-by-frame basis would fail to successfully detect the boundaries. To effectively handle these cases, our method tracks, using a Bayes filter, the detected boundaries over frames. Experiments using real-world videos show promising results.	algorithm;autonomous car;dvd region code;sensor	Young-Woo Seo;Ragunathan Rajkumar	2014	17th International Conference on Information Fusion (FUSION)		computer vision;simulation;geography;cartography	Robotics	42.14824212468397	-45.36176652845197	13396
059c4e4bfbea14df332b5764ac9b04cb265f207a	facial action transfer with personalized bilinear regression	bilinear regression;facial action transfer	Facial Action Transfer (FAT) has recently attracted much attention in computer vision due to its diverse applications in the movie industry, computer games, and privacy protection. The goal of FAT is to “clone” the facial actions from the videos of one person (source) to another person (target). In this paper, we will assume that we have a video of the source person but only one frontal image of the target person. Most successful methods for FAT require a training set with annotated correspondence between expressions of different subjects, sometimes including many images of the target subject. However, labeling expressions is time consuming and error prone (i.e., it is difficult to capture the same intensity of the expression across people). Moreover, in many applications it is not realistic to have many labeled images of the target. This paper proposes a method to learn a personalized facial model, that can produce photo-realistic person-specific facial actions (e.g., synthesize wrinkles for smiling), from only a neutral image of the target person. More importantly, our learning method does not need an explicit correspondence of expressions across subjects. Experiments on the Cohn-Kanade and the RU-FACS databases show the effectiveness of our approach to generate video-realistic images of the target person driven by spontaneous facial actions of the source. Moreover, we illustrate applications of FAT to face de-identification.	algorithm;bcs-facs;bilinear filtering;bilinear transform;cognitive dimensions of notations;computer vision;de-identification;face detection;pc game;personalization;spontaneous order;test set;texture synthesis	Dong Huang;Fernando De la Torre	2012		10.1007/978-3-642-33709-3_11	computer vision;computer science;multimedia	Vision	31.935702867396348	-47.720054749220566	13415
68931e27fb41232ac2a2cc14f1cf892580530b5d	multi-view semantic temporal video segmentation	silicon;temporal video segmentation;training;semantics;video sequences;indexes;trajectory;action recognition;impart multi view action data set;cameras	In this work, we propose a multi-view temporal video segmentation approach that employs a Gaussian scoring process for determining the best segmentation positions. By exploiting the semantic action information that the dense trajectories video description offers, this method can detect intra-shot actions as well, unlike shot boundary detection approaches. We compare the temporal segmentation results of the proposed method to both single-view and multi-view methods, and also compare the action recognition results obtained on ground truth video segments to the ones obtained on the proposed multi-view segments, on the IMPART multi-view action data set.	audio description;free viewpoint television;ground truth;shot transition detection	Thomas Theodoridis;Anastasios Tefas;Ioannis Pitas	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7533100	database index;computer vision;computer science;trajectory;machine learning;video tracking;pattern recognition;semantics;multimedia;silicon;scale-space segmentation	Vision	37.89548949148207	-50.03402155605659	13416
1c530de1a94ac70bf9086e39af1712ea8d2d2781	sparsity conditional energy label distribution learning for age estimation		By observing that the faces at close ages are similar, some Label Distribution Learning (LDL) methods have been proposed to solve age estimation tasks that they treat age distributions as the training targets. However, these existent LDL methods are limited because they can hardly extract enough useful information from complex image features. In this paper, Sparsity Conditional Energy Label Distribution Learning (SCE-LDL) is proposed to solve this problem. In the proposed SCE-LDL, age distributions are used as the training targets and energy function is utilized to define the age distribution. By assigning a suitable energy function, SCELDL can learn distributed representations, which provides the model with strong expressiveness for capturing enough of the complexity of interest from image features. The sparsity constraints are also incorporated to ameliorate the model. Experiment results in two age datasets show remarkable advantages of the proposed SCE-LDL model over the previous proposed age estimation methods.	algorithm;baseline (configuration management);gaussian blur;internet information services;mathematical optimization;performance;scalable cluster environment;sparse matrix	Xu Yang;Xin Geng;Deyu Zhou	2016			feature (computer vision);machine learning;artificial intelligence;expressivity;computer science	AI	26.47788671669802	-46.44028579363535	13463
8a5c9df5a640946bdd57170eb996a19f01dc7eb7	direct sensorimotor control for low-cost mobile tracking applications	direction control;object position encoding;microcontrollers;control systems;motor drives;velocity control;custom analog vlsi architecture;hysteresis;biologically inspired system;1 5 ms direct sensorimotor control low cost mobile tracking applications mobile vehicle custom analog vlsi architecture object position encoding hysteresis sensing environment motor control signals motor drive signals pulse width modulation speed control direction control;very large scale integration;mobile robots;layout;pulse width modulated;motor drive signals;mobile vehicle;chip;machine control;machine control tracking mobile robots vlsi encoding position control velocity control robot vision motor drives;low cost mobile tracking applications;vehicles hysteresis motors very large scale integration motor drives pulse width modulation control systems biological information theory layout microcontrollers costs;low power;robot vision;position control;field of view;vlsi;analog vlsi;biological information theory;motor control signals;sensorimotor control;1 5 ms;direct sensorimotor control;vehicles;encoding;speed control;neural oscillator;hysteresis motors;pulse width modulation;sensing environment;tracking;motor control	A biologically inspired system for tracking objects in a visual scene is presented. The uniqueness of the system is in the absence of a microcontroller to convert sensory information to tracking decisions, reducing power, size, weight, and cost of the overall system. The system consists of a mobile vehicle outfitted with a custom analog VLSI architecture for encoding the position of an object of interest in the vehicles's field of view. Once determined, the object of interest retains hysteresis proportional to its size and intensity to limit the potential for distraction by other objects in the sensing environment. The encoded position of the object of interest is directly converted to a series of motor control signals to drive the vehicle in the direction of the object. The motor drive signals are pulse width modulated to control the speed and direction of travel induced by two de motors via a conventional differential steering arrangement. Neural oscillators are used to drive the de motors to provide a compact single-chip system for tracking bright objects. The nature of the system is sufficiently modular so that it can be adapted relatively easily to tracking other features of visual objects and even to objects representative of other sensing modalities. The system described here is one of the first efforts to fully integrate and apply analog VLSI (aVLSI) sensorimotor control to a mobile vehicle and to analyze the complete system from a control systems' perspective. The system described here has the advantages of aVLSI integration in its small size (0.011-mm/sup 2/ elements), low power (0.3 /spl mu/W per element), and fast system response time (1.5 ms from sensory input to motor response).		Denise Michelle Wilson;Eric Dennis Blom;Michael A. Marra;Bruce L. Walcott	2000	IEEE Trans. Industrial Electronics	10.1109/41.857975	control engineering;electronic engineering;computer science;engineering;control system;electrical engineering;control theory;very-large-scale integration	Robotics	46.13340215148935	-33.38630454873538	13467
3a3f137cc71f95a11bb71d65e7c6817995b587e1	robust regression in rkhs — an overview	kernel;regression analysis belief networks compressed sensing hilbert spaces;bayes methods;training;robustness kernel estimation europe signal processing bayes methods training;estimation;signal processing;robust non linear regression robust regression in rkhs learning with kernels kernel greedy algorithm for robust denoising kgard;robustness;europe;bayesian approach robust nonlinear regression rkhs reproducing kernel hilbert spaces sparse vector noise component sparsity aware compressed sensing theory greedy approach convex relaxation sparsity promoting task least squares cost	The paper deals with the task of robust nonlinear regression in the presence of outliers. The problem is dealt in the context of reproducing kernel Hilbert spaces (RKHS). In contrast to more classical approaches, a recent trend is to model the outliers as a sparse vector noise component and mobilize tools from the sparsity-aware/compressed sensing theory to impose sparsity on it. In this paper, three of the most popular approaches are considered and compared. These represent three major directions in sparsity-aware learning context; that is, a) a greedy approach b) a convex relaxation of the sparsity-promoting task via the l\ norm-based regularization of the least-squares cost and c) a Bayesian approach making use of appropriate priors, associated with the involved parameters.	approximation error;compressed sensing;emoticon;greedy algorithm;hilbert space;least squares;linear programming relaxation;nonlinear system;sparse approximation;sparse matrix	George Papageorgiou;Pantelis Bouboulis;Sergios Theodoridis	2015	2015 23rd European Signal Processing Conference (EUSIPCO)	10.1109/EUSIPCO.2015.7362910	principal component regression;mathematical optimization;kernel principal component analysis;machine learning;pattern recognition;mathematics	ML	27.504963929334203	-34.39687294923836	13578
94ead3662a08a3118b2958a5b4a168a4d034df53	path patterns: analyzing and comparing real and simulated crowds	crowd simulation;stochastic optimization;crowd comparison;clustering;hierarchical dirichlet process;data driven	Crowd simulation has been an active and important area of research in the field of interactive 3D graphics for several decades. However, only recently has there been an increased focus on evaluating the fidelity of the results with respect to real-world situations. The focus to date has been on analyzing the properties of low-level features such as pedestrian trajectories, or global features such as crowd densities. We propose a new approach based on finding latent Path Patterns in both real and simulated data in order to analyze and compare them. Unsupervised clustering by non-parametric Bayesian inference is used to learn the patterns, which themselves provide a rich visualization of the crowd's behaviour. To this end, we present a new Stochastic Variational Dual Hierarchical Dirichlet Process (SV-DHDP) model. The fidelity of the patterns is then computed with respect to a reference, thus allowing the outputs of different algorithms to be compared with each other and/or with real data accordingly.	3d computer graphics;algorithm;cluster analysis;crowd simulation;high- and low-level;systemverilog;variational principle	He Wang;Jan Ondrej;Carol O'Sullivan	2016		10.1145/2856400.2856410	simulation;computer science;stochastic optimization;machine learning;data mining;crowd simulation;cluster analysis;hierarchical dirichlet process	ML	32.64774086791795	-47.867928259847545	13605
de84f25d52cf04cab94eb43b4a808bd2839cc350	recognition of people's positioning by cooperative mobile robots for human groups steering	feasibility people positioning recognition human groups steering multiple cooperative mobile robots systems multiple human localization multiple homogeneous mobile robots trinocular stereo vision spatial noise data filtering multiple sensor data fusion stereo vision 2d point transformation clustering based segmentation;object recognition;pattern clustering;image segmentation mobile robots pattern clustering robot vision object recognition stereo image processing sensor fusion multi robot systems cooperative systems position control;image segmentation;legged locomotion;mobile robot;mobile robots;data filtering;computer vision;feasibility;multi sensor data fusion;2d point transformation;navigation;clustering based segmentation;multiple human localization;robot vision;cooperative systems;position control;mobile robots humans stereo vision legged locomotion robot kinematics navigation image segmentation laboratories tracking computer vision;trinocular stereo vision;stereo image processing;stereo vision;multi robot systems;people positioning recognition;human groups steering;humans;sensor fusion;multiple sensor data fusion;multiple cooperative mobile robots systems;tracking;multiple homogeneous mobile robots;robot kinematics;spatial noise data filtering	In this research we attempt to build a multi cooperative mobile robot system able to perform the process of steering people in indoors. When a target-group is integrated by several persons, steering task is more difficult to accomplish, specially when only a robot is intended to be used. Since that, the problem can be overcomed if we consider that a cooperative mobile robot system’s performance is higher for steering multiple humans than only one, and will greatly improve the performance of the tasks. In this paper, we detail a novel method for multiple human localization which has been developed by using multi homogeneous mobile robots, equipped with trinocular stereo vision. As a first step of this research the method includes spatial noise data filtering, multi sensor data fusion and clustering based segmentation. In addition, some experimental results are shown to verify the feasibility of the method.	cluster analysis;human–computer interaction;mobile robot;stationary process;stereopsis	Edgar Martinez;Akihisa Ohya;Shin'ichi Yuta	2003		10.1109/CIRA.2003.1222276	mobile robot;feasibility study;computer vision;simulation;computer science;artificial intelligence	Robotics	50.55369915427353	-38.36451285040065	13626
22e0f460638d45b99033e23ce70ca75299fc9e28	optical flow based urban road vehicle tracking	image sampling;video surveillance;vehicles image motion analysis computer vision target tracking optical imaging mathematical model;image resolution;nonparametric statistics;intelligent transportation systems;feature template;feature template vehicle tracking optical flow markov chain monte carlo;feature weight urban road vehicle tracking intelligent transportation surveillance scale change interference image color low resolution video data markov chain monte carlo optical flow mcmc sampling tracking algorithm of mcmc sampling tracking algorithm optical flow method vehicle moving direction vehicle moving speed second order autoregressive motion model nonparameter characteristic feature template;markov chain monte carlo;image colour analysis;feature extraction;object tracking;optical flow;vehicle tracking;markov processes;video surveillance feature extraction image colour analysis image resolution image sampling image sequences intelligent transportation systems markov processes monte carlo methods nonparametric statistics object tracking road vehicles;monte carlo methods;road vehicles;image sequences	Vehicle tracking is an important part in intelligent transportation surveillance. But now vehicle tracking faces with the problems such as scale change, the interference of similar color, low resolution video data and so on. In this paper an improved Markov chain Monte Carlo(MCMC) named optical flow MCMC(OF-MCMC) sampling tracking algorithm is proposed for vehicle tracking. First, we use the optical flow method to get the moving direction of the vehicle in initial frames, which can solve the problem of scale change, what's more the optical flow method can get the moving speed of the vehicle which replaces the second-order autoregressive motion model owing to the non-parameter characteristic. Second, when calculating whether one particle is accepted or not, a distance factor is considered, which can relieve the interference of similar vehicle nearby. Finally, to deal with vehicle tracking in low resolution of the video data, we generate a more accurate feature template with different features weighted to get better tracking results. Experimental results show that the proposed tracking algorithm has better performance than some traditional ones.	algorithm;autoregressive model;color;hidden surface determination;image resolution;interference (communication);markov chain monte carlo;optical flow;sampling (signal processing);vehicle tracking system	Ya Liu;Yao Lu;Qingxuan Shi;Jianhua Ding	2013	2013 Ninth International Conference on Computational Intelligence and Security	10.1109/CIS.2013.89	nonparametric statistics;computer vision;intelligent transportation system;simulation;speech recognition;image resolution;markov chain monte carlo;feature extraction;computer science;video tracking;optical flow;markov process;statistics;monte carlo method	Robotics	43.6444731905807	-46.0113274932819	13651
529b147e0a706768ebf1994c99f35163ba887411	a rate control algorithm for video coding in augmented reality applications	histograms;image coding;measurement;image edge detection;feature extraction;encoding;object detection	Most of latest-generation multimedia systems are equipped with increasingly-effective object detection algorithms (e.g., intelligent video surveillance systems, augmented reality applications, sharing platforms for multimedia data, etc.). Unfortunately, image and video compression makes object detection more difficult since such operations erase most of the computed visual features. In this paper we propose a video rate control strategy that exploits a saliency metric to identify image regions where features are likely to be found. According to the pixel statistics, the approach decides whether to increase the coding quality or include some side information that specifies the keypoint locations. Experimental results on HEVC coder show that the proposed rate control algorithm improves both the detection accuracy and the rate-distortion performance with respect to state-of-the-art strategies.	algorithm;augmented reality;closed-circuit television;control theory;cross-validation (statistics);data compression;distortion;high efficiency video coding;mathematical optimization;object detection;pixel;programming paradigm	Simone Milani;Gianluca Agresti;Giancarlo Calvagno	2016	2016 Picture Coding Symposium (PCS)	10.1109/PCS.2016.7906323	computer vision;feature detection;simulation;object-class detection;feature extraction;computer science;video tracking;histogram;mathematics;multimedia;encoding;measurement;statistics	Vision	37.60959215173997	-51.840139883725925	13658
72a7ba6695824d37a648f8735c16cb7cbb065fb4	self-explanatory convex sparse representation for image classification	image classification;会议论文;image classification convex hull sparse representation coordinate descent;computer vision;vectors;vectors computer vision image classification image representation learning artificial intelligence;image representation;dictionaries minimization educational institutions sparse matrices encoding linear programming kernel;lagrange multiplier self explanatory convex sparse representation image classification computer vision k means algorithm variation scheme scsr basis vector dictionary learning subproblem block wise coordinate descent method;learning artificial intelligence	Sparse representation technique has been widely used in various areas of computer vision over the last decades. Unfortunately, in the current formulations, there are no explicit relationship between the learned dictionary and the original data. By tracing back and connecting sparse representation with the K-means algorithm, a novel variation scheme termed as self-explanatory convex sparse representation (SCSR) has been proposed in this paper. To be specific, the basis vectors of the dictionary are refined as convex combination of the data points. The atoms now would capture a notion of centroids similar to K-means, leading to enhanced interpretability. Sparse representation and K-means are thus unified under the same framework in this sense. Besides, an appealing property also emerges that the weight and code matrices both tend to be naturally sparse without additional constraints. Compared with the standard formulations, SCSR is easier to be extended into the kernel space. To solve the corresponding sparse coding sub problem and dictionary learning sub problem, block-wise coordinate descent and Lagrange multipliers are proposed accordingly. To validate the proposed algorithm, it is implemented in image classification, a successful applications of sparse representation. Experimental results on several benchmark data sets, such as UIUC-Sports, Scene 15, and Caltech-256 demonstrate the effectiveness of our proposed algorithm.	algorithm;basis (linear algebra);benchmark (computing);computer vision;coordinate descent;data point;dictionary;emergence;experiment;k-means clustering;lagrange multiplier;machine learning;mathematical optimization;neural coding;optimization problem;sparse approximation;sparse matrix;user space	Bao-Di Liu;Yu-Xiong Wang;Bin Shen;Yu-Jin Zhang;Yanjiang Wang;Weifeng Liu	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.363	computer vision;mathematical optimization;contextual image classification;k-svd;computer science;artificial intelligence;machine learning;pattern recognition;sparse approximation;mathematics	Vision	25.935020657246984	-41.20942451209848	13829
37184822c412f084dff3d325845e1a89ac8269fc	environment perception using dynamic polylines and particle based occupancy grids	microprocessors;particle filtering environment representation stereovision occupancy grid polygonal model;polygonal model;atmospheric measurements;particle measurements;real time;occupancy grid;individual object;stereovision;computer architecture;estimation;particle filter;image representation;feature extraction;object tracking;stereo image processing;particle filtering;traffic engineering computing;vehicles;estimation vehicle dynamics computer architecture particle measurements atmospheric measurements microprocessors vehicles;border scanner algorithm particle based occupancy grid dynamic entity tracking dynamic entity modeling driving environment measurement sensors dynamic polyline obstacle representation environment perception rigid model based cuboid representation model free occupancy grid representation polyline based obstacle extraction system stereovision data processing real time flexible method occupancy grid processing;traffic engineering computing feature extraction image representation object detection object tracking stereo image processing;vehicle dynamics;object detection;environment representation;dynamic properties	Modeling and tracking dynamic entities in the driving environment is a complex task, as one has to accommodate multiple types of scenarios. Extraction of dynamic properties of obstacles becomes difficult when the measurement sensors do not provide speed directly. The dynamic polyline representation of obstacles is a compromise between the rigid model-based cuboid representation and the model-free representation of occupancy grids. This paper presents a polyline-based obstacle extraction system, based on a particle-based occupancy grid generated by processing of stereovision data. We have developed a real-time flexible method for occupancy grid modeling and representation using particles that move from cell to cell and are created and destroyed based on measurements derived from stereovision. The results of the occupancy grid processing are subjected to the Border Scanner algorithm, which extracts polylines from occupied cells, by taking into consideration only the most relevant information of the grid. The resulting system is able to extract individual objects from the occupancy grid, model them as polylines, and estimate their speed without making assumptions about their shape or size.	algorithm;cuboid;entity;real-time clock;real-time computing;sensor;stereopsis	Andrei Vatavu;Radu Danescu;Sergiu Nedevschi	2011	2011 IEEE 7th International Conference on Intelligent Computer Communication and Processing	10.1109/ICCP.2011.6047875	computer vision;simulation;geography;occupancy grid mapping;computer graphics (images)	Robotics	50.501632479172244	-39.58992670560596	13934
085b5f9fd49432edab29e2c64f2a427fbce97f67	what do 15,000 object categories tell us about classifying and localizing actions?	video signal processing image classification image motion analysis image representation;universiteitsbibliotheek;encoding games accuracy cameras training visualization neural networks;action recognition object categories automatic human action classification automatic human action localization video representation object encoding	This paper contributes to automatic classification and localization of human actions in video. Whereas motion is the key ingredient in modern approaches, we assess the benefits of having objects in the video representation. Rather than considering a handful of carefully selected and localized objects, we conduct an empirical study on the benefit of encoding 15,000 object categories for action using 6 datasets totaling more than 200 hours of video and covering 180 action classes. Our key contributions are i) the first in-depth study of encoding objects for actions, ii) we show that objects matter for actions, and are often semantically relevant as well. iii) We establish that actions have object preferences. Rather than using all objects, selection is advantageous for action recognition. iv)We reveal that object-action relations are generic, which allows to transferring these relationships from the one domain to the other. And, v) objects, when combined with motion, improve the state-of-the-art for both action classification and localization.	action potential;encode;emoticon;internationalization and localization	Mihir Jain;Jan C. van Gemert;Cees Snoek	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2015.7298599	computer vision;machine learning;pattern recognition;mathematics	Vision	33.224566425670794	-50.26065872452162	13946
88e2a8b8b4403a6970ea562b62c02049c7700917	fusing range and intensity images for mobile robot localization	nonlinear filters;probability;mobile robot;location estimation;laser sensor intensity images range images mobile robot localization symmetries and perturbation model spmodel probabilistic representation model extended kalman filter integration mechanism uncertain geometric information multisensor systems location estimation indoor mobile robotics macrobe mobile robot;kalman filters;mobile robots;laser ranging;indexing terms;computer vision;range image;feature extraction;mobile robots robot sensing systems sensor systems robustness solid modeling sensor fusion infrared sensors uncertainty machine vision two dimensional displays;stereo image processing;mobile robot localization;sensor fusion;extended kalman filter;probability sensor fusion mobile robots laser ranging feature extraction stereo image processing kalman filters nonlinear filters	In this paper, we present the two-dimensional (2-D) version of the symmetries and perturbation model (SPmodel), a probabilistic representation model and an EKF integration mechanism for uncertain geometric information that is suitable for sensor fusion and integration in multisensor systems. We apply the SPmodel to the problem of location estimation in indoor mobile robotics, experimenting with the mobile robot MACROBE. We have chosen two types of complementary sensory information: 1) range images; 2) intensity images; obtained from a laser sensor. Results of these experiments show that fusing simple and computationally inexpensive sensory information can allow a mobile robot to precisely locate itself. They also demonstrate the generality of the proposed fusion and integration mechanism.	experiment;extended kalman filter;mobile robot;robotic mapping;robotics	José Neira;Juan D. Tardós;Joachim Horn;Günther Schmidt	1999	IEEE Trans. Robotics and Automation	10.1109/70.744604	control engineering;mobile robot;computer vision;simulation;computer science;engineering	Robotics	52.31445606857492	-35.52537709716654	13949
09dc45f036781ccfc51cdc7a6f2057aba8b5ff10	classifying collisions with spatio-temporal action graph networks		Events defined by the interaction of objects in a scene often are of critical importance, yet such events are typically rare and available labeled examples insufficient to train a conventional deep model that performs well across expected object appearances. Most deep learning activity recognition models focus on global context aggregation and do not explicitly consider object interactions inside the video, potentially overlooking important cues relevant to interpreting activity in the scene. In this paper, we show that a new model for explicit representation of object interactions significantly improves deep video activity classification for driving collision detection. We propose a Spatio-Temporal Action Graph (STAG) network, which incorporates spatial and temporal relations of objects. The network is automatically learned from data, with a latent graph structure inferred for the task. As a benchmark to evaluate performance on collision detection tasks, we introduce a novel data set based on data obtained from real life driving collisions and near-collisions. This data set reflects the challenging task of detecting and classifying accidents in a richly varying but yet highly constrained setting, that is very relevant to the evaluation of autonomous driving and alerting systems. Our experiments confirm that our STAG model offers significantly improved results for collision activity classification.		Roei Herzig;Elad Levi;Huijuan Xu;Eli Brosh;Amir Globerson;Trevor Darrell	2018	CoRR			ML	32.570982396995674	-48.31078804498323	13969
4ea759e13b0991772c61a4ede058d59d5e33a71b	scale resilient, rotation invariant articulated object matching	dynamic programming;kernel;reliability;object articulation;image motion analysis;blurry imagery;video signal processing;image matching;articulated object representation;kernel vectors image edge detection optimization indexes videos reliability;object scaling;indexes;motion blur;vectors;structure constraints;scale resilient rotation invariant articulated object matching;image edge detection;object rotation;image representation;path cost factorization;cluttered videos;dense pixels;optimization;exemplar image;blurry imagery scale resilient rotation invariant articulated object matching cluttered videos exemplar image articulated object representation dense pixels motion blur structure constraints object scaling object rotation object articulation optimal pixel walks shortest path problem dynamic programming path cost factorization;video signal processing dynamic programming image matching image motion analysis image representation;shortest path problem;videos;optimal pixel walks	A novel method is proposed for matching articulated objects in cluttered videos. The method needs only a single exemplar image of the target object. Instead of using a small set of large parts to represent an articulated object, the proposed model uses hundreds of small units to represent walks along paths of pixels between key points on an articulated object. Matching directly on dense pixels is key to achieving reliable matching when motion blur occurs. The proposed method fits the model to local image properties, conforms to structure constraints, and remembers the steps taken along a pixel path. The model formulation handles variations in object scaling, rotation and articulation. Recovery of the optimal pixel walks is posed as a special shortest path problem, which can be solved efficiently via dynamic programming. Further speedup is achieved via factorization of the path costs. An efficient method is proposed to find multiple walks and simultaneously match multiple key points. Experiments show that the proposed method is efficient and reliable and can be used to match articulated objects in fast motion videos with strong clutter and blurry imagery.	biconnected component;clutter;dynamic programming;experiment;fits;feature extraction;gaussian blur;image scaling;pixel;shortest path problem;speedup	Hao Jiang;Tai-Peng Tian;Kun He;Stan Sclaroff	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247669	database index;computer vision;kernel;computer science;machine learning;dynamic programming;pattern recognition;reliability;mathematics;shortest path problem	Vision	48.79084593989733	-49.714349197435006	14058
eb272dc5e57e7bc1e6240f11b303229df0fdb7cc	real-time shot transition detection in compressed mpeg video streams	mpeg video;real time;video	Digital libraries became necessary vehicles that provide users with powerful and easy-to-use tools for searching, browsing, and retrieving media information. The starting point for these en- deavors is the same: segmentation of video material into shots. The aim of this study is to segment MPEG video streams into shots. A fully automatic detection for both abrupt and gradual transitions (dis- solve and fade groups) with minimal decoding in real time is devel- oped. Each detection process was explored through two phases: macroblock type analysis in bidirectional predictive pictures (B- frames), and on-demand intensity information analysis. The abrupt transition detection is explored first by examining the number of for- ward and backward macroblocks (p- and b-MBs) in consecutive B-frames, and then an intensity histogram comparison is applied to confirm detected transitions. The gradual transition is detected first by examining the intracoded predicted macroblocks (i-MBs) within successive B-frames, and then the detection is confirmed by check- ing the parabolic shape of the frame variances of the candidate sequence. Results of the study show remarkable detection rate for both abrupt and gradual transitions. © 2008 SPIE and IS&T.	moving picture experts group;real-time transcription;shot transition detection;streaming media	Mona A. Fouad;Fatma Bayoumi;Hoda Onsi;Mohamed G. Darwish	2008	J. Electronic Imaging	10.1117/1.2912059	computer vision;video;computer science;block-matching algorithm;multimedia;computer graphics (images)	Vision	39.5909953168202	-51.85924428040534	14064
9b1a1a1f9fba3bf3eff4d6b28ed38f7cfe7a3404	approximating the sum operation for marginal-map inference	marginal map inference;graphical model;approximate inference	We study the marginal-MAP problem on graphical models, and present a novel approximation method based on direct approximation of the sum operation. A primary difficulty of marginal-MAP problems lies in the non-commutativity of the sum and max operations, so that even in highly structured models, marginalization may produce a densely connected graph over the variables to be maximized, resulting in an intractable potential function with exponential size. We propose a chain decomposition approach for summing over the marginalized variables, in which we produce a structured approximation to the MAP component of the problem consisting of only pairwise potentials. We show that this approach is equivalent to the maximization of a specific variational free energy, and it provides an upper bound of the optimal probability. Finally, experimental results demonstrate that our method performs favorably compared to previous methods.	approximation algorithm;connectivity (graph theory);expectation–maximization algorithm;graphical model;marginal model;maximal set;time complexity;variational principle	Qiang Cheng;Feng Chen;Jianwu Dong;Wenli Xu;Alexander T. Ihler	2012			mathematical optimization;computer science;machine learning;graphical model;statistics	AI	25.880212370206095	-29.003831544994828	14118
2cba97c851779e51c962e9420e1e40b8b3a80370	unsupervised classification with non-gaussian mixture models using ica	gaussian mixture model;unsupervised classification	We present an unsupervised classification algorithm based on an ICA mixture model. The ICA mixture model assumes that the observed data can be categorized into several mutually exclusive data classes in which the components in each class are generated by a linear mixture of independent sources. The algorithm finds the independent sources, the mixing matrix for each class and also computes the class membership probability for each data point. This approach extends the Gaussian mixture model so that the classes can have non-Gaussian structure. We demonstrate that this method can learn efficient codes to represent images of natural scenes and text. The learned classes of basis functions yield a better approximation of the underlying distributions of the data, and thus can provide greater coding efficiency. We believe that this method is well suited to modeling structure in high-dimensional data and has many potential applications.	algorithm;algorithmic efficiency;approximation;basis function;categorization;code;data point;independent computing architecture;mixture model;unsupervised learning	Te-Won Lee;Michael S. Lewicki;Terrence J. Sejnowski	1998			computer science;machine learning;pattern recognition;mixture model;mathematics;statistics	ML	30.772377861771314	-33.534651870534425	14121
30f3d33ab3aee6db4057243e0d2f28714315f9fd	consistent multi-layer subtask tracker via hyper-graph regularization		Most multi-task learning based trackers adopt similar task definition by assuming that all tasks share a common feature set, which can’t cover the real situation well. In this paper, we define the subtasks from the novel perspective, and develop a structured and consistent multi-layer multi-subtask tracker with graph regularization. The tracking task is completed by the collaboration of multi-layer subtasks. Different subtasks correspond to the tracking of different parts in the target area. The correspondences of the subtasks among the adjacent frames are consistent and smooth. The proposed model introduces hyper-graph regularizer to preserve the global and local intrinsic geometrical structures among and inside target candidates or trained samples, and decomposes the representative matrix of the subtasks into two components: low-rank property captures the subtask relationship, group-sparse property identifies the outlier subtasks. Moreover, a collaborate metric scheme is developed to find the best candidate, by concerning both discrimination reliability and representation accuracy. We show that the proposed multi-layer multi-subtask learning based tracker is a general model, which accommodates most existing multi-task trackers with the respective merits. Encouraging experimental results on a large set of public video sequences justify the effectiveness and robustness of the proposed tracker, and achieve comparable performance against many state-of-the-art methods.	computer multitasking;layer (electronics);multi-task learning;sparse matrix	Baojie Fan;Yang Cong	2017	Pattern Recognition	10.1016/j.patcog.2017.02.008	computer vision;simulation;machine learning;data mining;mathematics	Vision	30.06492929671768	-46.90391562408948	14128
d1edb5ab71499f8c275cb950bd494856e11fcb9d	motion analysis via feature point tracking technology	motion analysis;sift;motion vector;human motion;human body;object tracking;image sequence;feature point tracking;dynamic time warping	"""In this paper, we propose a tracking method via SIFT algorithm for recording the trajectory of human motion in image sequence. Instead of using a human model that present the human body to analyze motion. Only exact two feature points from the local region of a trunk, one for joints and one for limb. We calculate the similarity between two features of trajectories. The method of computing similarity is based on the """"motion vector"""" and """"angle"""". We can know the degree of the angle by the connect line from joint to limb in a plane which is using the core of object to be the center. The proposed method consists of two parts. The first is to track the feature points and output the file which record motion trajectory. The second part is to analyze features of trajectory and adopt DTW (Dynamic Time Warping) to calculate the score to show the similarity between two trajectories."""		Yu-Shin Lin;Shih-Ming Chang;Joseph C. Tsai;Timothy K. Shih;Hui-Huang Hsu	2011		10.1007/978-3-642-17829-0_28	computer vision;structure from motion;human body;simulation;quarter-pixel motion;computer science;machine learning;dynamic time warping;video tracking;motion estimation;scale-invariant feature transform;motion field;computer graphics (images)	Vision	48.715783557996545	-45.79877081488541	14140
34d294ded4bdbab9d76ec959fa1c8a34703f0200	material recognition from local appearance in global context		Recognition of materials has proven to be a challenging problem due to the wide variation in appearance within and between categories. Global image context, such as where the material is or what object it makes up, can be crucial to recognizing the material. Existing methods, however, operate on an implicit fusion of materials and context by using large receptive fields as input (i.e., large image patches). Many recent material recognition methods treat materials as yet another set of labels like objects. Materials are, however, fundamentally different from objects as they have no inherent shape or defined spatial extent. Approaches that ignore this can only take advantage of limited implicit context as it appears during training. We instead show that recognizing materials purely from their local appearance and integrating separately recognized global contextual cues including objects and places leads to superior dense, per-pixel, material recognition. We achieve this by training a fully-convolutional material recognition network end-toend with only material category supervision. We integrate object and place estimates to this network from independent CNNs. This approach avoids the necessity of preparing an impractically-large amount of training data to cover the product space of materials, objects, and scenes, while fully leveraging contextual cues for dense material recognition. Furthermore, we perform a detailed analysis of the effects of context granularity, spatial resolution, and the network level at which we introduce context. On a recently introduced comprehensive and diverse material database [14], we confirm that our method achieves state-of-the-art accuracy with significantly less training data compared to past methods.	bottom-up proteomics;category theory;pixel;titan rain;top-down and bottom-up design;yet another	Gabriel Schwartz;Ko Nishino	2016	CoRR		computer vision;computer science;machine learning	Vision	28.06033178229417	-50.91307669627482	14184
b187924e9769b7f651ec314c310b543a9abd81c6	classification of nematode image stacks by an information fusion based multilinear approach		Abstract In this letter, we present to use an information fusion based multilinear analysis approach to classify multi-focal image stacks. First, image fusion techniques such as the nonsubsampled contourlet transform sparse representation (NSCTSR) are used to combine relevant information of multi-focal images within a given image stack into a single image, which is more informative and complete than any single image in the given image stack. Second, multi-focal images within a stack are fused along 3 orthogonal directions, and multiple features extracted from the fused images along different directions are combined by using canonical correlation analysis (CCA). Finally, because multi-focal image stacks represent the effect of different factors - texture, shape, different instances within the same class and different classes of the objects, we embed the information fusion methods within a multilinear analysis (MA) framework to propose an information fusion based multilinear classifier. The experimental results demonstrated that the information fusion based multilinear classifier can reach a higher classification rate (96.6%) than the previous multilinear based approach (86.4%), even we only use the texture feature instead of the combination of texture and shape features as in the previous work.		Min Liu;Xueping Wang;Hongzhong Zhang	2017	Pattern Recognition Letters	10.1016/j.patrec.2017.09.024	multilinear subspace learning;stack (abstract data type);mathematics;multilinear map;contourlet;pattern recognition;computer vision;artificial intelligence;canonical correlation;machine learning;multilinear principal component analysis;sparse approximation;image fusion	Vision	30.57334815784814	-45.00304499560004	14193
4198badf365134593a5bf5f25b2c5f354758eeb5	"""commentary paper 2 on """"visual players detection and tracking in soccer matches"""""""	visual players detection;video signal processing;special issues and sections;surveillance;soccer matches;image classification;industries;noise measurement;visualization;end to end video system;objects tracking;object classification;visual players tracking;objects detection;cameras;object detection;conferences	This paper presents an end-to-end video system for detection, tracking, and classifying objects within the scope of a soccer match.	end-to-end encryption	Alan J. Lipton	2008	2008 IEEE Fifth International Conference on Advanced Video and Signal Based Surveillance	10.1109/AVSS.2008.39	computer vision;contextual image classification;simulation;visualization;computer science;noise measurement;multimedia	Robotics	39.887196113954175	-46.87958105464456	14222
2f04c7aaac3a884088be550d1be51b4a0b585a2e	robust, real-time 3d tracking of multiple objects with similar appearances		This paper proposes a novel method for tracking multiple moving objects and recovering their three-dimensional (3D) models separately using multiple calibrated cameras. For robustly tracking objects with similar appearances, the proposed method uses geometric information regarding 3D scene structure rather than appearance. A major limitation of previous techniques is foreground confusion, in which the shapes of objects and/or ghosting artifacts are ignored and are hence not appropriately specified in foreground regions. To overcome this limitation, our method classifies foreground voxels into targets (objects and artifacts) in each frame using a novel, probabilistic two-stage framework. This is accomplished by step-wise application of a track graph describing how targets interact and the maximum a posteriori expectation-maximization algorithm for the estimation of target parameters. We introduce mixture models with semiparametric component distributions regarding 3D target shapes. In order to not confuse artifacts with objects of interest, we automatically detect and track artifacts based on a closed-world assumption. Experimental results show that our method outperforms state-of-the-art trackers on seven public sequences while achieving real-time performance.	3d modeling;closed-world assumption;expectation–maximization algorithm;experiment;film-type patterned retarder;mixture model;norm (social);real-time clock;real-time locating system;rollover (key);semiparametric model;voxel	Taiki Sekii	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.463	computer vision;pattern recognition;mathematics;computer graphics (images)	Vision	45.14369443932542	-47.88899415317762	14231
682d194235ba3b573889836ba118502e8b525728	backpropagation through the void: optimizing control variates for black-box gradient estimation		Gradient-based optimization is the foundation of deep learning and reinforcement learning, but is difficult to apply when the mechanism being optimized is unknown or not differentiable. We introduce a general framework for learning low-variance, unbiased gradient estimators, applicable to black-box functions of discrete or continuous random variables. Our method uses gradients of a surrogate neural network to construct a control variate, which is optimized jointly with the original parameters. We demonstrate this framework for training discrete latent-variable models. We also give an unbiased, action-conditional extension of the advantage actor-critic reinforcement learning algorithm.		Will Grathwohl;Dami Choi;Yuhuai Wu;Geoffrey Roeder;David K. Duvenaud	2017	CoRR		machine learning;differentiable function;artificial intelligence;estimator;mathematical optimization;reinforcement learning;deep learning;mathematics;discrete optimization;control variates;random variable;backpropagation	ML	24.677559197445674	-30.500821884680914	14261
e4e60d900eab17405ba85746db017cea9fcb447d	a generalized pyramid matching kernel for human action recognition in realistic videos	kernel based classification method;biosensing techniques;pyramid matching kernel;video analysis;human action recognition;video recording;algorithms;pattern recognition automated;humans	"""Human action recognition is an increasingly important research topic in the fields of video sensing, analysis and understanding. Caused by unconstrained sensing conditions, there exist large intra-class variations and inter-class ambiguities in realistic videos, which hinder the improvement of recognition performance for recent vision-based action recognition systems. In this paper, we propose a generalized pyramid matching kernel (GPMK) for recognizing human actions in realistic videos, based on a multi-channel """"bag of words"""" representation constructed from local spatial-temporal features of video clips. As an extension to the spatial-temporal pyramid matching (STPM) kernel, the GPMK leverages heterogeneous visual cues in multiple feature descriptor types and spatial-temporal grid granularity levels, to build a valid similarity metric between two video clips for kernel-based classification. Instead of the predefined and fixed weights used in STPM, we present a simple, yet effective, method to compute adaptive channel weights of GPMK based on the kernel target alignment from training data. It incorporates prior knowledge and the data-driven information of different channels in a principled way. The experimental results on three challenging video datasets (i.e., Hollywood2, Youtube and HMDB51) validate the superiority of our GPMK w.r.t. the traditional STPM kernel for realistic human action recognition and outperform the state-of-the-art results in the literature."""	bag-of-words model;epilepsy, generalized;existential quantification;genetic heterogeneity;kernel (operating system);matching;numerous;super paper mario;temporal lobe;video clip;visual descriptor;weight;videocassette	Jun Zhu;Quan Zhou;Weijia Zou;Rui Zhang;Wenjun Zhang	2013		10.3390/s131114398	computer vision;computer science;machine learning;pattern recognition;tree kernel	AI	32.832854807004985	-49.03579514551974	14263
7a1078981478f785c03b58dbd09bab4894a36ecc	fast and scalable position-based layout synthesis		The arrangement of objects into a layout can be challenging for non-experts, as is affirmed by the existence of interior design professionals. Recent research into the automation of this task has yielded methods that can synthesize layouts of objects respecting aesthetic and functional constraints that are non-linear and competing. These methods usually adopt a stochastic optimization scheme, which samples from different layout configurations, a process that is slow and inefficient. We introduce an physics-motivated, continuous layout synthesis technique, which results in a significant gain in speed and is readily scalable. We demonstrate our method on a variety of examples and show that it achieves results similar to conventional layout synthesis based on Markov chain Monte Carlo (McMC) state-search, but is faster by at least an order of magnitude and can handle layouts of unprecedented size as well as tightly-packed layouts that can overwhelm McMC.	gain;markov chain monte carlo;mathematical optimization;monte carlo method;nonlinear system;pack unit;physical object;scalability;stochastic optimization	Tomer Weiss;Alan Litteneker;Noah Duncan;Masaki Nakada;Chenfanfu Jiang;Lap-Fai Yu;Demetri Terzopoulos	2018	IEEE transactions on visualization and computer graphics	10.1109/TVCG.2018.2866436	stochastic optimization;automation;theoretical computer science;order of magnitude;scalability;computer science;markov chain monte carlo	Visualization	46.87709352040988	-28.00448017817202	14283
dc574bb8f426e3013576bb34120d3fad146a54b5	modeling of the manuo-ocular coordination during object guiding through a path	eye;object guiding manuo ocular eye hand coordination eye movements arm movements coordination control;target tracking manuoocular coordination modeling object guiding human eye hand coordination medical diagnosis human performance;manuo ocular;arm movements;accuracy;visualization;eye hand coordination;object guiding;trajectory;eye movements;mathematical model;coordination control;target tracking eye object detection;humans;target tracking;humans target tracking accuracy trajectory mathematical model visualization;object detection	Knowledge on human eye-hand coordination can be used for human-like system design and medical diagnosis. This document analyses and briefly presents the parameters of the coordination while executing different eye-hand related tasks. Existing quantitative model of manuo-ocular coordination, capable of simulating the human performance in target tracking, is redesigned for a capability to simulate the performance of object guiding through a visible predefined path. Qualitative model, based on quantitative model, is proposed and explained.	experiment;human reliability;mathematical model;pursuit-evasion;simulation;systems design	Saulius Niauronis;Raimondas Zemblys;Vincas Laurutis	2012	2012 IEEE 12th International Conference on Bioinformatics & Bioengineering (BIBE)	10.1109/BIBE.2012.6399744	computer vision;eye–hand coordination;simulation;visualization;computer science;trajectory;mathematical model;accuracy and precision;statistics;eye movement	Robotics	49.0047983091258	-31.54211585555193	14287
5d044b8b1062bd0f3f50e6734aa5a018e1e78c94	performance robustness of feature extraction for target detection & classification	weapons cepstral analysis feature extraction object detection pattern classification principal component analysis sensors;robustness to environmental uncertainties feature extraction pattern classification;acoustic sensing systems performance robustness feature extraction environmental uncertainties automated target detection automated target classification principal component analysis cepstrum symbolic dynamic filtering mortar launchers;feature extraction robustness training testing mortar support vector machines principal component analysis	Performance robustness of feature extraction with respect to environmental uncertainties is often critical for automated target detection & classification. This paper focuses on performance robustness in the sense that the extracted features are desired to be largely insensitive to environmental uncertainties, while they should be capable of recognizing the effects of small perturbations in the underlying system dynamics for detection & classification. From this perspective, performance robustness of three feature extraction algorithms, namely, principal component analysis, cepstrum, and symbolic dynamic filtering, is evaluated for target classification by making use of the respective field data collected from different sites. These algorithms have been evaluated for robust classification of two different types of mortar launchers with acoustic sensing systems, based on the training and testing data sets from the same and different field sites. The results, generated with training and testing data from different field sites, characterize performance robustness of the respective feature extraction algorithms, when compared with those generated with the corresponding data sets from the same field site.	acoustic cryptanalysis;algorithm;approximation algorithm;cepstrum;feature extraction;k-nearest neighbors algorithm;mortar methods;perturbation theory;principal component analysis;refinement (computing);sample rate conversion;sparse approximation;sparse matrix;support vector machine;system dynamics;weight function	Brian M. Smith;Pritthi Chattopadhyay;Asok Ray;Shashi Phoha;Thyagaraju Damarla	2014	2014 American Control Conference	10.1109/ACC.2014.6858590	feature extraction;engineering;machine learning;pattern recognition;data mining;feature	ML	37.488634047737435	-34.33830139638389	14290
4c16fed5bcc8fd236ce2980cd23c093c8a8d53fa	nonlinear generative models for dynamic shape and dynamic appearance	object recognition;nonlinear mapping;shape legged locomotion humans image reconstruction computer science lighting principal component analysis face recognition object recognition tensile stress;tensile stress;generic model;legged locomotion;face recognition;shape;nonlinear dimensionality reduction;approximate solution;image reconstruction;principal component analysis;humans;lighting;computer science;facial expression;geometric structure	Our objective is to learn representations for the shape and the appearance of moving (dynamic) objects that supports tasks such as synthesis, pose recovery, reconstruction and tracking. In this paper we introduce a framework that aim to learn a landmark-free correspondence-free global representations of dynamic appearance manifolds. We use nonlinear dimensionality reduction to achieve an embedding of the global deformation manifold that preserves the geometric structure of the manifold. Given such embedding, a nonlinear mapping is learned from the embedding space into the visual input space. Therefore, any visual input is represented by a linear combination of nonlinear bases functions centered along the manifold in the embedding space. We also show how approximate solution for the inverse mapping can be obtained in a closed form which facilitate recovery of the intrinsic body configuration. We use the framework to learn the gait manifold as an example of a dynamic shape manifold, as well as to learn the manifolds for some simple gestures and facial expressions as examples of dynamic appearance manifolds.	approximation algorithm;manifold regularization;nonlinear dimensionality reduction;nonlinear system	Ahmed M. Elgammal	2004	2004 Conference on Computer Vision and Pattern Recognition Workshop	10.1109/CVPR.2004.407	iterative reconstruction;facial recognition system;computer vision;shape;computer science;cognitive neuroscience of visual object recognition;machine learning;lighting;mathematics;geometry;nonlinear dimensionality reduction;stress;facial expression;principal component analysis	Vision	30.77991374348869	-46.872969004648034	14296
ee9ca2b39bc5de9400e2d16853f44f18a9b36203	automated multi-target tracking in public traffic in the presence of data association uncertainty		Automated driving systems of emerging and future vehicles need to resolve the behaviors of other traffic participants or targets in order to safely navigate in public traffic. In this paper, a comprehensive multi-target tracking system is outlined that addresses target birth/appearance and death/disappearance processes in the presence of measurement data association uncertainties. The tracking system is based on the joint integrated probabilistic data association filter, which is adapted to specifically include algorithms that handle track initiation and termination, clutter density estimation and track maintenance. The workings of the proposed algorithms are demonstrated via multiple traffic scenario simulations that show how tracks can be initialized and terminated autonomously, and how data association uncertainties can affect tracking performance if not handled correctly.	algorithm;autonomous car;clutter;correspondence problem;mathematical model;nonlinear system;numerical analysis;numerical integration;numerical methods for ordinary differential equations;probabilistic data association filter;prototype;randomness;simulation;spectral density;tracking system;white noise	Andinet Hunde;Beshah Ayalew	2018	2018 Annual American Control Conference (ACC)	10.23919/ACC.2018.8431852	real-time computing;control engineering;computer science;radar tracker;density estimation;probabilistic data association filter;clutter;tracking system	Mobile	49.69457975301531	-32.6368093726572	14304
fc1735883206a58ef6acbdfdc13cfe74f8c5359d	ellipse fitting for imaged cross sections of a surface of revolution	surface of revolution;ellipse;reconstruction;intrinsic matrix;pose estimation	This paper addresses the problem of accurately fitting the elliptical projections of the cross sections of a surface of revolution (SOR) with the given intrinsic matrix of a camera. By the new approach proposed in this paper, the image points of the SOR cross sections are fitted under the two geometric constraints which are derived from the configuration characteristics of SOR. It is demonstrated by comparison with other previous methods that our method can fit the elliptical projections of SOR cross sections more accurately and robustly. In this paper, we also describe the applications of the algorithm in Euclidean reconstruction of SOR and pose estimation for SOR-shaped object, such as spacecraft. We accurately fit the imaged cross sections of a surface of revolution.The ellipses can be fitted under the circular point and orthogonality constraints.An effective optimization is elaborately designed.The method can serve for reconstruction and localization of a surface of revolution.	cross section (geometry);curve fitting	Chang Liu;Weiduo Hu	2015	Pattern Recognition	10.1016/j.patcog.2014.09.028	mathematical optimization;pose;surface of revolution;mathematics;geometry;ellipse	Vision	53.70763226004743	-49.75741470252355	14381
c1ba9faee9fe2665064b471cc90183b35995b1a1	crowd security detection based on entropy model		Identifying the terror attack, illegal public gathering or other mass events risks by utilizing cameras is an important concern both in crowd security area and in pattern recognition research area. This paper provides a physical entropy model to measure the crowd security level. The entropy model was created by identifying individuals’ moving velocity and the related probability. The individuals are represented by Harris Corners in videos, thus to avoid the time-consuming human recognition task. Simulation experiment and video detection experiments were conducted, verified that in the disordered state, the entropy is higher; while in ordered state, the entropy is much lower; when the crowd security has a sudden change, the entropy will change. It was verified that the entropy is the applicable indicator of crowd security. By recognizing the entropy mutation, it is possible to automatically detect the abnormal crowd behavior and to set the warning alarm.	experiment;harris affine region detector;pattern recognition;simulation;velocity (software development)	Ying Zhao;Mengqi Yuan;Guofeng Su;Tao Chen	2015			data mining;crowd psychology;corner detection;security level;computer security;computer science	Vision	40.50580033476916	-45.32115817092655	14471
27c5d1c0ddd2347010fd9906c6e5d8ceb40f51ea	markov chain monte carlo combined with deterministic methods for markov random field optimization	message passing;pixel;markov processes;local minima;monte carlo methods;belief propagation;minimisation;graph cuts;data mining;simulated annealing;computer vision;markov chain monte carlo;energy minimization;energy states;tree graphs;graph cut;stochastic processes	Many vision problems have been formulated as energy minimization problems and there have been significant advances in energy minimization algorithms. The most widely-used energy minimization algorithms include graph cuts, belief propagation and tree-reweighted message passing. Although they have obtained good results, they are still unsatisfactory when it comes to more difficult MRF problems such as non-submodular energy functions, highly connected MRFs, and high-order clique potentials. There have also been other approaches, known as stochastic sampling-based algorithms, which include simulated annealing, Markov chain Monte Carlo and population based Markov chain Monte Carlo. They are applicable to any general energy models but they are usually slower than deterministic methods. In this paper, we propose new algorithms which elegantly combine stochastic and deterministic methods. Sampling-based methods are boosted by deterministic methods so that they can rapidly move to lower energy states and easily jump over energy barriers. In different point of view, the sampling-based method prevents deterministic methods from getting stuck at local minima. Consequently, a combination of both approaches substantially increases the quality of the solutions. We present a thorough analysis of the proposed methods in synthetic MRF problems by controlling the hardness of the problems. We also demonstrate experimental results for the photomontage problem which is the most difficult one among the standard MRF benchmark problems.	algorithm;belief propagation;benchmark (computing);cut (graph theory);energy level;energy minimization;experiment;markov chain monte carlo;markov random field;mathematical optimization;maxima and minima;message passing;monte carlo method;sampling (signal processing);simulated annealing;software propagation;submodular set function;synthetic intelligence	Wonsik Kim;Kyoung Mu Lee	2009	2009 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPRW.2009.5206504	mathematical optimization;combinatorics;cut;markov chain monte carlo;computer science;stochastic optimization;machine learning;mathematics;statistics	Vision	32.98451617803017	-33.48281372815837	14513
4a443ab002221a59899b567a074d4ce612c7b3b5	world feature detection and mapping using stereovision and inertial sensors	vision system;statistical approach;feature detection;image segmentation;map building;autonomous vehicle;mobile robot;robot navigation;sensor fusion;vision;3d structure;article;outlier removal;inertial sensor;3d reconstruction;inertial sensors	This paper explores the fusion of inertial information with vision for 3D reconstruction. A method is proposed for vertical line segment detection and subsequent local geometric map building. Visual and inertial sensing are two sensory modalities that can be explored to give robust solutions on image segmentation and recovery of 3D structure from images, increasing the capabilities of autonomous vehicles and enlarging the application potential of vision systems. From the inertial sensors, a camera stereo rig, and a few system parameters we can recover the 3D parameters of the ground plane and vertical lines. The homography between stereo images of ground points can be found. By detecting the vertical line segments in each image, and using the homography of ground points for the foot of each segment, the lines can be matched and reconstructed in 3D. The mobile robot then maps the detected vertical line segments in a world map as it moves. To build this map an outlier removal method is implemented and a statistical approach used, so that a simplified metric map can be obtained for robot navigation. © 2003 Elsevier Science B.V. All rights reserved.	3d reconstruction;augmented reality;autonomous robot;feature detection (computer vision);feature detection (web development);gyro;homography (computer vision);image segmentation;inertial navigation system;map;mobile robot;real-time clock;real-time computing;robotic mapping;sensor;stereopsis;user interface;vanishing point;vertical bar;wearable computer	Jorge Lobo;Carlos Queiroz;Jorge Dias	2003	Robotics and Autonomous Systems	10.1016/S0921-8890(03)00011-3	3d reconstruction;mobile robot;vision;inertial measurement unit;computer vision;simulation;machine vision;computer science;feature detection;sensor fusion;image segmentation	Robotics	50.82191901808215	-39.139039809193214	14515
60ea6b59ac2d6f9dea504d7b162a1c1e0aede5e9	fine-tuning contextual-based optimum-path forest for land-cover classification	satellite imagery fine tuning contextual based optimum path forest land cover classification contextual based learning neighboring pixel pixelwise oriented classification technique metaheuristic framework nondiscrete markovian model optimum path forest classifier;support vector machines;remote sensing geophysical image processing image classification land cover learning artificial intelligence markov processes;optimum path forest opf contextual classification;hidden markov models;feature extraction;satellites;optimization;markov processes;context modeling;markov processes optimization feature extraction support vector machines context modeling satellites hidden markov models	Contextual-based learning aims at considering neighboring pixels to improve pixelwise-oriented classification techniques. In this letter, we presented a metaheuristic framework for the optimization of nondiscrete Markovian models considering the optimum-path forest (OPF) classifier, and we proposed a postprocessing procedure to avoid overcorrection over high-frequency regions. The proposed approach outperformed previous results obtained with standard OPF in satellite imagery.	mathematical optimization;metaheuristic;pixel	Daniel Osaku;Danillo Roberto Pereira;Alexandre L. M. Levada;João Paulo Papa	2016	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2016.2541458	support vector machine;computer vision;feature extraction;computer science;machine learning;pattern recognition;context model;markov process;satellite;hidden markov model	Vision	31.71286388307121	-43.80379377797947	14559
1d2c24279ff9e9e7ddd0206b7b90acd608a3c894	detecting separation of moving objects based on non-parametric bayesian scheme for tracking by particle filter	non-parametric bayesian scheme;object group separate;object group;detecting separation;previous method;particle filter;new approach;shorter time;whole process	This paper proposes a new approach for detecting separation of a group of moving objects tracked by the method based on the particle filter. When some objects with overlaps come into the area filmed with the camera, they are treated as one object and tracked by a group of particles. When the object group separates into each object or some groups, the method fails in tracking or there are some objects which are not tracked. The proposed method detects the separation of the object group by the non-parametric Bayesian scheme. The method can perform the whole process in shorter time than the previous method and remains the same performance.	particle filter;sensor	Yasuchika Takeda;Shinji Fukui;Yuji Iwahori;Robert J. Woodham	2011		10.1007/978-3-642-23866-6_12	computer vision;simulation;geography;machine learning	Vision	45.05916409950667	-48.04496955539418	14563
3d78f77533438843e38c55d72656fbd3f53f6925	reliable fusion of stereo matching and depth sensor for high quality dense depth maps	biological patents;biomedical journals;text mining;europe pubmed central;citation search;segmentation;citation networks;stereo matching;research articles;abstracts;open access;life sciences;clinical guidelines;full text;texture constraint;depth sensor;fusion move;rest apis;orcids;multiscale pseudo two layer model;europe pmc;biomedical research;bioinformatics;literature search	"""Depth estimation is a classical problem in computer vision, which typically relies on either a depth sensor or stereo matching alone. The depth sensor provides real-time estimates in repetitive and textureless regions where stereo matching is not effective. However, stereo matching can obtain more accurate results in rich texture regions and object boundaries where the depth sensor often fails. We fuse stereo matching and the depth sensor using their complementary characteristics to improve the depth estimation. Here, texture information is incorporated as a constraint to restrict the pixel's scope of potential disparities and to reduce noise in repetitive and textureless regions. Furthermore, a novel pseudo-two-layer model is used to represent the relationship between disparities in different pixels and segments. It is more robust to luminance variation by treating information obtained from a depth sensor as prior knowledge. Segmentation is viewed as a soft constraint to reduce ambiguities caused by under- or over-segmentation. Compared to the average error rate 3.27% of the previous state-of-the-art methods, our method provides an average error rate of 2.61% on the Middlebury datasets, which shows that our method performs almost 20% better than other """"fused"""" algorithms in the aspect of precision."""	algorithm;binocular disparity;computer stereo vision;computer vision;constrained optimization;estimated;fuse device component;graphics processing unit;obstruction;pattern matching;pixel;pseudo brand of pseudoephedrine;range imaging;real-time locating system;scheme;segmentation action;sensor;structured-light 3d scanner;texture mapping;biologic segmentation;sensor (device)	Jing Liu;Chunpeng Li;Xuefeng Fan;Zhaoqi Wang	2015		10.3390/s150820894	computer vision;text mining;computer science;bioinformatics;data mining;segmentation;information retrieval	Vision	47.48723468938425	-50.46371994879361	14604
27c90e7d80fefb75122902af5ec10154dd2c2179	event-based detector for non-intrusive load monitoring based on the hilbert transform	detectors;power main event based detector nonintrusive load monitoring hilbert transform smart grid nilm energy consumption disaggregation single point sensor sampling frequency steady state transient state detection derivate filter smart meter;measurement;home appliances;microwave filters;event detection;transient analysis;smart power grids building management systems domestic appliances hilbert transforms power consumption power system measurement power system transients smart meters;transient analysis microwave filters detectors measurement smart meters home appliances event detection;energy efficiency non intrusive load monitoring nilm energy disaggregation transient states event based detection;smart meters	Within the emerging new technologies in Smart Grids, the concern about Non-Intrusive Load Monitoring (NILM) techniques has increased over the last five years. These techniques aim to disaggregate the energy consumption on a household or commercial building into individual appliances from the use of a single-point sensor. Most efforts attempt to disaggregate by using a low sampling frequency and by focusing on steady consumption states. Nevertheless, to achieve a better accuracy, it is necessary a higher sampling frequency and to look up both steady states and transient states. This paper proposes a novel event-based detector that could be used in NILM systems in a previous stage in order to detect transient states and help to the labelling task. This could lead to a better performance, specially taking into account small power changes which are not usually well detected on most low-frequency systems. It is based on the use of Hilbert Transform to obtain the envelope of the signal and the use of an average and a derivate filter to obtain a set of spikes that characterize each transition. The performance has been tested using a set of known metrics obtaining good results in two cases: sampling at 1kHz, which the aim of a possible integration in smart meters; and in a real scenario where 20 different appliance-loads are connected to the power main.	hilbert transform;sampling (signal processing);smart tv;smart meter;steady state	José M. Alcalá;Jesús Ureña;Álvaro Hernández	2014	Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA)	10.1109/ETFA.2014.7005320	embedded system;detector;electronic engineering;real-time computing;computer science;engineering;measurement	Embedded	35.42264074891389	-31.664255630157456	14606
49891105fd8423914e48685c47cecf77f75cb5e1	fast good features selection for wide area monitoring	change detection;motion compensation;surveillance;surveillance system;apparent motion;feature tracking;motion estimation;mobile object;computer vision;camera motion;object detection tracking surveillance motion compensation motion estimation active vision feature extraction;outdoor environments surveillance wide area monitoring active vision ego motion estimation feature extraction camera motion reference map good features badly tracked feature rejection compensation change detection method mobile object location;feature extraction;feature selection;cameras surveillance computerized monitoring computer vision object detection streaming media parameter estimation mathematics computer science computer errors;tracking;object detection;active vision	Recently the surveillance of wide areas has pointed the interest of the research community. The use of active vision seems to be the most effective solutions for these needs. Against the better acquiring resolution there is the problem of the apparent motion inducted by the camera motion known as ego-motion. Feature based methods for ego-motion estimation are widely used in computer vision but they deal with feature recovery and with errors in feature tracking. In this paper, we propose a fast method to extract and select new features during camera motion. This is achieved by adopting a reference map containing well trackable features that is updated at each frame by introducing new good features related to regions appearing in the current image. A new procedure is applied to reject badly tracked features. The current frame and the background after compensation are processed by a change detection method in order to locate mobile objects. Results are presented in the context of a visual-based surveillance system for monitoring outdoor environments.		Christian Micheloni;Gian Luca Foresti	2003		10.1109/AVSS.2003.1217931	computer vision;simulation;active vision;feature extraction;computer science;motion estimation;tracking;motion compensation;feature selection;change detection;feature;computer graphics (images)	ML	45.05479210273691	-45.96014196390031	14632
28e559f18a25d3ea0d0018b98fbf1b15152ee195	sensor selection by gmb-rem in real robot position estimation	gaussian mixture;mobile robot;sensor selection;bayes theorem;robot localisation;density estimation;probability distribution;position estimation;expectation maximisation	Modelling and reducing uncertainty are two essential problems with mobile robot localisation. Previously we developed a robot localisation system, namely, the Gaussian Mixture of Bayes with Regularised Expectation Maximisation (GMB-REM), using a single sensor. GMB-REM allows a robot’s position to be modelled as a probability distribution, and uses Bayes’ theorem to reduce the uncertainty of its location. In this paper, a new system for performing sensor selection is introduced, namely an enhanced form of GMB-REM. Empirical results show the new system outperforms GMBREM using sonar alone. More specifically, it is able to select between multiple sensors at each robot’s position, and further minimises the average robot localisation error.	course (navigation);dead reckoning;expectation–maximization algorithm;kernel density estimation;mobile robot;obstacle avoidance;robotic mapping;sonar (symantec);sensor;the australian	Takamasa Koshizen	2000	Journal of Intelligent and Robotic Systems	10.1023/A:1008152032268	probability distribution;mobile robot;monte carlo localization;computer vision;density estimation;computer science;engineering;machine learning;pattern recognition;bayes' theorem	Robotics	51.62667059661731	-34.3299151312893	14636
a65b93c01518755291e19a0545c1a3d20e401c0a	a large contextual dataset for classification, detection and counting of cars with deep learning		We have created a large diverse set of cars from overhead images, which are useful for training a deep learner to binary classify, detect and count them. The dataset and all related material will be made publically available. The set contains contextual matter to aid in identification of difficult targets. We demonstrate classification and detection on this dataset using a neural network we call ResCeption. This network combines residual learning with Inception-style layers and is used to count cars in one look. This is a new way to count objects rather than by localization or density estimation. It is fairly accurate, fast and easy to implement. Additionally, the counting method is not car or scene specific. It would be easy to train this method to count other kinds of objects and counting over new scenes requires no extra set up or assumptions about object locations.	artificial neural network;box counting;deep learning;internationalization and localization;overhead (computing)	T. Nathan Mundhenk;Goran Konjevod;Wesam A. Sakla;Kofi Boakye	2016		10.1007/978-3-319-46487-9_48	computer vision;computer science;machine learning;data mining	Vision	28.091611709893037	-49.26425478739422	14644
d1a8f9bfb157901b0c55210fb2e2d86e8c3a7f3c	an online video composition system	ptz;pan tilt zoom camera;control systems;minimization;video streaming;video information;video signal processing;heuristic rule;pan tilt zoom;distortion;temporal resolution;cameras streaming media signal processing control systems laboratories distortion spatial resolution signal resolution costs robustness;streaming media;video cameras;video switcher;signal processing;video recording;signal resolution;signal characteristics;robustness;it management;user view selection;online video composition system;user view selection online video composition system pan tilt zoom camera ptz video information heuristic rule minimization video switcher signal characteristics;cameras;video streaming video cameras video recording video signal processing;spatial resolution	This paper presents an information-driven online video composition system. The composition work handled by the system includes dynamically setting multiple pan/tilt/zoom (PTZ) cameras to proper poses and selecting the best close-up view for passive viewers. The main idea of the composition system is to maximize captured video information with limited cameras. Unlike video composition based on heuristic rules, our video composition is formulated as a process of minimizing distortions between ideal signals (i.e. signals with infinite spatial-temporal resolution) and displayed signals. The formulation is consistent with many well-known empirical approaches widely used in previous systems and may provide analytical explanations to those approaches. Moreover, it provides a novel approach for studying video composition tasks systematically. The composition system allows each user to select a personal close-up view. It manages PTZ cameras and a video switcher based on both signal characteristics and users' view selections. Additionally, it can automate the video composition process based on past users' view-selections when immediate selections are not available. We demonstrate the performance of this system with real meetings	distortion;heuristic;pan–tilt–zoom camera;video clip	Qiong Liu;Xiaojin Shi;Don Kimber;Frank Zhao;Frank Raab	2005	2005 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2005.1521467	computer vision;image resolution;distortion;computer science;temporal resolution;signal processing;video tracking;multimedia;programming language;robustness;computer graphics (images)	Visualization	38.37892882501552	-44.70004833244009	14657
0da48fc1ae9bb532e0b58ef329e6e711b9dd555e	joint probabilistic curve clustering and alignment	cluster algorithm;time course;probabilistic approach;gene expression;feature vector	Clustering and prediction of sets of curves is an important problem in many areas of science and engineering. It is often the case that curves tend to be misaligned from each other in a continuous manner, either in space (across the measurements) or in time. We develop a probabilistic framework that allows for joint clustering and continuous alignment of sets of curves in curve space (as opposed to a fixed-dimensional featurevector space). The proposed methodology integrates new probabilistic alignment models with model-based curve clustering algorithms. The probabilistic approach allows for the derivation of consistent EM learning algorithms for the joint clustering-alignment problem. Experimental results are shown for alignment of human growth data, and joint clustering and alignment of gene expression time-course data.	algorithm;cluster analysis;dynamic time warping;machine learning;spline (mathematics);time complexity	Scott Gaffney;Padhraic Smyth	2004			correlation clustering;constrained clustering;probabilistic analysis of algorithms;gene expression;feature vector;k-medians clustering;fuzzy clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis	ML	31.037155921322594	-30.682809690449655	14671
043d2d50a8c19a655ebd23eb64af92959015e786	error bounds for maximum likelihood matrix completion under sparse factor models	maximum likelihood;complexity regularization;maximum likelihood estimation;noise measurement;sparse estimation;highly quantized one bit observations maximum likelihood matrix completion sparse factor models random noise sparse matrices sparsity penalized maximum likelihood approach additive gaussian noise;sparse matrices gaussian noise maximum likelihood estimation;matrix completion;dictionaries;sparse matrices noise dictionaries noise measurement maximum likelihood estimation data models;sparse estimation complexity regularization matrix completion maximum likelihood;sparse matrices;noise;data models	This paper examines a general class of matrix completion tasks where entry wise observations of the matrix are subject to random noise or corruption. Our particular focus here is on settings where the matrix to be estimated follows a sparse factor model, in the sense that it may be expressed as the product of two matrices, one of which is sparse. We analyze the performance of a sparsity-penalized maximum likelihood approach to such problems to provide a general-purpose estimation result applicable to any of a number of noise/corruption models, and describe its implications in two stylized scenarios - one characterized by additive Gaussian noise, and the other by highly-quantized one-bit observations. We also provide some supporting empirical evidence to validate our theoretical claims in the Gaussian setting.	gaussian blur;general-purpose modeling;image noise;noise (electronics);quantization (signal processing);sparse matrix;the matrix;utility functions on indivisible goods	Akshay Soni;Swayambhoo Jain;Jarvis D. Haupt;Stefano Gonella	2014	2014 IEEE Global Conference on Signal and Information Processing (GlobalSIP)	10.1109/GlobalSIP.2014.7032147	gaussian noise;econometrics;pattern recognition;sparse approximation;mathematics;maximum likelihood sequence estimation;estimation theory;statistics	ML	28.731023036356504	-29.917442692365007	14682
70d0aeaaed41048f5b67c676a6a4027e889263f5	a cgans-based scene reconstruction model using lidar point cloud		Road scene reconstruction is a fundamental and crucial module at the perception phase for autonomous vehicles, and will influence the later phase, such as object detection, motion planing and path planing. Traditionally, self-driving car uses Lidar, camera or fusion of the two kinds of sensors for sensing the environment. However, single Lidar or camera-based approaches will miss crucial information, and the fusion-based approaches often consume huge computing resources. We firstly propose a conditional Generative Adversarial Networks (cGANs)-based deep learning model that can rebuild rich semantic scene images from upsampled Lidar point clouds only. This makes it possible to remove cameras to reduce resource consumption and improve the processing rate. Simulation on the KITTI dataset also demonstrates that our model can reestablish color imagery from a single Lidar point cloud, and is effective enough for real time sensing on autonomous driving vehicles.	algorithm;autonomous car;autonomous robot;bilateral filter;deep learning;generative adversarial networks;object detection;planning;point cloud;real-time locating system;sensor;simulation;upsampling	Zhenchao Ouyang;Yu Liu;Changjie Zhang;Jianwei Niu	2017	2017 IEEE International Symposium on Parallel and Distributed Processing with Applications and 2017 IEEE International Conference on Ubiquitous Computing and Communications (ISPA/IUCC)	10.1109/ISPA/IUCC.2017.00167	point cloud;human–computer interaction;iterative reconstruction;computer vision;object detection;resource consumption;deep learning;solid modeling;perception;lidar;computer science;artificial intelligence	Robotics	51.179507625108215	-42.63302880529467	14690
19e78e41582c6b31c5fbba6e513c3e3a227cd336	learning generative models of invariant features	robot localization;cameras solid modeling robot vision systems layout computer science computational geometry visualization robot localization noise robustness lighting;generic model;intelligent robots;learning model;learning artificial intelligence intelligent robots feature extraction;scale invariant feature transform;feature extraction;visual features;learning artificial intelligence;robot localization generative models visual features scale invariant feature transform training images feature extraction pose dependent behavior learning imaging geometry robotic tasks model learning framework;invariant feature	We present a method for learning a set of models of visual features which are invariant to scale and translation in the image domain. The models are constructed by first applying the scale-invariant feature transform (SIFT) to a set of training images, and matching the extracted features across the images, followed by learning the pose-dependent behavior of the features. The modeling process avoids assumptions with respect to scene and imaging geometry, but rather learns the direct mapping from camera pose to feature observation. Such models are useful for applications to robotic tasks, such as localization, as well as visualization tasks. We present the model learning framework, and experimental results illustrating the success of the method for learning models that are useful for robot localization.	feature model;ground truth;online and offline;robot;robotic mapping;scale-invariant feature transform	Robert Sim;Gregory Dudek	2004	2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)	10.1109/IROS.2004.1389955	robot learning;feature learning;computer vision;feature extraction;computer science;artificial intelligence;machine learning;pattern recognition;scale-invariant feature transform;feature	Robotics	45.76521163085947	-51.90582170419586	14728
961be31ce2a39a9301c4df334ba71e3a4b9a29a1	co-consistent regularization with discriminative feature for zero-shot learning		With the development of deep learning, zero-shot learning (ZSL) issues deserve more attention. Due to the problems of projection domain shift and discriminative feature extraction, we propose an end-to-end framework, which is different from traditional ZSL methods in the following two aspects: (1) we use a cascaded network to automatically locate discriminative regions, which can better extract latent features and contribute to the representation of key semantic attributes. (2) our framework achieves mapping in visual-semantic embedding space and calculation procedure of the dot product in deep learning framework. In addition, a joint loss function is designed for the regularization constraint of the whole method and achieves supervised learning, which enhances generalization ability in test set. In this paper, we make some experiments on Animals with Attributes 2 (AwA2), Caltech-UCSD Birds 200-2011 (CUB) and SUN datasets, which achieves better results compared to the state-of-the-art methods.		Yanling Tian;Weitong Zhang;Qieshi Zhang;Jun Cheng;Pengyi Hao;Gang Lu	2018		10.1007/978-3-030-04167-0_4	supervised learning;discriminative model;machine learning;dot product;feature extraction;pattern recognition;artificial intelligence;deep learning;computer science;embedding;regularization (mathematics);test set	NLP	25.068345817618646	-50.673846144184694	14826
c1ad60f8f07f8b86aa88e432e7138ff17d3e3e6c	learning probabilistic submodular diversity models via noise contrastive estimation		Modeling diversity of sets of items is important in many applications such as product recommendation and data summarization. Probabilistic submodular models, a family of models including the determinantal point process, form a natural class of distributions, encouraging effects such as diversity, repulsion and coverage. Current models, however, are limited to small and medium number of items due to the high time complexity for learning and inference. In this paper, we propose FLID, a novel log-submodular diversity model that scales to large numbers of items and can be efficiently learned using noise contrastive estimation. We show that our model achieves state of the art performance in terms of model fit, but can be also learned orders of magnitude faster. We demonstrate the wide applicability of our model using several experiments.	association rule learning;cubic function;experiment;point process;scalability;submodular set function;time complexity	Sebastian Tschiatschek;Josip Djolonga;Andreas Krause	2016			speech recognition;machine learning;mathematics;communication	ML	25.724329549127514	-29.773130301315938	14842
139ca20d6e9ef6556425fbdec1a208bff61dca40	a multivariate gradient and mutual information measure method for hyperspectral image visualization		Hyperspectral imaging is becoming relevant in many applications. A large number of bands can be used to distinguish between different materials, quality inspection and mineral exploration. However, the information gathered by the narrow spectral bands in a hyperspectral image is not only large, but shows a high degree of correlation between bands. This redundancy needs to be reduced for efficient computation and storage. In this paper, we propose a new similarity metric, which combines gradient information and mutual information as positive interactive information. We apply the similarity metric for the visualization of hyperspectral images. Experimental methods show that the proposed method is superior to the state-of-the-art methods.	computation;gradient descent;mutual information	Anthony Amankwah	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518380	data visualization;computer vision;redundancy (engineering);visualization;spectral bands;computer science;mutual information;artificial intelligence;computation;multivariate statistics;hyperspectral imaging	Robotics	30.397672331491698	-43.473907646283664	14846
2f59406cce55c7bb9a78521bd14755a0db0aee7d	annealed importance sampling	high dimensionality;simulated annealing;data analysis;sequential importance sampling;thermodynamics;tempered transitions;importance sampling;free energy;free energy computation;markov chain;estimation of normalizing constants	Abstract. Simulated annealing — moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions — has traditionally been used as an inexact method of handling isolated modes in Markov chain samplers. Here, it is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler. The Markov chain aspect allows this method to perform acceptably even for high-dimensional problems, where finding good importance sampling distributions would otherwise be very difficult, while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases. This annealed importance sampling procedure resembles the second half of the previously-studied tempered transitions, and can be seen as a generalization of a recently-proposed variant of sequential importance sampling. It is also related to thermodynamic integration methods for estimating ratios of normalizing constants. Annealed importance sampling is most attractive when isolated modes are present, or when estimates of normalizing constants are required, but it may also be more generally useful, since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in Markov chain samplers.	autocorrelation;cobham's thesis;converge;importance sampling;markov chain;particle filter;sampling (signal processing);simulated annealing;thermodynamic integration	Radford M. Neal	2001	Statistics and Computing	10.1023/A:1008923215028	econometrics;markov chain;mathematical optimization;simulated annealing;markov chain monte carlo;importance sampling;slice sampling;mathematics;data analysis;umbrella sampling;statistics	ML	27.612677065123325	-28.429168121389267	14874
80a8302639675e11c1de4725b5850355b128e8b3	learning locally minimax optimal bayesian networks	bayesian framework;bayesian network;model selection;computacion informatica;theoretical framework;bayesian approach;ciencias basicas y experimentales;normalized maximum likelihood;probability distribution;minimum description length;background knowledge;grupo a;information theoretic;prediction;bayesian networks	We consider the problem of learning Bayesian network models in a non-informative setting, where the only available information is a set of observational data, and no background knowledge is available. The problem can be divided into two different subtasks: learning the structure of the network (a set of independence relations), and learning the parameters of the model (that fix the probability distribution from the set of all distributions consistent with the chosen structure). There are not many theoretical frameworks that consistently handle both these problems together, the Bayesian framework being an exception. In this paper we propose an alternative, information-theoretic framework which sidesteps some of the technical problems facing the Bayesian approach. The framework is based on the minimax-optimal Normalized Maximum Likelihood (NML) distribution, which is motivated by the Minimum Description Length (MDL) principle. The resulting model selection criterion is consistent, and it provides a way to construct highly predictive Bayesian network models. Our empirical tests show that the proposed method compares favorably with alternative approaches in both model selection and prediction tasks.	bayesian network;exception handling;information theory;minimax;minimum description length;model selection	Tomi Silander;Teemu Roos;Petri Myllymäki	2010	Int. J. Approx. Reasoning	10.1016/j.ijar.2010.01.012	bayesian average;econometrics;bayes factor;bayesian experimental design;variable-order bayesian network;wake-sleep algorithm;marginal likelihood;computer science;machine learning;free energy principle;pattern recognition;bayesian network;mathematics;bayesian linear regression;graphical model;bayesian hierarchical modeling;bayesian statistics;bayesian econometrics;bayesian information criterion;dynamic bayesian network;empirical probability;statistics	ML	28.413469451584465	-31.378347379163174	14914
3f5b76e52afb136eeea443234b7ec69d6058625c	on mixing in pairwise markov random fields with application to social networks	rapid mixing;markov random fields;social network modelling;glauber dynamics	We consider pairwise Markov random fields which have a number of important applications in statistical physics, image processing and machine learning such as Ising model and labeling problem to name a couple. Our own motivation comes from the need to produce synthetic models for social networks with attributes. First, we give conditions for rapid mixing of the associated Glauber dynamics and consider interesting particular cases. Then, for pairwise Markov random fields with submodular energy functions we construct monotone perfect simulation.	glauber;image processing;ising model;machine learning;markov chain;markov random field;simulation;social network;submodular set function;synthetic intelligence;monotone	Konstantin Avrachenkov;Lenar Iskhakov;Maksim Mironov	2016		10.1007/978-3-319-49787-7_11	markov chain;combinatorics;discrete mathematics;machine learning;mathematics;markov renewal process;markov chain mixing time;markov model;variable-order markov model	ML	26.693929259980095	-29.215237940144075	15019
3bfe67917363f153ca4cdfac587eed50a94cd831	simultaneous localization and map-building using active vision	mobile robot;real time;stereo image processing mobile robots robot vision active vision computerised navigation;mobile robots;robot kinematics robot vision systems sonar navigation simultaneous localization and mapping mobile robots image reconstruction cameras robot sensing systems head measurement uncertainty;journal article;robot vision;automatic detection;stereo image processing;real time experiments localization map building active vision slam high performance stereo head uncertainty based measurement selection automatic map maintenance goal directed steering;high performance;simultaneous localization and map building;simultaneous localisation and map building;active vision;computerised navigation	ÐAn active approach to sensing can provide the focused measurement capability over a wide field of view which allows correctly formulated Simultaneous Localization and Map-Building (SLAM) to be implemented with vision, permitting repeatable longterm localization using only naturally occurring, automatically-detected features. In this paper, we present the first example of a general system for autonomous localization using active vision, enabled here by a high-performance stereo head, addressing such issues as uncertainty-based measurement selection, automatic map-maintenance, and goal-directed steering. We present varied real-time experiments in a complex environment. Index TermsÐActive vision, simultaneous localization and map-building, mobile robots.	active vision;autonomous robot;binocular disparity;experiment;machine vision;mobile robot;parallax;real-time clock;simultaneous localization and mapping;sparse matrix;structure from motion	Andrew J. Davison;David W. Murray	2002	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/TPAMI.2002.1017615	computer stereo vision;mobile robot;stereo cameras;computer vision;simulation;machine vision;computer science;artificial intelligence;mobile robot navigation;computer graphics (images)	Vision	51.948871745612024	-40.69211583077559	15076
57dedfdb1a1db72f27864996074c4baca84572c9	joint interpretation of on-board vision and static gps cartography for determination of correct speed limit	computer vision;speed limits;adaptive cruise control;pattern recognition;cartography;autonomous intelligent cruise control;speed control;driver support systems	We present here a first prototype of a “Speed Limit Support” Advance Driving Assistance System (ADAS) producing permanent reliable information on the current speed limit applicable to the vehicle. Such a module can be used either for information of the driver, or could even serve for automatic setting of the maximum speed of a smart Adaptive Cr uise Control (ACC). Our system is based on a joint interpretation of cartographic information (f or static reference information) with on-board visi on, used for traffic sign detection and recognition (in cluding supplementary sub-signs) and visual road lines localization (for detection of lane changes). The visual traffic sign detection part is quite ro bust (90% global correct detection and recognition for m ain speed signs, and 80% for exit-lane sub-signs detection). Our approach for joint interpretation w ith cartography is original, and logic-based rather than probability-based, which allows correct behavi our even in cases, which do happen, when both vision and cartography may provide the same erroneo us information. INTRODUCTION AND RELATED WORK Too many accidents are still provoked by excessive p ed limit. It would therefore be interesting to offer drivers permanently updated in formation on current speed limit, so they have more chance to adapt their speed. Alternately, a more advanced very valuable feature would be a smart Adaptive Cruise Control (ACC) auto matically adapting vehicle speed to current speed-limit. However, these kinds of functi ons can be really valuable and of practical use only if the produced speed-limit information is really reliable. Most current GPS navigators now include a function to inform the dri ve of the supposed current speed-limit, this information extracted from GPS cartographic da ta is neither always complete nor systematically up-to-date. Moreover, temporary spee d limits for road works, and variable speed limits enforced by LED signs, are by definiti o not included in pre-defined digital cartographic data. Visual traffic sign recognition (TSR) can be quite robust, but it is unavoidable to miss some signs when occlusion by an other vehicle (especially truck) occurs. Figure 1. By nature, variable speed limits and roadwork temp orary speed limits cannot be obtained from the static cartographic information.	architecture design and assessment system;cartography;device driver;gps navigation device;global positioning system;on-board data handling;prototype;temporary variable;traffic sign recognition;visi on	Alexandre Bargeton;Fabien Moutarde;Fawzi Nashashibi;Anne-Sophie Puthon	2010	CoRR		computer vision;simulation;computer science;pattern recognition;electronic speed control	Vision	48.514455122870935	-36.30134411708983	15112
d6b9e4a6c1a92086bb754d656998f55a5d669f17	a vlsi architecture for real time object detection on high resolution images	image resolution;very large scale integration;computer architecture;object detection correlation real time systems computer architecture very large scale integration image resolution program processors;correlation;program processors;object detection;real time systems	This paper describes a VLSI-based SIMD multiprocessor system for the implementation of a set of basic object detection algorithms. The system architecture takes advantage of modern fast EDRAM-technology to support the communication requirements of 800 Mbytes/s between main memory and processors imposed by high resolution images. A specialized processing element (PE) architecture for implementation in VLSI which efficiently implements the basic set of algorithms is presented. The performance of a single PE is discussed with respect to the different algorithms. A system consisting of Ą processing elements realized in 0.6μ CMOS-technology is able to localize a 128×128 pixel template in a 1024×1024 pixel image at a rate of 10 frames/second (sustained performance 2.1 · 109 Ops/s).	algorithm;cmos;central processing unit;computer data storage;edram;image resolution;multiprocessing;object detection;pixel;requirement;simd;systems architecture;very-large-scale integration	Marco Cavadini;Matthias Wosnitza;Markus Thaler;Gerhard Tröster	1996	1996 8th European Signal Processing Conference (EUSIPCO 1996)		parallel computing;real-time computing;computer science;computer graphics (images)	Robotics	44.186850123769084	-35.30262562487262	15130
2ec424007d72261e197378a4e632140b3469fc14	action recognition using direction-dependent feature pairs and non-negative low rank sparse model	direction specific dictionary;direction dependent feature pairs;non negative low rank sparse model;action recognition	In this paper, we propose to use direction-dependent feature pairs (DDFP) to represent actions and a novel non-negative low rank sparse model (NLRM) is developed to encode the features. We summarize our main contributions into three aspects. First, for a video we apply eight different directions to describe the spatio-temporal relations between features, and construct directional feature pairs according to their relative positions. Second, we present a non-negative low rank sparse model which incorporates the low rank term and the nonnegative constraint. Our model can not only ensure the consistency of similar DDFP by the low rank term, but also enforce the sparsity of coding coefficients by the modified l2,1-norm regularization. Third, we utilize a direction-specific dictionary for each direction and encode DDFP of a specific direction by the corresponding dictionary. A video is finally represented by the concatenation of each direction’s pooling result. Experimental results on the KTH, Weizmann and UCF sports dataset show the effectiveness of our proposed framework for human action recognition.	coefficient;concatenation;dictionary;encode;elastic net regularization;experiment;feature vector;low-rank approximation;sparse matrix	Biyun Sheng;Wankou Yang;Changyin Sun	2015	Neurocomputing	10.1016/j.neucom.2015.01.064	speech recognition;machine learning;pattern recognition;mathematics;statistics	Vision	25.65443267025021	-44.876358986245904	15256
16f89129d9041a7a9919a6b9dc8816136d25b4f0	a vision-based vehicle speed warning system	vehicles hidden markov models estimation alarm systems roads markov processes testing;hidden markov model;image matching;hidden markov model vehicular digital video recorder warning system;testing;warning system;computer vision;embedded systems;hidden markov models;estimation;roads;feature extraction;video recording;frame rate vision based vehicle speed warning system pac duo embedded platform vehicle digital video recorder image capturing hidden markov model module vehicle speed prediction speed threshold value feature matching method image pixels speed estimation contiguous images speed range;vehicles;markov processes;video recording alarm systems computer vision embedded systems feature extraction hidden markov models image matching road vehicles;alarm systems;vehicular digital video recorder;road vehicles	In this paper, a vision based vehicle speed warning system was proposed which has been implemented on the PAC Duo embedded platform. The system consisted of a vehicle digital video recorder which used to capture the image in front of the vehicle, a hidden Markov model module to predict the next instant vehicle speed from the previous vehicle speeds, and a warning system to give the driver a warning signal when the predicted vehicle speed is larger than a predefined threshold value. The difficulty of the system is how to get the vehicle speed under the condition of without real vehicle speed provided. In this system, the vehicle speed provided for the hidden Markov model to predict the next instant vehicle speed is estimated from the images captured via the digital video recorder. Two methods have been evaluated for estimating the vehicle speed. One is using feature matching method between the two contiguous images; the other is via the pre-calculating distance matching of each pixel of the image. The testing speed range is from 0 to 110 km/hr and the processing frame rate is 5 fps. The accuracy of the speed estimation is the feature matching method: ±8%; and the other method: ±12%.	digital video recorder;embedded system;feature model;hidden markov model;markov chain;pixel	Shih-Chieh Huang;Chien-Chuan Lin;Ming-Shi Wang	2012	2012 9th International Conference on Ubiquitous Intelligence and Computing and 9th International Conference on Autonomic and Trusted Computing	10.1109/UIC-ATC.2012.110	embedded system;computer vision;estimation;simulation;feature extraction;computer science;machine learning;warning system;software testing;markov process;hidden markov model;statistics	Robotics	40.487638958694355	-45.71740451842	15260
16fa1c7a7b792f4b7ca49fbb1e4a50b7d4f3ff44	improving the accuracy of action classification using view-dependent context information	view-independent classifier;multiple view;human action recognition system;view-independent component;view-dependent context information;initial guess;posterior distribution;new view;smart home domain;view dependent knowledge;action classification	This paper presents a human action recognition system that decomposes the task in two subtasks. First, a view-independent classifier, shared between the multiple views to analyze, is applied to obtain an initial guess of the posterior distribution of the performed action. Then, this posterior distribution is combined with view based knowledge to improve the action classification. This allows to reuse the view-independent component when a new view has to be analyzed, needing to only specify the view dependent knowledge. An example of the application of the system into an smart home domain is discussed.	experiment;home automation;humans;ieee systems, man, and cybernetics society;plausibility structure;statistical classification;time-slot interchange	Rodrigo Cilla;Miguel A. Patricio;Antonio Berlanga;José M. Molina López	2011		10.1007/978-3-642-21222-2_17	computer vision;computer science;artificial intelligence;machine learning;data mining	Vision	35.12921479098003	-46.30456049374315	15265
03016c0cf9a468e3c7b28dd0bda250923f350b0e	optimal projection of observations in a bayesian setting		Abstract Optimal dimensionality reduction methods are proposed for the Bayesian inference of a Gaussian linear model with additive noise in presence of overabundant data. Three different optimal projections of the observations are proposed based on information theory: the projection that minimizes the Kullback–Leibler divergence between the posterior distributions of the original and the projected models, the one that minimizes the expected Kullback–Leibler divergence between the same distributions, and the one that maximizes the mutual information between the parameter of interest and the projected observations. The first two optimization problems are formulated as the determination of an optimal subspace and therefore the solution is computed using Riemannian optimization algorithms on the Grassmann manifold. Regarding the maximization of the mutual information, it is shown that there exists an optimal subspace that minimizes the entropy of the posterior distribution of the reduced model; a basis of the subspace can be computed as the solution to a generalized eigenvalue problem; an a priori error estimate on the mutual information is available for this particular solution; and that the dimensionality of the subspace to exactly conserve the mutual information between the input and the output of the models is less than the number of parameters to be inferred. Numerical applications to linear and nonlinear models are used to assess the efficiency of the proposed approaches, and to highlight their advantages compared to standard approaches based on the principal component analysis of the observations.	optimal projection equations	Loïc Giraldi;Olivier P. Le Maître;Ibrahim Hoteit;Omar M. Knio	2018	Computational Statistics & Data Analysis	10.1016/j.csda.2018.03.002	mathematics;econometrics;statistics;linear model;information theory;curse of dimensionality;dimensionality reduction;mutual information;mathematical optimization;principal component analysis;bayesian inference;optimization problem	ML	29.204194156057753	-29.0296534047984	15277
8b17cacd5210f4ddb28dd58352c0902455682242	feature map building based on shape similarity for home robot apriattenda	moving object;feature map home robot map building shape similarity;image motion analysis;apriattenda home robot;map building;mobile robot;feature map;mobile robotics;simultaneous localization;mobile robots;robot vision;object shape similarity;fuzzy shaped objects feature map building apriattenda home robot simultaneous localization mobile robotics robot continuous localization odometry error object shape similarity laser ranger finder indoor environment uncertain information handling moving objects;laser ranger finder;shape similarity;feature extraction;fuzzy shaped objects;indoor environment;slam robots feature extraction image motion analysis mobile robots object detection robot vision;moving objects;uncertain information handling;robot continuous localization;odometry error;mobile robots simultaneous localization and mapping robot sensing systems indoor environments biomimetics shape control control engineering planning research and development data mining;slam robots;home robot;object detection;feature map building	Recently, the problem of map building and simultaneous localization is a hot topic in mobile robotics field In this paper, a feature map building approach based on shape similarity for the development of home robot is presented In the map building process, the robot is continuously localized as the map is being built. The odometry error between each constitutive step is eliminated by comparing the shape similarity of the corresponding objects in scan obtained from the laser ranger finder against the global map, which was built in the previous step. This developed algorithm is capable of producing faithful global map of indoor environment, as well as handling uncertain information, such as moving objects and fuzzy-shaped objects. The experiment is carried out using home robot ApriAttendatrade, developed by Toshiba Corporation.	algorithm;domestic robot;feature model;level of detail;mathematical optimization;mobile robot;odometry;optimization problem;robotic mapping;robotics;scale (map);shape context;simultaneous localization and mapping;ranger	Jiang Zhu;Nafis Ahmad;Hideichi Nakamoto;Nobuto Matsuhira	2006	2006 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2006.340370	mobile robot;computer vision;simulation;computer science;artificial intelligence	Robotics	51.24997836248279	-37.72895009782935	15284
dcc3075071057c8fdb905701c98530d42d4a6183	optimizing of searching co-motion point-pairs for statistical camera calibration	qa75 electronic computers computer science szamitastechnika;image motion analysis;calibration image motion analysis statistical analysis image matching cameras;szamitogeptudomany;cameras calibration layout statistics geometry humans video sequences robustness tracking shape;image matching;projective geometry;statistical analysis;human assistance comotion point pair optimization statistical multicamera calibration image pairs matching statistics matching overlapping views projective geometry entropy based thresholding;image registration;camera calibration;co motions camera calibration image registration;calibration;cameras	In the paper we introduce an algorithm for matching partially overlapping image-pairs where the object of interest is in motion, even if the motion is discontinuous and in an unstructured environment. In our previous work [Z. Szlavik et al, 2004] we have shown that by using co-motion statistics matching of overlapping views can be done and then the projective geometry can be estimated. Here, we show how to optimize searching for concurrently moving pixels. The robust algorithm we describe here finds point correspondences in two images by using entropy-based thresholding and without searching for any structures and without the need for tracking continuous motion. Our method makes it possible to (re)calibrate multicamera systems without human assistance.	algorithm;camera resectioning;optimizing compiler;pixel;thresholding (image processing)	Zoltán Szlávik;Tamás Szirányi;László Havasi;Csaba Benedek	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1530271	computer vision;mathematical optimization;projective geometry;calibration;camera resectioning;computer science;image registration;pattern recognition;mathematics	Robotics	49.942827996412696	-49.13682288937625	15292
b02e8103482d1319dc56853a93ceff1ff607116c	the evaluation of data sources using multivariate entropy tools		We provide crucial insights into a recently proposed Shannon-type entropy balance equation for multivariate joint distributions.The decomposition can be plotted in an entropy ternary diagram.Each axis of the ternary diagram provides specific information about the distributions.We use both tools in the exploratory analysis of machine learning datasets.These tools are applicable to supervised and unsupervised tasks. We introduce from first principles an analysis of the information content of multivariate distributions as information sources. Specifically, we generalize a balance equation and a visualization device, the Entropy Triangle, for multivariate distributions and find notable differences with similar analyses done on joint distributions as models of information channels.As an example application, we extend a framework for the analysis of classifiers to also encompass the analysis of data sets. With such tools we analyze a handful of UCI machine learning task to start addressing the question of how well do datasets convey the information they are supposed to capture about the phenomena they stand for.	machine learning;self-information	Francisco J. Valverde-Albacete;Carmen Peláez-Moreno	2017	Expert Syst. Appl.	10.1016/j.eswa.2017.02.010	cross entropy;data mining;principle of maximum entropy;artificial intelligence;joint entropy;machine learning;conditional entropy;information diagram;transfer entropy;multivariate statistics;computer science;data analysis	ML	30.17542052043482	-35.62827196338414	15301
59f37a53ecdb0e5dd3ea9800a4c180f82bc47285	feature extraction in through-the-wall radar imaging	automatic target classification;target classification;image segmentation;support vector machines;feature extraction radar imaging layout image segmentation sensor arrays buildings humans wideband array signal processing object detection;superquadrics;stationary objects;trees mathematics feature extraction image classification image segmentation radar imaging support vector machines;synthetic aperture through the wall radar imaging experiments feature extraction automatic target classification stationary objects enclosed structures sar image image segmentation superquadrics recursive splitting tree support vector machines nearest neighbor classifiers indoor targets;image classification;trees mathematics;classification;through the wall;classification through the wall radar imaging feature extraction superquadrics;artificial neural networks;enclosed structures;indoor targets;three dimensional displays;feature extraction;radar imaging;sar image;nearest neighbor classifiers;recursive splitting tree;synthetic aperture through the wall radar imaging experiments;support vector machine;nearest neighbor classifier	This paper deals with the problem of automatic target classification or Through-the-Wall radar imaging. The proposed scheme considers stationary objects in enclosed structures and works on the SAR image rather than the raw data. It comprises segmentation, feature extraction based on superquadrics, and classification. We present a recursive splitting tree to obtain optimum parameters for feature extraction. Support vector machines and nearest neighbor classifiers are then applied to successfully classify among different indoor targets. The classification methods are tested and evaluated using real data generated from synthetic aperture Through-the-Wall radar imaging experiments.	experiment;feature extraction;recursion;stationary process;superquadrics;support vector machine;synthetic data	Christian Debes;Jürgen T. Hahn;Abdelhak M. Zoubir;Moeness G. Amin	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495929	support vector machine;computer vision;computer science;machine learning;pattern recognition;artificial neural network	Robotics	32.85813597245465	-43.798177565744226	15319
2a71f2fdec100a2bd7b687275ea1423108982e5b	incrementally perceiving hazards in driving		Perceiving hazards on road is significantly important because hazards have large tendency to cause vehicle crash. For this purpose, the feedbacks of more than one hundred drivers with different experience for safe driving are gathered. The obtained feedbacks indicate that the irregular motion behaviour, such as crossing or overtaking of traffic participants, and low illumination condition are highly threatening to drivers. Motivated by that, this paper fulfills the hazards detection by involving motion, color, near-infrared, and depth clues of traffic scene. Specifically, an incremental motion consistency measurement model is firstly built to infer the irregular motion behaviours, which is achieved by incremental graph regularized least soft-threshold squares (GRLSS) incorporating the better Laplacian distribution of the noise estimation in optical flow into the motion modeling. Second, multi-source cues are adaptively weighted and fused by a saliency based Bayesian integrated model for arousing driver’s attention when potential hazards appears, which can better reflect the video content and select the better band(s) for hazards prediction in different illumination conditions. Finally, the superiority of the proposed method relating to other competitors is verified by testing on twelve difficult video clips captured by ourselves, which contain color, near-infrared and recovered depth simultaneously and no registration or frame align-	digital video;feedback;multi-source;optical flow;video clip	Yuan Yuan;Jianwu Fang;Qi Wang	2018	Neurocomputing	10.1016/j.neucom.2017.12.017	motion analysis;artificial intelligence;overtaking;pattern recognition;mathematics;competitor analysis;salience (neuroscience);laplace distribution;graph;crash;optical flow	Vision	41.30371504052854	-46.33801092847035	15336
c0153b92a59fe0cb6405c888dd536d44d34a92fb	enhancing detection model for multiple hypothesis tracking		Tracking-by-detection has become a popular tracking paradigm in recent years. Due to the fact that detections within this framework are regarded as points in the tracking process, it brings data association ambiguities, especially in crowded scenarios. To cope with this issue, we extended the multiple hypothesis tracking approach by incorporating a novel enhancing detection model that included detection-scene analysis and detection-detection analysis; the former models the scene by using dense confidential detections and handles false trajectories, while the latter estimates the correlations between individual detections and improves the ability to deal with close object hypotheses in crowded scenarios. Our approach was tested on the MOT16 benchmark and achieved competitive results with current state-of-the-art trackers.	benchmark (computing);confidentiality;correspondence problem;programming paradigm;sensor;software development	Jiahui Chen;Hao Sheng;Yonghui Zhang;Zhang Xiong	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2017.266	artificial intelligence;pattern recognition;computer vision;computer science;bittorrent tracker	Vision	33.38593173191989	-50.317526398158336	15402
0599df9fe9ef0063e91f4fbc40612d67c8a64017	a wide-field direction-selective avlsi spiking neuron	biology computing;integrate and fire neuron direction selective avlsi spiking neuron natural images artificial images fly visual system anatomical layout one dimensional array elementary motion detectors local motion information motion output signal contrast stimulus size;circuit noise;band pass filters;fly visual system;natural images;local motion information;motion output;chip;analog very large scale integrated;neural chips;spiking neurons;direction selective avlsi spiking neuron;artificial images;one dimensional array;signal processing;analog computers;neurons visual system analog computers band pass filters delay robustness biology computing circuit noise signal processing gain control;vlsi;direction selectivity;robustness;integrate and fire neuron;neurons;signal contrast;anatomical layout;analogue processing circuits;visual system;neural chips vlsi analogue processing circuits;gain control;stimulus size;elementary motion detectors	WedescribeananalogVery-Large-ScaleIntegrated(aVLSI) wide-fielddirection-selecti ve spikingneuron that responds robustly to bothnatural andartificial images. Thecircuitry is basedon a model of the wide-field direction-selecti ve neurons in the fly visual system. The architecture of the chip closelyfollows theanatomical layout of thefly visual system.Thecircuit hasa one-dimensionalarrayof 37 elementarymotiondetectors(EMDs); eachof which provides local motion informationto the wide-fieldspikingneuron. Normalisationandadaptationareemployedin thecircuitsat eachprocessingstage.Theinputsto theEMDssaturatewith high contrastsso that themotionoutputis not confounded with the signalcontrast. Theoutputs of the EMDs areaggregatedin anonlinearwayto produceamotionoutput that is independentof thestimulussize. Theglobal aggregated signalis thenusedto driveanintegrate-and-fireneuron. All thecomputationis analogexcept for theoutput of thespiking neuron. We show resultsfrom thefabricatedcircuit.	artificial neuron	Shih-Chii Liu	2003		10.1109/ISCAS.2003.1206441	chip;computer vision;analog computer;electronic engineering;automatic gain control;visual system;telecommunications;computer science;electrical engineering;signal processing;band-pass filter;very-large-scale integration;robustness	ML	45.388099995018216	-32.330749786289054	15495
9744e978a58e0baf2ab2b0fd562674f3a5d5fe2b	translation and scale invariant landmark recognition using receptive field neural networks	robot sensing systems;neural networks;mobile robot;path planning;real time;neural networks mobile robots robot sensing systems sonar navigation laser feedback laboratories motion detection real time systems layout laser beams;laser beams;mobile robots;layout;neural net work;detection algorithm;sonar navigation;network architecture;receptive field;laser feedback;motion detection;scale invariance;neural network;real time systems;north carolina	In this paper we present a neural net- work based approach to landmark recognition. Landmarks are used by mobile robots for navi- gation and self referencing information. Due to the motion of the robot, the location and size of the landmark in the sensor is always changing. The network architecture presented here is capable of overcoming changes in scale as well as position of the landmark. The neu- ral network is directly fed with intensity im- ages. This is a significant reduction in compu- tation overhead since no high-level image pro- cessing is needed. Since robot motion in un- structured environments requires dynamic path planning, the detection algorithm must also be able to output guidance results. The network described here is able to output results in real time for the input patterns. We have trained the network to recognize common landmarks such as road signs (stop sign, yield sign, pedestrian-crossing sign, etc.). The algorithm has been successfully integrated into a mobile robot testbed developed at the Robotics and In- telligent Systems Laboratory at North Carolina State University.	neural networks	Ren C. Luo;Harsh Potlapalli;David W. Hislop	1992		10.1109/IROS.1992.587385	mobile robot;computer vision;simulation;computer science;engineering;artificial intelligence;control theory;artificial neural network	Vision	50.056810256691634	-30.70415693084929	15635
3a813f339f85de1cdc1627f123082d538801fb7c	stochastic road shape estimation	computer vision;shape;stochastic processes;data mining;data capture;frames per second;radar imaging;particle filter;radar tracking;image processing;field of view;feature extraction;rate of change	We describe a new system for estimating road shape ahead of a vehicle for the purpose of driver assistance. The method utilises a single on board colour camera, together with inertial and velocity information, to estimate both the position of the host car with respect to the lane it is following and also the width and curvature of the lane ahead at distances of up to 80 metres. The system’s image processing extracts a variety of different styles of lane markings from road imagery, and is able to compensate for a range of lighting conditions. Road shape and car position are estimated using a particle filter. The system, which runs at 10.5 frames per second, has been applied with some success to several hours’ worth of data captured from highways under varying imaging conditions.	algorithm;ground truth;image processing;particle filter;stochastic matrix;velocity (software development)	Ben Southall;Camillo J. Taylor	2001		10.1109/ICCV.2001.10022	computer vision;simulation;particle filter;field of view;image processing;feature extraction;computer science;automatic identification and data capture;frame rate	Vision	43.30517376479582	-42.015829045010335	15725
b4590d97d0113b6b8cb13b135bded4ec5e263d26	the geometry of statistical efficiency and matrix statistics		We will place certain parts of the theory of statistical efficiency into the author’s operator trigonometry (1967), thereby providing new geometrical understanding of statistical efficiency. Important earlier results of Bloomfield and Watson, Durbin and Kendall, Rao and Rao, will be so interpreted. For example, worse case relative least squares efficiency corresponds to and is achieved by the maximal turning antieigenvectors of the covariance matrix. Some little-known historical perspectives will also be exposed. The overall view will be emphasized.	least squares;maximal set;statistical model	Karl E. Gustafson	2007	JAMDS	10.1155/2007/94515	econometrics;calculus;mathematics;statistics	ML	34.12139154346984	-27.583516221710795	15729
328b8bfb11c8a15cf06cd5c70c0ec1d8eafdee9c	a nonparametric bayesian model of visual short-term memory		We present a nonparametric Bayesian model of the organization of visual short-term memory based on the Dirichlet process mixture model. Our model implements the idea that items in visual short-term memory can be encoded at multiple levels of abstraction, where the appropriate levels of abstraction and how much weight should be given to each level can be automatically determined. A capacity limit is implemented in this model by favoring small numbers of clusters of items. We show that various biases and distortions reported in visual short-term recall and recognition memory literatures can be quite naturally and elegantly explained by the model.	bayesian network;distortion;long short-term memory;mixture model;principle of abstraction	Emin Orhan;Robert A. Jacobs	2011			probability distribution;mixture model;gamma distribution;statistics;dirichlet distribution;uniform distribution (continuous);posterior probability;dirichlet process;mathematics;concentration parameter	Vision	30.652062273616448	-33.49529148037179	15733
6228852508222cb4d0fe09271da79d658385743d	building continuous occupancy maps with moving robots		Mapping the occupancy level of an environment is important for a robot to navigate in unknown and unstructured environments. To this end, continuous occupancy mapping techniques which express the probability of a location as a function are used. In this work, we provide a theoretical analysis to compare and contrast the two major branches of Bayesian continuous occupancy mapping techniques— Gaussian process occupancy maps and Bayesian Hilbert maps—considering the fact that both utilize kernel functions to operate in a rich high-dimensional implicit feature space and use variational inference to learn parameters. Then, we extend the recent Bayesian Hilbert maps framework which is so far only used for stationary robots, to map large environments with moving robots. Finally, we propose convolution of kernels as a powerful tool to improve different aspects of continuous occupancy mapping. Our claims are also experimentally validated with both simulated and real-world datasets.	algorithm;convolution;experiment;feature vector;gaussian process;hilbert space;map;robot;simulation;stationary process;variational principle	Ransalu Senanayake;Fabio Tozeto Ramos	2018			occupancy;machine learning;artificial intelligence;robot;computer science	AI	39.41600223888567	-26.34836839551379	15809
965ea6592a83af8d27b8caf356333c9bcdad45a2	synchronous image acquisition based on network synchronization	application software;real time;light emitting diodes;computer vision;image acquisition;synchronization;computer aided manufacturing;cost efficiency;production;cameras synchronization hardware application software production computer aided manufacturing real time systems computer errors light emitting diodes computer vision;computer errors;cameras;hardware;real time systems	In this paper, a software-based system for the real-time synchronization of images captured by a low-cost camera framework is presented. It is most well suited for cases where special hardware cannot be utilized (e.g. remote or wireless applications) and when cost efficiency is critical. The proposed method utilizes messages to establish a consensus on the time of image acquisition and NTP synchronization of computer clocks. It also provides with an error signal, in case of failure of the synchronization. The evaluation of the proposed algorithm using a precise LED array system (1ms accuracy) proves the effectiveness of this method.	algorithm;client–server model;computer vision;cost efficiency;emoticon;interpolation;mobile app;pattern recognition;real-time clock;server (computing);unreachable memory;video post-processing;virtual camera system;wiring	George C. Litos;Xenophon Zabulis;George A. Triantafyllidis	2006	2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW'06)	10.1109/CVPRW.2006.200	embedded system;synchronization;computer vision;application software;real-time computing;simulation;computer science;data synchronization;cost efficiency;computer-aided manufacturing;light-emitting diode	Vision	45.98115385510234	-36.353124754516074	15823
ea25c2588fc78bcb27e7550ad6879aa061c326a4	detecting and quantifying unusual interactions by correlating salient motion	image motion analysis;bottom up;multidimensional cooccurrence matrix;scene interpretation;matrix algebra;multi dimensional;frequency of occurrence;image representation;feature extraction;matrix algebra image motion analysis feature extraction image representation;unusual interactions detection;motion detection layout feature extraction data mining event detection entropy computer science frequency measurement image analysis prototypes;correlating salient motion;one dimensional vector unusual interactions quantification correlating salient motion unusual interactions detection scene interpretation features bottom up extraction salient features representation hierarchical cooccurrence framework multidimensional cooccurrence matrix;one dimensional vector;hierarchical cooccurrence framework;features bottom up extraction;salient features representation;unusual interactions quantification;co occurrence matrix;dynamic scenes	A significant problem in scene interpretation is efficient bottom-up extraction and representation of salient features. In this paper, we address the problem of correlating salient motion at a spatio-temporal level and also across spatially separated regions since it is in the interactions that more sophisticated scene interpretation can be found. We show that it is possible to spatio-temporally locate and detect salient motion events and interactions in two contrasting scenarios using the same hierarchical co-occurrence framework. Thus generating a concise description of a dynamic scene from the sequence data alone. Results show it is possible to reduce a highly populated multi-dimensional co-occurrence matrix representing correlations between salient motion regions, to a one dimensional vector with clearly separable unusual activity. The results also show that the method inherently provides a quantifiable measure of the saliency of an interaction through its frequency of occurrence.	algorithm;bottom-up parsing;co-occurrence matrix;code refactoring;document-term matrix;high- and low-level;interaction;online and offline;population;sensor;top-down and bottom-up design;while	Hayley Shi-Wen Hung;Shaogang Gong	2005	IEEE Conference on Advanced Video and Signal Based Surveillance, 2005.	10.1109/AVSS.2005.1577241	computer vision;feature extraction;computer science;machine learning;pattern recognition;top-down and bottom-up design;co-occurrence matrix	Vision	38.88797919092094	-50.01229047978473	15901
b3fabc0993f482478c54d56c352ec8c32d385d39	sampling-based sweep planning to exploit local planarity in the inspection of complex 3d structures	robot sensing systems;robot sensing systems inspection probabilistic logic planning path planning;image segmentation;2d sweep paths sampling based sweep planning local planarity complex 3d structures hybrid algorithm path planning sensor coverage planar areas back and forth sweep paths collision avoidance randomized planning procedure traveling salesman problem planning algorithm autonomous underwater vehicle inspection ship hull randomized configurations occluded areas;path planning;automatic optical inspection;inspection;ships;travelling salesman problems automatic optical inspection autonomous underwater vehicles collision avoidance image segmentation ships;autonomous underwater vehicles;travelling salesman problems;planning;collision avoidance;probabilistic logic	We present a hybrid algorithm that plans feasible paths for 100% sensor coverage of complex 3D structures. The structures to be inspected are segmented to isolate planar areas, and back-and-forth sweep paths are generated to view as much of these planar areas as possible while avoiding collision. A randomized planning procedure fills in the remaining gaps in coverage. The problem of selecting an order to traverse the elements of the inspection is solved by reduction to the traveling salesman problem. We present results of the planning algorithm for an autonomous underwater vehicle inspecting the in-water portion of a ship hull. The randomized configurations succeed in observing confined and occluded areas, while the 2D sweep paths succeed in covering the open areas.	automated planning and scheduling;autonomous robot;computer monitor;hybrid algorithm;interpreter (computing);planar graph;randomized algorithm;sampling (signal processing);traverse;travelling salesman problem;waypoint	Brendan Englot;Franz S. Hover	2012	2012 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2012.6386126	planning;computer vision;mathematical optimization;simulation;inspection;computer science;engineering;motion planning;image segmentation;probabilistic logic	Robotics	52.83754233967665	-24.66620274345704	15924
6b6f6a9004302dd79c1ac9449fbc3df0c052f328	real-time head pose estimation using weighted random forests		In this paper we proposed to real-time head pose estimation based on weighted random forests. In order to make real-time and accurate classification, weighted random forests classifier, was employed. In the training process, we calculate accuracy estimation using preselected out-of-bag data. The accuracy estimation determine the weight vector in each tree, and improve the accuracy of classification when the testing process. Moreover, in order to make robust to illumination variance, binary pattern operators were used for preprocessing. Experiments on public databases show the advantages of this method over other algorithm in terms of accuracy and illumination invariance.	3d pose estimation;random forest;real-time clock	Hyunduk Kim;Myoung-Kyu Sohn;Dong-Ju Kim;Nuri Ryu	2014		10.1007/978-3-319-11289-3_56	computer vision;pattern recognition;statistics	Vision	40.708683566323444	-49.59424141970909	15997
819098482013b84043ff5d8b06a88608e4a5f527	terrain traversability analysis methods for unmanned ground vehicles: a survey	mobile robots;unmanned ground vehicles;terrain traversability;survey	Motion planning for unmanned ground vehicles (UGV) constitutes a domain of research where several disciplines meet, ranging from artificial intelligence and machine learning to robot perception and computer vision. In view of the plurality of related applications such as planetary exploration, search and rescue, agriculture, mining and off-road exploration, the aim of the present survey is to review the field of 3D terrain traversability analysis that is employed at a preceding stage as a means to effectively and efficiently guide the task of motion planning. We identify that in the epicenter of all related methodologies, 3D terrain information is used which is acquired from LIDAR, stereo range data, color or other sensory data and occasionally combined with static or dynamic vehicle models expressing the interaction of the vehicle with the terrain. By taxonomizing the various directions that have been explored in terrain perception and analysis, this review takes a step toward agglomerating the dispersed contributions from individual domains by elaborating on a number of key similarities as well as differences, in order to stimulate research in addressing the open challenges and inspire future	artificial intelligence;computer vision;machine learning;motion planning;planetary scanner;unmanned aerial vehicle	Panagiotis Papadakis	2013	Eng. Appl. of AI	10.1016/j.engappai.2013.01.006	mobile robot;computer vision;simulation;computer science;artificial intelligence	AI	52.54272969734273	-33.04157906234356	16000
3312cb3e39ea62e691e5be12f96a2c185fa09cd8	probabilistic landmark based localization of rail vehicles in topological maps	railways;robot sensing systems;rails;sensor system;event triggered counting approach;eddy currents;bayesian approach;radar tracking;hidden markov model;bayes methods;personal transport;sensor integration;eddy current sensor system;train localization;mobile robots;local system;eddy current;autonomic system;flexible sensor integration;bayesian filter approach;topological map;computational modeling;hidden markov models;map association;feature extraction;transportation;bayesian filtering;vehicles;hidden markov models rails vehicles radar tracking robot sensing systems computational modeling feature extraction;velocity estimation;autonomous systems;flexible sensor integration probabilistic landmark based localization rail vehicle topological map logistics transport personal transport autonomous systems eddy current sensor system train localization velocity estimation event triggered counting approach map association bayesian filter approach hidden markov model;filtering theory;probabilistic landmark based localization;rail vehicle;electric sensing devices;logistics transport;transportation bayes methods eddy currents electric sensing devices filtering theory hidden markov models mobile robots railways	Localization of rail vehicles is fundamental for any autonomous systems to perform tasks in logistics or personal transport. This contribution presents a novel onboard localization system, based on an eddy current sensor system (ECS), that is capable of a precise train localization when combined with a simple topological map. In contrary to commonly applied travel distance determination by integrating the estimated velocity, we propose an event triggered counting approach, which makes use of the unique sensor capabilities. Rail switches are chosen as landmarks for a global map association and as reliable start and end points for the counting procedure. They are extracted via a Bayesian filter approach, in particular hidden Markov models are applied for detection and classification. Additional features are modeled in a subsequent step and merged within a topological map employing a naïve Bayesian approach in the spatial domain. This allows for a flexible sensor integration and an easy determination of the most probable vehicle position based on traveled distanced.	algorithm;autonomous robot;autonomous system (internet);bayesian network;global positioning system;hidden markov model;internationalization and localization;logistics;markov chain;naivety;network switch;sensor;sleeper;velocity (software development)	Stefan Hensel;Carsten Hasberg	2010	2010 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2010.5652274	computer vision;electronic engineering;simulation;computer science;engineering;eddy current;hidden markov model	Robotics	53.05115339019182	-34.61891004629568	16020
1642358cd9410abe9ee512d34ba68296b308770e	robustness analysis of pedestrian detectors for surveillance		To obtain effective pedestrian detection results in surveillance video, there have been many methods proposed to handle the problems from severe occlusion, pose variation, clutter background, and so on. Besides detection accuracy, a robust surveillance video system should be stable to video quality degradation by network transmission, environment variation, and so on. In this paper, we conduct the research on the robustness of pedestrian detection algorithms to video quality degradation. The main contribution of this paper includes the following three aspects. First, a large-scale distorted surveillance video data set (DSurVD) is constructed from high-quality video sequences and their corresponding distorted versions. Second, we design a method to evaluate detection stability and a robustness measure called robustness quadrangle, which can be adopted to the visualize detection accuracy of pedestrian detection algorithms on high-quality video sequences and stability with video quality degradation. Third, the robustness of seven existing pedestrian detection algorithms is evaluated by the built DSurVD. Experimental results show that the robustness can be further improved for existing pedestrian detection algorithms. In addition, we provide much in-depth discussion on how different distortion types influence the performance of pedestrian detection algorithms, which is important to design effective pedestrian detection algorithms for surveillance.		Yuming Fang;Guanqun Ding;Yuan Yuan;Weisi Lin;Haiwen Liu	2018	IEEE Access	10.1109/ACCESS.2018.2840329	robustness (computer science);object detection;pedestrian;distributed computing;computer vision;computer science;clutter;video quality;distortion;detector;pedestrian detection;artificial intelligence	Vision	42.74037702822176	-46.07958116233935	16112
c8ea01aa5ebfb90b05e37d249a5c50698502742e	automatic bearing fault diagnosis based on one-class ν-svm	vibration analysis;novelty detection;envelope spectrum;bearing;svm;fault diagnosis	Rolling-element bearings are among the most used elements in industrial machinery, thus an early detection of a defect in these components is necessary to avoid major machine failures. Vibration analysis is a widely used condition monitoring technique for high-speed rotating machinery. Using the information contained in the vibration signals, an automatic method for bearing fault detection and diagnosis is presented in this work. Initially, a one-class @n-SVM is used to discriminate between normal and faulty conditions. In order to build a model of normal operation regime, only data extracted under normal conditions is used. Band-pass filters and Hilbert Transform are then used sequentially to obtain the envelope spectrum of the original raw signal that will finally be used to identify the location of the problem. In order to check the performance of the method, two different data sets are used: (a) real data from a laboratory test-to-failure experiment and (b) data obtained from a fault-seeded bearing test. The results showed that the method was able not only to detect the failure in an incipient stage but also to identify the location of the defect and qualitatively assess its evolution over time.		Diego Fernández-Francos;David Martínez-Rego;Oscar Fontenla-Romero;Amparo Alonso-Betanzos	2013	Computers & Industrial Engineering	10.1016/j.cie.2012.10.013	structural engineering;control engineering;support vector machine;bearing;computer science;engineering;machine learning;vibration;forensic engineering	Robotics	37.17883645512575	-30.688478217972307	16115
1ce6c18f54963b75a369bf20c3e9c0e5f0b2dd88	real-time optimal path planning and wind estimation using gaussian process regression for precision airdrop		This paper presents a time-critical cargo drop strategy that allows a fixed-wing unmanned aerial vehicle (UAV) carrying a cargo under an unknown wind field, to accomplish the cargo drop mission within the least amount of time while minimizing the cargo landing error. Specifically, we treat the spatial wind distribution as a noisy vector field and apply the Gaussian process (GP) regression method to estimate the wind model. In order to optimize the strategy, the objective function to be maximized has been chosen as the weighted sum of two conflicting objectives: more knowledge of the wind field and less travel time. We present some simulation results to compare the performance of the proposed strategy with a conventional method. Results demonstrate the advantage of the proposed method in terms of accuracy and multi-functionality over the non-estimation strategy.	aerial photography;effective method;gaussian process;kriging;loss function;mathematical model;motion planning;optimization problem;real-time clock;simulation;unmanned aerial vehicle;weight function;window of opportunity	Shiyi Yang;Nan Wei;Soo Jeon;Ricardo Bencatel;Anouck R. Girard	2017	2017 American Control Conference (ACC)	10.23919/ACC.2017.7963341	vector field;wind speed;control theory;computer science;motion planning;kriging;gaussian process	Robotics	52.4818860980915	-27.23580448955687	16125
169bb68d1cf40a27516434cf8b3f7ddf274d325f	local statistical modeling via a cluster-weighted approach with elliptical distributions	empirical study;computacion informatica;elliptical distribution;statistical model;statistical properties;cluster weighted modeling;mixture model;ciencias basicas y experimentales;matematicas;settore secs s 01 statistica;point of view;grupo a;mixture models;model based clustering;mixture of distributions	Cluster-weighted modeling (CWM) is a mixture approach to mo deling the joint probability of data coming from a heterogeneous population . Under Gaussian assumptions, we investigate statistical properties of CWM from both theo retical and numerical points of view; in particular, we show that Gaussian CWM includes mixt ures of distributions and mixtures of regressions as special cases. Further, we intro duce CWM based on Studentt distributions, which provides a more robust fit for groups of observations with longer than normal tails or noise data. Theoretical results are illustr ated using some empirical studies, considering both simulated and real data. Some generalizat ions of such models are also outlined.	cluster-weighted modeling;numerical analysis;statistical model;tails	Salvatore Ingrassia;Simona C. Minotti;Giorgio Vittadini	2012	J. Classification	10.1007/s00357-012-9114-3	econometrics;machine learning;mixture model;mathematics;statistics	ML	31.00961474277411	-24.729737131237645	16141
5ed766a21cf2f0e07334b7cd01e3eac3dad6fbff	a fuzzy approach for background subtraction	object detection mathematical model aggregates fuzzy sets wiener filter mathematics cameras background noise lighting uncertainty;moving object;video surveillance;fuzzy approach;color;choquet integral fuzzy approach background subtraction static cameras objects detection;maintenance engineering;indexing terms;statistical model;fuzzy set theory;background maintenance;object detection fuzzy set theory;choquet integral;image color analysis;aggregates;feature extraction;foreground detection;pixel;background subtraction;mathematical model;static cameras;choquet integral background subtraction background maintenance foreground detection;objects detection;object detection	Background Subtraction is a widely used approach to detect moving objects from static cameras. Many different methods have been proposed over the recent years and can be classified following different mathematical model: determinist model, statistical model or filter model. The presence of critical situations i.e. noise, illumination changes and structural background changes introduce two main problems: The first one is the uncertainty in the classification of the pixel in foreground and background. The second one is the imprecision in the localization of the moving object. In this context, we propose a fuzzy approach for background subtraction. For this, we use the Choquet integral in the foreground detection and propose fuzzy adaptive background maintenance. Results show the pertinence of our approach.	background subtraction;fuzzy logic;internationalization and localization;mathematical model;pixel;relevance;statistical model	Fida El Baf;Thierry Bouwmans;Bertrand Vachon	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4712338	maintenance engineering;statistical model;computer vision;index term;background subtraction;feature extraction;computer science;machine learning;pattern recognition;mathematical model;mathematics;fuzzy set;choquet integral;pixel;statistics	Vision	45.33866516101383	-46.61693346346907	16143
7cb3eac464ad6607819f45e920c4784ad631e0b9	boosting performance for 2d linear discriminant analysis via regression	ridge regression;small sample size;image databases;linear discriminate analysis;strontium;boosting linear discriminant analysis covariance matrix face recognition computational efficiency image databases vectors principal component analysis strontium;conference paper;boosting;face recognition;vectors;pairwise distances;regression analysis face recognition;principal component analysis;2d linear discriminant analysis;regression analysis;face recognition 2d linear discriminant analysis pairwise distances ridge regression;computational efficiency;linear discriminant analysis;covariance matrix	Two dimensional linear discriminant analysis (2DLDA) has received much interest in recent years. However, 2DLDA could make pairwise distances between any two classes become significantly unbalanced, which may affect its performance. Moreover 2DLDA could also suffer from the small sample size problem. Based on these observations, we propose two novel algorithms called regularized 2DLDA and Ridge Regression for 2DLDA (RR-2DLDA). Regularized 2DLDA is an extension of 2DLDA with the introduction of a regularization parameter to deal with the small sample size problem. RR-2DLDA integrates ridge regression into Regularized 2DLDA to balance the distances among different classes after the transformation. These proposed algorithms overcome the limitations of 2DLDA and boost recognition accuracy. The experimental results on the Yale, PIE and FERET databases showed that RR-2DLDA is superior not only to 2DLDA but also other state-of-the-art algorithms.	algorithm;boosting (machine learning);database;feret (facial recognition technology);facial recognition system;linear discriminant analysis;principal component analysis;unbalanced circuit	Nam Nguyen;Wanquan Liu;Svetha Venkatesh	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761898	facial recognition system;covariance matrix;strontium;computer science;machine learning;pattern recognition;mathematics;linear discriminant analysis;tikhonov regularization;boosting;elastic net regularization;regression analysis;statistics;principal component analysis	Vision	25.872391805336505	-42.343608447880264	16146
8ecd52728ec93e2a18874c737415321ea55abf68	hand parsing for fine-grained recognition of human grasps in monocular images	shape image segmentation cameras feature extraction robot sensing systems;robot sensing systems;image segmentation;grasp recognition technique fine grained recognition monocular images human hand grasp types human hand use grasp categories fine grained grasp classification data driven learning pixel wise segmentation finger regions palm regions appearance based cues finger texture hand shape supervised fine grained grasp classifier wearable camera automatic hand parsing technique;object recognition image classification image segmentation image sensors image texture learning artificial intelligence manipulators;shape;feature extraction;cameras	We propose a novel method for performing fine-grained recognition of human hand grasp types using a single monocular image to allow computational systems to better understand human hand use. In particular, we focus on recognizing challenging grasp categories which differ only by subtle variations in finger configurations. While much of the prior work on understanding human hand grasps has been based on manual detection of grasps in video, this is the first work to automate the analysis process for fine-grained grasp classification. Instead of attempting to utilize a parametric model of the hand, we propose a hand parsing framework which leverages a data-driven learning to generate a pixel-wise segmentation of a hand into finger and palm regions. The proposed approach makes use of appearance-based cues such as finger texture and hand shape to accurately determine hand parts. We then build on the hand parsing result to compute high-level grasp features to learn a supervised fine-grained grasp classifier. To validate our approach, we introduce a grasp dataset recorded with a wearable camera, where the hand and its parts have been manually segmented with pixel-wise accuracy. Our results show that our proposed automatic hand parsing technique can improve grasp classification accuracy by over 30 percentage points over a state-of-the-art grasp recognition technique.	high- and low-level;parametric model;parsing;pixel;wearable computer	Akanksha Saran;Damien Teney;Kris M. Kitani	2015	2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2015.7354088	computer vision;feature extraction;shape;computer science;machine learning;pattern recognition;image segmentation	Robotics	35.71833841517089	-46.78791524799257	16391
2fd1333c1e2e70b63f0b7fdc2f7a39d7783505ee	fourier tags: smoothly degradable fiducial markers for use in human-robot interaction	degradation;information encoding;image coding;image resolution;application software;human robot interaction;noise robustness;computer vision;bar codes;robot vision;position control;bar code;fourier transforms;user interfaces bar codes fourier transforms identification technology image coding image resolution man machine systems robot vision;identification technology;degradation robot vision systems robot kinematics computer science cameras computer vision encoding noise robustness underwater tracking application software;underwater tracking;effective resolution;fourier tags;computer science;degradable fiducial markers;encoding;user interfaces;man machine systems;robot vision systems;cameras;effective resolution fourier tags degradable fiducial markers human robot interaction information encoding position control bar code;robot kinematics	In this paper we introduce the Fourier tag, a synthetic fiducial marker used to visually encode information and provide controllable positioning. The Fourier tag is a synthetic target akin to a bar-code that specifies multi-bit information which can be efficiently and robustly detected in an image. Moreover, the Fourier tag has the beneficial property that the bit string it encodes has variable length as a function of the distance between the camera and the target. This follows from the fact that the effective resolution decreases as an effect of perspective. This paper introduces the Fourier tag, describes its design, and illustrates its properties experimentally.	barcode;bit array;encode;experiment;fast fourier transform;fiducial marker;human–robot interaction;smoothing;synthetic data;synthetic intelligence	Junaed Sattar;Eric Bourque;Philippe Giguère;Gregory Dudek	2007	Fourth Canadian Conference on Computer and Robot Vision (CRV '07)	10.1109/CRV.2007.34	human–robot interaction;fourier transform;computer vision;application software;simulation;degradation;image resolution;computer science;artificial intelligence;user interface;robot kinematics;encoding;computer graphics (images)	Vision	48.825037870662406	-42.02362361945448	16490
5a3523db70e5a0a24b1a38ea8bd55a25c63972d8	joint human detection and head pose estimation via multistream networks for rgb-d videos		We propose a multistream multitask deep network for joint human detection and head pose estimation in RGB-D videos. To achieve high accuracy, we jointly utilize appearance, shape, and motion information as inputs. Based on the depth information, we generate scale invariant proposals, which are then fed into a novel contextual region of interest pooling (CRP) layer in our deep network. This CRP has two branches to deal with contextual information for each subject. The proposed method outperforms state-of-the-art approaches on three public datasets.	3d pose estimation;computer multitasking;region of interest	Guyue Zhang;Jun Liu;Hengduo Li;Yan Qiu Chen;Larry S. Davis	2017	IEEE Signal Processing Letters	10.1109/LSP.2017.2731952	pattern recognition;artificial intelligence;mathematics;region of interest;pose;pooling;rgb color model;computer vision	Vision	29.984262994854216	-51.16992995945751	16545
c1b718641c37c06a16f2be09e368fc3606ca2b6f	a cmos contact imager for locating individual cells	object recognition;image processing;closed loop systems;object recognition biological techniques cellular biophysics closed loop systems cmos image sensors medical image processing;lab on a chip systems;chip;cmos image sensors;image contrast;cmos contact imager;dynamic threshold;biological cells;lab on a chip signal generators signal design quantization lighting sensor phenomena and characterization instruments optical sensors cmos image sensors optical noise;background illumination;medical image processing;closed loop systems cmos contact imager individual cell location lab on a chip systems image processing image contrast object recognition dynamic threshold background illumination control signals;lab on a chip;biomedical image processing;individual cell location;biological techniques;sample preparation;cellular biophysics;control signals	We describe the design of a contact imager for applications in lab-on-a-chip systems, such as sample preparation and manipulation and monitoring of cells. This is a challenging task because most cells are nearly transparent, so the contrast between the presence and absence of a cell is small. Thus additional image processing is necessary to locate cells. To enhance the image contrast and facilitate object recognition, the contact imager implements on-chip one bit quantization with a dynamic threshold that adapts to the background illumination. The imager is capable of locating dark objects in a bright background or bright objects in a dark background. The locations of recognized cells are generated as outputs to alleviate computational requirements for generating control signals in closed-loop systems	cmos;computation;dark web;image processing;image sensor;outline of object recognition;pixel;requirement;source lines of code	Honghao Ji;David Sander;Alfred Haas;Pamela Abshire	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1693345	chip;embedded system;computer vision;electronic engineering;lab-on-a-chip;telecommunications;image processing;computer science;engineering;cognitive neuroscience of visual object recognition	Embedded	45.48293472365519	-32.78000284764118	16612
644bbb031230dd453a1f0c38ec788191b0c1e86a	non-uniform subset selection for active learning in structured data		Several works have shown that relationships between data points (i.e., context) in structured data can be exploited to obtain better recognition performance. In this paper, we explore a different, but related, problem: how can these inter-relationships be used to efficiently learn and continuously update a recognition model, with minimal human labeling effort. Towards this goal, we propose an active learning framework to select an optimal subset of data points for manual labeling by exploiting the relationships between them. We construct a graph from the unlabeled data to represent the underlying structure, such that each node represents a data point, and edges represent the inter-relationships between them. Thereafter, considering the flow of beliefs in this graph, we choose those samples for labeling which minimize the joint entropy of the nodes of the graph. This results in significant reduction in manual labeling effort without compromising recognition performance. Our method chooses non-uniform number of samples from each batch of streaming data depending on its information content. Also, the submodular property of our objective function makes it computationally efficient to optimize. The proposed framework is demonstrated in various applications, including document analysis, scene-object recognition, and activity recognition.	active learning (machine learning);activity recognition;algorithmic efficiency;data model;data point;information;joint entropy;loss function;optimization problem;outline of object recognition;self-information;streaming media;submodular set function	Sujoy Paul;Jawadul H. Bappy;Amit K. Roy-Chowdhury	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.95	joint entropy;active learning;pattern recognition;data point;submodular set function;feature extraction;computer science;data modeling;activity recognition;machine learning;data model;artificial intelligence	Vision	25.466788331890143	-46.634677869262745	16688
c9b875bc288cf09f89c918164bbd54566e9aa119	on-line construction of iconic maps	alpha backtracking iconic map online construction virtual reality creation virtual map creation a priori unknown environment mobile robot image based map spatial occupancy;mobile robot;iconic map online construction;virtual reality;a priori unknown environment;mobile robots;layout;optical coupling;inspection;virtual map creation;alpha backtracking;mobile robots virtual reality layout image reconstruction visualization robotics and automation humans inspection optical arrays optical coupling;visualization;optical arrays;image reconstruction;virtual reality mobile robots;spatial occupancy;humans;virtual reality creation;robotics and automation;image based map	This paper describes a n approach to the automated creotion of virtual realities (or virtual maps) of a n a priori unknown environment by using a mobile robot. The method we propose is aimed at the creation of a n image-based or iconic map, rather than a representation in terms of 2D or 3 0 spatial occupancy. A key aspect of this is having a mobile robot automatically select points and views of interest that can be used to exemplify the appearance of the environment . This paper develops the use of alpha-backtracking as a technique to eficiently select these points of estimated globally max imum interest.	backtracking;exemplification;map;mobile robot;virtual reality	Eric Bourque;Gregory Dudek	2000		10.1109/ROBOT.2000.846371	mobile robot;computer vision;simulation;computer science;engineering;artificial intelligence;virtual reality;computer graphics (images)	Robotics	50.118561218371454	-39.030933499382336	16704
8adf488d19cfb84223cca1a09bddb1cf8bc9980a	pseudo-linear regression identification based on generalized orthonormal transfer functions: convergence conditions and bias distribution		In this paper we generalize three identification recursive algorithms belonging to the pseudo-linear class, by introducing a predictor established on a generalized orthonormal function basis. Contrary to the existing identification schemes that use such functions, no constraint on the model poles is imposed. Not only this predictor parameterization offers the opportunity to relax the convergence conditions of the associated recursive schemes, but it entails a modification of the bias distribution linked to the basis poles. This result is specific to pseudo-linear regression properties, and cannot be transposed to most of prediction error method algorithms. A detailed bias distribution is provided, using the concept of equivalent prediction error, which reveals strong analogies between the three proposed schemes, corresponding to ARMAX, Output Error and a generalization of ARX models. That leads to introduce an indicator of the basis poles location effect on the bias distribution in the frequency domain. As shown by the simulations, the said basis poles play the role of tuning parameters, allowing to manage the model fit in the frequency domain, and allowing efficient identification of fast sampled or stiff discrete-time systems.	algorithm;arx;distortion;kerrison predictor;monte carlo method;recursion;simulation;stiff equation;transfer function	Bernard Vau;Henri Bourlès	2018	CoRR			ML	28.903640608179042	-25.360071876749515	16830
6d75f0944682c18e96049be4bbe865563920b6d3	chemical concentration map building through bacterial foraging optimization based search algorithm by mobile robots	robot sensing systems;optimisation;electronic mail;map building;mobile robot;bismuth;real time;search algorithm;transducers;chemical analysis;mobile robots;bacterial foraging optimization algorithm;base station;area measurement;search problems;remote computer chemical gas concentration map bacterial foraging optimization search algorithm mobile robot gas sensing khenose smart transducer bacterial chemotactic behavior sensor reading;transducers chemical analysis chemical variables measurement gas sensors mobile robots optimisation search problems;gas sensors;fires;robot sensing systems area measurement electronic mail bismuth fires;chemical variables measurement	In this article we present implementation of Bacterial Foraging Optimization algorithm inspired search by multiple robots in an unknown area in order to find the region with highest chemical gas concentration as well as to build the chemical gas concentration map. The searching and map building tasks are accomplished by using mobile robots equipped with smart transducers for gas sensing called “KheNose”. Robots perform the search autonomously via bacterial chemotactic behavior. Moreover, simultaneously the robots send their sensor readings of the chemical concentration and their position data to a remote computer (a base station), where the data is combined, interpolated, and filtered to form an real-time map of the chemical gas concentration in the environment.	interpolation;mathematical optimization;mobile robot;real-time clock;remote computer;search algorithm;transducer	Mirbek Turduev;Murat Kirtay;Pedro Angelo Morais de Sousa;Veysel Gazi	2010	2010 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2010.5642297	mobile robot;simulation;computer science;artificial intelligence	Robotics	53.65403571516465	-29.612572994518455	17050
abcbb2b180593bd66cf51ae1fc38e6e73473f617	new classifier based on compressed dictionary and ls-svm	ls svm;restricted isometry property;compressed dictionary;sparsity;compressive sensing	Inspired by the compressive sensing (CS) theory, a new classifier based on compressed dictionary and Least Squares Support Vector Machine (LS-SVM) is proposed to deal with large scale problems. The coefficients of support vectors can be recovered from a few measurements if LS-SVM is approximated to sparse structure. Using the known Cholesky decomposition, we approximate the given kernel matrix to represent the coefficients of support vectors sparsely by a low-rank matrix that we have used as a dictionary. The proposed measurement matrix being coupled with the dictionary forms a compressed dictionary that proves to satisfy the restricted isometry property (RIP). Our classifier has the quality of low storage and computational complexity, high degree of sparsity and information preservation. Experiments on benchmark data sets show that our classifier has positive performance.	dictionary;least squares	Chen Sun;Licheng Jiao;Hongying Liu;Shuyuan Yang	2016	Neurocomputing	10.1016/j.neucom.2016.08.024	speech recognition;k-svd;machine learning;restricted isometry property;pattern recognition;mathematics;compressed sensing;sparsity-of-effects principle;statistics	HCI	26.217736199989154	-39.523391588170206	17091
ff39119facecfd7f0c8ab91a488723e250e6f613	bayesian analysis of generalized partially linear single-index models	free knot spline;gibbs sampler;dimension reduction;free knot splines;variable selection;reversible jump markov chain monte carlo;generalized linear model;article;single index model;overdispersion;regression splines	We extend generalized partially linear single-index models by incorporating a random residual effect into the nonlinear predictor so that the newmodels can accommodate data with overdispersion. Based on the free-knot spline techniques, we develop a fully Bayesian method to analyze the proposedmodels. Tomake themodels spatially adaptive, we further treat the number and positions of spline knots as random variables. As random residual effects are introduced, many of the completely conditional posteriors become standard distributions, which greatly facilitates sampling. We illustrate the proposed models and estimation method with a simulation study and an analysis of a recreational trip data set. © 2013 Elsevier B.V. All rights reserved.	b-spline;coefficient;kerrison predictor;nonlinear system;radial (radio);radial basis function;reversible-jump markov chain monte carlo;sampling (signal processing);simulation;single-index model;spline (mathematics);wavelet	Wai-Yin Poon;Hai-Bin Wang	2013	Computational Statistics & Data Analysis	10.1016/j.csda.2013.07.018	econometrics;generalized linear mixed model;mathematical optimization;quasi-likelihood;gibbs sampling;single-index model;multivariate adaptive regression splines;machine learning;generalized linear model;mathematics;feature selection;overdispersion;statistics;dimensionality reduction	AI	29.930684574320416	-24.32912506046245	17093
6f0375277f7a11f693c8a14c94a64dac9abc225a	determining motion parameters using intensity guided range sensing	point correspondence;motion parameter;graph-matching;motion parameters;range image;feature extraction;intensity image	Abstract   This paper describes a method for the direct computation of rotation and translation parameters of an object using multisensory data gathered from intensity and range sensors. Intensity images which can be gathered rapidly are used as the basis for guiding the extraction of a small set of correspondent points whose three-dimensional coordinates can be determined by a range sensing device. Library-model centered motion transformations are computed using a least squares approach. Object centered motion parameters are derived by computing the transformation between successive positions of the object. Results are presented for range and intensity images of synthetic and laboratory scenes		Jake K. Aggarwal;Michael J. Magee	1986	Pattern Recognition	10.1016/0031-3203(86)90021-X	computer vision	Vision	51.316284052442896	-39.035933183911546	17129
42adc46c96dfde8e3bada012ce2342a465fbe3c1	on scalable inference with stochastic gradient descent		In many applications involving large dataset or online updating, stochastic gradient descent (SGD) provides a scalable way to compute parameter estimates and has gained increasing popularity due to its numerical convenience and memory efficiency. While the asymptotic properties of SGD-based estimators have been established decades ago, statistical inference such as interval estimation remains much unexplored. The traditional resampling method such as the bootstrap is not computationally feasible since it requires to repeatedly draw independent samples from the entire dataset. The plug-in method is not applicable when there are no explicit formulas for the covariance matrix of the estimator. In this paper, we propose a scalable inferential procedure for stochastic gradient descent, which, upon the arrival of each observation, updates the SGD estimate as well as a large number of randomly perturbed SGD estimates. The proposed method is easy to implement in practice. We establish its theoretical properties for a general class of models that includes generalized linear models and quantile regression models as special cases. The finite-sample performance and numerical utility is evaluated by simulation studies and two real data applications.	bootstrapping (statistics);generalized linear model;inferential theory of learning;interval arithmetic;numerical analysis;online algorithm;plug-in (computing);randomness;scalability;simulation;software studies;stochastic gradient descent	Yixin Fang;Jinfeng Xu;Lei Yang	2017	CoRR		mathematical optimization;mathematics;statistical inference;covariance matrix;resampling;estimator;stochastic gradient descent;inference;interval estimation;backpropagation	ML	27.752995369406555	-28.111625506364394	17170
d9cf4099cd1553ed9bc14d534e8cc91243382c03	studies of the adaptive network-constrained linear regression and its application	p e ratio;adaptive network penalty;variable selections	The network-constrained criterion is one of the fundamental variable selection models for high-dimensional datawith correlated features. It is distinguished fromothers in that it can select features and simultaneously encourage global smoothness of the coefficients over the network via penalizing the weighted sum of squares of the scaled difference of the coefficients between neighbor vertices. However, becausemore features were selected while it was applied for the process of analysis of the ‘‘China Stock Market Financial Database— Financial Ratios’’, the so-called adaptive network-constrained criterion was proposed to tackle the problem via assigning various weights to the lasso penalty. Similar to the adaptive lasso, the proposed model enjoys consistency in variable selection if the weights have been given correctly in advance. The simulations show that the proposedmodel performed better than the other variable selection techniques mentioned in the paper with regards to model fitting; meanwhile, it selected fewer features than the network-constrained criterion. Furthermore, the mean value of the cross-validation likelihood and the number of selected features are tested to be accurate enough for practical applications. © 2015 Elsevier B.V. All rights reserved.	coefficient;cross-validation (statistics);curve fitting;feature selection;lasso;simulation;weight function	Hu Yang;Danhui Yi	2015	Computational Statistics & Data Analysis	10.1016/j.csda.2015.06.008	econometrics;mathematical optimization;mathematics;statistics;price–earnings ratio	AI	27.423680394091754	-24.88784643079752	17179
d42a205eefb3baef90f8c39c4cf2505479718e05	motion of an uncalibrated stereo rig: self-calibration and metric reconstruction	metric reconstruction;epipolar constraint;geometry;motion estimation;testing;stereo image sequence uncalibrated stereo rig self calibration metric reconstruction motion estimation epipolar constraint squared distances sum redundancy;redundancy;algebra;matrix decomposition;retina;image reconstruction;stereo image sequence;stereo image processing;sum of squares;squared distances sum;self calibration;robustness;calibration;cameras image reconstruction matrix decomposition stereo image processing retina calibration robustness testing geometry algebra;cameras;uncalibrated stereo rig	We address in this paper the problem of self-calibration and metric reconstruction (up to a scale) from one unknown motion of an uncalibrated stereo rig. The epipolar constraint is first formulated for two uncalibrated images. The problem then becomes one of estimating unknowns such that the discrepancy from the epipolar constraint, in terms of sum of squared distances between points and their corresponding epipolar lines, is minimized. Redundancy of the information contained in a sequence of stereo images makes this method more robust than using a sequence of monocular images. Real data have been used to test the proposed method, and the results obtained are quite good.		Zhengyou Zhang;Quang-Tuan Luong;Olivier Faugeras	1994		10.1109/ICPR.1994.576407	iterative reconstruction;computer vision;mathematical optimization;calibration;computer science;motion estimation;mathematics;geometry;software testing;fundamental matrix;explained sum of squares;redundancy;matrix decomposition;robustness;epipolar geometry	Vision	53.5137180919457	-50.12963912728246	17206
6ae65d17309535ea856b27b977ecb85df73f17e3	model-based fault diagnosis of a planetary gear: a novel approach using transmission error	aerospace engineering;vibrations;gears;feature extraction;sun;fault diagnosis	Extensive prior studies aimed at the development of diagnostic methods for planetary gearboxes have mainly examined acceleration and acoustic emission signals. Recently, due to the relationship between gear mesh stiffness and transmission error (TE), TE-based techniques have emerged as a promising way to analyze dynamic behavior of spur and helical gears. However, to date, TE has not been used as a measure to detect faults in planetary gears. In this paper, we propose a new methodology for model-based fault diagnostics of planetary gears using TE signals. A lumped parametric model of planetary gear dynamics was built to extract simulated TE signals, while accounting for the planet phasing effect, which is a peculiar characteristic of the planetary gear. Next, gear dynamic analysis was performed using TE signals, and TE-based damage features were calculated from the processed TE signals to quantitatively represent health conditions. The procedures described aforesaid were then applied to a case study of a planetary gear in a wind turbine gear train. From the results, we conclude that TE signals can be used to detect the faults, while enhancing understanding of the complex dynamic behaviors of planetary gears.	acoustic cryptanalysis;parametric model;planetary scanner;test engineer	Jungho Park;Jong Moon Ha;Hyun-Seok Oh;Byeng Dong Youn;Joo Ho Choi;Nam Ho Kim	2016	IEEE Transactions on Reliability	10.1109/TR.2016.2590997	structural engineering;feature extraction;gear;engineering;vibration;forensic engineering	Robotics	37.92121829012923	-30.469451002203968	17209
068662b9e4177e835ce8aa61b25de2b8026138e0	sparse-smooth regularized singular value decomposition	splines;fmri;svd;62h25;wavelets	We consider penalized singular value decomposition (SVD) for a (noisy) data matrix when the left singular vector has a sparse structure and the right singular vector is a discretized function. Such situations typically arise from spatio-temporal data where only some small spatial regions are ''activated'' as in fMRI data. We use two penalties that impose sparsity and smoothness. However, it is shown, somewhat surprisingly, that the value of only one parameter has to be chosen. This is in stark contrast to the penalized SVD models proposed by Huang et al. (2009) [12] and by Lee et al. (2010) [14]. We carry out some simulation studies and use an artificial fMRI data set and a real data set to illustrate the proposed approach.	singular value decomposition;sparse	Zhaoping Hong;Heng Lian	2013	J. Multivariate Analysis	10.1016/j.jmva.2013.02.011	spline;wavelet;econometrics;mathematical optimization;mathematics;singular value decomposition;statistics	Vision	26.590223795242615	-34.4400242866612	17256
5433158fe7e20d466106faae9bc6adb4b80ef71e	hyperspectral target detection using kernel matched subspace detector	eigenvalues and eigenfunctions;hyperspectral imagery;generalized likelihood ratio test;image matching;signal detection;object detection kernel hyperspectral sensors detectors testing hyperspectral imaging pixel vectors principal component analysis laboratories;feature space;object detection geophysical signal processing image representation eigenvalues and eigenfunctions image matching;geophysical signal processing;hyperspectral digital imagery collection experiment hyperspectral target detection kernel matched subspace detector nonlinear realization generalized likelihood ratio test dimensional feature space kernel eigenvector representation;image representation;linear model;target detection;object detection;eigenvectors	In this paper we present a nonlinear realization of a subspace signal detection approach based on the generalized likelihood ratio test (GLRT) - so called matched subspace detectors (MSD). The linear model for MSD is first extended to a high, possibly infinite, dimensional feature space and then the corresponding nonlinear GLRT expression is obtained. In order to address the intractability of the GLRT in the nonlinear feature space we kernelize the nonlinear GLRT using kernel eigenvector representations as well as the kernel trick where dot products in the nonlinear feature space are implicitly computed by kernels. The proposed kernel-based nonlinear detector, so called kernel matched subspace detector (KMSD), is applied to a given hyperspectral imagery - HYDICE (hyperspectral digital imagery collection experiment) images - to detect targets of interest. KMSD showed superior detection performance over MSD for the HYDICE images tested in this paper.	detection theory;feature vector;kernel (operating system);kernel method;linear model;nonlinear system;sensor	Heesung Kwon;Nasser M. Nasrabadi	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1421826	computer vision;feature vector;likelihood-ratio test;eigenvalues and eigenvectors;hyperspectral imaging;machine learning;linear model;pattern recognition;mathematics;statistics;detection theory	Robotics	31.337645727342032	-41.79769853409576	17282
30d0811d298c0e521b7ae5fb08f05a89f86b63ba	fast maximum likelihood estimation using continuous-time neural point process models	continuous time;point process;gaussian quadrature;fast;neural spiking;statistical analysis;estimation	A recent report estimates that the number of simultaneously recorded neurons is growing exponentially. A commonly employed statistical paradigm using discrete-time point process models of neural activity involves the computation of a maximum-likelihood estimate. The time to computate this estimate, per neuron, is proportional to the number of bins in a finely spaced discretization of time. By using continuous-time models of neural activity and the optimally efficient Gaussian quadrature, memory requirements and computation times are dramatically decreased in the commonly encountered situation where the number of parameters p is much less than the number of time-bins n. In this regime, with q equal to the quadrature order, memory requirements are decreased from O(n p) to O(q p), and the number of floating-point operations are decreased from O(n p 2) to O(q p 2). Accuracy of the proposed estimates is assessed based upon physiological consideration, error bounds, and mathematical results describing the relation between numerical integration error and numerical error affecting both parameter estimates and the observed Fisher information. A check is provided which is used to adapt the order of numerical integration. The procedure is verified in simulation and for hippocampal recordings. It is found that in 95 % of hippocampal recordings a q of 60 yields numerical error negligible with respect to parameter estimate standard error. Statistical inference using the proposed methodology is a fast and convenient alternative to statistical inference performed using a discrete-time point process model of neural activity. It enables the employment of the statistical methodology available with discrete-time inference, but is faster, uses less memory, and avoids any error due to discretization.	computation;discretization;estimated;fisher information;float;gaussian quadrature;hl7publishingsubsection <operations>;inference;mathematics;neuron;neurons;normal statistical distribution;numerical analysis;numerical error;numerical integration;phenylephrine hydrochloride 10 mg oral tablet;point process;population parameter;process modeling;programming paradigm;requirement;simulation	Kyle Q. Lepage;Christopher J. MacDonald	2015	Journal of Computational Neuroscience	10.1007/s10827-015-0551-y	discretization error;econometrics;mathematical optimization;estimation;gaussian quadrature;point process;mathematics;statistics	ML	25.98955868946971	-24.790569231219962	17357
e18e2c51a502156197bf35c196e02a8e538ae21e	guest editorial open discussion of robot grasping benchmarks, protocols, and metrics		Automated grasping has a long history of research that is increasing due to interest from industry. One Grand Challenge for robotics is Universal Picking: the ability to robustly grasp a broad variety of objects in diverse environments for applications from warehouses to assembly lines to homes. Although many researchers now openly share code and data, it is challenging to compare and/or reproduce experimental results to identify which aspects of which approaches work best due to variations in assumptions and experimental protocols, e.g., sensors, lighting, robot arms, grippers, and objects. In computer vision, the emergence of specific reproducible benchmarks advanced the field considerably and provide “gradients” that expose gaps in the state of the art. With physical experiments, however, performance depends crucially on the hardware and environment and it is not possible for every lab to experiment with the exact same conditions. Nor is exact uniformity desirable as methods must ultimately work across various sensors, robot arms, grippers, object sets, and environments. Industrial practitioners characterize picking in terms of the three R’s: rate, reliability, and range (class of objects). One metric for comparison is Mean Picks Per Hour (MPPH), which is common in the logistics industry where it is recognized that human workers can operate in the range of 400-600 MPPH for warehousing operations [1]∗. This can be formalized as:	circuit complexity;coat of arms;computer vision;darpa grand challenge;emergence;experiment;gradient;logistics;robot;robotics;sensor	Jeffrey Mahler;Robert Platt;Alberto Rodriguez;Matei T. Ciocarlie;Aaron M. Dollar;Renaud Detry;Máximo A. Roa;Holly A. Yanco;Adam Norton;Joe Falco;Karl Van Wyk;Elena Messina;Jürgen Leitner;Douglas Morrison;Matthew T. Mason;Oliver Brock;Lael Odhner;Andrey Kurenkov;Matthew Matl;Kenneth Y. Goldberg	2018	IEEE Trans. Automation Science and Engineering	10.1109/TASE.2018.2871354	grippers;human–computer interaction;robot;grasp;computer science;robotics;artificial intelligence	Robotics	48.06396216446993	-30.622316184917082	17365
58624126246b2132e3c2934a82e619238f7ce24b	optimization of two bottleneck programs in sar system on gpgpu		The Synthetic Aperture Radar (SAR) system is a kind of modern high-resolution microwave imaging radar used in all-weather and all day long to provide remote sensing means and generate high resolution images of the land under illumination of radar beam. Unlike optical sensors, SAR algorithm needs a post-processing process on the data acquired to form the final image. In this article, we use the General Purpose Graphic Processing Units (GPGPU) to accelerate two of SAR algorithms, PGA (Phase Gradient Autofocus) and PDE (Partial Differential Equations), which are two computational intensive algorithms in the post-processing process for the system. Our work shows that the GPU architecture has different acceleration effects on the two algorithms. PGA can achieve an acceleration of 21.7% and PDE can get a speed up of 2.58(times ) on GPGPU. We analyse the reasons for the results and conclude that GPU is a promising platform to accelerate the SAR system.	general-purpose computing on graphics processing units	Yonghui Zhang;Zuocheng Xing;Cang Liu;Chuan Tang;Lirui Chen;Qinglin Wang	2016		10.1007/978-981-10-3159-5_11	computer vision;acceleration;autofocus;architecture;synthetic aperture radar;radar;speedup;general-purpose computing on graphics processing units;microwave imaging;artificial intelligence;computer science	Logic	41.70939722200714	-32.832990855347816	17386
bc88c7177982a213829f09912dfc822da81da09f	hupba8k+: dataset and ecoc-graph-cut based segmentation of human limbs	ecoc;human limb segmentation;graph cuts	Human multi-limb segmentation in RGB images has attracted a lot of interest in the research community because of the huge amount of possible applications in fields like Human–Computer Interaction, Surveillance, eHealth, or Gaming. Nevertheless, human multi-limb segmentation is a very hard task because of the changes in appearance produced by different points of view, clothing, lighting conditions, occlusions, and number of articulations of the human body. Furthermore, this huge pose variability makes the availability of large annotated datasets difficult. In this paper, we introduce the  HuPBA 8 k + dataset. The dataset contains more than 8000 labeled frames at pixel precision, including more than 120 000 manually labeled samples of 14 different limbs. For completeness, the dataset is also labeled at frame-level with action annotations drawn from an 11 action dictionary which includes both single person actions and person–person interactive actions. Furthermore, we also propose a two-stage approach for the segmentation of human limbs. In the first stage, human limbs are trained using cascades of classifiers to be split in a tree-structure way, which is included in an Error-Correcting Output Codes (ECOC) framework to define a body-like probability map. This map is used to obtain a binary mask of the subject by means of GMM color modelling and Graph-Cuts theory. In the second stage, we embed a similar tree-structure in an ECOC framework to build a more accurate set of limb-like probability maps within the segmented user mask that are fed to a multi-label Graph-Cut procedure to obtain final multi-limb segmentation. The methodology is tested on the novel  HuPBA 8 k + dataset, showing performance improvements in comparison to state-of-the-art approaches. In addition, a baseline of standard action recognition methods for the 11 actions categories of the novel dataset is also provided.	cut (graph theory)	Daniel Sánchez;Miguel Ángel Bautista;Sergio Escalera	2015	Neurocomputing	10.1016/j.neucom.2014.07.069	computer vision;cut;machine learning;data mining;scale-space segmentation	Vision	31.84244781416333	-51.01015992667073	17393
d66613cdb22a7b404e41c6d5483967cc3fb099f0	decomposition and extraction: a new framework for visual classification	feature extraction visualization image decomposition kernel pipelines object recognition image edge detection	In this paper, we present a novel framework for visual classification based on hierarchical image decomposition and hybrid midlevel feature extraction. Unlike most midlevel feature learning methods, which focus on the process of coding or pooling, we emphasize that the mechanism of image composition also strongly influences the feature extraction. To effectively explore the image content for the feature extraction, we model a multiplicity feature representation mechanism through meaningful hierarchical image decomposition followed by a fusion step. In particularly, we first propose a new hierarchical image decomposition approach in which each image is decomposed into a series of hierarchical semantical components, i.e, the structure and texture images. Then, different feature extraction schemes can be adopted to match the decomposed structure and texture processes in a dissociative manner. Here, two schemes are explored to produce property related feature representations. One is based on a single-stage network over hand-crafted features and the other is based on a multistage network, which can learn features from raw pixels automatically. Finally, those multiple midlevel features are incorporated by solving a multiple kernel learning task. Extensive experiments are conducted on several challenging data sets for visual classification, and experimental results demonstrate the effectiveness of the proposed method.	entity name part qualifier - adopted;experiment;feature extraction;feature learning;multiple kernel learning;multistage amplifier;pixel;statistical classification;visual basic[.net];multiplicity	Yuqiang Fang;Qiang Chen;Lin Sun;Bin Dai;Shuicheng Yan	2014	IEEE Transactions on Image Processing	10.1109/TIP.2014.2330792	computer vision;feature detection;feature extraction;computer science;machine learning;pattern recognition;feature	Vision	26.971494502342715	-46.805953523625476	17459
0574c83b9582ba184ac7078b9bdd75d65c92cc93	a multi-sensor fusion system for moving object detection and tracking in urban driving environments	radar tracking laser radar cameras vehicles sensor phenomena and characterization;object tracking automobiles feature selection image classification image fusion mobile robots object detection object recognition;multisensor fusion system autonomous car movement classification data association tracking model selection visual recognition information vision sensor darpa urban challenge urban driving environments moving object tracking moving object detection	A self-driving car, to be deployed in real-world driving environments, must be capable of reliably detecting and effectively tracking of nearby moving objects. This paper presents our new, moving object detection and tracking system that extends and improves our earlier system used for the 2007 DARPA Urban Challenge. We revised our earlier motion and observation models for active sensors (i.e., radars and LIDARs) and introduced a vision sensor. In the new system, the vision module detects pedestrians, bicyclists, and vehicles to generate corresponding vision targets. Our system utilizes this visual recognition information to improve a tracking model selection, data association, and movement classification of our earlier system. Through the test using the data log of actual driving, we demonstrate the improvement and performance gain of our new tracking system.	autonomous car;correspondence problem;darpa grand challenge (2007);data logger;image sensor;model selection;object detection;radar;tracking system	Hyunggi Cho;Young-Woo Seo;B. V. K. Vijaya Kumar;Ragunathan Rajkumar	2014	2014 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2014.6907100	computer vision;simulation;tracking system;video tracking;remote sensing	Robotics	48.886376222510535	-36.449324595134385	17480
3ad67c6d2d7c9eeb87ec1121ccdb2067d518a7b4	on the absolute orientation problem in computer vision	polar decomposition absolute orientation least squares;rigid body;mathematical structure;image matching;computer vision singular value decomposition transmission line matrix methods cybernetics military computing laboratories image segmentation quaternions object recognition least squares methods;singular value decomposition;line segments;absolute orientation problem;polar decomposition;data mining;polar decomposition absolute orientation problem computer vision line segments singular value decomposition quaternions 2d objects mathematical structure;computer vision;least squares;2d objects;matrix decomposition;three dimensional displays;fast algorithm;least square;singular value decomposition computer vision image matching;absolute orientation;optimization;transmission line matrix methods;quaternions;noise;data models	An important problem in computer vision is to determine the orientation of a rigid body in an image. This can be accomplished by matching points or line segments that naturally appear on the object. Several elegant and computationally fast algorithms based on the singular value decomposition and quaternions have been introduced to solve this problem. In this article, the authors first examine the important special case of identifying the attitude of 2D objects and introduce a particularly elegant solution based on the mathematical structure of the complex plane. Motivated by this simple solution to the 2D case, a new derivation of the 3D case based on the polar decomposition is presented. This derivation is in many ways more natural than previous derivations, particularly when the model and data contain no noise.	algorithm;computer vision;image noise;mathematical structure;natural proof;singular value decomposition;time complexity	Rodney G. Roberts;Daniel W. Repperger	2009	2009 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2009.5346717	computer vision;mathematical optimization;mathematics;geometry;least squares	Vision	52.65589014283096	-50.86287748004954	17494
d986aa89b1f72e69bec2340f7fad5f8109215661	human action recognition via multi-scale 3d stationary wavelet analysis	three dimensional displays wavelet transforms feature extraction legged locomotion wavelet analysis computer vision;wavelet analysis;human action recognition 3d stationary wavelet transform multi scale analysis spatio temporal representation;legged locomotion;computer vision;wavelet transforms;three dimensional displays;feature extraction;wavelet transforms computer vision image motion analysis image recognition;view based multiscale spatio temporal representation human action recognition multiscale 3d stationary wavelet analysis computer vision	Multi-scale methods, especially wavelets, are being used in various computer vision applications, including surveillance, robotics, and human-centered computing. Human action recognition is one of the core areas that dominate the aforementioned applications. In this paper, the 3D multi-scale stationary wavelet analysis is used to build a view-based multi-scale spatio-temporal representation of the human actions. The proposed representation benefits from the ability of the 3D stationary wavelet transform to fuse the spatio-temporal information highlighted at different scales and orientations. Experimental results using Weizmann and KTH datasets revealed a good performance in various scenarios with different conditions.	computer vision;human-centered computing;robotics;stationary process;stationary wavelet transform	Maryam N. Al-Berry;Hala Mousher Ebied;Ashraf Saad Hussein;Mohamed F. Tolba	2014	2014 14th International Conference on Hybrid Intelligent Systems	10.1109/HIS.2014.7086208	wavelet;computer vision;second-generation wavelet transform;continuous wavelet transform;computer science;machine learning;pattern recognition;discrete wavelet transform;lifting scheme	Robotics	36.42019840521	-50.5304390373165	17517
e780534bf0b9a69a19da7fb0e062909060d5af23	spatially precise contextual features based on superpixel neighborhoods for land cover mapping with high resolution satellite image time series		High resolution image time series as those provided by Sentinel-2 allow to target semantically rich nomenclatures for land cover mapping. However, at 10 m resolution, pixel based classification fails to correctly identify some classes for which pixel context is discriminative. Recent advances in deep convolutional neural networks show promising results to tackle this problem, but the lack of complete annotation over large areas, the computational cost and the dimensionality of the feature space (much larger than those used in computer vision) does not allow to use these approaches in operational mapping applications yet. Contextual information can be calculated by applying a fixed-size neighborhood filter, but this can cause the loss of linear objects and the rounding of sharp corners. In Object Based Image Analysis, segmentation is used to extract objects for calculating contextual features while maintaining the high-frequency elements in the image. However, these do not necessarily include spectrally diverse pixels in a neighborhood, which can be relevant for characterizing the context. Superpixels place themselves in between the fixed-neighborhood and the object-based methods, in that they include spectrally diverse pixels in the same segment by imposing size and compacity constraints, while remaining adaptive to the natural boundaries in the image. This study assesses and compares the ability of these three types of neighborhood to improve classification performance on context-dependent classes, in a high-resolution Sentinel-2 time series land cover mapping problem.	algorithmic efficiency;artificial neural network;computer vision;context-sensitive language;convolutional neural network;feature vector;high-resolution scheme;image analysis;image resolution;object-based language;pixel;rounding;time series	Dawa Derksen;Jordi Inglada;Julien Michel	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518961	convolutional neural network;computer vision;pixel;image segmentation;feature vector;feature extraction;artificial intelligence;satellite image time series;computer science;curse of dimensionality;image resolution	Vision	31.35290458523018	-46.33284942209469	17547
9690bd51597c3c1358f605f3fcbedd50f28fca7f	multi-object tracking based on a modular knowledge hierarchy	computer vision;a priori knowledge;modular system;object tracking	An important goal of research in computer vision systems is to develop architectures which are general and robust and at the same time transparent and easily transferable from one domain to another. To this extent this paper discusses and demonstrates the versatility of a multi-object tracking framework based on the so called knowledge hierarchy. The systematic description and analysis of a priori knowledge provides means not only for reducing the complexity of the multi-object tracking problem but also for building modular systems for solving it. The modularity of the framework, an essential ingredient for versatility, allows to replace individual parts of an existing system without altering the rest of the system or the overall architecture. The paper presents the modular framework including the knowledge hierarchy for multi object tracking. In order to demonstrate the transferability of the proposed approach the tracking framework is then applied to three different tracking scenarios (parking lot surveillance, people interaction monitoring, and dining table setup).	computer vision;dikw pyramid;fits;modular programming;tracking system	Martin Spengler;Bernt Schiele	2003		10.1007/3-540-36592-3_36	computer vision;a priori and a posteriori;simulation;computer science;artificial intelligence;video tracking	Vision	46.607454917220046	-38.481773330461955	17585
832224f6eb46bef6adea44972ea73d2fdce7521a	learning pre-attentive driving behaviour from holistic visual features	image features;visual features;false positive	The aim of this paper is to learn driving behaviour by associating the actions recorded from a human driver with pre-attentive visual input, implemented using holistic image features (GIST). All images are labelled according to a number of driving–relevant contextual classes (eg, road type, junction) and the driver’s actions (eg, braking, accelerating, steering) are recorded. The association between visual context and the driving data is learnt by Boosting decision stumps, that serve as input dimension selectors. Moreover, we propose a novel formulation of GIST features that lead to an improved performance for action prediction. The areas of the visual scenes that contribute to activation or inhibition of the predictors is shown by drawing activation maps for all learnt actions. We show good performance not only for detecting driving–relevant contextual labels, but also for predicting the driver’s actions. The classifier’s false positives and the associated activation maps can be used to focus attention and further learning on the uncommon and difficult situations.	activation function;computer vision;gist;holism;map;sensor;visual descriptor	Nicolas Pugeault;Richard Bowden	2010		10.1007/978-3-642-15567-3_12	computer vision;type i and type ii errors;computer science;machine learning;feature	Vision	31.32800552418377	-49.679555273946775	17662
a8a71d1fbf4d877a10b7949a4b8284f0a838d49c	improving motion planning in weakly connected configuration spaces	probabilistic motion planner methods;path planning;configuration space;connectors;multi rrt;weakly connected configuration spaces;robots planning connectors joining processes sampling methods software algorithms path planning;robot path planning;robots;motion planning;joining processes;software algorithms;planning;multi rrt probabilistic motion planner methods weakly connected configuration spaces robot path planning visibility prm;sampling methods;robots path planning;visibility prm	Even if the probabilistic motion planner methods (PRM or RRT) have been successful for robot path planning, it remains a challenge in a constrained cases with narrow passages. The RRT is a powerful tool for a simple request, but the performances of approaches falls sharply when the search has several narrow passages. The introduction of several trees can reduce this problem, but has the disadvantage of requiring the control of the number and the growth of these trees. However, this can be done using the properties of the Visibility-PRM. Combining the ideas of the Visibility-PRM with a multi-RRT (local trees), a new algorithm is presented and experimental results show the importance and effectiveness of the method.	algorithm;connected component (graph theory);connectivity (graph theory);image resolution;motion planning;online and offline;performance;probabilistic roadmap;statistical relational learning;succession	David Flavigné;Michel Taïx	2010	2010 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2010.5650612	computer vision;mathematical optimization;simulation;computer science;artificial intelligence;mathematics;motion planning	Robotics	52.4434361183612	-24.015191184121914	17665
a0ffe27b37f658e049ee10d2c1ed402b9ae3fe5f	co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation		Skeleton-based human action recognition has recently drawn increasing attentions with the availability of large-scale skeleton datasets. The most crucial factors for this task lie in two aspects: the intra-frame representation for joint co-occurrences and the inter-frame representation for skeletons’ temporal evolutions. In this paper we propose an end-to-end convolutional co-occurrence feature learning framework. The co-occurrence features are learned with a hierarchical methodology, in which different levels of contextual information are aggregated gradually. Firstly point-level information of each joint is encoded independently. Then they are assembled into semantic representation in both spatial and temporal domains. Specifically, we introduce a global spatial aggregation scheme, which is able to learn superior joint co-occurrence features over local aggregation. Besides, raw skeleton coordinates as well as their temporal difference are integrated with a two-stream paradigm. Experiments show that our approach consistently outperforms other state-of-the-arts on action recognition and detection benchmarks like NTU RGB+D, SBU Kinect Interaction and PKU-MMD.		Chao Li;Qiaoyong Zhong;Di Xie;Shiliang Pu	2018		10.24963/ijcai.2018/109	machine learning;pattern recognition;co-occurrence;artificial intelligence;rgb color model;temporal difference learning;skeleton (computer programming);computer science;feature learning	Vision	27.590398352679518	-51.92086669627279	17680
77e6f4d41c054ca8e2aa94461bc4a9c239ea71d7	motion and structure estimation from stereo image sequences	metodo cuadrado menor;gaussian noise;desigualdad cramer rao;minimum variance;filtering;methode moindre carre;structure from stereo;nonlinear least squares;closed form solution;filtro kalman;least squares method;variance minimale;iterative algorithms;stereo image;weighted least square;optimal estimation;filtre kalman;simulation;kalman filters;depth maps motion parameter estimation 3d point pattern recognition picture processing stereo image sequences closed form approximate matrix weighted solution matrix weighted least squares;picture processing;motion estimation image sequences stereo vision least squares approximation kalman filters filtering iterative algorithms closed form solution least squares methods gaussian noise;kalman filter;simulacion;motion estimation;least squares approximation;ecuacion forma cerrada;motion from stereo;robotics;stereo image sequences;matrix algebra;indexing terms;three dimensional;matrix weighted least squares;experimental result;cramer rao inequality;depth maps;3d point;closed form approximate matrix weighted solution;image sequence;motion parameter estimation;equation forme close;stereo vision;picture processing matrix algebra parameter estimation pattern recognition;estimacion parametro;resultado experimental;pattern recognition;robotica;non linearite;no linealidad;inegalite cramer rao;minimal variance;nonlinearity;secuencia imagen;ground truth;robotique;parameter estimation;estimation optimale;estimation parametre;cramer rao bound;structural estimation;depth map;resultat experimental;extended kalman filter;least squares methods;varianza minima;sequence image;closed form equation;estimacion optima;image sequences	A closed-form approximate matrix-weighted solution for estimating motion parameters from point correspondences in two stereo image pairs is presented. The corresponding algorithm is noniterative and fast. Simulation shows that the solution is significantly more reliable than the unweighted and the scalar-weighted solutions, except when the number of point correspondences is less than seven. The matrix-weighted least squares solution can be used as a final solution if the speed requirement does not allow iterations. An approach to optimal estimation of the motion parameters and the structure of the three-dimensional points is also introduced. In experiments with a calibrated real stereo setup, the motion parameters and depth maps were automatically computed from real-world images. Some ground truths were used to validate the accuracy of these results. >		Juyang Weng;Paul R. Cohen;Nicolas Rebibo	1992	IEEE Trans. Robotics and Automation	10.1109/70.143354	kalman filter;computer vision;closed-form expression;mathematical optimization;computer science;motion estimation;mathematics;robotics;motion field;least squares;statistics	Robotics	51.97811770804961	-51.62387806407026	17687
716562f48859c6cc2aab058b4fd7b1f12c591d12	skeleton-based orienteering for level set estimation		In recent years, the use of unmanned vehicles for monitoring spatial environmental phenomena has gained increasing attention. Within this context, an interesting problem is level set estimation, where the goal is to identify regions of space where the analyzed phenomena (for example the PH value in a body of water) is above or below a given threshold level. Typically, in the literature this problem is approached with techniques which search for the most interesting sampling locations to collect the desired information (i.e., locations where we can gain the most information by sampling). However, the common assumption underlying this class of approaches is that acquiring a sample is expensive (e.g., in terms of consumed energy and time). In this paper, we take a different perspective on the same problem by considering the case where a mobile vehicle can continuously acquire measurements with a negligible cost, through high rate sampling sensors. In this scenario, it is crucial to reduce the path length that the mobile platform executes to collect the data. To address this issue, we propose a novel algorithm, called Skeleton-Based Orienteering for Level Set Estimation (SBOLSE). Our approach starts from the LSE formulation introduced in [10] and formulates the level set estimation problem as an orienteering problem. This allows one to determine informative locations while considering the length of the path. To combat the complexity associated with the orienteering approach, we propose a heuristic approach based on the concept of topological skeletonization. We evaluate our algorithm by comparing it with the state of the art approaches (i.e., LSE and LSE-batch) both on a real world dataset extracted from mobile platforms and on a synthetic dataset extracted from CO2 maps. Results show that our approach achieves a near optimal classification accuracy while significantly reducing the travel distance (up to 70% w.r.t LSE and 30% w.r.t. LSE-batch).	algorithm;estimation theory;heuristic;information;mobile device;mobile operating system;online and offline;ph (complexity);sampling (signal processing);sensor;subroutine;synthetic intelligence;topological derivative;unmanned aerial vehicle	Lorenzo Bottarelli;Manuele Bicego;Jason Blum;Alessandro Farinelli	2016		10.3233/978-1-61499-672-9-1256	artificial intelligence;skeleton (computer programming);mathematical optimization;machine learning;computer science;level set;orienteering	AI	48.11434352274056	-28.945156799834663	17745
a95bf01cf3f9138575e35e73a549ae3b4dbf6105	real-time event-driven spiking neural network object recognition on the spinnaker platform	spinnaker platform;lif neuron model;spiking silicon retina;neurons sensors hardware adaptation models visualization computational modeling testing;object recognition;leaky integrate and fire neuron model;probability;sensors;spinnaker system;computational hardware platform;view tuned neurons;testing;event driven platform;hmax object recognition model;neural chips;visualization;computational modeling;character recognition task;probability real time event driven spiking neural network spinnaker platform hmax object recognition model event driven platform visual input spiking silicon retina spinnaker system computational hardware platform leaky integrate and fire neuron model lif neuron model view tuned neurons character recognition task;probability neural chips object recognition;real time event driven spiking neural network;neurons;adaptation models;hardware;visual input	This paper presents a real-time spiking neural network adaptation of the HMAX object recognition model on an event-driven platform. Visual input is provided by a spiking silicon retina, while the SpiNNaker system is used as a computational hardware platform for implementation. We show the implementation of a simple Leaky Integrate-and-Fire (LIF) neuron model on SpiNNaker to create an event driven network, where a neuron only updates when it receives an interrupt indicating that a new input spike has been received. The model output consists of view tuned neurons which respond selectively to a particular view of an object. The network can be used to discriminate between objects, or between the same object at different views. On a 26 class character recognition task, the correct class is always assigned the highest probability (69.42% on average).	artificial neural network;biological neuron model;event-driven programming;low insertion force;optical character recognition;outline of object recognition;real-time clock;spinnaker;spiking neural network	Garrick Orchard;Xavier Lagorce;Christoph Posch;Steve B. Furber;Ryad B. Benosman;Francesco Galluppi	2015	2015 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2015.7169171	computer vision;visualization;computer science;sensor;artificial intelligence;cognitive neuroscience of visual object recognition;machine learning;probability;software testing;computational model;statistics;spiking neural network	Vision	45.85500527465212	-31.56345333262854	17757
7913c8c97350ecbf80b1e8b1e88d22c6452b317e	robust estimators under a functional common principal components model	projection pursuit;functional data analysis;robust estimation;outliers;common principal component model;fisher consistency	When dealing with several populations of functional data, equality of the covariance operators is often assumed even when seeking for a lower-dimensional approximation to the data. Usually, if this assumption does not hold, one estimates the covariance operator of each group separately, which leads to a large number of parameters. As in the multivariate setting, this is not satisfactory since the covariance operators may exhibit some common structure, as is, for instance, the assumption of common principal directions. The existing procedures to estimate the common directions are sensitive to atypical observations. For that reason, robust projection-pursuit estimators for the common directions under a common principal component model are considered. A numerical method to compute the first directions is also provided. Under mild conditions, consistency results are obtained. A Monte Carlo study is performed to compare the finite sample behaviour of the estimators based on robust scales and on the standard deviation. The usefulness of the proposed approach is illustrated on a real data set. © 2016 Published by Elsevier B.V.	approximation;component-based software engineering;monte carlo method;numerical method;population;principal component analysis	Juan Lucas Bali;Graciela Boente	2017	Computational Statistics & Data Analysis	10.1016/j.csda.2016.08.017	projection pursuit;econometrics;mathematical optimization;functional data analysis;outlier;fisher consistency;mathematics;statistics	ML	29.752278178786867	-24.981056647785383	17782
fe55853dd94aa2dff913045dadd7b173895f52e1	sonic millip3de: an architecture for handheld 3d ultrasound	random access memory;accelerators transducers ultrasonic imaging biomedical imaging three dimensional displays computer architecture array signal processing random access memory hardware 3d ultrasound handheld ultrasound beamforming;ultrasonic imaging;accelerators;transducers;biomedical imaging;array signal processing;transducers ultrasonic imaging biomedical imaging three dimensional displays computer architecture array signal processing random access memory;computer architecture;3d ultrasound;three dimensional displays;beamforming;handheld ultrasound;hardware	3D ultrasound is becoming common for noninvasive medical imaging because of its high accuracy, safety, and ease of use. Unlike other modalities, ultrasound transducers require little power, which makes handheld imaging platforms possible, and several low-resolution 2D devices are commercially available today. However, the extreme computational requirements (and associated power requirements) of 3D ultrasound image formation have, to date, precluded handheld 3D-capable devices. The authors describe the Sonic Millip3De, a new system architecture and accelerator for 3D ultrasound beamforming--the most computationally intensive aspect of image formation. Their three-layer die-stacked design combines a new approach to the ultrasound imaging algorithm better suited to hardware with a custom beamforming accelerator that employs massive data parallelism and a streaming pipeline architecture to achieve high-quality 3D ultrasound imaging within a full-system power of 15 W in 45-nm semiconductor technology (400× less than a conventional DSP solution). Under anticipated scaling trends, the authors project that Sonic Millip3De will achieve the target 5-W power budget by the 16-nm technology node.	algorithm;beamforming;data parallelism;handheld game console;image formation;image scaling;medical imaging;medical ultrasound;multitier architecture;parallel computing;pipeline (computing);requirement;semiconductor device fabrication;sonic the hedgehog 2;sonic the hedgehog 4: episode i;systems architecture;transducer;usability	Richard Sampson;Ming Yang;Siyuan Wei;Chaitali Chakrabarti;Thomas F. Wenisch	2014	IEEE Micro	10.1109/MM.2014.49	medical imaging;embedded system;transducer;computer science;ultrasonic sensor;beamforming	Arch	41.25741328795857	-33.42444381601299	17789
81046111aa6b9254580f6190aa1ac8b602bab6dc	on combining visual slam and dense scene flow to increase the robustness of localization and mapping in dynamic environments	moving object;image motion analysis;visually impaired users;gps based approach localization mapping robustness visual slam applications static features camera egomotion crowded real world dynamic environments dense scene flow representation moving object detection visually impaired users atocha railway station alcala de henares madrid spain;visualization simultaneous localization and mapping cameras vectors optical imaging feature extraction robustness;dynamic environment;large scale;visualization;handicapped aids;optical imaging;vectors;global positioning system;feature extraction;simultaneous localization and mapping;visual impairment;robustness;object detection cameras feature extraction global positioning system handicapped aids image motion analysis;cameras;object detection	In this paper, we introduce the concept of dense scene flow for visual SLAM applications. Traditional visual SLAM methods assume static features in the environment and that a dominant part of the scene changes only due to camera egomotion. These assumptions make traditional visual SLAM methods prone to failure in crowded real-world dynamic environments with many independently moving objects, such as the typical environments for the visually impaired. By means of a dense scene flow representation, moving objects can be detected. In this way, the visual SLAM process can be improved considerably, by not adding erroneous measurements into the estimation, yielding more consistent and improved localization and mapping results. We show large-scale visual SLAM results in challenging indoor and outdoor crowded environments with real visually impaired users. In particular, we performed experiments inside the Atocha railway station and in the city-center of Alcalá de Henares, both in Madrid, Spain. Our results show that the combination of visual SLAM and dense scene flow allows to obtain an accurate localization, improving considerably the results of traditional visual SLAM methods and GPS-based approaches.	autonomous robot;experiment;global positioning system;internationalization and localization;object detection;simultaneous localization and mapping;vii;visual odometry	Pablo Fernández Alcantarilla;José Javier Yebes Torres;Javier Almazán;Luis Miguel Bergasa	2012	2012 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2012.6224690	computer vision;simulation;visualization;global positioning system;feature extraction;computer science;optical imaging;robustness;computer graphics (images);simultaneous localization and mapping	Robotics	42.77633925214552	-45.61526718966042	17803
5f35c0e8d35a64eec44120db695fc3ff0d987ba9	independent component analysis using multilayer networks	independent component analysis algorithm;estimation theory;blind source separation independent component analysis algorithm mixed signal distribution multilayer network density estimation source signal function statistical characteristic;sample size;convergence;score function;neural nets;multilayer networks density estimation independent component analysis ica;blind source separation;independent component analysis;mixed signal distribution;density estimation;nonhomogeneous media;estimation;independent component analysis ica;statistical characteristic;independent component analysis nonhomogeneous media kernel probability density function density functional theory signal generators source separation multi layer neural network statistics cost function;source signal function;neural nets blind source separation estimation theory independent component analysis;multilayer network density estimation;approximation methods;signal processing algorithms;multilayer networks;algorithm design and analysis	A basic element in most independent component analysis (ICA) algorithms is the choice of a model for the score functions of the unknown sources. In this letter, a novel ICA algorithm is proposed, which is truly blind to the particular underlying distribution of the mixed signals. Using a multilayer network density estimation technique, the algorithm reconstructs score functions of the source signals. We show with experiments involving linear mixtures of various source signals with different statistical characteristics that the new algorithm not only outperforms state-of-the-art ICA methods but also our approach only requires a fraction of the sample sizes in order to outperform methods with partially adaptive score functions.	algorithm;apollonian network;dynamic music;experiment;independent computing architecture;independent component analysis	Weiqin Li;Haibo Zhang	2007	IEEE Signal Processing Letters	10.1109/LSP.2007.900031	sample size determination;independent component analysis;algorithm design;estimation;density estimation;convergence;computer science;machine learning;pattern recognition;mathematics;blind signal separation;score;estimation theory;artificial neural network;statistics	ML	30.54393513485189	-32.022340052502074	17840
2548c6141679ba550be1c335ec99cdea4d1abbf0	object tracking based on online representative sample selection via non-negative least square	object tracking;template update;sample selection;non-negative least square	In the most tracking approaches, a score function is utilized to determine which candidate is the optimal one by measuring the similarity between the candidate and the template. However, the representative samples selection in the template update is challenging. To address this problem, in this paper, we treat the template as a linear combination of representative samples and propose a novel approach to select representative samples based on the coefficient constrained model. We formulate the objective function into a non-negative least square problem and obtain the solution utilizing standard non-negative least square. The experimental results show that the observation module of our approach outperforms several other observation modules under the same feature and motion module, such as support vector machine, logistic regression, ridge regression and structured outputs support vector machine.	coefficient;logistic regression;loss function;optimization problem;smoothing;support vector machine	Weihua Ou;Di Yuan;Qiao Liu;Yongfeng Cao	2017	Multimedia Tools and Applications	10.1007/s11042-017-4672-3	artificial intelligence;computer science;support vector machine;video tracking;sampling (statistics);statistics;logistic regression;pattern recognition;linear combination;score;least squares	AI	31.280725529574305	-38.13733773724718	17855
fdff00367a8401e3ae86c1817d27a5effb658a37	boosted multivariate trees for longitudinal data	smoothing parameter;p splines;marginal model;multivariate regression tree;gradient boosting	Machine learning methods provide a powerful approach for analyzing longitudinal data in which repeated measurements are observed for a subject over time. We boost multivariate trees to fit a novel flexible semi-nonparametric marginal model for longitudinal data. In this model, features are assumed to be nonparametric, while feature-time interactions are modeled semi-nonparametrically utilizing P-splines with estimated smoothing parameter. In order to avoid overfitting, we describe a relatively simple in sample cross-validation method which can be used to estimate the optimal boosting iteration and which has the surprising added benefit of stabilizing certain parameter estimates. Our new multivariate tree boosting method is shown to be highly flexible, robust to covariance misspecification and unbalanced designs, and resistant to overfitting in high dimensions. Feature selection can be used to identify important features and feature-time interactions. An application to longitudinal data of forced 1-second lung expiratory volume (FEV1) for lung transplant patients identifies an important feature-time interaction and illustrates the ease with which our method can find complex relationships in longitudinal data.	assumed;cross reactions;cross-validation (statistics);estimated;expiration, function;feature selection;gradient boosting;interaction;iteration;lung transplantation;machine learning;marginal model;overfitting;patients;population parameter;pulmonary function test/forced expiratory volume 1;semiconductor industry;smoothing (statistical technique);structure of parenchyma of lung;trees (plant);unbalanced circuit	Amol Pande;Liang Li;Jeevanantham Rajeswaran;John Ehrlinger;Udaya B. Kogalur;Eugene Blackstone;Hemant Ishwaran	2016	Machine Learning	10.1007/s10994-016-5597-1	econometrics;marginal model;computer science;machine learning;pattern recognition;mathematics;gradient boosting;statistics	ML	29.704774163806707	-25.852244991608003	17927
9b58ecacbe850231a1c91d7cf4cebd5d0aaa95a3	analysis and tracking of human gait via a marker-free system	human body;mean error;video;ccd camera;optical flow	This paper presents a marker-free methodology for facilitating the analysis and the tracking of human motion during gait. It consists of recognition and reconstruction of the legs of a walking human. In order to study the gait, a video system composed of three synchronized CCD cameras has been devised. It provides three different grey level image sequences of the same scene. First of all, a model of the human, based on tapered superquadric curves, has been defined ; the leg can be divided into three parts : the thigh, the calf, and the foot. The whole methodology can be split following this scheme determination of the boundaries of the human in motion by using successively an optical flow process and a crest lines extraction algorithm, prediction of the location of the human body in the 3D space, direct reconstruction of the each part of the leg with a Least Median of Squares regression, and finally application of a spatial coherence process. The method described has been tested on synthetic images (mean error of about 1.2 mm and maximal error of about 5 mm along the coordinate axes) and on image sequences of a walking human.		Elodie F. Calais;Louis Legrand	2000			computer vision;human body;video;internationalization and localization;computer science;motion estimation;optical flow;mean squared error;gait;tracking;charge-coupled device	Vision	48.92868206259756	-45.43067995100909	17948
58048ff8a79e1bdd2e353b3393d139f244bc2835	offline sketch parsing via shapeness estimation		In this work, we target at the problem of offline sketch parsing, in which the temporal orders of strokes are unavailable. It is more challenging than most of existing work, which usually leverages the temporal information to reduce the search space. Different from traditional approaches in which thousands of candidate groups are selected for recognition, we propose the idea of shapeness estimation to greatly reduce this number in a very fast way. Based on the observation that most of hand-drawn shapes with well-defined closed boundaries can be clearly differentiated from nonshapes if normalized into a very small size, we propose an efficient shapeness estimation method. A compact feature representation as well as its efficient extraction method is also proposed to speed up this process. Based on the proposed shapeness estimation, we present a three-stage cascade framework for offline sketch parsing. The shapeness estimation technique in this framework greatly reduces the number of false positives, resulting in a 96.2% detection rate with only 32 candidate group proposals, which is two orders of magnitude less than existing methods. Extensive experiments show the superiority of the proposed framework over stateof-the-art works on sketch parsing in both effectiveness and efficiency, even though they leveraged the temporal information of strokes.	algorithm;benchmark (computing);cascade framework;estimation theory;experiment;online and offline;parsing;sketch;unavailability	Jie Wu;Changhu Wang;Liqing Zhang;Yong Rui	2015			computer science;artificial intelligence;theoretical computer science;machine learning;pattern recognition;data mining	AI	31.621845068165822	-49.88007454736333	17972
8fbc6d32d320b83fa93991d8c072f1572826c16e	hierarchical tensor manifold modeling for multi-group analysis				Hideaki Ishibashi;Masayoshi Era;Tetsuo Furukawa	2018	IEICE Transactions			Vision	30.848658945867726	-35.940775905465934	18016
7ed2c1f5df9826c042df63cb1337f417a252adcc	a finite smoothing algorithm for linear l1 estimation	finite convergence;65f20;huber estimator;62j05;newton s method;smoothing;90c05	In this paper a new method for solving the linear $l_1 $ problem is described, analysed, and tested. The method is based on smoothing the nondifferentiable $l_1 $ function. The smoothing can be done in a well-onditioned manner since the method has finite convergence. Extensive numerical tests demonstrate significant superiority to existing simplex-type codes. Furthermore, the tests show that the new algorithm is very well suited for vector processing.	algorithm;smoothing	Kaj Madsen;Hans Bruun Nielsen	1993	SIAM Journal on Optimization	10.1137/0803010	econometrics;mathematical optimization;mathematics;newton's method;statistics;smoothing	Theory	29.927554598116334	-26.091456449531112	18025
1f5e9952cd225dc4677146ae0f9901f070d19cb7	3d-assisted facial texture super-resolution	face recognition;face modeling;super resolution	In this paper we propose a new framework for super-resolving facial images under arbitrary pose. While example-based super-resolution methods have demonstrated impressive results for face super-resolution under given pose and imaging conditions, they have limitations dealing with different poses and illuminations. Due to these limitations their application to face super-resolution in generalized situations is either impractical or sub-optimal. The proposed framework utilizes a 3D morphable face model in order to address the problem of face super-resolution under arbitrary pose. This framework does not assume any pre-defined pose for the subject thus it can be readily applied to any pose. The main contribution of this work is defining a framework in which a 3D morphable model can be used in conjunction with any of the example-based super-resolution methods. Experimental results prove the potential power of this method in face superresolution and its application to face recognition.	curve fitting;facial recognition system;image resolution;pose (computer vision);super-resolution imaging;texture mapping	Pouria Mortazavian;Josef Kittler;William J. Christmas	2009		10.5244/C.23.119	facial recognition system;computer vision;face detection;speech recognition;computer science;three-dimensional face recognition;face hallucination;superresolution	Vision	47.598368987255775	-51.389534565320616	18027
559858e845027735368c712e10e470093706a2e0	robust camera pose recovery using stochastic geometry	lens distortion;three dimensional;electrical engineering and computer science;thesis;machine vision;camera calibration;stochastic geometry	The objective of three-dimensional (3-D) machine vision is to infer geometric properties (shape, dimensions) and photometric attributes (color, texture, reflectance) from a set of two-dimensional images. Such vision tasks rely on accurate camera calibration, that is, estimates of the camera's intrinsic parameters, such as focal length, principal point, and radial lens distortion, and extrinsic parameters-orientation, position, and scale relative to a fixed frame of reference. This thesis introduces methods for automatic recovery of precise extrinsic camera pose among a large set of images, assuming that accurate intrinsic parameters and rough estimates of extrinsic parameters are available. Although the motivating application is metric 3-D reconstruction of urban environments from pose-annotated hemispherical imagery, few domain-specific restrictions are required or imposed. Orientation is recovered independently of position via the detection and optimal alignment of translation-invariant image features (vanishing points). Known orientations constrain pair-wise epipolar geometry, reducing the determination of local translational offsets to a simple linear form. Local motion estimates are assembled into a global constraint set to determine camera positions with respect to a common coordinate frame. The final result is an assignment of accurate pose, along with its uncertainty, to every relevant camera. The methods developed in this work are notable in several respects. First, they scale linearly in space and time usage with the number of input images. Second, they robustly and automatically achieve global optimality even with poorly known initial pose. Third, they represent every geometric quantity stochastically, as a sample from a probability distribution defined over an appropriate parameter space; explicit measures of uncertainty are produced alongside every parameter estimate and propagated throughout. Finally, they estimate feature correspondence probabilistically, and therefore neither require nor produce explicit correspondence. A theoretical framework for the appropriate geometric models and reasoning tasks is presented, followed by descriptions of methods for optimal recovery of camera pose within this framework. Performance is analyzed in simulation and on large sets (tens of thousands) of real images of urban environments. Experiments show that the proposed methods can produce pose estimates consistent to roughly 0.1' (2 milliradians) of orientation and 5 centimeters of position, as well as epipolar registration within 4 pixels, across wide-baseline data sets spanning several hundred meters, outperforming bundle adjustment via manual correspondence for the same input data. Thesis Supervisor: Seth Teller Title: Associate Professor of Computer Science and Engineering	3d computer graphics;baseline (configuration management);bundle adjustment;camera resectioning;computer engineering;distortion;epipolar geometry;experiment;focal (programming language);file spanning;machine vision;pixel;radial (radio);simulation	Matthew E. Antone	2001			computer vision;camera auto-calibration;simulation;mathematics;optics	Vision	51.88985479300485	-45.18377714959747	18113
05b3d10a85e0380df44abb30ddba9f7da5adf80f	boosted multi-task learning for face verification with applications to web image and video search	machine learning;face recognition;local binary pattern;image resolution;image classification;low resolution;multi task learning;internet;learning artificial intelligence;video compression;local binary patterns;compression artifacts;search engines;training data;feature extraction;boosting	Face verification has many potential applications including filtering and ranking image/video search results on celebrities. Since these images/videos are taken under uncontrolled environments, the problem is very challenging due to dramatic lighting and pose variations, low resolutions, compression artifacts, etc. In addition, the available number of training images for each celebrity may be limited, hence learning individual classifiers for each person may cause overfitting. In this paper, we propose two ideas to meet the above challenges. First, we propose to use individual bins, instead of whole histograms, of Local Binary Patterns (LBP) as features for learning, which yields significant performance improvements and computation reduction in our experiments. Second, we present a novel Multi-Task Learning (MTL) framework, called Boosted MTL, for face verification with limited training data. It jointly learns classifiers for multiple people by sharing a few boosting classifiers in order to avoid overfitting. The effectiveness of Boosted MTL and LBP bin features is verified with a large number of celebrity images/videos from the web.	algorithm;algorithmic learning theory;belief propagation;boosting (machine learning);compression artifact;computation;computer multitasking;experiment;gradient boosting;local binary patterns;multi-task learning;overfitting;uncontrolled format string	Xiaogang Wang;Cha Zhang;Zhengyou Zhang	2009	2009 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPRW.2009.5206736	facial recognition system;computer vision;local binary patterns;computer science;machine learning;pattern recognition	Vision	26.910794649082632	-47.96771223755825	18167
1d484d1efa10a61e8182f00b00e078b2f9868109	a bayesian approach to background modeling	background modeling;gaussian mixture;bayesian approach;surveillance;real time;bayesian methods;traffic control;layout;visual surveillance;statistical distributions;hidden markov models;bayesian learning;machine vision;probability distribution;vehicle dynamics;gaussian distribution;bayesian methods layout gaussian distribution hidden markov models surveillance probability distribution vehicle dynamics statistical distributions traffic control image sequences;dynamic scenes;image sequences	Learning background statistics is an essential task for several visual surveillance applications such as incident detection and traf.c management. In this paper, we propose a new method for modeling background statistics of a dynamic scene. Each pixel is represented with layers of Gaussian distributions. Using recursive Bayesian learning, we estimate the probability distribution of mean and covariance of each Gaussian. The proposed algorithm preserves the multimodality of the background and estimates the number of necessary layers for representing each pixel. We compare our results with the Gaussian mixture background model. Experiments conducted on synthetic and video data demonstrate the superior performance of the proposed approach.	algorithm;algorithmic efficiency;computer display standard;consistency model;experiment;mixture model;multimodal interaction;pixel;recursion;synthetic intelligence	Oncel Tuzel;Fatih Murat Porikli;Peter Meer	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Workshops	10.1109/CVPR.2005.384	probability distribution;machine vision;bayesian probability;computer science;machine learning;pattern recognition;mathematics;statistics	Vision	45.16749602350404	-49.3927990212243	18198
a515acf113ce84d87860703ae3606cb9e9d4f5e3	a unified maximum likelihood approach for estimating symmetric properties of discrete distributions		Symmetric distribution properties such as support size, support coverage, entropy, and proximity to uniformity, arise in many applications. Recently, researchers applied different estimators and analysis tools to derive asymptotically sample-optimal approximations for each of these properties. We show that a single, simple, plug-in estimator—profile maximum likelihood (PML)– is sample competitive for all symmetric properties, and in particular is asymptotically sampleoptimal for all the above properties.	approximation;circuit complexity;plug-in (computing)	Jayadev Acharya;Hirakendu Das;Alon Orlitsky;Ananda Theertha Suresh	2017			pattern recognition;artificial intelligence;estimating equations;maximum likelihood;statistics;computer science;maximum likelihood sequence estimation	ML	28.968551933116537	-27.55663930671689	18242
1515d6a6c833a1410d38c7f0f3ed127ece853248	uav intelligent path planning for wilderness search and rescue	intelligent path planning;optimal solution;uav;search and rescue;optimisation;pediatrics;wilderness;approximation algorithms;path planning;large hadron collider;remotely operated vehicles;bayesian modeling;data mining;wilderness search and rescue;telerobotics aerospace robotics computational complexity optimisation path planning remotely operated vehicles;search;sea surface;probability dsitribution map;computational complexity;probability distribution map;wisar intelligent path planning wilderness search rescue probability distribution map uav unmanned aerial vehicles onboard video camera np hard;probability distribution;aerospace robotics;uavs;unmanned aerial vehicles path planning cameras probability distribution intelligent robots computer science usa councils time factors solids object detection;telerobotics;np hard;onboard video camera;unmanned aerial vehicles;wisar;cameras;rescue	In the priority search phase1 of Wilderness Search and Rescue, a probability distribution map is created. Areas with higher probabilities are searched first in order to find the missing person in the shortest expected time. When using a UAV to support search, the onboard video camera should cover as much of the important areas as possible within a set time. We explore several algorithms (with and without set destination) and describe some novel techniques in solving this problem and compare their performances against typical WiSAR scenarios. This problem is NP-hard, but our algorithms yield high quality solutions that approximate the optimal solution, making efficient use of the limited UAV flying time.	aerial photography;approximation algorithm;average-case complexity;bayesian network;discretization;display resolution;feature data;geographic coordinate system;geographic information system;global positioning system;google earth;internet;kaby lake;motion planning;np-hardness;performance;scr-584 radar;scalability;sensor;sparse matrix;sudoku solving algorithms;the forest;thematic map;unmanned aerial vehicle;webserver directory index	Lanny Lin;Michael A. Goodrich	2009	2009 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2009.5354455	remotely operated underwater vehicle;telerobotics;probability distribution;computer vision;simulation;large hadron collider;computer science;engineering;artificial intelligence;np-hard;motion planning;computational complexity theory;bayesian inference;operations research;approximation algorithm;statistics	Robotics	53.272456994160436	-26.551154823577438	18250
b9c151e1c6a834c881ceb3c224cb28c6351be247	some considerations on physical analysis of data	locality;ensemble empirical mode decomposition;adaptivity;noise assisted data analysis;physical analysis of data;global domain analysis;empirical mode decomposition	In this paper, we present some general considerations about data analysis from the perspective of a physical scientist and advocate the physical, instead of mathematical, analysis of data. These considerations have been accompanying our development of novel adaptive, local analysis methods, especially the empirical mode decomposition and its major variation, the ensemble empirical mode decomposition, and its preliminary mathematical explanations. A particular emphasis will be on the advantages and disadvantages of mathematical and physical constraints associated with various analysis methods. We argue that, using data analysis in a given temporal domain of observation as an example, the mathematical constraints imposed on data may lead to difficulties in understanding the physics behind the data. With such difficulties in mind, we promote adaptive, local analysis method, which satisfies fundamental physical principle of consequent evolution of a system being not able to change the past evolution of the system. We also argue, using the ensemble empirical mode decomposition as an example, that noise can be helpful to extract physically meaningful signals hidden in noisy data.	hilbert–huang transform;signal-to-noise ratio;singular value decomposition	Zhaohua Wu;Norden E. Huang;Xianyao Chen	2011	Advances in Adaptive Data Analysis	10.1142/S1793536911000660	simulation;computer science;artificial intelligence;hilbert–huang transform;machine learning;statistics	AI	27.72343829021332	-37.55521068147214	18329
78c1749c0f9235a5be0a4f05e5c57f1651150b51	non-negativity constrained missing data estimation for high-dimensional and sparse matrices		Latent factor (LF) models have proven to be accurate and efficient in extracting hidden knowledge from high-dimensional and sparse (HiDS) matrices. However, most LF models fail to fulfill the non-negativity constraints that reflect the non-negative nature of industrial data. Yet existing non-negative LF models for HiDS matrices suffer from slow convergence leading to considerable time cost. An alternating direction method-based non-negative latent factor (ANLF) model decomposes a non-negative optimization process into small sub-tasks. It updates each LF non-negatively based on the latest state of those trained before, thereby achieving fast convergence and maintaining high prediction accuracy and scalability. This paper theoretically analyze the characteristics of an ANLF model, and presents detailed empirical study regarding its performance on several HiDS matrices arising from industrial applications currently in use. Therefore, its capability of addressing HiDS matrices is validated in both theory and practice.	mathematical optimization;missing data;negativity (quantum mechanics);scalability;sparse matrix	Xin Luo;Shuai Li	2017	2017 13th IEEE Conference on Automation Science and Engineering (CASE)	10.1109/COASE.2017.8256293	missing data;mathematics;mathematical optimization;empirical research;sparse matrix;scalability;data modeling;matrix (mathematics);linear programming;convergence (routing)	ML	27.072641387976986	-35.470785645262545	18352
3e4ec7bdd279573d328a26b720854894e68230ed	efficient relative attribute learning using graph neural networks		A sizable body of work on relative attributes provides evidence that relating pairs of images along a continuum of strength pertaining to a visual attribute yields improvements in a variety of vision tasks. In this paper, we show how emerging ideas in graph neural networks can yield a solution to various problems that broadly fall under relative attribute learning. Our main idea is the observation that relative attribute learning naturally benefits from exploiting the graph of dependencies among the different relative attributes of images, especially when only partial ordering is provided at training time. We use message passing to perform end to end learning of the image representations, their relationships as well as the interplay between different attributes. Our experiments show that this simple framework is effective in achieving competitive accuracy with specialized methods for both relative attribute learning and binary attribute prediction, while relaxing the requirements on the training data and/or the number of parameters, or both.	attribute grammar;binary classification;brain–computer interface;computer multitasking;encode;experiment;human body weight;message passing;neural networks;requirement;triune continuum paradigm;universal instantiation;web page	Zihang Meng;Nagesh Adluru;Hyunwoo J. Kim;Glenn Fung;Vikas Singh	2018		10.1007/978-3-030-01264-9_34	message passing;artificial intelligence;partially ordered set;machine learning;computer science;artificial neural network;end-to-end principle;training set;multi-task learning;binary number;graph	Vision	24.8700237118695	-48.228497707513796	18376
2654512ed26b20a4e120334486ed0a4684e59917	a novel indefinite kernel dimensionality reduction algorithm: weighted generalized indefinite kernel discriminant analysis	indefinite kernel function;weighting function;undersampled problem;indefinite kernel discriminant analysis;classification accuracy	Kernel methods are becoming increasingly popular for many real-world learning problems. And these methods for data analysis are frequently considered to be restricted to positive definite kernels. In practice, however, indefinite kernels arise and demand application in pattern analysis. In this paper, we present several formal extensions of kernel discriminant analysis (KDA) methods which can be used with indefinite kernels. In particular they include indefinite KDA (IKDA) based on generalized singular value decomposition (IKDA/GSVD), pseudo-inverse IKDA, null space IKDA and range space IKDA. Similar to the case of LDA-based algorithms, IKDA-based algorithms also fail to consider that different contribution of each pair of class to the discrimination. To remedy this problem, weighted schemes are incorporated into IKDA extensions in this paper and called them weighted generalized IKDA algorithms. Experiments on two real-world data sets are performed to test and evaluate the effectiveness of the proposed algorithms and the effect of weights on indefinite kernel functions. The results show that the effect of weighted schemes is very significantly.	algorithm;dimensionality reduction;experiment;indefinite sum;kernel (linear algebra);kernel (operating system);kernel method;linear discriminant analysis;machine learning;pattern recognition;performance;singular value decomposition;technological singularity	Jing Yang;Liya Fan	2013	Neural Processing Letters	10.1007/s11063-013-9330-9	mathematical optimization;kernel fisher discriminant analysis;mathematical analysis;discrete mathematics;string kernel;weight function;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;mathematics;tree kernel;variable kernel density estimation;statistics	AI	24.917194704587228	-40.45239145144571	18394
7eba4464fb4de1a9bef33dcf2a9f85c6f4cbbdef	tractable and reliable registration of 2d point sets	publikationer;datorseende och robotik autonoma system;konferensbidrag;2d registration;l1 norm;artiklar;rapporter;optimization;matematik	This paper introduces two new methods of registering 2D point sets over rigid transformations when the registration error is based on a robust loss function. In contrast to previous work, our methods are guaranteed to compute the optimal transformation, and at the same time, the worst-case running times are bounded by a low-degree polynomial in the number of correspondences. In practical terms, this means that there is no need to resort to ad-hoc procedures such as random sampling or local descent methods that cannot guarantee the quality of their solutions. We have tested the methods in several different settings, in particular, a thorough evaluation on two benchmarks of microscopic images used for histologic analysis of prostate cancer has been performed. Compared to the state-of-theart, our results show that the methods are both tractable and reliable despite the presence of a significant amount of outliers.	benchmark (computing);best, worst and average case;cobham's thesis;hoc (programming language);loss function;monte carlo method;polynomial;sampling (signal processing)	Erik Ask;Olof Enqvist;Linus Svärm;Fredrik Kahl;Giuseppe Lippolis	2014		10.1007/978-3-319-10590-1_26	mathematical optimization;simulation;taxicab geometry;mathematics;algorithm;statistics	Vision	50.5885098319844	-51.1054952652948	18435
7d6a8c2a857a4eb3d9f6c3c470906f2182c9473a	hierarchical grid-based multi-people tracking-by-detection with global optimization	global optimal data association tracking by detection hierarchical grid based detection orientation estimation;video sequences hierarchical grid based multipeople tracking by detection approach global optimization complex interaction mutual occlusion frame by frame detection fast oriented distance transform discretized state space data association problem grid based network flow model convex problem integer linear programming form association affinity model body orientation 3d appearance based orientation estimation motion based orientation estimation;trajectory;estimation;image edge detection;three dimensional displays;solid modeling;optimization;transforms image sequences integer programming linear programming object tracking sensor fusion;image edge detection trajectory estimation target tracking three dimensional displays optimization solid modeling;target tracking	We present a hierarchical grid-based, globally optimal tracking-by-detection approach to track an unknown number of targets in complex and dense scenarios, particularly addressing the challenges of complex interaction and mutual occlusion. Frame-by-frame detection is performed by hierarchical likelihood grids, matching shape templates through a fast oriented distance transform. To allow recovery from misdetections, common heuristics such as nonmaxima suppression within observations is eschewed. Within a discretized state-space, the data association problem is formulated as a grid-based network flow model, resulting in a convex problem casted into an integer linear programming form, giving a global optimal solution. In addition, we show how a behavior cue (body orientation) can be integrated into our association affinity model, providing valuable hints for resolving ambiguities between crossing trajectories. Unlike traditional motion-based approaches, we estimate body orientation by a hybrid methodology, which combines the merits of motion-based and 3D appearance-based orientation estimation, thus being capable of dealing also with still-standing or slowly moving targets. The performance of our method is demonstrated through experiments on a large variety of benchmark video sequences, including both indoor and outdoor scenarios.	affinity analysis;benchmark (computing);clinical use template;control theory;convex optimization;correspondence problem;discretization;distance transform;experiment;flow network;futures studies;global optimization;heuristic (computer science);high- and low-level;integer (number);integer programming;interaction;linear programming;matching;mathematical optimization;maxima and minima;numerous;obstruction;state space;unstable medical device problem;zero suppression	Lili Chen;Wei Wang;Giorgio Panin;Alois Knoll	2015	IEEE Transactions on Image Processing	10.1109/TIP.2015.2451013	computer vision;mathematical optimization;estimation;trajectory;machine learning;mathematics;solid modeling;statistics	Vision	49.98385736624363	-48.91023586827616	18541
6ed0dff199831d4d5b357ad9d899dbd4329cb962	lip segmentation and tracking under map-mrf framework with unknown segment number	segment number;map mrf framework;color lip segmentation;hierarchical model	This paper proposes a color lip segmentation method with unknown true segment number. Firstly, we build up a multi-layer hierarchical model, in which each layer corresponds to one segment cluster. Subsequently, a Markov random field derived from this model is obtained such that the segmentation problem is formulated as a labeling optimization problem under the maximum a posteriori Markov random field (MAP-MRF) framework. Suppose the pre-assigned number of segment clusters may over-estimate the ground truth, whereby leading to the over-segmentation. We present a rival penalized iterative algorithm capable of performing segment clusters and over-segmentation elimination simultaneously. Based upon this algorithm, we propose a lip segmentation and tracking scheme, featuring the robust performance to the estimate of the number of segment clusters. Experimental results show the efficacy of the proposed method in comparison with the existing counterparts.	markov random field	Yiu-ming Cheung;Meng Li;Xiaochun Cao	2013	Neurocomputing	10.1016/j.neucom.2012.10.009	computer vision;speech recognition;computer science;pattern recognition;mathematics;scale-space segmentation;hierarchical database model	Vision	44.13403969956524	-51.42962508777429	18586
aa619d245e46e0b6f0d94f3e0ed2f8b6ca302209	image antiblurring and statistic filter of feature space displacement: application to visual odometry for outdoor ground vehicle		Precise, reliable, and low-cost vehicular localization across a continuous spatiotemporal domain is an important problem in the field of outdoor ground vehicles. This paper proposes a visual odometry algorithm, where an ultrarobust and fast feature-matching scheme is combined with an effective antiblurring frame selection strategy. Our method follows the procedure of finding feature correspondences from consecutive frames and minimizing their reprojection error. The blurred image is a great challenge for localization with a sharp turn or fast movement. So we attempt to mitigate the impact of blur with an image singular value decomposition antiblurring algorithm. Moreover, a statistic filter of feature space displacement and circle matching are proposed to screen or prune potential matching features, so as to remove the outliers caused by mismatching. An evaluation of benchmark dataset KITTI and real outdoor data, with blur, low texture, and illumination change, demonstrates that the proposed ego-motion scheme significantly achieved performance with respect to the other state-of-the-art visual odometry approaches to a certain extent.		Xiangmo Zhao;Hai-Gen Min;Zhigang Xu;Xia Wu;Xiao-Chi Li;Peng-Peng Sun	2018	J. Sensors	10.1155/2018/2987819	computer vision;artificial intelligence;statistic;feature vector;outlier;visual odometry;singular value decomposition;engineering	Vision	51.26992241866264	-45.32127374360396	18592
769209a340ad2e6f02d88839940cbba53769273c	performance driven facial animation using illumination independent appearance-based tracking	face deformation;stereo image processing computer animation computer vision face recognition image sequences solid modelling;marker less image sequence;real time;illumination variation estimation;computer vision;face recognition;independent linear subspaces;3d face model facial animation illumination independent appearance based tracking marker less image sequence appearance based tracker face images illumination variation estimation appearance model independent linear subspaces face deformation;stereo image processing;facial animation;face modeling;facial animation lighting humans deformable models face detection computer vision image sequences real time systems anatomy lips;appearance based tracker;face images;illumination independent appearance based tracking;computer animation;appearance model;3d face model;solid modelling;image sequences	We introduce a procedure to estimate human face high level animation parameters from a marker-less image sequence in presence of strong illumination changes. We use an efficient appearance-based tracker to stabilise face images and estimate illumination variation. This is achieved by using an appearance model composed by two independent linear subspaces modelling face deformation and illumination changes respectively. The system is very simple to train and is able to re-animate a 3D face model in real-time	c++;computer vision;experiment;graphical model;graphical user interface;high-level programming language;illumination (image);real-time clock;real-time locating system;synthetic intelligence	José Miguel Buenaposada;Enrique Muñoz	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.927	facial recognition system;computer vision;active appearance model;computer facial animation;computer science;computer animation;multimedia;computer graphics (images)	Vision	47.18480357101815	-49.388391101587835	18630
3db404d21edae047b45b6c9935e052ecaf1cbd46	representational oriented component analysis (roca) for face recognition with one sample image per training class	image sampling;eigenvalues and eigenfunctions;image recognition;representational oriented component analysis;image classification;generalisation artificial intelligence face recognition image sampling image representation image classification eigenvalues and eigenfunctions statistical analysis learning artificial intelligence;independent component analysis;image analysis face recognition independent component analysis samarium principal component analysis linear discriminant analysis matched filters image recognition power system modeling face detection;roca classifier;frgc ver 1 0 dataset;frgc ver 1 0 dataset representational oriented component analysis roca classifier face recognition subspace method image representation image misregistration generalized eigenvector algorithm;face recognition;statistical analysis;image representation;principal component analysis;visual learning;generalized eigenvector algorithm;component analysis;samarium;subspace method;matched filters;image analysis;image misregistration;generalisation artificial intelligence;learning artificial intelligence;power system modeling;face detection;linear discriminant analysis	Subspace methods such as PCA, LDA, ICA have become a standard tool to perform visual learning and recognition. In this paper we propose representational oriented component analysis (ROCA), an extension of OCA, to perform face recognition when just one sample per training class is available. Several novelties are introduced in order to improve generalization and efficiency: (1) combining several OCA classifiers based on different image representations of the unique training sample is shown to greatly improve the recognition performance. (2) To improve generalization and to account for small misregistration effect, a learned subspace is added to constrain the OCA solution, (3) a stable/efficient generalized eigenvector algorithm that solves the small size sample problem and avoids overfitting. Preliminary experiments in the FRGC Ver 1.0 dataset show that ROCA outperforms existing linear techniques (PCA, OCA) and some commercial systems.	algorithm;experiment;facial recognition system;independent computing architecture;local-density approximation;overfitting;principal component analysis;statistical classification;ver (command);visual learning	Fernando De la Torre;Ralph Gross;Simon Baker;B. V. K. Vijaya Kumar	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)	10.1109/CVPR.2005.301	facial recognition system;independent component analysis;computer vision;contextual image classification;face detection;image analysis;speech recognition;computer science;machine learning;pattern recognition;samarium;mathematics;linear discriminant analysis;matched filter;statistics;principal component analysis	Vision	28.47369311019169	-43.244982576004894	18640
31c2c312c291008a9db1c888c4a41eed291b75f4	deep object centric policies for autonomous driving		While learning visuomotor skills in an end-to-end manner is appealing, deep neural networks are often uninterpretable and fail in surprising ways. For robotics tasks, such as autonomous driving, models that explicitly represent objects may be more robust to new scenes and provide intuitive visualizations. We describe a taxonomy of models which leverage both object instances and end-to-end learning. In the Grand Theft Auto V simulator, we show that object-centric models outperform object-agnostic methods in scenes with other vehicles and pedestrians, even with an imperfect detector. We also demonstrate that our architectures perform well on real-world environments by evaluating on the Berkeley DeepDrive Video dataset, where an object-centric model outperforms object-agnostic models in the low-data regimes.	artificial neural network;autonomous car;autonomous robot;coherence (physics);deep learning;end-to-end principle;instance (computer science);object detection;robotics;sparse matrix;taxonomy (general)	Dequan Wang;Coline Devin;Qi-Zhi Cai;Fisher Yu;Trevor Darrell	2018	CoRR		machine learning;computer science;artificial neural network;detector;artificial intelligence;uninterpretable;imperfect;robotics	Robotics	29.065049782602934	-49.20626224322758	18707
1945a6907f81805e5578d2003bee9cae6c4478af	image content-based active sensor planning for a mobile trinocular active vision system	vision system;active sensor;image sensors sensor systems machine vision cameras focusing image reconstruction robot sensing systems robot vision systems object detection process planning;affine projection;closed form solution;predefined target object image content based active sensor planning mobile trinocular active vision system stationary state generalized cameras parameter deterministic geometric specification geometric parameter mobile system operation captured image;mobile robot;dynamic system;mobile robots;working conditions;image sensors;stationary state;3d model;robot vision;scale invariant feature transform;local features;field of view;stereo vision;mobile robots image sensors active vision cameras robot vision;mobile systems;3d reconstruction;cameras;active vision	In this paper, we present a sensor planning approach for a mobile trinocular active vision system. At the stationary state (i.e., no motion) the sensor planning system calculates the generalized cameras' parameters (i.e., translational distance from the center, zoom, focus and vergence) using deterministic geometric specifications of both the sensors and the objects in their field of view. Some of these geometric parameters are difficult to be predetermined for the mobile system operation. In this paper, a new sensor planning approach, based on processing the content of the captured images, is proposed. The approach uses a combination of a closed-form solution for the translation between the three cameras, the vergence angle of the cameras as well as zoom and focus settings with the results of the correspondences between the acquired images and a predefined target object(s) obtained using the SIFT algorithm. We demonstrate the accuracy of the new approach using practical experiments.	active vision;algorithm;experiment;scale-invariant feature transform;sensor;stationary process;stationary state;vergence	Aly A. Farag;Alaa E. Abdel-Hakim	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1421722	mobile robot;computer vision;simulation;machine vision;computer science	Robotics	51.82392986072062	-35.91516350568117	18713
1704094f6de07952bb23aa51a270111e005e643a	markov random field-based clustering of vibration data	unsupervised learning;robot sensing systems;vibration data clustering;pattern clustering;optimisation;unsupervised learning markov processes mobile robots optimisation pattern clustering terrain mapping;vibrations;generic model;mobile robot;nickel;vibration segment markov random field vibration data clustering mobile robot unsupervised learning vibration signal constrained expectation maximization;prior distribution;mobile robots;vibration segment;markov random field;surface properties;constrained expectation maximization;data dependence;expectation maximization algorithm;terrain mapping;markov processes;efficient estimation;context;vibrations markov processes data models nickel context robot sensing systems;data models;vibration signal	A safe traversal of a mobile robot in an unknown environment requires the determination of local ground surface properties. As a first step, a broad structure of the underlying environment can be established by clustering terrain sections which exhibit similar features. In this work, we focus on an unsupervised learning approach to segment different terrain types according to the clustering of acquired vibration signals. Therefore, we present a Markov random field-based clustering approach taking the inherent temporal dependencies between consecutive measurements into account. The applied generative model assumes that the class labels of neighboring vibration segments are generated by prior distributions with similar parameters. A temporally constrained expectation maximization algorithm enables the efficient estimation of its parameters considering a predefined set of neighboring vibration segments. Since the size of the neighbor set proves to be data-dependent, we derive a general means of estimating this set size from the observed data. We show that the Markov random field clustering approach generates valid models for a variety of driving speeds even in situations of frequent terrain changes.	cluster analysis;coherence (physics);data dependency;expectation–maximization algorithm;generative model;markov chain;markov random field;mixture model;mobile robot;sensitivity and specificity;tree traversal;unsupervised learning	Philippe Komma;Andreas Zell	2010	2010 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2010.5649527	unsupervised learning;mobile robot;correlation clustering;constrained clustering;computer vision;fuzzy clustering;computer science;artificial intelligence;canopy clustering algorithm;machine learning;pattern recognition;cluster analysis;statistics	Robotics	47.76976973481079	-26.940360761013633	18727
fa10330f5603da721df7e2075981c4c5a2030e90	6dof decoupled roto-translation alignment of large-scale indoor point clouds		We address the problem of 6DOF alignment of large-scale point clouds of indoor spaces such that extensive 3D models can be assembled out of multiple point clouds. We present an algorithm that it is fast, insensitive to initial alignment and tolerates very low overlap. The algorithm is designed to exploit inherent characteristics of indoor spaces. It loosens the tight coupling between translation and rotation estimation such that these can be performed in consecutive steps with estimation problems of reduced complexity which can be reliably solved using strong features characteristic to indoor spaces. First, the point clouds are rotationally aligned to gravity using PCA. Then, the translation along gravity is computed through floor matching. Subsequently, the ground-plane rotation is determined by cross-correlating histogram signatures of surface normal directions. Finally, the ground-plane translation is determined by seeking the location where the bi-variate shift histogram of point pairs with high curvature values (called CPSHs) peaks. This voting-like approach avoids establishing correspondences through computationally demanding feature extraction and matching processes. To support very low overlap cases, the CPSH-based alignment is furthermore cast into a probabilistic framework that involves computing the CPSHs on segments of the point clouds and finally fused using an ML estimator. The results show that the proposed approach succeeds in the alignment of datasets for which general-purpose algorithms fail while being at least as efficient as the fastest methods previously proposed.	point cloud	Anas Al-Nuaimi;Sebastian Hilsenbeck;Adrian Garcea;Eckehard G. Steinbach	2017	Computer Vision and Image Understanding	10.1016/j.cviu.2016.08.004	mathematical optimization;simulation;mathematics;geometry	Vision	51.24812531239788	-46.95605154469002	18778
18d866b416bb1b180d4afb28ebbd4a8004d7a68a	spline-based motion recovery for 3d surfaces using nonrigid shape properties	dentistry;spline;motion control;differential geometry;geometry;biomedical imaging;motion estimation;deformable models;visualization;image registration;spline shape control motion estimation deformable models motion control visualization dentistry geometry biomedical imaging image registration;shape control;qualitative evaluation	We present a spline-based nonrigid motion and point correspondence recovery method for 3D surfaces. This method is based on differential geometry. Shape information is used to recover the point correspondences. In contrast to the majority of shape-based methods which assume that shape (unit normal, curvature) changes are minimumafter motion, our method focuses on the nonrigid relationship between before-motion and after-motion shapes. This nonrigid shape relationship is described by modeling the underlying non-rigid motion; we model it as a spline transformation which has global control over the entire motion field along with the local deformation integrated within. This provides our method certain advantages over some pure differential geometric methods which also utilize the nonrigid shape relationship but only work on local areas without a global control. For example, motion regularity is hard to implement in these pure differential geometric methods but is not a problem when the motion field is controlled by a spline transformation. Furthermore, the small deformation constraint introduced by the previous works is relaxed in our method. Experiments on both synthetic and real motions have been conducted. The quantitive and qualitative evaluations of our method are presented in this paper.	approximation algorithm;correspondence problem;experiment;lagrangian relaxation;motion field;normal (geometry);point of interest;spline (mathematics);synthetic intelligence	Min Li;Chandra Kambhamettu;Maureen Stone	2004	2004 Conference on Computer Vision and Pattern Recognition Workshop	10.1109/CVPR.2004.442	medical imaging;motion control;spline;differential geometry;computer vision;mathematical optimization;visualization;computer science;image registration;motion estimation;mathematics;geometry;motion field	Vision	53.02344962067885	-51.79331637202945	18787
d18c3aadcfb04a7e5be3bb79273fb285807f9d43	bootstrap prediction regions for multivariate autoregressive processes	vector autoregression;prediction error;autoregressive process;quantile estimation;var model	Two new methods for improving prediction regions in the context of vector autoregressive (VAR) models are proposed. These methods, which are based on the bootstrap technique, take into account the uncertainty associated with the estimation of the model order and parameters. In particular, by exploiting an independence property of the prediction error, we will introduce a bootstrap procedure that allows for better estimates of the forecasting distribution, in the sense that the variability of its quantile estimators is substantially reduced, without requiring additional bootstrap replications. The proposed methods have a good performance even if the disturbances distribution is not Gaussian. An application to a real data set is presented.	autoregressive model;booting;bootstrapping (statistics);spatial variability	Matteo Grigoletto	2005	Statistical Methods and Applications	10.1007/s10260-005-0113-y	econometrics;star model;pattern recognition;mathematics;vector autoregression;statistics	ML	29.46768043469184	-24.191574437191942	18853
13f06b08f371ba8b5d31c3e288b4deb61335b462	depth and appearance for mobile scene analysis	object recognition;optimisation;image motion analysis;video signal processing image motion analysis iterative methods object recognition optimisation traffic engineering computing;video signal processing;structure from motion mobile scene analysis pedestrian detection video ground plane estimation pedestrian zone robust stereo depth cues appearance based object detection object object occlusions iterative approach scene geometry belief propagation global optimization procedure geometry estimation;exact solution;psi_visics;iterative methods;system integration;pedestrian detection;graphical model;traffic engineering computing;image analysis geometry robustness object detection layout legged locomotion graphical models iterative methods belief propagation testing;global optimization;structure from motion;quantitative evaluation;scene analysis	In this paper, we address the challenging problem of simultaneous pedestrian detection and ground-plane estimation from video while walking through a busy pedestrian zone. Our proposed system integrates robust stereo depth cues, ground-plane estimation, and appearance-based object detection in a principled fashion using a graphical model. Object-object occlusions lead to complex interactions in this model that make an exact solution computationally intractable. We therefore propose a novel iterative approach that first infers scene geometry using belief propagation and then resolves interactions between objects using a global optimization procedure. This approach leads to a robust solution in few iterations, while allowing object detection to benefit from geometry estimation and vice versa. We quantitatively evaluate the performance of our proposed approach on several challenging test sequences showing strolls through busy shopping streets. Comparisons to various baseline systems show that it outperforms both a system using no scene geometry and one just relying on structure-from-motion without dense stereo.	baseline (configuration management);belief propagation;busy beaver;collision detection;computational complexity theory;depth perception;global optimization;graphical model;interaction;iteration;mathematical optimization;object detection;pedestrian detection;software propagation;structure from motion	Andreas Ess;Bastian Leibe;Luc Van Gool	2007	2007 IEEE 11th International Conference on Computer Vision	10.1109/ICCV.2007.4409092	computer vision;structure from motion;simulation;computer science;cognitive neuroscience of visual object recognition;machine learning;mathematics;iterative method;graphical model;global optimization;system integration	Vision	49.74739960063392	-48.14574432910145	19011
1b81c2686e1057ec5700cb641e37b8b98570d4ce	background subtraction and 3d localization of moving and stationary obstacles at level crossings	3d localization;obstacle detection;video surveillance;confidence measure;pixel image color analysis three dimensional displays noise cameras belief propagation motion detection;independent component analysis;color independent component analysis technique;stereo camera;video surveillance collision avoidance independent component analysis;video surveillance system background subtraction 3d localization obstacle detection system stereo camera color independent component analysis technique level crossing stereo matching algorithm;level crossing;stereo matching;belief propagation;three dimensional displays;image color analysis;pixel;background subtraction;stereo vision;stereo vision level crossing railroad safety color independent component analysis belief propagation confidence measure;railroad safety;collision avoidance;obstacle detection system;video surveillance system;stereo matching algorithm;color independent component analysis;motion detection;cameras;noise	This paper proposes an obstacle detection system for the purpose of preventing accidents at level crossings. In order to avoid the limits of already proposed technologies, this system uses stereo cameras to detect and localize multiple targets at the level crossing. In a first step, a background subtraction module is performed using the Color Independent Component Analysis (CICA) technique which allows to detect vehicles even if they are stopped (the main cause of accidents at Level Crossings). A novel robust stereo matching algorithm is then used to reliably localize in 3D each segmented object. Standard stereo datasets and real-world images are used to evaluate the performances of the proposed algorithm, showing the efficiency and robustness of the proposed video surveillance system.	algorithm;background subtraction;belief propagation;closed-circuit television;codebook;computer stereo vision;independent computing architecture;independent component analysis;mixture model;performance;sampling (signal processing);software propagation;stationary process;stereo camera;stereo cameras	Nizar Fakhfakh;Louahdi Khoudour;El-Miloudi El-Koursi;Jean-Luc Bruyelle;Alain Dufaux;Jacques Jacot	2010	2010 2nd International Conference on Image Processing Theory, Tools and Applications	10.1109/IPTA.2010.5586765	independent component analysis;stereo camera;computer vision;simulation;background subtraction;computer science;noise;stereopsis;level crossing;machine learning;pixel;belief propagation	Robotics	42.500994981219065	-45.60135395335478	19022
d1f64ee96598a4d7cf6d1bdb38250876c3a56c7a	automated detection and interpretation of earthquake seismograms by adaptive pattern recognition	pattern recognition	The measurement and interpretation of ground motion in seismograms is the primar source for our knowledge of the earth’s interior. As in other scientific areas, seismology has seen great improvements of its experimental facilities in the last decades. State-of-the-art observatories monitor earthquakes magnitudes below they are recognized by humans. Additionally, these observatories record explosions in quarries up to 1000 km apart and nuclear explosions all over the world. Over a year, these events sum up to some 10.000 data sets that must be found and processed in TeraBytes of raw data which are contaminated by local noise bursts of car traffic, sonic bangs and other sources. This task of routine preselection and processing is about to exceed the possibilities of human data analysis; any further progess will depend on automated methods. However, these methods must yield human-like performance not to degrade the achieved sensitivity and reliability.	pattern recognition	Manfred Joswig	1991		10.1007/978-3-642-77382-2_25	seismology;speech recognition;pattern recognition	Vision	40.88495592090517	-40.000338047614555	19047
930604945c6e0d7dab7080dda03260c31ccb4359	efficient label collection for unlabeled image datasets	classification accuracy label collection unlabeled image dataset visual classifier surveillance autonomous navigation scene understanding supervised classifier labeling process active learning group based labeling labeling workload label noise trained classifier re clustering data hierarchically clustered data visual concept granularity high performing classifier labeling technique;surveillance image classification learning artificial intelligence pattern clustering;noise	Visual classifiers are part of many applications including surveillance, autonomous navigation and scene understanding. The raw data used to train these classifiers is abundant and easy to collect but lacks labels. Labels are necessary for training supervised classifiers, but the labeling process requires significant human effort. Techniques like active learning and group-based labeling have emerged to help reduce the labeling workload. However, the possibility of collecting label noise affects either the efficiency of these systems or the performance of the trained classifiers. Further, many introduce latency by iteratively retraining classifiers or re-clustering data. We introduce a technique that searches for structural change in hierarchically clustered data to identify a set of clusters that span a spectrum of visual concept granularities. This allows us to efficiently label clusters with less label noise and produce high performing classifiers. The data is hierarchically clustered only once, eliminating latency during the labeling process. Using benchmark data we show that collecting labels with our approach is more efficient than existing labeling techniques, and achieves higher classification accuracy. Finally, we demonstrate the speed and efficiency of our system using real-world data collected for an autonomous navigation task.	active learning (machine learning);autonomous robot;benchmark (computing);cluster analysis;iteratively reweighted least squares;supervised learning	Maggie Wigness;Bruce A. Draper;J. Ross Beveridge	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2015.7299090	sequence labeling;computer science;noise;machine learning;pattern recognition;data mining	Vision	31.68274084414902	-47.01817059697664	19064
bc5aec324e3dc6fdc28dd2ca9268696f777afdb1	a deformable model driven visual method for handling clothes	manipulators;cameras manipulators stereo image processing state estimation robot vision;deformable models robot vision systems humans intelligent systems clothing industry textile industry smart cameras state estimation manipulators computer graphics;state estimation visual method clothes handling deformable model driven method manipulators stereo cameras;state estimation;robot vision;stereo image processing;deformable model;cameras	In this paper, we propose a deformable model-driven method to obtain the 3D information necessary for handling clothes by manipulators from observation with stereo cameras. The task considered in this paper is to hold up a target part of clothes (e.g. one shoulder of a pullover) by the second manipulator, when the clothes are held in the air at any point by the first manipulator. First, the method calculates possible 3D shapes of the hanging clothes by simulating the clothes deformation. The 3D shape whose appearance gives the best fit with the observed images is selected as estimation of the current state. Then, based on the estimated shape, the 3D position and normal direction of the part where the second manipulator should hold are calculated. The results of preliminary experiments using actual two manipulators have shown the good potential of the proposed method.	accu (organisation);bottom-up proteomics;computational complexity theory;control theory;curve fitting;emoticon;experiment;model-driven architecture;normal (geometry);resultant;simulation;stereo camera;stereo cameras;top-down and bottom-up design	Yasuyo Kita;Fuminori Saito;Nobuyuki Kita	2004	IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004	10.1109/ROBOT.2004.1308874	stereo cameras;computer vision;simulation;engineering;computer graphics (images)	Robotics	50.727919763007165	-40.7333980409313	19105
a4c32bebbef4dbe06a845991f1c5fd551058d942	bidirectional markov chain monte carlo particle filter for articulated human motion tracking		A novel framework of particle filter, named bidirectional Markov chain Monte Carlo particle filter (BMCMCPF), has been proposed to estimate articulated human movement state and action category jointly. Owing to the reason that we regard action category as the estimated state in our framework, firstly the motion models for every possible action are built via autoregressive modeling for the captured motion data with minimum distance. Meanwhile, the dynamic model and observation model also get coupled so that tracking and recognition can achieve synchronously. Then, the state estimation is completed by using the bidirectional Marko chain Monte Carlo sampling. BMCMCPF can not only improve the tracking performance as its global optimization property, but also smooth the joint’s movement trajectories to ensure the motion coordination. The experimental results on HumanEva datasets show that the effectiveness of BMCMCPF with unknown motion modality in solving the tracking problem.	markov chain monte carlo;monte carlo method;particle filter	Anan Yu;Chuanzhen Li;Long Ye;Jingling Wang;Qin Zhang	2017		10.1007/978-981-10-8108-8_38	global optimization;mathematical optimization;autoregressive model;monte carlo method;markov chain monte carlo;mathematics;particle filter;match moving	Vision	44.865187941108985	-47.249136441798576	19106
3d3affe4272f4d0c00721fd648e5f2f816964aa3	simultaneous pose, focal length and 2d-to-3d correspondences from noisy observations	comunicacion de congreso;conference report;robots	Matching methods based on RANSAC rely on having a small outlier rate. This way, the probability of obtaining a correct minimal set of points is high enough to be achieved in fixed time. If the percentage of outliers increases, the number of RANSAC loops needed to obtain a correct set of minimal points grows exponentially. We propose a matching algorithm that performs robust matching under the presence of a large percentage of outliers. Our approach is a generalization of the work in [1] to deal with uncalibrated cameras and noisy 3D information.	algorithm;focal (programming language);matching (graph theory);random sample consensus	Adrián Peñate Sánchez;Eduard Serradell;Francesc Moreno-Noguer;Juan Andrade-Cetto	2013		10.5244/C.27.82	robot;computer science;artificial intelligence	Vision	51.41030045608278	-46.536857678277855	19197
c36ccaaff069067d293c3d6316d876cd75f9a10b	binary linear compression for multi-label classification		In multi-label classification tasks, labels are commonly related with each other. It has been well recognized that utilizing label relationship is essential to multi-label learning. One way to utilizing label relationship is to map labels to a lower-dimensional space of uncorrelated labels, where the relationship could be encoded in the mapping. Previous linear mapping methods commonly result in regression subproblems in the lower-dimensional label space. In this paper, we disclose that mappings to a low-dimensional multi-label regression problem can be worse than mapping to a classification problem, since regression requires more complex model than classification. We then propose the binary linear compression (BILC) method that results in a binary label space, leading to classification subproblems. Experiments on several multi-label datasets show that, employing classification in the embedded space results in much simpler models than regression, leading to smaller structure risk. The proposed methods are also shown to be superior to some state-of-the-art approaches.	algorithm;compressed sensing;derivative-free optimization;embedded system;experiment;mathematical optimization;multi-label classification	Wen-Ji Zhou;Yang Yu;Min-Ling Zhang	2017		10.24963/ijcai.2017/496	machine learning;artificial intelligence;multi-label classification;compression (physics);computer science;binary number	AI	24.77200466211662	-44.964540990905896	19238
be45d9beba4353a3f46ba502b07f772f5efe3b6d	spatio-temporal alignment and hyperspherical radon transform for 3d gait recognition in multi-view environments	image recognition;multi view environments;image motion analysis;image processing;radon transform;legged locomotion;spatio temporal alignment;3d voxel reconstruction spatio temporal alignment hyperspherical radon transform 3d gait recognition multi view environments multicamera scenarios data representation;biometrics;gait recognition;layout;hyperspherical radon transform;data representation;noise measurement;multicamera scenarios;data analysis;spatio temporal data;transforms data analysis gait analysis image motion analysis image reconstruction image sequences pattern recognition;three dimensional displays;image reconstruction;conference report;robustness image reconstruction legged locomotion image recognition data analysis layout biometrics linear discriminant analysis image processing image analysis;transforms;gait analysis;pattern recognition;3d voxel reconstruction;time domain;robustness;image analysis;3d gait recognition;linear discriminant analysis;cameras;leg;image sequences	This paper presents a view-invariant approach to gait recognition in multi-camera scenarios exploiting a joint spatio-temporal data representation and analysis. First, multi-view information is employed to generate a 3D voxel reconstruction of the scene under study. The analyzed subject is tracked and its centroid and orientation allow recentering and aligning the volume associated to it, thus obtaining a representation invariant to translation, rotation and scaling. Temporal periodicity of the walking cycle is extracted to align the input data in the time domain. Finally, Hyperspherical Radon Transform is presented as an efficient tool to obtain features from spatio-temporal gait templates for classification purposes. Experimental results prove the validity and robustness of the proposed method for gait recognition tasks with several covariates.	3d reconstruction from multiple images;algorithm;align (company);data (computing);date and time representation by country;dimensionality reduction;gait analysis;image scaling;linear separability;quasiperiodicity;sparse language;sparse matrix;voxel	Cristian Canton-Ferrer;Josep R. Casas;Montse Pardàs	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops	10.1109/CVPRW.2010.5544615	iterative reconstruction;layout;computer vision;radon transform;image analysis;speech recognition;gait analysis;image processing;time domain;computer science;noise measurement;pattern recognition;mathematics;external data representation;linear discriminant analysis;data analysis;biometrics;robustness	Vision	46.98757957844426	-49.56118013637128	19381
36e41e67410d00b23127f04e9a8958a3988eae4f	flexible synthesis of video frames based on motion hints	quantisation signal access protocols closed loop systems image coding meta data probability;jpip server video frames motion hint probability flexible synthesis loose description global description metadata continuous motion invertible motion multiple frames spatially overlapping probabilistic multiscale approach quantization contrast changes laplacian pyramid generating interframe predictions closed loop prediction open loop prediction remote browsing surveillance footage jpeg2000 interactive protocol server;noise quantization signal noise measurement laplace equations decoding tracking image color analysis	In this paper, we propose the use of “motion hints” to produce interframe predictions. A motion hint is a loose and global description of motion that can be communicated using metadata; it describes a continuous and invertible motion model over multiple frames, spatially overlapping other motion hints. A motion hint provides a reasonably accurate description of motion but only a loose description of where it is applicable; it is the task of the client to identify the exact locations where this motion model is applicable. The focus of this paper is a probabilistic multiscale approach to identifying these locations of applicability; the method is robust to noise, quantization, and contrast changes. The proposed approach employs the Laplacian pyramid; it generates motion hint probabilities from observations at each scale of the pyramid. These probabilities are then combined across the scales of the pyramid starting from the coarsest scale. The computational cost of the approach is reasonable, and only the neighborhood of a pixel is employed to determine a motion hint probability, which makes parallel implementation feasible. This paper also elaborates on how motion hint probabilities are exploited in generating interframe predictions. The scheme of this paper is applicable to closed-loop prediction, but it is more useful in open-loop prediction scenarios, such as using prediction in conjunction with remote browsing of surveillance footage, communicated by a JPEG2000 Interactive Protocol (JPIP) server. We show that the interframe predictions obtained using the proposed approach are good both visually and in terms of PSNR.	algorithmic efficiency;closed-circuit television;computation;frame (physical object);jpeg 2000;jpip;peak signal-to-noise ratio;pixel;probabilistic database;probability;pyramid (geometry);quantization (signal processing);server (computer);server (computing)	Aous Thabit Naman;David S. Taubman	2014	IEEE Transactions on Image Processing	10.1109/TIP.2014.2332763	computer vision;speech recognition;computer science;theoretical computer science	Vision	40.80185137975671	-35.75338242990903	19425
5df6fbdfc293175e7e7ddadadee525e3a7494410	bayesian field theory: nonparametric approaches to density estimation	eigenvalues and eigenfunctions;field theory;non gaussian models;nonparametric density estimation;neural net training;probability;nongaussian models;learning;neural nets;gaussian processes;bayesian field theory;boundary conditions;bayes methods;low dimensional problems;discretization;bayesian methods;learning bayesian field theory nonparametric density estimation non gaussian models nongaussian models discretization computational feasibility low dimensional problems neural net training;density estimation;computational feasibility;quantum mechanics;computational complexity;bayesian methods lagrangian functions quantum mechanics encoding boundary conditions eigenvalues and eigenfunctions gaussian processes;learning artificial intelligence;encoding;lagrangian functions;neural nets probability bayes methods computational complexity learning artificial intelligence	Bayesian field theory denotes a nonparametric Bayesian approach for learning functions from observational data. Based on the principles of Bayesian statistics, a particular Bayesian field theory is defined by combining two models: a likelihood model, providing a probabilistic description of the measurement process, and a prior model, providing the information necessary to generalize from training to non–training data. The particular likelihood models discussed in the paper are those of general density estimation, Gaussian regression, clustering, classification, and models specific for inverse quantum problems. Besides problem typical hard constraints, like normalization and positivity for probabilities, prior models have to implement all the specific, and often vague, a priori knowledge available for a specific task. Nonparametric prior models discussed in the paper are Gaussian processes, mixtures of Gaussian processes, and non–quadratic potentials. Prior models are made flexible by including hyperparameters. In particular, ∗Email: lemm@uni-muenster.de, WWW: http://pauli.uni-muenster.de/∼lemm/	cluster analysis;gaussian blur;gaussian process;mixture model;quantum field theory;statistical classification;vagueness;www	Jörg C. Lemm	2000		10.1109/IJCNN.2000.857868	mathematical optimization;density estimation;bayesian probability;boundary value problem;computer science;machine learning;probability;discretization;gaussian process;mathematics;field theory;computational complexity theory;artificial neural network;encoding;statistics	ML	30.283426292136493	-31.88238413566397	19480
11194b7a37a829ad1aefdf1e1076a9c8d133b292	building face reconstruction from sparse view of monocular camera	feature extraction and matching;building faces detection;linear triangulation;3d reconstruction	This paper proposes a method for building detection and 3D reconstruction of building face from sparse view of monocular camera. According to this method, building faces are detected by using color, straight line, edge and vanishing point. In the next step, building faces from multi view are extracted. Point clouds of building face are obtained from triangulation step. The building faces are reconstructed by plane fitting afterward. The simulation results will demonstrate the effectiveness of this method.	sparse	Le-My Ha;Kang-Hyun Jo	2011		10.1007/978-3-642-25944-9_73	3d reconstruction;computer vision;computer science;mathematics;geometry;computer graphics (images)	Vision	50.78249622275522	-48.10036431766787	19483
0eaa0951f301cd00bf958e12f7e5f5c3ad4ab121	a wavelet transform-artificial neural network (wt-ann) based rotating machinery fault diagnostics methodology	artificial neural network;wavelet transform	This paper outlines a Wavelet Transform (WT) based Artificial Neural Network (ANN) input data pre-processing scheme and presents the results of localized gear tooth defect recognition tests by employing this proposed methodology. The methodology consists of calculating Daubechies’ 20-order (DAUB-20) mean-square dilation WTs of the data, and then selecting predominant wavelet coefficients distributed to certain levels of these WTs as inputs to ANNs for pattern recogn iti n. The test results show that a fairly small sized backpropagation network trained with a reasonably small number of training sets can detect and classify various types or degrees of failures occurring on a spur gear pair successfully.	artificial neural network;backpropagation;coefficient;data pre-processing;dilation (morphology);preprocessor;software bug;wavelet transform	Seref Naci Engin;Kayhan Gulez	1999			wavelet;wavelet transform;probabilistic neural network;artificial neural network;machine learning;dilation (morphology);artificial intelligence;backpropagation;small number;computer science	ML	36.58334968845628	-31.97434334978003	19495
f9d451fdfebb30325a4e33d01fe1b59dbcbed364	partial discharge pattern classification using composite versions of probabilistic neural network inference engine	partial discharge pd;probabilistic neural network pnn;partial discharge;pattern classification;probabilistic neural network;artificial neural network ann;composite original probabilistic neural network copnn;artificial neural network	A major requirement of any power apparatus is the reliable performance of its insulation. The incidence of minor flaws and irregularities such as voids, surface imperfections, in the electrical insulation is however inevitable and lead to partial discharge (PD). Since each defect has a unique degradation mechanism, it is imperative to ascertain the correlation between the discharge patterns and the type of defect in order to evaluate the quality of the insulation. Efforts to correlate discharge patterns with the type of defects have been undertaken by several researchers. Though encouraging attempts to recognize and classify simple PD defect sources have been reported, misclassifications still occur, which affect the assessment of the index of the insulating degradation. A Composite Probabilistic Neural Network Inference System has been devised and elucidated in this research using two versions of Probabilistic Neural Network. The inference is obtained based on the outcome to innovatively conceived fourteen unique characteristic vector inputs to enable an accurate and reliable decision in the classification of complex stochastic PD patterns thus obviating the necessity of skilled operators. Validation of the fingerprints of PD patterns has also been carried out using well-established techniques. 2007 Elsevier Ltd. All rights reserved.	artificial neural network;discharger;elegant degradation;fingerprint;imperative programming;incidence matrix;inference engine;probabilistic neural network;software bug	B. Karthikeyan;S. Gopal;S. Venkatesh	2008	Expert Syst. Appl.	10.1016/j.eswa.2007.02.005	partial discharge;probabilistic neural network;computer science;artificial intelligence;machine learning;data mining;time delay neural network;artificial neural network	ML	34.90842905976619	-30.693871622539486	19541
34b14ae503f4555d0254fc78ceb01be09ec439e3	facial feature localization using graph matching with higher order statistical shape priors and global optimization	graph theory;complex objects;deformable face matching;optimisation;probability;probability face recognition feature extraction graph theory image matching optimisation;convex quadratic programs;shape face training computational modeling optimization nose image edge detection;image matching;facial feature localization;convex quadratic programming;training;graph matching;dual decomposition approach;higher order;higher order statistics;higher order statistical shape priors;objective function;probabilistic model;convex quadratic programs facial feature localization graph matching higher order statistical shape priors global optimization deformable face matching facial images probabilistic model dual decomposition approach;computational modeling;face recognition;shape;image edge detection;feature extraction;graphical model;facial features;global optimization;face;optimization;shape priors;facial images;nose;matching method;point distribution model	This paper presents a graphical model for de-formable face matching and landmark localization under an unknown non-rigid warp. The proposed model learns and combines statistics of both appearance and shape variations of facial images (learnt purely from a set of frontal training images) in a complex objective function in an unsupervised manner. Local and global shape variations are included in the objective function as binary and higher order clique potentials. The proposed approach exploits the sparseness of facial features to reduce the complexity of inference over the probabilistic model. Besides presenting a method for face feature localization, the paper proposes a framework for incorporation of statistical shape priors as higher order cliques into MRFs. The problem of optimizing the objective function is performed using the dual decomposition approach in which the higher order subproblems based on point distribution models are formulated as instances of convex quadratic programs. The evaluation of the approach for feature localization is performed both on the frontal and rotated images of the XM2VTS dataset images as well as images collected from Google. The method shows high robustness to partial occlusion, pose changes etc. The method is then applied as an initialization step for a more costly matching method and is shown to be instrumental in improving performance and reducing runtime.	algorithm;bitwise operation;global optimization;graphical model;iterative method;lagrangian relaxation;loss function;markov random field;matching (graph theory);mathematical optimization;neural coding;optimization problem;run time (program lifecycle phase);statistical model;unsupervised learning	Shervin Rahimzadeh Arashloo;Josef Kittler;William J. Christmas	2010	2010 Fourth IEEE International Conference on Biometrics: Theory, Applications and Systems (BTAS)	10.1109/BTAS.2010.5634502	facial recognition system;face;statistical model;point distribution model;computer vision;higher-order logic;feature extraction;shape;computer science;graph theory;machine learning;pattern recognition;probability;mathematics;graphical model;computational model;matching;global optimization	Vision	46.831318128306435	-51.91903417576829	19554
d65e3c317487b8c7a895f6049351ed7d9229e4fe	an approach to integrating shape and biomedical attributes in vascular models	drug delivery;biomedical cad;mechanical property;computer model;image understanding;geometric representation;blood vessel;magnetic resonance image;shape representation;medical image;vascular modeling;interaction model;blood flow;shape modeling;physiological models;swept volumes;growth model;tissue engineering	Computational models have been used widely in tissue engineering research and have proven to be powerful tools for bio-mechanical analysis (i.e., blood flow, growth models, drug delivery, etc). This paper focuses on developing higher-fidelity models for vascular structures and blood vessels that integrate computational shape representations with bio-mechanical properties and features. Previous work in computeraided vascular modeling comes from two communities. For those in bio-medical imaging, the goal of past research has been to develop image understanding techniques for the interpretation of x-ray, magnetic resonance imaging (MRI), or other radiological data. These representations are predominantly discrete shape models that are not tied to physiological properties. The other corpus of existing work comes from those interested in developing physiological models for vascular growth and behavior based on bio-medical attributes. These models usually either have a highly simplified shape representation, or lack one entirely. Further, neither of these representations are suitable for the kind of interactive modeling required by tissue engineering applications. This paper aims to bridge these two approaches and develop a set of mathematical tools and algorithms for feature-based representation and computer-aided modeling of vascular trees for use in computer-aided tissue engineering applications. The paper offers a multi-scale representation based on swept volumes and a feature-based representation that can attribute the geometric representation with information about blood flow, pressure, and other bio-mechanical properties. The paper shows how the resulting representation can be used as part of an overall approach for designing and visualizing vascular scaffolds. As a real-world example, we show how this computational model can be used to develop a tissue scaffold with an embedded Preprint submitted to Elsevier 20 February 2007 vascular structure. Such scaffolds may prove useful in a number of bio-medical applications, including the growth of replacement tissue grafts and in-vitro study of the pharmacological affects of new drugs on tissue cultures.	algorithm;british informatics olympiad;computation;computational model;computer vision;embedded system;medical imaging;resonance	Jie Li;William C. Regli;Wei Sun	2007	Computer-Aided Design	10.1016/j.cad.2007.03.002	computer vision;engineering;blood flow;magnetic resonance imaging;biological engineering;tissue engineering;engineering drawing;mechanical engineering	AI	39.56773142585775	-39.042288181867555	19568
5e8a34a8d9ea698de404cd985352a576446b686a	estimation of linear non-gaussian acyclic models for latent factors	model identification;causal analysis;latent factors;independent component analysis;causal relation;covariance structure	Many methods have been proposed for discovery of causal relations among observed variables. But one often wants to discover causal relations among latent factors rather than observed variables. Some methods have been proposed to estimate linear acyclic models for latent factors that are measured by observed variables. However, most of the methods use data covariance structure alone for model identification, and this leads to a number of indistinguishable models. In this paper, we show that a linear acyclic model for latent factors is identifiable when the data are non-Gaussian.	causal filter;consistency model;directed acyclic graph;ising model;latent variable;system identification	Shohei Shimizu;Patrik O. Hoyer;Aapo Hyvärinen	2009	Neurocomputing	10.1016/j.neucom.2008.11.018	latent class model;latent variable;independent component analysis;structural equation modeling;econometrics;system identification;computer science;machine learning;pattern recognition;mathematics;probabilistic latent semantic analysis;local independence;latent variable model;statistics	AI	28.01693548310026	-26.130644955800975	19671
3130cf90affcbed4ed44e6ae7f12d6dd5fec6842	3d shape reconstruction from a humanoid generated video sequence	cameras three dimensional displays robot vision systems shape trajectory image reconstruction;trajectory;shape;three dimensional displays;image reconstruction;shape recognition humanoid robots image reconstruction image sensors image sequences;monocular visual based control 3d shape reconstruction humanoid generated video sequence monocular video sequence walking humanoid robot space carving algorithm;robot vision systems;cameras	This paper presents a strategy for estimating the geometry of an interest object from a monocular video sequence acquired by a walking humanoid robot. The problem is solved using a space carving algorithm, which relies on both the accurate extraction of the occluding boundaries of the object as well as the precise estimation of the camera pose for each video frame. For data acquisition, a monocular visual-based control has been developed that drives the trajectory of the robot around an object placed on a small table. Due to the stepping of the humanoid, the recorded sequence is contaminated with artefacts that affect the correct extraction of contours along the video frames. To overcome this issue, a method that assigns a fitness score for each frame is proposed, delivering a subset of camera poses and video frames that produce consistent 3D shape estimations of the objects used for experimental evaluation.		Pablo Arturo Martínez González;David Varas;Mario Castelán;M. Camacho;Ferran Marqués;Gustavo Arechavaleta	2014	2014 IEEE-RAS International Conference on Humanoid Robots	10.1109/HUMANOIDS.2014.7041439	iterative reconstruction;computer vision;simulation;shape;trajectory;video tracking;computer graphics (images)	Robotics	49.40348995223309	-47.02855681032445	19679
c1abe6db821895a7b5e0c0a53928fbf8df081239	adaptive wavelet series estimation in separable nonparametric regression models	rate of convergence;curse of dimensionality;regression model;additive model;wavelet series;data dependence;nonparametric regression;curve estimation;wavelet regularization;adaptive estimation	It is well-known that multivariate curve estimation suffers from the “curse of dimensionality.” However, reasonable estimators are possible, even in several dimensions, under appropriate restrictions on the complexity of the curve. In the present paper we explore how much appropriate wavelet estimators can exploit a typical restriction on the curve such as additivity. We first propose an adaptive and simultaneous estimation procedure for all additive components in additive regression models and discuss rate of convergence results and data-dependent truncation rules for wavelet series estimators. To speed up computation we then introduce a wavelet version of functional ANOVA algorithm for additive regression models and propose a regularization algorithm which guarantees an adaptive solution to the multivariate estimation problem. Some simulations indicate that wavelets methods complement nicely the existing methodology for nonparametric multivariate curve estimation.	wavelet transform	Umberto Amato;Anestis Antoniadis	2001	Statistics and Computing	10.1023/A:1011929305660	econometrics;mathematical optimization;curse of dimensionality;machine learning;multivariate kernel density estimation;mathematics;rate of convergence;additive model;nonparametric regression;regression analysis;statistics	ML	29.24561358794794	-25.507381552746345	19696
b7b81c626c40f307b96de0f7568bf6d22ac4145e	motion context adaptive fusion of inertial and visual pedestrian navigation		We use motion context recognition to enhance the result of our infrastructure-free indoor navigation algorithm. Target applications are difficult navigation scenarios such as first responder, rescue, and tactical applications. Our navigation algorithm uses inertial navigation and visual navigation fusion. Random Forest classifier algorithm is taught with training data from Inertial Measurement Unit and visual navigation data to classify between walking, running and climbing. This information is used both in pedestrian navigation to do stationarity detection with adaptive threshold and in particle filter fusion to exclude visual data from during climbing. Methods are evaluated in an indoor navigation test where person wearing tactical equipment moves through a building. Results show improvement of positioning accuracy based on loop closure error on the test track especially when the movement is fast paced. The loop closure error was reduced on average 4 % in two tests when movement was slow and 14 % when movement was fast.	algorithm;inertial navigation system;machine vision;particle filter;random forest;stationary process	Jesperi Rantanen;Maija M&#x00E4;kel&#x00E4;;Laura Ruotsalainen;Martti Kirkko-Jaakkola	2018	2018 International Conference on Indoor Positioning and Indoor Navigation (IPIN)	10.1109/IPIN.2018.8533872		Robotics	47.5264367371445	-40.90933587671259	19697
7559df35d786dad7577824153720795f7da762c8	localizing by describing: attribute-guided attention localization for fine-grained recognition		A key challenge in fine-grained recognition is how to find and represent discriminative local regions. Recent attention models are capable of learning discriminative region localizers only from category labels with reinforcement learning. However, not utilizing any explicit part information, they are not able to accurately find multiple distinctive regions. In this work, we introduce an attribute-guided attention localization scheme where the local region localizers are learned under the guidance of part attribute descriptions. By designing a novel reward strategy, we are able to learn to locate regions that are spatially and semantically distinctive with reinforcement learning algorithm. The attribute labeling requirement of the scheme is more amenable than the accurate part location annotation required by traditional part-based fine-grained recognition methods. Experimental results on the CUB-200-2011 dataset [1] demonstrate the superiority of the proposed scheme on both fine-grained recognition and attribute recognition.	algorithm;algorithmic efficiency;discriminative model;experiment;matrix regularization;reinforcement learning	Xiao Liu;Jiang Wang;Shilei Wen;Errui Ding;Yuanqing Lin	2017			computer vision;machine learning;pattern recognition;mathematics	Vision	29.115672189471915	-50.99765184231521	19795
6e9ad0a57f40ce17baa8f1ed1b154c05c9e7ed82	sniffer: an electronic nose	pattern recognition system;waste water recognition;partially overlapping sensitivities;fuzzy classification;sensor systems;sensor phenomena and characterization;sniffer;chemical engineering computing;municipal sewers;leather tanneries;resistance change;conducting polymer sensor array;waste disposal;chemical engineering computing chemical sensors pattern classification waste disposal sensor fusion electric sensing devices;sensor response shape;shape;waste water;pattern classification;conducting polymer;pattern recognition;municipal sewers sniffer electronic nose conducting polymer sensor array partially overlapping sensitivities pattern recognition system linguistic fuzzy classification method sensor response shape linguistic expressions fuzzy partition odorants resistance change waste water recognition industrial activities leather tanneries;electronic noses sensor arrays sensor phenomena and characterization sensor systems acoustic sensors polymers chemical sensors shape intelligent sensors fuzzy systems;electronic noses;acoustic sensors;sensor fusion;linguistic fuzzy classification method;electronic nose;polymers;odorants;linguistic expressions;fuzzy systems;sensor arrays;intelligent sensors;fuzzy partition;electric sensing devices;chemical sensors;industrial activities	In this paper we present SNIFFER, an electronic nose consisting of an array of conducting polymer sensors with partially overlapping sensitivities, and a pattern recognition system based on a new linguistic fuzzy classification method The method describes the shape of the sensor responses in terms of linguistic expressions, which are derived from a fuzzy partition of the area occupied by each response. The sensors are exposed to odorants and the percentage change in resistance is used for classification purposes. Results of the application of SNIFFER to the recognition of different types of waste water produced both by the industrial activities of leather tanneries and by municipal sewers are presented.	fuzzy classification;pattern recognition;polymer;relative change and difference;sensor	Fabio Di Francesco;Beatrice Lazzerini;Francesco Marcelloni;Giovanni Pioggia	1999		10.1109/KES.1999.820153	structural engineering;electronic engineering;engineering;forensic engineering	Robotics	36.93921279036284	-33.8117907130266	19805
2580c000246590980126f5979de1a68aeb27f14c	geometric pdes on weighted graphs for semi-supervised classification	databases;standards;image processing;pattern classification database management systems graph theory partial differential equations;databases partial differential equations weighted graphs p laplacian equations eikonal equations semisupervised classification tasks geometric pde image processing data classification;equations databases standards labeling vectors image processing classification algorithms;vectors;classification algorithms;labeling	In this paper, we consider the adaptation of two Partial Differential Equations (PDEs) on weighted graphs, p-Laplacian and eikonal equations, for semi-supervised classification tasks. These equations are a discrete analogue of well known geometric PDEs, which are widely used in image processing. While the p-Laplacian on graphs was intensively used in data classification, few works relate to the eikonal equation for data classification. The methods are illustrated through semi-supervised classification tasks on databases, where we compare the two algorithms. The results show that these methods perform well regarding the state-of-the-art and are applicable to the task of semi-supervised classification.	algorithm;database;experiment;image processing;machine learning;matrix regularization;rendering equation;run time (program lifecycle phase);semi-supervised learning;semiconductor industry;supervised learning	Matthieu Toutain;Abderrahim Elmoataz;Olivier Lézoray	2014	2014 13th International Conference on Machine Learning and Applications	10.1109/ICMLA.2014.43	labeling theory;discrete mathematics;image processing;computer science;machine learning;pattern recognition;mathematics	ML	29.54827982316646	-40.56901317617509	19822
902bd133b9bf4feb1ba9b3b5c41406efbb642241	robust kernelized multi-view self-representations for clustering by tensor multi-rank minimization		Most recently, tensor-SVD is implemented on multi-view self-representation clustering and has achieved the promising results in many real-world applications such as face clustering, scene clustering and generic object clustering. However, tensor-SVD based multi-view self-representation clustering is proposed originally to solve the clustering problem in the multiple linear subspaces, leading to unsatisfactory results when dealing with the case of non-linear subspaces. To handle data clustering from the non-linear subspaces, a kernelization method is designed by mapping the data from the original input space to a new feature space in which the transformed data can be clustered by a multiple linear clustering method. In this paper, we make an optimization model for the kernelized multi-view self-representation clustering problem. We also develop a new efficient algorithm based on the alternation direction method and infer a closed-form solution. Since all the subproblems can be solved exactly, the proposed optimization algorithm is guaranteed to obtain the optimal solution. In particular, the original tensor-based multi-view self-representation clustering problem is a special case of our approach and can be solved by our algorithm. Experimental results on several popular real-world clustering datasets demonstrate that our approach achieves the state-of-the-art performance.	algorithm;cluster analysis;feature vector;kernel (operating system);kernel method;kernelization;mathematical optimization;nonlinear system;singular value decomposition	Yanyun Qu;Jinyan Liu;Yuan Xie;Wensheng Zhang	2017	CoRR		linear subspace;machine learning;tensor;pattern recognition;artificial intelligence;minification;cluster analysis;kernelization;feature vector;computer science;special case	AI	24.92962696019243	-42.11264646498772	19884
7bdc374a8ae40a5ee45a51e8dad360e7cdc484dd	robust domain adaptation on the l1-grassmannian manifold		Domain adaptation aims to remedy the loss in classification performance that often occurs due to domain shifts between training and testing datasets. This problem is known as the dataset bias attributed to variations across datasets. Domain adaptation methods on Grassmann manifolds are among the most popular, including Geodesic Subspace Sampling and Geodesic Flow Kernel. Grassmann learning facilitates compact characterization by generating linear subspaces and representing them as points on the manifold. However, Grassmannian construction is based on PCA which is sensitive to outliers. This motivates us to find linear projections that are robust to noise, outliers, and dataset idiosyncrasies. Hence, we combine L1-PCA and Grassmann manifolds to perform robust domain adaptation. We present empirical results to validate improvements and robustness for domain adaptation in object class recognition across datasets.	digital single-lens reflex camera;domain adaptation;kernel (operating system);manifold regularization;statistical manifold;unsupervised learning;webcam	Sriram Kumar;Andreas E. Savakis	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2016.136	mathematical optimization;topology;machine learning;mathematics	Vision	26.753140028785996	-43.19214837234144	19984
ef8834c8b4429fe35e275a3fb9c06ceaab36035c	detection and recognition of 3d targets in panchromatic gray scale imagery using a biologically-inspired algorithm	target classification 3d target detection 3d target recognition panchromatic gray scale imagery biologically inspired algorithm biologically inspired map seeking circuit template matching problem computer vision vehicle location 2d panchromatic image target pose;azimuth;object recognition;target classification;map seeking circuit;3d target recognition;image resolution;image matching;vehicle location;biologically inspired algorithm;image classification;3d target detection;three dimensional;panchromatic gray scale imagery;computer vision;image edge detection;target recognition;three dimensional displays;solid modeling;stereo image processing;integrated circuit modeling;image recognition target recognition vehicles object detection circuits computer vision biological system modeling prototypes azimuth testing;biologically inspired map seeking circuit;target pose;stereo image processing computer vision image classification image matching object detection object recognition pose estimation;target detection;2d panchromatic image;template matching;template matching problem;target recognition map seeking circuit;algorithm design and analysis;object detection;pose estimation	A three-dimensional (3D) target detection and recognition algorithm, using the biologically-inspired MapSeeking Circuit (MSC), is implemented to efficiently solve the typical template matching problem in computer vision. Given a 3D template model of a vehicle, this prototype locates the vehicle in a two-dimensional (2D) panchromatic image and determines its pose (i.e. viewing azimuth, elevation, scale, and in-plane rotation). In our implementation, we introduce a detection stage followed by the spawning of multiple MSC processes in parallel to classify and determine the pose of the detection candidates. Our implementation increases the speed of detection and allows efficient classification when multiple targets are present in the same image. We present promising results after applying our algorithm to challenging real world test imagery.	3d computer graphics;algorithm;computer vision;grayscale;prototype;template matching	Patricia Murphy;Pedro A. Rodriguez;Sean R. Martin	2009	2009 IEEE Applied Imagery Pattern Recognition Workshop (AIPR 2009)	10.1109/AIPR.2009.5466310	computer vision;geography;pattern recognition;remote sensing	Vision	50.60866321699553	-39.38059188294863	20010
06b684b39bc06054fe622a6c3371d851d18259ac	relative location for light field saliency detection	object detection feature extraction;raw image;light field;saliency detection;会议论文;plenoptic camera;relative location;cameras focusing image color analysis estimation lenses feature extraction optical imaging;plenoptic cameras relative location light field saliency detection light field images salient regions depth labels;plenoptic camera light field saliency detection relative location raw image	Light field images, which capture multiple images from different angles of a scene, have been proved that can detect salient regions more effectively. Instead of estimating depth labels from light field images, we proposed to extract relative locations, which can distinguish whether the object is located before the focus plane of the main lens or not, for saliency detection. The relative locations are calculated by comparing raw light field images captured by plenoptic cameras and central views of scenes. The relative locations are then integrated to a modified saliency detection framework to obtain the salient regions. Experimental results demonstrate that the proposed relative locations can help to improve the accuracy of results, which is also efficient. Moreover, the modified framework outperforms the state-of-the-art methods for light field images saliency detection.	light field	Hao Sheng;Shuo Zhang;Xiaoyu Liu;Zhang Xiong	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7471953	computer vision;computer science;light field	Vision	42.58338851400696	-51.3203222572568	20041
a4fb2d6adf7df2db797ce94255badac46aff9240	a compact shot representation for video semantic indexing	video signal processing image representation indexing;semantics visualization indexing feature extraction yttrium video signal processing histograms;trecvid sin compact shot representation video semantic indexing key frame kf difference frame df;difference frame df video semantic indexing compact shot representation key frame kf	This paper presents a compact shot representation for video semantic indexing (SIN). The proposed representation consists of visual cues from only two frames, i.e., key frame (KF) and difference frame (DF), which are both constructed with spatial pyramid. The KF describes static information while the generated DF captures non-static information. Each region of DF is derived from the same location in a selected frame, which has the most salient difference compared with the key frame in that region. We introduce a variation of DF to further enhance our model. Experimental results on TRECVID SIN demonstrate that our method obtains better accuracy than the state-of-the-art, while requiring less storage space and consuming time.	direction finding;frame language;kalman filter;key frame	Jinzhuo Wang;Wenmin Wang;Ronggang Wang;Wen Gao	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7351407	computer vision;computer science;theoretical computer science;information retrieval	Vision	38.37510090580062	-51.8891232921799	20109
068e3a85e3d1f3bce08a37b7baa887e7fafd0afa	ensemble of vector and binary descriptor for loop closure detection	ensemble learning;appearance based localization;loop closure detection	Loop closure detection plays an important role in vSLAM for building and updating maps of the surrounding environment. An efficient vSLAM system needs an informative descriptor for landmark description and stable model for making decisions. Most of the solutions dependent on using a single descriptor for landmark description, whereas other solutions proposed to use a combination of descriptors. However, these solutions still have the limitation in correctly detecting a previously visited landmark. In this paper, an ensemble of loop closure detection is proposed using Bayesian filter models for making decisions. In this approach, a set of different keypoint descriptors is used as input to bag-of-word descriptors. After that, these descriptors, i.e., SIFT, SURF, and ORB, are used to construct Bayesian filter models and ensemble learning algorithm for loop closure detection. The proposed approach is validated on a public dataset, namely City-Center dataset (CiC). The results shown that the proposed ensemble algorithm outperforms single model and existing loop closure detection system approaches. It gives 87.96 % for ensemble learning and 86.36 % for the best single model and 37, 80, 81 % for FAB-MAP, PIRF-Nav2.0, and RTAB-MAP, respectively.	for loop	Mohammed Omar Salameh;Azizi Abdullah;Shahnorbanun Sahran	2015		10.1007/978-3-319-31293-4_27	machine learning;pattern recognition;control theory	Robotics	31.45560932893112	-49.5308616171119	20131
1d70bbed6adbaee80fbf9aa9ccd512a762f48cbf	a hybrid object detection technique from dynamic background using gaussian mixture models	adaptive background model;background modeling;comparative analysis;complexity theory;probability;difference operator;gaussian processes;probability computer vision decision theory gaussian processes object detection;model adaptation;hybrid object detection technique;subtraction decision;computer vision;pixel object detection adaptation model computational modeling complexity theory stability analysis distance measurement;distance measurement;gaussian mixture model;computational modeling;adaptation model;machine vision;pixel;decision theory;background subtraction;stability analysis;machine vision application;subtraction decision hybrid object detection technique gaussian mixture model adaptive background model machine vision application probability;object detection	Adaptive background modelling based object detection techniques are widely used in machine vision applications for handling the challenges of real-world multimodal background. But they are constrained to specific environment due to relying on environment specific parameters, and their performances also fluctuate across different operating speeds. On the other side, basic background subtraction (BBS) is not suitable for real applications due to manual background initialization requirement and its inability to handle repetitive multimodal background. However, it shows better stability across different operating speeds and can better eliminate noise, shadow, and trailing effect than adaptive techniques as no model adaptability or environment related parameters are involved. In this paper, we propose a hybrid object detection technique for incorporating the strengths of both approaches. In our technique, Gaussian mixture models (GMM) is used for maintaining an adaptive background model and both probabilistic and basic subtraction decisions are utilized for calculating inexpensive neighbourhood statistics for guiding the final object detection decision. Experimental results with two benchmark datasets and comparative analysis with recent adaptive object detection technique show the strength of the proposed technique in eliminating noise, shadow, and trailing effect while maintaining better stability across variable operating speeds.	background subtraction;benchmark (computing);google map maker;image noise;machine vision;mixture model;multimodal interaction;object detection;performance;qualitative comparative analysis	Mahfuzul Haque;M. Manzur Murshed;Manoranjan Paul	2008	2008 IEEE 10th Workshop on Multimedia Signal Processing	10.1109/MMSP.2008.4665205	qualitative comparative analysis;computer vision;von neumann stability analysis;background subtraction;machine vision;decision theory;computer science;machine learning;pattern recognition;probability;mixture model;gaussian process;computational model;pixel;statistics	Vision	43.70779161724135	-49.384331660589694	20171
7fcf20d01c4d5ed67e98b319de7765aadf094fe8	comparison of fda-based and pca-based features in fault diagnosis of automobile gearboxes	automobile gearbox;fisher discriminant analysis;gaussian mixture model;k nearest neighbor;continuous wavelet transform;fault diagnosis	Several advantages for machine condition monitoring and fault diagnosis, as reducing maintenance costs, improving productivity and increasing machine availability, are formerly reported. Gearbox is one of the most popular machines in the world. The importance and need to this machine is clear; so, fault diagnosis of them is a core research area in the condition monitoring field. This paper presents an intelligent method to diagnose a kind of automotive multi-speed gearbox, operating in constant speed, using the vibration signal. In this research, the studied gears are located on the main input shaft which is supported with a tachometer sensor.	mean time between failures;tachometer	M. H. Gharavian;F. Almas Ganj;Abdolreza Ohadi;Hojat Heidari Bafroui	2013	Neurocomputing	10.1016/j.neucom.2013.04.033	speech recognition;continuous wavelet transform;computer science;machine learning;pattern recognition;mixture model;k-nearest neighbors algorithm	Robotics	36.75263711095958	-30.36226021484263	20198
4fa70fb737a514afe8e902a9d8fd56f6cc3d6b60	invariants of six points and projective reconstruction from three uncalibrated images	representacion proyectiva;projection geometrique;sturm s method projective reconstruction uncalibrated images projective invariants invariant relationship space invariants image invariants accuracy stability image noise epipolar geometry;geometrical projection;invariance image reconstruction geometry;geometry;projective geometry;proyeccion geometrica;representation projective;calculo automatico;image bruitee;computing;algorithme;calcul automatique;invariance;imagen sonora;epipolar geometry;algorithm;this invariant;reconstruction image;reconstruccion imagen;image reconstruction;projective representation;noisy image;uncalibrated images;image reconstruction geometry cameras stability computational modeling calibration machine vision shape books transmission line matrix methods;self calibration;projective reconstruction;invariant;algoritmo	AbstructThere are three projective invariants of a set of six points in general position in space. It is well known that these invariants cannot be recovered from one image, however an invariant relationship does exist between space invariants and image invariants. This invariant relationship is first derived for a single image. Then this invariant relationship is used to derive the space invariants, when multiple images are available. This paper establishes that the minimum number of images for computing these invariants is three, and the computation of invariants of six points from three images can have as many as three solutions. Algorithms are presented for computing these invariants in closed form. The accuracy and stability with respect to image noise, selection of the triplets of images and distance between viewing positions are studied both through real and simulated images. Applications of these invariants are also' presented. Both the results of Faugeras [l] and Hartley et al. [2] for projective reconstruction and Sturm's method [3] for epipolar geometry determination from two uncalibrated images with at least seven points are extended to the case of three uncalibrated images with only six points.	autostereogram;computation;epipolar geometry;hartley (unit);image noise;invariant (computer science);sturm's theorem	Long Quan	1995	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.368154	computer vision;computing;projective geometry;topology;projective representation;computer science;invariant;mathematics;geometry;epipolar geometry	Vision	52.63410681179678	-51.77926180399133	20199
6ff47da7d62dafd460fdfa15924700301018bfc8	learning a mahalanobis distance metric via regularized lda for scene recognition	minimisation;image recognition;least squares approximations;measurement training image recognition vectors principal component analysis feature extraction accuracy;matrix algebra;nonnegative l 2 norm regularization mahalanobis distance metric regularized lda scene recognition intraclass variations full parameter matrix nonnegatively constrained minimization problem multiple regularized linear discriminant analysis candidate projection pool pairwise squared differences learning instances diagonal selection matrix least squares;scene recognition metric learning mahalanobis metric regularized lda non negative l 2 norm regularization;learning artificial intelligence;minimisation image recognition learning artificial intelligence least squares approximations matrix algebra	Constructing a suitable distance metric for scene recognition is a very challenging task due to the huge intra-class variations. In this paper, we propose a novel framework for learning a full parameter matrix in Mahalanobis metric, where the learning process is formulated as a non-negatively constrained minimization problem in a projected space. To fully capture the structure of scenes, we first apply multiple regularized linear discriminant analysis (LDA) to form a candidate projection pool. Second, we adopt the pairwise squared differences of the projected samples as the learning instances. Finally, the diagonal selection matrix is learned through least squares with non-negative L2-norm regularization. Experiments on two datasets in scene recognition show the effectiveness and efficiency of our approach.	experiment;least squares;linear discriminant analysis;manifold regularization;matrix regularization	Meng Da Wu;Jun Zhou;Jun Sun	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6467562	computer vision;minimisation;mathematical optimization;computer science;machine learning;pattern recognition;mathematics;statistics	Vision	25.85358727908674	-42.67935396050604	20217
02139d149802bb27fd93808b8fc9cfea26c40301	learning data-efficient hierarchical features for robotic graspable object recognition		Robotic graspable object recognition is a crucial ingredient in many exciting autonomous manipulation applications. However, identifying complex image features from limited data remains largely unsolved. In this paper, we leverage the advantages of two kinds of feature representation approaches, kernel descriptors and deep neural networks, to present a novel hierarchical feature learning framework for robotic graspable object recognition. This work enables the recovery of sparse and compressible features from limited data examples. Firstly, we design multiple kernel descriptors from the raw RGB-D images to adequately capture the discriminative structure of the object. Then, the extracted abstract representations are transferred to a four-layer deep neural network to generate more representative features for final graspable discrimination. Our network obtains impressive generalization capability with limited training data. Extensive experiments are carried out to validate the proposed method and the results show the state-of-the-art performance in discriminating graspable object task under limited-data.	artificial neural network;autonomous robot;computation;deep learning;experiment;feature learning;high- and low-level;kernel (operating system);mathematical optimization;multimodal interaction;outline of object recognition;overfitting;principal component analysis;singularity project;sparse matrix	Zhichao Wang;Bin Wang;Chuangqiang Guo;Zhiqi Li;Yang Liu;Hong Liu	2017	2017 IEEE International Conference on Advanced Intelligent Mechatronics (AIM)	10.1109/AIM.2017.8014081	feature (computer vision);discriminative model;visualization;feature extraction;artificial neural network;artificial intelligence;training set;machine learning;cognitive neuroscience of visual object recognition;computer science;computer vision;feature learning	Robotics	26.68564630057727	-48.77975909865637	20244
013b19fb5476ac83361f40cdcdf8d08e1b8166f1	real-time, long-term hand tracking with unsupervised initialization	generative particle filter based observation model long term hand tracking unsupervised initialization error recovery three stage hand detector spatial information temporal information hand hypothesis random forest detector linear classifier temporal statistics hand detection discriminative confidence map;image classification;hand detection;random forest hand tracking particle filter hand detection;technology and engineering;particle filter;statistics image classification object detection object tracking particle filtering numerical methods;object tracking;random forest;hand tracking;statistics;object detection;particle filtering numerical methods	This paper proposes a complete tracking system that is capable of long-term, real-time hand tracking with unsupervised initialization and error recovery. Initialization is steered by a three-stage hand detector, combining spatial and temporal information. Hand hypotheses are generated by a random forest detector in the first stage, whereas a simple linear classifier eliminates false positive detections. Resulting detections are tracked by particle filters that gather temporal statistics in order to make a final decision. The detector is scale and rotation invariant, and can detect hands in any pose in unconstrained environments. The resulting discriminative confidence map is combined with a generative particle filter based observation model to enable robust, long-term hand tracking in real-time. The proposed solution is evaluated using several challenging, publicly available datasets, and is shown to clearly outperform other state of the art object tracking methods.	linear classifier;particle filter;random forest;real-time clock;real-time transcription;sensor;tracking system;unsupervised learning	Vincent Spruyt;Alessandro Ledda;Wilfried Philips	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738769	random forest;computer vision;contextual image classification;particle filter;computer science;machine learning;video tracking;pattern recognition;statistics	Robotics	41.80708490368523	-47.930052118918496	20256
41081056a56f9b66b8955304aa7bb0535d4cb3b2	truncated robust principal component analysis and noise reduction for single cell rna-seq data		The development of single cell RNA sequencing (scRNA-seq) has enabled innovative approaches to investigating mRNA abundances. In our study, we are interested in extracting the systematic patterns of scRNA-seq data in an unsupervised manner, thus we have developed two extensions of robust principal component analysis (RPCA). First, we present a truncated version of RPCA (tRPCA), that is much faster and memory efficient. Second, we introduce a noise reduction in tRPCA with (L_2) regularization (tRPCAL2). Unlike RPCA that only considers a low-rank L and sparse S matrices, the proposed method can also extract a noise E matrix inherent in modern genomic data. We demonstrate its usefulness by applying our methods on the peripheral blood mononuclear cell (PBMC) scRNA-seq data. Particularly, the clustering of a low-rank L matrix showcases better classification of unlabeled single cells. Overall, the proposed variants are well-suited for high-dimensional and noisy data that are routinely generated in genomics.	noise reduction;robust principal component analysis	Krzysztof Gogolewski;Maciej Sykulski;Neo Christopher Chung;Anna Gambin	2018		10.1007/978-3-319-94968-0_32	computer science;machine learning;artificial intelligence;noise reduction;matrix decomposition;cluster analysis;regularization (mathematics);robust principal component analysis;unsupervised learning;principal component analysis;matrix (mathematics)	ML	26.69091351583836	-38.166632371297304	20260
fbf2d4a9b85af5c7b51b6dd3e0e2dd5eb81f7cf5	a leader-following approach based on probabilistic trajectory estimation and virtual train model		The paper proposes a novel multi-robot leader-following approach that can offer accurate trajectory estimation and tracking control in scenarios where 1) temporary outage of vision detection happens due to the leader robot moving out of view, illumination variation, vision occlusion, motion blurring, etc., and 2) the leader moves at a time-varying linear velocity. The approach estimates the trajectory of the leader robot within the local reference frame of the follower using noise-corrupted odometry information and intermittent inter-robot relative observations based on detection of fiducial markers using an RGBD camera. It also introduces the virtual train model in the trajectory tracking, with which the follower is controlled to keep its linear velocity synchronous with that of the leader and a constant separation distance. Results are obtained based on evaluating the proposed leader-following approach in a test containing sharp turns and a zig-zag pattern in the trajectory, and variation in the speed of the leader robot.	algorithm;downtime;fiducial marker;mobile robot;odometry;particle filter;prototype;reference frame (video);robot;sun outage;velocity (software development)	Mao Shan;Ying Zou;Mingyang Guan;Changyun Wen;Cheng-Leong Ng	2017	2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2017.8317748	computer vision;constant linear velocity;artificial intelligence;odometry;probabilistic logic;local reference frame;trajectory;engineering;fiducial marker	Robotics	49.2612870065189	-44.93452519189023	20263
2285615ba308f39c838033b69635d39a4cad268c	digital and discrete geometry		Geometry comes from measurements of visible shapes, sizes, patterns, and positions. It is one of the most basic human subjects in civilization. Through thousands of years, humans have developed different types of geometry including: elementary geometry, analytic geometry, differential geometry, topology, algebraic geometry, and many other related research areas. Along with the fast development of digital computers, in recent years, people have been studying digital geometry. Digital geometry focuses on digital objects, which are usually represented by a finite number of integer points or vectors. However, in a much larger sense, digital objects could be digital data saved in computers or data sets in electronic form. Digital geometry comes from two primary sources: digital images and digital data displays of computer graphics. Computer memory is the primary domain of digital geometry. Since computer memory is arranged by arrays, its location (called address) in the arrangement is similar to n-dimensional grid points in Euclidean space. Therefore, digital geometry can also be viewed as geometry of grid space. The main difference between digital space and Euclidean space is how we measure the distance between two points. Digital geometry is a branch of discrete geometry that mainly consists of the study of geometric relationships among discrete objects. In this chapter, we will give a brief introduction to the different types of geometry, especially digital and discrete geometry.	computational geometry;computer graphics;computer memory;digital data;digital geometry;digital image;linear algebra;primary source	Li M. Chen	2014		10.1007/978-3-319-12099-7	discrete differential geometry;digital geometry	Graphics	44.76062866064516	-27.150057813872444	20273
8b3beaab1e1d00f04a0f89827eac67e2704834b8	heart rate measurement based on event timing coding observed by video camera				Takashi G. Sato;Yoshifumi Shiraki;Takehiro Moriya	2017	IEICE Transactions		computer vision;free-space optical communication;real-time computing;simulation;image sensor	Mobile	44.6630109796658	-30.27859409865164	20303
33ea19d00393b526881a98f0e53e59b73e433059	road image update using in-vehicle camera images and aerial image	road image update;image segmentation;image resolution;hidden feature removal;obstacles;in vehicle camera images;automated highways;roads cameras image resolution image registration image sequences vehicles pixel;road image mosaicing method;roads;road regions road image update in vehicle camera images aerial image resolution occlusions obstacles road image mosaicing method image registration;image registration;pixel;road regions;vehicles;aerial image resolution;occlusions;cameras;image segmentation automated highways hidden feature removal image registration image resolution;image sequences	Road image is becoming important for several applications such as car navigation systems, traffic environment research, city modeling. Usually, a road image can be obtained from an aerial image but the resolution of the aerial image is often low, or it contains occlusions by obstacles. Therefore, the update of road image is required. In this paper, we propose a road image mosaicing method using in-vehicle camera images and an aerial image. We first perform image registration of road regions between these images, and then, we generate a large road image by performing image mosaicing of road regions in invehicle camera images. In an experiment, we achieved resolution improvement and occlusions removal, and also succeeded in update of a large road image.	aerial photography;automotive navigation system;digital image;image registration;resolution (logic)	Masafumi Noda;Tomokazu Takahashi;Daisuke Deguchi;Ichiro Ide;Hiroshi Murase;Yoshiko Kojima;Takashi Naito	2011	2011 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2011.5940470	image quality;image texture;image restoration;computer vision;feature detection;image resolution;binary image;geography;image processing;digital image;remote sensing;computer graphics (images)	Vision	49.28173436149208	-48.10979422481488	20307
22d70275704c8394046fc98ed1c06a9e0dcddbc2	classification of partial discharge signals by combining adaptive local iterative filtering and entropy features	emi events (discharge sources);emi method;classification;dispersion entropy;expert’s system;partial discharge;permutation entropy	Electro-Magnetic Interference (EMI) is a measurement technique for Partial Discharge (PD) signals which arise in operating electrical machines, generators and other auxiliary equipment due to insulation degradation. Assessment of PD can help to reduce machine downtime and circumvent high replacement and maintenance costs. EMI signals can be complex to analyze due to their nonstationary nature. In this paper, a software condition-monitoring model is presented and a novel feature extraction technique, suitable for nonstationary EMI signals, is developed. This method maps multiple discharge sources signals, including PD, from the time domain to a feature space which aids interpretation of subsequent fault information. Results show excellent performance in classifying the different discharge sources.	2-methylcitrate dehydratase activity;abbreviations;classification;conflict (psychology);discharger;double minutes;downtime;emi;elegant degradation;experiment;expert systems;expert system;familial partial lipodystrophy, type 2;feature extraction;feature vector;fingerprint;ips community suite;insulation device component;interference (communication);manuscripts;map;modulation;pattern recognition;star trek generations;statistical interference;support vector machine;transformer;transformers;ultra high frequency	Imene Mitiche;Gordon Morison;Alan Nesbitt;Michael Hughes-Narborough;Brian G. Stewart;Philip Boreham	2017		10.3390/s18020406	partial discharge;feature extraction;filter (signal processing);time domain;electromagnetic interference;feature vector;downtime;engineering;emi;electronic engineering	ML	37.367961654588676	-31.506051831401262	20355
a80a2587f7359422c9fb15793bc4b300fe9d490e	on scale-free prior distributions and their applicability in large-scale network inference with gaussian graphical models	network inference;complex network;bayesian inference;prior distribution;gene network;random networks;small samples;large scale;scale free;markov chain monte carlo;gaussian graphical model;high throughput	This paper concerns the specification, and performance, of scale-free prior distributions with a view toward large-scale network inference from small-sample data sets. We devise three scale-free priors and implement them in the framework of Gaussian graphical models. Gaussian graphical models are used in gene network inference where high-throughput data describing a large number of variables with comparatively few samples are frequently analyzed by practitioners. And, although there is a consensus that many such networks are scale-free, the modus operandi is to assign a random network prior. Simulations demonstrate that the scale-free priors outperform the random network prior at recovering scale-free trees with degree exponents near 2, such as are characteristic of many real-world systems. On the other hand, the random network prior compares favorably at recovering scale-free trees characterized by larger degree exponents.	computer simulation;gene regulatory network;graphical model;high-throughput computing;random graph;throughput;world-system	Paul Sheridan;Takeshi Kamimura;Hidetoshi Shimodaira	2009		10.1007/978-3-642-02466-5_9	high-throughput screening;variable elimination;econometrics;gene regulatory network;prior probability;variable-order bayesian network;gibbs sampling;markov chain monte carlo;computer science;scale-free network;pattern recognition;bayesian network;mathematics;graphical model;bayesian inference;complex network;statistics;belief propagation	ML	26.221781607309318	-28.341865197727554	20391
49db0d48a2a6763b5fb35347ab44c58babb9f503	unsupervised deep learning for structured shape matching		We present a novel method for computing correspondences across shapes using unsupervised learning. Our method allows to compute a non-linear transformation of given descriptor functions, while optimizing for global structural properties of the resulting maps, such as their bijectivity or approximate isometry. To this end, we use the functional maps framework, and build upon the recently proposed FMNet architecture for descriptor learning. Unlike the method proposed in that work, however, we show that learning can be done in a purely unsupervised setting, without having access to any ground truth correspondences. This results in a very general shape matching method, which can be used to establish correspondences within shape collections or even just a single shape pair, without any prior information. We demonstrate on a wide range of challenging benchmarks, that our method leads to significant improvement compared to the existing axiomatic methods and achieves comparable, and in some cases superior results to even the supervised learning techniques.		Jean-Michel Roufosse;Maks Ovsjanikov	2018	CoRR			Vision	28.625111121676866	-48.43399653055222	20392
4c047eb1dee8ba6ef0f7e7610813c1b00084f2a5	robust recursive absolute value inequalities discriminant analysis with sparseness	absolute value;feature extraction;linear discriminant analysis;robust modeling;sparse projection	"""In this paper, we propose a novel absolute value inequalities discriminant analysis (AVIDA) criterion for supervised dimensionality reduction. Compared with the conventional linear discriminant analysis (LDA), the main characteristics of our AVIDA are robustness and sparseness. By reformulating the generalized eigenvalue problem in LDA to a related SVM-type """"concave-convex"""" problem based on absolute value inequalities loss, our AVIDA is not only more robust to outliers and noises, but also avoids the SSS problem. Moreover, the additional L1-norm regularization term in the objective makes sure sparse discriminant vectors are obtained. A successive linear algorithm is employed to solve the proposed optimization problem, where a series of linear programs are solved. The superiority of our AVIDA is supported by experimental results on artificial examples as well as benchmark image databases."""	algorithm;anterior descending branch of left coronary artery;avida;benchmark (computing);concave function;convex optimization;database;dimensionality reduction;linear discriminant analysis;linear programming;manifold regularization;mathematical optimization;neural coding;optimization problem;recursion;sensorineural hearing loss (disorder);sparse matrix;taxicab geometry;synovial sarcoma	Chun-Na Li;Zeng-Rong Zheng;Ming-Zeng Liu;Yuan-Hai Shao;Wei-Jie Chen	2017	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2017.05.011	mathematical optimization;robustness (computer science);mathematics;avida;machine learning;dimensionality reduction;artificial intelligence;absolute value;sss*;linear discriminant analysis;discriminant;optimization problem;pattern recognition	ML	24.699461794912075	-39.92045659223325	20394
54ae3f82f3468f3c257a6fc07a9415b817943089	incident detection from low-angle images of heavy traffics in tunnels	image segmentation;low angle images;road traffic;traffic management incident detection low angle images tunnels heavy traffic congestions;vehicles signal processing algorithms physics telecommunication traffic layout humans morphology shape intelligent transportation systems signal processing;traffic management;tunnels;heavy traffic;heavy traffic congestions;traffic engineering computing;incident detection;tunnels image segmentation object detection road traffic traffic engineering computing;object detection	Accidents or abnormally stalled vehicles in tunnels are liable to induce additional incidents that would be more fatal. They also would induce heavy traffic congestions by disturbing the following traffics. Therefore, it is important to detect such the primary incidents in tunnels as soon as possible, and to inform traffic management officers about them. However, it is difficult to detect incidents correctly distinguishing from pure congestions. In particular, it will become more difficult to detect incidents from low-angled and seriously occluded images as in tunnels. In this paper, a dedicated method for precise segmentation of such the occluded vehicles is described. The proposed algorithm was examined by experiments using two year video images obtained from three tunnels, and it was proved to be effective for quite ill conditions such as heavy traffics in tunnels.	algorithm;experiment;markov random field;mathematical morphology;signal processing	Shunsuke Kamijo;Hiroshi Inoue	2007	2007 IEEE Intelligent Transportation Systems Conference	10.1109/ITSC.2007.4357634	simulation;engineering;transport engineering;computer security	Vision	42.2431285758848	-44.10716273302665	20398
e1f8d93fdc42f6a9292c77f7f164b7a0fe0256f3	bayesian inference in sparse gaussian graphical models		One of the fundamental tasks of science is to find explainable relationships between observed phenomena. One approach to this task that has received attention in recent years is based on probabilistic graphical modelling with sparsity constraints on model structures. In this paper, we describe two new approaches to Bayesian inference of sparse structures of Gaussian graphical models (GGMs). One is based on a simple modification of the cutting-edge block Gibbs sampler for sparse GGMs, which results in significant computational gains in high dimensions. The other method is based on a specific construction of the Hamiltonian Monte Carlo sampler, which results in further significant improvements. We compare our fully Bayesian approaches with the popular regularisation-based graphical LASSO, and demonstrate significant advantages of the Bayesian treatment under the same computing costs. We apply the methods to a broad range of simulated data sets, and a real-life financial data set.	graphical model;sparse	Peter Orchard;Felix V. Agakov;Amos J. Storkey	2013	CoRR		computer science;machine learning;graphical model	ML	27.33702819913716	-32.25438373362411	20469
424a8d41cbf808f822c305dd453e53bebcb8f9d3	information-theoretic exploration with bayesian optimization	robot sensing systems;bayes methods;mutual information;optimization;entropy	We consider an autonomous exploration problem in which a mobile robot is guided by an information-based controller through an a priori unknown environment, choosing to collect its next measurement at the location estimated to be most informative within its current field of view. We propose a novel approach to predict mutual information (MI) using Bayesian optimization. Over several iterations, candidate sensing actions are suggested by Bayesian optimization and added to a committee that repeatedly trains a Gaussian process (GP). The GP estimates MI throughout the robot's action space, serving as the basis for an acquisition function used to select the next candidate. The best sensing action in the committee is executed by the robot. This approach is compared over several environments with two batch methods, one which chooses the most informative action from a set of pseudo-random samples whose MI is explicitly evaluated, and one that applies GP regression to this sample set. Our computational results demonstrate that the proposed method provides not only computational efficiency and rapid map entropy reduction, but also robustness in comparison with competing approaches.	action potential;angularjs;autonomous robot;bayesian optimization;computation;computational complexity theory;exploration problem;gaussian process;information gain in decision trees;interpolation;iteration;kullback–leibler divergence;marginal model;mathematical optimization;maximal set;mobile robot;mutual information;pseudorandomness;real-time clock;scalability;sparse language;sparse matrix;theory	Shi Bai;Jinkun Wang;Fanfei Chen;Brendan Englot	2016	2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2016.7759289	entropy;computer science;artificial intelligence;machine learning;data mining;mutual information;statistics	Robotics	51.67030287620343	-25.84453944065887	20472
206c5f7e207b3c7f951d8417b4bc1d958fe024ab	improving the robustness of particle filter-based visual trackers using online parameter adaptation	video signal processing;adaptive mean shift tracker particle filtering visual trackers parameter adaptation video sequences;robustness particle tracking target tracking particle filters uncertainty video sequences application software parameter estimation motion measurement computer vision;mean shift;video signal processing computer vision image sequences parameter estimation particle filtering numerical methods;computer vision;particle filter;model updating;parameter estimation;particle filtering numerical methods;image sequences	In particle filter-based visual trackers, dynamic velocity components are typically incorporated into the state update equations. In these cases, there is a risk that the uncertainty in the model update stage can become amplified in unexpected and undesirable ways, leading to erroneous behavior of the tracker. Moreover, the use of a weak appearance model can make the estimates provided by the particle filter inaccurate. To deal with this problem, we propose a continuously adaptive approach to estimating uncertainty in the particle filter, one that balances the uncertainty in its static and dynamic elements. We provide quantitative performance evaluation of the resulting particle filter tracker on a set of ten video sequences. Results are reported in terms of a metric that can be used to objectively evaluate the performance of visual trackers. This metric is used to compare our modified particle filter tracker and the continuously adaptive mean shift tracker. Results show that the performance of the particle filter is significantly improved through adaptive parameter estimation, particularly in cases of occlusion and erratic, nonlinear target motion.	estimation theory;kalman filter;mean shift;nonlinear system;particle filter;performance evaluation;software bug;velocity (software development)	Andrew D. Bagdanov;Alberto Del Bimbo;Fabrizio Dini;Walter Nunziati	2007	2007 IEEE Conference on Advanced Video and Signal Based Surveillance	10.1109/AVSS.2007.4425313	computer vision;simulation;kernel adaptive filter;particle filter;mean-shift;auxiliary particle filter;computer science;control theory;mathematics;estimation theory;statistics	Vision	45.435841430720316	-47.332385180099536	20474
e85b79419c72e043f44ee6fd1adc172d98e7d8c7	real-time machine vision fpga implementation for microfluidic monitoring on lab-on-chips	time 13 97 ms real time machine vision fpga implementation field programmable gate array device lab on chips real time microfluidic monitoring continuous flows plug flows circular channels point of care system fiducial markers grayscale uncompressed video resolution velocity 20 mm s size 200 mum frequency 170 mhz;field programmable gate array fpga;flow detection;monitoring;machine vision;microfluidic;monitoring field programmable gate array fpga flow detection lab on chip machine vision microfluidic;microchannel flow biomems field programmable gate arrays fluidic devices health care lab on a chip;lab on chip	A machine vision implementation on a field-programmable gate array (FPGA) device for real-time microfluidic monitoring on Lab-On-Chips is presented in this paper. The machine vision system is designed to follow continuous or plug flows, for which the menisci of the fluids are always visible. The system discriminates between the front or “head” of the flow and the back or “tail” and is able to follow flows with a maximum speed of 20 mm/sec in circular channels of a diameter of 200 μm (corresponding to approx. 60 μl/sec). It is designed to be part of a complete Point-of-Care system, which will be portable and operate in non-ideal laboratory conditions. Thus, it is able to cope with noise due to lighting conditions and small LoC displacements during the experiment execution. The machine vision system can be used for a variety of LoC devices, without the need for fiducial markers (such as redundancy patterns) for its operation. The underlying application requirements called for a complete hardware implementation. The architecture uses a variety of techniques to improve performance and minimize memory access requirements. The system input is 8 bit grayscale uncompressed video of up to 1 Mpixel resolution. The system uses an operating frequency of 170 Mhz and achieves a computational time of 13.97 ms (worst case), which leads to a throughput of 71.6 fps for 1 Mpixel video resolution.	acclimatization;algorithm;approximation;best, worst and average case;british informatics olympiad;clock rate;computation;control unit;data compression;diameter (qualifier value);display resolution;edge detection;execution;experiment;fiducial marker;field-programmability;field-programmable gate array;flow;frame grabber;grayscale color map;liquid substance;machine vision;meniscus structure of joint;microfluidics;microscope device component;microsoft windows;millimeter per second;pixel;plug (physical object);preparation;preprocessor;quantity;real-time clock;real-time transcription;requirement;small;spartan;specification;throughput;time complexity;uncompressed video	Calliope-Louisa Sotiropoulou;Liberis Voudouris;Christos Gentsos;Athanasios M. Demiris;Nikolaos Vassiliadis;Spiridon Nikolaidis	2014	IEEE Transactions on Biomedical Circuits and Systems	10.1109/TBCAS.2013.2260338	embedded system;electronic engineering;microfluidics;machine vision;lab-on-a-chip;computer hardware;computer science;engineering;electrical engineering;operating system;nanotechnology	Embedded	44.75190084938	-34.528673130794964	20484
24432318457ccbb2449574ed54cda14fb2f04e5e	scale-adaptive real-time crowd detection and counting for drone images		We propose a scale-adaptive crowd detection and counting approach for drone images. Based on local feature points and density estimation considering the image scale, we detect dense crowds over multiple distances and introduce an extremely fast counting strategy with high accuracy for our detected crowd regions. We compare our results with a recent CNN-based state-of-the-art approach and validate both methods for different scaling factors on a novel crowd dataset. The results show that our proposed method outperforms the pre-trained CNN-based approach and receives very precise counting results for different zoom factors, resolutions and crowd sizes. Its low computational complexity makes it highly suitable for real-time analysis or embedded systems.		Markus Küchhold;Maik Simon;Volker Eiselein;Thomas Sikora	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451289	computer vision;computational complexity theory;feature extraction;image segmentation;crowds;scaling;density estimation;zoom;pattern recognition;artificial intelligence;computer science;image resolution	Robotics	41.168665342398874	-43.23403072492224	20487
247b8e6b0e1014e707117c08fc880bb341b2c248	advantages of dynamic analysis in hog-pca feature space for video moving object classification	training;hidden markov models;trajectory;feature extraction;principal component analysis;hybrid classifier moving object classification hog pca hmm;video surveillance hidden markov models image classification pedestrians principal component analysis road traffic traffic engineering computing;virat database hog pca feature space video moving object classification dynamic analysis video surveillance vehicle pedestrian object classification static knn classifier dynamic hidden markov model based classifier;hidden markov models vehicle dynamics feature extraction trajectory training principal component analysis conferences;vehicle dynamics;conferences	Classification of moving objects for video surveillance applications still remains a challenging problem due to the video inherently changing conditions such as lighting or resolution. This paper proposes a new approach for vehicle/pedestrian object classification based on the learning of a static kNN classifier, a dynamic Hidden Markov Model (HMM)-based classifier, and the definition of a fusion rule that combines the two outputs. The main novelty consists in the study of the dynamic aspects of the moving objects by analysing the trajectories of the features followed in the HOG-PCA feature space, instead of the classical trajectory study based on the frame coordinates. The complete hybrid system was tested on the VIRAT database and worked in real time, yielding up to 100% peak accuracy rate in the tested video sequences.	closed-circuit television;feature vector;hidden markov model;hybrid system;markov chain;virat	Miriam M. Lopez;Lucio Marcenaro;Carlo S. Regazzoni	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178177	computer vision;vehicle dynamics;feature extraction;computer science;trajectory;machine learning;video tracking;pattern recognition;hidden markov model;principal component analysis	Vision	38.53107794598903	-47.62806056022486	20532
515634081d715da98d62ceada042bcf91eb7df22	generalized coupled symmetric tensor factorization for link prediction	generalized coupled symmetric tensor factorization link prediction performance improvement joint data analysis symmetric matrix factorization loss functions latent factors gctf tensor factorisation models probabilistic interpretation multiway array matrices relational datasets missing connection existence prediction missing link prediction;probability;tensile stress symmetric matrices numerical models global positioning system predictive models electronic mail probabilistic logic;tensors data analysis matrix decomposition probability relational databases sensor fusion;data analysis;symmetric matrix coupled tensor factorization link prediction missing data data fusion;matrix decomposition;relational databases;sensor fusion;tensors	This study deals with the missing link prediction, the problem of predicting the existence of missing connections between entities of interest. Link prediction is addressed using coupled analysis of relational datasets represented by several matrices, including symmetric ones and multiway arrays, that will be simply called tensors. We propose to use an approach based on probabilistic interpretation of tensor factorisation models, i.e., Generalised Coupled Tensor Factorisation (GCTF), which can simultaneously fit a large class of tensor models to higher-order tensors/matrices with common latent factors using different loss functions. In addition, we propose the algorithm for factorization of symmetric matrices. Numerical experiments demonstrate that joint analysis of data from multiple sources via coupled factorisation and integration of symmetric matrices to models improves the link prediction performance and the selection of right loss function and tensor model is crucial for accurately predicting missing links.	algorithm;entity;experiment;latent variable;list of code lyoko episodes;loss function;monoid factorisation;numerical method	Beyza Ermis;Ali Taylan Cemgil;Evrim Acar	2013	2013 21st Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2013.6531411	combinatorics;discrete mathematics;tensor;relational database;computer science;machine learning;probability;mathematics;sensor fusion;data analysis;matrix decomposition;statistics	ML	28.65570946023284	-34.32206673999373	20603
88584e0889fe018b56417d107777a43f0d66d8e7	estimation of robot's and user's views and blind regions in occlusion environments	object recognition;hidden feature removal;robot user interaction human friendly robots blind region estimation visual occlusion environment recognition stereo camera images;robot vision;humanoid robots;stereo image processing;visual perception;man machine systems;orbital robotics human robot interaction robot vision systems cameras;visual perception hidden feature removal humanoid robots man machine systems object recognition robot vision stereo image processing	"""Human-friendly robots will work mainly in usual living spaces for humans, such as offices, kitchens and living rooms. In these environments, there are many objects which can cause the visual occlusion. So it will often occur that the robot cannot observe some object by the occlusion while the user can observe it, or vice versa, according to their positions. In these situations, the robot is required to recognize this """"invisibility,"""" that is, to interact with the user considering the possibility that there is some object which the user can observe but the robot cannot or vice versa. In this paper, we propose a method for estimation of robot's and user's views and blind regions from images obtained by a stereo camera. Our method is one of the fundamental techniques to realize the robot which can handle the occlusion problems mentioned above. Experimental results in real occlusion environments show the effectiveness of our method."""	hidden surface determination;humans;robot;stereo camera	Jun-ichi Imai;Masahide Kaneko	2007	RO-MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2007.4415143	mobile robot;robot learning;computer vision;simulation;visual perception;computer science;humanoid robot;artificial intelligence;social robot;cognitive neuroscience of visual object recognition;mobile robot navigation;personal robot	Robotics	46.81555423625663	-41.86161330004819	20615
b9a72f9f75eb7aa51ecfa01c975c1c4f35c416ab	basis adaptation for sparse nonlinear reinforcement learning		This paper presents a new approach to representation discovery in reinforcement learning (RL) using basis adaptation. We introduce a general framework for basis adaptation as nonlinear separable least-squares value function approximation based on finding Fréchet gradients of an error function using variable projection functionals. We then present a scalable proximal gradientbased approach for basis adaptation using the recently proposed mirror-descent framework for RL. Unlike traditional temporal-difference (TD) methods for RL, mirror descent based RL methods undertake proximal gradient updates of weights in a dual space, which is linked together with the primal space using a Legendre transform involving the gradient of a strongly convex function. Mirror descent RL can be viewed as a proximal TD algorithm using Bregman divergence as the distance generating function. We present a new class of regularized proximal-gradient based TD methods, which combine feature selection through sparse L1 regularization and basis adaptation. Experimental results are provided to illustrate and validate the approach. Introduction There has been rapidly growing interest in reinforcement learning in representation discovery (Mahadevan 2008). Basis construction algorithms (Mahadevan 2009) combine the learning of features, or basis functions, and control. Basis adaptation (Bertsekas and Yu 2009; Castro and Mannor 2010; Menache, Shimkin, and Mannor 2005) enables tuning a given parametric basis, such as the Fourier basis (Konidaris, Osentoski, and Thomas 2011), radial basis functions (RBF) (Menache, Shimkin, and Mannor 2005), polynomial bases (Lagoudakis and Parr 2003), etc. to the geometry of a particular Markov decision process (MDP). Basis selection methods combine sparse feature selection through L1 regularization with traditional least-squares type RL methods (Kolter and Ng 2009; Johns, Painter-Wakefield, and Parr 2010), linear complementarity methods (Johns, Painter-Wakefield, and Parr 2010), approximate linear programming (Petrik et al. 2010), or convex-concave optimization methods for sparse off-policy TD-learning (Liu, Mahadevan, and Liu 2012). Copyright c © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. In this paper, we present a new framework for basis adaptation as nonlinear separable least-squares approximation of value functions using variable projection functionals. This framework is adapted from a well-known classical method for nonlinear regression (Golub and Pereyra 1973), when the model parameters can be decomposed into a linear set, which is fit by classical least-squares, and a nonlinear set, which is fit by a Gauss-Newton method based on computing the gradient of an error function based on variable projection functionals. Mirror descent is a highly scalable online convex optimization framework (Nemirovksi and Yudin 1983). Online convex optimization (Zinkevich 2003) explores the use of first-order gradient methods for solving convex optimization problems. Mirror descent can be viewed as a first-order proximal gradient based method (Beck and Teboulle 2003) using a distance generating function that is a Bregman divergence (Bregman 1967). We combine basis adaptation with mirrordescent RL (Mahadevan and Liu 2012), a recently developed first-order approach to sparse RL. The proposed approach is also validated with some experiments showing improved performance compared to previous work. Reinforcement Learning Reinforcement learning can be viewed as a stochastic approximation framework (Borkar 2008) for solving MDPs, which are defined by a set of states S, a set of (possibly state-dependent) actions A (As), a dynamical system model comprised of the transition probabilities P a ss′ specifying the probability of transitioning to state s′ from state s under action a, and a reward model R specifying the payoffs received. A policy π : S → A is a deterministic mapping from states to actions. Associated with each policy π is a value function V π , which is a fixed point of the Bellman equation: V π = T(V ) = R + γPV π (1) where 0 ≤ γ < 1 is a discount factor, and T is the Bellman operator. An optimal policy π∗ is one whose associated value function dominates all others, and is defined by the following nonlinear system of equations:	approximation algorithm;artificial intelligence;bellman equation;bregman divergence;complementarity (physics);concave function;convex function;convex optimization;dynamical system;elastic net regularization;experiment;feature selection;first-order reduction;fixed point (mathematics);gauss–newton algorithm;gradient descent;least squares;legendre transformation;linear programming;markov chain;markov decision process;mathematical optimization;matrix regularization;newton's method;nonlinear system;proximal gradient methods for learning;radial (radio);radial basis function;rapid application development;reinforcement learning;scalability;sparse matrix;stochastic approximation;temporal difference learning	Sridhar Mahadevan;Stephen Giguere;Nicholas Jacek	2013			mathematical optimization;artificial intelligence;machine learning;proximal gradient methods	AI	25.79775202471875	-32.3035329643143	20661
4950ed8af166f8a544c3ff32225b845440d3aef8	joint classification of complementary features based on multitask compressive sensing with application to synthetic aperture radar automatic target recognition			automatic target recognition;compressed sensing;computer multitasking;synthetic intelligence	Lizhong Jin;Junjie Chen;Xinguang Peng	2018	J. Electronic Imaging	10.1117/1.JEI.27.5.053034	artificial intelligence;computer vision;compressed sensing;pattern recognition;synthetic aperture radar;computer science;automatic target recognition	Vision	29.699784367630848	-44.12291237501271	20715
7d834251324e0c9564f8b4286ac9d5a9e8f8f2a7	"""comments on the paper by pierre d. glynn, """"modeling np and pu transport with a surface complexation model and spatially variant sorption capacities: implications for reactive transport modeling and performance assessments of nuclear waste disposal sites"""", computers & geosciences 29 (2003) 331-349"""	reactive transport;performance assessment;nuclear waste disposal	Comments on the paper by Pierre D. Glynn, Modeling Np and Pu transport with a surface complexation model and spatially variant sorption capacities : Implications for reactive transport modeling and performance assessments of nuclear waste disposal sites, Computers & Geosciences 29 (2003) 331-349		Ivars Neretnieks	2005	Computers & Geosciences	10.1016/j.cageo.2005.06.013	radioactive waste	NLP	41.668552420833535	-28.916507937179876	20720
e1483f0d06e15bc430e5ff3a5fc0a82f1281b26a	video tracking system optimization using evolution strategies	trained system;different level;video-based tracking system;system performance;final system;evolution strategy;different situation;different scene;optimization strategy;different type;video tracking system optimization;optimization procedure;optimization;video;video tracking;tracking	A video-based tracking system for airport surveillance, composed by modules performing vision tasks at different levels, is adapted for operational conditions by means of Evolution Strategies (ES). An optimization procedure has been carried out considering different scenes composed of representative trajectories, supported by a global evaluation metric proposed to quantify the system performance. The generalization problem (the search of appropriate solutions for general situations, avoiding over-adaptation to particular conditions) is approached considering evaluation of ES-individuals over combinations of trajectories to build the fitness function. In this way, the optimization procedure covers sets of trajectories representing different types of problems. Besides, alternative operators for aggregating partial evaluations have been analysed. Results show how the optimization strategy provides a sensitive tuning of performance related to input parameters at different levels, and how the combination of different situations improves the generalization capability of the trained system. The global performance final system after optimization is also compared with representative algorithms in the state of the art of visual tracking. © 2007 Wiley Periodicals, Inc. Int J Imaging Syst Technol, 17, 75–90, 2007	evolution strategy;mathematical optimization;program optimization;tracking system;video tracking	Jesús Caja García;Óscar Pérez;Antonio Berlanga;José M. Molina López	2007	Int. J. Imaging Systems and Technology	10.1002/ima.20100	computer vision;simulation;video;computer science;artificial intelligence;machine learning;video tracking;tracking;evolution strategy	Robotics	33.52246268153795	-45.38019579253469	20725
a2c443c5f8492c6beef5ccdd1e59c86379d26610	biased manifold embedding for person-independent head pose estimation	non linear dimensionality reduction;manifold learning;head pose estimation;regression analysis;face modeling and analysis	Head pose estimation is an integral component of face recognition systems and human computer interfaces. To determine the head pose, face images with varying pose angles can be considered to lie on a smooth low-dimensional manifold in high-dimensional feature space. In this paper, we propose a novel supervised approach to manifold-based non-linear dimensionality reduction for head pose estimation. The Biased Manifold Embedding method is pivoted on the ideology of using the pose angle information of the face images to compute a biased geodesic distance matrix, before determining the low-dimensional embedding. A Generalized Regression Neural Network (GRNN) is used to learn the non-linear mapping, and linear multi-variate regression is finally applied on the low-dimensional space to obtain the pose angle. We tested this approach on face images of 24 individuals with pose angles varying from -90◦ to +90◦ with a granularity of 2◦. The results showed significant reduction in the error of pose angle estimation, and robustness to variations in feature spaces, dimensionality of embedding and other parameters.	3d pose estimation;artificial neural network;distance (graph theory);distance matrix;facial recognition system;feature vector;human computer;nonlinear dimensionality reduction;nonlinear system	Vineeth N. Balasubramanian;Sethuraman Panchanathan	2007			computer vision;3d pose estimation;computer science;machine learning;pattern recognition;nonlinear dimensionality reduction;regression analysis	Vision	26.54050713254826	-42.692327505819236	20791
44638a36f1b67f209f056044b30c3afae7f14bce	on directional multiple-output quantile regression	proyeccion;metodo estadistico;analyse multivariable;cuantila;multivariate analysis;multivariate quantile quantile regression multiple output regression halfspace depth portfolio optimization value at risk;62j05;quantile regression;multivariate quantile;regression quantile;value at risk;statistical method;lagrange multiplier;statistical regression;62jxx;portfolio optimization;65c60;62h05;halfspace depth;methode statistique;optimisation portefeuille;regresion multiple;regresion estadistica;projection;exact computation;multiplicateur lagrange;multiplicador lagrange;analisis multivariable;multiple output regression;quantile;optimizacion cartera;regression statistique;regression multiple;multivariate regression;multiple regression	This paper sheds some new light on projection quantiles. Contrary to the sophisticated set analysis used in Kong and Mizera (2008) [13], we adopt a more parametric approach and study the subgradient conditions associated with these quantiles. In this setup, we introduce Lagrange multipliers which can be interpreted in various interesting ways, in particular in a portfolio optimization context. The corresponding projection quantile regions were already shown to coincide with the halfspace depth ones in Kong and Mizera (2008) [13], but we provide here an alternative proof (completely based on projection quantiles) that has the advantage of leading to an exact computation of halfspace depth regions from projection quantiles. Above all, we systematically consider the regression case, which was barely touched in Kong and Mizera (2008) [13]. We show in particular that the regression quantile regions introduced in Hallin, Paindaveine, and Šiman (2010) [6,7] can also be obtained fromprojection (regression) quantiles, whichmay lead to a faster computation of those regions in some particular cases. © 2010 Elsevier Inc. All rights reserved.	computation;general linear model;lagrange multiplier;mathematical optimization;subderivative;subgradient method	Davy Paindaveine;Miroslav Siman	2011	J. Multivariate Analysis	10.1016/j.jmva.2010.08.004	econometrics;multivariate statistics;mathematical optimization;quantile;quantile regression;projection;linear regression;portfolio optimization;mathematics;multivariate analysis;lagrange multiplier;regression analysis;statistics;value at risk	AI	32.87143923775447	-24.205640751494276	20795
c30a3d5a3e033dabe1bb4bb8e0a7063a3c914988	automatic visual speech segmentation and recognition using directional motion history images and zernike moments	motion analysis;zernike moments;temporal segmentation;optical flow;directional motion history image	Appearance-based visual speech recognition using only video signals is presented. The proposed technique is based on the use of directional motion history images (DMHIs), which is an extension of the popular optical-flow method for object tracking. Zernike moments of each DMHI are computed in order to perform the classification. The technique incorporates automatic temporal segmentation of isolated utterances. The segmentation of isolated utterance is achieved using pair-wise pixel comparison. Support vector machine is used for classification and the results are based on leave-one-out paradigm. Experimental results show that the proposed technique achieves better performance in visemes recognition than others reported in literature. The benefit of this proposed visual speech recognition method is that it is suitable for real-time applications due to quick motion tracking system and the fast classification method employed. It has applications in command and control using lip movement to text conversion and can be used in noisy environment and also for assisting speech impaired persons.	computation;concatenation;experimental system;maxima and minima;motion history images;optical flow;overwriting (computer science);pixel;programming paradigm;real-time clock;real-time computing;speech recognition;speech segmentation;support vector machine;tracking system	Ayaz A. Shaikh;Dinesh Kant Kumar;Jayavardhana Gubbi	2012	The Visual Computer	10.1007/s00371-012-0751-7	velocity moments;computer vision;speech recognition;computer science;pattern recognition;optical flow	Vision	40.03260199221833	-48.66117179367925	20812
64fecc16c883e3431a48a8def72dfff6a5f526de	aviator hand tracking based on depth images		Detecting and tracking aviator’s hand in the cockpit is a fundamental task on analyzing and identifying the behavior of aviators. Due to the complicated conditions in the cockpit - such as the lighting varies, the space of Cockpit is narrow, the operation of aviator is sophisticated - tracking the hand in Aircraft Cockpit has more difficulties than tracking the hand in human-machine interaction. We propose a hand tracking method to track the aviator’s hand based on depth images. In our experiment, most of the common flight operations are tested. The average error of hand position tracking is 6.4 mm and the ratio of losing tracking is only 1.4%, which indicate that the proposed algorithm has the ability to tracking the aviator’s hand in the aircraft cockpit accurately.		Xiaolong Wang;Shan Fu	2016		10.1007/978-981-10-3966-9_44	computer vision;cockpit;kalman filter;artificial intelligence;region growing;computer science	Vision	47.05632270951801	-41.5588073527936	20853
235f8e797bc10561ecd684023d2c980d990ea217	end-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation		Recently, Deep Convolutional Neural Networks (DCNNs) have been applied to the task of human pose estimation, and have shown its potential of learning better feature representations and capturing contextual relationships. However, it is difficult to incorporate domain prior knowledge such as geometric relationships among body parts into DCNNs. In addition, training DCNN-based body part detectors without consideration of global body joint consistency introduces ambiguities, which increases the complexity of training. In this paper, we propose a novel end-to-end framework for human pose estimation that combines DCNNs with the expressive deformable mixture of parts. We explicitly incorporate domain prior knowledge into the framework, which greatly regularizes the learning process and enables the flexibility of our framework for loopy models or tree-structured models. The effectiveness of jointly learning a DCNN with a deformable mixture of parts model is evaluated through intensive experiments on several widely used benchmarks. The proposed approach significantly improves the performance compared with state-of-the-art approaches, especially on benchmarks with challenging articulations.	3d pose estimation;benchmark (computing);casio loopy;convolutional neural network;end-to-end principle;experiment;neural network software;sensor	Wei Yang;Wanli Ouyang;Hongsheng Li;Xiaogang Wang	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.335	computer vision;computer science;artificial intelligence;machine learning	Vision	25.749147272092273	-49.01124219148143	20866
757ec70262c338323a53c686af3570a7314a8dc3	multi-kernel linear programming support vector regression with prior knowledge	linear programming support vector regression multi kernel s prior knowledge;support vector machines linear programming regression analysis;kernel support vector machines data models linear programming vectors accuracy training;feature spaces multikernel linear programming support vector regression model multikernel knowledge prior knowledge calibrated simulator	This paper proposes a multi-kernel linear programming support vector regression with prior knowledge in order to obtain an accurate regression model in the case of the scarcity of measured data available. In the algorithm, multi-kernel and prior knowledge which may be exact or biased from a calibrated simulator have been incorporated into the framework of linear programming support vector regression by utilizing multiple feature spaces and modifying optimization formulation. Some experiments from a synthetic example have been carried out, and the results show that the proposed algorithm is effective, and that the obtained model is sparse and accurate. The proposed algorithm shows great potential in some practical applications where the experimental data is few and the prior knowledge from a simulator is available.	algorithm;approximation algorithm;dhrystone;experiment;kernel (operating system);linear programming;mathematical optimization;microwave;self-tuning;simulation;social inequality;sparse matrix;support vector machine;synthetic intelligence;system identification	Jinzhu Zhou;Na Li;Liwei Song	2014	2014 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2014.6889369	principal component regression;least squares support vector machine;kernel method;feature vector;machine learning;pattern recognition;data mining;polynomial kernel	ML	27.554751257498673	-34.20640569380736	21043
9cb4cdd5aa633c242cffcdf6990964457d885ee7	multimodal object categorization by a robot	unsupervised learning;audio visual systems;unsupervised object categorization;probability;plsa;intelligent robots;object categorization;multimodal object categorization;haptic information;probabilistic latent semantic analysis plsa;robot vision;multimodal;audio visual;statistical techniques;haptic interfaces;robot;probabilistic latent semantic analysis;object detection	In this paper unsupervised object categorization by robots is examined. We propose an unsupervised multimodal categorization based on audio-visual and haptic information. The robot uses its physical embodiment to grasp and observe an object from various view points as well as listen to the sound during the observation. The proposed categorization method is an extension of probabilistic latent semantic analysis(pLSA), which is a statistical technique. At the same time the proposed method provides a probabilistic framework for inferring the object property from limited observations. Validity of the proposed method is shown through some experimental results.	automatic differentiation;bottom-up proteomics;categorization;haptic technology;multimodal interaction;online algorithm;probabilistic latent semantic analysis;robot;vii	Tomoaki Nakamura;Takayuki Nagai;Naoto Iwahashi	2007	2007 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2007.4399634	unsupervised learning;computer vision;boosting methods for object categorization;computer science;machine learning;pattern recognition;probabilistic latent semantic analysis;categorization	Robotics	36.99488835265871	-40.87539960747932	21056
3636dfc3eafa70a789de663f938c2942a8286537	unsupervised 3d object recognition and reconstruction in unordered datasets	object motion;image recognition;object recognition;image motion analysis;image databases;computer graphics;image matching;camera matrix;image database;fundamental matrix;unsupervised 3d object recognition;layout;3d object recognition;computer vision;automatic recognition;local features;feature extraction;image reconstruction;sparse bundle adjustment algorithm unsupervised 3d object recognition object reconstruction unordered datasets automatic recognition image databases image matching camera matrix invariant local features ransac algorithm object motion;computer graphics object recognition visual databases image reconstruction image matching image motion analysis cameras;object recognition image reconstruction image recognition cameras image databases layout feature extraction computer science sparse matrices computer vision;object reconstruction;computer science;unordered datasets;structure and motion;sparse bundle adjustment algorithm;sparse matrices;cameras;invariant local features;bundle adjustment;ransac algorithm;visual databases	This paper presents a system for fully automatic recognition and reconstruction of 3D objects in image databases. We pose the object recognition problem as one of finding consistent matches between all images, subject to the constraint that the images were taken from a perspective camera. We assume that the objects or scenes are rigid. For each image, we associate a camera matrix, which is parameterised by rotation, translation and focal length. We use invariant local features to find matches between all images, and the RANSAC algorithm to find those that are consistent with the fundamental matrix. Objects are recognised as subsets of matching images. We then solve for the structure and motion of each object, using a sparse bundle adjustment algorithm. Our results demonstrate that it is possible to recognise and reconstruct 3D objects from an unordered image database with no user input at all.	3d single-object recognition;algorithm;bundle adjustment;camera matrix;database;focal (programming language);fundamental matrix (computer vision);outline of object recognition;random sample consensus;sparse matrix	Matthew Brown;David G. Lowe	2005	Fifth International Conference on 3-D Digital Imaging and Modeling (3DIM'05)	10.1109/3DIM.2005.81	iterative reconstruction;layout;computer vision;camera matrix;ransac;sparse matrix;feature extraction;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;fundamental matrix;bundle adjustment;3d single-object recognition;computer graphics	Vision	49.52926580127397	-49.67031905242641	21066
8a23f21b676a2692177a40a020e871c57a476d69	crowdcam: dynamic region segmentation		We consider the problem of segmenting dynamic regions in CrowdCam images, where a dynamic region is the projection of a moving 3D object on the image plane. Quite often, these regions are the most interesting parts of an image. CrowdCam images is a set of images of the same dynamic event, captured by a group of non-collaborating users. Almost every event of interest today is captured this way. This new type of images raises the need to develop new algorithms tailored specifically for it. We propose an algorithm that segments the dynamic regions in CrowdCam images. The proposed algorithm combines cues that are based on geometry, appearance and proximity. First, geometric reasoning is used to produce rough score maps that determine, for every pixel, how likely it is to be the projection of a static or dynamic scene point. These maps are noisy because CrowdCam images are usually few and far apart both in space and in time. Then, we use similarity in appearance space and proximity in the image plane to encourage neighboring pixels to be labeled similarly as either static or dynamic. We define an objective function that combines all the cues and solves it using an MRF solver. The proposed method was tested on publicly available CrowdCam datasets, as well as a new and challenging dataset we collected. Our results are better than the current state-ofthe-art.	algorithm;experiment;image plane;loss function;map;markov random field;optimization problem;pixel;rough set;solver	Nir Zarrabi;Shai Avidan;Yael Moses	2018	CoRR			Vision	49.48125252429603	-49.89969067847411	21096
83cf7f9c1fd9b1ced122832be3a0231010a38a46	efficient camera path planning algorithm for human motion overview	path planning;viewpoint selection;visualisation;human motion;camera path planning	Camera path planning for character motions is a fundamental and important research topic, benefiting many animation applications. Existing optimal-based approaches are generally computationally expensive and infeasible for interactive applications. In this paper, we propose an efficient approach that can take many constraints of finding the camera path into account and can potentially enable interactive camera control. Instead of solving a highly complicated camera optimization problem in a spatiotemporal four-dimensional space, we heuristically determine the camera path based on an efficient greedy-based tree traversal approach. The experimental results show that the proposed approach can efficiently generate a smooth, informative, and aesthetic camera path that can reveal the significant features of character motions. Moreover, the conducted user study also shows that the generated camera paths are comparable to those of a state-of-the-art approach and those made by professional animators. Copyright # 2011 John Wiley & Sons, Ltd.	anim;algorithmic efficiency;analysis of algorithms;automated planning and scheduling;binary tree;computation;graphics processing unit;greedy algorithm;heuristic;information;john d. wiley;kinesiology;mathematical optimization;motion planning;optimization problem;tree (data structure);tree traversal;usability testing;vector quantization;virtual world	I-Cheng Yeh;Chao-Hung Lin;Hung-Jen Chien;Tong-Yee Lee	2011	Journal of Visualization and Computer Animation	10.1002/cav.398	smart camera;computer vision;camera auto-calibration;simulation;any-angle path planning;computer science;artificial intelligence;motion planning;computer graphics (images)	Graphics	53.29315415785551	-44.953635620239545	21125
dbaca67b5e262ba845aa2013156916e0648ca7a4	time varying metric learning for visual tracking	metric learning;wishart process;visual tracking	"""A new Time Varying Metric Learning model and its Sequential Monte Carlo solution;Wishart Process is introduced to model the time varying metric transition;Side information constraint is adopted to train the model;The proposed TVML model is applied to visual tracking; Traditional tracking-by-detection based methods treat the target and the background as a binary classification problem. This two class classification method suffers from two main drawbacks. Firstly, the learning result may be unreliable when the number of training samples is not large enough. Secondly, the binary classifier tends to drift because of the complex background tracking conditions. In this paper, we propose a new model called Time Varying Metric Learning (TVML) for visual tracking. We adopt the Wishart Process to model the time varying metrics for target features, and apply the Recursive Bayesian Estimation (RBE) framework to learn the metric from the data with """"side information contraint"""". Metric learning with side information is able to omit the clustering of negative samples, which is more preferable in complex background tracking scenarios. The recursive Bayesian model ensures the learned metric is accurate with limited training samples. The experimental results demonstrate the comparable performance of the TVML tracker compared to state-of-the-art methods, especially when there are background clutters."""	video tracking	Jiatong Li;Baojun Zhao;Chenwei Deng;Richard Y. D. Xu	2016	Pattern Recognition Letters	10.1016/j.patrec.2016.06.017	computer vision;simulation;eye tracking;computer science;machine learning;pattern recognition;mathematics;statistics	Vision	33.132177883749286	-47.045566537767485	21139
4e975f46857b71feadd9d69623177c9910f63de0	combining mean-shift and particle filter for object tracking	image features;histograms;quantization;color;particle filtering numerical methods feature extraction image colour analysis image registration monte carlo methods object tracking;mean shift;color histogram;algorithm fusion object tracking mean shift particle filter;satisfiability;particle filter;image color analysis;image colour analysis;feature extraction;image registration;object tracking;monte carlo integration particle filter object tracking illumination condition change target tracking tracking performance tracking speed mean shift technique image registration rgb color histogram image feature bhattacharyya coefficient similarity measurement object model candidate regions;particle filters;target tracking;target tracking particle filters histograms quantization videos color image color analysis;monte carlo methods;particle filtering numerical methods;color image;algorithm fusion;videos;object model	Mean-shift is an effective algorithm for object tracking. However, it has a poor performance when the illumination condition changes fast or the tracking target being shadowed. By contract, particle filter based object tracking has a better tracking performance, but the tracking speed is much slower compared to mean-shift. Owing to the limitations of just using a single algorithm, a novel object tracking method based on both mean-shift and particle filter is proposed in this paper. A system with feedback has been constructed by the proposed method, in which the mean-shift technique is used for initial registration, and the particle filter is called to improve the performance of tracking when the tracking result with mean-shift is unconvincing. RGB color histogram is exploited as image feature and Bhattacharyya coefficient is used for measuring the similarity between object model and candidate regions. Tracking experiments evaluated on various videos show that the proposed method is well-behaved in cases that objects have shift-variant, rotation and scaling, and achieves a satisfying tracking speed.	algorithm;coefficient;color histogram;experiment;feature (computer vision);image scaling;jaccard index;local binary patterns;mean shift;mean squared error;particle filter;variable shadowing	Da Tang;Yu-Jin Zhang	2011	2011 Sixth International Conference on Image and Graphics	10.1109/ICIG.2011.118	computer vision;particle filter;computer science;video tracking;pattern recognition;mathematics;computer graphics (images)	Robotics	43.00341029412017	-51.30639098878615	21194
7644264d388f9c26c24275b93f2d1923a3d9ba93	sufficient dimension reduction in regressions through cumulative hessian directions	diverging parameters;dimension reduction;inverse regression;central subspace;asymptotic properties;sufficient dimension reduction;cumulant;kernel smoothing	To reduce the predictors dimension without loss of information on the regression, we develop in this paper a sufficient dimension reduction method which we term cumulative Hessian directions. Unlike many other existing sufficient dimension reduction methods, the estimation of our proposal avoids completely selecting the tuning parameters such as the number of slices in slicing estimation or the bandwidth in kernel smoothing. We also investigate the asymptotic properties of our proposal when the predictors dimension diverges. Illustrations through simulations and an application are presented to evidence the efficacy of our proposal and to compare it with existing methods.	dimensionality reduction;hessian;simulation;smoothing;sufficient dimension reduction	Li-Mei Zhang;Li-Ping Zhu;Li-Xing Zhu	2011	Statistics and Computing	10.1007/s11222-010-9172-5	sufficient dimension reduction;econometrics;mathematical optimization;machine learning;sliced inverse regression;mathematics;statistics;dimensionality reduction;kernel smoother;cumulant	ML	28.883271289156554	-25.414167287272097	21221
1e8c16778883662fca346f64ced7faa77a87cda5	a contribution to automatic design of image processing systems--breeding optimized non-linear and oriented kernels for texture analysis	image processing;texture classification;texture analysis;system design;evolutionary optimization	The rapid development in image processing technology allows the tackling of application of increasing complexity. For efficient design of application specific systems design automation techniques are required. This paper reports on activities for texture classification employing non-linear oriented kernels configured by evolutionary optimization techniques. Our approach was tested with benchmark and application data from leather inspection and found viable and competitive in both cases.	benchmark (computing);cooperative breeding;image processing;mathematical optimization;nonlinear system;systems design	Stefanie Peters;Andreas Koenig	2006	2006 Sixth International Conference on Hybrid Intelligent Systems (HIS'06)	10.1109/HIS.2006.2	computer vision;computer science;machine learning;texture compression;engineering drawing	EDA	34.49960047206288	-38.51622278923103	21336
6bde5d52b96cd543eee62d8d0a32230d383722b0	evaluation of visual tracking in extremely low frame rate wide area motion imagery	columbus large image format;video surveillance;image motion analysis;clif dataset;low frame rate wide area motion imagery;image formation;surveillance;wide area surveillance;video surveillance image motion analysis image registration object tracking;wami;camera motion;visualization;visualization target tracking cameras videos surveillance vehicles;image registration;background registration visual tracking low frame rate wide area motion imagery wami security related task wide area surveillance video columbus large image format clif dataset vehicle tracking task;object tracking;registration;wide area surveillance video;robust performance;vehicles;vehicle tracking;wide area surveillance visual tracking registration low frame rate;low frame rate;target tracking;vehicle tracking task;visual tracking;background registration;security related task;cameras;videos	Visual tracking in wide area motion imagery (WAMI) is an important problem in security related tasks. The extremely low frame rate and the large camera motion in such videos, however, introduce challenging constraints that distinguish the task from traditional image tracking. In this study we evaluate the performance of several state-of-the-art visual trackers on the wide area surveillance videos. Specifically, we compare five visual trackers on sequences selected from the Columbus Large Image Format (CLIF) dataset for the vehicle tracking task. The experiments are conducted in two configurations: one with background registration to compensate for camera motion, and the other without. We evaluate the tracking performances both qualitatively and quantitatively. The experimental results show that (1) traditional visual trackers meet problems in the wide area surveillance videos and (2) background registration helps enhance the tracking performances, although there exists needed improvement for operational robust performance. We expect the visual tracking evaluation on low-frame rate wide area surveillance videos to motivate future research to addresses the related challenges and provide annotated images for benchmark purposes.	benchmark (computing);columbus;common logic;experiment;performance;vehicle tracking system;video tracking	Haibin Ling;Yi Wu;Erik Blasch;Genshe Chen;Haitao Lang;Li Bai	2011	14th International Conference on Information Fusion		computer vision;simulation;geography;computer graphics (images)	Vision	42.78675203069272	-46.056127595396326	21369
ae6bf42e4f11f3f6287e77d8622c7d2aac71e526	building occupancy detection from carbon-dioxide and motion sensors		Occupant detection using carbon-dioxide sensors is prevalent but its accuracy is restricted by the inherent sensing delays. This paper proposes an indoor occupant detection method using real-time carbon-dioxide and Pyroelectric Infrared (PIR) sensor measurements overcoming the sensing delays. The occupancy detection problem is formulated as a classification problem wherein the classifier learns from offline carbon-dioxide data and the actual occupancy measurements of the room. While the classifier can provide realtime occupancy detection, the delays in carbon-dioxide sensors influence their accuracy. To overcome the delays, observations from PIR sensors are combined with the results of the single-layer feedforward neural network (SLFN) based classifier. The classifier works in four steps: (i) data-preprocessing, (ii) feature-selection, (iii) learning, and (iv) validation. The data is preprocessed by smoothing and several features are selected as input to the SLFN. Then, the classifier is validated with realtime experiments. Our results demonstrate that the proposed approach provides accuracy up to 99.79% and also overcomes the delays found in carbon-dioxide sensors.		Chaoyang Jiang;Zhenghua Chen;Lih Chieh Png;Korkut Bekiroglu;Seshadhri Srinivasan;Rong Su	2018	2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV)	10.1109/ICARCV.2018.8581229	occupancy;control engineering;feedforward neural network;smoothing;computer science;artificial intelligence;pattern recognition	Robotics	38.71807342709736	-42.668843557363125	21454
9f121e8233bc414829ddb24e7720edeb4ac501de	linguistic decision making for robot route learning	manipulators;online linguistic id3 learning linguistic decision making robot route learning problem machine learning model nonlinear mapping robot environment interaction linguistic decision tree robot behavior adaptive system training parallelization control parallelization training data quality;nonlinear control systems;path planning;mobile robots;data analysis;computational linguistics;path planning computational linguistics data analysis decision making decision trees learning artificial intelligence manipulators mobile robots nonlinear control systems;task decomposition atomic action dynamic behavior decision linguistic decision tree robot route learning;learning artificial intelligence;decision trees;institutional repository research archive oaister	Machine learning enables the creation of a nonlinear mapping that describes robot-environment interaction, whereas computing linguistics make the interaction transparent. In this paper, we develop a novel application of a linguistic decision tree for a robot route learning problem by dynamically deciding the robot's behavior, which is decomposed into atomic actions in the context of a specified task. We examine the real-time performance of training and control of a linguistic decision tree, and explore the possibility of training a machine learning model in an adaptive system without dual CPUs for parallelization of training and control. A quantified evaluation approach is proposed, and a score is defined for the evaluation of a model's robustness regarding the quality of training data. Compared with the nonlinear system identification nonlinear auto-regressive moving average with eXogeneous inputs model structure with offline parameter estimation, the linguistic decision tree model with online linguistic ID3 learning achieves much better performance, robustness, and reliability.	adaptive system;algorithm;central processing unit;computation (action);control system;decision tree model;dual;embedding;estimation theory;experiment;global descriptor table;lejos rcx;linearizability;linguistics;machine learning;mobile robot;nonlinear system identification;online and offline;parallel computing;population parameter;real-time locating system;robot control;robustness (computer science);telbivudine	Hongmei He;T. Martin McGinnity;Sonya A. Coleman;Bryan Gardiner	2014	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2013.2258037	natural language processing;mobile robot;robot learning;decision engineering;computer science;artificial intelligence;computational linguistics;machine learning;decision tree;motion planning;data analysis;id3 algorithm	ML	48.87933757698916	-25.39163411425439	21493
178b25d3dd1e18db67e56debbbfd8daa41273b7f	multitask principal component analysis		Principal Component Analysis (PCA) is a canonical and well-studied tool for dimensionality reduction. However, when few data are available, the poor quality of the covariance estimator at its core may compromise its performance. We leverage this issue by casting the PCA into a multitask framework, and doing so, we show how to solve simultaneously several related PCA problems. Hence, we propose a novel formulation of the PCA problem relying on a novel regularization. This regularization is based on a distance between subspaces, and the whole problem is solved as an optimization problem over a Riemannian manifold. We experimentally demonstrate the usefulness of our approach as pre-processing for EEG signals.	biometrics;computer multitasking;dimensionality reduction;electroencephalography;experiment;image processing;linear discriminant analysis;locality of reference;mathematical optimization;matrix regularization;optimization problem;preprocessor;principal component analysis;synthetic intelligence	Ikko Yamane;Florian Yger;Maxime Berar;Masashi Sugiyama	2016			estimator;machine learning;pattern recognition;dimensionality reduction;principal component analysis;artificial intelligence;multi-task learning;covariance;regularization (mathematics);mathematical optimization;mathematics;optimization problem;compromise	ML	25.081756176986936	-40.17066775346556	21554
b46913ca0cdf6610c0b834fb2b618bbf508cb15e	reply to determining structural identifiability of parameter learning machines	jacobian matrix;exhaustive summaries;parameter redundancy;derivative matrix;local identifiability;qa mathematics inc computing science;global identifiability	The paper Ran and Hu (2014, Neurocomputing) examines identifiability and parameter redundancy in classes of models used in machine learning. This note discusses the results on global identifiability and also clarifies that the paper’s results on parameter redundancy already exist in the paper Cole et al. (2010, Mathematical Biosciences).	machine learning;neurocomputing	Diana J. Cole	2016	Neurocomputing	10.1016/j.neucom.2015.09.016	jacobian matrix and determinant;econometrics;matrix calculus;machine learning;mathematics;statistics	AI	32.83654133434644	-26.03170074762611	21580
2c5804f0e27623cc35f73d5b305b14ea19c7bcf3	multi-layer graph constraints for interactive image segmentation via game theory	game theory;image segmentation;multi layer graph;nonparametric learning;superpixel	The combination of pixels and superpixels has been widely used for image segmentation, where the pixels and superpixels are segmented together. These combination methods can obtain more robust results by using more informative superpixel features. However, since the superpixel may not accurately capture the details for the small and slender regions, the results of these combination methods are often label inconsistent with the objects. Furthermore, these methods also fall into expensive time cost due to introducing more interactions between pixels and superpixels. To overcome the above problems, in this paper, we propose an interactive image segmentation method based on multi-layer graph constraints. The relationships between pixels/superpixels and labels are introduced into the conventional combination framework to further improve the segmentation accuracy. The segmentation model is constructed based on the estimation of probabilities of pixels and superpixels by a nonparametric learning framework. Then the probabilities of pixels and superpixels are updated iteratively by utilizing the game theory based optimization strategy. Experiments on challenging data sets demonstrate that the proposed method can obtain better segmentation results than the state-of-the-art methods.	algorithm;classical xy model;game theory;grabcut;image segmentation;information;interaction;ll grammar;layer (electronics);linear equation;mathematical optimization;pixel;read-write memory;the matrix;wxwidgets	Tao Wang;Quan-Sen Sun;Zexuan Ji;Qiang Chen;Peng Fu	2016	Pattern Recognition	10.1016/j.patcog.2016.01.018	game theory;computer vision;computer science;machine learning;pattern recognition;mathematics;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation	Vision	44.147388330583894	-51.903045762328766	21584
4ad3145e1d676911f3d65f0f76144e5338f9db8b	airborne monitoring of ground traffic behaviour for hidden threat assessment	kalman filter;kalman filters;string matching;ground traffic behaviour;traffic engineering computing;computerised monitoring;trajectory classification;image classification;hidden threat assessment;airborne monitoring;behaviour recognition technique;tracking filter;trajectory approximation;sensor fusion;behaviour recognition;history;trajectory;acceleration	This paper focuses on development of behaviour recognition technique for airborne monitoring of ground traffic to detect hidden threats. To enhance tracking accuracy, sensor fusion and smoothing are applied with Kalman filter. To tackle behaviour recognition, trajectory approximation and classification methodology is proposed using differential geometric quantities and string matching. Simulation on a ground vehicle is done to verify the feasibility of the proposed algorithms.	airborne ranger;approximation;kalman filter;motion planning;sensor web;simulation;smoothing;string searching algorithm;swarm;unmanned aerial vehicle	Seungkeun Kim;Rafal Zbikowski;Antonios Tsourdos;Brian A. White	2010	2010 13th International Conference on Information Fusion		computer vision;simulation;engineering;machine learning	Robotics	51.591034563030135	-34.05889325634579	21642
10bf197aa7dd57a2b174d5ad36389a21f32707b8	low-rank matrix factorization with attributes	matrix factorization;information retrieval;artificial intelligent;tensor product;learning methods;matrix completion;function approximation;science learning;collaborative filtering	We develop a new collaborative filtering (CF) method that combines both previously known users’ preferences, i.e. standard CF, as well as product/user attributes, i.e. classical function approximation, to predict a given user’s interest in a particular product. Our method is a generalized low rank matrix completion problem, where we learn a function whose inputs are pairs of vectors – the standard low rank matrix completion problem being a special case where the inputs to the function are the row and column indices of the matrix. We solve this generalized matrix completion problem using tensor product kernels for which we also formally generalize standard kernel properties. Benchmark experiments on movie ratings show the advantages of our generalized matrix completion method over the standard matrix completion one with no information about movies or people, as well as over standard multi-task or single task learning methods.	approximation;benchmark (computing);collaborative filtering;computer multitasking;experiment;the matrix	Jacob D. Abernethy;Francis R. Bach;Theodoros Evgeniou;Jean-Philippe Vert	2006	CoRR		tensor product;mathematical optimization;sparse matrix;function approximation;computer science;theoretical computer science;collaborative filtering;machine learning;mathematics;augmented matrix;matrix decomposition	ML	28.657652069726307	-34.97056201161093	21653
cab1f0d6fd7cccce8fe69188638b0cb2b9fd8e82	dense rgb-d-inertial slam with map deformations		While dense visual SLAM methods are capable of estimating dense reconstructions of the environment, they suffer from a lack of robustness in their tracking step, especially when the optimisation is poorly initialised. Sparse visual SLAM systems have attained high levels of accuracy and robustness through the inclusion of inertial measurements in a tightly-coupled fusion. Inspired by this performance, we propose the first tightly-coupled dense RGB-D-inertial SLAM system. Our system has real-time capability while running on a GPU. It jointly optimises for the camera pose, velocity, IMU biases and gravity direction while building up a globally consistent, fully dense surfel-based 3D reconstruction of the environment. Through a series of experiments on both synthetic and real world datasets, we show that our dense visual-inertial SLAM system is more robust to fast motions and periods of low texture and low geometric variation than a related RGB-D-only SLAM system.	3d reconstruction;experiment;graphics processing unit;mathematical optimization;real-time clock;real-time computing;simultaneous localization and mapping;sparse;surfel;synthetic intelligence;velocity (software development)	Tristan Laidlow;Michael Bloesch;Wenbin Li;Stefan Leutenegger	2017	2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2017.8206591	artificial intelligence;robustness (computer science);3d reconstruction;computer vision;computer science;surfel;inertial frame of reference;inertial measurement unit;rgb color model	Robotics	52.743507623060005	-45.41363102982017	21708
3d0a5a08ae5594d6d3eed40a90fc4d0f547d8f4e	multiple moving object recognitions in video based on log gabor-pca approach		Object recognition in the video sequence or images is one of the subfield of computer vision. Moving object recognition from a video sequence is an appealing topic with applications in various areas such as airport safety, intrusion surveillance, video monitoring, intelligent highway, etc. Moving object recognition is the most challenging task in intelligent video surveillance system. In this regard, many techniques have been proposed based on different methods. Despite of its importance, moving object recognition in complex environments is still far from being completely solved for low resolution videos, foggy videos, and also dim video sequences. All in all, these make it necessary to develop exceedingly robust techniques. This paper introduces multiple moving object recognition in the video sequence based on LoG Gabor-PCA approach and Angle based distance Similarity measures techniques used to recognize the object as a human, vehicle etc. Number of experiments are conducted for indoor and outdoor video sequences of standard datasets and also our own collection of video sequences comprising of partial night vision video sequences. Experimental results show that our proposed approach achieves an excellent recognition rate. Results obtained are satisfactory and competent.	closed-circuit television;computer vision;experiment;image resolution;log gabor filter;outline of object recognition;video	M. T. Gopala Krishna;M. Ravishankar;D. R. Ramesh Babu	2013		10.1007/978-3-319-01778-5_10	computer vision;simulation;computer science;video tracking;multimedia;3d single-object recognition	Vision	41.45128339105124	-44.68870462655024	21728
323c12b895ace6ddedf9d7a2884e77183bafc7dc	intrinsic dimension estimation using packing numbers	intrinsic dimension	We propose a new algorithm to estimate the intrinsic dimensi on of data sets. The method is based on geometric properties of the data and requires neither parametric assumptions on the data generati g model nor input parameters to set. The method is compared to a similar, widelyused algorithm from the same family of geometric techniques . Experiments show that our method is more robust in terms of the data g enerating distribution and more reliable in the presence of noise.	algorithm;correlation dimension;experiment;intrinsic dimension;packing dimension;set packing	Balázs Kégl	2002			combinatorics;discrete mathematics;computer science;mathematics;intrinsic dimension;statistics	ML	31.605905219193467	-30.24302592682498	21776
70542b97fdf6ef7111ae02866769352589c0d4fd	categorization in natural time-varying image sequences	histograms;time varying;video surveillance;image sequences monitoring birds surveillance histograms face detection spatial resolution testing statistics costs;baseline bag of features approach natural time varying image sequences single image categorization multiview categorization approach natural environment monitoring manual localization;training;baseline bag of features approach;multiview categorization approach;natural environment monitoring;accuracy;birds;natural environment;feature extraction;pixel;video surveillance image sequences;image sequence;single image categorization;manual localization;image sequences;natural time varying image sequences	Approaches to single image categorization do not easily generalize to natural time-varying image sequences. In natural environments, object categories tend to have few features that help to distinguish between each other and the surrounding environment. To better discriminate between categories and the surrounding environment, we propose a multi-view categorization approach that exploits the statistics of image sequences rather than single images. The approach is unbiased towards redundant views - that is, it does not matter how many times an object appears from the same viewpoint. At the same time, the approach does not penalize for missing views, so that we do not have to capture an object at all viewpoints to successfully categorize the object. We first present a data set for studying natural environment monitoring: an image sequence of birds at a feeder station. After manual localization, a baseline bag of features approach was found to perform significantly worse on the proposed data set compared to the standard Caltech 101 data set. We find that our approach increases the categorization accuracy from 48% to 58% on average when compared to an equivalent single view categorization method. Finally, we show how the same metric proposed for the supervised categorization can be used to transform, in an unsupervised manner, an image sequence into a manageable set of categories.	autostereogram;baseline (configuration management);caltech 101;categorization;embedded system;free viewpoint television;ibm notes;supervised learning;unbalanced circuit;unsupervised learning	Teresa Ko;Stefano Soatto;Deborah Estrin	2009	2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2009.5204208	computer vision;feature extraction;boosting methods for object categorization;computer science;segmentation-based object categorization;pattern recognition;data mining;histogram;mathematics;accuracy and precision;natural environment;pixel;statistics	Vision	35.098371777346195	-51.69334303460835	21852
84f3b6826e9e243b84264c470ad3b3362c8fb0cc	sparse bayesian multi-task learning		We propose a new sparse Bayesian model for multi-task regression and classification. The model is able to capture correlations between tasks, or more specifically a low-rank approximation of the covariance matrix, while being sparse in the features. We introduce a general family of group sparsity inducing priors based on matrix-variate Gaussian scale mixtures. We show the amount of sparsity can be learnt from the data by combining an approximate inference approach with type II maximum likelihood estimation of the hyperparameters. Empirical evaluations on data sets from biology and vision demonstrate the applicability of the model, where on both regression and classification tasks it achieves competitive predictive performance compared to previously proposed methods.	approximation algorithm;bayesian network;bayesian programming;computer multitasking;lasso;low-rank approximation;machine learning;multi-task learning;sparse matrix	Cédric Archambeau;Shengbo Guo;Onno Zoeter	2011			computer science;machine learning;pattern recognition;sparse approximation;statistics	ML	28.086224897921753	-32.85344140971981	21874
0816f114ddd3007ec24abcd0161aace92dc7e550	generalized grid framework for multi sensor data fusion	robot sensing systems;fuzzy set;object detection grid occupancy grid grid of evidence fuzzy set histogram filter;measurement by laser beam;polar grids;histogram filter;multisensor data fusion;occupancy grid;sensor fusion grid computing;laser radar;grid of evidence;data representation;grid;multi sensor data fusion;distance measurement;3d grids;vehicles laser radar cameras radar distance measurement measurement by laser beam robot sensing systems;tracking schemas;vehicles;generalized grid framework;sensor fusion;grid computing;cameras;object detection;radar;tracking schemas multisensor data fusion data representation generalized grid framework polar grids 3d grids object detection	This paper presents a methodology for multi sensor data fusion that uses the accumulation grid idea for the representation of data. A generalized grid framework is introduced to represent measurement data and fusion results in a common way. This allows the definition of standardized prediction and fusion operations and includes the variation between Cartesian and polar grids as well as the extension to 3D grids. It is shown that most of the typical object detection and tracking schemas can be represented by the framework. Two application examples including processing results from the field of automotive environment recognition are given to demonstrate special cases of the framework.	grid computing;object detection;tree accumulation;video tracking	Ulrich Scheunert;Norman Mattern;Philipp Lindner;Gerd Wanielik	2008	2008 11th International Conference on Information Fusion		computer vision;computer science;data mining;remote sensing	Robotics	51.09194924780609	-35.80083611602384	21898
783b84da7ad96394e954896e7c1e3a403017841b	long-term tracking algorithm with the combination of multi-feature fusion and yolo		In recent year correlation filtering based algorithms have achieved significant performance in tracking. In traditional, the previous frame has been trained in order to get the prediction position of the next frame. However, in experiments we find that once the present frame drifts, the latter frame will be affected and accumulate error, which can cause the loss of the target eventually. In that case, the tracker cannot track a target for a long time. To solve these problems and design an efficient long-term tracking algorithm, we propose a long-term tracking algorithm by combining the short-term tracker and the YOLO v2 detector. We use the SURF algorithm to get the similarity of the tracking result and the current contrast template, once the similarity is lower than a threshold, the YOLO v2 will be activated and find the right target through a three-stage cascade selecting mechanism we designed before, then the short-term tracker will be restarted and the contrast template will be updated. In this way, the short-term tracker can be transformed to a long-term tracker which is able to track a target for a long time in complex circumstance. Besides, we also adopt the compound feature to improve our short-term tracker, so our algorithm has better accuracy and robustness. The experimental results demonstrate the proposed approach outperforms state-of-the-art approaches on large-scale benchmark datasets.	algorithm	Sicong Jiang;Jianing Zhang;Yunzhou Zhang;Feng Qiu;Dongdong Wang;Xiaobo Liu	2018		10.1007/978-981-13-1702-6_39	fold (higher-order function);robustness (computer science);filter (signal processing);algorithm;detector;computer science	Vision	42.213578073798104	-48.82357885924796	21899
8b5e30e760c5c595f57fac5de783296ee2d2a483	visual imaging of invisible hazardous substances using bacterial inspiration	spatiotemporal substance visual imaging bacterial inspiration nerve gas nuclear radiation multiple robotic agent visual representation voronoi partition method bacterium mathematical model invisible spatial hazardous substance;hazardous materials;visual imaging bacterium inspired algorithm environmental monitoring optimal coverage simplistic agents;hazardous areas;mathematical model visualization robots microorganisms sociology statistics equations;computational geometry;service robots;qa75 electronic computers computer science;multi agent systems;robot vision;service robots computational geometry hazardous areas hazardous materials multi agent systems multi robot systems robot vision;bacterium inspired algorithm;multi robot systems;article;environmental monitoring	Providing a visual image of a hazardous substance such as nerve gas or nuclear radiation using multiple robotic agents could be very useful particularly when the substance is invisible. Such visual representation could show where the hazardous substance concentration is highest through the deployment of a higher density of robotic agents to that area enabling humans to avoid such areas. We present an algorithm that is capable of doing the aforementioned with very minimal cost when compared with other techniques such as Voronoi partition methods. Using a mathematical proof, we show that the algorithm would always converge to the distribution of a spatial quantity under investigation. The mathematical model of the bacterium as developed by Berg and Brown is used in this paper, and through simulations and physical experiments, we show that a controller based upon the model is capable of being used to visually represent an invisible spatial hazardous substance using simplistic agents with the future possibility of the same algorithm being used to track a rapidly changing spatiotemporal substance. We believe that the algorithm has this potential because of its low communication and computational needs.	algorithm;berg connector;computation;controller (computing);converge;experiment;mathematical model;robot;simulation;software deployment;voronoi diagram	John Oluwagbemiga Oyekan;Dongbing Gu;Huosheng Hu	2013	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMCA.2012.2231410	computer vision;simulation;computational geometry;computer science;artificial intelligence;machine learning;environmental monitoring;hazardous waste	Robotics	53.20198050170432	-28.148803583255784	21901
16c8760c68f9c13e31cae77ab95827fad297d6ef	determining the number of signals correlated across multiple data sets for small sample support	detectors;covariance matrices;signal processing;principal component analysis;correlation;europe;data models	This paper presents a detection scheme for determining the number of signals that are correlated across multiple data sets when the sample size is small compared to the dimensions of the data sets. To accommodate the sample-poor regime, we decouple the problem into several independent two-channel order-estimation problems that may be solved separately by a combination of principal component analysis (PCA) and canonical correlation analysis (CCA). Since the signals that are correlated across all data sets must be a subset of the signals that are correlated between any pair of data sets, we keep only the correlated signals for each pair of data sets. Then, a criterion inspired by a traditional information-theoretic criterion is applied to estimate the number of signals correlated across all data sets. The performance of the proposed scheme is verified by simulations.	heuristic;information theory;monte carlo method;principal component analysis;simulation	Yang Song;Tanuj Hasija;Peter J. Schreier;David Ramírez	2016	2016 24th European Signal Processing Conference (EUSIPCO)	10.1109/EUSIPCO.2016.7760504	econometrics;pattern recognition;mathematics;statistics	ML	32.12199293488762	-29.963358268815277	21915
4afa840865b878365d1d77f5e01cf9d2422bd870	feature selection for value function approximation using bayesian model selection	model selection;marginal likelihood;reinforcement learning;real world application;policy evaluation;bayesian model selection;state space;feature selection;value function approximation;value function;gaussian process	Feature selection in reinforcement learning (RL), i.e. choosing basis functions such that useful approximations of the unkown value function can be obtained, is one of the main challenges in scaling RL to real-world applications. Here we consider the Gaussian process based framework GPTD for approximate policy evaluation, and propose feature selection through marginal likelihood optimization of the associated hyperparameters. Our approach has two appealing benefits: (1) given just sample transitions, we can solve the policy evaluation problem fully automatically (without looking at the learning task, and, in theory, independent of the dimensionality of the state space), and (2) model selection allows us to consider more sophisticated kernels, which in turn enable us to identify relevant subspaces and eliminate irrelevant state variables such that we can achieve substantial computational savings and improved prediction performance.	approximation algorithm;basis function;bayes factor;bayesian network;bellman equation;computation;feature selection;gaussian process;image scaling;marginal model;mathematical optimization;model selection;reinforcement learning;relevance;state space	Tobias Jung;Peter Stone	2009		10.1007/978-3-642-04180-8_60	marginal likelihood;computer science;state space;machine learning;pattern recognition;gaussian process;mathematics;bellman equation;feature selection;reinforcement learning;model selection;statistics	ML	25.546967188062798	-31.35058458257471	21969
e1a1c3d3e0336d6bfc48fd480a9d18a4f92edc60	training restricted boltzmann machines: an introduction	restricted boltzmann machines;neural networks;parallel tempering;gibbs sampling;markov random fields;contrastive divergence learning;markov chains	Restricted Boltzmann machines (RBMs) are probabilistic graphical models that can be interpreted as stochastic neural networks. They have attracted much attention as building blocks for the multi-layer learning systems called deep belief networks, and variants and extensions of RBMs have found application in a wide range of pattern recognition tasks. This tutorial introduces RBMs from the viewpoint of Markov random fields, starting with the required concepts of undirected graphical models. Different learning algorithms for RBMs, including contrastive divergence learning and parallel tempering, are discussed. As sampling from RBMs, and therefore also most of their learning algorithms, are based on Markov chain Monte Carlo (MCMC) methods, an introduction to Markov chains and MCMC techniques is provided. Experiments demonstrate relevant aspects of RBM training.	algorithm;artificial neural network;bayesian network;deep belief network;experiment;graph (discrete mathematics);graphical model;layer (electronics);machine learning;markov chain monte carlo;markov random field;monte carlo method;parallel tempering;pattern recognition;restricted boltzmann machine;sampling (signal processing);stochastic neural network	Asja Fischer;Christian Igel	2014	Pattern Recognition	10.1016/j.patcog.2013.05.025	boltzmann machine;markov chain;gibbs sampling;computer science;theoretical computer science;machine learning;pattern recognition;parallel tempering;artificial neural network;algorithm;statistics	ML	24.720720714395267	-30.322486766408403	21992
75d4ea92c75f51593128f90fb60fcf4a9516d4a5	algorithm 963: estimation of stochastic covariance models using a continuum of moment conditions	continuous time;stochastic covariance;principal component process;wishart process;continuum of moment conditions;estimation;characteristic function	We describe the implementation of a parameter estimation method suitable for models commonly used in quantitative finance. The Continuum-Generalized Method of Moments (CGMM) is a Generalized Method of Moments (GMM) type of methodology that applies a continuum of moment conditions to achieve the efficiency of a Maximum Likelihood method. Instead of the transition density, the more commonly available conditional characteristic function is used for estimation. We test the CGMM and a simpler version, called the CMM, on simulated time series to check the recovery of the parameters. We also applied CMM to two stochastic covariance models, the Wishart Affine Stochastic Correlation (WASC) model and the Principal Components Stochastic Volatility (PCSV) model. This illustrates the power of CGMM, as stochastic covariance models are generally hard to estimate. The estimation method is fully implemented in MATLAB.	algorithm;capability maturity model;central processing unit;characteristic function (convex analysis);cryptographic hash function;data point;discretization;estimation theory;matlab;markov chain;newton's method;observable;time series;triune continuum paradigm;volatility	Marcos Escobar;Benedikt Rudolph;Rudi Zagst	2016	ACM Trans. Math. Softw.	10.1145/2834115	econometrics;mathematical optimization;estimation;characteristic function;mathematics;statistics;covariance function	ML	31.500462548128585	-26.151275961731425	22088
03dc673718826a8dc3d85e58f8954c7080818c76	reducing the computational cost of bayesian indoor positioning systems	bayesian network;gibbs sampler;monte carlo sampling;bayes methods;indoor communication;computational efficiency bayesian methods sampling methods monte carlo methods infrared sensors handheld computers training data packaging communications society computer science;cost reduction;sampling technique;low latency;sampling methods bayes methods cost reduction indoor communication monte carlo methods;slice sampling bayesian indoor positioning systems computational cost reduction monte carlo sampling strategies gibbs metropolis samplers uniform sampling localization networks;sampling methods;indoor positioning;monte carlo methods;analytical model	In this work we show how to reduce the computational cost of using Bayesian networks for localization. We investigate a range of Monte Carlo sampling strategies, including Gibbs and Metropolis. We found that for our Gibbs samplers, most of the time is spent in slice sampling. Moreover, our results show that although uniform sampling over the entire domain suffers occasional rejections, it has a much lower overall computational cost than approaches that carefully avoid rejections. The key reason for this efficiency is the flatness of the full conditionals in our localization networks. Our sampling technique is also attractive because it does not require extensive tuning to achieve good performance, unlike the Metropolis samplers. We demonstrate that our whole domain sampling technique converges accurately with low latency. On commodity hardware our sampler localizes up to 10 points in less than half a second, which is over 10 times faster than a common general-purpose Bayesian sampler. Our sampler also scales well, localizing 51 objects with no location information in the training set in less than 6 seconds. Finally, we present an analytic model that describes the number of evaluations per variable using slice sampling. The model allows us to analytically determine how flat a distribution should be so that whole domain sampling is computationally more efficient when compared to other methods	algorithmic efficiency;bayesian network;commodity computing;computation;general-purpose modeling;global positioning system;glossary of computer graphics;indoor positioning system;internationalization and localization;metropolis;monte carlo method;sampling (signal processing);slice sampling;test set	Konstantinos Kleisouris;Richard P. Martin	2006	2006 3rd Annual IEEE Communications Society on Sensor and Ad Hoc Communications and Networks	10.1109/SAHCN.2006.288512	metropolis–hastings algorithm;sampling;econometrics;simulation;importance sampling;computer science;slice sampling;rejection sampling;umbrella sampling;monte carlo integration;statistics;monte carlo method	Mobile	37.65015683473293	-26.689501688514813	22137
064b797aa1da2000640e437cacb97256444dee82	coarse-to-fine face alignment with multi-scale local patch regression		Facial landmark localization plays an important role in face recognition and analysis applications. In this paper, we give a brief introduction to a coarse-to-fine pipeline with neural networks and sequential regression. First, a global convolutional network is applied to the holistic facial image to give an initial landmark prediction. A pyramid of multi-scale local image patches is then cropped to feed to a new network for each landmark to refine the prediction. As the refinement network outputs a more accurate position estimation than the input, such procedure could be repeated several times until the estimation converges. We evaluate our system on the 300-W dataset [11] and it outperforms the recent state-of-the-arts.	artificial neural network;facial recognition system;holism;refinement (computing)	Zhiao Huang;Erjin Zhou;Zhimin Cao	2015	CoRR		computer vision;computer science;machine learning;pattern recognition	Vision	30.948047430403566	-49.60651926298431	22277
1bd2a2c1481bffcd6c17edcc6b2fc44afd605db7	mining for similarities in time series data using wavelet-based feature vectors and neural networks	wavelet analysis;power quality;large data sets;time series;data mining;classification;feature vector;wavelet transform;nonstationary time series;pattern classification;time varying data;time series data;similarity detection;electric power;s transform;classification accuracy;time frequency analysis;neural network;knowledge discovery	This paper presents a comparison between different wavelet feature vectors for data mining of nonstationary time series that occurs in an electricity supply network. Three different wavelet algorithms are simulated and applied on nine classes of power signal time series, which primarily belongs to an important problem area called electric power quality. In contrast to the wavelet analysis, the paper presents a new approach called S-transform-based time frequency analysis in processing power quality disturbance data. Certain pertinent feature vectors are extracted using the well-known wavelet methods and the new approach using S-transform. Neural networks are then used to compute the classification accuracy of the feature vectors. Certain characteristics of the wavelet feature vectors are apparent from the results. Further in large data sets partitioning is done and similarities of pattern vectors present in different sections are determined. The approach is a general one and can be applied to pattern classification, similarity determination, and knowledge discovery in time varying data patterns occurring in many practical sciences and engineering problems. r 2006 Published by Elsevier Ltd.	algorithm;applications of artificial intelligence;approximation;artificial neural network;coefficient;data mining;discrete wavelet transform;distortion;electric power quality;feature vector;frequency analysis;image scaling;modulus of continuity;pattern recognition;relevance;s transform;soft computing;time series;wavelet;whole earth 'lectronic link	Pradipta Kishore Dash;Maya Nayak;Manas Ranjan Senapati;Ian W. C. Lee	2007	Eng. Appl. of AI	10.1016/j.engappai.2006.06.018	computer science;machine learning;time series;pattern recognition;data mining;knowledge extraction;artificial neural network;statistics	DB	35.572184539582565	-32.18500461053116	22309
9d530c341b3eda84c0b0a2c3149232daf16056f4	hierarchical temporal memory enhanced one-shot distance learning for action recognition		One-shot action recognition is one of the most challenging tasks due to the very limited training samples. For one-shot video action recognition, randomly selected frames from cluttered frame features may result in a poor performance. To use the most valuable frames in a better feature space, this paper proposes Hierarchical Temporal Memory Enhanced One-shot Distance Learning (HED). Firstly, we introduce temporal triplet from different frames, so that the intra-class distance will be decreased while the inter-class distance will be increased. Secondly, the Hierarchical Temporal Memory (HTM), a biological plausible unsupervised model for sequence prediction, is employed to enhance the one-shot action recognition by finding the most valuable frames in a video sequence. Finally, the selected frames together with the temporal triplet trained model are used to get the corresponding category label. Extensive experiments conducted on three benchmark datasets (i.e UCF11, UCF50 and HMDB51) demonstrate that we can achieve significant improvement than the state-of-the-art methods.	benchmark (computing);experiment;feature vector;hierarchical temporal memory;pattern recognition;randomness;triplet state	Yixiong Zou;Yemin Shi;Yaowei Wang;Yu Shu;Qingsheng Yuan;Yonghong Tian	2018	2018 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2018.8486447	artificial intelligence;distance education;pattern recognition;feature extraction;feature vector;computer science;hierarchical temporal memory;hidden markov model	Vision	31.315455498478943	-50.62196947228265	22312
63a0ca648eafa80d1775dc9082fea16a113eea4b	morphological neural networks for real-time vision based self-localization	local algorithm;mobile robot;real time;false recognition;real time vision;indoor environment;neural network	In this paper we present some real time results of the implementation on a mobile robot of visual self-localization algorithms  based on Morphological Heteroassociative Memories (MHM). We propose a dual set of min/max MHM storing the views that serve  as landmarks for self localization. The binarized input images are subject to erosion in order to increase the robustness  of the recall process. We present some empirical results on basic navigation experiments in an indoor environment. We use  as the measure of performance of our approach the rate of false recognition, conditioned to some landmark being recognized.  	artificial neural network;real-time transcription	Ivan Villaverde;S. Ibañez;F. Xabier Albizuri;Manuel Graña	2005		10.1007/3-540-32391-0_15	computer vision;artificial intelligence;machine learning;time delay neural network	Robotics	47.33182467134436	-36.79249375850425	22341
8e069b2447751a2ac44056203a47fa97ac803e4b	person search in a scene by jointly modeling people commonness and person uniqueness	gmm;fisher vector;generative model;person search	This paper presents a novel framework for a multimedia search task: searching a person in a scene using human body appearance. Existing works mostly focus on two independent problems related to this task, i.e., people detection and person re-identification. However, a sequential combination of these two components does not solve the person search problem seamlessly for two reasons: 1) the errors in people detection are carried into person re-identification unavoidably; 2) the setting of person re-identification is different from that of person search which is essentially a verification problem. To bridge this gap, we propose a unified framework which jointly models the commonness of people (for detection) and the uniqueness of a person (for identification). We demonstrate superior performance of our approach on public benchmarks compared with the sequential combination of the state-of-the-art detection and identification algorithms.	algorithm;benchmark (computing);search problem;unified framework	Yuanlu Xu;Bingpeng Ma;Rui Huang;Liang Lin	2014		10.1145/2647868.2654965	simulation;computer science;generalized method of moments;machine learning;generative model	Vision	33.76307143342184	-50.25530362747584	22391
a32fc216e389f053ce554404b8807c2255e7e406	visual data association for real-time video tracking using genetic and estimation of distribution algorithms	real time;video tracking;estimation of distribution algorithm;genetics;evolutionary computation	In this article, an efficient and novel approach for video data association is developed. This new method is formulated as a search across the hypotheses space defined by the possible association among tracks and detections, carried out for each frame of a video sequence. The full data association problem in visual tracking is formulated as a combinatorial hypotheses search with a heuristic evaluation function taking into account structural and specific information such as distance, shape, color, etc. To guarantee real-time performance, a time limit is set for the search process explore alternative solutions. This time limit defines the upper bound of the number of evaluations depending on search algorithm efficiency. Estimation distribution algorithms are proposed as an efficient evolutionary computation technique to search in this hypothesis space. Finally, an exhaustive comparison of the performance of alternative algorithms is carried out considering complex representative situations in real video sets. VC 2009 Wiley Periodicals, Inc. Int J Imaging Syst Technol, 19, 208–220, 2009; Published online in Wiley InterScience (www.interscience. wiley.com). DOI 10.1002/ima.20196	algorithmic efficiency;correspondence problem;estimation of distribution algorithm;evaluation function;evolutionary computation;genetic algorithm;heuristic evaluation;john d. wiley;real-time clock;real-time computing;search algorithm;sensor;video tracking	Miguel A. Patricio;Jesús Caja García;Antonio Berlanga;José M. Molina López	2009	Int. J. Imaging Systems and Technology	10.1002/ima.20196	computer vision;mathematical optimization;estimation of distribution algorithm;computer science;theoretical computer science;machine learning;video tracking;mathematics;statistics;evolutionary computation	Vision	44.936134657508674	-48.296156787986824	22452
107da162ed1c9252f35bb357f278faecb77bfdd6	a general framework for multi-human tracking using kalman filter and fast mean shift algorithms	mean shift;kalman filter;human tracking	The task of reliable detection and tracking of multiple objects becomes highly complex for crowded scenarios. In this paper, a robust framework is presented for multi-Human tracking. The key contribution of the work is to use fast calculation for mean shift algorithm to perform tracking for the cases when Kalman filter fails due to measurement error. Local density maxima in the difference image usually representing moving objects are outlined by a fast non-parametric mean shift clustering procedure. The proposed approach has the robust ability to track moving objects, both separately and in groups, in consecutive frames under some kinds of difficulties such as rapid appearance changes caused by image noise and occlusion.	algorithm;cluster analysis;connected component (graph theory);fast fourier transform;hidden surface determination;image noise;kalman filter;low-pass filter;maxima;mean shift;minimum bounding box;streaming media;velocity (software development)	Ahmed Ali;Kenji Terada	2010	J. UCS	10.3217/jucs-016-06-0921	kalman filter;computer vision;simulation;mean-shift;fast kalman filter;computer science;moving horizon estimation	Vision	44.65903329583952	-48.27909977674647	22550
cb96847abc7ed371c6e55ddde528203a1de180a1	stabilized sparse online learning for sparse data		Stochastic gradient descent (SGD) is commonly used for optimization in large-scale machine learning problems. Langford et al. (2009) introduce a sparse online learning method to induce sparsity via truncated gradient. With high-dimensional sparse data, however, this method suffers from slow convergence and high variance due to heterogeneity in feature sparsity. To mitigate this issue, we introduce a stabilized truncated stochastic gradient descent algorithm. We employ a soft-thresholding scheme on the weight vector where the imposed shrinkage is adaptive to the amount of information available in each feature. The variability in the resulted sparse weight vector is further controlled by stability selection integrated with the informative truncation. To facilitate better convergence, we adopt an annealing strategy on the truncation rate, which leads to a balanced trade-off between exploration and exploitation in learning a sparse weight vector. Numerical experiments show that our algorithm compares favorably with the original truncated gradient SGD in terms of prediction accuracy, achieving both better sparsity and stability.	algorithm;experiment;information;machine learning;mathematical optimization;numerical method;simulated annealing;sparse matrix;spatial variability;stochastic gradient descent;thresholding (image processing);truncation	Yuting Ma;Tian Zheng	2017	Journal of Machine Learning Research		mathematical optimization;computer science;online machine learning;machine learning;pattern recognition;sparse approximation;statistics	ML	24.78330273678956	-35.344018327221	22567
3e9cbbd3fb814782269c52388b251ce329f1850c	robust people detection and tracking from an overhead time-of-flight camera		In this paper we describe a system for robust detection of people in a scene, by using an overhead Time of Flight (ToF) camera. The proposal addresses the problem of robust detection of people, by three means: a carefully designed algorithm to select regions of interest as candidates to belong to people; the generation of a robust feature vector that efficiently model the human upper body; and a people classification stage, to allow robust discrimination of people and other objects in the scene. The proposal also includes a particle filter tracker to allow people identification and tracking. Two classifiers are evaluated, based on Principal Component Analysis (PCA), and Support Vector Machines (SVM). The evaluation is carried out on a subset of a carefully designed dataset with a broad variety of conditions, providing results comparing the PCA and SVM approaches, and also the performance impact of the tracker, with satisfactory results.	algorithm;bit error rate;feature vector;overhead (computing);particle filter;pixel;principal component analysis;region of interest;scala;spatial variability;support vector machine;time-of-flight camera;velocity (software development)	Alvaro Fernandez-Rincon;David Fuentes-Jimenez;Cristina Losada-Gutierrez;Marta Marrón Romera;Carlos A. Luna;Javier Macías Guarasa;Manuel Mazo	2017		10.5220/0006169905560564	computer vision;tracking system	Vision	41.121779537091534	-48.324667956567424	22571
fb1627ed224bf7b1e3d80c097316ed7703951df2	deep transfer network for face recognition using 3d synthesized face		Face recognition has experienced a flurry of advances with deep learning. However, training a model requires a lot of data. In order to meet this condition, some researchers use the 3D rendering technique to synthesize fake face images to expand the training data. Experimental results have demonstrated that this method is an effective way. There exist, however, dataset bias between the real 2D real face images and 3D synthesized face images. In this paper, we use Deep Transfer Network(DTN) to reduce dataset bias. First, we utilize the 3DMM face model to synthesize face images with various poses and natural expression. We choose the Inception-Resnet-V1 as our benchmark model. Then, we optimize our DTN based on maximum mean discrepancy(MMD) of the shared feature extraction layers and the discrimination layers. Our experiments demonstrate that the model jointly trained using synthesized images and real images is more robust than using either dataset (2D real faces or 3D synthesized faces). Furthermore, the performance obtained by our approach is comparable to the-state-of-the-art results to the systems trained on millions of real images.	3d rendering;artificial neural network;benchmark (computing);convolutional neural network;deep learning;delay-tolerant networking;domain adaptation;experiment;facial recognition system;feature extraction;image;marginal model;outline of object recognition;transfer-based machine translation	Zhanfu An;Weihong Deng;Jiani Hu	2017	2017 IEEE Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2017.8305094	computer vision;artificial intelligence;3d rendering;deep learning;computer science;feature extraction;real image;facial recognition system;training set	Vision	25.928876519293894	-50.045601257401756	22610
e5bc699370a92ab64189bd1d0068675bb532f259	exploration with fastslam technique	occupancy grid;mobile robots;data association;accuracy;robot vision;estimation;position control;probability distribution;simultaneous localization and mapping;slam robots mobile robots position control robot vision;vehicle orientation fastslam technique multiple vehicles occupancy grids simultaneous localization and mapping visual sensors central agent vehicle position;vehicles;slam robots;vehicles simultaneous localization and mapping estimation probability distribution accuracy	In this work we investigate the exploration of an environment with multiple vehicles using a strategy based on occupancy grids and a technique of simultaneous localization and mapping (SLAM). The exploration strategy uses concepts of costs and utility from frontier cells. Besides, the used SLAM method is based on a FastSLAM algorithm with landmarks extracted from visual sensors and a common map of characteristics. Both activities are coordinated by a central agent. The results obtained by simulation show that when two vehicles can communicate with a central agent building a common map of characteristics, the exploration task becomes more efficient than that performed with only one vehicle. This occurs because the exploration time is reduced while the accuracy of vehicle position and orientation is maintained or even improved with the same number of particles. In this paper, we present new results in the data association of FastSLAM algorithm.	algorithm;correspondence problem;robot;scale-invariant feature transform;sensor;simulation;simultaneous localization and mapping	Adao de Melo Neto;Paulo Fernando Ferreira Rosa;Thiago Eustaquio Alves de Oliveira;Paulo Cesar Pellanda	2011	2011 9th IEEE International Conference on Control and Automation (ICCA)	10.1109/ICCA.2011.6137960	control engineering;probability distribution;mobile robot;computer vision;estimation;simulation;computer science;mathematics;accuracy and precision;statistics;simultaneous localization and mapping	Robotics	52.962601998987765	-34.10549442253654	22613
1565d0148b827534030db85cf71312bbb03679c3	safe human-robot-coexistence: emergency-stop using a high-speed vision-chip	cycle time;microcontrollers;human computer interaction;binary image;safety systems;emergency stop human robot coexistence vision chip safety system;chip;robot vision;industrial robots;occupational safety;humans cameras service robots robot vision systems safety pixel gravity robot kinematics computer vision production systems;collision avoidance industrial robots occupational safety robot vision digital signal processing chips microcontrollers safety systems tracking human computer interaction;digital signal processing chips;collision avoidance;8 bit human robot coexistence emergency stop industrial robots workspace human safety tracking vision chip pixel parallel masking summation operation binary images microcontroller safety system;high speed;tracking	The coexistence of humans and industrial robots in a common workspace provides the advantage of increased flexibility in production or longer system up-time during maintenance. However, it is fundamentally necessary to guarantee the safety of the human. This paper presents an approach that uses a specialized tracking-vision-chip to realize a high-speed emergency-stop for safe human-robot-coexistence. The presented approach uses the ability of the vision-chip to perform pixel-parallel masking and fast summation-operations on binary images to detect whether the robot and human are too close. After initial evaluation in a (semi)-simulation, the approach was realized in an experimental system. Even with only a small 8-bit microcontroller controlling the vision-chip and the communication, a cycle time of more than 500Hz was achieved.	8-bit;binary image;coexist (image);experimental system;humans;industrial robot;microcontroller;pixel;uptime;workspace	Dirk Ebert;Takashi Komuro;Akio Namiki;Masatoshi Ishikawa	2005	2005 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2005.1545242	chip;microcontroller;embedded system;computer vision;simulation;binary image;cycle time variation;computer science;engineering;occupational safety and health;tracking;system safety	Robotics	45.17403336221966	-35.395914844152124	22681
234b6858208d802dea6c48379ef425de693081a8	a fast dense stereo matching algorithm with an application to 3d occupancy mapping using quadrocopters	realtime safe navigation fast dense stereo matching algorithm 3d occupancy mapping quadrocopters stereo correspondences stereo block matching depth aware method sse instructions middlebury benchmark tsukuba stereo pair core i5 cpu 3d occupancy grid maps stereo images euroc robotic challenge;stereo image processing autonomous aerial vehicles helicopters image matching microprocessor chips robot vision;cameras three dimensional displays correlation robot vision systems benchmark testing buildings	We propose a fast algorithm for computing stereo correspondences and correcting the mismatches. The correspondences are computed using stereo block matching and refined with a depth-aware method. We compute 16 disparities at the same time using SSE instructions. We evaluated our method on the Middlebury benchmark and obtained promosing results for practical realtime applications. The use of SSE instructions allows us to reduce the time needed to process the Tsukuba stereo pair to 8 milliseconds (125 fps) on a Core i5 CPU with 2×3.3 GHz. Our disparity refinement method has corrected 40% of the wrong matches with an additional computational time of 5.2% (0.41ms). The algorithm has been used to build 3D occupancy grid maps from stereo images. We used the datasets provided by the EuRoC Robotic Challenge. The reconstruction was accurate enough to perform realtime safe navigation.	algorithm;benchmark (computing);binocular disparity;central processing unit;computation;computer stereo vision;fits;list of intel core i5 microprocessors;pattern matching;real-time clock;real-time computing;refinement (computing);streaming simd extensions;time complexity	Radouane Ait Jellal;Andreas Zell	2015	2015 International Conference on Advanced Robotics (ICAR)	10.1109/ICAR.2015.7251515	computer stereo vision;stereo cameras;computer vision;simulation;computer science;computer graphics (images)	Robotics	52.39404877667121	-45.714225806429035	22684
00bd11663bd85c9f1f104500bc9064ff76fd520e	small-sample classification of hyperspectral data in a graph-based semi-supervision framwork		Using spectra of hyperspectral remote sensing imagery to identify and classify land cover has been a hot topic thanks for its high resolution spectrum. However, when the quantity of labeled samples is too small, the classification accuracy of hyperspectral data will be reduced greatly. Most classification algorithms take dimensional reduction strategy and require plentiful labeled samples in order to learn the classifier that could then recognize a specific material. But in most remote sensing situations labeling samples is a costly task and the valued information would be lost with dimensional reduction. Sparse representation as a fast and effective algorithm has the advantages that it can perform quite well with small labeled samples without dimensional reduction. Therefore, we proposed a new framework to construct a graph-based semi-supervised model to solve paucity problem of labeled samples and combine the k nearest neighbor (knn) graph to take the advantage of space features. In this new model, sparse representation is used to build the probability matrix by estimating if a pairwise pixels belonging to the same class, and this probability matrix is integrated into ℓ1norm graph to form a more discriminating graph called dℓ1graph. Then we combine this dℓ1graph with knn graph in proportion. The new graph can employ both spectral values and space information of hyperspectral data. We demonstrate the effectiveness of our proposal on the Indiana Pines hyperspectral data set and the results outperform state of the art.	k-nearest neighbors algorithm;pixel;reduction strategy (lambda calculus);semi-supervised learning;semiconductor industry;sparse approximation;sparse matrix;stochastic matrix	Chunmei Zhang;Junyan Wang;Yunbin Zhang;Yaoyao Liu	2017	2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2017.8127676	computer vision;pixel;dimensional reduction;k-nearest neighbors algorithm;pairwise comparison;statistical classification;computer science;sparse approximation;stochastic matrix;hyperspectral imaging;artificial intelligence;pattern recognition	ML	29.758555186864466	-43.99261299577504	22850
95f18347515062598bd0d827ef31cb0f7950e9f4	saliency-based video segmentation with graph cuts and sequentially updated priors	graph theory;image segmentation;video signal processing;kalman filter video segmentation saliency markov random fields map estimation graph cuts;saliency;automatic segmentation;kalman filters;sequential updated prior;markov random fields;kalman filter;video segmentation;maximum likelihood estimation;maximum a posteriori estimation;markov random field;visualization;graph cut;pixel;random processes;map estimation;humans;markov processes;image segmentation humans markov random fields biological system modeling hidden markov models testing random variables laboratories systems engineering and theory educational institutions;visual attention;video signal processing graph theory image segmentation kalman filters markov processes maximum likelihood estimation random processes;graph cuts;saliency based visual attention;kalman filter saliency based video segmentation graph cut sequential updated prior maximum a posteriori estimation markov random field saliency based visual attention;saliency based video segmentation	This paper proposes a new method for achieving precise video segmentation without any supervision or interaction. The main contributions of this report include 1) the introduction of fully automatic segmentation based on the maximum a posteriori (MAP) estimation of the Markov random field (MRF) with graph cuts and saliencydriven priors and 2) the updating of priors and feature likelihoods by integrating the previous segmentation results and the currently estimated saliency-based visual attention. Test results indicate that our new method precisely extracts probable regions from videos without any supervised interactions.	cut (graph theory);interaction;markov chain;markov random field;supervised learning	Ken Fukuchi;Kouji Miyazato;Akisato Kimura;Shigeru Takagi;Junji Yamato	2009	2009 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2009.5202577	kalman filter;computer vision;cut;computer science;graph theory;machine learning;pattern recognition;mathematics;scale-space segmentation;statistics	Vision	45.060059797720335	-49.41415877852779	22861
55426c7e2ac2ff784e056ba5d589a8dfd1c48fde	boostmotion: boosting a discriminative similarity function for motion estimation	stress;gaussian noise;speckle;heart;motion estimation;testing;left ventricle;temporal information;boosting;boosting motion estimation stress motion measurement gaussian noise testing heart speckle data systems automation;selection combining;data systems;motion measurement;similarity function;automation	Motion estimation for applications where appearance undergoes complex changes is challenging due to lack of an appropriate similarity function. In this paper, we propose to learn a discriminative similarity function based on an annotated database that exemplifies the appearance variations. We invoke the LogitBoost algorithm to selectively combine weak learners into one strong similarity function. The weak learners based on local rectangle features are constructed as nonparametric 2D piecewise constant functions, using the feature responses from both images, to strengthen the modeling power and accommodate fast evaluation. Because the negatives possess a location parameter measuring their closeness to the positives, we present a locationsensitive cascade training procedure, which bootstraps negatives for later stages of the cascade from the regions closer to the positives. This allows viewing a large number of negatives and steering the training process to yield lower training and test errors. In experiments of estimating the motion for the endocardial wall of the left ventricle in echocardiography, we compare the learned similarity function with conventional ones and obtain improved performances. We also contrast the proposed method with a learning-based detection algorithm to demonstrate the importance of temporal information in motion estimation. Finally, we insert the learned similarity function into a simple contour tracking algorithm and find that it reduces drifting.	algorithm;centrality;computer-aided design;discriminative model;experiment;logitboost;motion estimation;performance;similarity measure;smoothing;spatial analysis	Shaohua Kevin Zhou;Bogdan Georgescu;Dorin Comaniciu;Jie Shao	2006	2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)	10.1109/CVPR.2006.73	speckle pattern;gaussian noise;computer vision;computer science;automation;machine learning;pattern recognition;motion estimation;mathematics;software testing;stress;data system;boosting;heart;statistics	Vision	33.8372058589573	-47.11592631715619	22872
df2d9cc6dfaa59e3e0bb614d8ee5a5451a8dbb37	bayesian warped gaussian processes		Warped Gaussian processes (WGP) [1] model output observations in regression tasks as a parametric nonlinear transformation of a Gaussian process (GP). The use of this nonlinear transformation, which is included as part of the probabilistic model, was shown to enhance performance by providing a better prior model on several data sets. In order to learn its parameters, maximum likelihood was used. In this work we show that it is possible to use a non-parametric nonlinear transformation in WGP and variationally integrate it out. The resulting Bayesian WGP is then able to work in scenarios in which the maximum likelihood WGP failed: Low data regime, data with censored values, classification, etc. We demonstrate the superior performance of Bayesian warped GPs on several real data sets.	bayesian network;gaussian blur;gaussian process;international conference on functional programming;nonlinear system;statistical model	Miguel Lázaro-Gredilla	2012			econometrics;machine learning;mathematics;statistics	ML	27.855915371165644	-29.64544753249007	22883
3085eff5d447f786a45674dfec00bc67a24054f5	unlimited road-scene synthetic annotation (ursa) dataset		In training deep neural networks for semantic segmentation, the main limiting factor is the low amount of ground truth annotation data that is available in currently existing datasets. The limited availability of such data is due to the time cost and human effort required to accurately and consistently label real images on a pixel level. Modern sandbox video game engines provide open world environments where traffic and pedestrians behave in a pseudo-realistic manner. This caters well to the collection of a believable road-scene dataset. Utilizing open-source tools and resources found in single-player modding communities, we provide a method for persistent, ground truth, asset annotation of a game world. By collecting a synthetic dataset containing upwards of 1, 000, 000 images, we demonstrate realtime, on-demand, ground truth data annotation capability of our method. Supplementing this synthetic data to Cityscapes dataset, we show that our data generation method provides qualitative as well as quantitative improvements—for training networks—over previous methods that use video games as surrogate.		Matt Angus;Mohamed ElBalkini;Samin Khan;Ali Harakeh;Oles Andrienko;Cody Reading;Steven Lake Waslander;Krzysztof Czarnecki	2018	2018 21st International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2018.8569519	machine learning;artificial neural network;artificial intelligence;test data generation;real image;computer science;ground truth;sandbox (computer security);synthetic data;segmentation;annotation	Vision	27.421857132794923	-49.92461530451502	22905
9068493ad59e2cc9594d6c5189e937384a9754c8	general highlight detection in sport videos	sports video analysis;attention computation;auto regressive;highlight detection;sports video;temporal resolution;multi resolution	Attention is a psychological measurement of human reflection against stimulus. We propose a general framework of highlight detection by comparing attention intensity during the watching of sports videos. Three steps are involved: adaptive selection on salient features, unified attention estimation and highlight identification. Adaptive selection computes feature correlation to decide an optimal set of salient features. Unified estimation combines these features by the technique of multi-resolution auto-regressive (MAR) and thus creates a temporal curve of attention intensity. We rank the intensity of attention to discriminate boundaries of highlights. Such a framework alleviates semantic uncertainty around sport highlights and leads to an efficient and effective highlight detection. The advantages are as follows: (1) the capability of using data at coarse temporal resolutions; (2) the robustness against noise caused by modality asynchronism, perception uncertainty and feature mismatch; (3) the employment of Markovian constrains on content presentation, and (4) multi-resolution estimation on attention intensity, which enables the precise allocation of event boundaries.	autoregressive model;extensibility;feature vector;modality (human–computer interaction);sampling (signal processing);simulation;tree accumulation;video content analysis	Reede Ren;Joemon M. Jose	2009		10.1007/978-3-540-92892-8_5	computer vision;simulation;temporal resolution;multimedia;autoregressive model;statistics	Vision	38.23695690373954	-44.7678829591852	22906
b20bde26d83bdd13a2b9a29309c939f9b637f6e1	tips and tricks for building bayesian networks for scoring game-based assessments	prior information;weight of evidence;classification consistency;evidence centered assessment design;bayesian networks	Game-based assessments produce multiple, dependent observations from student game play. Bayesian networks can model the dependence, but, typically, only a small amount of pilot data are available at the time the network is constructed. This paper examines the process of creating Bayesian network scoring models, focusing on several practical techniques that have been used in the construction of models for Physics Playground. In particular, the following techniques are helpful: 1i¾źThe use of evidence-centered assessment design to define latent competency variables and observable indicator variables. 2i¾źThe use of correlation matrixes to uncover and validate the conditional independence structure of the Bayes net. 3i¾źThe use of discrete IRT models to create large portion of the Bayesian networks from a single spreadsheet. 4i¾źAdjusting the Bayes net parameters using both hand tuning and a generalized EM algorithm, creating networks which are a mixture of expert opinion and data. 5i¾źUsing expected classification accuracy matrixes to judge assessment validity and reliability. 6i¾źUsing evidence balance sheets to identify unusual subjects and observable indicators.	bayesian network	Russell G. Almond	2015		10.1007/978-3-319-28379-1_18	variable-order bayesian network;bayesian programming;computer science;machine learning;data mining;bayesian statistics;statistics	AI	24.655297084907712	-25.059555005481574	22907
e6e6e005f87a2c6abf3014fb8fab0741f629d2d7	stairways detection based on approach evaluation and vertical vanishing point		Stair region detection and distance estimation from a stair image are challenging activities to support visually impaired navigation safely in unknown environments. In this paper, a framework is proposed for detecting stair region from a stair image utilising some natural and unique property of a stair. One unique property of them is, every stair stepu0027s beginning and ending horizontal edge point intersects with two vertical edge points creating three connected point (TCP). The TCPs are used to validate the stair edge segments and calculate the vertical vanishing point to justify the stair edges. This justification ensures that validated edge segments are arranged in an increasing parallel order which is the other unique property of a stair. These increasing edge segments are verified by utilising the y coordinate value of the vanishing point and the detection of stair candidate region is confirmed by these properties. In addition, the triangular similarity is used for distance estimation from camera to st...	vanishing point	Md. Khaliluzzaman;Kaushik Deb	2018	IJCVR	10.1504/IJCVR.2018.10013163	computer vision;artificial intelligence;gabor filter;computer science;y-coordinate;vanishing point	Vision	50.50821963444571	-38.65570548303483	22921
b2e86467f5572cee454b37fe3b63d61dce8681db	sure-tuned tapering estimation of large covariance matrices	operator norms;sure;cross validation;frobenius norm;tapering estimator;covariance matrix	Bandable covariance matrices are often used to model the dependence structure of variables that follow a nature order. It has been shown that the tapering covariance estimator attains the optimal minimax rates of convergence for estimating large bandable covariance matrices. The estimation risk critically depends on the choice of the tapering parameter.We develop a Stein’s Unbiased Risk Estimation (SURE) theory for estimating the Frobenius risk of the tapering estimator. SURE tuning selects the minimizer of SURE curve as the chosen tapering parameter. An extensiveMonte Carlo study shows that SURE tuning is often comparable to the oracle tuning and outperforms cross-validation. We further illustrate SURE tuning using rock sonar spectrum data. The real data analysis results are consistent with simulation findings. © 2012 Elsevier B.V. All rights reserved.	cross-validation (statistics);minimax;oracle nosql db;performance tuning;sonar (symantec);simulation	Feng Yi;Hui Zou	2013	Computational Statistics & Data Analysis	10.1016/j.csda.2012.09.007	estimation of covariance matrices;econometrics;covariance matrix;mathematical optimization;matrix norm;mathematics;cross-validation;statistics	ML	31.08654652026985	-24.192081935270917	22930
4b5457d4db15a9b4753338b709fdd5a70d17ad1c	convex coordinates based on lattice independent sets as pattern features	image recognition;image segmentation;independent component analysis convex coordinate lattice independent pattern set linear pattern feature extraction algorithm lattice autoassociative morphological memory image denoising pattern recognition content based image retrieval hyperspectral image unsupervised segmentation erosive noise dilative noise pattern classification mushroom shape database;independent set;lattices image retrieval feature extraction associative memory image denoising pattern recognition hyperspectral sensors hyperspectral imaging image segmentation content based retrieval;visual databases content addressable storage content based retrieval feature extraction image classification image denoising image recognition image retrieval image segmentation independent component analysis spectral analysis;dilative noise;linear pattern feature extraction algorithm;image classification;unsupervised segmentation;independent component analysis;hyperspectral image unsupervised segmentation;erosive noise;feature extraction;pattern classification;pattern recognition;associative memory;lattice autoassociative morphological memory;image denoising;spectral analysis;lattice independent pattern set;content based image retrieval;content addressable storage;mushroom shape database;hyperspectral image;convex coordinate;content based retrieval;visual databases;image retrieval	Lattice associative memories have been proposed for image denoising and pattern recognition. We have shown that they can be applied to other domains, like image retrieval and hyperspectral image unsupervised segmentation. In both cases the key idea is that autoassociative morphological memories selective sensitivity to erosive and dilative noise can be applied to detect the lattice independence between patterns. The convex coordinates obtained by linear unmixing based on the sets of lattice independent patterns define a feature extraction process. These features may be useful either for content based image retrieval (CBIR) as well as for pattern classification. We present some CBIR and recognition results on a mushroom shape database, including the comparison with other linear feature extraction algorithms (ICA and CCA).	algorithm;computation;content-based image retrieval;convex function;convex set;data point;feature extraction;independent computing architecture;noise reduction;pattern recognition;smart meter;vertex (geometry)	Manuel Graña;Ramón Moreno;F. Xabier Albizuri	2006	2006 IEEE International Conference on Fuzzy Systems	10.1109/FUZZY.2006.1681718	independent component analysis;computer vision;contextual image classification;independent set;feature extraction;image retrieval;computer science;machine learning;pattern recognition;image segmentation	Vision	28.8270756140468	-42.191303120745495	22958
3193f96f163928e5d694585d61de9161aeaadfee	visual servoing from uncalibrated cameras for uncalibrated robots	epipolar geometry;visual servoing		robot;visual servoing	Takeharu Sato;Jun Sato	2000	Systems and Computers in Japan	10.1002/1520-684X(200012)31:14%3C11::AID-SCJ2%3E3.0.CO;2-M	computer vision;simulation;computer science;visual servoing;epipolar geometry;computer graphics (images)	Robotics	52.216534464033295	-42.24308309766775	23013
014c046edcecf6b7f44344cb67c9aa63ee2327c6	multiple eigenspaces	appearance-based object representation;image grouping;multiple eigenspaces;view-based navigation map;dimensionality reduction;visual learning;appearance-based object recognition;principal component analysis (pca)	In this paper, we propose a novel self-organizing framework to construct multiple, low-dimensional eigenspaces from a set of training images. Grouping of images is systematically and robustly performed via eigenspace-growing in terms of low-dimensional eigenspaces. To further increase the robustness, the eigenspace-growing is initiated independently with many small groups of images—seeds. All these grown eigenspaces are treated as hypotheses that are subject to a selection procedure eigenspace-selection, based on the MDL principle, which selects the 8nal resulting set of eigenspaces as an e9cient representation of the training set, taking into account the number of images encompassed by the eigenspaces, the dimensions of the eigenspaces, and their corresponding residual errors. We have tested the proposed method on a number of standard image sets, and the signi8cance of the approach with respect to the recognition rate has been demonstrated. ? 2002 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.	algorithm;computer vision;hoc (programming language);horst rittel;mdl (programming language);organizing (structure);pattern recognition;seeds (cellular automaton);self-organization;test set	Ale&#x0161; Leonardis;Horst Bischof;Jasna Maver	2002	Pattern Recognition	10.1016/S0031-3203(01)00198-4		Vision	41.760388805110885	-50.656473046564955	23028
9208ffe2b09d8fd631ed5aaf1f5fec26fb1b89ee	multisensor object identification on airports using autoassociative neural networks	sensors;airports;training;institut fur flugfuhrung;autoassociative neural network architecture multisensor object identification neural identification method traffic participants airports dissimilar sensors information fusion object feature sets neural network based identifier rbf networks;radial basis function networks;traffic engineering computing airports object detection radial basis function networks sensor fusion;global positioning system;sensors airports radial basis function networks global positioning system training radar;radar	This paper presents a neural identification method of traffic participants at airports. The traffic objects are observed by several dissimilar sensors. Their information is fused to possible object feature sets. These feature sets are processed by a neural network based identifier, so that traffic participants are identified on the basis of very few observations. The neural network identifier is composed of several RBF networks including networks with autoassociative network architecture.	artificial neural network;identifier;network architecture;radial basis function network;sensor	Karin Haese	1998	9th European Signal Processing Conference (EUSIPCO 1998)		engineering;artificial intelligence;machine learning;data mining	ML	47.927394262541895	-36.19025217576443	23030
391b311ccb6ab57076838b9f7f0da463aa447731	hmm-based characterization of channel behavior for networked control systems	temporal correlation;learning algorithm;hidden markov model;behavior modeling;model complexity;network control;a priori information;quality criteria;networked control system;majority voting;communication channels	We study the problem of characterizing the behavior of lossy and data corrupting communication channels in a networked control setting, where the channel's behavior exhibits temporal correlation. We propose a behavior characterization mechanism based on a hidden Markov model (HMM). The use of a HMM in this regard presents multiple challenges including dealing with incomplete observation sequences (due to data losses and corruptions) and the lack of a priori information about the model complexity (number of states in the model). We address the first challenges by using the plant state information and history of received/applied control inputs to fill in the gaps in the observation sequences, and by enhancing the HMM learning algorithm to deal with missing observations. Further, we adopt two model quality criteria for determining behavior model complexity. The contributions of this paper include: (1) an enhanced learning algorithm for refining the HMM model parameters to handle missing observations, and (2) simultaneous use of two well-defined model quality criteria to determine the model complexity. Simulation results demonstrate over 90% accuracy in predicting the output of a channel at a given time step, when compared to a traditional HMM based model that requires complete knowledge of the model complexity and observation sequence.	algorithm;behavior model;control system;hidden markov model;lossy compression;markov chain;simulation	Jian Chang;Krishna K. Venkatasubramanian;Chinwendu Enyioha;Shreyas Sundaram;George J. Pappas;Insup Lee	2012		10.1145/2185505.2185508	simulation;computer science;machine learning;data mining	ML	35.14195642461652	-27.467521796322306	23097
2e977ffcdd21a9e96444cdb76c0c08336e2e6a1c	coreset-based adaptive tracking		We propose a method for learning from streaming visual data using a compact, constant size representation of all the data that was seen until a given moment. Specifically, we construct a “coreset” representation of streaming data using a parallelized algorithm, which is an approximation of a set with relation to the squared distances between this set and all other points in its ambient space. We learn an adaptive object appearance model from the coreset tree in constant time and logarithmic space and use it for object tracking by detection. Our method obtains excellent results for object tracking on three standard datasets over more than 100 videos. The ability to summarize data efficiently makes our method ideally suited for tracking in long videos in presence of space and time constraints. We demonstrate this ability by outperforming a variety of algorithms on the TLD dataset with 2685 frames on average. This coreset based learning approach can be applied for both real-time learning of small, varied data and fast learning of big data.	algorithm;approximation;big data;coreset;l (complexity);parallel computing;real-time locating system;streaming media;time complexity	Abhimanyu Dubey;Nikhil Naik;Dan Raviv;Rahul Sukthankar;Ramesh Raskar	2015	CoRR		computer vision;theoretical computer science;machine learning;mathematics	Vision	33.052114784346394	-34.82698160956054	23113
11f32fed609fdbbf5f590614651fd981f21f5b24	on the regularization parameter selection for sparse code learning in electrical source separation	ciencia;projetos;investigacao;publicacoes;iscte iul	Source separation of whole-home electrical consumption also known as energy disaggregation plays a crucial role in energy savings and sustainable development. One important approach towards accurate energy disaggregation is based on sparse code learning. The sparsity-based source separation algorithms allow to build models that explicitly generalize across multiple different devices of the same category. While this method has recently been investigated, yet the importance of the degree of sparseness given by the regularization parameter is rarely considered. In this paper we aim at investigating the performance of learning representations from the aggregated electrical load signal with sparse models for energy disaggregation. In particular we focus our study on the influence of the regularization parameter in the overall approach. The computational experiments yielded in real data from home electrical energy consumption show that for several degrees of sparseness a reliable scheme for energy disaggregation can be obtained with statistical significance.	algorithm;electrical load;experiment;neural coding;source separation;sparse matrix	Marisa B. Figueiredo;Bernardete Ribeiro;Ana de Almeida	2013		10.1007/978-3-642-37213-1_29	econometrics;mathematical optimization;machine learning;mathematics;statistics	AI	30.558536579214735	-34.61548151499159	23132
1ce4a2f07186c96b68a73f2ee8559a8e5a8e6d81	sensing planning to optimize work object location measurements in intelligent robotics	spatial uncertainties sensing planning pose estimation;intelligent robots;path planning;computational geometry;spatial uncertainty sensing planning work object location measurements intelligent robotics geometrical representation covariance matrix pose estimation;optimal control;robot vision;covariance matrices;feature extraction;computational geometry intelligent robots optimal control path planning robot vision object detection parameter estimation feature extraction covariance matrices;parameter estimation;object detection;covariance matrix;intelligent robots robot sensing systems jacobian matrices robot kinematics covariance matrix sensor systems optical sensors optical distortion parameter estimation modeling;pose estimation	This paper presents a method for planning the sensing features when the geometrical representation of the target object is known. The presented method is a synthesis -form and can be used in several measurement applications in robotics. Sensing planning is an important issue when the measurement data is sparse, includes a lot of noise or there are tight time-requirements. The criteria for selecting the measurement locations and orientations is a posteriori error covariance matrix of the parameters to be estimated. The presented approach is verified by simulation tests in the case of work object location.	orientation (graph theory);requirement;robotics;simulation;sparse matrix	Mikko Sallinen;Tapio Heikkilä	2005	2005 International Symposium on Computational Intelligence in Robotics and Automation	10.1109/CIRA.2005.1554281	computer vision;covariance matrix;pose;optimal control;3d pose estimation;feature extraction;computational geometry;computer science;machine learning;motion planning;estimation theory	Robotics	52.20554128794531	-35.169737390337524	23150
27836b126b352a160d3bb3d8d8eb9fb9edd2ab0e	recognition combined human pose tracking using single depth images		This paper presents a method for tracking human poses in real-time from depth image sequences. The key idea is to adopt recognition for generating the model to be tracked. In contrast to traditional methods utilizing a single-typed 3D body model, we directly define the human body model based on the body part recognition result of the captured depth image, which leads to the reliable tracking regardless of users’ appearances. Moreover, the proposed method has the ability to efficiently reduce the tracking drift by exploiting the joint information inserted into our body model. Experimental results on real-world environments show that the proposed method is effective for estimating various human poses in real-time.	real-time clock	Wonjun Kim;ByungIn Yoo;Jae-Joon Han;Changkyu Choi	2014		10.1117/12.2037644	computer vision;simulation;multimedia	AI	44.0520142276203	-46.74608163668574	23162
bff5d681d469d2348364cb3aa5507a8bf328d275	market basket analysis from egocentric videos		This paper presents Visual Market Basket Analysis (VMBA), a novel application domain for egocentric vision systems. The final goal of VMBA is to infer the behavior of the customers of a store during their shopping. The analysis relies on image sequences acquired by cameras mounted on shopping carts. The inferred behaviors can be coupled with classic Market Basket Analysis information (i.e., receipts) to help retailers to improve the management of spaces and marketing strategies. To set up the challenge, we collected a new dataset of egocentric videos during real shopping sessions in a retail store. Video frames have been labeled according to a proposed hierarchy of 14 different customer behaviors from the beginning (cart picking) to the end (cart releasing) of their shopping. We benchmark different representation and classification techniques and propose a multi-modal method which exploits visual, motion and audio descriptors to perform classification with the Directed Acyclic Graph SVM learning architecture. Experiments highlight that employing multimodal representations and explicitly addressing the task in a hierarchical way is beneficial. The devised approach based on Deep Features achieves an accuracy of more than 87% over the 14 classes of the considered dataset. © 2018 Elsevier B.V. All rights reserved.	affinity analysis;application domain;asch conformity experiments;benchmark (computing);directed acyclic graph;frame (video);modal logic;multimodal interaction	Vito Santarcangelo;Giovanni Maria Farinella;Antonino Furnari;Sebastiano Battiato	2018	Pattern Recognition Letters	10.1016/j.patrec.2018.06.010	architecture;artificial intelligence;support vector machine;mathematics;application domain;directed acyclic graph;affinity analysis;pattern recognition;hierarchy;exploit;cart	AI	33.430545316606874	-49.25429030834995	23175
d6134a877cb71234704ad4c967b8cb754b23b3e0	class-oriented spectral partitioning for hyperspectral image classification	support vector machines;training;nw indiana class oriented spectral partitioning hyperspectral image classification multiple classifier system airborne visible infrared imaging spectrometer spectral partitioning method;hyperspectral imaging support vector machines signal to noise ratio training partitioning algorithms accuracy;accuracy;infrared imaging airborne radar geophysical techniques hyperspectral imaging;hyperspectral imaging;signal to noise ratio;partitioning algorithms	This paper presents a new approach for class-oriented spectral partitioning for hyperspectral image classification. First, without empirical information, we automatically search the spectral bands that correspond to a specific class by using different band selection approaches. Then, the obtained class-oriented spectral partitions are used respectively as the input of a group of classifiers, the results of which are combined together to generate a final one by a multiple classifier system. Our experimental results, conducted with the well-known Indians Pines test site hyperspectral image collected by the Airborne Visible Infra-Red Imaging Spectrometer (AVIRIS) in NW Indiana, suggest that our presented spectral partitioning method leads to competitive results when compared with other state-of-the-art approaches.	binary space partitioning;computer vision;learning classifier system;netware;whole earth 'lectronic link	Yi Liu;Jun Li;Antonio J. Plaza;Kun Tan	2015	2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2015.7326951	full spectral imaging;support vector machine;computer vision;computer science;hyperspectral imaging;accuracy and precision;signal-to-noise ratio;remote sensing	Vision	30.6723392656654	-43.83728483569589	23193
17107e72e68694ed1c960437d5b19e48dfd04452	a nearly-linear time framework for graph-structured sparsity		We introduce a framework for sparsity structures defined via graphs. Our approach is flexible and generalizes several previously studied sparsity models. Moreover, we provide efficient projection algorithms for our sparsity model that run in nearly-linear time. In the context of sparse recovery, our framework achieves an informationtheoretically optimal sample complexity for a wide range of parameters. We complement our theoretical analysis with experiments showing that our algorithms also improve on prior work in practice.	algorithm;compressed sensing;experiment;graph (discrete mathematics);sample complexity;sparse matrix;time complexity	Chinmay Hegde;Piotr Indyk;Ludwig Schmidt	2015			mathematical optimization;computer science;machine learning;data mining	ML	25.93427775840436	-33.07927729477859	23244
b5a081692b4e96a7bcf3018418ceebe90f9e4eb7	nonlinear causal discovery for high dimensional data: a kernelized trace method	eigenvalues and eigenfunctions;kernel;electronic mail;linear trace method;hilbert spaces;kernel methods;hilbert spaces data handling;linear trace method nonlinear causal discovery high dimensional data kernel methods;abt scholkopf;nonlinear causal discovery;real world data experiments nonlinear causal discovery high dimensional data kernelized trace method linear trace method linearly coupled high dimensional observations high dimensional reproducing kernel hilbert space anticausal direction rkhs nonlinearly coupled causal pairs synthetic data;hilbert space;accuracy;kernel eigenvalues and eigenfunctions hilbert space accuracy covariance matrices electronic mail meteorology;covariance matrices;high dimensional data;data handling;meteorology	Causal discovery for high-dimensional observations is a useful tool in many fields such as climate analysis and financial market analysis. A linear Trace method has been proposed to identify the causal direction between two linearly coupled high-dimensional observations X and Y. However, in reality, the relations between X and Y are usually nonlinear and consequently the linear Trace method may fail. In this paper, we propose a method to infer the nonlinear causal relations for two high-dimensional observations X and Y. The idea is to map the observations to high dimensional Reproducing Kernel Hilbert Space (RKHS) such that the nonlinear relations become simple linear ones. We show that the linear Trace condition holds for the causal direction but it is violated for the anti-causal direction in RKHS. Based on this theoretical result, we develop a simple algorithm to infer the causal direction for nonlinearly coupled causal pairs. Synthetic data and real world data experiments are conducted to show the effectiveness of our proposed method.	algorithm;causal filter;experiment;hilbert space;kernel method;newton's method;nonlinear system;synthetic data;synthetic intelligence	Zhitang Chen;Kun Zhang;Lai-Wan Chan	2013	2013 IEEE 13th International Conference on Data Mining	10.1109/ICDM.2013.103	mathematical optimization;mathematical analysis;discrete mathematics;computer science;machine learning;mathematics;statistics;hilbert space	DB	26.800161240291786	-37.53374204117545	23247
8d037881cb7a774c84f34a498edc855e60a8fbb0	obstacle detection by stereo vision, introducing the pq method	obstacle detection;autonomous vehicle;search space;autonomous mobile robot;computer vision;computational complexity;stereo vision;3d reconstruction	Safe, robust operation of an autonomous vehicle in cross-country environments relies on sensing of the surroundings. Thanks to the reduced cost of vision hardware, and increasing computational power, computer vision has become an attractive alternative for this task. This paper concentrates on the use of stereo vision for obstacles detection in cross-country environments where the ground surface can not be modeled as ramps, i.e. linear patches. Given a 3D reconstruction of the surrounding environment, obstacles are detected using the concept of compatible points. The concept classify points as obstacles if they fall within the volume of cone located with its apex at the point being evaluated. The cone may be adjusted adjusted according the physical parameters of the vehicle. The paper introduces a novel Projection and Quantification method that based on vehicle orientation rotates the 3D information onto an intuitive two dimensional surface plot and quantifies the information into bins adjusted to the specific application. In this way the search space for compatible points is significantly reduced. The new method is evaluated for a specific robotic application and the results are compared to previous results on a number of typical scenarios. Combined with an intuitive representation of obstacles in a two dimensional surface plot, the results indicate a significant reduction in the computational complexity for relevant scenarios.	3d reconstruction;algorithm;apex (geometry);autonomous robot;computation;computational complexity theory;computer vision;reduced cost;stereopsis	H. J. Andersen;K. Kirk;T. L. Dideriksen;C. Madsen;M. B. Holte	2005			3d reconstruction;stereo cameras;computer vision;simulation;computer science;stereopsis;computational complexity theory	Vision	51.88691955146768	-40.1526971668584	23275
86762f78c717c648a064c1aafe0d1978ccce9c0f	an instance-specific parameter tuning approach using fuzzy logic for a post-processing topological map-matching algorithm		Map Matching Algorithms (MMAs) are developed to solve spatial ambiguities that arise in the process of assigning GPS measurements onto a digital roadway network. Scarce systematic parameter tuning approaches exist in the literature for optimizing MMA performance. Thus, a novel framework is proposed for a systematic calibration of the parameters of a post-processing MMA. The calibration approach consists of an Instance-specific Parameter Tuning Strategy (IPTS) that employs Fuzzy Logic principles. The proposed fuzzy IPTS tool determines algorithm-specific parameter values based on instance-specific information a priori to the execution of the MMA. Finally, the proposed IPTS tool is able to adjust to two particular decision maker preferences on algorithm performance, namely solution quality and computational time.	algorithm;computation;fuzzy logic;global positioning system;map matching;time complexity;video post-processing	Carola A. Blazquez;Jana Ries;Pablo A. Miranda;Deepak Soni	2018	IEEE Intelligent Transportation Systems Magazine	10.1109/MITS.2018.2867527	fuzzy logic;simulation;mathematical optimization;engineering;map matching;topological map;blossom algorithm	EDA	52.547222231957896	-27.23512307769679	23328
96a9ca7a8366ae0efe6b58a515d15b44776faf6e	grid loss: detecting occluded faces		Detection of partially occluded objects is a challenging computer vision problem. Standard Convolutional Neural Network (CNN) detectors fail if parts of the detection window are occluded, since not every sub-part of the window is discriminative on its own. To address this issue, we propose a novel loss layer for CNNs, named grid loss, which minimizes the error rate on sub-blocks of a convolution layer independently rather than over the whole feature map. This results in parts being more discriminative on their own, enabling the detector to recover if the detection window is partially occluded. By mapping our loss layer back to a regular fully connected layer, no additional computational cost is incurred at runtime compared to standard CNNs. We demonstrate our method for face detection on several public face detection benchmarks and show that our method outperforms regular CNNs, is suitable for realtime applications and achieves state-of-the-art performance.	algorithmic efficiency;apple maps;computer vision;convolution;convolutional neural network;desktop computer;face detection;floating point systems;fundamental fysiks group;overfitting;overhead (computing);real-time computing;run time (program lifecycle phase);sensor;speedup	Michael Opitz;Georg Waltner;Georg Poier;Horst Possegger;Horst Bischof	2016		10.1007/978-3-319-46487-9_24	computer vision;speech recognition;computer science;machine learning	Vision	29.615937805942615	-51.726841361853964	23340
1072cda0a77f80e691bbbb7f0035627c196ed0ee	estimation of graphical models through structured norm minimization		Estimation of Markov Random Field and covariance models from high-dimensional data represents a canonical problem that has received a lot of attention in the literature. A key assumption, widely employed, is that of sparsity of the underlying model. In this paper, we study the problem of estimating such models exhibiting a more intricate structure comprising simultaneously of sparse, structured sparse and dense components. Such structures naturally arise in several scientific fields, including molecular biology, finance and political science. We introduce a general framework based on a novel structured norm that enables us to estimate such complex structures from high-dimensional data. The resulting optimization problem is convex and we introduce a linearized multi-block alternating direction method of multipliers (ADMM) algorithm to solve it efficiently. We illustrate the superior performance of the proposed framework on a number of synthetic data sets generated from both random and structured networks. Further, we apply the method to a number of real data sets and discuss the results.	algorithm;augmented lagrangian method;graphical model;markov chain;markov random field;mathematical optimization;optimization problem;sparse matrix;synthetic data	Davoud Ataee Tarzanagh;George Michailidis	2017	Journal of Machine Learning Research		mathematical optimization;machine learning;mathematics;algorithm;statistics	ML	27.688848265678093	-33.07802923025108	23342
04b402215e377542d489c043c0f4c1b006b83aae	robust nonnegative garrote variable selection in linear regression	multiple linear regression;nonnegative garrote;variable selection;s estimation;mm estimation	Robust selection of variables in a linear regression model is investigated. Many variable selection methods are available, but very few methods are designed to avoid sensitivity to vertical outliers aswell as to leverage points. The nonnegative garrotemethod is a powerful variable selection method, developed originally for linear regression but recently successfully extended to more complex regression models. The method has good performances and its theoretical properties have been established. The aim is to robustify the nonnegative garrote method for linear regression as to make it robust to vertical outliers and leverage points. Several approaches are discussed, and recommendations towards a final good performing robust nonnegative garrote method are given. The proposed method is evaluated via a simulation study that also includes a comparison with existing methods. The method performs very well, and often outperforms existing methods. A real data application illustrates the use of the method in practice. © 2014 Elsevier B.V. All rights reserved.	feature selection;performance;robustification;selection (genetic algorithm);simulation;twelve leverage points	Irène Gijbels;Inge Vrinssen	2015	Computational Statistics & Data Analysis	10.1016/j.csda.2014.11.009	econometrics;mathematical optimization;linear regression;machine learning;mathematics;feature selection;statistics	AI	27.606702394866012	-24.992587685773366	23375
36ce7e6224d16decc3627f3da76ee0d166fae9ae	multi-layer joint gait-pose manifold for human motion modeling	topology;manifolds;legged locomotion;training;motion estimation;joints;topology aware local learning multilayer joint gait pose manifold human motion modeling gait kinematics human motion estimation multilayer topology motion synthesis multilayer jgpm training data diversification;training data;computational modeling;gait analysis;manifolds topology training training data computational modeling legged locomotion joints;motion estimation gait analysis	We present a multi-layer joint gait-pose manifold (multi-layer JGPM) for human motion modeling to enhance the representative capability of the original JGPM that represents gait kinematics by two variables. One is the pose to denote a series of stages in a walking cycle and the other is the gait to reflect the individual walking styles. Coupling pose and gait variables in the same latent space was shown effective for human motion estimation. However, the original JGPM is limited to one kind of human gaits, and its learning cannot be scaled up to a large dataset due to a high computational load. This work overcomes the limitations of the previous method by involving a multi-layer topology prior that is able to accommodate a variety of walking styles, leading to better motion synthesis results. Moreover, to learn multi-layer JGPM effectively and efficiently, we adopted two techniques, training data diversification and topology-aware local learning. The experimental results confirm the advantages and superiority of our proposed method over several existing Gaussian process-based motion models.	algorithm;diversification (finance);extrapolation;gaussian process;inverse kinematics;kinesiology;layer (electronics);motion estimation;nonlinear dimensionality reduction;resultant;speech synthesis;stochastic gradient descent;toroidal graph	Meng Ding;Guoliang Fan	2013	2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)	10.1109/FG.2013.6553783	computer vision;simulation;engineering;machine learning	Vision	29.042378434832035	-41.22967700547002	23381
d8c6c1f1c6b8185405c3a2e8d67b7fb3f043016d	linked tucker2 decomposition for flexible multi-block data analysis		In this paper, we propose a new algorithm for a flexible group multi-way data analysis called the linked Tucker2 decomposition (LT2D). The LT2D can decompose given multiple tensors into common factor ma- trices, individual factor matrices, and core tensors, simultaneously. When we have a set of tensor data and want to estimate common components and/or individual characteristics of the data, this decomposition model is very useful. In order to develop an efficient algorithm for the LT2D, we imposed orthogonality constraints to factor matrices and applied alter- nating least squares (ALS) algorithm to the optimization criterion. We conducted some experiments to demonstrate the advantages and conver- gence properties of the proposed algorithm. Finally, we discuss potential applications of the proposed method.		Tatsuya Yokota;Andrzej Cichocki	2014		10.1007/978-3-319-12643-2_14	econometrics;mathematical optimization;computer science;algorithm;statistics	Robotics	28.492743368313864	-35.59537881639462	23440
d6359aa15e076096a685510e2c7245aa92e8a8b0	an effective video synopsis approach with seam carving	optical distortion;surveillance;optical imaging;video synopsis seam carving video surveillance;surveillance video video synopsis approach surveillance camera object tracking method seam carving method;optical signal processing;object tracking;video surveillance object tracking;electron tubes;electron tubes surveillance optical distortion optical imaging object tracking optical signal processing	With a growth of surveillance cameras, the amount of captured videos expands. Manually analyzing and retrieving surveillance video is labor intensive and expensive. It would be much more convenient to generate a video digest, with which we can view the video in a fast and motion-preserving way. In this paper, we propose a novel video synopsis approach to generate condensed video, which uses an object tracking method for extracting important objects. This method will generate video tubes and a seam carving method to condense the original video. Experimental results demonstrate that our proposed method can achieve a high condensation rate while preserving all the important objects of interest. Therefore, this approach can enable users to view the surveillance video with great efficiency.	algorithm;closed-circuit television;computer performance;cryptographic hash function;emoticon;seam carving;video synopsis	Ke Li;Bo Yan;Weiyi Wang;Hamid Gharavi	2016	IEEE Signal Processing Letters	10.1109/LSP.2015.2496558	computer vision;computer science;video capture;video tracking;optical imaging;multimedia;video processing;computer graphics (images)	Vision	39.651140431178426	-51.14858049317064	23472
0d7fcdb99dc0d65b510f2b0b09d3d3cfed390261	robust face recognition with class dependent factor analysis	databases;decision models;generic model;gaussian processes;bayes methods;manifold learning;learning system;learning artificial intelligence bayes methods face recognition gaussian processes;face recognition;factor analysis;mixture of gaussians;experimental evaluation;bayesian decision model robust face recognition class dependent factor analysis facial expressions manifold embeddings nonlinear manifold learning technique gaussian mixture;facial expression;learning artificial intelligence	A general framework for face recognition under different variations such as illumination and facial expressions is proposed. The model utilizes the class information in a supervised manner to define separate manifolds for each class. Manifold embeddings are achieved by a nonlinear manifold learning technique. Inside each manifold, a mixture of Gaussians is designated to introduce a generative model. By this way, a novel connection between the manifold learning and probabilistic generative models is achieved. The proposed model learns system parameters in a probabilistic framework, allowing a Bayesian decision model. Experimental evaluations with face recognition under illumination changes and facial expressions were performed to realize the ability of the proposed model to handle different types of variations. Our recognition performances were comparable to state-of art results.	facial recognition system;factor analysis;generative model;illumination (image);manifold regularization;mixture model;nonlinear dimensionality reduction;nonlinear system;performance;supervised learning	Birkan Tunç;Volkan Dagli;Muhittin Gökmen	2011	2011 International Joint Conference on Biometrics (IJCB)	10.1109/IJCB.2011.6117508	facial recognition system;computer vision;decision model;machine learning;pattern recognition;mixture model;gaussian process;nonlinear dimensionality reduction;factor analysis;facial expression;statistics;manifold alignment	Vision	24.99249201590571	-47.748713488903064	23587
49fcded359ba3d43262c14f4e18592d84187521c	human pose estimation via deep part detection		In this paper, we propose to treat human pose estimation as an object detection problem. Specifically, the human pose is determined by the location of different body parts, different body parts are taken as different objects and a deep Convolutional Neural Network (CNN) is trained to detect these parts. We take advantage of the recently proposed fast and accurate deep object detector, SSD (Single Shot MultiBox Detector) to train a human body part detection model. We also propose to train a human body orientation model, with which to distinguish the left from the right of the body limbs. By combining body orientation and body part detection, we design a simple yet efficient human pose estimation method, which infers the location and category of the joints and link them together to form a whole pose. Experimental results show that, compared with other part detection based human pose estimation approaches, our method achieves better results on two public human pose estimation datasets.	3d pose estimation;human-based computation	Xiangyang Wang;Jiacheng Hu;Yongxia Jin;Zhi Liu;Xiaoqiang Zhu;Qiuyu Zhu;Haiwu Zhao	2017		10.1007/978-981-10-8108-8_6	convolutional neural network;computer vision;pose;object detection;detector;computer science;artificial intelligence	Vision	30.56579199530015	-50.89009962570015	23639
74c86e3c159f63e0d888d4350b66223cc52c16bf	group structure influence on group lasso consistency	group variables;kernel;regularization path;group lasso;consistency condition;regularization group lasso lasso;loading;data mining;regularization;group lasso consistency;machine learning;chromium;input variables analysis of variance machine learning symmetric matrices computer science information technology petroleum pattern analysis sufficient conditions polynomials;regression analysis;sparsity patterns;correlation;sparsity patterns group structure influence group lasso consistency regression method group variables consistency condition regularization path univariate variables;group structure influence;lasso;regression method;covariance matrix;univariate variables	Group Lasso is a recently proposed regression method that can be used to select group variables. When studying the consistency condition of the regularization path of group Lasso, we assume that the groupings of the univariate variables are known and fixed, that is, the group structure is given. In this paper, we address the issue of the influence of group structure on the group Lasso consistency. Based on the analysis of the consistency condition, we argue that the sparsity patterns is the determinant, the different group structures can lead to different consistencies, and the degree of the correlation between the relevant groups and the irrelevant groups is the key factor. Experimental results also demonstrate that the group Lasso is consistent under low correlation conditions.	algorithm;experiment;lasso;relevance;simulation;sparse matrix	Mei Wang;Shizhong Liao	2009	2009 Fifth International Conference on Natural Computation	10.1109/ICNC.2009.249	econometrics;pattern recognition;mathematics;statistics	ML	28.301692031097343	-35.113342145790284	23679
2c48126e1f52a34f836ed795b007b44b4c9bcc7f	a population monte carlo scheme with transformed weights and its application to stochastic kinetic models	degeneracy of importance weights;stochastic kinetic models;population monte carlo;importance sampling	This paper addresses the Monte Carlo approximation of posterior probability distributions. In particular, we consider the population Monte Carlo (PMC) technique, which is based on an iterative importance sampling (IS) approach. An important drawback of this methodology is the degeneracy of the importance weights (IWs) when the dimension of either the observations or the variables of interest is high. To alleviate this difficulty, we propose a new method that performs a nonlinear transformation of the IWs. This operation reduces the weight variation, hence it avoids degeneracy and increases the efficiency of the IS scheme, specially when drawing from proposal functions which are poorly adapted to the true posterior. For the sake of illustration, we have applied the proposed algorithm to the estimation of the parameters of a Gaussian mixture model. This is a simple problem that enables us to discuss the main features of the proposed technique. As a practical application, we have also considered the challenging problem of estimating the rate parameters of a stochastic kinetic model (SKM). SKMs are multivariate systems that model molecular interactions in biological and chemical problems. We introduce a particularization of the proposed algorithm to SKMs and present numerical results. E. K. acknowledges the support of Ministerio de Educación of Spain (Programa de Formación de Profesorado Universitario, ref. AP2008-00469). This work has been partially supported by Ministerio de Economı́a y Competitividad of Spain (program Consolider-Ingenio 2010 CSD2008-00010 COMONSENS, project DEIPRO TEC2009-14504-C02-01 and project COMPREHENSION TEC2012-38883-C02-01). E. Koblents and J. Mı́guez Department of Signal Theory and Communications, Universidad Carlos III de Madrid, Madrid, Spain E-mail: ekoblents,jmiguez@tsc.uc3m.es	algorithm;approximation;degeneracy (graph theory);importance sampling;interaction;iterative method;mixture model;monte carlo method;nonlinear system;numerical analysis;sampling (signal processing);signal processing	Eugenia Koblents;Joaquín Míguez	2015	Statistics and Computing	10.1007/s11222-013-9440-2	monte carlo method in statistical physics;econometrics;mathematical optimization;dynamic monte carlo method;hybrid monte carlo;importance sampling;monte carlo molecular modeling;mathematics;kinetic monte carlo;monte carlo integration;statistics;monte carlo method	ML	32.070477350579345	-26.190262322254565	23741
239b1d6ef8f945c70683d251768792c724464a27	computational steering by direct image manipulation	scientific application;computational steering;data structure	Computational steering requires the coupling of simulation and visualization elements, but if the latter is targetted at general requirements, little or no information about the calculation itself may survive in the final image. Consequently, changes made there cannot, in general, propagate back to the source. For example, in a study of a chemical reaction, a line on a graph simply consists of linked pairs of x and y coordinates, with no indication that these denote the concentrations of, say, oxygen or hydrogen at certain times. This paper will introduce a new visualization taxonomy and data structure which allow changes in the simulation to be accomplished by direct image manipulation, allowing more intuitive steering of a range of scientific applications.	computation;computational steering;data structure;graph (discrete mathematics);hydrogen;requirement;simulation	Fotis Chatzinikos;Helen Wright	2001			computational science;computer vision;data structure;computer science;theoretical computer science	Visualization	43.95154354133696	-26.826784189013466	23749
32384734966dc9ab8a5ce672696b90477035d6fa	parameter selection for principal curves	slope heuristics;slope heuristics model selection oracle inequality parameter selection penalty calibration principal curves;model selection;oracle inequality;complexity theory;shape approximation;oracle inequality parameter selection nonlinear generalization parameterized curve data cloud probability distribution principal curve problem risk minimization model selection penalization;approximation algorithms;empirical risk minimization;upper bound;indexes;statistical distributions;shape;parameter selection;principal component analysis;probability distribution;indexation;statistical distributions curve fitting;principal curves;context principal component analysis shape approximation algorithms complexity theory indexes upper bound;curve fitting;point of view;context;penalty calibration;principal component	Principal curves are nonlinear generalizations of the notion of first principal component. Roughly, a principal curve is a parameterized curve in which passes through the “middle” of a data cloud drawn from some unknown probability distribution. Depending on the definition, a principal curve relies on some unknown parameters (number of segments, length, turn, etc.) which have to be properly chosen to recover the shape of the data without interpolating. In this paper, we consider the principal curve problem from an empirical risk minimization perspective and address the parameter selection issue using the point of view of model selection via penalization. We offer oracle inequalities and implement the proposed approach to recover the hidden structures in both simulated and real-life data.	empirical risk minimization;interpolation;model selection;nonlinear system;oracle nosql db;penalty method;principal component analysis;real life;tag cloud	Gérard Biau;Aurélie Fischer	2012	IEEE Transactions on Information Theory	10.1109/TIT.2011.2173157	probability distribution;econometrics;mathematical optimization;mathematics;approximation algorithm;statistics;principal component analysis	ML	31.68984675878041	-29.884347795741032	23780
6689d1f369b2313882c13088dd6be40183472815	adaptive visual system for tracking low resolution colour targets	moving object;background modeling;video streaming;probabilistic method;shadow detection;low resolution;visual surveillance;object tracking;detection rate;visual tracking;expectation maximisation;visual system;environmental problem	This paper addresses the problem of using appearance and motion models in classifying and tracking objects when detailed information of the object’s appearance is not available. The approach relies upon motion, shape cues and colour information to help in associating objects temporally within a video stream. Unlike previous applications of colour in object tracking, where relatively large-size targets are tracked, our method is designed to track small colour targets. Our approach uses a robust background model based around Expectation Maximisation to segment moving objects with very low false detection rates. The system also incorporates a shadow detection algorithm which helps alleviate standard environmental problems associated with such approaches. A colour transformation derived from anthropological studies to model colour distributions of low-resolution targets is used along with a probabilistic method of combining colour and motion information. This provides a robust visual tracking system which is capable of performing accurately and consistently within a real world visual surveillance arena. This paper shows the system successfully tracking multiple people moving independently and the ability of the approach to recover lost tracks due to occlusions and background clutter.	clutter;color mapping;expectation–maximization algorithm;sensor;shape context;streaming media;tracking system;video tracking	Pakorn KaewTrakulPong;Richard Bowden	2001		10.5244/C.15.26	computer vision;simulation;image resolution;visual system;eye tracking;computer science;probabilistic method;video tracking;computer graphics (images)	Vision	43.839641006835336	-46.93723366052817	23820
5c5c0eb2dea55965c02f0218a54d9378d8a86536	on the consistency theory of high dimensional variable screening	qa mathematics	Variable screening is a fast dimension reduction technique for assisting high dimensional feature selection. As a preselection method, it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model. The performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones. When the data dimension p is substantially larger than the sample size n, variable screening becomes crucial as 1) Faster feature selection algorithms are needed; 2) Conditions guaranteeing selection consistency might fail to hold. This article studies a class of linear screening methods and establishes consistency theory for this special class. In particular, we prove the restricted diagonally dominant (RDD) condition is a necessary and sufficient condition for strong screening consistency. As concrete examples, we show two screening methods SIS and HOLP are both strong screening consistent (subject to additional constraints) with large probability if n > O((ρs+σ/τ)2 log p) under random designs. In addition, we relate the RDD condition to the irrepresentable condition, and highlight limitations of SIS.	algorithm;computation;diagonally dominant matrix;dimensionality reduction;feature selection	Xiangyu Wang;Chenlei Leng;David B. Dunson	2015			econometrics;mathematical optimization;computer science;mathematics;statistics	ML	25.084364408306673	-25.881149248316987	23853
cfb92b4afee814e1b1683ab8840ad0c7a58d990c	prediction of human driving behavior using dynamic bayesian networks	sistema lineal;prediccion;esquiva colision;pedestrian safety;poison control;conduccion vehiculo;driving behavior prediction;injury prevention;conduite vehicule;safety literature;vehicle driving;traffic safety;injury control;linear system;collision avoidance system;dynamical system;home safety;systeme dynamique;reseau bayes;injury research;safety abstracts;human factors;red bayes;dynamic bayesian network;occupational safety;comportement utilisateur;safety;switching linear dynamic system;bayes network;safety research;accident prevention;violence prevention;collision avoidance;bicycle safety;user behavior;sistema dinamico;systeme lineaire;esquive collision;collision warning system;poisoning prevention;falls;prediction;ergonomics;suicide prevention;comportamiento usuario	This paper presents a method of predicting future human driving behavior under the condition that its resultant behavior and past observations are given. The proposed method makes use of a dynamic Bayesian network and the junction tree algorithm for probabilistic inference. The method is applied to behavior prediction for a vehicle assumed to stop at an intersection. Such a predictive system would facilitate warning and assistance to prevent dangerous activities, such as red-light violations, by allowing detection of a deviation from normal behavior.	dynamic bayesian network	Toru Kumagai;Motoyuki Akamatsu	2006	IEICE Transactions	10.1093/ietisy/e89-d.2.857	simulation;prediction;computer science;suicide prevention;human factors and ergonomics;injury prevention;dynamical system;machine learning;bayesian network;linear system;computer security;dynamic bayesian network;statistics	Vision	43.16387880535909	-28.09802281186145	23855
98bc6ba2c54981ef99c25f4dd7e447f186816fa2	multi-view anomaly detection via probabilistic latent variable models		We propose a nonparametric Bayesian probabilistic latent variable model for multi-view anomaly detection, which is the task of finding instances that have inconsistent views. With the proposed model, all views of a non-anomalous instance are assumed to be generated from a single latent vector. On the other hand, an anomalous instance is assumed to have multiple latent vectors, and its different views are generated from different latent vectors. By inferring the number of latent vectors used for each instance with Dirichlet process priors, we obtain multi-view anomaly scores. The proposed model can be seen as a robust extension of probabilistic canonical correlation analysis for noisy multi-view data. We present Bayesian inference procedures for the proposed model based on a stochastic EM algorithm. The effectiveness of the proposed model is demonstrated in terms of performance when detecting multi-view anomalies and imputing missing values in multi-view data with anomalies.	anomaly detection;bayesian network;expectation–maximization algorithm;experiment;gaussian process;generative model;latent dirichlet allocation;latent variable model;missing data;sensor;steen rasmussen	Tomoharu Iwata;Makoto Yamada	2014	CoRR		latent class model;latent dirichlet allocation;machine learning;pattern recognition;data mining;mathematics;probabilistic latent semantic analysis	ML	28.23471354914294	-31.59461606599515	23902
d528b97b8d63561abc28a78c10a43b91e76d6da9	globally continuous and non-markovian crowd activity analysis from videos		Automatically recognizing activities in video is a classic problem in vision and helps to understand behaviors, describe scenes and detect anomalies. We propose an unsupervised method for such purposes. Given video data, we discover recurring activity patterns that appear, peak, wane and disappear over time. By using non-parametric Bayesian methods, we learn coupled spatial and temporal patterns with minimum prior knowledge. To model the temporal changes of patterns, previous works compute Markovian progressions or locally continuous motifs whereas we model time in a globally continuous and non-Markovian way. Visually, the patterns depict flows of major activities. Temporally, each pattern has its own unique appearance-disappearance cycles. To compute compact pattern representations, we also propose a hybrid sampling method. By combining these patterns with detailed environment information, we interpret the semantics of activities and report anomalies. Also, our method fits data better and detects anomalies that were difficult to detect previously.	fits;sampling (signal processing);temporal logic;unsupervised learning	He Wang;Carol O'Sullivan	2016		10.1007/978-3-319-46454-1_32	computer vision;simulation;computer science;machine learning;data mining	Vision	38.153592038528956	-47.0864917910432	23917
93110df24181d04215510d18a020dbaaae90b82a	joint action segmentation and classification by an extended hidden markov model	image motion analysis;image segmentation;image classification;inference mechanisms;journal article;hidden markov models	Hidden Markov models (HMMs) provide joint segmentation and classification of sequential data by efficient inference algorithms and have therefore been employed in fields as diverse as speech recognition, document processing, and genomics. However, conventional HMMs do not suit action segmentation in video due to the nature of the measurements which are often irregular in space and time, high dimensional and affected by outliers. For this reason, in this paper we present a joint action segmentation and classification approach based on an extended model: the hidden Markov model for multiple, irregular observations (HMM-MIO). Experiments performed over a concatenated version of the popular KTH action dataset and the challenging CMU multi-modal activity dataset (CMU-MMAC) report accuracies comparable to or higher than those of a bag-of-features approach, showing the usefulness of improved sequential models for joint action segmentation and classification tasks.	algorithm;concatenation;document processing;hidden markov model;markov chain;modal logic;statistical classification	Ehsan Zare Borzeshi;Óscar Pérez;Richard Y. D. Xu;Massimo Piccardi	2013	IEEE Signal Processing Letters	10.1109/LSP.2013.2284196	computer vision;contextual image classification;computer science;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;markov model;scale-space segmentation;hidden markov model	Vision	35.3709751119572	-48.27522539984426	24015
f64dd850733c25c764d408a05412f28aad308e88	cyber heart: the employment of an iterative design process to develop a left ventricular heart graphical display	treatment planning;digital simulation medical computing data visualisation cardiology biomechanics;heart disease;cardiology;biomechanics;left ventricular;left ventricle;medical computing;data visualisation;heart disease iterative design process left ventricular heart graphical display model based representation mathematical model visualization software pseudo stress analysis pseudo strain analysis interactive model software implementation virtual model clinical treatment planning artificial heart manufacture;iterative design;mathematical model;simulation study;interaction model;heart employment process design displays shape mathematical model deformable models visualization stress capacitive sensors;digital simulation;software implementation	This paper presents a model-based representation of the heart shape, and dynamics. The left ventricle (LV) of the heart is constructed as a mathematical model, close to the shape of actual LV. The model is made suitable for simulating the deformation of the heart during the cardiac cycle. A pseudo stress and strain analysis are conducted on the ventricular myocardium, and visualized. The visualization software is capable of simulating the conditions of a diseased heart. The interactive model and software implementation is useful for wide range of simulation studies that can be integrated with this virtual model that can help in fields like clinical treatment planning, manufacture of artificial heart, study of heart diseases etc.	3d modeling;iterative design;logical volume management;mathematical model;simulation;visualization software	Reginald C. Jegathese;Goo Lay Guan;L. Antony Rajiv;E. Y. K. Ng;Dhanjoo N. Ghista;Edmond C. Prakash	2003		10.1109/CYBER.2003.1253460	iterative design;simulation;computer science;biomechanics;mathematical model;statistics	Robotics	39.226996358912956	-38.52020324562856	24027
ec2542ea600af7b9e380dcffa0a639601f2187b8	enhancing human pose estimation with temporal clues		We address the challenging problem of human pose estimation, which can be adopted as a preprocessing step providing accurate and refined humanpose information for gait recognition and other applications. In this paper, we propose a method and augmented Pose-NMS to process the human pose estimation in the consecutive frames based on a reasonable assumption. The poses between the adjacent frames have small changes. Firstly we merge the multiple estimated pose candidates in a single frame to get the representative pose candidates. Then we propagate the final candidate backward and forward to increase the number of the confident candidates based on the Bayesian theory. We apply our method to the Buffy Video dataset and obtain the competitive result to the state-of-art.	3d pose estimation	Jianliang Hao;Zhaoxiang Zhang;Yunhong Wang	2014		10.1007/978-3-319-12484-1_40	computer vision;pattern recognition	Vision	33.35470201104699	-49.4773670604834	24047
1c63e1359d5dac301f536ad4dacbd5fc78a968b3	recovering epipolar geometry by reactive tabu search	cost function;motion estimation;computer vision motion estimation;computer vision;epipolar geometry;equations cost function computer vision laboratories pattern recognition automation computer science computational geometry image reconstruction layout;reactive tabu search;real images epipolar geometry recovery reactive tabu search uncalibrated images feature points cost function	We proposea new approachto recover epipolargeometryfrom a pair of uncalibratedimages. By minimizing a proposedcost function, our approachmatchesthe detectedfeature pointsfrom an imagepair, discardsthe outliersandrecovers the epipolargeometrysimutaneously. Experimentson realimagesshow thatthisapproachis effectiveandfast.	epipolar geometry;tabu search	Qifa Ke;Gang Xu;Songde Ma	1998		10.1109/ICCV.1998.710804	triangulation;computer vision;mathematical optimization;computer science;machine learning;motion estimation;fundamental matrix;epipolar geometry	Vision	51.73688054389417	-50.75764792865337	24051
d421ef2e3ab5f6bb6257463a2c86f588e43af99e	implementation and optimization of image processing algorithms on handheld gpu	application development;frames per second;optimisation;non photorealistic rendering;paper;video streaming;mobile device;image processing;video streaming application program interfaces computer graphic equipment coprocessors notebook computers optimisation rendering computer graphics;embedded cpu;api;opengl es 2 0;programmable shaders;real time;computer graphic equipment;mobile computer;gpu;coprocessors;video stream resolution optimization gpu image processing handheld device embedded cpu programmable shaders opengl es 2 0 api real time video scaling cartoon style non photorealistic rendering harris corner detector;computer vision;harris corner detector;gpgpu;cartoon style non photorealistic rendering;graphics processing unit streaming media rendering computer graphics real time systems pixel image color analysis;streaming media;image color analysis;pixel;application program interfaces;handheld device;notebook computers;algorithms;arm;optimization;mobile devices gpu gpgpu mobile computing opengl es 2 0;opengl;video stream resolution;rendering computer graphics;mobile computing;graphics processing unit;mobile devices;real time video scaling;real time systems;rendering	The advent of GPUs with programmable shaders on handheld devices has motivated embedded application developers to utilize GPU to offload computationally intensive tasks and relieve the burden from embedded CPU. In this work, we propose an image processing toolkit on handheld GPU with programmable shaders using OpenGL ES 2.0 API. By using the image processing toolkit, we show that a range of image processing algorithms map readily to handheld GPU. We employ real-time video scaling, cartoon-style non-photorealistic rendering, and Harris corner detector as our example applications. In addition, we propose techniques to achieve increased performance with optimized shader design and efficient sharing of GPU workload between vertex and fragment shaders. Performance is evaluated in terms of frames per second at varying video stream resolution.	algorithm;central processing unit;corner detection;embedded system;graphics processing unit;handheld game console;harris affine region detector;image processing;image scaling;mathematical optimization;mobile device;non-photorealistic rendering;opengl es;real-time locating system;shader;streaming media;unbiased rendering	Nitin Singhal;In Kyu Park;Sungdae Cho	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5651740	computer hardware;computer science;mobile device;multimedia;mobile computing;video post-processing;computer graphics (images)	EDA	42.50361502127817	-35.605054108476146	24057
c656481ddaae9f2319d39fd6b66cd18a3ee6dacd	from image sequences to recognized moving polyhedral objects	wireframe cad models computational geometry computer vision visual motion three dimensional geometry three dimensional motion polyhedral objects;design automation computational geometry machine vision;three dimensional geometry;cad;computational geometry;three dimensional motion;three dimensional;visual motion;computer vision;wireframe cad models;image sequence;computer vision cad computational geometry;polyhedral objects	This paper describes the combination of several novel algorithms into a system that obtains visual motion from a sequence of images and uses it to recover a three-dimensional description of the motion and geometry of the scene in terms of moving extended straight edges. The system goes on to recognize the recovered geometry as an object from a database of wireframe models, a stage that also resolves the depth/speed scaling ambiguity inherent in visual motion processing, resulting in absolute depth and motion recovery. The processing sequence is demonstrated on imagery from a well-carpentered CSG model and on natural imagery of simple polyhedral objects.	algorithm;constructive solid geometry;image scaling;polyhedron;wire-frame model	David W. Murray;David A. Castelow;Bernard F. Buxton	1989	International Journal of Computer Vision	10.1007/BF00133031	three-dimensional space;computer vision;structure from motion;computational geometry;computer science;cad;mathematics;geometry;computer graphics (images)	Vision	53.18350352409825	-51.322720164711995	24088
85de1ba51de64125f70d1ea5b306f0cd964198c2	rgbd-hudaact: a color-depth video database for human daily activity recognition	databases;histograms;video databases;image recognition;video streaming;history;interest points;image fusion;visualization;video cameras;image color analysis;image colour analysis;image representation;feature extraction;action recognition;video databases feature extraction image colour analysis image fusion image recognition image representation video cameras;home monitoring;humans;databases humans history image color analysis cameras histograms visualization;motion history image;human activity recognition;video database;red green blue color depth video database human daily activity recognition home monitoring color video camera depth sensor rgbd hudaact database multimodality sensor combination multimodality fusion scheme feature representation method spatio temporal interest point motion history image;human activity;cameras;activity recognition	In this paper, we present a home-monitoring oriented human activity recognition benchmark database, based on the combination of a color video camera and a depth sensor. Our contributions are two-fold: 1) We have created a publicly releasable human activity video database (i.e., named as RGBD-HuDaAct), which contains synchronized color-depth video streams, for the task of human daily activity recognition. This database aims at encouraging more research efforts on human activity recognition based on multi-modality sensor combination (e.g., color plus depth). 2) Two multi-modality fusion schemes, which naturally combine color and depth information, have been developed from two state-of-the-art feature representation methods for action recognition, i.e., spatio-temporal interest points (STIPs) and motion history images (MHIs). These depth-extended feature representation methods are evaluated comprehensively and superior recognition performances over their uni-modality (e.g., color only) counterparts are demonstrated.	activity recognition;color depth	Bingbing Ni;Gang Wang;Pierre Moulin	2011		10.1109/ICCVW.2011.6130379	computer vision;visualization;feature extraction;computer science;histogram;multimedia;image fusion;computer graphics (images);activity recognition	Vision	36.995716317745476	-50.275376034235464	24097
39398d08fa444bb7fbae314fdae6b698acea01ac	exact and robust conformal inference methods for predictive machine learning with dependent data		We extend conformal inference to general settings that allow for time series data. Our proposal is developed as a randomization method and accounts for potential serial dependence by including block structures in the permutation scheme. As a result, the proposed method retains the exact, model-free validity when the data are i.i.d. or more generally exchangeable, similar to usual conformal inference methods. When exchangeability fails, as is the case for common time series data, the proposed approach is approximately valid under weak assumptions on the conformity score.		Victor Chernozhukov;Kaspar Wüthrich;Yinchu Zhu	2018			machine learning;time series;permutation;uniformization (probability theory);mathematics;serial dependence;artificial intelligence;inference;conformal map	AI	25.320823702039373	-26.085541693670482	24104
bc3c2e42fdbb68014b32401540a9ecdc966a7f09	a computational and theoretical analysis of local null space discriminant method for pattern classification	local null space;local discriminant analysis;manifold learning;dimensionality reduction;theoretical analysis;pattern classification	Many problems in pattern classi ̄cation and feature extraction involve dimensionality reduction as a necessary processing. Traditional manifold learning algorithms, such as ISOMAP, LLE, and Laplacian Eigenmap, seek the low-dimensional manifold in an unsupervised way, while the local discriminant analysis methods identify the underlying supervised submanifold structures. In addition, it has been well-known that the intraclass null subspace contains the most discriminative information if the original data exist in a high-dimensional space. In this paper, we seek for the local null space in accordance with the null space LDA (NLDA) approach and reveal that its computational expense mainly depends on the quantity of connected edges in graphs, which may be still unacceptable if a great deal of samples are involved. To address this limitation, an improved local null space algorithm is proposed to employ the penalty subspace to approximate the local discriminant subspace. Compared with the traditional approach, the proposed method can achieve more e±ciency so that the overload problem is avoided, while slight discriminant power is lost theoretically. A comparative study on classi ̄cation shows that the performance of the approximative algorithm is quite close to the genuine one.	analysis of algorithms;approximation algorithm;computation;feature extraction;graph (discrete mathematics);graph embedding;isomap;kernel (linear algebra);linear discriminant analysis;machine learning;nonlinear dimensionality reduction	Miao Cheng;Bin Fang;Yuan Yan Tang;Hengxin Chen	2011	IJPRAI	10.1142/S0218001411008476	mathematical optimization;computer science;machine learning;pattern recognition;optimal discriminant analysis;mathematics;nonlinear dimensionality reduction;linear discriminant analysis;multiple discriminant analysis;dimensionality reduction	AI	24.797835401288665	-39.98974188930256	24107
6f47a9eff38288067d97863caa99f63e3160acbe	use of thermal point cloud for thermal comfort measurement and human pose estimation in robotic monitoring		This paper describes applications of thermal point cloud to lifestyle support robots. 3D information is useful for recognizing human and objects based on their shapes, while thermal information is useful for assessing the residential and the human states as well as for detecting human. Combining these two kinds of information will be beneficial to the robots which live with and support people at home or in care houses. This paper shows two applications of thermal point cloud. One is thermal comfort measurement based on predictive mean vote (PMV) which uses, as one of the factors, the amount of clothing estimated by thermal information. The other is human pose estimation only by depth images, which has an advantages in terms of privacy and insensitivity to illumination changes. We developed methods for these applications and show experimental results.	3d pose estimation;activity recognition;approximation algorithm;artificial neural network;deep learning;experiment;point cloud;robot;sensor;statistical classification;synthetic data	Kaichiro Nishi;Mitsuhiro Demura;Jun Miura;Shuji Oishi	2017	2017 IEEE International Conference on Computer Vision Workshops (ICCVW)	10.1109/ICCVW.2017.168	simulation;point cloud;computer vision;artificial intelligence;pose;thermal comfort;thermal;computer science	Robotics	35.675597120935635	-48.920620992009965	24115
286adcbc2aa38edaf3192f881f0d496d392d3897	human posture recognition by combining silhouette and infrared cast shadows	transforms image recognition;cameras transforms feature extraction three dimensional displays light sources relays biological system modeling;biological system modeling;three dimensional displays;feature extraction;weighted majority vote scheme human posture recognition infrared cast shadow multiinfrared light system body silhouette infrared filter distance transforms;transforms;activity recognition cast shadows infrared illumination posture recognition distance transform;relays;cameras;light sources	This paper proposes a multi-infrared lights system for human posture recognition, which uses as input the combination of a body silhouette and cast shadows. More precisely, in our experimental setup, we installed 4 infrared lights in the different upper corners of a room to project shadows of a person, and a camera with an infrared filter in the ceiling to capture images. A simple electronic device is used to turn on and off each light in turn to get one cast shadow by frame. To illustrate the feasibility of this approach, we consider a simple scene where the shadows are projected directly on the ground. The features used for recognition are based on the distance transforms of the combination of the body silhouette and each cast shadow. A weighted majority vote scheme is used to decide what is the corresponding posture. Our approach was validated on a new data set captured in our laboratory, and compared with a traditional mono-camera system.	distance transform;poor posture	Rafik Gouiaa;Jean Meunier	2015	2015 International Conference on Image Processing Theory, Tools and Applications (IPTA)	10.1109/IPTA.2015.7367095	computer vision;feature extraction;computer science;machine learning;computer graphics (images)	Robotics	48.63554171243547	-41.436246796981315	24139
069ebb57ccca31ab68983e07044e65ce1a04174f	4d facial expression recognition	databases;image classification face recognition feature extraction;histograms;facial expression recognition;hidden markov model;computer model;image classification;local binary pattern;three dimensional;computational modeling;face recognition;hidden markov models;vectors;three dimensional displays;feature extraction;fully automatic facial expression recognition 4d facial expression recognition discrete expression classification dynamic 3d sequences facial movements local binary patterns orthogonal planes feature extraction;facial expression;videos;three dimensional displays hidden markov models databases vectors videos histograms computational modeling	In this paper, we focus on discrete expression classification using dynamic 3D sequences (4D data) recording the facial movements. A robust approach for registering 4D data is proposed and a variant of local binary patterns on three orthogonal planes is used for feature extraction. We present a fully automatic facial expression recognition pipeline. The system was evaluated on the publicly available facial expression database BU-4DFE and promising results were obtained.	coherence (physics);feature extraction;feature selection;hidden markov model;local binary patterns;markov chain	Tianhong Fang;Xi Zhao;Shishir K. Shah;Ioannis A. Kakadiaris	2011	2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)	10.1109/ICCVW.2011.6130440	three-dimensional space;computer vision;contextual image classification;local binary patterns;feature extraction;computer science;machine learning;pattern recognition;histogram;computational model;facial expression;hidden markov model	Vision	37.482640962845565	-50.4202397393358	24153
4423522df1dd92d04e303ae2e2f7fa0d998fe862	regularized max pooling for image categorization		We propose Regularized Max Pooling (RMP) for image classification. RMP classifies an image (or image region) by extracting feature vectors at multiple subwindows at multiple locations and scales. Unlike Spatial Pyramid Matching where the subwindows are defined purely based on geometric correspondence, RMP accounts for the deformation of discriminative parts. The amount of deformation and the discriminative ability for multiple parts are jointly learned during training. An RMP model is a collection filters. Each filter is anchored to a specific image subwindow and associated with a set of deformation coefficients. The anchoring subwindows are predetermined at various locations and scales, while the filters and deformation coefficients are learnable parameters of the model. Fig. 1 shows a possible way to define subwindows. To classify a test image, RMP extracts feature vectors for all anchoring subwindows. The classification score of an image is the weighted sum of all filter responses. Each filter yields a set of filter responses, one for each level of deformation. The deformation coefficients are the weights for these filter responses. Given a set of images {Ii} n i=1 and labels {yi|yi ∈ {1,−1}} n i=1 , consider a particular set of geometrically defined subwindows which can encode semantic content of an image at different locations and scales (e.g., Fig 1). Let {I j}mj=1 denote the set of subwindows for image I. Let φ be the feature function of which the input is an image region and the output is a column vector. Let D j be the feature matrix computed at location j for all images and K j the corresponding kernel, i.e., D j = [φ(I j 1) · · ·φ(I j n)] and K j = (D ) D j . The joint kernel for all subwindows is the sum of all kernels: K = ∑j=1 K ; this corresponds to concatenating all feature vectors computed at all subwindows. Given the kernel K, we train an Least-Squares SVM and obtain a coefficient vector and bias term α ,b. The filter for subwindow j can be computed as w j = D α . For a particular subwindow j and an image I, the regularized maximum score is defined:	categorization;coefficient;computer vision;concatenation;encode;kernel (operating system);least squares;risk management plan;standard test image;weight function	Minh Hoai	2014			machine learning;pattern recognition;data mining;mathematics	Vision	25.838715615470985	-46.44656284346879	24245
27e5a084817e67aa3e4b0ace1940df5ae52d4cc6	markov constraints: steerable generation of markov sequences	input device;generic algorithm;sequence constraints;markov property;left to right;interactive application;temporal properties;csp;constraint satisfaction problem;branch and bound;interactive applications;global constraints;markov chains;markov chain	Markov chains are a well known tool to model temporal properties of many phenomena, from text structure to fluctuations in economics. Because they are easy to generate, Markovian sequences, i.e. temporal sequences having the Markov property, are also used for content generation applications such as text or music generation that imitate a given style. However, Markov sequences are traditionally generated using greedy, left-to-right algorithms. While this approach is computationally cheap, it is fundamentally unsuited for interactive control. This paper addresses the issue of generating steerable Markovian sequences. We target interactive applications such as games, in which users want to control, through simple input devices, the way the system generates a Markovian sequence, such as a text, a musical sequence or a drawing. To this aim, we propose to revisit Markov sequence generation as a branch and bound constraint satisfaction problem (CSP). We propose a CSP formulation of the basic Markovian hypothesis as elementary Markov Constraints (EMC). We propose algorithms that achieve domain-consistency for the propagators of EMCs, in an event-based implementation of CSP. We show how EMCs can be combined to estimate the global Markovian probability of a whole sequence, and accommodate for different species of Markov generation such as fixed order, variable-order, or smoothing. Such a formulation, although more costly than traditional greedy generation algorithms, yields the immense advantage of being naturally steerable, since control specifications can be represented by arbitrary additional constraints, without any modification of the generation algorithm. We illustrate our approach on simple yet combinatorial chord sequence and melody generation problems and give some performance results.	branch and bound;constraint satisfaction problem;greedy algorithm;input device;markov chain;markov property;propagator;smoothing	François Pachet;Pierre Roy	2010	Constraints	10.1007/s10601-010-9101-4	markov chain;mathematical optimization;maximum-entropy markov model;markov kernel;combinatorics;computer science;machine learning;markov blanket;mathematics;markov algorithm;markov process;markov model;algorithm;variable-order markov model	AI	36.60916812977341	-26.86951744398131	24279
b99d31cd6cf75d36804a152d4d8f972b00c48478	non-rigid image registration using fully convolutional networks with deep self-supervision		We propose a novel non-rigid image registration algorithm that is built upon fully convolutional networks (FCNs) to optimize and learn spatial transformations between pairs of images to be registered. Different from most existing deep learning based image registration methods that learn spatial transformations from training data with known corresponding spatial transformations, our method directly estimates spatial transformations between pairs of images by maximizing an image-wise similarity metric between fixed and deformed moving images, similar to conventional image registration algorithms. At the same time, our method also learns FCNs for encoding the spatial transformations at the same spatial resolution of images to be registered, rather than learning coarse-grained spatial transformation information. The image registration is implemented in a multi-resolution image registration framework to jointly optimize and learn spatial transformations and FCNs at different resolutions with deep selfsupervision through typical feedforward and backpropagation computation. Since our method simultaneously optimizes and learns spatial transformations for the image registration, our method can be directly used to register a pair of images, and the registration of a set of images is also a training procedure for FCNs so that the trained FCNs can be directly adopted to register new images by feedforward computation of the learned FCNs without any optimization. The proposed method has been evaluated for registering 3D structural brain magnetic resonance (MR) images and obtained better performance than state-of-the-art image registration algorithms.	algorithm;backpropagation;computation;deep learning;feedforward neural network;image registration;mathematical optimization;resonance	Hongming Li;Yong Fan	2017	CoRR		artificial intelligence;pattern recognition;computation;computer science;deep learning;training set;backpropagation;image registration;image resolution	Vision	25.367811708171843	-51.52018827141613	24299
dcd7f4473eebf462fc53b0e9714d29ae74c87bf8	efficient single image super-resolution via graph embedding	graph theory;image patches;geometrical structure;local geometric structure super resolution graph embedding manifold learning;psnr;image resolution;manifolds;ssr single image superresolution graph embedding superresolution gesr high resolution image single low resolution observation lr observation projection matrix mapping lr image patch space hr image patch space intrinsic geometrical structure manifold learning based sr methods image degeneration neighbor embedding based sr nesr sparse representation based sr;manifolds strontium image reconstruction training image resolution dictionaries psnr;training;downsampling;manifold learning;会议论文;matrix algebra;strontium;matrix algebra graph theory image representation image resolution learning artificial intelligence;objective evaluation;algorithmic solutions;single images;image representation;image reconstruction;dictionaries;image degeneration;super resolution;projection matrix;super re;graph embedding;graph embeddings;learning artificial intelligence;sparse representation;high resolution image;geometric structure;local geometric structure;benchmark tests	We explore in this paper efficient algorithmic solutions to single image super-resolution (SR). We propose the GESR, namely Graph Embedding Super-Resolution, to super-resolve a high-resolution (HR) image from a single low-resolution (LR) observation. The basic idea of GESR is to learn a projection matrix mapping the LR image patch to the HR image patch space while preserving the intrinsic geometrical structure of original HR image patch manifold. While GESR resembles other manifold learning-based SR methods in persevering the local geometric structure of HR and LR image patch manifold, the innovation of GESR lies in that it preserves the intrinsic geometrical structure of original HR image patch manifold rather than LR image patch manifold, which may be contaminated because of image degeneration (e.g., blurring, down-sampling and noise). Experiments on benchmark test images show that GESR can achieve very competitive performance as Neighbor Embedding based SR (NESR) and Sparse representation based SR (SSR). Beyond subjective and objective evaluation, all experiments show that GESR is much faster than both NESR and SSR.	autostereogram;benchmark (computing);experiment;graph embedding;high-resolution scheme;image resolution;lr parser;nonlinear dimensionality reduction;sampling (signal processing);sparse;super-resolution imaging	Junjun Jiang;Ruimin Hu;Zhen Han;Kebin Huang;Tao Lu	2012	2012 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2012.102	iterative reconstruction;upsampling;computer vision;graph embedding;image resolution;strontium;peak signal-to-noise ratio;manifold;projection;computer science;graph theory;machine learning;sparse approximation;mathematics;geometry;nonlinear dimensionality reduction;superresolution	Vision	28.217468924979286	-40.086801787513544	24306
c4bed433de4e2e16cf088b40dca323516ac65ba9	visual tracking via context-aware local sparse appearance model		Most existing local sparse trackers are prone to drifting away as they do not make use of discriminative information of local patches. In this paper, we propose an effective context-aware local sparse appearance model to alleviate the drift problem caused by background clutter and occlusions. First, considering that different local patches should have different impacts on the likelihood computation, we present a novel Impact Allocation Strategy (IAS) with integration of the spatial-temporal context. Varying positive impact factors are adaptively assigned to different local patches based on their ability distinguishing the spatial context, which provides discriminative information to prevent the tracker from drifting. Furthermore, we exploit temporal context to introduce some historical information for more accurate locating. Second, we present a new patch-based dictionary update method being able to update each patch independently with the validation of effectiveness. On the one hand, we introduce sparsity concentration index to check whether the local patch to be updated is a valid local patch from the target object. On the other hand, spatial context is further employed to eliminate the effect of the background. Experimental results show the superiority and competitiveness of the proposed method on the benchmark data set compared to other state-of-the-art algorithms. 2018 Elsevier Inc. All rights reserved.	algorithm;benchmark (computing);clutter;computation;internet authentication service;patch (computing);sparse dictionary learning;sparse matrix	Guiji Li;Manman Peng;Ke Nai;Zhiyong Li;Keqin Li	2018	J. Visual Communication and Image Representation	10.1016/j.jvcir.2018.09.004	computer vision;spatial contextual awareness;pattern recognition;artificial intelligence;discriminative model;mathematics;computation;active appearance model;clutter;bittorrent tracker;exploit;eye tracking	AI	42.374635425997496	-49.18375646198591	24398
890f5b51fa1252b3b1228c70f8b35db125fca043	visualization techniques for 3d vector fields: an application to electrostatic fields of molecules	3d vector field;critical point;visualization technique;vector field;molecular graphics;selective visualization;stream function;topological skeleton	In chemistry, computer graphics is now well established as a tool to interpret simulation results, since molecules are complicated in their structures and mutual interactions. As a probe to study such molecular interactions, electrostatic fields are considered to be useful. However, since they are given as 3D vector fields having cusps in the fields, conventional drawing techniques are not applicable. In this article, two new approaches are presented to visualize the electrostatic fields of molecules. One is an extension of topological skeletons, by which interactions between atoms having opposite charges are expressed, which is not shown with the conventional methods. The other is to define new functions called selective functions to select regions of interest only from the geometrical features of the fields. Furthermore, from the definition of the new functions, mathematical relations between the topological skeletons and selective functions are discussed. An example is presented in applications to chemical reactions to show how the scheme is used. Copyright # 2001 John Wiley & Sons, Ltd.	ab initio quantum chemistry methods;animat;automatic parallelization;bsd;columbia (supercomputer);computation;computer animation;computer graphics;computer science;hiroshi ishii (computer scientist);ibm research;interaction;john d. wiley;molecular graphics;molecular orbital;numerical analysis;numerical differentiation;parallel rendering;region of interest;scientific visualization;simulation;systems engineering;topological skeleton	Susumu Handa;Hiroshi Kashiwagi;Toshikazu Takada	2001	Journal of Visualization and Computer Animation	10.1002/vis.255	vector field;molecular graphics;computer science;theoretical computer science;mathematics;geometry;topological skeleton;critical point;stream function;computer graphics (images)	Graphics	43.989488708619085	-26.839888650700484	24431
5132c4dc71d19017e2feb98d107cc330cad0b7a2	non-negative matrix factorization on manifold	graph theory;nearest neighbor searches;matrix factorization;manifolds;information retrieval;data mining;affinity graph non negative matrix factorization information retrieval computer vision pattern recognition original data matrix parts based data representation;data representation;matrix decomposition data handling data structures graph theory;matrix decomposition;data structures;non negative matrix factorization;pattern recognition;clustering algorithms;mutual information;approximation methods;matrix decomposition information retrieval computer vision pattern recognition singular value decomposition face recognition humans clustering algorithms convergence data mining;data handling;geometric structure	Recently non-negative matrix factorization (NMF) has received a lot of attentions in information retrieval, computer vision and pattern recognition. NMF aims to find two non-negative matrices whose product can well approximate the original matrix. The sizes of these two matrices are usually smaller than the original matrix. This results in a compressed version of the original data matrix. The solution of NMF yields a natural parts-based representation for the data. When NMF is applied for data representation, a major disadvantage is that it fails to consider the geometric structure in the data. In this paper, we develop a graph based approach for parts-based data representation in order to overcome this limitation. We construct an affinity graph to encode the geometrical information and seek a matrix factorization which respects the graph structure. We demonstrate the success of this novel algorithm by applying it on real world problems.	affinity analysis;approximation algorithm;computer vision;data (computing);encode;information retrieval;non-negative matrix factorization;pattern recognition	Deng Cai;Xiaofei He;Xiaoyun Wu;Jiawei Han	2008	2008 Eighth IEEE International Conference on Data Mining	10.1109/ICDM.2008.57	computer science;graph theory;theoretical computer science;machine learning;pattern recognition;mathematics;essential matrix;state-transition matrix;matrix decomposition;block matrix;integer matrix;adjacency matrix	Vision	26.927912497410276	-40.792696250848635	24443
26d25c21a4da2429d5b923b86410ad73320d61ba	stereo reconstruction of droplet flight trajectories	stereo reconstruction;multi view geometry;stereo reconstruction 3d flight trajectory extraction high speed stereo capture multicamera tracking technique local motion models 3d flight trajectory reconstruction learning stereo camera system stereo information;cameras trajectory target tracking three dimensional displays image reconstruction aerodynamics;aerodynamics;nonlinear motion;stereo image processing computer vision feature extraction image motion analysis learning artificial intelligence object tracking;trajectory;three dimensional displays;image reconstruction;multi target tracking;computer vision feature extraction image motion analysis learning artificial intelligence object tracking stereo image processing;parameter estimation;target tracking;cameras;stereo information stereo reconstruction 3d flight trajectory extraction high speed stereo capture multicamera tracking technique local motion models 3d flight trajectory reconstruction learning stereo camera system	We developed a new method for extracting 3D flight trajectories of droplets using high-speed stereo capture. We noticed that traditional multi-camera tracking techniques fare poorly on our problem, in part due to the fact that all droplets have very similar shapes, sizes and appearances. Our method uses local motion models to track individual droplets in each frame. 2D tracks are used to learn a global, non-linear motion model, which in turn can be used to estimate the 3D locations of individual droplets even when these are not visible in any camera. We have evaluated the proposed method on both synthetic and real data and our method is able to reconstruct 3D flight trajectories of hundreds of droplets. The proposed technique solves for both the 3D trajectory of a droplet and its motion model concomitantly, and we have found it to be superior to 3D reconstruction via triangulation. Furthermore, the learned global motion model allows us to relax the simultaneity assumptions of stereo camera systems. Our results suggest that, even when full stereo information is available, our unsynchronized reconstruction using the global motion model can significantly improve the 3D estimation accuracy.	3d reconstruction;correspondence problem;estimated;experiment;extrapolation;match moving;nonlinear system;numerous;obstruction;preparation;silo (dataset);stereo camera;stereoscopy;synthetic data;synthetic intelligence;triangulation (geometry)	Luis A. Zarrabeitia;Faisal Z. Qureshi;Dhavide A. Aruliah	2015	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2014.2353638	iterative reconstruction;computer vision;simulation;aerodynamics;trajectory;estimation theory;statistics;computer graphics (images)	Vision	51.94153088091244	-49.21634611677595	24477
f5ad544fac7259eda518f95f75d68b9f281da706	random finite sets in multi-target tracking - efficient sequential mcmc implementation		Over the last few decades multi-target tracking (MTT) has proved to be a challenging and attractive research topic. MTT applications span a wide variety of disciplines, including robotics, radar/sonar surveillance, computer vision and biomedical research. The primary focus of this dissertation is to develop an effective and efficient multi-target tracking algorithm dealing with an unknown and time-varying number of targets.rnrnThe emerging and promising Random Finite Set (RFS) framework provides a rigorous foundation for optimal Bayes multi-target tracking. In contrast to traditional approaches, the collection of individual targets is treated as a set-valued state. The intent of this dissertation is two-fold; first to assert that the RFS framework not only is a natural, elegant and rigorous foundation, but also leads to practical, efficient and reliable algorithms for Bayesian multi-target tracking, and second to provide several novel RFS based tracking algorithms suitable for the specific Track-Before-Detect (TBD) surveillance application.rnrnOne main contribution of this dissertation is a rigorous derivation and practical implementation of a novel algorithm well suited to deal with multi-target tracking problems for a given cardinality. The proposed Interacting Population-based MCMC-PF algorithm makes use of several Metropolis-Hastings samplers running in parallel, which interact through genetic variation.rnrnAnother key contribution concerns the design and implementation of two novel algorithms to handle a varying number of targets. The first approach exploits Reversible Jumps. The second approach is built upon the concepts of labeled RFSs and multiple cardinality hypotheses. The performance of the proposed algorithms is also demonstrated in practical scenarios, and shown to significantly outperform conventional multi-target PF in terms of track accuracy and consistency.rnrnThe final contribution seeks to exploit external information to increase the performance of the surveillance system. In multi-target scenarios, kinematic constraints from the interaction of targets with their environment or other targets can restrict target motion. Such motion constraint information is integrated by using a fixed-lag smoothing procedure, named Knowledge-Based Fixed-Lag Smoother (KB-Smoother). The proposed combination IP-MCMC-PF/KB-Smoother yields enhanced tracking.	markov chain monte carlo	Melanie Bocquel	2013			cardinality;data mining;smoothing;sonar;population;markov chain monte carlo;bayes' theorem;robotics;artificial intelligence;bayesian probability;computer science	Crypto	39.38353126249546	-26.830005634680052	24563
0667bd18e0537359bc44a8f2ad94074b2e3a346e	fast pixelwise adaptive visual tracking of non-rigid objects	image segmentation;nonrigid object fast pixelwise adaptive visual tracking real time video single object tracking unconstrained environment generalized hough transform probabilistic segmentation method color descriptor gradient descriptor foreground color distribution background color distribution adaptive shape model nonrigid deformation rigid deformation;image sequence analysis image motion analysis;hough transforms image colour analysis image motion analysis image segmentation object tracking probability video signal processing;computational modeling;shape;robustness;adaptation models shape image segmentation videos computational modeling tracking robustness;adaptation models;tracking;videos	In this paper, we present a new algorithm for real-time single-object tracking in videos in unconstrained environments. The algorithm comprises two different components that are trained “in one shot” at the first video frame: a detector that makes use of the generalized Hough transform with color and gradient descriptors and a probabilistic segmentation method based on global models for foreground and background color distributions. Both components work at pixel level and are used for tracking in a combined way adapting each other in a co-training manner. Moreover, we propose an adaptive shape model as well as a new probabilistic method for updating the scale of the tracker. Through effective model adaptation and segmentation, the algorithm is able to track objects that undergo rigid and non-rigid deformations and considerable shape and appearance variations. The proposed tracking method has been thoroughly evaluated on challenging benchmarks, and outperforms the state-of-the-art tracking methods designed for the same task. Finally, a very efficient implementation of the proposed models allows for extremely fast tracking.	acclimatization;algorithm;area striata structure;benchmark (computing);co-training;detectors;discriminative model;generalised hough transform;generic drugs;gradient;image segmentation;muscle rigidity;musculoskeletal diseases;physical object;pixel;real-time clock;real-time locating system;silo (dataset);video tracking;web colors;biologic segmentation;videocassette	Stefan Duffner;Christophe Garcia	2017	IEEE Transactions on Image Processing	10.1109/TIP.2017.2676346	computer vision;shape;computer science;video tracking;pattern recognition;mathematics;tracking;image segmentation;scale-space segmentation;computational model;robustness;computer graphics (images)	Vision	44.39975119581056	-49.59265809103795	24602
90091a0beece3f48d71d62db1e722ad6b61a3807	a markov random field model for object matching under contextual constraints	bayesian framework;optimal solution;object recognition;object matching;markov processes probability image sequences computer vision;probability;mrf;image matching;contextual constraints;markov random field;computer vision;posterior distribution;machine vision;probability object recognition image matching machine vision markov processes;probability markov random field object matching contextual constraints object recognition high level vision mrf bayesian framework maximum a posteriori;map estimation;markov processes;maximum a posteriori;high level vision;image sequences	This paper presents a Markov random eld (MRF) model for object recognition in high level vision. The labeling state of a scene in terms of a model object is considered as an MRF or couples MRFs. Within the Bayesian framework, the optimal solution is deened as the maximum a posteriori (MAP) estimate of the MRF. The posterior distribution is derived based on sound mathematical principles from theories of MRF and probability, which is in contrast to heuristic formulations. An experimental result is presented.	domain-driven design;heuristic;high-level programming language;markov chain;markov random field;outline of object recognition;theory	Stan Z. Li	1994		10.1109/CVPR.1994.323915	computer vision;materials recovery facility;machine vision;computer science;maximum a posteriori estimation;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;probability;mathematics;markov process;posterior probability;statistics	Vision	45.90982798449394	-49.879299822820535	24685
5a553cc4bd133ba11f38422f1895d1750e1b6452	ensemble methods for convex regression with applications to geometric programming based circuit design		Convex regression is a promising area for bridging statistical estimation and deterministic convex optimization. New piecewise linear convex regression methods (Hannah and Dunson, 2011; Magnani and Boyd, 2009) are fast and scalable, but can have instability when used to approximate constraints or objective functions for optimization. Ensemble methods, like bagging, smearing and random partitioning, can alleviate this problem and maintain the theoretical properties of the underlying estimator. We empirically examine the performance of ensemble methods for prediction and optimization, and then apply them to device modeling and constraint approximation for geometric programming based circuit design.	approximation algorithm;bootstrap aggregating;bridging (networking);circuit design;computational complexity theory;control theory;convex function;convex optimization;ensemble learning;estimation theory;geometric programming;instability;linear approximation;linear model;mathematical optimization;monte carlo method;piecewise linear continuation;random search;scalability;tree (data structure)	Lauren Hannah;David B. Dunson	2012	CoRR		mathematical optimization;conic optimization;convex optimization;convex combination;linear matrix inequality;nonlinear programming;machine learning;mathematics;statistics;proper convex function	ML	24.932611306037572	-32.8509543460019	24711
8b5ad19c547c7141f4e86b567f23dac9c3af227b	parameter instability regimes for sparse proximal denoising programs		Compressed sensing theory explains why Lasso programs recover structured high-dimensional signals with minimax order-optimal error. Yet, the optimal choice of the program’s governing parameter is often unknown in practice. It is still unclear how variation of the governing parameter impacts recovery error in compressed sensing, which is otherwise provably stable and robust. We establish a novel notion of instability in Lasso programs when the measurement matrix is identity. This is the proximal denoising setup. We prove asymptotic cusp-like behaviour of the risk as a function of the parameter choice, and illustrate the theory with numerical simulations. For example, a 0.1% underestimate of a Lasso parameter can increase the error significantly; and a 50% underestimate can cause the error to increase by a factor of 10. We hope that revealing parameter instability regimes of Lasso programs helps to inform a practitioner’s choice.	compressed sensing;computer simulation;decade (log scale);instability;lasso;minimax;noise reduction;numerical analysis;sparse matrix	Aaron Berk;Yaniv Plan;Özgür Yilmaz	2018	CoRR		lasso (statistics);mathematical optimization;compressed sensing;matrix (mathematics);noise reduction;mathematics;instability;minimax	ML	27.25518300571505	-32.75276130318122	24760
f1b66a3cb9e2a24fb2de9f04270333d8a98cdc45	a modified approach to objective surface generation within the gauss-newton parameter identification to ignore outlier data points	gradient descent methods;three sigma;gauss newton methods;outlier data;parameter identification;glycemic modelling	The Gauss-Newton method is a simple iterative gradient descent method used to modify a mathematical model by minimising the least-squares residuals between the modelled response, and some observed behaviour. A common issue for parameter identification methods that optimise least-square residuals is the sporadic occurrence of outlying data in the observation data set.#R##N##R##N#This research proposes an amendment to the Gauss-Newton parameter identification approach that limits the influence of outlying data by dissipating the contribution of outlying data to the objective function that drives iterations. The modified method was tested in two and three-dimensional parameter identification exercises using virtual data from the dynamic insulin sensitivity and secretion test (DISST). The data incorporated random normally distributed noise (CV = 3%) or random normally distributed noise in concert with an outlying data point. The proposed method performed similarly to the original method when no outlying data was included and found the model that fit accurately to the majority of data points when an outlying data point was present.#R##N##R##N#The proposed approach provides a valuable tool for the rejection of outlier data that is operator independent, does not require multiple stages of analysis, or manual removal of data.	data point;gauss–newton algorithm;newton	Rebecca A. L. Gray;Paul Docherty;Liam M. Fisk;Rua Murray	2016	Biomed. Signal Proc. and Control	10.1016/j.bspc.2016.06.009	econometrics;machine learning;mathematics;statistics	ML	26.810234579948354	-23.977529329375937	24817
a15e652ec32a9c762aa392c2d92441664e39d607	camera tamper detection using codebook model for video surveillance	video surveillance;real time camera tamper detection model codebook model ctdm video surveillance system adaptive background codebook model background pixel process video image sequence feature values light movement problem shadow movement problem warning interface displacement tamper obstruction tamper;surveillance abstracts cameras;video coding;obstruction detection camera tamper detection model background codebook model displacement detection;cameras;video surveillance cameras image sequences video coding;image sequences	The proposed model, Camera Tamper Detection Model (CTDM), is used for the camera tamper detection in a video surveillance system. In this paper, an adoptive background codebook model is trained for classifying foreground and background pixels from video image sequence. According to the color model, both chroma and brightness of pixels are adopted as feature values and they are able to tackle the problem of light and shadow movement. During a training process of background pixels, CDTM automatically creates clusters in a background codebook model to classify pixels. After training stage, a well adaptive background model is acquired which is a basis for camera tamper detection. A warning interface of camera tamper detection is provided in the surveillance system which can classify tamper type, e.g. displacement tamper or obstruction tamper, in order to ensure the security of video surveillance system. The experimental results show that the accuracy rate of real-time camera tamper detection in the research can reach 100%, either in the indoor or outdoor experimental scenes.	closed-circuit television;codebook;displacement mapping;pixel;real-time clock;robertson–seymour theorem;virtual camera system	Chun-Liang Tung;Pei-Ling Tung;Che-Wei Kuo	2012	2012 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2012.6359641	computer vision;multimedia;computer graphics (images)	Vision	41.6595766544168	-45.31970674781578	24834
92b748f2629b3227a9c56bc9e580f45eb5bdfba5	novel adaptive eye detection and tracking for challenging lighting conditions	conference item	The paper develops a novel technique that significantly improves the performance of Haar-like feature-based object detectors in terms of speed, detection rate under di cult lighting conditions, and reduced number of false-positives. The method is implemented and validated for driver monitoring under very dark, very bright, and normal conditions. The framework includes a fast adaptive detector designed to cope with rapid lighting variations, as well as an implementation of a Kalman filter for reducing the search region and indirect support of eye monitoring and tracking. The proposed methodology e↵ectively works under low-light conditions without using infrared illumination or any other extra lighting support. Experimental results, performance evaluation, and comparing a standard Haar-like detector with the proposed adaptive eye detector, show noticeable improvements.	haar wavelet;jones calculus;kalman filter;local binary patterns;motion estimation;performance evaluation;pixel;preprocessor;recursion;scale-invariant feature transform;sensor;sinewave synthesis;statistical classification;video graphics array	Mahdi Rezaei;Reinhard Klette	2012		10.1007/978-3-642-37484-5_35	computer vision;simulation;computer science	Vision	41.7659341488634	-49.56310907843096	24899
d02da5b57fa460847503d8ef2d6a49809595a8e6	schlauschleimer in reichsautobahnen: slime mould imitates motorway network in germany	biology;road transport;germany;biological transport networks;unconventional computing;slime mould	Purpose – The purpose of this paper is to develop experimental laboratory biological techniques for approximation of principle transport networks, optimizing transport links, and developing optimal solutions to current transport problems. It also aims to study how slime mould of Physarum polycephalum approximate autobahn networks in Germany.Design/methodology/approach – The paper considers the 21 most populous urban areas in Germany. It represents these areas with source of nutrients placed in the positions of slime mould growing substrate corresponding to the areas. At the beginning of each experiment slime mould is inoculated in the Berlin area. Slime mould exhibits foraging behavior and spans sources of nutrients (which represent urban areas) with a network of protoplasmic tubes (which approximate vehicular transport networks). The study analyzes structure of transport networks developed by slime mould and compares it with families of known proximity graphs. It also imitates slime‐mould response to sim...		Andrew Adamatzky;Theresa Schubert	2012	Kybernetes	10.1108/03684921211257865	slime mold;operations research;unconventional computing	Vision	46.642449635577734	-25.701645883331977	24922
78a71067e91fbbad4b55881ed1beb99b222c32c7	emovi-slam: embedded monocular visual inertial slam with scale update for large scale mapping and localization		In recent researches, monocular simultaneous localization and mapping (SLAM) remains a well-known technique for ego-motion tracking however it significantly suffers from scale drift. Depth estimation in a monocular vision system, which is yet a challenging factor, is relevant to this drift issue and hence monocular SLAM remains unsuitable for large scale mapping and localization. This paper presents a novel solution, a wearable and embedded EMoVI-SLAM system, to resolve scale drift through multi-sensor fusion architecture for integrating visual and inertial data, using monocular SLAM as basis of a visual framework. Firstly, the unknown scale parameter in a monocular vision system is addressed based on the IMU measurements, meanwhile gravity direction and gyroscope bias is initialized. Secondly, the estimated pose from monocular visual sensor and the IMU sensor is fused together using Unscented Kalman Filter (UKF). Furthermore, to minimize scale drift, the scale is re-computed after IMU bias errors exceeds the safe threshold limit. Finally, the experiments are carried out by mounting embedded SLAM system on a head-gear in two different test-environments for indoor and outdoor large-scale motion as well as on EuRoC dataset. Experiment results shows that proposed algorithm performs better than the state of the art visual inertial SLAM systems.	algorithm;embedded system;experiment;geographic coordinate system;gyroscope;kalman filter;loose coupling;sensor;simultaneous localization and mapping;wearable computer	Sami Ullah;Bowen Song;Weidong Chen	2018	2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)	10.1109/ROMAN.2018.8525794	inertial measurement unit;computer vision;kalman filter;architecture;simultaneous localization and mapping;artificial intelligence;monocular;scale parameter;monocular vision;computer science;gyroscope	Robotics	53.59173302384208	-40.18788714815415	24930
887401904cd72a4fa657a86abc26afe012e3fe81	region tracking using perspective motion model		"""The video (or motion picture) media has recently grown rapidly and become widely popular such that it is now being used by various desktop PC applications. Among these applications of the video media, there are video conferencing, visual surveillance, agriculture automation, medical imaging, vision-based control, and so on. One of the issues that is becoming continually more important in these applications is region tracking. There are two traditional region tracking methods: the dense correspondence method and the contour tracking method. The dense correspondence method is computationally too expensive because of its full-search correspondence computation. The contour tracking method also is computationally expensive due to so many iterations and have a danger of running into local minima. The widely used a$ne motion model is the simplest and most general motion model for tracking in 2D space. Black and Jepson [1] proposed an eigentracking method with the a$ne motion model and used multi-scale eigenspace representation of a target region template. However, the a$ne motion model cannot track the deformation such as the deformation from rectangles to general quadrangles. The perspective (or projective) motion model can be one possible solution. Poelman and Kanade [2] suggested a method of shape and motion recovery. Steinbach et al. [3] proposed a 3D motion estimation from multi-frame image sequence using the perspective motion model. The studies mentioned thus far have utilized the geometric information of images, such as epipolar lines. In this paper, we suggest a """"tting method of the motion vectors in a target region into the perspective motion model and the selection method of feature points. The perspective motion model is basic and #exible to express 3D real motion. Also, since the proposed method considers the motion vectors only of the feature points in the target region, noises of motion information are reduced and corrected throughout the processes. Fig. 1 shows the overall process of the proposed method."""	analysis of algorithms;computation;desktop computer;epipolar geometry;iteration;maxima and minima;medical imaging;motion estimation;trust region	Song-Ha Choi;Seong-Whan Lee	2000	Pattern Recognition	10.1016/S0031-3203(00)00058-3	machine learning;artificial intelligence;mathematics	Vision	52.053539050044044	-48.35548070353199	24956
b963bac638072284373b7f5e8bcab765e3b336c8	a combined pmht and imm approach to multiple-point target tracking in infrared image sequence	signal image and speech processing;biometrics;infrared imaging;pattern recognition;image processing and computer vision;target tracking	Data association and model selection are important factors for tracking multiple targets in a dense clutter environment. In this paper, we provide an effective solution to the tracking of multiple single-pixel maneuvering targets in a sequence of infrared images by developing an algorithm that combines a sequential probabilistic multiple hypothesis tracking (PMHT) and interacting multiple model (IMM). We explicitly model maneuver as a change in the target’s motion model and demonstrate its effectiveness in our tracking application discussed in this paper. We show that inclusion of IMM enables tracking of any arbitrary trajectory in a sequence of infrared images without any a priori special information about the target dynamics. IMM allows us to incorporate different dynamic models for the targets and PMHT helps to avoid the uncertainty about the observation origin. It operates in an iterative mode using expectation-maximization (EM) algorithm. The proposed algorithm uses observation association as missing data.		Mukesh A. Zaveri;S. N. Merchant;Uday B. Desai	2007	EURASIP J. Image and Video Processing	10.1155/2007/19139	computer vision;speech recognition;tracking system;computer science;archaeology;pattern recognition;biometrics	Vision	46.44943981126083	-47.1578726733173	25013
83e093a07efcf795db5e3aa3576531d61557dd0d	facial landmark localization using robust relationship priors and approximative gibbs sampling		We tackle the facial landmark localization problem as an inference problem over a Markov Random Field. Efficient inference is implemented using Gibbs sampling with approximated full conditional distributions in a latent variable model. This approximation allows us to improve the runtime performance 1000-fold over classical formulations with no perceptible loss in accuracy. The exceptional robustness of our method is realized by utilizing a L1-loss function and via our new robust shape model based on pairwise topological constraints. Compared with competing methods, our algorithm does not require any prior knowledge or initial guess about the location, scale or pose of the face.	approximation algorithm;gibbs sampling;language localisation;latent variable model;markov chain;markov random field;run time (program lifecycle phase)	Karsten Vogt;Oliver Müller;Jörn Ostermann	2015		10.1007/978-3-319-27863-6_34	econometrics;pattern recognition;statistics	Vision	47.06419142623266	-51.05309442488988	25080
41a96c688bf0da4e33299c1d9b289ffcc2fbd2fd	cycle regression analysis: simultaneous estimation of trigonometric components of a time series	trigonometric function;statistical forecasting;time series;simultaneite;ciclo;analisis regresion;simultaneidad;fonction trigonometrique;prevision statistique;simultaneity;serie temporelle;funcion trigonometrica;serie temporal;analyse regression;regression analysis;cycle;prevision estadistica	Abstract   Cycle regression analysis is a continuously evolving family of algorithms that provides the simultaneous estimation of all parameters of a sinusoidal model. The newest members of the family, which add spectral analysis to the nonlinear regression methodology, are introduced in this paper.  The new algorithms have been applied to some of the most classic time-series data sets in the literature. A comparison, of the results of the analysis are made to the results of applying two other forecasting techniques: stepwise estimation procedure and asymptotic maximum likelihood approach. The ability of the cycle regression analysis method to simultaneously estimate parameters is shown to provide an inherent advantage over these other two techniques.  A set of contemporary business data from the M-2 competition is also analyzed using cycle regression analysis and the results presented. One of these new cycle regression analysis algorithms was selected to be included among the forecasting techniques used in the international M-2 competition.	time series	LeRoy F. Simmons;Laurette Poulos Simmons;Mayur Mehta;Vivek Shah	1990	Computers & OR	10.1016/0305-0548(90)90044-8	econometrics;simultaneity;trigonometric functions;calculus;time series;polynomial regression;regression diagnostic;mathematics;regression analysis;statistics	ML	32.40203456000505	-24.15049328863365	25219
fe8df499fee8989844185ddb9b8d9d31ceef7e66	approximations with reweighted generalized belief propagation	generalized belief propagation;article in monograph or in proceedings		approximation;belief propagation;software propagation	Wim Wiegerinck	2005			biology	ML	28.424148039703947	-29.331356104336333	25233
3f374d42d1eab3eb0074051d6c676bb71d77a1c5	deep self-taught learning for remote sensing image classification		This paper addresses the land cover classification task for remote sensing images by deep self-taught learning. Our selftaught learning approach learns suitable feature representations of the input data using sparse representation and undercomplete dictionary learning. We propose a deep learning framework which extracts representations in multiple layers and use the output of the deepest layer as input to a classification algorithm. We evaluate our approach using a multispectral Landsat 5 TM image of a study area in the North of Novo Progresso (South America) and the Zurich Summer Data Set provided by the University of Zurich. Experiments indicate that features learned by a deep self-taught learning framework can be used for classification and improve the results compared to classification results using the original feature representation.	algorithm;deep learning;dictionary;experiment;machine learning;multispectral image;sparse approximation;sparse matrix	Anika Bettge;Ribana Roscher;Susanne Wenzel	2017	CoRR		artificial intelligence;multispectral image;machine learning;land cover;remote sensing;pattern recognition;deep learning;computer science;sparse approximation;contextual image classification	ML	30.187183859031514	-44.660443059841086	25253
4db75bb6d9c061a5d800cfdc5e1e7d879819ce9c	multi-object tracking based on particle probability hypothesis density tracker in microscopic video	video signal processing bayes methods biological techniques biology computing image sequences object tracking optical microscopy sensor fusion;biological system modeling;microscopy;trajectory;shape;target tracking shape trajectory microscopy biological system modeling;pf phd tracking method multiobject tracking particle probability hypothesis density tracker microscopic video biological objects microscopy video automated tracking framework microobject trajectories recursive bayesian state estimation trajectory association ellipse target model shape parameters point like targets orientation constraint model positional constraint model data association crossing trajectories multitarget tracking manual tracking simulated image sequences microtubule movement;target tracking	Research on biological objects requires tracking hundreds of micro-objects from the microscopy video. We propose an automated tracking framework to extract trajectories of micro-objects. This framework uses a particle probability hypothesis density (PF-PHD) tracker to implement a recursive Bayesian state estimation and trajectories association. In the framework, an ellipse target model is presented to describe the micro-objects with shape parameters instead of point-like targets. Furthermore, an orientation and positional constraint model is developed to deal with the data association of crossing trajectories in multitarget tracking. Using this framework, a significantly larger number of tracks are obtained than manual tracking. The experiments on simulated image sequences of microtubule movement are performed in order to evaluate the proposed PF-PHD tracking method.	algorithm;bayesian network;correspondence problem;experiment;preprocessor;recursion;sensor;signal-to-noise ratio;spawn (computing)	Chunmei Shi;Lingling Zhao;Peijun Ma;Xiaohong Su;Junjie Wang;Chiping Zhang	2014	2014 IEEE International Conference on Bioinformatics and Bioengineering	10.1109/BIBE.2014.24	computer vision;simulation;tracking system;shape;microscopy;trajectory;machine learning	Vision	50.478448598019526	-42.15975981288622	25294
bea86a4eac1c67ef2aff3410de3fa4b806db86af	piecewise linear models with guaranteed closeness to the data	model components;modelizacion;piecewise linear model;parametric model;ajustamiento modelo;optimisation;approximation lineaire;piecewise linear;sample size;piecewise linear approximation;classical mle framework;parametric statistics;piecewise linear approximation maximal likelihood estimate mle expectation maximization em kullback leibler divergence kld sparse em;optimizacion;maximum likelihood;piecewise linear techniques;maximal likelihood estimate mle;divergence;tamano muestra;kullback leibler divergence;piecewise linear techniques maximum likelihood estimation maximum likelihood detection piecewise linear approximation parametric statistics parameter estimation pattern recognition;modele lineaire;maximum vraisemblance;linear approximation;taille echantillon;modelo lineal;maximum likelihood estimation;piecewise linear techniques approximation theory maximum likelihood estimation modelling;technique lineaire par morceau;sparse em;ajustement modele;modelisation;approximation theory;maximum likelihood estimate;nonparametric data density;realite terrain;expectation maximization;linearisation morceau;point sets;model matching;linear model;aproximacion lineal;kullback leibler divergence kld;maximum likelihood detection;segment droite;pattern recognition;linearizacion trozo;algorithme em;segmento recta;realidad terreno;ground truth;optimization;teoria mezcla;line segment;algoritmo em;modele donnee;parameter estimation;guaranteed closeness;mixture theory;em algorithm;modeling;theorie melange;nonparametric data density piecewise linear model guaranteed closeness piecewise linear approximation point sets model components maximum likelihood estimation parametric model classical mle framework kullback leibler divergence;piecewise linearization;expectation maximization em;maxima verosimilitud;divergencia;data models;first fit	This paper addresses the problem of piecewise linear approximation of point sets without any constraints on the order of data points or the number of model components (line segments). We point out two problems with the maximum likelihood estimate (MLE) that present serious drawbacks in practical applications. One is that the parametric models obtained using a classical MLE framework are not guaranteed to be close to data points. It is typically impossible, in this classical framework, to detect whether a parametric model fits the data well or not. The second problem is related to accurately choosing the optimal number of model components. We first fit a nonparametric density to the data points and use it to define a neighborhood of the data. Observations inside this neighborhood are deemed informative; those outside the neighborhood are deemed uninformative for our purpose. This provides us with a means to recognize when models fail to properly fit the data. We then obtain maximum likelihood estimates by optimizing the Kullback-Leibler Divergence (KLD) between the nonparametric data density restricted to this neighborhood and a mixture of parametric models. We prove that, under the assumption of a reasonably large sample size, the inferred model components are close to their ground-truth model component counterparts. This holds independently of the initial number of assumed model components or their associated parameters. Moreover, in the proposed approach, we are able to estimate the number of significant model components without any additional computation.	addresses (publication format);areal density (computer storage);assumed;choose (action);clinical act of insertion;computation;convergence (action);data point;electron microscopy;estimated;fits;fibrosis of extraocular muscles, congenital, with synergistic divergence;ground truth;inference;information;kernel;kullback–leibler divergence;linear approximation;mathematical model;mathematics;maximum likelihood estimates;nevus sebaceous;parametric model;piecewise linear continuation;point cloud;population parameter;regular expression;algorithm	Longin Jan Latecki;Marc Sobel;Rolf Lakämper	2009	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2009.13	econometrics;mathematical optimization;mathematics;maximum likelihood;statistics	ML	29.794331370458607	-31.34327036051395	25308
07678529b7cc6b6aabc71ac003eebf4b15040658	spectrally optimal factorization of incomplete matrices	minimisation;singular value decomposition computer vision iterative methods minimisation;optimal solution;bilinear model;observation matrix;cost function;young diagram;approximation algorithms;singular value;singular value decomposition;iterative algorithm;matrix decomposition iterative algorithms cost function computer vision singular value decomposition large scale systems approximation algorithms robot vision systems motion estimation photometry;computer vision;low rank matrix;iterative methods;distance measurement;computational modeling;young diagram spectrally optimal incomplete matrix factorization computer vision bilinear model singular value decomposition observation matrix iterative local refinement cost function minimization problem low rank matrix;minimization problem;spectrally optimal incomplete matrix factorization;matrix decomposition;global optimization;optimization;structure from motion;iterative local refinement;spectral norm	From the recovery of structure from motion to the separation of style and content, many problems in computer vision have been successfully approached by using bilinear models. The reason for the success of these models is that a globally optimal decomposition is easily obtained from the singular value decomposition (SVD) of the observation matrix. However, in practice, the observation matrix is often incomplete, the SVD can not be used, and only suboptimal solutions are available. The majority of these solutions are based on iterative local refinements of a given cost function, and lack any guarantee of convergence to the global optimum. In this paper, we propose a globally optimal solution, for particular patterns of missing entries. To achieve this goal, we re-formulate the problem as the minimization of the spectral norm of the matrix of residuals, i.e., we seek the completion of the observation matrix such that the largest singular value of its difference to a low rank matrix is the smallest possible. The class of patterns of missing entries we deal with is known as the Young diagram, which includes, as particular cases, many relevant situations, such as the missing of an entire submatrix. We describe experiments that illustrate how our globally optimal solution has impact in practice.	algorithm;bilinear filtering;computation;computer vision;diagram;experiment;global optimization;iterative method;loss function;maxima and minima;singular value decomposition;small private online course;structure from motion;the matrix	Pedro M. Q. Aguiar;João M. F. Xavier;Marko Stosic	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587675	mathematical optimization;combinatorics;discrete mathematics;mathematics;iterative method;global optimization	Vision	51.61683296739153	-50.47775858826479	25392
c3aeaf2977501425f6d6fa2b8e12ae43cf8ea32b	tire classification from still images and video	all season;studded;frequency analysis;support vector machines;snow;edge detection;edge map;training;tire classification;image classification;classification;law;tire treads;approximation theory;binary edge map frequency representation still images all season tires snow tires studded tires summer tires tire usage law violation infringers automatic tire classification method video frame sequence support vector machine classifier svm classifier feature extraction training images two stage process tire tread image representation low dimensional approximation principal component analysis pca robustness improvement illumination changes perspective changes;summer tires tire classification principal component analysis support vector machine frequency analysis edge map all season snow studded;vectors;tires training feature extraction vectors snow principal component analysis support vector machines;snow tires;image representation;summer tires;feature extraction;principal component analysis;tyres;tires;laws;video signals;lighting;support vector machine;video;video signals approximation theory edge detection feature extraction image classification image representation image sequences law lighting principal component analysis support vector machines tyres;studded tires;image sequences	The use of different types of tires (e.g., all-season, snow, studded, summer) is regulated by law in several states and countries. Violation of tire usage laws typically results in substantial fines for infringers. In this paper, we propose an automated method to classify tires into snow, all-season and summer tires from still images or from a sequence of video frames. Our method first trains a Support Vector Machine (SVM) classifier on features extracted from a set of training images. Classification of test tire images is a two-stage process that entails feature extraction and tire classification based on the processing of the extracted features by the previously trained SVM classifier. The principle underlying the feature extraction stage is the representation of tire images via a low-dimensional approximation obtained from Principal Component Analysis (PCA). In order to improve robustness to changes in illumination and perspective, the features are extracted from the frequency representation of the binary edge map of the tire tread image. Our experimental results show that the proposed method achieves high classification accuracy.	approximation;feature extraction;frame (video);principal component analysis;support vector machine	Orhan Bulan;Edgar A. Bernal;Robert P. Loce;Wencheng Wu	2012	2012 15th International IEEE Conference on Intelligent Transportation Systems	10.1109/ITSC.2012.6338693	computer vision;simulation;engineering;forensic engineering	Vision	39.88810815641706	-46.42599588194005	25393
5bed2453a5b0c54a4a4a294f29c9658658a9881e	angular-similarity-preserving binary signatures for linear subspaces	locality sensitive hashing binary signature angular similarity;nearest neighbor searches;hamming distance face recognition nearest neighbor searches face measurement linear matrix inequalities;locality sensitive hashing;measurement;binary signature;face recognition;hamming distance;gesture recognition computer vision face recognition;action recognition linear subspace angular similarity preserving binary signature method computer vision pattern recognition hamming distance face recognition gesture recognition;期刊论文;face;angular similarity;linear matrix inequalities	We propose a similarity-preserving binary signature method for linear subspaces. In computer vision and pattern recognition, linear subspace is a very important representation for many kinds of data, such as face images, action and gesture videos, and so on. When there is a large amount of subspace data and the ambient dimension is high, the cost of computing the pairwise similarity between the subspaces would be high and it requires a large storage space for storing the subspaces. In this paper, we first define the angular similarity and angular distance between the subspaces. Then, based on this similarity definition, we develop a similarity-preserving binary signature method for linear subspaces, which transforms a linear subspace into a compact binary signature, and the Hamming distance between two signatures provides an unbiased estimate of the angular similarity between the two subspaces. We also provide a lower bound of the signature length sufficient to guarantee uniform distance-preservation between every pair of subspaces in a set. Experiments on face recognition, gesture recognition, and action recognition verify the effectiveness of the proposed method.	angularjs;biologic preservation;computation (action);computer vision;data cube;electronic signature;experiment;facial recognition system;gesture recognition;hamming distance;hearing loss, high-frequency;negativity (quantum mechanics);pattern recognition;social inequality;type signature;orders - hl7publishingdomain;videocassette	Jianqiu Ji;Jianmin Li;Qi Tian;Shuicheng Yan;Bo Zhang	2015	IEEE Transactions on Image Processing	10.1109/TIP.2015.2451173	facial recognition system;face;hamming distance;speech recognition;computer science;machine learning;pattern recognition;mathematics;locality-sensitive hashing;signature recognition;measurement;statistics	Vision	31.349692882981195	-40.407571406488316	25410
8d5bdcd693f7313679c3adb0a228ff48a247bcee	a probabilistic framework for segmentation and tracking of multiple non rigid objects for video surveillance	video surveillance;probability;image segmentation;video signal processing;new object initialization moving object segmentation event recognition pixel segmentation object blob matching multiple nonrigid object tracking probabilistic video segmentation method video surveillance multiple foreground objects static monocular camera probabilistic information combining segmented foreground objects nonbipartite matching problem;surveillance;image matching;bipartite matching;surveillance image segmentation image matching probability video signal processing;video surveillance biological system modeling humans histograms cameras probability object segmentation markov random fields labeling object detection	This paper presents a probabilistic framework for segmenting and tracking multiple non rigid foreground objects for video surveillance, using a static monocular camera. The algorithm combines information in a probabilistic sense and poses the problem of matching the segmented foreground objects with blobs in the next frame as a non bipartite matching problem. To solve this problem, probability is calculated for each possible matching. Initialization of new objects is also treated in a probabilistic manner. The new framework is shown to be able to handle a greater set of difficult situations and to improve performance significantly.	algorithm;closed-circuit television;initialization (programming);matching (graph theory)	Aleksandar Ivanovic;Thomas S. Huang	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1418763	computer vision;bipartite graph;computer science;machine learning;segmentation-based object categorization;video tracking;pattern recognition;probability;mathematics;image segmentation;scale-space segmentation;statistics	Vision	45.78701837198945	-49.68749298531131	25458
3b4a880c116d6eb40d2137682a5f0deca3b3be49	approach to estimation and prediction for normal boiling point (nbp) of alkanes based on a novel molecular distance-edge (mde) vector			model-driven engineering;neutral body posture	Shushen Liu;Chenzhong Cao;Zhiliang Li	1998	Journal of Chemical Information and Computer Sciences	10.1021/ci970109z	econometrics;simulation;chemistry	Theory	34.59964530829682	-34.56802475347407	25486
34cf128129926041976e47a1090575ba98478edd	a real-time stereo depth extraction hardware for intelligent home assistant robot	robot sensing systems;video sequence;depth map extraction algorithm;sum of absolute difference;consumer product;robot technology industry;median filter;intelligent robots;real time;median filter real time stereo depth extraction hardware intelligent home assistant robot consumer product robot technology industry obstacles avoidance stereo vision system video sequence depth map extraction algorithm;service robots;real time stereo depth extraction hardware;human robot interaction;stereo vision system;depth extraction intelligent home assistant robot human robot interaction sum of absolute differences;robot vision;energy consumption;feature extraction;pixel;stereo image processing;stereo vision;intelligent home assistant robot;next generation;pixel robot sensing systems real time systems service robots hardware;depth extraction;collision avoidance;consumer products;obstacles avoidance;depth map;stereo image processing collision avoidance consumer products feature extraction home automation human robot interaction image sequences intelligent robots median filters real time systems robot vision;extraction method;median filters;home automation;hardware;sum of absolute differences;real time systems;image sequences	One of the fastest growing next generation consumer products is a robot technology industry and it is rapidly forming a huge market. A depth extraction is one of the key techniques to adopt a home assistant robot for avoiding obstacles and looking for a transfer route. However, a home assistant robot mounted stereo vision systems produce unpredictable changes in video sequences when the robot is walking and needs hard computation to produce an accurate estimation. In this paper, we propose hardware based realtime stereo depth extraction method for an intelligent home assistant robot. For fast adaption of the external environment, we present depth map extraction algorithm by using preprocessing, parallel prediction searching employing median filter. Experimental results show that the proposed method reduces the processing time and the energy consumption compared to the conventional methods. This implementation is suitable for adaptive real-time depth extraction that is compatible with current robot applications.	algorithm;computation;depth map;depth perception;emoticon;fastest;field-programmable gate array;hardware description language;home automation;median filter;next-generation network;peripheral;preprocessor;prototype;real-time clock;robot;stereo camera;stereopsis;verilog	Dong-Sun Kim;Sang-Seol Lee;Byeong-Ho Choi	2010	IEEE Transactions on Consumer Electronics	10.1109/TCE.2010.5606326	human–robot interaction;median filter;sum of absolute differences;embedded system;home automation;computer vision;simulation;feature extraction;computer science;engineering;stereopsis;final good;pixel;depth map	Robotics	44.775679977595814	-35.549799379561215	25575
1e548536aad7f24535122d1dd61c6dca3fc6d67e	multi-hierarchical independent correlation filters for visual tracking		For visual tracking, most of the traditional correlation filters (CF) based methods suffer from the bottleneck of feature redundancy and lack of motion information. In this paper, we design a novel tracking framework, called multihierarchical independent correlation filters (MHIT). The framework consists of motion estimation module, hierarchical features selection, independent CF online learning, and adaptive multi-branch CF fusion. Specifically, the motion estimation module is introduced to capture motion information, which effectively alleviates the object partial occlusion in the temporal video. The multi-hierarchical deep features of CNN representing different semantic information can be fully excavated to track multi-scale objects. To better overcome the deep feature redundancy, each hierarchical features are independently fed into a single branch to implement the online learning of parameters. Finally, an adaptive weight scheme is integrated into the framework to fuse these independent multi-branch CFs for the better and more robust visual object tracking. Extensive experiments on OTB and VOT datasets show that the proposed MHIT tracker can significantly improve the tracking performance. Especially, it obtains a 20.1% relative performance gain compared to the top trackers on the VOT2017 challenge, and also achieves new state-of-the-art performance on the VOT2018 challenge.	bittorrent tracker;clutter;curse of dimensionality;experiment;gaussian blur;image resolution;java platform, standard edition;motion estimation;online machine learning;orfeo toolbox;surround sound;video tracking	Shuai Bai;Zhiqun He;Ting-Bing Xu;Zheng Zhu;Yuan Dong;Hongliang Bai	2018	CoRR			Vision	30.31686175458936	-50.737963820291576	25585
52b058d928bd76ac54129ca11f9a534839213818	multiframe scene flow with piecewise rigid motion		We introduce a novel multiframe scene flow approach that jointly optimizes the consistency of the patch appearances and their local rigid motions from RGB-D image sequences. In contrast to the competing methods, we take advantage of an oversegmentation of the reference frame and robust optimization techniques. We formulate scene flow recovery as a global non-linear least squares problem which is iteratively solved by a damped Gauss-Newton approach. As a result, we obtain a qualitatively new level of accuracy in RGB-D based scene flow estimation which can potentially run in real-time. Our method can handle challenging cases with rigid, piecewise rigid, articulated and moderate non-rigid motion, and does not rely on prior knowledge about the types of motions and deformations. Extensive experiments on synthetic and real data show that our method outperforms state-of-the-art.	experiment;gauss–newton algorithm;linear least squares (mathematics);mathematical optimization;newton;non-linear least squares;real-time clock;reference frame (video);robust optimization;synthetic intelligence	Vladislav Golyanik;Kihwan Kim;Robert Maier;Matthias Nießner;Didier Stricker;Jan Kautz	2017	2017 International Conference on 3D Vision (3DV)	10.1109/3DV.2017.00039	robust optimization;piecewise;reference frame;non-linear least squares;computer vision;mathematical optimization;rgb color model;congruence (geometry);mathematics;least squares;artificial intelligence	Vision	52.367484849163375	-48.55606268523967	25626
6a5c74bf22d59b3a44e1813e6b958443c3c0d40d	non-adaptive pooling strategies for detection of rare faulty items	compressed sensing;sparse matrices compressed sensing fault diagnosis signal reconstruction;non adaptive pooling strategy compressed genotyping genetic screening signal reconstruction sparse binary pooling matrix n dimensional signal binary sparse signal fault detection rare faulty item;signal reconstruction;testing compressed sensing noise algorithm design and analysis vectors noise measurement sparse matrices;sparse matrices;fault diagnosis	We study non-adaptive pooling strategies for detection of rare faulty items. Given a binary sparse N dimensional signal x, how to construct a sparse binary M × N pooling matrix F such that the signal can be reconstructed from the smallest possible number M of measurements y = Fx? We show that a very small number of measurements is possible for random spatially coupled design of pools F. Our design might find application in genetic screening or compressed genotyping. We show that our results are robust with respect to the uncertainty in the matrix F when some elements are mistaken.	backpropagation;compressed sensing;genetic algorithm;robustness (computer science);sparse matrix;the matrix	Pan Zhang;Florent Krzakala;Marc Mézard;Lenka Zdeborová	2013	2013 IEEE International Conference on Communications Workshops (ICC)	10.1109/ICCW.2013.6649458	signal reconstruction;sparse matrix;computer science;theoretical computer science;machine learning;pattern recognition;sparse approximation;mathematics;compressed sensing	Robotics	34.550227722430186	-29.26970786978316	25645
7ad34c3c9647bbfd9e404f802a21fee9e1029f13	incremental sparse bayesian ordinal regression	basis function-based method;ordinal regression;sparse bayesian learning	Ordinal Regression (OR) aims to model the ordering information between different data categories, which is a crucial topic in multi-label learning. An important class of approaches to OR models the problem as a linear combination of basis functions that map features to a high-dimensional non-linear space. However, most of the basis function-based algorithms are time consuming. We propose an incremental sparse Bayesian approach to OR tasks and introduce an algorithm to sequentially learn the relevant basis functions in the ordinal scenario. Our method, called Incremental Sparse Bayesian Ordinal Regression (ISBOR), automatically optimizes the hyper-parameters via the type-II maximum likelihood method. By exploiting fast marginal likelihood optimization, ISBOR can avoid big matrix inverses, which is the main bottleneck in applying basis function-based algorithms to OR tasks on large-scale datasets. We show that ISBOR can make accurate predictions with parsimonious basis functions while offering automatic estimates of the prediction uncertainty. Extensive experiments on synthetic and real word datasets demonstrate the efficiency and effectiveness of ISBOR compared to other basis function-based OR approaches.	algorithm;basis function;bayesian network;categories;estimated;experiment;hyperactive behavior;increment;marginal model;mathematical optimization;multi-label classification;nonlinear system;numerous;occam's razor;ordinal position;ordinal data;ordinal regression;sparse matrix;synthetic intelligence	Chang Li;Maarten de Rijke	2018	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2018.07.015	mathematics;artificial intelligence;machine learning;marginal likelihood;ordinal number;ordinal regression;linear combination;basis function;matrix (mathematics);bottleneck;bayesian probability	ML	26.196546122208787	-34.435110904903475	25646
54c868a760d37b35429baf4985cdd7a72d166f93	detecting dominant points on online scripts with a simple approach	online scripts dominant point detection real time response polygon pen movement trace handwritten character recognition;real time;computational geometry handwritten character recognition real time systems;computational geometry;turning handwriting recognition fluctuations robustness detectors computational efficiency computational complexity conferences application software computer graphics;turning point;computational efficiency;handwritten character recognition;real time systems	We proposed a new dominant point detection method. It has the following advantages: robustness, computational efficiency, and real-time response to pen movement. We construct a variable that is the ratio of the height to the width of an imagined rectangle whose bottom coincides with the polygon enclosed by the pen movement trace, and area is equal to the polygonal area. For we are on-line watching whether the value of this variable exceeds a given threshold, we can find dominant points in real-time. Only when the fluctuation comparing to the scale of a curve is big enough, value of this variable can exceed the given threshold. By this way, pseudo turning points can be rejected. As a new point comes in, only a small number of computations are needed to update the value of this variable. Effectiveness of this method was confirmed by experiments.	computation;experiment;online and offline;quantum fluctuation;real-time clock;sensor	Su Yang;Guozhong Dai	2002		10.1109/IWFHR.2002.1030935	computer vision;speech recognition;computational geometry;computer science;artificial intelligence;machine learning	AI	47.275692153257985	-44.47659414082776	25691
45a7fb0be16689ff1ce0988d88ba58e13de061fc	fpga implementation of hardware processing modules as coprocessors in brain-machine interfaces	neural nets brain computer interfaces coprocessors field programmable gate arrays kalman filters;field programmable gate array;random access memory;decoding;neural nets;sorting;kalman filters;kalman filter;coprocessors;spike sorting;motor cortex;computer architecture;fpga implementation;lever press;brain machine interface;field programmable gate arrays decoding sorting hardware computer architecture kalman filters random access memory;brain computer interfaces;field programmable gate arrays;probabilistic neural network;high performance;action potentials brain humans man machine systems neural networks computer probability;neural signal computation fpga implementation hardware processing modules coprocessors brain machine interfaces real time computation portability flexibility bmi hpm field programmable gate array spike sorting probabilistic neural network pnn neural ensemble decoding kalman filter lever pressing task water rewards parallelism feature computation time matlab implementations;real time computing;hardware	Real-time computation, portability and flexibility are crucial for practical brain-machine interface (BMI) applications. In this work, we proposed Hardware Processing Modules (HPMs) as a method for accelerating BMI computation. Two HPMs have been developed. One is the field-programmable gate array (FPGA) implementation of spike sorting based on probabilistic neural network (PNN), and the other is the FPGA implementation of neural ensemble decoding based on Kalman filter (KF). These two modules were configured under the same framework and tested with real data from motor cortex recording in rats performing a lever-pressing task for water rewards. Due to the parallelism feature of FPGA, the computation time was reduced by several dozen times, while the results are almost the same as those from Matlab implementations. Such HPMs provide a high performance coprocessor for neural signal computation.	artificial neural network;biological neural networks;brain neoplasms;brain–computer interface;cerebral cortex;computation (action);coprocessor;field-programmability;field-programmable gate array;interface device component;kalman filter;matlab;neural ensemble;parallel computing;probabilistic neural network;rna processing, post-transcriptional;real-time transcription;rewards;sorting;time complexity;perineuronal net (cell component)	Yaoyao Hao;Xiaoping Zhu;Ting Zhao;Yiwen Wang;Yaowu Chen;Weidong Chen;Xiaoxiang Zheng	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6091142	kalman filter;brain–computer interface;parallel computing;computer hardware;computer science;theoretical computer science;machine learning;artificial neural network;field-programmable gate array	Robotics	41.6069111004873	-26.317504822609415	25703
7e481a07f829b9f9bb5d3f28eb7bbaff0558014a	2d articulated human pose tracking: a hybrid approach	hybrid tracking framework 2d articulated human pose tracking low level feature matching high level object matching object detector brightness constancy assumption profile pose detector frontal pose detector fusion strategy decoding strategy pose estimation articulated joint tracking;detectors estimation joints three dimensional displays cameras principal component analysis;object tracking decoding image coding image fusion image matching object detection	In tracking, there are two fundamental ways to solve the correspondence problem, either as a low-level feature matching or through high-level object matching. Most of the 2D pose tracking methods are based on high-level object matching. This makes them highly dependent on the object detectors, which are typically trained in specific views, limiting pose trackers to those view-points only. We propose a systematic approach for 2D pose tracking that combines low-level feature matching and high-level object matching approaches in a unified framework. We utilized brightness constancy assumption to find the corresponding pixels in two consecutive frames. We combine this tracking with frontal and profile pose detectors through a decoding and fusion strategy, to enable continuous pose estimation and tracking over wide range of view-points. The added advantage of our approach is, we not only track each limb, we can also track an articulated joint between them without requiring any 3D estimate of the skeleton. In addition to being computationally efficient, this hybrid tracking framework generalizes to unseen pose variations and compares favorably with existing work.	3d pose estimation;algorithmic efficiency;correspondence problem;high- and low-level;pixel;sensor;unified framework	Ali Hassan;Murtaza Taj	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025307	computer vision;simulation;pose;3d pose estimation;pattern recognition;articulated body pose estimation	Vision	42.457338506252746	-50.444287041614935	25817
8f5450037cba1ba1f5c2f73fa4ffa66558eae5bd	modelling relational data using bayesian clustered tensor factorization		We consider the problem of learning probabilistic models fo r c mplex relational structures between various types of objects. A model can hel p us “understand” a dataset of relational facts in at least two ways, by finding in terpretable structure in the data, and by supporting predictions, or inferences ab out whether particular unobserved relations are likely to be true. Often there is a t radeoff between these two aims: cluster-based models yield more easily interpret abl representations, while factorization-based approaches have given better pr edictive performance on large data sets. We introduce the Bayesian Clustered Tensor Factorization (BCTF) model, which embeds a factorized representation of relatio ns in a nonparametric Bayesian clustering framework. Inference is fully Bayesia n but scales well to large data sets. The model simultaneously discovers interp retable clusters and yields predictive performance that matches or beats previo us probabilistic models for relational data.	bayesian network;cluster analysis;mega city (the matrix)	Ilya Sutskever;Ruslan Salakhutdinov;Joshua B. Tenenbaum	2009			machine learning;pattern recognition;data mining;mathematics;statistics	ML	27.923379945531547	-31.70712615720005	25822
9136584da21a431298b0ea123235ad6e5b4da791	study of data fusion of ais and radar	target tracking data fusion;automatic identification system;radar tracking;extrapolation;rada;data fusion;fuzzy association data fusion target tracking automatic identification system ais rada;automatic identification system ais;fuzzy fusion extrapolation method;fuzzy set theory;fuzzy fusion extrapolation method target tracking data fusion ais radar automatic identification system;ais;marine vehicles;global positioning system;radar antennas;target tracking extrapolation fuzzy set theory radar tracking sensor fusion;fuzzy association;sensor fusion;target tracking;radar	In this paper, the function and necessity of target data fusion of radar and Automatic Identification System (AIS) are discussed. The characteristic and difference of tracking performance, target data category and precision between radar and AIS are analyzed. We propose a fuzzy fusion extrapolation method for target tracking data fusion processing based on AIS and radar. The proposed method can improve the performance and stability of vessel traffic service by AIS.	automatic identification and data capture;extrapolation;interference (communication);radar;vehicle tracking system	Liu Chang;Shi Xiaofei	2009	2009 International Conference of Soft Computing and Pattern Recognition	10.1109/SoCPaR.2009.133	computer vision;automatic identification system;computer science;data mining;sensor fusion	Robotics	50.378536290859145	-33.464202009156935	25828
3fd325cb4972b129c7b635e585348eeef030e101	markerless tracking algorithm based on 3d model for augmented reality system	augmented reality system;feature matching;key frame selection problem;stratified reconstruction;sift operator;prior knowledge;real time;markerless augmented reality	We present a markerless augmented reality(AR) system based on 3D model. First, the feature of environment was extracted using SIFT operator, then the method of stratified reconstruction was used to reconstruct the 3D scene, after that we constructed the database of prior knowledge using the KD-Tree. Finally, we tracked the 3D model based on these prior knowledge via feature matching and pose estimation in real time. Experimental results demonstrated that this method is sufficient for markerless tracking registration. With the prior knowledge, key frame selection problem can be avoided and the running speed is also increased.	3d modeling;algorithm;augmented reality	Peng Yao;Can Chen;Dongdong Weng	2012		10.1007/978-3-642-36669-7_91	computer vision;simulation;geography;computer graphics (images)	Vision	50.02629398632213	-46.502683700383166	25832
71e95c3a31dceabe9cde9f117615be8bf8f6d40e	a probabilistic approach to realistic face synthesis	realistic rendering face modeling probabilistic approach face synthesis;face synthesis;realistic face synthesis;probability avatars face recognition;probability;realistic rendering;skin;avatar creation;probabilistic approach;face computational modeling pixel lighting probabilistic logic shape skin;de identification;computational modeling;face recognition;shape;compact representation;face normalization;pixel;generic face specular map;face modeling;avatars;face;probabilistic face diffuse model;lighting;probabilistic logic;de identification probabilistic approach realistic face synthesis probabilistic face diffuse model generic face specular map face normalization avatar creation	This paper presents a novel approach to face modeling for realistic synthesis, powered by a probabilistic face diffuse model and a generic face specular map. We first construct a probabilistic face diffuse model for estimating the albedo and the normals of a face from an unknown input image. Then, we introduce a generic face specular map for estimating the specularity of the face. Using the estimated albedo, normal and specular information, we can synthesize the face under arbitrary lighting and viewing directions realistically. Unlike many existing face modeling techniques, our approach can retain both the diffuse and specular properties of the face without involving an elaborating 3D matching procedure. Thanks to the compact representation and the effective inference scheme, our technique can be applied to many practical applications, such as face normalization, avatar creation and de-identification.	3-dimensional matching;de-identification;specularity	Hyunjung Shim;Inwoo Ha;Taehyun Rhee;James Dokyoon Kim;Chang-Yeong Kim	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5653024	facial recognition system;face;computer vision;simulation;shape;computer science;probability;lighting;mathematics;skin;probabilistic logic;computational model;pixel	Vision	47.29406191282506	-51.30115394616925	25850
273c100a77f7a52c5d468c1a8408cdad9413e9b3	estimating average-case learning curves using bayesian, statistical physics and vc dimension methods	bayesian statistics;learning curve;vc dimension	Michael Kearns· AT&T Bell Laboratories Murray Hill, New Jersey Robert Schapire AT&T Bell Laboratories Murray Hill, New Jersey In this paper we investigate an average-case model of concept learning, and give results that place the popular statistical physics and VC dimension theories of learning curve behavior in a common framework.	best, worst and average case;concept learning;vc dimension	David Haussler;Michael Kearns;Manfred Opper;Robert E. Schapire	1991			econometrics;vc dimension;computer science;machine learning;mathematics;bayesian statistics;learning curve;statistics	ML	25.68777273525573	-24.000973958746663	25860
b1fe3695a3ce4a65095a1adff1b93377dd4c2c8b	a discriminative null space based deep learning approach for person re-identification	null space;person re identification;metric learning;scnn;null foley sammon transform	Person re-identification across multiple camera views is a rather challenging task due to various view points, illuminations, backgrounds and poses. How to extract discriminative features is the most critical way to overcome these challenges. In this paper, we design a discriminative null space based deep learning approach for person re-identification. Firstly, a Siamese Convolutional Neural Network (SCNN) is designed to automatically learn effective semantic features for person re-identification in different camera views. Furthermore, to obtain better recognition performance, we adopt the Null Foley-Sammon Transform (NFST) metric learning approach to combine the low-level, mid-level features and high-level features learned by the SCNN in a new discriminative null space. In this null space, images of the same person are collapsed into a single point thus minimizing the within-class scatter to the extreme and maximizing the relative between-class separation simultaneously. Finally, the comprehensive evaluations demonstrate that our approach outperforms all state-of-the-art methods on the Market-1501, which is the world's largest person re-identification benchmark dataset.	benchmark (computing);convolutional neural network;deep learning;high- and low-level;kernel (linear algebra);sammon mapping	Shuangqun Li;Xinchen Liu;Wu Liu;Huadong Ma;Haitao Zhang	2016	2016 4th International Conference on Cloud Computing and Intelligence Systems (CCIS)	10.1109/CCIS.2016.7790306	computer vision;kernel;machine learning;pattern recognition	Vision	29.997045732688527	-50.37599292986138	25878
42369c4d48c9920b9f34f33260b6bd9650fde261	a research on the qr code recognition improvement using the cloud-based pre-generated image matching scheme	image recognition;servers image recognition image matching cameras shape mobile handsets;qr code recognition method cloud based pregenerated image matching scheme quick response code two dimensional code smartphone market;image matching;servers;shape;mobile handsets;cloud computing qr code recognition;smart phones cloud computing image coding image matching qr codes;cameras	This paper describes a method to recognize a Quick Response Code (QR) a novel. The QR Code is a two-dimensional code, which is currently used in various fields. According to the salient growth smartphone market, the recognition distance of QR Code is increased but the recognition angle is still limited. To tackle the issue, we propose a QR Code recognition method using `cloud-based pre-generated image matching'. Our experimental results show the efficiency of the proposed method.	cloud computing;image registration;qr code;smartphone	Misun Ahn;Seunghyun Hong;Sungwon Lee	2015	2015 International Conference on Information Networking (ICOIN)	10.1109/ICOIN.2015.7057912	embedded system;computer vision;shape;computer science;theoretical computer science;server	Robotics	40.780550274250345	-51.2762881205097	25958
317d42ec9d872445d7ea8373cf97c11e512d56d6	a line feature based slam with low grade range sensors using geometric constraints and active exploration for mobile robot	mobile robot;slam;line feature;satisfiability;geometric constraint;covariance matrices;feature extraction;indoor environment;hough transform;navigation system;ekf;infrared;extended kalman filter;geometric constraints;environmental modeling	This paper describes a geometrically constrained Extended Kalman Filter (EKF) framework for a line feature based SLAM, which is applicable to a rectangular indoor environment. Its focus is on how to handle sparse and noisy sensor data, such as PSD infrared sensors with limited range and limited number, in order to develop a low-cost navigation system. It has been applied to a vacuum cleaning robot in our research. In order to meet the real-time objective with low computing power, we develop an efficient line feature extraction algorithm based upon an iterative end point fit (IEPF) technique assisted by our constrained version of the Hough transform. It uses a geometric constraint that every line is orthogonal or parallel to each other because in a general indoor setting, most furniture and walls satisfy this constraint. By adding this constraint to the measurement model of EKF, we build a geometrically constrained EKF framework which can estimate line feature positions more accurately as well as allow their covariance matrices to converge more rapidly when compared to the case of an unconstrained EKF. The experimental results demonstrate the accuracy and robustness to the presence of sensor noise and errors in an actual indoor environment.	algorithm;converge;extended kalman filter;feature extraction;hough transform;image noise;iterative method;mobile robot;plasma cleaning;real-time clock;sensor;simultaneous localization and mapping;sparse matrix;vacuum cleaner	Young-Ho Choi;Tae-Kyeong Lee;Se-Young Oh	2008	Auton. Robots	10.1007/s10514-007-9050-y	computer vision;simulation;computer science;machine learning;extended kalman filter	Robotics	53.19345487340498	-35.792926915098114	25989
c88cbfad533f75c2bf133ac39b1296253976be56	online visual tracking via two view sparse representation	iterative methods online visual tracking two view sparse representation method online tracking method object construction state model construction object templates;visual tracking object model sparse representation state model;visualization;vectors;optimisation computer vision iterative methods optical tracking;linear programming;optimization;signal processing algorithms;vectors signal processing algorithms optimization educational institutions linear programming tracking visualization;tracking	In this letter, we present a novel online tracking method based on sparse representation. In contrast to existing “sparse representation”-based tracking algorithms, this work adopts the sparse representation method to construct both object and state models. The tracked object can be sparsely represented by a series of object templates, and also can be sparsely represented by candidate samples in the current frame. Furthermore, we propose a unified objective function to integrate object and state models, and cast the tracking problem as an optimization problem that can be solved in an iteration manner. Finally, we compare the proposed tracker with nine state-of-the-art tracking methods by using some challenging image sequences. Both qualitative and quantitative evaluations demonstrate that our tracker achieves favorable performance in terms of both accuracy and speed.	algorithm;iteration;mathematical optimization;optimization problem;sparse approximation;sparse matrix	Dong Wang;Huchuan Lu;Chunjuan Bo	2014	IEEE Signal Processing Letters	10.1109/LSP.2014.2322389	computer vision;visualization;computer science;linear programming;theoretical computer science;machine learning;video tracking;mathematics;tracking	Vision	46.78724135614992	-49.809441471799246	26062
7a2aaffbace52e37357fc8afa1aca00ba162f287	multi-view face pose estimation based on supervised isa learning	face pose estimation;optimisation;supervised learning;face recognition;optimisation face recognition learning artificial intelligence;face recognition system multi view face pose estimation supervised isa learning independent subspace analysis multi view face examples isa view subspaces supervised method;independent subspace analysis;learning artificial intelligence;multi view;instruction sets character generation face recognition;pose estimation	Independent subspace analysis (ISA) is able to learn view-subspaces unsupervisedly from (view-unlabeled) multi-view face examples (S.Z. Li et al., 2001). We explain underlying reasons for the emergent formation of ISA view-subspaces. Based on the analysis, we present a supervised method for more effective learning of view-subspace, assuming that view-labeled face examples are available. The models thus learned give more accurate pose estimation than those obtained with the unsupervised ISA.	3d pose estimation	Stan Z. Li;HongJiang Zhang;Xianhuan Peng;XinWen Hou;QianSheng Cheng	2002		10.1109/AFGR.2002.1004140	facial recognition system;computer vision;pose;computer science;machine learning;pattern recognition;supervised learning	Vision	27.099017298576673	-45.16019412282765	26081
735840e1fd9701b759b7d9aec0d8d57eeb14dc7b	conjunctive patches subspace learning with side information for collaborative image retrieval	semantics measurement radio frequency image retrieval visualization geometry manifolds;optimisation;measurement;subspace learning;manifolds;real world image database collaborative image retrieval conjunctive patches subspace learning side information content based image retrieval image management relevance feedback schemes semantic gap pairwise constraints computer vision face recognition image classification subspace learning framework discriminative information labeled log images constrained optimization problem;geometry;semantics;image classification;journal article;log data;computer vision;visualization;radio frequency;face recognition;document image processing;visual databases computer vision document image processing face recognition image classification image retrieval optimisation;subspace learning collaborative image retrieval cir log data side information;side information;collaborative image retrieval cir;visual databases;image retrieval	Content-based image retrieval (CBIR) has attracted substantial attention during the past few years for its potential practical applications to image management. A variety of relevance feedback schemes have been designed to bridge the semantic gap between low-level visual features and high-level semantic concepts for an image retrieval task. Various collaborative image retrieval (CIR) schemes aim to utilize the user historical feedback log data with similar and dissimilar pairwise constraints to improve the performance of a CBIR system. However, existing subspace learning approaches with explicit label information cannot be applied for a CIR task although the subspace learning techniques play a key role in various computer vision tasks, e.g., face recognition and image classification. In this paper, we propose a novel subspace learning framework, i.e., conjunctive patches subspace learning (CPSL) with side information, for learning an effective semantic subspace by exploiting the user historical feedback log data for a CIR task. CPSL can effectively integrate the discriminative information of labeled log images, the geometrical information of labeled log images, and the weakly similar information of unlabeled images together to learn a reliable subspace. We formulate this problem into a constrained optimization problem and then present a new subspace learning technique to exploit the user historical feedback log data. Extensive experiments on both synthetic datasets and a real-world image database demonstrate the effectiveness of the proposed scheme in improving the performance of a CBIR system by exploiting the user historical feedback log data.	algorithm;body dysmorphic disorders;committed information rate;computer vision;constrained optimization;constraint (mathematics);content-based image retrieval;experiment;facial recognition system;high- and low-level;learning disorders;mathematical optimization;optimization problem;relevance feedback;solutions;synthetic data;synthetic intelligence;tracer;world file	Lining Zhang;Lipo Wang;Weisi Lin	2012	IEEE Transactions on Image Processing	10.1109/TIP.2012.2195014	computer vision;contextual image classification;visualization;manifold;image retrieval;computer science;machine learning;pattern recognition;semantics;radio frequency;information retrieval;measurement	Vision	25.149362085629715	-44.99060288003219	26088
b2cad331c1bff4d944a73ae7b7f6a11686e33630	three-dimensional object detection and layout prediction using clouds of oriented gradients		"""We develop new representations and algorithms for three-dimensional (3D) object detection and spatial layout prediction in cluttered indoor scenes. RGB-D images are traditionally described by local geometric features of the 3D point cloud. We propose a cloud of oriented gradient (COG) descriptor that links the 2D appearance and 3D pose of object categories, and thus accurately models how perspective projection affects perceived image boundaries. We also propose a """"Manhattan voxel"""" representation which better captures the 3D room layout geometry of common indoor environments. Effective classification rules are learned via a structured prediction framework that accounts for the intersection-over-union overlap of hypothesized 3D cuboids with human annotations, as well as orientation estimation errors. Contextual relationships among categories and layout are captured via a cascade of classifiers, leading to holistic scene hypotheses with improved accuracy. Our model is learned solely from annotated RGB-D images, without the benefit of CAD models, but nevertheless its performance substantially exceeds the state-of-the-art on the SUN RGB-D database. Avoiding CAD models allows easier learning of detectors for many object categories."""	3d projection;algorithm;computer-aided design;cuboid;holism;image gradient;object detection;point cloud;sensor;structured prediction;voxel	Zhile Ren;Erik B. Sudderth	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.169	computer vision;simulation;computer science;machine learning	Vision	29.99164259837899	-49.496365703977524	26140
1be32039596ff52fa09772f4606b65845d1c5853	analysis-by-synthesis: pedestrian tracking with crowd simulation models in a multi-camera video network	crowd simulation;multi camera systems;pedestrian tracking	For tracking systems consisting of multiple cameras with overlapping field-of-views, homography-based approaches are widely adopted to significantly reduce occlusions among pedestrians by sharing information among multiple views. However, in these approaches, the usage of information under real-world coordinates is only at a preliminary level. Therefore, in this paper, a multi-camera tracking system with integrated crowd simulation is proposed in order to explore the possibility to make homography information more helpful. Two crowd simulators with different simulation strategies are used to investigate the influence of the simulation strategy on the final tracking performance. The performance is evaluated by multiple object tracking precision and accuracy (MOTP and MOTA) metrics, for all the camera views and the results obtained under real-world coordinates. The experimental results demonstrate that crowd simulators boost the tracking performance significantly, especially for crowded scenes with higher density. In addition, a more realistic simulation strategy helps to further improve the overall tracking result. 2014 Elsevier Inc. All rights reserved.	crowd simulation;experiment;glossary of computer graphics;homography (computer vision);match moving;speech coding;tracking system	Zhixing Jin;Bir Bhanu	2015	Computer Vision and Image Understanding	10.1016/j.cviu.2014.10.001	computer vision;simulation;computer science;crowd simulation;multimedia	AI	43.02173951597002	-45.958349203838964	26171
1dd7c8a3445fbdb1d2f144d468696b9963f01cc3	towards deep compositional networks	standards;neural networks;cost function;convolution;visualization;computational modeling;mathematical model	Hierarchical feature learning based on convolutional neural networks (CNN) has recently shown significant potential in various computer vision tasks. While allowing high-quality discriminative feature learning, the downside of CNNs is the lack of explicit structure in features, which often leads to overfitting, absence of reconstruction from partial observations and limited generative abilities. Explicit structure is inherent in hierarchical compositional models, however, these lack the ability to optimize a well-defined cost function. We propose a novel analytic model of a basic unit in a layered hierarchical model with both explicit compositional structure and a well-defined discriminative cost function. Our experiments on two datasets show that the proposed compositional model performs on a par with standard CNNs on discriminative tasks, while, due to explicit modeling of the structure in the feature units, affording a straight-forward visualization of parts and faster inference due to separability of the units.	approximation;artificial neural network;backpropagation;computer vision;convolution;convolutional neural network;deep learning;discriminative model;embedded system;experiment;explicit modeling;feature learning;generative model;glossary of computer graphics;hierarchical database model;linear separability;loss function;mathematical optimization;overfitting;parametric model;software propagation	Domen Tabernik;Matej Kristan;Jeremy L. Wyatt;Ale&#x0161; Leonardis	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7900171	visualization;computer science;artificial intelligence;machine learning;pattern recognition;mathematical model;convolution;computational model;artificial neural network;statistics	Vision	25.260006147063383	-48.93249454273935	26234
357da13fa9fdfffe3b62739002ed06060841cb3f	frontal gait recognition from occluded scenes	occlusion;kinect;frontal gait recognition;sequence alignment	In this paper, we propose a method using Kinect depth data to address the problem of occlusion in frontal gait recognition. We consider situations where such depth cameras are mounted on top of entry and exit points of a zone under surveillance, respectively capturing the back and front views of each subject passing through the zone. A feature set corresponding to the back view is derived from the depth information along the contour of the silhouette, while periodic variation of the skeleton structure of the lower body region as estimated by Kinect is extracted from the front view. These feature sets preserve gait dynamics at a high resolution and can be extracted efficiently. In congested places like airports, railway stations and shopping malls, multiple persons move into the surveillance zone one after another, thereby causing occlusion of the target. The proposed recognition procedure compares the unoccluded frames of a cluttered test sequence with the matching frames of a training sequence. Dynamic programming based local sequence alignment is used to determine this frame correspondence. The method is computationally efficient and shows encouraging results under different levels	algorithmic efficiency;dynamic programming;entry point;gait analysis;hidden surface determination;image resolution;kinect;response time (technology);seasonality;sequence alignment	Pratik Chattopadhyay;Shamik Sural;Jayanta Mukherjee	2015	Pattern Recognition Letters	10.1016/j.patrec.2015.06.004	computer vision;simulation;computer science;sequence alignment	Vision	44.0356908304742	-45.02396911568287	26236
7b22f3dead154db45df03f6a57c440e70b2e25e3	salient object detection in rgb-d image based on saliency fusion and propagation	saliency propagation;会议论文;salient object detection;multiple cues fusion;rgb d image	Automatic detection of salient objects in images attracts much research attention for its usage in numerous multimedia applications. In this paper, we propose a saliency fusion and propagation strategy based salient object detection method for RGB-D images, in which multiple cues are fused to provide high precision detection result and saliency propagation is utilized to improve the completeness of salient objects. To each RGB-D image, we firstly generate the saliency maps based on color cue, location cue and depth cue independently. Then, we fuse the saliency maps and obtain a high precision saliency map. Finally, we propagate saliency to obtain more complete salient objects. We evaluate the proposed method on two public data sets for salient object detection, NJU400 and RGBD Benchmark. The experimental results demonstrate saliency fusion and propagation are effective in salient object detection and our method outperforms the state-of-the-art methods.	benchmark (computing);information privacy;map;object detection;software propagation	Jingfan Guo;Tongwei Ren;Jia Bei;Yujin Zhu	2015		10.1145/2808492.2808551	computer vision;geography;kadir–brady saliency detector;machine learning;pattern recognition	Vision	42.24142912641225	-51.50438506224787	26253
307499c0ae9080a239128202c507d74dd876cf79	an adaptive spherical view representation for navigation in changing environments	g400 computer science;human memory;omnidirectional vision;mobile robot;g700 artificial intelligence;mobile robot navigation;visual features;persistent mapping	Real-world environments such as houses and offices change over time, meaning that a mobile robot’s map will become out of date. In this work, we introduce a method to update the reference views in a hybrid metrictopological map so that a mobile robot can continue to localize itself in a changing environment. The updating mechanism, based on the multi-store model of human memory, incorporates a spherical metric representation of the observed visual features for each node in the map, which enables the robot to estimate its heading and navigate using multi-view geometry, as well as representing the local 3D geometry of the environment. A series of experiments demonstrate the persistence performance of the proposed system in real changing environments, including analysis of the long-term stability.	course (navigation);experiment;mobile robot;persistence (computer science)	Feras Dayoub;Tom Duckett;Grzegorz Cielniak	2009		10.1016/j.robot.2011.02.013	mobile robot;computer vision;simulation;computer science;artificial intelligence;social robot;memory;mobile robot navigation	Robotics	50.42893380808973	-37.60513916461253	26256
f0d09ef48e57a891c49b4e1db63629f7a4d0c356	gaussian mixture spline trajectory: learning from a dataset, generating trajectories without one		Most optimization-based motion planners use a naive linear initialization, which does not use previous planning experience. We present an algorithm called ‘Gaussian mixture spline trajectory’ (GMST) that leverages motion datasets for generating trajectories for new planning problems. Unlike other trajectory prediction algorithms, our method does not retrieve trajectories from a dataset. Instead, it first uses a Gaussian mixture model (GMM) to modelize the likelihood of the trajectories to be inside the dataset and then uses the GMMu0027s parameters to generate new trajectories. As the use of the dataset is restricted only to the learning phase it can take advantage of very large datasets. Using both abstract and robot system planning problems, we show that the GMST algorithm decreases the computation time and number of iterations of optimization-based planners while increasing their success rates as compared to that obtained with linear initialization.	spline (mathematics)	Thibault Barbie;R. Kabutan;R. Tanaka;T. Nishida	2018	Advanced Robotics	10.1080/01691864.2018.1465849	engineering;mixture model;control theory;computation;mathematical optimization;initialization;trajectory;spline (mathematics);gaussian	Robotics	48.519102023816465	-27.212286289840545	26257
16326bc70a5182427924f12e46e94a82dcfe4642	background recovery by fixed-rank robust principal component analysis	reflection removal;robust pca;background recovery	Background recovery is a very important theme in computer vision applications. Recent research shows that robust principal component analysis (RPCA) is a promising approach for solving problems such as noise removal, video background modeling, and removal of shadows and specularity. RPCA utilizes the fact that the background is common in multiple views of a scene, and attempts to decompose the data matrix constructed from input images into a low-rank matrix and a sparse matrix. This is possible if the sparse matrix is sufficiently sparse, which may not be true in computer vision applications. Moreover, algorithmic parameters need to be fine tuned to yield accurate results. This paper proposes a fixed-rank RPCA algorithm for solving background recovering problems whose low-rank matrices have known ranks. Comprehensive tests show that, by fixing the rank of the low-rank matrix to a known value, the fixed-rank algorithm produces more reliable and accurate results than existing low-rank RPCA algorithm.	algorithm;computer vision;lagrange multiplier;low-rank approximation;robust principal component analysis;sparse matrix;specularity	Wee Kheng Leow;Yuan Cheng;Xiang Lin;Terence Sim;Lewis Foo	2013		10.1007/978-3-642-40261-6_6	computer vision;mathematical optimization;machine learning;mathematics	Vision	28.555423641975604	-38.843677438890936	26269
5394afb4495b8158f01da238e59868f384711605	nonlinear feature transformation and deep fusion for alzheimer's disease staging analysis	alzheimer s disease ad;metric learning;deep neural networks;svm classifier;feature fusion;mild cognitive impairment mci	In this study, we develop a novel nonlinear metric learning method to improve biomarker identification for Alzheimer's Disease (AD) and Mild Cognitive Impairment (MCI). Formulated under a constrained optimization framework, the proposed method learns a smooth nonlinear feature space transformation that makes the mapped data more linearly separable for SVMs. The thin-plate spline (TPS) is chosen as the geometric model due to its remarkable versatility and representation power in generating sophisticated yet smooth deformations. In addition, a deep network based feature fusion strategy through stacked denoising sparse auto-encoder (DSAE) is adopted to integrate cross-sectional and longitudinal features estimated from MR brain images. Using the ADNI dataset, we evaluate the effectiveness of the proposed feature transformation and feature fusion strategies and demonstrate the improvements over the state-of-the-art solutions within the same category.		Bibo Shi;Yani Chen;Pin Zhang;Charles D. Smith;Jundong Liu	2017	Pattern Recognition	10.1016/j.patcog.2016.09.032	computer science;artificial intelligence;machine learning;pattern recognition	Vision	25.874198094812314	-48.54238298737345	26376
1b8f9d45e40cfc79d515fa6dd470513b0680900f	dynamic arm gesture recognition using spherical angle features and hidden markov models		We introduce a vision-based arm gesture recognition (AGR) system using Kinect. The AGR system learns the discrete Hidden Markov Model (HMM), an effective probabilistic graph model for gesture recognition, from the dynamic pose of the arm joints provided by theKinect API. BecauseKinect’s viewpoint and the subject’s arm length can substantially affect the estimated 3Dpose of each joint, it is difficult to recognize gestures reliably with these features.The proposed system performs the feature transformation that changes the 3D Cartesian coordinates of each joint into the 2D spherical angles of the corresponding arm part to obtain viewinvariant and more discriminative features. We confirmed high recognition performance of the proposed AGR system through experiments with two different datasets.		HyeSuk Kim;Incheol Kim	2015	Adv. Human-Computer Interaction	10.1155/2015/785349	computer vision;speech recognition;engineering;communication	Vision	36.33538289168936	-48.74139095494627	26381
283af4c5bfc4a113c387c693946f8d8e72a18efa	domestic interaction on a segway base	human robot interaction	To be useful in a home environment, an assistive robot needs to be capable of a broad range of interactive activities such as locating objects, following specific people, and distinguishing among different people. This paper presents a Segway-based robot that successfully performed all of these tasks en route to a second place finish in the RoboCup@Home 2007 competition. The main contribution is a complete description and analysis of the robot system and its implemented algorithms that enabled the robot’s successful human-robot interaction in this broad and challenging forum. We describe in detail a novel person recognition algorithm, a key component of our overall success, that included two co-trained classifiers, each focusing on different aspects of the person (face and shirt color).	algorithm;assistive technology;autonomous robot;co-training;color;human–robot interaction;outline of object recognition;robot	W. Bradley Knox;Juhyun Lee;Peter Stone	2008		10.1007/978-3-642-02921-9_45	computer vision;simulation;computer science;artificial intelligence;social robot	Robotics	46.23489546161508	-38.670198479598525	26403
87a66ccc68374ffb704ee6fb9fa7df369718095c	multi-person pose estimation with local joint-to-person associations		Despite of the recent success of neural networks for human pose estimation, current approaches are limited to pose estimation of a single person and cannot handle humans in groups or crowds. In this work, we propose a method that estimates the poses of multiple persons in an image in which a person can be occluded by another person or might be truncated. To this end, we consider multiperson pose estimation as a joint-to-person association problem. We construct a fully connected graph from a set of detected joint candidates in an image and resolve the joint-to-person association and outlier detection using integer linear programming. Since solving joint-to-person association jointly for all persons in an image is an NP-hard problem and even approximations are expensive, we solve the problem locally for each person. On the challenging MPII Human Pose Dataset for multiple persons, our approach achieves the accuracy of a state-of-the-art method, but it is 6,000 to 19,000 times faster.	3d pose estimation;anomaly detection;approximation;artificial neural network;connectivity (graph theory);integer programming;linear programming;np-hardness	Umar Iqbal;Juergen Gall	2016		10.1007/978-3-319-48881-3_44	computer vision;3d pose estimation;machine learning;pattern recognition;articulated body pose estimation;mathematics	Vision	32.17807712332425	-47.72941037443198	26406
94114fe572977ef6460a9bbc4d386f65059daba3	nonlinear discriminant mapping using the laplacian of a graph	linear discriminate analysis;isomap;laplacian of a graph;linear discriminant analysis	In this paper, an algorithm for nonlinear discriminant mapping (NDM) is presented, which elegantly integrates the ideas of both linear discriminant analysis (LDA) and Isomap by using the Laplacian of a graph. The objective of NDM is to find a linear subspace project of nonlinear data set, which preserves maximum difference between between-class scatter and within-class scatter. 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.	algorithm;isomap;linear discriminant analysis;maxdiff;nonlinear system;pattern recognition	Hong Tang;Tao Fang;Pengfei Shi	2006	Pattern Recognition	10.1016/j.patcog.2005.08.006	mathematical optimization;kernel fisher discriminant analysis;computer science;machine learning;pattern recognition;optimal discriminant analysis;mathematics;linear discriminant analysis;multiple discriminant analysis	Vision	25.709906284025937	-40.92380425598714	26410
ebe80253a0864d6ff4f15d418988449097ac4cde	efficient background modeling using nonparametric histogramming	video surveillance;image segmentation;foreground segmentation results background modeling nonparametric histogramming high definition surveillance cameras video analytics video object extraction high resolution surveillance videos gaussian mixture modeling foreground object extraction histogram based background model computational complexity general computer gmm;videos computational modeling three dimensional displays histograms irrigation;gaussian processes;image sensors;computational complexity;feature extraction;proceedings paper;mixture models;video surveillance computational complexity feature extraction gaussian processes image segmentation image sensors mixture models	With rapid increase in the deployment of high-definition surveillance cameras, the need of efficient video analytics for extracting video objects from high-resolution surveillance videos in real time has become more and more demanding. Conventional background modeling methods, e.g., the Gaussian mixture modeling (GMM), although having long been proven to be effective for foreground object extraction, are actually not efficient enough for the real-time analysis of high-resolution videos. We thus propose a novel background modeling approach using nonparametric histogramming that can derive a holistic, histogram-based background model for each pixel with low computational complexity. Due to the simple algorithm design, the proposed approach can be easily implemented by fixed-point computation. Without using any accelerator (like CUDA, Intel SIMD, or Intel IPP library), multi-threading or sub-sampling technique, our implementation of the proposed algorithm achieves high efficiency for the processing of 1920×1080 color videos at ~18.81 fps on a general computer (Intel Core i7 3.4GHz CPU). In the experimental comparisons, the proposed approach is ~3.9 times faster than the GMM, while giving comparable foreground segmentation results.	algorithm design;approximation algorithm;cuda;central processing unit;closed-circuit television;computation;computational complexity theory;experiment;general computer corporation;google map maker;hdmi;holism;image resolution;integrated performance primitives;modal logic;pixel;real-time clock;real-time computing;simd;sampling (signal processing);semiconductor industry;software deployment;thread (computing);video content analysis	Horng-Horng Lin;Li-Chen Shih;Jen-Hui Chuang	2013	2013 Seventh International Conference on Distributed Smart Cameras (ICDSC)	10.1109/ICDSC.2013.6778219	computer vision;background subtraction;feature extraction;computer science;machine learning;mixture model;image sensor;gaussian process;image segmentation;computational complexity theory;computer graphics (images)	Vision	43.010061551989644	-36.3634230931537	26426
3ea307635c762fc5bf9826f9053ed98b004231b8	non-parametric bayesian learning with deep learning structure and its applications in wireless networks	telecommunication computing belief networks learning artificial intelligence radio networks;bayes methods;performance evaluation nonparametric bayesian learning deep learning structure wireless networks infinite hierarchical nonparametric bayesian model metropolis hastings method;vectors;cognitive radio;signal processing;inference algorithms;signal processing algorithms;metropolis hastings algorithm non parametric bayesian learning deep learning indian buffet process;data models;bayes methods signal processing algorithms data models inference algorithms vectors signal processing cognitive radio	In this paper, we present an infinite hierarchical non-parametric Bayesian model to extract the hidden factors over observed data, where the number of hidden factors for each layer is unknown and can be potentially infinite. Moreover, the number of layers can also be infinite. We construct the model structure that allows continuous values for the hidden factors and weights, which makes the model suitable for various applications. We use the Metropolis-Hastings method to infer the model structure. Then the performance of the algorithm is evaluated by the experiments. Simulation results show that the model fits the underlying structure of simulated data.	bayesian network;deep learning;experiment;fits;metropolis;metropolis–hastings algorithm;modified huffman coding;simulation	Erte Pan;Zhu Han	2014	2014 IEEE Global Conference on Signal and Information Processing (GlobalSIP)	10.1109/GlobalSIP.2014.7032319	data modeling;cognitive radio;wake-sleep algorithm;computer science;machine learning;signal processing;pattern recognition;statistics	Robotics	29.06443516542623	-31.01356264518596	26520
7a852cf7403f88f7323e9d75730d5249f5829d74	detecting pedestrians using patterns of motion and appearance	20 pixels;human sensing;computer vision image motion analysis image representation image sequences image resolution object detection feature extraction;video sequence frames;image motion analysis;image resolution;image sequence analysis;image representations;walking person;300 pixels;computer vision;detector scanning;15 pixels;boosting;false positive rate;motion information;detection style algorithm;image representation;machine vision;motion patterns;feature extraction;15 pixels pedestrian detection motion patterns motion appearance image intensity information motion information detection style algorithm detector scanning video sequence frames adaboost walking person image motion representation low resolution images 300 pixels 20 pixels;pedestrian detection;adaboost;motion appearance;image motion representation;image intensity information;tracking;motion detection detectors face detection image resolution humans motion analysis snow rain pattern recognition object detection;object detection;low resolution images;image sequences	This paper describes a pedestrian detection system that integrates image intensity information with motion information. We use a detection style algorithm that scans a detector over two consecutive frames of a video sequence. The detector is trained (using AdaBoost) to take advantage of both motion and appearance information to detect a walking person. Past approaches have built detectors based on appearance information, but ours is the first to combine both sources of information in a single detector. The implementation described runs at about 4 frames/second, detects pedestrians at very small scales (as small as 20/spl times/15 pixels), and has a very low false positive rate. Our approach builds on the detection work of Viola and Jones. Novel contributions of this paper include: i) development of a representation of image motion which is extremely efficient, and ii) implementation of a state of the art pedestrian detection system which operates on low resolution images under difficult conditions (such as rain and snow).	sensor	Paul A. Viola;Michael J. Jones;Daniel Snow	2003		10.1109/ICCV.2003.1238422	adaboost;computer vision;simulation;image resolution;machine vision;feature extraction;false positive rate;computer science;machine learning;tracking;boosting;computer graphics (images)	Vision	39.740172784844006	-47.28316798780802	26542
818ae64aba66c927362bb83b8ce9df6183d1e50a	new measure of boolean factor analysis quality	neural network application;matrix factorization;high dimensionality;data mining;data model;hopfield neural network;boolean factor analysis;machine learning;expectation maximization;factor analysis;statistics;bars problem;associative memory;binary data;synthetic data;information gain;boolean matrix factorization	"""Learning of objects from complex patterns is a long-term challenge in philosophy, neuroscience, machine learning, data mining, and in statistics. There are some approaches in literature trying to solve this difficult task consisting in discovering hidden structure of highdimensional binary data and one of them is Boolean factor analysis. However there is no expert independent measure for evaluating this method in terms of the quality of solutions obtained, when analyzing unknown data. Here we propose information gain, model-based measure of the rate of success of individual methods. This measure presupposes that observed signals arise as Boolean superposition of base signals with noise. For the case whereby a method does not provide parameters necessary for information gain calculation we introduce the procedure for their estimation. Using an extended version of the """"Bars Problem"""" generation of typical synthetics data for such a task, we show that our measure is sensitive to all types of data model parameters and attains its maximum, when best fit is achieved."""	factor analysis	Alexander A. Frolov;Dusan Húsek;Pavel Polyakov	2011		10.1007/978-3-642-20282-7_11	expectation–maximization algorithm;data model;computer science;machine learning;pattern recognition;data mining;mathematics;kullback–leibler divergence;factor analysis;matrix decomposition;statistics;synthetic data	Logic	28.496078985497224	-31.692964117632847	26546
205ce07abd543a5b5f60bb2a397b37c008e59343	fast automatic airport detection in remote sensing images using convolutional neural networks		Fast and automatic detection of airports from remote sensing images is useful for many military and civilian applications. In this paper, a fast automatic detection method is proposed to detect airports from remote sensing images based on convolutional neural networks using the Faster R-CNN algorithm. This method first applies a convolutional neural network to generate candidate airport regions. Based on the features extracted from these proposals, it then uses another convolutional neural network to perform airport detection. By taking the typical elongated linear geometric shape of airports into consideration, some specific improvements to the method are proposed. These approaches successfully improve the quality of positive samples and achieve a better accuracy in the final detection results. Experimental results on an airport dataset, Landsat 8 images, and a Gaofen-1 satellite scene demonstrate the effectiveness and efficiency of the proposed method.	algorithm;artificial neural network;convolutional neural network;experiment;fastest;google earth;object detection;preprocessor;sensor	Fen Chen;Ruilong Ren;Tim Van de Voorde;Wenbo Xu;Guiyun Zhou;Yan Zhou	2018	Remote Sensing	10.3390/rs10030443	artificial intelligence;remote sensing;convolutional neural network;computer vision;geology;geometric shape	Robotics	33.41103745972158	-45.24830732771302	26549
a0975c94508fd8cec00b9ed82a61aec14a1246e1	is it my body? body extraction from uninterpreted sensory data based on the invariance of multiple sensory attributes	adaptive systems gaussian distribution robot kinematics;adaptive systems;discrimination hyperplane body extraction uninterpreted sensory data multiple sensory attribute robot body gaussian distribution;gaussian distribution;data mining robot sensing systems intelligent robots robotics and automation humans adaptive systems data engineering process design buildings intelligent sensors;robot kinematics	Finding the body in uninterpreted sensory data is one of the fundamental competences to construct the body representation that influences on adaptabilities of the robot to the changes in the environment and the robot body. The invariance of sensations in self-observation seems a promising key information to find the body. However, since each sensory attribute can be invariant only in the observation of a part of the body, the robot should complementarily utilize the invariance of the multiple sensory attributes. In this paper, we propose a method of body-nonbody discrimination by complementarily utilizing multiple sensory attributes based on a conjecture about the distribution of the variance of sensations for each observing posture, where it can be approximated by a mixture of two Gaussian distributions, which are for observing the body and the nonbody, respectively. By estimating the distribution, the robot can automatically find a discrimination hyperplane to judge whether it observes its body in the current observing posture. Simple experiments show the validity of the proposed method.	approximation algorithm;experiment;poor posture;robot;self-awareness	Yuichiro Yoshikawa;Yoshiki Tsuji;Koh Hosoda;Minoru Asada	2004	2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)	10.1109/IROS.2004.1389756	normal distribution;computer vision;computer science;artificial intelligence;adaptive system;machine learning;mathematics;robot kinematics	Robotics	47.85839293894962	-31.488112399470968	26656
80788cf497681cf4a4ad6a78bd1580bf69ddb579	vision-based surveillance system for monitoring traffic conditions		With the rapid advancement of sensing technologies, it has been feasible to collect various types of traffic data such as traffic volume and travel times. Vision-based approach is one of the major scheme actively used for the automated traffic data collection, and continues to gain traction to a broader utilization. It collects video streams from cameras installed near roads, and processes the video streams frame by frame using image processing algorithms. The widely used algorithms include vehicle detection and vehicle tracking which recognize every vehicle in the camera view and track it in the consecutive frames. Vehicle counts and speed can be estimated from the detection and tracking results. Continuous efforts have been made for the performance improvement of the algorithms and for their effective applications. However, little research has been found on the application to the various view settings of highway CCTV cameras as well as the reliability of the speed estimation. This paper proposes a vision-based system that integrates vehicle detection, vehicle tracking, and field of view calibration algorithms to obtain vehicle counting data and to estimate individual vehicle speed. The proposed system is customized for the video streams collected from highway CCTVs which have various settings in terms of focus and view angles. The system detects and tracks every vehicle in the view unless it is occluded by other vehicles. It is also capable of handling occlusions that occurs frequently depending on the view angles. The system has been tested on the several different views including congested scenes. Vehicle counts and speed estimation results are compared to the manual counting and GPS data, respectively. The comparison signifies that the system has a high potential to extract reliable information about highway traffic conditions from highway CCTVs.	algorithm;camera resectioning;closed-circuit television;device driver;experiment;global positioning system;image processing;logistics;robustness (computer science);streaming media;traction teampage;vehicle tracking system;website monitoring	Man-Woo Park;Jung In Kim;Young-Joo Lee;Jinwoo Park;Wonho Suh	2017	Multimedia Tools and Applications	10.1007/s11042-017-4521-4	image processing;computer vision;vehicle tracking system;digital image processing;computer science;streams;artificial intelligence;data collection;global positioning system;ivms;field of view	Robotics	43.038966821499784	-42.642291375462136	26671
157de8798e64adfa43cf48af05b15aa62537bf21	exploring depth information for object segmentation and detection	depth information object segmentation object detection mrf graphical model super pixels object class label best segment proposal selection unified energy function joint energy term optimal labelings 3d distances;object detection geometry image segmentation;proposals image segmentation object segmentation labeling object detection databases graphical models	We propose a new framework for performing object segmentation and detection simultaneously. Our method leverages with an MRF graphical model that comprises two kinds of nodes and two types of labels for inference. Specifically, we decompose an image into super pixels and generate segment proposals from each super pixel. The super pixels are then duplicated to form the two types of nodes. For each segmentation node, the model is to predict the object class label, while it is to decide the label corresponding to the best segment proposal selection at each detection node. The former is clearly a segmentation problem and the latter a detection problem. We link the two tasks by establishing a unified energy function that has a joint energy term accounting for the compatibility of the segmentation and detection labelings. Marginalizing by fixing either type of variables, the energy function can be switched into the one specifically for detection or segmentation. This property enables an alternating procedure to conveniently obtain the optimal labelings. To better explain the geometry about the objects and the scene, we use the depth information so that 3-D distances between super pixels are available in computing each energy term. Experimental results on a dataset with depth information are provided to support the effectiveness of our method.	complexity;graphical model;markov random field;mathematical optimization;pixel;tagged union	Tyng-Luh Liu;Kai-Yueh Chang;Shang-Hong Lai	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.743	computer vision;range segmentation;object-class detection;viola–jones object detection framework;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;scale-space segmentation	Vision	43.3830924412547	-51.70217360670975	26674
3bd3cbddeb393b24fe32f6c815a1b9e56183231b	maximum margin binary classifiers using intrinsic and penalty graphs	eigenvalues and eigenfunctions;standards;support vector machines;laplace equations;matrix decomposition;signal processing;optimization	In this paper a variant of the binary Support Vector Machine classifier that exploits intrinsic and penalty graphs in its optimization problem is proposed. We show that the proposed approach is equivalent to a two-step process where the data is firstly mapped to an optimal discriminant space of the input space and, subsequently, the original SVM classifier is applied. Our approach exploits the underlying data distribution in a discriminant space in order to enhance SVMs generalization ability. We also extend this idea to the Least Squares SVM classifier, where the adoption of the intrinsic and penalty graphs acts as a regularizer incorporating discriminant information in the overall solution. Experiments on standard and recently introduced datasets verify our analysis since, in the cases where the classes forming the problem are not well discriminated in the original feature space, the exploitation of both intrinsic and penalty graphs enhances performance.	experiment;feature vector;least squares;linear discriminant analysis;mathematical optimization;optimization problem;statistical classification;support vector machine	Berkay Kicanaoglu;Alexandros Iosifidis;Moncef Gabbouj	2016	2016 24th European Signal Processing Conference (EUSIPCO)	10.1109/EUSIPCO.2016.7760653	mathematical optimization;machine learning;pattern recognition;mathematics	ML	25.13295805620154	-42.203506698237206	26684
b9dc25b2b692470e883d40dcc26cf9bb2d0bca76	study of remote bearing fault diagnosis based on bp neural network combination	economic benefit;remote fault diagnosis bp neural network plc rolling bearing;rolling bearings backpropagation control engineering computing expert systems fault diagnosis maintenance engineering mechanical engineering computing programmable controllers;programmable controllers;mechanical engineering computing;economic benefits remote bearing fault diagnosis bp neural network combination plc based remote fault diagnosis system remote system bearing fault diagnosis maintenance expert system diagnostic information cross boundary diagnosis rolling bearing fault detection;expert systems;fault diagnosis matlab rolling bearings indexes training fault detection neurons;training;maintenance engineering;plc;backpropagation;production process;indexes;rolling bearings;bp neural network;indexation;fault detection;rolling bearing;fault diagnosis system;remote fault diagnosis;control engineering computing;neurons;matlab;fault diagnosis;neural network;expert system	In this paper, the rolling bearing is detected by the PLC-based remote fault diagnosis system. This system carries out remote system bearing fault diagnosis and maintenance through the combination of expert system and BP Neural Network. Since the advantages of rapid exchange of diagnostic information, accelerating the fault diagnosis rate, and reducing negative effects of failure on production process, many enterprises adopt remote fault diagnosis technology to carry out rolling bearing detection. Enterprises adopting cross-boundary diagnosis of rolling bearing fault detection show that this method brings enormous economic benefits. The research has important economic significance and practical value.	artificial neural network;computer network programming;data acquisition;expert system;fault detection and isolation;matlab;real-time clock;simulation	Yunyun Yang;Wei Tang	2011	2011 Seventh International Conference on Natural Computation	10.1109/ICNC.2011.6022177	control engineering;embedded system;engineering;forensic engineering	Robotics	36.651178926609276	-30.549965271679458	26810
fa04731db3aa1b8626d6cbcd50f4d35c76e8bd67	map retrieval in 3d using view-dependent local map descriptor	databases;clutter;visualization;trajectory;three dimensional displays;feature extraction;robots	"""Map retrieval, the problem of similarity search over a large collection of 3D pointset maps previously built by mobile robots, is crucial for autonomous navigation in indoor and outdoor environments. Bag-of-words (BoW) methods constitute a popular approach to map retrieval; however, these methods have extremely limited descriptive ability because they ignore the spatial layout information of the local features. The main contribution of this paper is an extension of the bag-of-words map retrieval method to enable the use of spatial information from local features. Our strategy is to explicitly model a unique viewpoint of an input local map; the pose of the local feature is defined with respect to this unique viewpoint, and can be viewed as an additional invariant feature for discriminative map retrieval. Specifically, we wish to determine a unique viewpoint that is invariant to moving objects, clutter, occlusions, and actual viewpoints. Hence, we perform scene parsing to analyze the scene structure, and consider the """"center"""" of the scene structure to be the unique viewpoint. Our scene parsing is based on a Manhattan world assumption that imposes a quasi-Manhattan world constraint to enable the robust detection of a scene structure that is invariant to clutter and moving objects. Experimental results using the publicly available University of Michigan North Campus Long-Term Vision and LIDAR Dataset (NCLT dataset [1]) validate the efficacy of the proposed approach."""	autonomous robot;bag-of-words model in computer vision;clutter;map;mobile robot;parsing;point cloud;scene graph;similarity search	Yoshiki Takahashi;Kanji Tanaka	2016	2016 IEEE/SICE International Symposium on System Integration (SII)	10.1109/SII.2016.7843995	computer vision;simulation;geography;data mining	Vision	50.244384502286394	-43.9042747154164	26834
b356ea57537cd668ba23c753ed913b5aebe9231b	epipole estimation using affine motion parallax		Determining the motion of a camera from its image sequences has so far proved very difficult, and no practical algorithms have been found for freely moving cameras. This novel algorithm is based on motion parallax, but uses sparse visual motion estimates to extract the direction of translation of the camera directly, after which determination of the camera rotation and the depths of the image features follows easily. This method can also detect and reject independent motion, and provide a measure of the uncertainty of its estimates.	algorithm;backup;essential matrix;graceful exit;matrix method;parallax;signal-to-noise ratio;sparse matrix;utility functions on indivisible goods	Jonathan M. Lawn;Roberto Cipolla	1993		10.5244/C.7.38	computer vision;match moving;quarter-pixel motion;motion estimation;mathematics;motion field;computer graphics (images)	Vision	52.85735550625749	-50.08546604057616	26846
10fd596645ad4a9b28c097113040b409e29492a6	ensemble minimum sum of squared similarities sampling for nyström-based spectral clustering	low rank approximations nyström based spectral clustering ensemble minimum sum of squared similarities bioinformatics computational complexity performance limitations ems3 sampling weight mixtures subsample selection;clustering algorithms approximation algorithms eigenvalues and eigenfunctions standards bioinformatics laplace equations matrix decomposition;subsampling nystrom sampling clustering;sampling methods approximation theory pattern clustering	Spectral clustering is a powerful approach for clustering, with applications across multiple disciplines, including bioinformatics. However, the way its computational complexity scales limits its application in analyzing large datasets. This complexity can be reduced using the Nyström method, which subsamples the input data in a way that preserves its representational diversity. There are different established strategies for subsampling, yet they may have performance limitations for certain complex datasets. This paper we propose an alternative to those methods, introducing a new sampling procedure called Ensemble Minimum Sum of Squared Similarities (EMS3). We further improve on this method by using weight mixtures in subsample selection, yielding more accurate low-rank approximations than existing ensemble Nyström methods. We also provide a theoretical analysis of the upper error bound of the EMS3 algorithm, and demonstrate its performance in comparison to the leading spectral clustering methods that use Nyström sampling.	algorithm;approximation error;bioinformatics;chroma subsampling;cluster analysis;computation;computational complexity theory;experiment;nyström method;sampling (signal processing);scalability;singular value decomposition;spectral clustering;the matrix	Djallel Bouneffouf;Inanç Birol	2016	2016 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2016.7727697	correlation clustering;mathematical optimization;k-medians clustering;pattern recognition;cure data clustering algorithm;mathematics;cluster analysis;biclustering;statistics	ML	27.06704317283026	-38.10507986648119	26859
bdb58d1c2ba6fb141f719d9f2ebfbeb0f472ea3d	robust learning with missing data	bayesian network;bayesian classifier;gibbs sampling;naive bayesian classifier;bayesian method;incomplete data;bayesian classifiers;probability intervals;bayesian learning;bayesian belief network;missing at random;bayesian estimator;missing data;em algorithm;conditional probability;bayesian networks	This paper introduces a new method, called the robust Bayesian estimator (RBE), to learn conditional probability distributions from incomplete data sets. The intuition behind the RBE is that, when no information about the pattern of missing data is available, an incomplete database constrains the set of all possible estimates and this paper provides a characterization of these constraints. An experimental comparison with two popular methods to estimate conditional probability distributions from incomplete data—Gibbs sampling and the EM algorithm—shows a gain in robustness. An application of the RBE to quantify a naive Bayesian classifier from an incomplete data set illustrates its practical relevance.	algorithmic efficiency;bayesian network;bayesian programming;cluster analysis;database;expectation–maximization algorithm;gibbs sampling;hidden variable theory;machine learning;missing data;naive bayes classifier;relevance;sampling (signal processing);statistical classification	Marco Ramoni;Paola Sebastiani	2001	Machine Learning	10.1023/A:1010968702992	computer science;machine learning;pattern recognition;bayesian network;mathematics;statistics	ML	28.329935103091486	-31.040404964318817	26869
dc58a8dac3a58981da1a506c187afd854277fa62	ambiguous measurements multitarget tracking with random sets	filtering;ambiguous likelihood;mathematics;probability;atmospheric measurements;particle filter multitarget tracking ambiguous measurement random set ambiguous likelihood data fusion probability hypothesis density;ambiguous measurements;particle measurements;ambiguous measurement;multitarget tracking;data fusion;set theory;ambiguous measurements multitarget tracking probability hypothesis density particle filter;particle filter;target tracking extraterrestrial measurements particle tracking particle measurements filtering density measurement mathematics fuses particle filters statistics;target tracking particle filtering numerical methods probability random processes sensor fusion set theory;multi target tracking;random processes;probability hypothesis density;sensor fusion;target tracking;extraterrestrial measurements;random set;particle filtering numerical methods;sea measurements	In this paper, we represent multi-target state and ambiguous measurement as random sets, use ambiguous likelihood to fuse ambiguous data for tracking multiple targets, and we implement multi-target tracking with probability hypothesis density (PHD) particle filter. Simulations are also presented to demonstrate the performance in tracking a varying number of targets.	computer simulation;particle filter	Shurong Tian;Xiaoshu Sun;Biao Li	2008	2008 International Conference on Computational Intelligence and Security	10.1109/CIS.2008.111	stochastic process;computer vision;econometrics;computer science;mathematics;sensor fusion;statistics	Robotics	51.43586204029411	-34.2130618066013	26907
19736c06efe4fbf714050d1fcfaab5d6e92b2064	hybrid tracking algorithms for planar and non-planar structures subject to illumination changes	vision system;real time;hybrid tracking algorithms;virtual visual servoing approach;lighting augmented reality fuses streaming media real time systems robustness machine vision visual servoing cameras layout;computer vision;texture information integration hybrid tracking algorithms nonplanar structures illumination changes augmented reality virtual world registration techniques online augmentation 3d model based tracking algorithm monocular vision system virtual visual servoing approach;3d model;registration techniques;image registration;3d model based tracking algorithm monocular vision system;tracking augmented reality computer vision image registration;augmented reality;visual servoing;virtual world;texture information integration;nonplanar structures;online augmentation;tracking;illumination changes;virtual worlds	Augmented reality (AR) aims to fuse a virtual world and a real one in an image stream. When considering only a vision sensor, it relies on registration techniques that have to be accurate and fast enough for on-line augmentation. This paper proposes a real-time, robust and efficient 3D model-based tracking algorithm monocular vision system. A virtual visual servoing approach is used to estimate the pose between the camera and the object. The integration of texture information in the classical non-linear edge-based pose computation provides a more reliable tracker. Several illumination models have been considered and compared to better deal with the illumination change in the scene. The method presented in this paper has been validated on several video sequences for augmented reality applications.	algorithm;augmented reality;computation;computer-aided design;illumination (image);motion estimation;nonlinear system;online and offline;planar (computer graphics);real-time clock;virtual world;visual servoing	Muriel Pressigout;Éric Marchand	2006	2006 IEEE/ACM International Symposium on Mixed and Augmented Reality	10.1109/ISMAR.2006.297794	computer vision;augmented reality;simulation;computer science;image registration;tracking;visual servoing;computer graphics (images)	Vision	50.041633858860116	-46.58958329875336	26959
81bf98776ed7a791901a943cf9cb25b512b01550	hardware-accelerated ray-triangle intersection testing for high-performance collision detection	hardware accelerator;hardware architecture;collision detection;graphics hardware;ray tracing;high performance	We present a novel approach for hardware-accelerated collision detection. This paper describes the design of the hardware architecture for primitive inference testing components implemented on a multi-FPGA Xilinx Virtex-II prototyping system. This paper focuses on the acceleration of ray-triangle intersection operation which is the one of the most important operations in various applications such as collision detection and ray tracing. Also, the proposed hardware architecture is general for intersection operations of other object pairs such as sphere vs. sphere, oriented bounding box (OBB) vs. OBB, cylinder vs. cylinder and so on. The result is a hardware-accelerated ray-triangle intersection engine that is capable of out-performing a 2.8GHz Xeon processor, running a well-known high performance software ray-triangle intersection algorithm, by up to a factor of seventy. In addition, we demonstrate that the proposed approach could prove to be faster than current GPU-based algorithms as well as CPU based algorithms for ray-triangle intersection.	central processing unit;collision detection;cylinder seal;field-programmable gate array;graphics processing unit;hardware acceleration;minimum bounding box;möller–trumbore intersection algorithm;ray tracing (graphics);virtex (fpga)	Sung-Soo Kim;Seung-Woo Nam;Do-Hyung Kim;In-Ho Lee	2007	Journal of WSCG		embedded system;ray tracing;parallel computing;real-time computing;hardware acceleration;computer science;operating system;hardware architecture;graphics hardware;collision detection;computer graphics (images)	EDA	43.52218565648546	-32.41384292729045	26982
54ed052738ca0f4570c74931857b3275fca9993b	knowledge-guided deep fractal neural networks for human pose estimation		Human pose estimation using deep neural networks aims to map input images with large variations into multiple body keypoints, which must satisfy a set of geometric constraints and interdependence imposed by the human body model. This is a very challenging nonlinear manifold learning process in a very high dimensional feature space. We believe that the deep neural network, which is inherently an algebraic computation system, is not the most efficient way to capture highly sophisticated human knowledge, for example those highly coupled geometric characteristics and interdependence between keypoints in human poses. In this work, we propose to explore how external knowledge can be effectively represented and injected into the deep neural networks to guide its training process using learned projections that impose proper prior. Specifically, we use the stacked hourglass design and inception-resnet module to construct a fractal network to regress human pose images into heatmaps with no explicit graphical modeling. We encode external knowledge with visual features, which are able to characterize the constraints of human body models and evaluate the fitness of intermediate network output. We then inject these external features into the neural network using a projection matrix learned using an auxiliary cost function. The effectiveness of the proposed inception-resnet module and the benefit in guided learning with knowledge projection is evaluated on two widely used human pose estimation benchmarks. Our approach achieves state-of-the-art performance on both datasets.	3d pose estimation;artificial neural network;benchmark (computing);coupling (computer programming);deep learning;encode;feature vector;fractal;graphical user interface;heat map;high- and low-level;human-based computation;interdependence;loss function;nonlinear dimensionality reduction;nonlinear system;symbolic computation;test set	Guanghan Ning;Zhi Zhang;Zhiquan He	2018	IEEE Transactions on Multimedia	10.1109/TMM.2017.2762010	artificial intelligence;fold (higher-order function);knowledge engineering;computer science;computer vision;pattern recognition;symbolic computation;artificial neural network;feature vector;projection (linear algebra);machine learning;pose;nonlinear dimensionality reduction	Vision	25.12745692801655	-49.05069855414525	27022
d3f5502903b61806cd20d2a7454675b67dcae12a	solution for the complex correspondences in line stereo matching	image segmentation;stereo vision stereo image processing image reconstruction layout buildings object recognition earth computer vision topology;hidden feature removal;occlusion;complex correspondence;stereo image processing hidden feature removal image reconstruction image segmentation;nonideal segment algorithms line stereo matching 3d structures reconstruction artificial objects partial occlusion;nonideal segment algorithms;computer vision;stereo matching;image reconstruction;partial occlusion;stereo image processing;stereo vision;3d structures reconstruction;feature group line stereo matching complex correspondence occlusion fragment;line stereo matching;artificial objects;cameras;buildings;feature group;fragment	Line stereo matching is critical for most methods proposed to reconstruct 3-D structures of artificial objects. A large number of algorithms have been implemented which announced solving the line correspondences. Due to the influences of partial occlusion and fragmented lines resulted from nonideal segment algorithms, the line correspondences will be present in various cases such as one-to-zero, one-to-one, one-to-many or many-to-many correspondences. However the one-to-many and many-to-many correspondences are usually ignored or arenpsilat solved effectively in existing methods. Here the essential causation that the various cases are present will be explored to provide support for solving all the correspondences integrally. Based on the above analysis, a method that can be expected to meet the objective is proposed, which has been proved effective by the experiments.	algorithm;causality;computational complexity theory;computer stereo vision;experiment;many-to-many;one-to-many (data model);one-to-one (data model);whole earth 'lectronic link	Jiyang Wang;Gongjian Wen;Deren Li	2008	2008 Fourth International Conference on Natural Computation	10.1109/ICNC.2008.701	computer vision;pattern recognition;mathematics;computer graphics (images)	Robotics	51.85147913614277	-51.31854902343908	27063
3643710813b688ecd3b6241234f1e644e4e691a7	cutting pattern identification for coal mining shearer through a swarm intelligence–based variable translation wavelet neural network	bat algorithm;cutting pattern identification;disturbance coefficient;ensemble empirical mode decomposition;sound signal;variable translation wavelet neural network	As a sound signal has the advantages of non-contacted measurement, compact structure, and low power consumption, it has resulted in much attention in many fields. In this paper, the sound signal of the coal mining shearer is analyzed to realize the accurate online cutting pattern identification and guarantee the safety quality of the working face. The original acoustic signal is first collected through an industrial microphone and decomposed by adaptive ensemble empirical mode decomposition (EEMD). A 13-dimensional set composed by the normalized energy of each level is extracted as the feature vector in the next step. Then, a swarm intelligence optimization algorithm inspired by bat foraging behavior is applied to determine key parameters of the traditional variable translation wavelet neural network (VTWNN). Moreover, a disturbance coefficient is introduced into the basic bat algorithm (BA) to overcome the disadvantage of easily falling into local extremum and limited exploration ability. The VTWNN optimized by the modified BA (VTWNN-MBA) is used as the cutting pattern recognizer. Finally, a simulation example, with an accuracy of 95.25%, and a series of comparisons are conducted to prove the effectiveness and superiority of the proposed method.	accidental falls;acoustic cryptanalysis;artificial neural network;bat algorithm;biological neural networks;business architecture;coal;coefficient;common variable immunodeficiency;computer simulation;extraction;feature vector;finite-state machine;futures studies;genetic translation process;hilbert–huang transform;inspiration function;master of business administration;mathematical optimization;maxima and minima;microphone device component;neural network simulation;pattern language;rem sleep behavior disorder;swarm intelligence;wavelet;yag rx mode:prid:pt:eye.left:nom	Jing Xu;Zhongbin Wang;Chao Tan;Lei Si;Xinhua Liu	2018		10.3390/s18020382		AI	37.28859472172519	-32.267346728493536	27065
058feddc2d06479925445e7c5a65f691e9d9e83d	autonomous lane keeping based on approximate q-learning		Obstacle avoidance is one of the most important problems in autonomous robots. This paper suggests a collision avoidance system using reinforcement learning. Hand-crafted features are used to approximate Q value. With off-line learning, we develop a general collision avoidance system and use this system to unknown environment. Simulation results show that our mobile robot agent using reinforcement learning can safely explore a corridor even if the agent does not know the shape of corridor at all.	approximation algorithm;autonomous robot;mobile robot;obstacle avoidance;online and offline;q-learning;reinforcement learning;simulation	Jonggu Lee;Taewan Kim;Hyoun Jin Kim	2017	2017 14th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)	10.1109/URAI.2017.7992762	error-driven learning;collision avoidance system;q-learning;robot;artificial neural network;reinforcement learning;mobile robot;artificial intelligence;machine learning;obstacle avoidance;engineering	Robotics	50.68843567469157	-28.36132174162359	27110
097e157201da1c6abd06eb92c80de5e6762994d8	on advances in statistical modeling of natural images	non gaussian models;natural image statistics;image classification;natural images;heavy tail;wavelet decomposition;statistical model;texture analysis;bessel k form;image manifold;statistical analysis;generalized laplacian;image analysis;image decomposition;probability model;scale invariance;statistical image analysis	Statistical analysis of images reveals two interesting properties: (i) invariance of image statistics to scaling of images, and (ii) non-Gaussian behavior of image statistics, i.e. high kurtosis, heavy tails, and sharp central cusps. In this paper we review some recent results in statistical modeling of natural images that attempt to explain these patterns. Two categories of results are considered: (i) studies of probability models of images or image decompositions (such as Fourier or wavelet decompositions), and (ii) discoveries of underlying image manifolds while restricting to natural images. Applications of these models in areas such as texture analysis, image classification, compression, and denoising are also considered.	computer vision;image scaling;noise reduction;scene statistics;statistical model;tails;wavelet	A. Srivastava;A. B. Lee;Eero P. Simoncelli;S.-C. Zhu	2003	Journal of Mathematical Imaging and Vision	10.1023/A:1021889010444	statistical model;computer vision;contextual image classification;image analysis;heavy-tailed distribution;scale invariance;pattern recognition;mathematics;statistics	Vision	30.75628614502989	-38.76293991493469	27150
5fffe352f1ce695e102a90568a6409cdafb94354	a virtual environment for core skills training in vascular interventional radiology	interventional radiology;haptic device;collision detection;hybrid mass spring model;particle system;virtual environment;haptic;open source	We present a prototype for a new virtual environment aiming at training interventional radiologists in the core skills involved in using catheters or guidewires. The instrument is modelled as a hybrid mass-spring particle system while the vasculature is a rigid triangulated surface mesh. A specially designed commercial haptic device allows the trainee to use real instruments to guide the simulation through synthetically generated vasculature with different degrees of complexity. Open source libraries are used for the visualisation and collision detection. Preliminary results show reasonable behaviour.		Vincent Luboz;James Lai;Rafal Blazewski;Derek Gould;Fernando Bello	2008		10.1007/978-3-540-70521-5_25	simulation;engineering;multimedia;biological engineering	Robotics	39.29305107675385	-38.473811943697704	27187
789ba45df86eb70cd0ec080cde86650e3e505753	maximum-likelihood estimation of the discrete coefficient of determination in stochastic boolean systems	convergence;nonparametric statistics;boolean functions;stochastic processes boolean functions convergence inference mechanisms logic gates maximum likelihood estimation nonparametric statistics;inference mechanisms;maximum likelihood estimation;maximum likelihood estimation nonparametric approach convergence rate wiring identification logic gate system identification parametric ml approach steady state dynamical boolean system static boolean system discrete cod inference method coefficient of determination stochastic boolean system;logic gates;stochastic processes;system identification boolean systems coefficient of determination maximum likelihood estimation	The discrete Coefficient of Determination (CoD) has become a key component of inference methods for stochastic Boolean models. We develop a parametric maximum-likelihood (ML) method for the inference of the discrete CoD for static Boolean systems and for dynamical Boolean systems in the steady state. Using analytical and numerical approaches, we compare the performance of the parametric ML approach against that of common nonparametric alternatives for CoD estimation, which show that the parametric approach has the least bias, variance, and root mean-square (RMS) error, provided that the system noise level is not too high. Next we consider the application of the proposed estimation approach to the problem of system identification, where only partial knowledge about the system is available. Inference procedures are proposed for both the static and dynamical cases, and their performance in logic gate and wiring identification is assessed through numerical experiments. The results indicate that identification rates converge to 100% as sample size increases, and that the convergence rate is much faster as more prior knowledge is available. For wiring identification, the parametric ML approach is compared to the nonparametric approaches, and it produced superior identification rates, though as the amount of prior knowledge is reduced, its performance approaches that of the nonparametric ML estimator, which was generally the best nonparametric approach in our experiments.	bayesian network;coefficient of determination;computational complexity theory;converge;curse of dimensionality;experiment;kerrison predictor;logic gate;noise (electronics);numerical analysis;rate of convergence;steady state;system identification;wiring	Ting Chen;Ulisses M. Braga-Neto	2013	IEEE Transactions on Signal Processing	10.1109/TSP.2013.2264054	nonparametric statistics;stochastic process;econometrics;circuit minimization for boolean functions;convergence;logic gate;standard boolean model;maximum satisfiability problem;machine learning;mathematics;maximum likelihood;boolean function;statistics	ML	26.701878746653087	-26.136156754920044	27206
d90e38544e642d5cabfe466d9185f2c20cbd3611	kullback-leibler penalized sparse discriminant analysis for event-related potential classification		A brain computer interface (BCI) is a system which provides direct communication between the mind of a person and the outside world by using only brain activity (EEG). The event-related potential (ERP)-based BCI problem consists of a binary pattern recognition. Linear discriminant analysis (LDA) is widely used to solve this type of classification problems, but it fails when the number of features is large relative to the number of observations. In this work ∗vpeterson@sinc.unl.edu.ar 1 ar X iv :1 60 8. 06 86 3v 1 [ cs .C V ] 2 4 A ug 2 01 6 we propose a penalized version of the sparse discriminant analysis (SDA), called Kullback-Leibler penalized sparse discriminant analysis (KLSDA). This method inherits both the discriminative feature selection and classification properties of SDA and it also improves SDA performance through the addition of Kullback-Leibler class discrepancy information. The KLSDA method is design to automatically select the optimal regularization parameters. Numerical experiments with two real ERP-EEG datasets show that this new method outperforms standard SDA.	binary pattern (image generation);brain–computer interface;discrepancy function;erp;electroencephalography;experiment;feature selection;kullback–leibler divergence;linear discriminant analysis;pattern recognition;sparse matrix	Victoria Peterson;Hugo Leonardo Rufiner;Ruben D. Spies	2016	CoRR		speech recognition;computer science;machine learning;pattern recognition;optimal discriminant analysis;statistics	ML	25.030325493157118	-40.455088268387264	27233
7f351eee00419bce564812c13c3238b32ef02e17	hyperspectral image classification via jcr and svm models with decision fusion	support vector machines;training;representation coefficients hyperspectral image classification joint collaborative representation support vector machine models decision fusion feature extraction;testing;support vector machines image classification context modeling mathematical model hyperspectral imaging;support vector machines feature extraction geophysical image processing hyperspectral imaging image classification image representation remote sensing;yttrium;mathematical model;support vector machine svm collaborative representation cr hyperspectral image hsi classification joint model;hyperspectral imaging;context modeling	In this letter, we propose a novel hyperspectral image (HSI) classification method based on the joint collaborative representation (JCR) and support vector machine (SVM) models with decision fusion. First, motivated by the joint model, we adopt a JCR model to deal with HSI classification and develop an effective method to learn contextual basis vectors for the JCR model. Second, the mid-features are first extracted based on representation coefficients obtained by the JCR method and then used to train a multiclass SVM classifier. After that, we exploit a multiplicative fusion rule to combine the JCR and SVM models. We conduct numerous experiments to evaluate our method in comparison with other algorithms. The experimental results on three standard data sets demonstrate that our method achieves better performance than other competing ones.	algorithm;basis (linear algebra);coefficient;content repository api for java;effective method;experiment;horizontal situation indicator;oracle fusion architecture;support vector machine	Chunjuan Bo;Huchuan Lu;Dong Wang	2016	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2015.2504449	support vector machine;computer vision;computer science;hyperspectral imaging;yttrium;machine learning;pattern recognition;mathematical model;software testing;context model;structured support vector machine;physics;remote sensing	Vision	30.165168187226886	-44.010792207881494	27247
66b8617e6f78537de98ac68e986bae6f3f1e16b9	distributed inference for dirichlet process mixture models		Bayesian nonparametric mixture models based on the Dirichlet process (DP) have been widely used for solving problems like clustering, density estimation and topic modelling. These models make weak assumptions about the underlying process that generated the observed data. Thus, when more data are collected, the complexity of these models can change accordingly. These theoretical properties often lead to superior predictive performance when compared to traditional finite mixture models. However, despite the increasing amount of data available, the application of Bayesian nonparametric mixture models is so far limited to relatively small data sets. In this paper, we propose an efficient distributed inference algorithm for the DP and the HDP mixture model. The proposed method is based on a variant of the slice sampler for DPs. Since this sampler does not involve a pre-determined truncation, the stationary distribution of the sampling algorithm is unbiased. We provide both local thread-level and distributed machine-level parallel implementations and study the performance of this sampler through an extensive set of experiments on image and text data. When compared to existing inference algorithms, the proposed method exhibits state-of-the-art accuracy and strong scalability with up to 512 cores.	algorithm;approximation;cluster analysis;converge;experiment;gibbs sampling;mapreduce;mixture model;sampling (signal processing);scalability;slice sampling;stationary process;text corpus;time complexity;topic model;truncation;weak measurement	Hong Ge;Yutian Chen;Moquan Wan;Zoubin Ghahramani	2015			econometrics;machine learning;mathematics;statistics	ML	28.092442969281528	-28.840675010384775	27274
5b9cadfdfeae52e1ed7cc300079ff42f1ae91667	teste para regressão spline linear: simulação e aplicação a dados de crescimento de ovinos	regressao nao linear segmentada;growth curve;nonlinear models;ovinos;propriedades dos estimadores;regressao linear segmentada;sheep flock;linear and nonlinear regressions;properties of estimators;curva de crescimento;modelos nao lineares;computer simulation;simulacao;tese		spline (mathematics)	Diana Campos de Oliveira	2015			statistics;mathematics;spline (mathematics)	Crypto	35.82680430754065	-24.484797498777432	27287
6049a381a0b79a90344306a885f118b47936ac9f	occlusion-robust model learning for human pose estimation	support vector machines;torso;biological system modeling;training data;gaussian distribution;data models	In this paper we examine the efficacy of self-occlusion-aware appearance learning for the part based model. Appearance modeling with less accurate appearance data is problematic because it adversely affects entire learning process. We evaluate the effectiveness of mitigating the influence of self-occluded body parts to be modeled for better appearance modeling process. To meet this end, We introduce an effective method for scoring degree of self-occlusion and we employ an approach learning a sample proportionally weighted to the score. We present our approach improves the performance of human pose estimation.	3d pose estimation;effective method;hidden surface determination;human-based computation	Yuki Kawana;Norimichi Ukita	2015	2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)	10.1109/ACPR.2015.7486552	computer vision;simulation;computer science;machine learning	Vision	34.744502494580004	-46.51068702310361	27328
5c0f4df001069e60a7a8b9cc3e69dc4e68441632	use of pgm for form recognition	databases;directed graphs;model identification;object recognition;probability;probability directed graphs image classification interactive devices object recognition;shower areas online configuration;on line form;form model identification;pgm;form recognition;bayesian methods;keynote modelling;conditional probabilities;image classification;random variables;junctions;keynote form classification;probabilistic graphical model;electronic ink;probability distribution;shower design on line form probabilistic graphical model keynote modelling;shower design;conditional probabilities pgm form recognition probabilistic graphical model form model identification electronic pen electronic ink keynote form classification design modelling problem shower areas online configuration filled fields;ink;particle separators;filled fields;design modelling problem;junctions probability distribution bayesian methods random variables particle separators databases ink;conditional probability;electronic pen;interactive devices	This paper addresses the use of PGM (Probabilistic Graphical Model) for form model identification from just few items filled up by an electronic pen. Only the electronic ink is sent to the system without any indication on the form model. Two applications are made in this study: one is related to keynote form classification from its filled fields, while the second application concerns a design modelling problem for the on-line configuration of shower areas. In the former, only indications on the filled fields are sent to the system, while in the latter, the designer send strokes corresponding to the elements designed on the form model. In this application a unique form is proposed to the user to fill up the configuration of his shower area. The PGM is exploited advantageously in both cases translating precisely the relationships between corresponding elements in conditional probabilities, from individual elements up to the complete model constitution.	bayesian network;digital paper;electronic paper;graphical model;matlab;online and offline;random-access memory;system identification	Emilie Philippot;Abdel Belaïd;Yolande Belaïd	2012	2012 10th IAPR International Workshop on Document Analysis Systems	10.1109/DAS.2012.93	speech recognition;conditional probability;engineering;artificial intelligence;machine learning;engineering drawing;statistics	SE	41.51377238470616	-27.876245638613725	27378
2b9160a5b5a40344205a223221a0557eda3006ae	a study on the environmental map building for a mobile robot using infrared range-finder sensors	bayesian estimation mechanism environmental map building mobile robot infrared range finder sensors neural networks nonlinearity error backpropagation neural network probabilistic approach;map building;backpropagation neural network;sensor model;mobile robot;bayes methods;occupancy grid;mobile robots;probabilistic approach;backpropagation;microsensors mobile robots infrared detectors backpropagation bayes methods;bayesian estimator;infrared;microsensors;infrared detectors;neural network;infrared sensors mobile robots sensor phenomena and characterization costs mechanical sensors neural networks error correction bayesian methods charge coupled devices robot vision systems	"""This paper presents a methodology for building the high accuracy environmental map using a mobile robot. The design approach uses low cost inl?ared range-finder sensors incorporating with neural networks. To enhance the map quality, the emrs occurring f"""" the sawn are corrected The nonlinearity m r of the sensors is mmpensafed using a backpropagation neural network and the random error of readings including the uncertainty of environment is taken into a sensor model at probabilistic approach. The map is represented by occupancy grid h e w o r k and updated by the Bayesian estunation mechanism. The effectiveness of the proposed method is verified through a series of experiments (see \,ideo)."""	artificial neural network;backpropagation;experiment;mobile robot;nonlinear system;sensor	Heon-Hui Kim;Yun-Su Ha;Gang-Gyoo Jin	2003		10.1109/IROS.2003.1250713	mobile robot;computer vision;computer science;engineering;artificial intelligence;machine learning;artificial neural network	Robotics	53.202377109063	-33.74194486331893	27476
150f4d8a46dd90048acada63c42c12392c5706f5	automatic facial expression recognition using bags of motion words	facial expression recognition	We present a fully automatic approach for facial expression recognition based on a representation of facial motion using a vocabulary of local motion descriptors. Previous studies have shown that motion is sufficient for recognizing expressions. Moreover, by discarding appearance after optical flow estimation, our representation is invariant to the subjects’ ethnic background, facial hair and other confounders. Unlike most facial expression recognition approaches, ours is general and not specifically tailored to faces. Annotation efforts for training are minimal, since the user does not have to label frames according to the phase of the expression, or identify facial features. Only a single expression label per sequence is required. We show results on a database of 600 video sequences.	activity recognition;algorithm;algorithmic efficiency;brookgpu;coherence (physics);computation;graphics processing unit;hidden markov model;ibm notes;markov chain;multiprotocol label switching;optical flow;vocabulary	Liefei Xu;Philippos Mordohai	2010		10.5244/C.24.13	computer vision;facial action coding system;computer science;pattern recognition;three-dimensional face recognition;face hallucination	Vision	35.25649268000714	-48.99548134033188	27531
2f5e057e35a97278a9d824545d7196c301072ebf	capturing long-tail distributions of object subcategories	training computational modeling clustering algorithms optimization visualization force accuracy;large appearance variation object subcategories long tail distributions distributed algorithms large mixture models generalized notion of mixtures multiple subcategories discriminative clustering algorithm brute force fashion deformable mixtures voc objects object classes;pattern clustering distributed algorithms object detection	"""We argue that object subcategories follow a long-tail distribution: a few subcategories are common, while many are rare. We describe distributed algorithms for learning large- mixture models that capture long-tail distributions, which are hard to model with current approaches. We introduce a generalized notion of mixtures (or subcategories) that allow for examples to be shared across multiple subcategories. We optimize our models with a discriminative clustering algorithm that searches over mixtures in a distributed, """"brute-force"""" fashion. We used our scalable system to train tens of thousands of deformable mixtures for VOC objects. We demonstrate significant performance improvements, particularly for object classes that are characterized by large appearance variation."""	binary prefix;cluster analysis;distributed algorithm;long tail;mixture model;scalability	Xiangxin Zhu;Dragomir Anguelov;Deva Ramanan	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.122	computer vision;machine learning;pattern recognition;mathematics	Vision	34.7604995509797	-45.50048008043744	27553
b8647e06207a86042504249937cca21759bc3e54	automatic target recognition using multiple description coding models for multiple classifier systems	canal con ruido;ouverture synthetique;traitement signal;sistema multiple;image recognition;reconocimiento imagen;classification algorithm;deteccion blanco;high dimensionality;wireless network;radar abertura sintetica;cible multiple;noisy channel;multiple system;blanco multiple;intelligence artificielle;canal avec bruit;abertura sintetica;aperture;detection cible;automatic recognition;senal debil;signal processing;radar imaging;sar image;reconnaissance image;automatic target recognition;pattern classification;pattern recognition;multiple description coding;artificial intelligence;small signal;synthetic aperture;imagerie radar;multiple target;inteligencia artificial;reconnaissance forme;abertura optica;reconocimiento patron;ouverture optique;multiple classifier system;procesamiento senal;target detection;radar ouverture synthetique;signal faible;reconocimiento automatico;reconnaissance automatique;heterogeneous network;synthetic aperture radar;systeme multiple;classification forme	In this paper, we proposed a new multiple classifier system (MCS) based on multiple description coding (MDC) models. Our proposed method was inspired from the framework of transmitting data over heterogeneous network, especially wireless network. In order to support the idea of MDC in pattern classification, parallels between transmission of concepts (hypothesis) and transmission of information through a noisy channel are addressed. Furthermore, preliminary surveys on the biological plausible of the MDC concepts are also included. One of the benefits of our approach is that it allows us to formulate a generalized class of signal processing based weak classification algorithms. This will be very applicable for MCS in high dimensional classification problems, such as image recognition. Performance results for automatic target recognition are presented for synthetic aperture radar (SAR) images from the MSTAR public release data set.	automatic target recognition;multiple description coding	Widhyakorn Asdornwised;Somchai Jitapunkul	2003		10.1007/3-540-44938-8_34	computer vision;synthetic aperture radar;speech recognition;computer science;signal processing	Vision	31.12521308509268	-40.91621656998077	27584
1cd58eac46f362397b2d206499bb048144e9851c	camera cooperation for achieving visual attention	moving object;video surveillance;high resolution;pan tilt camera head;computer model;low resolution;kinematic calibration;field of view;stereo vision;camera calibration;visual attention	In this paper we address the problem of establishing a computational model for visual attention using cooperation between two cameras. More specifically we wish to maintain a visual event within the field of view of a rotating and zooming camera through the understanding and modeling of the geometric and kinematic coupling between a static camera and an active camera. The static camera has a wide field of view thus allowing panoramic surveillance at low resolution. High-resolution details may be captured by a second camera, provided that it looks in the right direction. We derive an algebraic formulation for the coupling between the two cameras and we specify the practical conditions yielding a unique solution. We describe a method for separating a foreground event (such as a moving object) from its background while the camera rotates. A set of outdoor experiments shows the two-camera system in operation.	computational model;experiment;image resolution	Radu Horaud;David Knossow;Markus Michaelis	2005	Machine Vision and Applications	10.1007/s00138-005-0182-9	computer simulation;smart camera;stereo camera;computer vision;camera auto-calibration;camera matrix;camera resectioning;simulation;image resolution;computer science;three-ccd camera;pinhole camera model;computer graphics (images)	Vision	50.0269117415743	-45.676060387933525	27656
884d0f2682c3ef167461598163b03e022d906e38	texture image classification with riemannian fisher vectors issued from a laplacian model	databases;probability density function;maximum likelihood estimation;brain modeling;laplace equations covariance matrices gaussian distribution image classification image texture;covariance matrices;brain modeling covariance matrices databases probability density function dispersion data models maximum likelihood estimation;dispersion;riemannian gaussian distribution texture image classification laplacian model image processing applications covariance matrices riemannian manifold generative models riemannian laplace distribution rld riemannian fisher vectors riemannian words model;data models	Many signal and image processing applications are based on the classification of covariance matrices. These latter are elements on a Riemannian manifold for which many generative models have been developed in the literature. Recently, the Riemannian Laplace distribution (RLD) has been proposed to model the within-class variability of images. In this context, the present paper proposes an application of RLDs to the definition of Riemannian Fisher vectors issued from this Laplacian model. The expression of these descriptors is derived for mixtures of RLDs and their relation with the Riemannian vectors of locally aggregated descriptors is shown. Some comparisons with the bag of Riemannian words model are also performed. All these aforementioned descriptors are applied to texture image classification to find the most discriminating one. Moreover, to determine the best model for fitting the data, the classification performances are compared to those given by the Riemannian Gaussian distribution.	computer vision;generative model;image processing;performance;spatial variability	Ioana Ilea;Lionel Bombrun;Christian Germain;Yannick Berthoumieu	2016	2016 IEEE 12th Image, Video, and Multidimensional Signal Processing Workshop (IVMSP)	10.1109/IVMSPW.2016.7528231	exponential map;statistical manifold;mathematical optimization;pattern recognition;mathematics;information geometry;statistics	Vision	31.132604049184256	-39.062806617234614	27752
2351fec06d2f69f3cfbcdf7e9b698275224b3d39	recursive nonlinear estimation: geometry of a space of posterior densities	bayes estimation;bayesian statistics;estimacion no lineal;aproximacion;algoritmo recursivo;differential geometry;systeme echantillonne;approximation;approximation theory;estimacion bayes;recursive algorithms;algorithme recursif;sampled data systems;estimacion parametro;geometrie differentielle;sistema muestreado;nonlinear estimation;recursive algorithm;parameter estimation;estimation parametre;geometria diferencial;estimation non lineaire;sampled system;estimation bayes;non linear estimation	Abstract   The paper establishes a geometric formulation for nonlinear parameter estimation using reduced statistics. If a  reduced , rather than sufficient statistic is used in estimation, an  equivalence class  of densities, [ p  ( t ) ], rather than the true posterior density  p  ( t )  is determined. Two questions arise in this connection: (1) What kind of a reduced statistic allows recursive computations? (2) What is the appropriate representative   p  ̂     (t)    of the equivalence class [ p  ( t ) ]? Typically, the first question is not posed at all and the second one is resolved by heuristic considerations. The present paper attempts to cast the problem into a solid mathematical structure. It closely follows the differential-geometric approach suggested in Kulhavý ( Automatica ,  26,  545–555, 1990) but goes into more detail. Roughly, admissible statistics are characterized here as homomorphisms of a group containing all possible likelihoods. A representative of the pertinent equivalence class is constructed by a projection along this class onto an orthogonal submanifold going through a prespecified density   p∗   and imbedded in the manifold of possible posteriors. The obtained projection   p  ̂     (t)    has attractive extremal properties: it minimizes the Kullback-Leibler distance from the “reference” point   p∗   and, at the same time, the dual Kullback-Leibler distance from the true point  p  ( t ) .	nonlinear system;recursion (computer science)	Rudolf Kulhavý	1992	Automatica	10.1016/0005-1098(92)90118-Y	sampled data systems;differential geometry;mathematical optimization;combinatorics;approximation;mathematics;bayesian statistics;estimation theory;statistics;recursion;approximation theory	Robotics	33.81947233603214	-25.40795750189848	27755
92e5c3ce0f1fd4e5778adc7a4edc066f4b38352a	an evaluation of extended vs. weighted least squares for parameter estimation in physiological modeling	weighted least squares;measurement error;gaussian maximum likelihood;maximum likelihood;predictive value;noisy data;weighted least square;standard deviation;extended least squares;statistical model;objective function;simulation study;parameter estimation;practice guideline;physiological models	Weighted least squares (WLS) is the technique of choice for parameter estimation from noisy data in physiological modeling. WLS can be derived from maximum likelihood theory, provided that the measurement error variance is known and independent of the model parameters and the weights are calculated as the inverse of the measurement error variance. However, using measured values in lieu of predicted values to quantify the measurement error variance is approximately valid only when the noise in the data is relatively low. This practice may thus introduce sampling variation in the resulting estimates, as weights can be seriously mis-specified. To avoid this, extended least squares (ELS) has been used, especially in pharmacokinetics. ELS uses an augmented objective function where the measurement error variance depends explicitly on the model parameters. Although it is more complex, ELS accounts for the Gaussian maximum likelihood statistical model of the data better than WLS, yet its usage is not as widespread. The use of ELS in high data noise situations will result in more accurate parameter estimates than WLS (when the underlying model is correct). To support this claim, we have undertaken a simulation study using four different models with varying amounts of noise in the data and further assuming that the measurement error standard deviation is proportional to the model prediction. We also motivate this in terms of maximum likelihood and comment on the practical consequences of using WLS and ELS as well as give practical guidelines for choosing one method over the other.	choose (action);estimated;estimation theory;extreme loading for structures;least squares;loss function;normal statistical distribution;optimization problem;population parameter;sample variance;sampling (signal processing);signal-to-noise ratio;simulation;standard deviation;statistical model;weight	Mary E. Spilker;Paolo Vicini	2001	Journal of biomedical informatics	10.1006/jbin.2001.1033	statistical model;mathematical optimization;maximum likelihood;quasi-maximum likelihood;maximum likelihood sequence estimation;estimation theory;standard deviation;least squares;statistics;observational error	ML	27.507129872696648	-24.001949622033738	27767
63001a83847e772597e7f5c6167b98a23b6988c6	bayesian learning using gaussian process for gas identification	mlp classifiers bayesian learning gaussian process gas identification principal components analysis taguchi gas sensors tgs microelectronic gas sensors k nearest neighbor knn multilayer perceptron classifiers;belief networks;principal component analysis array signal processing belief networks gas sensors gaussian processes learning artificial intelligence;gaussian processes;gas identification;mlp classifiers;array signal processing;multilayer perceptron;indexing terms;gas sensor;learning systems;gas sensor array;knn;multilayer perceptron classifiers;taguchi gas sensors;bayesian learning;principal components analysis;principal component analysis;microelectronic gas sensors;tgs;pattern recognition;k nearest neighbor;gaussian process;gas detectors;learning artificial intelligence;pattern recognition bayesian learning gas identification gas sensor array gaussian processes gps;gas sensors;principal com ponent analysis;gaussian processes gps;bayesian methods gaussian processes sensor arrays gas detectors gases pattern recognition principal component analysis neural networks microelectronics nearest neighbor searches	In this paper, a novel gas identification approach based on Gaussian process (GP) combined with principal components analysis is proposed. The effectiveness of this approach has been successfully demonstrated on an experimentally obtained dataset. Our aim is the identification of different gases with an array of commercial Taguchi gas sensors (TGS) as well as microelectronic gas sensors. The proposed approach is shown to outperform both K nearest neighbor (KNN) and multilayer perceptron (MLP) classifiers	experiment;gaussian process;k-nearest neighbors algorithm;memory-level parallelism;multilayer perceptron;principal component analysis;sensor;taguchi methods;the great giana sisters	Amine Bermak;Sofiane Brahim-Belhouari	2006	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2006.873804	speech recognition;computer science;engineering;machine learning;pattern recognition;gaussian process;k-nearest neighbors algorithm;statistics;principal component analysis	Vision	36.767534721700784	-33.77572106866098	27815
5763dcfa257323b8fe6b18bc6b1c92794e157e3a	stereo matching using conditional adversarial networks		Recently, adversarial networks have attracted increasing attentions for the promising results of generative tasks. In this paper we present the first application of conditional adversarial networks to stereo matching task. Our approach performs a conditional adversarial training process on two networks: a generator that learns the mapping from a pair of RGB images to a dense disparity map, and a discriminator that distinguishes whether the disparity map comes from the ground truth or from the generator. Here, both the generator and the discriminator take the same RGB image pair as an input condition. During this conditional adversarial training process, our discriminator gradually captures high-level contextual features to detect inconsistencies between the ground truth and the generated disparity maps. These high-level contextual features are incorporated into loss function in order to further help the generator to correct predicted disparity maps. We evaluate our model on the Scene Flow dataset and an improvement is achieved compared with the most related work pix2pix.	computer stereo vision	Hualong Huang;Bo Huang;Hongtao Lu;Huiyu Weng	2017		10.1007/978-3-319-70090-8_13	machine learning;artificial intelligence;discriminator;pattern recognition;computer science;adversarial system;ground truth;rgb color model	Vision	26.733999813336414	-50.550936993902006	27864
3c04bf7324eaf6a77822f0fb35f85dfa79eff781	epicflow: edge-preserving interpolation of correspondences for optical flow	middlebury epicflow optical flow estimation dense matching sparse matches variational energy minimization sparse to dense interpolation edge aware geodesic distance motion boundaries occlusions approximation scheme standard one level variational energy minimization edge preserving interpolation of correspondences mpi sintel kitti;variational techniques approximation theory differential geometry edge detection image matching image sequences interpolation minimisation;image edge detection accuracy minimization data structures boolean functions software	We propose a novel approach for optical flow estimation, targeted at large displacements with significant occlusions. It consists of two steps: (i) dense matching by edge-preserving interpolation from a sparse set of matches; (ii) variational energy minimization initialized with the dense matches. The sparse-to-dense interpolation relies on an appropriate choice of the distance, namely an edge-aware geodesic distance. This distance is tailored to handle occlusions and motion boundaries - two common and difficult issues for optical flow computation. We also propose an approximation scheme for the geodesic distance to allow fast computation without loss of performance. Subsequent to the dense interpolation step, standard one-level variational energy minimization is carried out on the dense matches to obtain the final flow estimation. The proposed approach, called Edge-Preserving Interpolation of Correspondences (EpicFlow) is fast and robust to large displacements. It significantly outperforms the state of the art on MPI-Sintel and performs on par on Kitti and Middlebury.	approximation;computation;distance (graph theory);energy minimization;interpolation;message passing interface;optical flow;sparse language;sparse matrix;variational principle	Jérôme Revaud;Philippe Weinzaepfel;Zaïd Harchaoui;Cordelia Schmid	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2015.7298720	mathematical optimization;combinatorics;bilinear interpolation;stairstep interpolation;mathematics;geometry;nearest-neighbor interpolation;multivariate interpolation	Vision	50.52177615971382	-51.63211416012884	27909
d289b71b610c82c5f7b378d2c71e56f4669985ae	human silhouette extraction method using region based background subtraction	background modeling;gait recognition;background subtraction;extraction method	Background subtraction methods have been used to obtain human silhouettes for gesture and gait recognition. However, background subtraction in pixel units is prone to error which decreases recognition performance significantly. In this paper we propose a novel background subtraction method that extracts foreground objects in region units. Together with the background model, an object's color and movement information are used to obtain the effective region object likelihood. Then an adaptive region decision function determines the object regions. Also, the sequential version of Horprasert's algorithm[2] is presented.	background subtraction	Jung-Ho Ahn;Hyeran Byun	2007		10.1007/978-3-540-71457-6_37	computer vision;speech recognition;background subtraction;computer science;pattern recognition	EDA	40.04535096293853	-48.82626243429845	27915
f96734630d102c5612420b3472d1c20eed0b96c1	shared shape spaces	image segmentation;high dimensionality;manifolds;gaussian processes;level set;gaze tracking;inference mechanisms;set theory;discrete cosine transform;three dimensional;latent variable model;accuracy;shape;three dimensional displays;discrete cosine transforms;image reconstruction;parameter space;shape parameter;shape image segmentation discrete cosine transforms three dimensional displays manifolds level set accuracy;discrete cosine transforms shape constrained segmentation parameter recovery 3d shape 3d pose shape topology shared gaussian process latent variable model multimodal shape parameter space level set pose recovery gaze tracking monocular 3d reconstruction;set theory discrete cosine transforms gaussian processes image reconstruction image segmentation inference mechanisms;gaussian process;3d reconstruction	We propose a method for simultaneous shape-constrained segmentation and parameter recovery. The parameters can describe anything from 3D shape to 3D pose and we place no restriction on the topology of the shapes, i.e. they can have holes or be made of multiple parts. We use Shared Gaussian Process Latent Variable Models to learn multimodal shape-parameter spaces. These allow non-linear embeddings of the high-dimensional shape and parameter spaces in low dimensional spaces in a fully probabilistic manner. We propose a method for exploring the multimodality in the joint space in an efficient manner, by learning a mapping from the latent space to a space that encodes the similarity between shapes. We further extend the SGP-LVM to a model that makes use of a hierarchy of embeddings and show that this yields faster convergence and greater accuracy over the standard non-hierarchical embedding. Shapes are represented implicitly using level sets, and inference is made tractable by compressing the level set embedding functions with discrete cosine transforms. We show state of the art results in various fields, ranging from pose recovery to gaze tracking and to monocular 3D reconstruction.	3d modeling;3d reconstruction;algorithm;approximation;autostereogram;cobham's thesis;discrete cosine transform;display resolution;distance transform;energy (psychological);expect;eye tracking;gaussian process;hilbert space;lvm;latent variable;multimodal interaction;nonlinear system;shape context;symposium on geometry processing	Victor Adrian Prisacariu;Ian D. Reid	2011	2011 International Conference on Computer Vision	10.1109/ICCV.2011.6126547	computer vision;mathematical optimization;pattern recognition;gaussian process;mathematics;statistics	Vision	46.347247162089225	-51.889975808816885	27947
0db36bf08140d53807595b6313201a7339470cfe	moving vistas: exploiting motion for describing scenes	motion attribute;image recognition;probability;chaotic system;snow;application software;chaos;computer model;dynamic model;in the wild dynamic scene;layout;computer vision;scene recognition;physics;layout humans application software computer vision snow automation educational institutions computational modeling physics chaos;computational modeling;trajectory;time series analysis;probability chaos image recognition natural scenes;dynamics;dynamic scene categorization;humans;correlation;video data;tornadoes;natural scenes;video data scene recognition dynamic scene categorization chaotic system in the wild dynamic scene motion attribute;dynamic scenes;automation	Scene recognition in an unconstrained setting is an open and challenging problem with wide applications. In this paper, we study the role of scene dynamics for improved representation of scenes. We subsequently propose dynamic attributes which can be augmented with spatial attributes of a scene for semantically meaningful categorization of dynamic scenes. We further explore accurate and generalizable computational models for characterizing the dynamics of unconstrained scenes. The large intra-class variation due to unconstrained settings and the complex underlying physics present challenging problems in modeling scene dynamics. Motivated by these factors, we propose using the theory of chaotic systems to capture dynamics. Due to the lack of a suitable dataset, we compiled a dataset of ‘in-the-wild’ dynamic scenes. Experimental results show that the proposed framework leads to the best classification rate among other well-known dynamic modeling techniques. We also show how these dynamic features provide a means to describe dynamic scenes with motion-attributes, which then leads to meaningful organization of the video data.	algorithm;categorization;chaos theory;compiler;computational model;outline of object recognition	Nitesh Shroff;Pavan K. Turaga;Rama Chellappa	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2010.5539864	computer simulation;layout;computer vision;dynamics;snow;application software;simulation;scene statistics;computer science;trajectory;automation;machine learning;time series;probability;tornado;computational model;correlation;statistics;computer graphics (images)	Vision	38.87453523616871	-47.38399025248255	27976
11c7f14f8d188d8a4535e7ea284456afb75f6cc4	use of off-line dynamic programming for efficient image interpretation	reinforcement learning;dynamic program;image interpretation;face recognition;information gain	Sparse coding is an unsupervised learning algorithm for finding concise, slightly higher-level representations of inputs, and has been successfully applied to self-taught learning, where the goal is to use unlabeled data to help on a supervised learning task, even if the unlabeled data cannot be associated with the labels of the supervised task [Raina et al., 2007]. However, sparse coding uses a Gaussian noise model and a quadratic loss function, and thus performs poorly if applied to binary valued, integer valued, or other non-Gaussian data, such as text. Drawing on ideas from generalized linear models (GLMs), we present a generalization of sparse coding to learning with data drawn from any exponential family distribution (such as Bernoulli, Poisson, etc). This gives a method that we argue is much better suited to model other data types than Gaussian. We present an algorithm for solving the L1regularized optimization problem defined by this model, and show that it is especially efficient when the optimal solution is sparse. We also show that the new model results in significantly improved self-taught learning performance when applied to text classification and to a robotic perception task.	algorithm;bernoulli polynomials;document classification;dynamic programming;generalized linear model;iteratively reweighted least squares;loss function;mathematical optimization;neural coding;optimization problem;principal component analysis;robot;sparse matrix;supervised learning;time complexity;unsupervised learning	Ramana Isukapalli;Russell Greiner	2003			mathematical optimization;computer science;artificial intelligence;machine learning;mathematics;kullback–leibler divergence;reinforcement learning;statistics	AI	25.057524490280553	-31.316771154164655	28007
d596904c2aad4653256e5f0bcdf5b73f1dd55665	a fast weighted registration method of 3d point cloud based on curvature feature	3d point cloud;curvature feature;distance-weighted icp	In order to realize the fast and accurate registration of 3D point cloud data, a new fast weighted registration method is proposed in this paper. Firstly, using curvature feature, the method samples the original 3D point cloud data to quickly find matching points and remove wrong point pairs. Secondly, by introducing the iterative re-weighted least squares (IRLS) algorithm, the method carries out coarse alignment of the scattered point cloud. Finally, the method presents an improved distance-weighted Iterative Closest Point (ICP) algorithm to achieve fine matching. The experimental results show that the method has good convergence, robustness and accuracy.	algorithm;feature model;iterative closest point;iterative method;iteratively reweighted least squares;matching (graph theory);point cloud;vergence	Bing Liu;Xuehai Gao;Houde Liu;Xueqian Wang;Bin Liang	2018		10.1145/3195588.3195595	robustness (computer science);point cloud;mathematical optimization;curvature;least squares;iterative closest point;mathematics;convergence (routing)	Vision	50.28029670884737	-51.23413015089299	28089
1ba8dc443b8c986a781fc1909a65c98ce958b10f	hierarchical decision-making of multi-sensor system for state estimation of machining process	data fusion decision making hierarchical architecture multi sensor system state estimation machining process;machining;sensor system;component;hierarchical architecture;team decision making;data fusion;state estimation;hierarchical architecture component data fusion decision making machining process;state estimation decision making machining sensor fusion;multi sensor system;sensory system;sensor fusion;decision making state estimation machining signal processing sensor systems condition monitoring inference algorithms noise level stress target tracking;machining process	Focused on the problem of state estimation in machining process, this paper bring out a hierarchical architecture of multi-sensory system containing level-data fusion and team-decision making. Taking the grinding wheel's cutting process as example, an experiment aimed at the architecture's performance on state monitoring and parameter adjustment is carried out, which obtained favorable results. The paper also makes a briefly analysis for it		Liang Wei;Li Yinhua;Li Jie	2006	APCCAS 2006 - 2006 IEEE Asia Pacific Conference on Circuits and Systems	10.1109/APCCAS.2006.342337	control engineering;computer vision;computer science;engineering;machine learning;sensor fusion;engineering drawing	Robotics	48.51561419232093	-32.27132536366394	28211
19a11034e1290b69e87bbd2068c32b967e3f3a01	a rapid scheme for slow-motion replay segment detection	slow-motion replay segment detection;slow-motion replay;last step;automatic detection;slow-motion feature;digital video;efficient data mining;frame-to-frame difference;mc-dct structure;rapid scheme;sports video;slow-motion segment	Efficient data mining for digital video has become increasingly important in recent years. In this paper, we present a new scheme for automatic detection of slow-motion replays in sports video. Several slow-motion features and some newly discovered characteristics of slow-motion segments are exploited to aid the detection. The first step of our method is based on the macroblock motion vector information, while the second step makes use of frame-to-frame difference under an MC-DCT structure to verify the output of the first step. The last step is applied to refine the segment boundaries. Unlike previous approaches, our method has great improvement in both speed and accuracy and a balance between efficiency and simplicity.		Wei-Hong Chuang;Dun-Yu Hsiao;Soo-Chang Pei;Homer H. Chen	2004		10.1007/978-3-540-30541-5_30	computer vision;real-time computing;simulation;computer science	NLP	39.79499953728238	-51.95936963790473	28236
87571fb681e7c99dc34041d038e7748610502131	on feature templates for particle filter based lane detection	maximum likelihood estimation;pattern discrimination feature templates particle filter lane detection feature descriptors image features particle hypothesis current measurement;mathematical models;feature extraction;traffic lanes;particle filtering numerical methods feature extraction maximum likelihood estimation;image analysis;detection and identification systems;particle filtering numerical methods;roads particle filters atmospheric measurements particle measurements feature extraction estimation	In this work we propose the application of state-of-the-art feature descriptors into a Particle Filter framework for the lane detection task. The key idea lies on the comparison of image features extracted from the actual measurement with a priori calculated descriptors. First, we demonstrate how a feature expectation can be extracted based on a particle hypothesis. We then propose to define the likelihood function in terms of the distance between the expected feature and the features calculated from the current measurement. We select the Histogram of Oriented Gradients as a descriptor and the Battacharyya distance as a metric. We show that this simple approach is powerful in terms of pattern discrimination and that it opens a new set of possibilities for increasing the robustness of lane detectors.	feature model;feature vector;histogram of oriented gradients;image gradient;machine learning;modal logic;particle filter;portable document format;scalability;sensor;support vector machine;synthetic data;synthetic intelligence	André Guilherme Linarth;Elli Angelopoulou	2011	2011 14th International IEEE Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2011.6083016	computer vision;feature detection;machine learning;pattern recognition;mathematics;feature	Vision	44.93317447969812	-49.169134399764246	28245
3d663e9dfd8acd62faea95abfa66d0ae70397c9a	efficient approximations to model-based joint tracking and recognition of continuous sign language	tracking approximation theory feature extraction gesture recognition;sign language processing;model based joint tracking;time complexity;preprocessing feature extraction;usu;sign language;training;head tracking;approximation approach;continuous sign language recognition;hypothesized word sequence;approximation theory;body part detection;visualization;handicapped aids;hidden markov models;preprocessing feature extraction model based joint tracking continuous sign language recognition hypothesized word sequence hand tracking head tracking body part detection gesture recognition sign language processing approximation approach;feature extraction;hand tracking;target tracking;gesture recognition;handicapped aids pattern recognition feature extraction particle tracking principal component analysis humans head spatial databases image recognition fuses;tracking	We propose several tracking adaptation approaches to recover from early tracking errors in sign language recognition by optimizing the obtained tracking paths w.r.t. to the hypothesized word sequences of an automatic sign language recognition system. Hand or head tracking is usually only optimized according to a tracking criterion. As a consequence, methods which depend on accurate detection and tracking of body parts lead to recognition errors in gesture and sign language processing. We analyze an integrated tracking and recognition approach addressing these problems and propose approximation approaches over multiple hand hypotheses to ease the time complexity of the integrated approach. Most state-of-the-art systems consider tracking as a preprocessing feature extraction part. Experiments on a publicly available benchmark database show that the proposed methods strongly improve the recognition accuracy of the system.	ampersand;approximation;baseline (configuration management);benchmark (computing);distortion;feature extraction;gesture recognition;iterative method;language identification;motion capture;preprocessor;time complexity;vehicle tracking system;word error rate	Philippe Dreuw;Jens Forster;Thomas Deselaers;Hermann Ney	2008	2008 8th IEEE International Conference on Automatic Face & Gesture Recognition	10.1109/AFGR.2008.4813439	computer vision;speech recognition;computer science;video tracking;pattern recognition	Vision	39.51007731762728	-49.42704714265526	28251
78e0ea139ce09ec8c951beb718bea1d6f1abd23b	keyframe extraction for human motion capture data based on joint kernel sparse representation	kernel;manifolds;data mining;skeleton;matrix decomposition;solid modeling;triangle con straint motion capture data keyframe extraction geodesic exponential kernel lp l2 norm;data mining kernel manifolds data models solid modeling skeleton matrix decomposition;data models	Human motion capture data, which are used to animate animation characters, have been widely used in many areas. To satisfy the high-precision requirement, human motion data are captured with a high frequency (120 frames/s) by a high-precision capture system. However, the high frequency and nonlinear structure make the storage, retrieval, and browsing of motion data challenging problems, which can be solved by keyframe extraction. Current keyframe extraction methods do not properly model two important characteristics of motion data, i.e., sparseness and Riemannian manifold structure. Therefore, we propose a new model called joint kernel sparse representation (SR), which is in marked contrast to all current keyframe extraction methods for motion data and can simultaneously model the sparseness and the Riemannian manifold structure. The proposed model completes the SR in a kernel-induced space with a geodesic exponential kernel, whereas the traditional SR cannot model the nonlinear structure of motion data in the Euclidean space. Meanwhile, because of several important modifications to traditional SR, our model can also exploit the relations between joints and solve two problems, i.e., the unreasonable distribution and redundancy of extracted keyframes, which current methods do not solve. Extensive experiments demonstrate the effectiveness of the proposed method.	experiment;kernel (operating system);key frame;keyword extraction;kinesiology;motion capture;neural coding;nonlinear system;regular expression;sparse approximation;sparse matrix;time complexity	Guiyu Xia;Huaijiang Sun;Xiaoqing Niu;Guoqing Zhang;Lei Feng	2017	IEEE Transactions on Industrial Electronics	10.1109/TIE.2016.2610946	data modeling;computer vision;kernel;manifold;computer science;machine learning;mathematics;geometry;solid modeling;matrix decomposition;skeleton	AI	29.430316943659083	-40.64552682280311	28266
843c947e3b67084521d05ab2e43c4454663e9d9e	dolphin detection and tracking	dolphins cameras tracking logic gates principal component analysis robustness streaming media;zoology behavioural sciences object detection object tracking video cameras video signal processing;dolphin pool positions dolphin detection dolphin tracking social marine mammals bottlenose dolphins social dynamics behavior prediction human interference multicamera setups captive dolphins behavior long term statistics video corpora background subtraction video data paths visualization real time compressive tracking algorithm 3d tracking	Detecting and tracking social marine mammals, including bottlenose dolphins, can help to explain their social dynamics, predict their behavior, and measure the impact of human interference. Multi-camera setups provide an opportunity to record the behavior of captive dolphins and create a massive dataset from which long term statistics can be extracted, but the use of human experts to track dolphin positions over time demands a high time and financial investment cost. In this paper, we examine automated methods to analyze large video corpora. We use background subtraction to detect dolphins over time in the video data and to visualize the paths by which dolphins regularly traverse their environment. We demonstrate the use of these background subtraction methods as a way to initialize a preexisting real-time compressive tracking algorithm, which previously required human interaction. Detecting when dolphins transfer between pools both supplements 3D tracking efforts, helping identify moments when dolphins move between cameras, and augments human performance, providing a way for researchers to manually annotate dolphin pool positions 14 times as fast.	algorithm;background subtraction;captive portal;dolphin;dolphin;human reliability;interference (communication);online algorithm;real-time clock;sensor;social dynamics;traverse;text corpus	Jeremy Karnowski;Edwin Hutchins;Christine M. Johnson	2015	2015 IEEE Winter Applications and Computer Vision Workshops	10.1109/WACVW.2015.10	computer vision;simulation;engineering;communication	Vision	38.31925925228764	-45.920685540808485	28276
bc77a5e4132fda4d8d42773de60c6616bb9815da	vicon motion capture and hd 1080 standard video data fusion based on minimized markers reprojection error		Summary. We present an algorithm for quantity motion capture and multi camera HD 1080 standard reference video data fusion. It consists of initial calibration step which is based on some set of selected frames and final fusion for the rest of frames. Implemented data fusion algorithm can be used in case that it is possible to find a time interval when both devices were recording the same sequence of poses. It is worth to emphasise there are no special calibration patterns used during calibration. Advantage of the algorithm is that the required calibration step can be perfomed simultaneously with actor calibration from Vicon Blade system. It is also allowed that cameras locations can be changed during acquisition process as long as they observe known motion capture markers. After calibration and synchronization reprojection is possible in real time for VGA resolution or in reduced frequency for HD 1080 standard. Performed experiments determined that average projection error is about 1.45 pixel in the Full-HD 1920×1080 reference video and it is perceptualy acceptable. Practical usage for training video depersonification was presented.	motion capture;reprojection error;virtual reality headset	Karol Jedrasiak;Lukasz Janik;Andrzej Polanski;Konrad Wojciechowski	2011		10.1007/978-3-642-23154-4_24	computer vision;simulation;computer science;computer graphics (images)	Vision	49.418237870592385	-45.19843654871095	28341
276168bd2e0de077e52f19f50579ffe137d9c50b	fisher's linear spectral mixture analysis	tratamiento datos;target signature constrained mixed pixel classification tscmpc;proyeccion;teledetection;spectral analysis geophysical signal processing geophysical techniques image classification image processing least squares approximations;errors;least squares approximations;lsma;erreur;image processing;feature vector constrained fisher s linear spectral mixture analysis fvc flsma;least square error;fisher s linear spectral mixture analysis flsma;least squares error lse;linear spectral mixture analysis;pixel classification;abundance constrained fisher s linear spectral mixture analysis ac flsma;signal to noise ratio snr;data processing;imagerie;image classification;traitement donnee;fisher s linear discriminant analysis flda;analyse moindres carres;linear discriminate analysis;linearly constrained discriminant analysis lcda;classification;traitement image;deteccion a distancia;discriminant analysis;analyse discriminante;feature vector;synthetic image experiments;analisis discriminante;least squares;imagery;constrained least square;mixed pixel classification abundance constrained fisher s linear spectral mixture analysis ac flsma feature vector constrained fisher s linear spectral mixture analysis fvc flsma fisher s linear discriminant analysis flda fisher s linear spectral mixture analysis flsma linearly constrained discriminant analysis lcda;geophysical signal processing;projection;remote sensing;pixel;least square;pattern classification;linearly constrained discriminant analysis linear spectral mixture analysis subpixel analysis mixed pixel classification least square error criterion orthogonal subspace projection pattern classification fisher lsma linear discriminant analysis feature vector constrained flsma abundance constrained flsma;rapport signal bruit;imagineria;relacion senal ruido;error;target abundance constrained mixed pixel classification tacmpc;spectral analysis;signal to noise ratio;mixed pixel classification;orthogonal subspace projection	Linear spectral mixture analysis (LSMA) has been widely used in subpixel analysis and mixed-pixel classification. One commonly used approach is based on either the least square error (LSE) criterion such as least squares LSMA or the signal-to-noise ratio (SNR) such as orthogonal subspace projection (OSP). Unfortunately, it is known that such criteria are not necessarily optimal for pattern classification. This paper presents a new and alternative approach to LSMA, called Fisher's LSMA (FLSMA). It extends the well-known pure-pixel-based Fisher's linear discriminant analysis to LSMA. Interestingly, what can be done for the LSMA can be also developed for the FLSMA. Of particular interest are two types of constraints imposed on the LSMA, target signature-constrained LSMA and target abundance-constrained LSMA, which can be also derived in parallel for the FLSMA, to be called feature-vector-constrained FLSMA (FVC-FLSMA) and abundance-constrained FLSMA (AC-FLSMA), respectively. Since Fisher's ratio used by the FLSMA is a more appropriate classification criterion than the LSE or SNR used for the LSMA, the FVC-FLSMA improves over the classical least squares based LSMA and SNR-based OSP in mixed-pixel classification. Similarly, the AC-FLSMA also improves abundance-constrained least squares based LSMA in quantification of abundance fractions	feature vector;fingerprint verification competition;fisher–yates shuffle;linear discriminant analysis;linear least squares (mathematics);map (parallel pattern);pixel;signal-to-noise ratio;whole earth 'lectronic link	Chein-I Chang;Baohong Ji	2006	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2006.872085	data processing;image processing;machine learning;pattern recognition;mathematics;least squares;statistics	Vision	30.53320303341674	-41.890849760637444	28348
45b7453bf5d129306e3bad042c79d1bc480200d1	action classification using a discriminative multilevel hdp-hmm	discriminative classification;computer science and information systems;slice sampling;hdp hmm;action classification;depth image sequences	We classify human actions occurring in depth image sequences using features based on skeletal joint positions. The action classes are represented by a multi-level Hierarchical Dirichlet Process-Hidden Markov Model (HDP-HMM). The non-parametric HDP-HMM allows the inference of hidden states automatically from training data. The model parameters of each class are formulated as transformations from a shared base distribution, thus promoting the use of unlabelled examples during training and borrowing information across action classes. Further, the parameters are learnt in a discriminative way. We use a normalized gamma process representation of HDP and margin based likelihood functions for this purpose. We sample parameters from the complex posterior distribution induced by our discriminative likelihood function using elliptical slice sampling. Experiments with two different datasets show that action class models learnt using our technique produce good classification results. & 2014 Elsevier B.V. All rights reserved.	action potential;experiment;hidden markov model;markov chain;sampling (signal processing);slice sampling	Natraj Raman;Stephen J. Maybank	2015	Neurocomputing	10.1016/j.neucom.2014.12.009	speech recognition;slice sampling;machine learning;pattern recognition;mathematics;discriminative model;statistics	AI	33.06790820237904	-47.56409251378988	28355
ab781f035720d991e244adb35f1d04e671af1999	recurrent attentional reinforcement learning for multi-label image recognition		Recognizing multiple labels of images is a fundamental but challenging task in computer vision, and remarkable progress has been attained by localizing semantic-aware image regions and predicting their labels with deep convolutional neural networks. The step of hypothesis regions (region proposals) localization in these existing multi-label image recognition pipelines, however, usually takes redundant computation cost, e.g., generating hundreds of meaningless proposals with nondiscriminative information and extracting their features, and the spatial contextual dependency modeling among the localized regions are often ignored or over-simplified. To resolve these issues, this paper proposes a recurrent attention reinforcement learning framework to iteratively discover a sequence of attentional and informative regions that are related to different semantic objects and further predict label scores conditioned on these regions. Besides, our method explicitly models longterm dependencies among these attentional regions that help to capture semantic label co-occurrence and thus facilitate multilabel recognition. Extensive experiments and comparisons on two large-scale benchmarks (i.e., PASCAL VOC and MSCOCO) show that our model achieves superior performance over existing state-of-the-art methods in both performance and efficiency as well as explicitly identifying image-level semantic labels to specific object regions.	artificial neural network;computation;computer vision;convolutional neural network;experiment;information;multi-label classification;pipeline (computing);reinforcement learning	Tianshui Chen;Zhouxia Wang;Guanbin Li;Liang Lin	2018			convolutional neural network;machine learning;pattern recognition;computer science;computation;reinforcement learning;computer vision;artificial intelligence	Vision	29.048017994939777	-51.08228306723866	28357
81edaac4e2b82ed93cbaa879f4dc9c81a5a11826	pca-based edge-preserving features for hyperspectral image classification		Edge-preserving features (EPFs) obtained by the application of edge-preserving filters to hyperspectral images (HSIs) have been found very effective in characterizing significant spectral and spatial structures of objects in a scene. However, a direct use of the EPFs can be insufficient to provide a complete characterization of spatial information when objects of different scales are present in the considered images. Furthermore, the edge-preserving smoothing operation unavoidably decreases the spectral differences among objects of different classes, which may affect the following classification. To overcome these problems, in this paper, a novel principal component analysis (PCA)-based EPFs (PCA-EPFs) method for HSI classification is proposed, which consists of the following steps. First, the standard EPFs are constructed by applying edge-preserving filters with different parameter settings to the considered image, and the resulting EPFs are stacked together. Next, the spectral dimension of the stacked EPFs is reduced with the PCA, which not only can represent the EPFs in the mean square sense but also highlight the separability of pixels in the EPFs. Finally, the resulting PCA-EPFs are classified by a support vector machine (SVM) classifier. Experiments performed on several real hyperspectral data sets show the effectiveness of the proposed PCA-EPFs, which sharply improves the accuracy of the SVM classifier with respect to the standard edge-preserving filtering-based feature extraction method, and other widely used spectral-spatial classifiers.	eclipse process framework;edge enhancement;edge-preserving smoothing;feature extraction;horizontal situation indicator;linear separability;mean squared error;pixel;principal component analysis;support vector machine	Xudong Kang;Xuanlin Xiang;Shutao Li;Jon Atli Benediktsson	2017	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2017.2743102	support vector machine;mathematics;filter (signal processing);computer vision;artificial intelligence;principal component analysis;spatial analysis;feature extraction;smoothing;hyperspectral imaging;contextual image classification	Vision	30.64371076225261	-44.66463151169553	28403
05c1fde513e23e2ec95bb9366665e716a2fcea7d	people detection in surveillance: classification and evaluation	video surveillance;video sequences;automatic people detection;state classification;article;person model;object detection;state classification person model object detection video surveillance automatic people detection video sequences;video surveillance image classification image sequences object detection	This paper is a postprint of a paper submitted to and accepted for publication in IET Computer Vision and is subject to Institution of Engineering and Technology Copyright. The copy of record is available at IET Digital Library and at IEEE Xplore.		Alvaro García-Martín;José María Martínez Sanchez	2015	IET Computer Vision	10.1049/iet-cvi.2014.0148	computer vision;speech recognition;object-class detection;computer science;video tracking;multimedia	Vision	40.27958129251364	-47.20368727022009	28404
104b92375880f6bdbbc3e07fa150dcf9468680f0	a simd-accelerated software rendering pipeline for 3d graphics processing	graphics processing units;search problems;multiprocessing systems;unnecessary lighting operations simd accelerated software rendering pipeline 3d graphics processing multicore architecture armv5 isa simd unit window search bounding box algorithm traversal stage culling strategy;rendering computer graphics;parallel processing;search problems graphics processing units multiprocessing systems parallel processing rendering computer graphics;pipelines rendering computer graphics graphics processing units registers computer architecture	This paper presents a SIMD-accelerated software rendering pipeline for 3D graphics processing with multi-core architecture. The multi-core architecture is based on ARMv5 ISA which employs a SIMD unit developed by this work. We also propose a window search bounding box algorithm that can achieve zero failure in pixel tests so that speedup traversal stage is about 20 times faster than the traditional method. Finally, we use an early culling strategy to decrease unnecessary lighting operations.	3d computer graphics;algorithm;application programming interface;application-specific integrated circuit;computer architecture simulator;coprocessor;correctness (computer science);device driver;graphics pipeline;graphics processing unit;intel core (microarchitecture);minimum bounding box;multi-core processor;opengl es;pipeline (software);pixel;simd;simulation;software rendering;speedup	Eric Shianda Yu;Chung-Ho Chen	2012	2012 IEEE Asia Pacific Conference on Circuits and Systems	10.1109/APCCAS.2012.6419066	embedded system;parallel processing;tiled rendering;graphics pipeline;parallel computing;image-based modeling and rendering;fragment processing;computer hardware;rendering;computer science;operating system;texture mapping unit;parallel rendering;real-time computer graphics;real-time rendering;texture memory;computer graphics;alternate frame rendering;volume rendering;general-purpose computing on graphics processing units;software rendering;3d computer graphics;computer graphics (images)	Visualization	43.44663447615814	-32.47290736967364	28433
4d2560ba5f162af5b227f6e94b3353ee33243ad9	scaling log-linear analysis to high-dimensional data	data mining;graphical modeling log linear analysis scaling high dimensional data association discovery data mining task primary statistical approach classical statistical machinery advanced data mining techniques;statistical analysis;entropy computational modeling lattices data mining analytical models maximum likelihood estimation particle separators;statistical analysis data mining;log linear analysis association discovery data modeling high dimensional data	Association discovery is a fundamental data mining task. The primary statistical approach to association discovery between variables is log-linear analysis. Classical approaches to log-linear analysis do not scale beyond about ten variables. We develop an efficient approach to log-linear analysis that scales to hundreds of variables by melding the classical statistical machinery of log-linear analysis with advanced data mining techniques from association discovery and graphical modeling.	algorithm;association rule learning;breadth-first search;clique (graph theory);computation;data mining;graph (discrete mathematics);graphical model;information theory;lexicographical order;log-linear model;marginal model;maximal set;memoization;multiple edges;numerical analysis;scalability;stepwise regression;time complexity	François Petitjean;Geoffrey I. Webb;Ann E. Nicholson	2013	2013 IEEE 13th International Conference on Data Mining	10.1109/ICDM.2013.17	computer science;data science;data mining;mathematics;exploratory data analysis;statistics	ML	26.185069346250003	-27.513458322601466	28465
80231609104ff179e83df9c25dcb45e0d1a88ff7	active learning for vision-based robot grasping	robots;vision;manipulation;active learning;grasping;interval estimation	Reliable vision-based grasping has proved elusive outside of controlled environments. One approach towards building more flexible and domain-independent robot grasping systems is to employ learning to adapt the robot's perceptual and motor system to the task. However, one pitfall in robot perceptual and motor learning is that the cost of gathering the learning set may be unacceptably high. Active learning algorithms address this shortcoming by intelligently selecting actions so as to decrease the number of examples necessary to achieve good performance and also avoid separate training and execution phases, leading to higher autonomy. We describe the IE-ID3 algorithm, which extends the Interval Estimation (IE) active learning approach from discrete to real-valued learning domains by combining IE with a classification tree learning algorithm (ID-3). We present a robot system which rapidly learns to select the grasp approach directions using IE-ID3 given simplified superquadric shape approximations of objects. Initial results on a small set of objects show that a robot with a laser scanner system can rapidly learn to pick up new objects, and simulation studies show the superiority of the active learning approach for a simulated grasping task using larger sets of objects. Extensions of the approach and future areas of research incorporating more sophisticated perceptual and action representation are discussed	3d scanner;active learning (machine learning);approximation;autonomy;data acquisition;decision tree learning;experiment;extensibility;heuristic;id3 algorithm;interaction;interval arithmetic;list of algorithms;machine learning;robot;robotics;ross quinlan;scalability;simulation;superquadrics;verification and validation;ranger	Marcos Salganicoff;Lyle H. Ungar;Ruzena Bajcsy	1996	Machine Learning	10.1023/A:1018280807006	laser scanning;robot;robot learning;vision;computer vision;interval estimation;decision tree learning;motor learning;computer science;artificial intelligence;social robot;machine learning;motor system;active learning;stability;active learning	Robotics	50.24165493910679	-27.312385120480897	28509
dc22e4798f8473407963e0f29600ca8763727392	camera calibration by global constraints on the motion of silhouettes		We address the problem of epipolar geometry using the motion of silhouettes. Such methods match epipolar lines or frontier points across views, which are then used as the set of putative correspondences. We introduce an approach that improves by two orders of magnitude the performance over state-of-the-art methods, by significantly reducing the number of outliers in the putative matching. We model the frontier points’ correspondence problem as constrained flow optimization, requiring small differences between their coordinates over consecutive frames. Our approach is formulated as a Linear Integer Program and we show that due to the nature of our problem, it can be solved efficiently in an iterative manner. Our method was validated on four standard datasets providing accurate calibrations across very different viewpoints.	camera resectioning;correspondence problem;epipolar geometry;fits;graphical model;integer programming;iterative method;mathematical optimization;pipeline (computing)	Gil Ben-Artzi	2017	2017 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2017.570	order of magnitude;computer vision;artificial intelligence;outlier;approximation algorithm;camera auto-calibration;camera resectioning;epipolar geometry;computer science;correspondence problem;integer	Vision	51.86953935112752	-48.098087673196765	28529
fed40250cb2bb5b4a31af8c8d29598b0ba7ffeeb	monocular visual odometry scale recovery using geometrical constraint		Scale recovery is one of the essential problems for monocular visual odometry. The camera height is usually used as an absolute reference to recover the scale. In this case, the precision of scale recovery depends on the accuracy of the road region detection and road geometrical model calculation. In previous works, road detection and road geometrical model calculation are solved sequentially: the road geometrical model calculation is based on the road detection and the road region detection is based on the color information. However, the color information of a road is not stable enough. In the proposed method, the estimated road geometrical model is taken into consideration to detect the road region as a feedback. Therefore, the road region detection and road geometrical model estimation can benefit each other. Delaunay Triangulation method is used to segment an input image to many triangles with the matched feature points as vertices. Every triangle region is classified as a road region or not by comparing their geometrical model with that of the road and the road geometrical model is updated online. We evaluate our visual odometry scale recovery method on the KITTI dataset and the results show that our method is achieving the best performance among all existing monocular visual odometry scale recovery methods without additional sensors.	benchmark (computing);delaunay triangulation;feedback;median filter;sensor;visual odometry	Xiangwei Wang;Hui Zhang;Xiaochuan Yin;Mingxiao Du;Qijun Chen	2018	2018 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2018.8462902	computer vision;control theory;vertex (geometry);monocular;delaunay triangulation;visual odometry;engineering;artificial intelligence	Robotics	48.769584484779344	-46.67720990274317	28533
31c04feafedf238019b04814a78647544bc9f636	non-linear covariance estimation for reconstructing neural activity with meg/eeg data		MEG/EEG brain imaging approaches are commonly based on linear covariance matrices that contain the prior information needed to solve the inverse problem. We expect that non-linear covariance matrices (or kernel matrices) provide more information than the widely used smoothers (Loreta, MSP) or data-based matrices (beamformers). Data-based covariance matrices have shortcomings such as being prone to be singular, having limited capability in modeling, complicated relationships in the data, and having a fixed form of representation. The multiple sparse priors (MSP) algorithm provides flexibility but in its original form it only contains smoothers. In this work, we propose to modify both MSP and beamformers by introducing a Gaussian kernel matrix with the objective of enhancing the reconstruction of neural activity. The proposed approach was tested with two well-known simulation benchmarks: Haufe’s and SPM. Simulation results showed improvements in the ROIs recognition with Haufe’s benchmark, and smaller localization error with SPM benchmark. A real data validation (MEG and EEG) was performed with the faces-scrambled dataset. The expected active sources were obtained, but their strength presented slight variations.	electroencephalography;magnetoencephalography	Leonardo Duque-Muñoz;Juan David Martínez-Vargas;Germán Castellanos-Domínguez;Jesus Francisco Vargas Bonilla;J. D. López	2017		10.1007/978-3-319-59740-9_33	kernel (linear algebra);computer science;machine learning;artificial intelligence;gaussian function;inverse problem;data validation;prior probability;estimation of covariance matrices;matrix (mathematics);covariance	ML	27.0483645555812	-34.29672746747399	28543
dc67b5914836322e97c54c074e15b14bfcce2788	aggregating disparate judgments using a coherence penalty	judgment aggregation;databases;probability;de finetti s theorem;approximation algorithms;probability density function;real time;biological system modeling;data mining;aggregation;data structure disparate judgments probabilistic judgment aggregation coherent approximation principle algorithm successive orthogonal projection coherence penalty weighted principle;disparate judgments;successive orthogonal projection;approximation theory;probability aggregation approximation theory coherence;coherence penalty weighted principle;orthogonal projection;coherent approximation principle algorithm;coherence;probabilistic judgment aggregation;guidelines humans approximation algorithms data structures databases data mining finance meteorology aggregates contracts;data structure;algorithm design and analysis	In this paper, practical algorithms for solving the probabilistic judgment aggregation problem are given. First, the scalable Coherent Approximation Principle (CAP) algorithm proposed by Predd, et al., and its computational savings gained through Successive Orthogonal Projection are explained. Implications of de Finetti's theorem in this situation are also discussed. Then a coherence penalty is defined and the Coherence Penalty Weighted Principle (CPWP) is proposed to take advantage of the data structure alongside the coherence approximation. Justification is given for the guideline that more coherent judges should be given larger weights. Simulation results with Brier Scores on both a collected database and simulated data are given for comparison. In addition to the CPWP, a recursive online variant with weight updates is presented to accommodate real-time aggregation problems.	algorithm;approximation;coherent;computation;data structure;iterative method;real-time web;recursion;scalability;simulation;terminator 2: judgment day	Guanchun Wang;Sanjeev R. Kulkarni;H. Vincent Poor	2009	2009 43rd Annual Conference on Information Sciences and Systems	10.1109/CISS.2009.5054683	algorithm design;mathematical optimization;probability density function;discrete mathematics;coherence;data structure;computer science;probability;mathematics;orthographic projection;statistics;approximation theory	AI	32.30084494448434	-31.51742377957589	28660
f6d06fe577afb83c092d70478ef72f5bdd0a038d	anomaly detection for automotive visual signal transition estimation		We developed an end-to-end pipeline for brake light transition detection based on cognitive theories of anomaly-detection and model based systems engineering principles. Inspired by cognitive theory, we decompose the visual input stream into the submodalities color, shape, intensity and motion which is closely coupled with a graphical model. A memory module that contains priors is populated by exploitation of knowledge from specifications and simulation. High-fidelity 3D-Simulations have been created to populate motion memory, whereas low-fidelity 3D-projections have been used to create geometric priors. We captured real world sequences containing RGB images, GPS- and inertial measurement-data and annotated them with boundingboxes for cars and their tail and brakelight states, which are used to evaluate the proposed pipeline.	3d modeling;anomaly detection;cognitive science;computer simulation;donald becker;end-to-end encryption;end-to-end principle;experiment;graphical model;köppen climate classification;memory module;neurotechnology;population;propagation of uncertainty;signal transition;software propagation;stream (computing);systems engineering	Tobias Weis;Martin Mundt;Patrick Harding;Visvanathan Ramesh	2017	2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2017.8317605	memory module;computer vision;artificial intelligence;inertial frame of reference;rgb color model;anomaly detection;signal transition;prior probability;model-based systems engineering;engineering;graphical model	Robotics	29.20180863525204	-48.67117261048133	28716
4e2ed0ebef1c0a5029ee2a00b4261c321df97fd3	a 2d/3d vision based approach applied to road detection in urban environments	image segmentation;neural nets;matrix algebra;v disparity map road detection computer vision image segmentation watershed transform;roads image segmentation three dimensional displays urban areas image color analysis artificial neural networks;roads;stereo image processing;artificial neural network 3d vision based approach 2d vision based approach road detection urban environments image segmentation stereo vision system 2d layer watershed transform v disparity technique road pattern recognition;transforms;transforms image segmentation matrix algebra neural nets object detection roads stereo image processing;object detection	This paper presents an approach for road detection based on image segmentation. This segmentation is resulted from merging 2D and 3D image processing data from a stereo vision system. The 2D layer returns a matrix containing pixel's clusters based on the Watershed transform. Whereas the 3D layer return labels, that are classified by the V-Disparity technique, to free spaces, obstacles and non-classified area. Thus, a feature's descriptor for each cluster is composed with features from both layers. The road pattern recognition was performed by an artificial neural network, trained to obtain a final result from this feature's descriptor. The proposed work reports real experiments carried out in a challenging urban environment to illustrate the validity and application of this approach.	artificial neural network;binocular disparity;conditional random field;data acquisition;experiment;graphics processing unit;image processing;image segmentation;pattern recognition;pixel;stereopsis;stereoscopy;thierry coquand;watershed (image processing)	Giovani B. Vitor;Danilo Alves de Lima;Alessandro Corrêa Victorino;Janito V. Ferreira	2013	2013 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2013.6629589	computer stereo vision;image texture;computer vision;feature detection;scale space;geography;machine learning;segmentation-based object categorization;image segmentation;scale-space segmentation;feature;computer graphics (images)	Robotics	43.008336114641665	-44.074598282189946	28759
2a13822d53e9f43cf057ac231e6c2b7b1db0e95f	probabilistic recurrent state-space models		State-space models (SSMs) are a highly expressive model class for learning patterns in time series data and for system identification. Deterministic versions of SSMs (e.g. LSTMs) proved extremely successful in modeling complex time series data. Fully probabilistic SSMs, however, are often found hard to train, even for smaller problems. To overcome this limitation, we propose a novel model formulation and a scalable training algorithm based on doubly stochastic variational inference and Gaussian processes. In contrast to existing work, the proposed variational approximation allows one to fully capture the latent state temporal correlations. These correlations are the key to robust training. The effectiveness of the proposed PR-SSM is evaluated on a set of real-world benchmark datasets in comparison to state-of-the-art probabilistic model learning methods. Scalability and robustness are demonstrated on a high dimensional problem.	algorithm;benchmark (computing);control theory;doubly stochastic model;gaussian process;mathematical optimization;pr/sm;scalability;sparse matrix;state space;statistical model;stochastic gradient descent;system identification;time series;variational principle	Andreas Doerr;Christian Daniel;Martin Schiegg;Duy Nguyen-Tuong;Stefan Schaal;Marc Toussaint;Sebastian Trimpe	2018			robustness (computer science);machine learning;time series;probabilistic logic;pattern recognition;artificial intelligence;state space;inference;system identification;computer science;gaussian process;statistical model	ML	25.406904138529537	-30.35463984620052	28773
be4ab22fd70526ea110f3d34af6a53a1694d74b0	extracting objects and events from mpeg videos for highlight-based indexing and retrieval	mpeg video;indexing and retrieval;video highlights extraction;compressed domain processing;video semantics;content based retrieval;electrical engineering electronics nuclear engineering	Automatic recognition of highlights from videos is a fundamental and challenging problem for content-based indexing and retrieval applications. In this paper, we propose techniques to solve this problem using knowledge supported extraction of semantics, and compressed-domain processing is employed for efficiency. Firstly, knowledgebased rules are utilized for shot detection on extracted DCimages, and statistical skin detection is applied for human object detection. Secondly, through filtering outliers in motion vectors, improved detection of camera motions like zooming, panning and tilting are achieved. Video highlight high-level semantics are then automatically extracted via low-level analysis in the detection of human objects and camera motion events, and finally these highlights are taken for shot-level annotation, indexing and retrieval. Results using a large test video data set have demonstrated the accuracy and robustness of the proposed techniques.	color space;compressed sensing;discrete cosine transform;experiment;face detection;high- and low-level;moving picture experts group;object detection;real-time clock;shot transition detection	Jinchang Ren;Jianmin Jiang;Juan Chen;Stanley S. Ipson	2010	Journal of Multimedia	10.4304/jmm.5.2.95-103	computer vision;computer science;multimedia;information retrieval	Vision	38.53191182679025	-50.96446560333866	28789
139d77572315d245ed27ad0916b4c488b6b09fda	dynamically regularized harmony learning of gaussian mixtures	maximum likelihood estimation entropy gaussian processes learning artificial intelligence;big dataset dynamically regularized harmony learning gaussian mixtures drhl algorithm gaussian mixture learning adaptive model selection consistent parameter estimation bayesian ying yang harmony learning average shannon entropy harmony function maximum likelihood estimators;maximum likelihood gaussian mixtures model selection regularization;adaptation models heuristic algorithms maximum likelihood estimation bayes methods parameter estimation entropy vectors	In this paper, a dynamically regularized harmony learning (DRHL) algorithm is proposed for Gaussian mixture learning with a favourite feature of both adaptive model selection and consistent parameter estimation. Specifically, under the framework of Bayesian Ying-Yang (BYY) harmony learning, we utilize the average Shannon entropy of the posterior probability per sample as a regularization term being controlled by a scale factor to the harmony function on Gaussian mixtures increasing from 0 to 1 dynamically. It is demonstrated by the experiments on both synthetic and real-world datasets that the DRHL algorithm can not only select the correct number of actual Gaussians in the dataset, but also obtain the maximum likelihood (ML) estimators of the parameters in the actual mixture. Moreover, the DRHL algorithm is scalable and can be implemented on a big dataset.	algorithm;bayesian network;entropy (information theory);estimation theory;experiment;model selection;scalability;shannon (unit);synthetic intelligence;yang	Hongyan Wang;Jinwen Ma	2014	2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2014.6974070	machine learning;pattern recognition;mathematics;maximum likelihood sequence estimation;statistics	ML	29.376466300598693	-31.184045967298623	28860
768553c685a0e6ce616a9e853ba94bd86a2c8629	building gas concentration gridmaps with a mobile robot	gas distribution mapping;mobile robot;gas source localisation;data collection;gas sensor;laser range scanning;data och systemvetenskap;datalogi;mobile nose;datavetenskap datalogi;computer and systems science;computer science;weight function	This paper addresses the problem of mapping the structure of a gas distribution by creating concentration gridmaps from the data collected by a mobile robot equipped with gas sensors. By contrast to metric gridmaps extracted from sonar or laser range scans, a single measurement from a gas sensor provides information about a comparatively small area. To overcome this problem, a mapping technique is introduced that uses a Gaussian weighting function to model the decreasing likelihood that a particular reading represents the true concentration with respect to the distance from the point of measurement. This method is evaluated in terms of its suitability regarding the slow response and recovery of the gas sensors, and experimental comparisons of different exploration strategies are presented. The stability of the mapped structures and the capability to use concentration gridmaps to locate a gas source are also discussed.	mobile robot;sonar (symantec);sensor;weight function	Achim J. Lilienthal;Tom Duckett	2004	Robotics and Autonomous Systems	10.1016/j.robot.2004.05.002	mobile robot;computer vision;simulation;weight function;computer science;data collection	Robotics	53.298187801261086	-34.71768525317156	28889
cecd1bae0495610fc6f1ee05f6bd65d701b75259	locating people in video from semantic descriptions: a new database and approach	image color analysis databases clothing torso semantics cameras color;video retrieval clothing image colour analysis video cameras video databases;people location symmetry driven approach candidate region quality clothing colour semantic query semantic information stationary cameras unconstrained sequence database automated semantic description based localisation systems camera networks semantic descriptions	The location of previously unseen and unregistered individuals in complex camera networks from semantic descriptions is a time consuming and often inaccurate process carried out by human operators, or security staff on the ground. To promote the development and evaluation of automated semantic description based localisation systems, we present a new, publicly available, unconstrained 110 sequence database, collected from 6 stationary cameras. Each sequence contains detailed semantic information for a single search subject who appears in the clip (gender, age, height, build, hair and skin colour, clothing type, texture and colour), and between 21 and 290 frames for each clip are annotated with the target subject location (over 11, 000 frames are annotated in total). A novel approach for localising a person given a semantic query is also proposed and demonstrated on this database. The proposed approach incorporates clothing colour and type (for clothing worn below the waist), as well as height and build to detect people. A method to assess the quality of candidate regions, as well as a symmetry driven approach to aid in modelling clothing on the lower half of the body, is proposed within this approach. An evaluation on the proposed dataset shows that a relative improvement in localisation accuracy of up to 21% is achieved over the baseline technique.	baseline (configuration management);consistency model;language localisation;offset binary;population;semantic query;sequence database;stationary process	Michael Halstead;Simon Denman;Sridha Sridharan;Clinton Fookes	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.770	computer vision;computer science;multimedia;information retrieval	Vision	35.01273765529201	-51.72090901076603	28988
6fd867375e5fc91f0afa31bb6d0ae9098c7098e9	proximity-based anomaly detection using sparse structure learning	anomaly detection;reference data;time series data;multivariate data;covariance matrix;maximum likelihood estimate;conditional distribution	We consider the task of performing anomaly detection in highly noisy multivariate data. In many applications involving real-valued time-series data, such as physical sensor data and economic metrics, discovering changes and anomalies in the way variables depend on one another is of particular importance. Our goal is to robustly compute the “correlation anomaly” score of each variable by comparing the test data with reference data, even when some of the variables are highly correlated (and thus collinearity exists). To remove seeming dependencies introduced by noise, we focus on the most significant dependencies for each variable. We perform this “neighborhood selection” in an adaptive manner by fitting a sparse graphical Gaussian model. Instead of traditional covariance selection procedures, we solve this problem as maximum likelihood estimation of the precision matrix (inverse covariance matrix) under the L 1 penalty. Then the anomaly score for each variable is computed by evaluating the distances between the fitted conditional distributions within the Markov blanket for that variable, for the (two) data sets to be compared. Using real-world data, we demonstrate that our matrix-based sparse structure learning approach successfully detects correlation anomalies under collinearities and heavy noise.	anomaly detection;markov blanket;markov chain;sparse matrix;test data;time series	Tsuyoshi Idé;Aurelie C. Lozano;Naoki Abe;Yan Liu	2009		10.1137/1.9781611972795.9	time series;anomaly detection;machine learning;artificial intelligence;pattern recognition;covariance matrix;statistics;collinearity;multivariate statistics;conditional probability distribution;covariance;data set;mathematics	AI	29.952796865820357	-30.64960893912011	29001
41143984fee0814406e7b1fa3d00b0f9e5558624	single user multitouch on the diamondtouch: from 2 x 1d to 2d	input device;expectation maximization;diamondtouch;mixture of gaussians;multitouch;tracking	The DiamondTouch is a widely used multi-touch surface that offers high quality touch detection and user identification. But its underlying detection mechanism relies on two 1D projections (x and y) of the 2D surface. This creates ambiguous responses when a single user exercises multiple contacts on the surface and limits the ability of the DiamondTouch to provide full support of common multi-touch interactions such as the unconstrained translation, rotation and scaling of objects with two fingers. This paper presents our solution to reduce this limitation. Our approach is based on a precise modeling, using mixtures of Gaussians, of the touch responses on each array of antennas. This greatly reduces the shadowing of the touch locations when two or more fingers align with each other. We use these accurate touch detections to implement two 1D touch trackers and a global 2D tracker. The evaluation of our system shows that, in many situations, it can provide the complete 2D locations of at least two contacts points from the same user.	align (company);display resolution;image scaling;interaction;mixture model;multi-touch;sensor	François Bérard;Yann Laurillau	2009		10.1145/1731903.1731905	computer vision;simulation;engineering;communication	HCI	49.21109879466201	-44.593063316151216	29008
d1fa3036c8605d1391212d68103a37402a4d1855	automatic counting of interacting people by using a single uncalibrated camera	mean shift tracking algorithm;two level hierarchical tracking;single uncalibrated camera;code optimization;mean shift;traffic flow;2 ghz automatic counting system interacting people single uncalibrated camera two level hierarchical tracking fast blob tracking method mean shift tracking algorithm automatic learning traffic flow;2 ghz;telecommunication traffic;automatic learning;video cameras;region of interest;automatic counting system;interacting people;fast blob tracking method;tracking;video cameras telecommunication traffic tracking;cameras videos information security counting circuits sensor systems infrared heating robustness power system reliability target tracking job shop scheduling	Automatic counting of people, entering or exiting a region of interest, is very important for both business and security applications. This paper introduces an automatic and robust people counting system which can count multiple people who interact in the region of interest, by using only one camera. Two-level hierarchical tracking is employed. For cases not involving merges or splits, a fast blob tracking method is used. In order to deal with interactions among people in a more thorough and reliable way, the system uses the mean shift tracking algorithm. Using the first-level blob tracker in general, and employing the mean shift tracking only in the case of merges and splits saves power and makes the system computationally efficient. The system setup parameter can be automatically learned in a new environment from a 3 to 5 minute-video with people going in or out of the target region one at a time. With a 2 GHz Pentium machine, the system runs at about 33 fps on 320times240 images without code optimization. Average accuracy rates of 98.5% and 95% are achieved on videos with normal traffic flow and videos with many cases of merges and splits, respectively	algorithm;algorithmic efficiency;blob detection;interaction;mathematical optimization;mean shift;program optimization;region of interest;scream tracker	Senem Velipasalar;Ying-li Tian;Arun Hampapur	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262768	computer vision;simulation;mean-shift;computer science;program optimization;traffic flow;tracking;programming language;computer graphics (images);region of interest	Vision	44.14478860286535	-43.21896157932166	29126
3b8cc2582449796626da77191da0fc6e110d3760	visual tracking with complementary deep feature optimization		The challenges of rotation, occlusion, and scale change in visual object tracking have been an urgent problem to be solved. In recent years, the online learning ability and discriminative power of convolutional neural network (CNN) features have gained great attention in many computer vision tasks. Previous studies have shown that individual layer CNN features can be used for dealing with specific tracking challenges, such as target accurate positioning, rotation, and deformation. However, tracking with single CNN features is not enough to deal with serious challenges, such as the above-mentioned. To handle this problem, we evaluate the contribution of specific CNN layers features in tracking tasks and present a complementary CNN features tracking framework, which treats tracking procedure as a deep CNN features optimizing. Our tracker does not merely fuse the CNN features but is a semantic level method, which adds a competition mechanism to optimize the target appearance encoding of CNN layers. Therefore, the proposed tracker is robust not only to target location but also to rotation and deformation. We also embed hard negative mining to enhance the discriminative power of the tracking model and use bounding box regression to refine the tracking result. Having compared our tracker performance with other state-of-the-art trackers, the obtained experimental results in a large-scale dataset demonstrate that the effectiveness of our proposed tracker outperforms state-of-the-art ones.	mathematical optimization	Wei Wang;Zhaoming Chen;Mingquan Shi	2018	J. Electronic Imaging	10.1117/1.JEI.27.4.043052	convolutional neural network;computer vision;artificial intelligence;discriminative model;video tracking;pattern recognition;bittorrent tracker;eye tracking;minimum bounding box;computer science	DB	30.463790526723866	-50.877874425255364	29159
5278b0eeabc25f20e3cfb80dcff808bea6c5994a	buffon's needle model based walker recognition with distributed binary sensor networks	probability;sensors;distributed binary sensing;biological system modeling;buffon s needle;random variables;information geometry;wireless sensor networks gait analysis hidden markov models probability;hidden markov models;dynamics;gait analysis;needles hidden markov models correlation biological system modeling sensors random variables dynamics;correlation;pyroelectric sensor network buffon needle model walker recognition distributed binary sensor network distributed binary sensing paradigm geometric probability model low data throughput gait biometric system wireless sensor network gait recognition static distribution limb motion hidden markov bn model dynamic distribution static gait feature dynamic gait feature random projection principle information geometry;wireless sensor networks;walker recognition;needles;information geometry buffon s needle walker recognition distributed binary sensing	This paper presents a novel distributed binary sensing paradigm for walker recognition based on a well-known geometric probability model: Buffon's needle. The research aims to achieve a low-data-throughput gait biometric system suitable for wireless sensor network applications. We presents two types of Buffon's needle (BN) models for gait recognition: (1) a classical BN model based on a static distribution of limb motions; and (2) a hidden Markov BN model based on a dynamic distribution of limb motions. These two models are used to estimate static and dynamic gait features, respectively. By utilizing the random projection principle and the information geometry of binary variables, invariant measures of gait features are developed that can be independent of the walking path of subjects. We have performed both simulations and experiments to verify the proposed sensing theories. Although the experiments are based on a pyroelectric sensor network, the proposed sensing paradigm can be extended to various sensing modalities.	amiga walker;biometrics;experiment;gait analysis;information geometry;markov chain;programming paradigm;random projection;rigid needle adapter;sensor;simulation;throughput;whole earth 'lectronic link	Rui Ma;Qi Hao	2012	2012 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)	10.1109/MFI.2012.6343025	computer vision;simulation;engineering;machine learning	Robotics	37.055293904854956	-41.110132087786944	29191
7c7967eb2941d8fc5db5f5c8dcc057ada5b79633	incremental probabilistic geometry estimation for robot scene understanding	service robots incremental probabilistic geometry estimation robot scene understanding mobile robots mapping methods slam scene reconstruction methods tilting laser scanners iterative construction geometric mesh streaming time of flight range data;slam robots computational geometry image representation iterative methods mesh generation mobile robots optical scanners service robots;range data;time of flight;mobile robot;optical scanners;computational geometry;laser scanner;service robots;mobile robots;surface reconstruction;scene reconstruction;iterative methods;optical imaging;image representation;simultaneous localization and mapping;cameras surface reconstruction probabilistic logic simultaneous localization and mapping optical imaging;probabilistic logic;scene understanding;mesh generation;slam robots;cameras	Our goal is to give mobile robots a rich representation of their environment as fast as possible. Current mapping methods such as SLAM are often sparse, and scene reconstruction methods using tilting laser scanners are relatively slow. In this paper, we outline a new method for iterative construction of a geometric mesh using streaming time-of-flight range data. Our results show that our algorithm can produce a stable representation after 6 frames, with higher accuracy than raw time-of-flight data.	algorithm;iterative method;mobile robot;simultaneous localization and mapping;sparse matrix	Louis-Kenzo Cahier;Tetsuya Ogata;Hiroshi G. Okuno	2012	2012 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2012.6225343	mobile robot;computer vision;simulation;computational geometry;computer science;artificial intelligence;machine learning	Robotics	52.150536335136515	-45.93216996421834	29206
dab3c0c7ac52109911ec1025e03b72efa6088d0b	semi-supervised deep attribute networks for fine-grained ship category recognition		Classifying ships in satellite or aerial images is a challenging problem in remote sensing imagery. This task requires data-hungry learning algorithms (in particular deep models) that build discriminative representations which capture highly variable ship categories. As labeled training data are scarce and expensive, ship category recognition should also rely on abundant unlabeled data in order to enhance its effectiveness. In this paper, we introduce a novel representation learning algorithm based on semi-supervised attributes. Our method allows us to learn deep and discriminative image characteristics shared among different categories while taking into account both labeled and unlabeled data. We demonstrate the effectiveness of this method on the challenging ship category recognition problem and we show its out-performance with respect to related baselines, especially under the regime of scarce labeled data and abundant unlabeled ones.	aerial photography;algorithm;baseline (configuration management);discriminative model;feature learning;machine learning;pattern recognition;semi-supervised learning;semiconductor industry	Quentin Oliveau;Hichem Sahbi	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8517589	task analysis;discriminative model;computer vision;labeled data;artificial intelligence;computer science;training set;feature learning;linear programming;pattern recognition	Vision	27.999544403939282	-50.544144159437394	29208
ef79bb498cdf0e69c4222a86e650212f64787feb	hybrid, frame and event based visual inertial odometry for robust, autonomous navigation of quadrotors		Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. These cameras do not suffer from motion blur and have a very high dynamic range, which enables them to provide reliable visual information during high speed motions or in scenes characterized by high dynamic range. However, event cameras output only little information when the amount of motion is limited, such as in the case of almost still motion. Conversely, standard cameras provide instant and rich information about the environment most of the time (in low-speed and good lighting scenarios), but they fail severely in case of fast motions, or difficult lighting such as high dynamic range or low light scenes. In this paper, we present the first state estimation pipeline that leverages the complementary advantages of these two sensors by fusing in a tightly-coupled manner events, standard frames, and inertial measurements. We show on the publicly available Event Camera Dataset that our hybrid pipeline leads to an accuracy improvement of 130% over event-only pipelines, and 85% over standard-framesonly visual-inertial systems, while still being computationally tractable. Furthermore, we use our pipeline to demonstrate— to the best of our knowledge—the first autonomous quadrotor flight using an event camera for state estimation, unlocking flight scenarios that were not reachable with traditional visualinertial odometry, such as low-light environments and highdynamic range scenes. SUPPLEMENTARY MATERIAL Video of the experiments: http://rpg.ifi.uzh.	autonomous robot;british informatics olympiad;cobham's thesis;experiment;frame language;gaussian blur;high dynamic range;image sensor;odometry;pipeline (computing);pixel	Antoni Rosinol Vidal;Henri Rebecq;Timo Horstschaefer;Davide Scaramuzza	2017	CoRR		motion blur;computer science;high dynamic range;inertial frame of reference;brightness;odometry;computer vision;visual odometry;artificial intelligence;pipeline transport	Vision	52.40624189248405	-45.00960772228211	29225
23515e0f5fbcba94dc78e719c127ea42d087c81d	a fast and robust pedestrian detection framework based on static and dynamic information	detectors;detectors feature extraction positron emission tomography tracking dynamics robustness training;sliding window strategy detection based tracking pedestrian detection dynamic information;training;traffic engineering computing feature extraction learning artificial intelligence motion estimation object detection object tracking pedestrians;motion estimation;positron emission tomography;dynamic information;pedestrians;sliding window strategy pedestrian detection framework static information dynamic information machine learning detection based tracking systems static pedestrian detectors motion information people detection performance motion cue false detection removal motion distribution detection speed adaboost detection window pyramid based scanning strategy 1hog feature extraction public data sets;detection based tracking;dynamics;feature extraction;object tracking;pedestrian detection;traffic engineering computing;robustness;learning artificial intelligence;tracking;object detection;sliding window strategy	With the powerful development of pedestrian detection technique based on sliding-window and machine-learning, detection-based tracking systems have become increasingly popular. Most of these systems rely on existing static pedestrian detectors only despite the obvious potential motion information for people detection. This paper proposes a novel pedestrian detection framework fusing static and dynamic features. Motion cue is firstly used to detect potential pedestrian regions. Secondly, static detector scans potential regions to get candidate pedestrian detections. Final detection results are improved by removing false detections based on their motion distribution. The proposed framework significantly raises detection speed and detection performance. Static detector of pedestrian in this paper is trained by AdaBoost with simplified HOG feature (1HOG). Additionally, we introduce a detection-window-pyramid based scanning strategy for quickly extracting 1HOG features. The experimental results on several public data sets show the effectiveness of the proposed approach.	adaboost;information privacy;machine learning;pedestrian detection;sensor;tracking system	Tao Xu;Hantian Liu;Yueliang Qian;Zhe Wang	2012	2012 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2012.66	computer vision;dynamics;detector;simulation;feature extraction;computer science;machine learning;video tracking;motion estimation;tracking;robustness	Robotics	41.03873267170851	-45.11849061281903	29315
30005a5979c3d0894f47391f58c7c5d1936e94e8	hierarchical variational bayesian matrix co-factorization	variational techniques bayes methods gaussian distribution inference mechanisms matrix decomposition maximum likelihood estimation;motion pictures;cold start problems;bayes methods;collaboration;variational techniques;bayesian methods;inference mechanisms;maximum likelihood estimation;computational modeling;machine learning;matrix decomposition;bayesian matrix factorization;matrix co factorization variational inference bayesian matrix factorization cold start problems collaborative prediction;inference algorithms;matrix co factorization variational inference;collaborative prediction;gaussian distribution;cold start problem hierarchical variational bayesian matrix cofactorization data matrix decomposition factor matrix variational posterior distribution gaussian likelihood empirical bayesian method hyperparameter marginal likelihood maximization hierarchical bayesian model variational inference algorithm gaussian wishart prior distribution movielens data mae rmse;bayesian methods matrix decomposition collaboration motion pictures computational modeling inference algorithms machine learning	Matrix co-factorization involves jointly decomposing several data matrices to approximate each data matrix as a product of two factor matrices, sharing some factor matrices in the factorization. We have recently developed variational Bayesian matrix co-factorization where factor matrices are inferred by computing variational posterior distributions in the case of Gaussian likelihood with Gaussian prior placed on factor matrices. Empirical Bayesian method was used, so hyperparameters are set to specific values determined by maximizing marginal likelihood. In this paper we present a hierarchical Bayesian model for matrix co-factorization in which we derive a variational inference algorithm to approximately compute posterior distributions over factor matrices as well as hyperparameters, placing Gaussian-Wishart prior on hyperparameters. Numerical experiments on MovieLens data demonstrate that the hierarchical variational Bayesian matrix co-factorization alleviates the over-fitting better than the empirical variational Bayesian matrix co-factorization, leading to the improved performance in terms of MAE and RMSE.	approximation algorithm;bayesian network;calculus of variations;experiment;marginal model;movielens;multi-factor authentication;numerical method;overfitting;variational principle	Jiho Yoo;Seungjin Choi	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288275	normal distribution;bayesian probability;variational message passing;machine learning;pattern recognition;mathematics;bayesian linear regression;maximum likelihood;bayesian statistics;matrix decomposition;computational model;statistics;collaboration	ML	28.883298769950812	-32.63352397173138	29328
d5c0b96985c04029b031e837406f67dbd02c9b14	survey on various traffic monitoring and reasoning techniques		Traffic monitoring and surveillance is advancing in recent years. This paper proposes a survey on an overview of various traffic sensing, monitoring techniques. Several projects have been developed for the detection and tracking of vehicles on multiple scenarios. The vehicle monitoring results depends mainly on the camera positioning. This paper gives a detailed description on the different camera positioning and monitoring which includes straight roads and intersections. In this survey a detailed description is given on preprocessing techniques, vehicle detection and tracking methods. Finally paper concludes with the challenges and the future scope.		H. Haritha;T. Senthil Kumar	2017		10.1007/978-3-319-57261-1_50	management science;simulation;computer science	AI	41.75801109567712	-41.475534965673525	29394
f369b231b0310745fbad50c690b67b5d8e569568	intrinsic bayesian model for high-dimensional unsupervised reduction	dimension reduction;inductive cognitive;intrinsic bayesian;unsupervised model	This paper proposes a novel algorithm for high-dimensional unsupervised reduction from intrinsic Bayesian model. The proposed algorithm is to assume that the pixel reflectance results from nonlinear combinations of pure component spectra contaminated by additive noise. The constraints are naturally expressed in intrinsic Bayesian literature by using appropriate abundance prior distributions. The posterior distributions of the unknown model parameters are then derived. The proposed algorithm consists of intrinsic Bayesian inductive cognition part and hierarchical reduction algorithm model part. The algorithm has several advantages over traditional distance based on Bayesian reduction algorithms. The proposed reduction algorithm from intrinsic Bayesian inductive cognitive model is used to decide which dimensions are advantageous and to output the recommended dimensions of the hyperspectral image. The algorithm can be interpreted as a novel fast reduction inference method for intrinsic Bayesian inductive cognitive model. We describe procedures for learning the model hyperparameters, computing the dimensions distribution, and extensions to the intrinsic Bayesian inductive cognition model. Experimental results on hyperspectral data demonstrate robust and useful properties of the proposed reduction algorithm.	bayesian network	Longcun Jin;Wanggen Wan;Yongliang Wu;Cui Bin;Xiaoqing Yu	2012	Neurocomputing	10.1016/j.neucom.2011.03.060	wake-sleep algorithm;computer science;machine learning;pattern recognition;mathematics;bayesian linear regression;bayesian hierarchical modeling;statistics;dimensionality reduction	NLP	29.603782218745884	-33.47669925941645	29433
a388df85d61c21a673cbe1a229b1e833eeb2ec08	occluded vehicle detection with local connected deep model	monocular vision;deep model;occlusion type matching;vehicle detection;occluded vehicle;期刊论文	Traditional vehicle detection algorithms do not include targeted processing to handle the vehicle occlusion phenomenon. To address this issue, this paper proposes a locally-connected, deep-model-based, occluded vehicle detection algorithm. Firstly, a suspected occluded vehicle is generated using a cascaded Adaboost Classifier. Any sub-images that are rejected during the last two stages of the cascaded Adaboost Classifier are considered as a suspected occluded vehicle. Then, eight types of vehicle occlusion visual models are manually established. The suspected occluded vehicle will be assigned to a certain type of model by color histogram matching. Finally, the sub image of the suspected occluded vehicle will be loaded into a locally connected deep model of the corresponding type to make the final determination. An experiment using the KITTI dataset has demonstrated that compared with existing vehicle detection algorithms such as the cascaded Adaboost, the Deformable Part Model (DPM), Deep Convolutional Neural Networks (DCNN) and the Deep Belief Network (DBN), this algorithm has a much higher occluded vehicle detection rate. Additionally, this method requires minimal extra processing time, at around 5 % higher than the cascaded Adaboost.	adaboost;algorithm;bayesian network;color histogram;convolutional neural network;deep belief network;histogram matching;national lidar dataset	Hai Wang;Yingfeng Cai;Xiaobo Chen;Long Chen	2015	Multimedia Tools and Applications	10.1007/s11042-015-3141-0	computer vision;simulation;speech recognition;computer science;monocular vision	AI	41.22918859210925	-45.971934293451994	29489
6121b3d97a38109c85941eeb6a93722e584e7519	socially adaptive path planning in human environments using inverse reinforcement learning	inverse reinforcement learning;learning from demonstration;navigation;obstacle avoidance;rgb d optical flow	A key skill for mobile robots is the ability to navigate e ciently through their environment. In the case of social or assistive robots, this involves navigating through human crowds. Typical performance criteria, such as reaching the goal using the shortest path, are not appropriate in such environments, where it is more important for the robot to move in a socially adaptive manner such as respecting comfort zones of the pedestrians. We propose a framework for socially adaptive path planning in dynamic environments, by generating human-like path trajectory. Our framework consists of three modules: a feature extraction module, Inverse Reinforcement Learning module, and a path planning module. The feature extraction module extracts features necessary to characterize the state information, such as density and velocity of surrounding obstacles, from a RGB-Depth sensor. The Inverse Reinforcement Learning module uses a set of demonstration trajectories generated by an expert to learn the expert’s behaviour when faced with di↵erent state features, and represent it as a cost function that respects social variables. Finally, the planning module integrates a threelayer architecture, where a global path is optimized according to a classical shortest-path objective using a global map known a priori, a local path is planned over a shorter distance using the features extracted from a RGB-D sensor and the cost function inferred from Inverse Reinforcement Learning module, and a low-level Beomjoon Kim E-mail: beomjoon.kim@mail.mcgill.ca Joelle Pineau School of Computer Science, McGill University, 3480 University, Canada Tel.: 514-398-5432 Fax: 514-398-3883 E-mail: jpineau@cs.mcgill.ca system handles avoidance of immediate obstacles. We evaluate our approach by deploying it on a real robotic wheelchair platform in various scenarios, and comparing the robot trajectories to human trajectories.	automated planning and scheduling;computer science;fax;feature extraction;high- and low-level;loss function;mobile robot;motion planning;reinforcement learning;shortest path problem;velocity (software development)	Beomjoon Kim;Joelle Pineau	2016	I. J. Social Robotics	10.1007/s12369-015-0310-2	computer vision;navigation;simulation;computer science;artificial intelligence;machine learning;obstacle avoidance	AI	51.177463048225114	-28.102868859661104	29498
3daa86227a654487be6bd568e415b907c4f46e24	stitching reliability for estimating camera focal length in panoramic image mosaicing	reliability engineering;cameras image sequences image resolution computer science reliability engineering computer science education length measurement robustness motion measurement position measurement;reliability;image resolution;reliability image sequences;panoramic image mosaicing;camera focal length estimation;camera rotational motion constraint;camera free rotation stitching reliability camera focal length estimation panoramic image mosaicing image sequences image intersection point camera rotational motion constraint;camera free rotation;length measurement;satisfiability;computer science education;image sequence;position measurement;robustness;panoramic image;computer science;image intersection point;motion measurement;stitching reliability;cameras;image sequences	This paper proposes a measurable criterion called “stitching reliability ’’ which reflects the stitching quality of panoramic mosaics. We also show that the accurate focal length from image sequences closely relates to the stitching reliability. However, current methods to recover the focal length are not very robust because mis-stitched areas in general panoramas are used in the computation of the focal length. In our method, the several images with high stitching reliability can be automatically selected to speed up the focal length computation. To measure the stitching reliability, our algorithm computes the difference between real and ideal position of “image intersection point ’’ for a camera rotational motion constraint. It is formally proved that the constraint is always satisfied in camera free rotation. Experiments show that the proposed method provides less deviation of focal length than others, and thus improves the quality of panoramic mosaics.	algorithm;computation;experiment;focal (programming language);image stitching	Hwa-Sung Kim;Hyun-Chul Kim;Won-Kyu Lee;Chang-Hun Kim	2000		10.1109/ICPR.2000.905408	computer vision;image resolution;image stitching;length measurement;computer science;reliability;mathematics;robustness;satisfiability;computer graphics (images)	Vision	53.44009813334223	-49.567482124557856	29522
c551c6e002ac4a6633476ffd8d97d90336d6c668	robust statistics for outlier detection	robust statistics;outlier detection	When analyzing data, outlying observations cause problems because they may strongly influence the result. Robust statistics aims at detecting the outliers by searching for the model fitted by the majority of the data. We present an overview of several robust methods and outlier detection tools. We discuss robust procedures for univariate, low-dimensional, and high-dimensional data such as estimation of location and scatter, linear regression, principal component analysis, and classification. C © 2011 John Wiley & Sons, Inc. WIREs Data Mining Knowl Discov 2011 1 73–79 DOI: 10.1002/widm.2	anomaly detection;data mining;john d. wiley;principal component analysis;sensor	Peter Rousseeuw;Mia Hubert	2011	Wiley Interdiscip. Rev. Data Min. Knowl. Discov.	10.1002/widm.2	robust statistics;anomaly detection;computer science;machine learning;data mining;statistics	ML	27.738159698982216	-24.412131656436554	29528
0df27f5b7dec409b12a577533f819e7b24071062	online classification of eye tracking data for automated analysis of traffic hazard perception	eye data;traffic hazard;classification;perception	Complex and hazardous driving situations often arise with the delayed perception of traffic objects. To automatically detect whether such objects have been perceived by the driver, there is a need for techniques that can reliably recognize whether the driver's eyes have fixated or are pursuing the hazardous object (i.e., detecting fixations, saccades, and smooth pursuits from raw eye tracking data). This paper presents a system for analyzing the driver's visual behavior based on an adaptive online algorithm for detecting and distinguishing between fixation clusters, saccades, and smooth pursuits.	eye tracking	Enkelejda Kasneci;Thomas C. Kübler;Gjergji Kasneci;Wolfgang Rosenstiel;Martin Bogdan	2013		10.1007/978-3-642-40728-4_56	computer vision;simulation;biological classification;perception	ML	41.154927477493715	-46.02445950272879	29535
46c7afc4e5b9d84bb188e255579c3a4642db33be	robust eye tracking in video sequence	online affine manifold model;adaptive mask matching;eye feature tracking	In this paper, we present a novel method for eye tracking, in detail describing the eye contour and the visible iris center. Combining the IVT (Incremental Visual Tracking) tracker, the proposed online affine manifold model, in which the sequentially learning shape and texture are modeled in the first stage and noniterative recovering estimation in the second stage, tracks the eye contour in video sequences. After that, an adaptive black round mask is generated to match the visible iris center. Experimental results of eye tracking indicate that our tracker works well in the PC or domestic camera captured image streams with considerable head and eyeball rotation.	eye tracking	Huchuan Lu;Shipeng Lu;Gang Yang	2012	Journal of Circuits, Systems, and Computers	10.1142/S0218126612500120	computer vision;simulation;computer science;computer graphics (images)	ML	47.65868317391639	-46.254589022503275	29579
c77c82f7cffe4812bd6d68ef999ed4f39c0cc90f	camera calibration of long image sequences with the presence of occlusions	optimisation;cameras calibration image sequences rendering computer graphics video sequences layout image reconstruction computer science robustness computational modeling;video signal processing;data reconstruction;non linear optimization;data reconstructions camera calibration long image sequences occlusion image reconstruction 3d image model uncalibrated video sequence stratified linear algorithm metric structure nonlinear optimization;calibration image sequences image reconstruction optimisation video cameras multidimensional signal processing video signal processing;image based modeling;3d model;video cameras;image reconstruction;image sequence;multidimensional signal processing;camera calibration;augmented reality;divide and conquer;calibration;conference lecture;image sequences	obtain an robust reconstruction the image sequence must CAMERA CALIBRATION OF LONG IMAGE SEQUENCES WITH THE PRESENCE OF OCCLUSIONS Miguel Sainz Antonio Susin Nader Bagherzadeh Image Based Modeling and Rendering Lab Dynamic Simulation Lab Image Based Modeling and Rendering Lab. Dept. of Electrical and Computer Science Dept. Matematica Aplicada 1 Dept. of Electrical and Computer Science University of California, Irvine, USA Universitat Politecnica de Catalunya, SPAIN University of California, Irvine, USA msainz@ece.uci.edu toni.susin@upc.es nader@uci.edu Camera calibration is a critical problem in applications such as augmented reality and image based model reconstruction. When constructing a 3D model of an object from an uncalibrated video sequence, large amounts of frames and self occlusions of parts of the object are common and difficult problems. In this paper we present a fast and robust algorithm that uses a divide and conquer strategy to split the video sequence into sub-sequences containing only the most relevant frames. Then a robust stratified linear based algorithm is able to calibrate each of the subsequences to a metric structure and finally the subsequences are merged together and a final non-linear optimization refines the solution. Examples of real data reconstructions are presented. INTRODUCTION In recent years Image Based Modeling and Rendering (IBMR) techniques have demonstrated the advantages of using real image data to greatly improve the rendering quality in virtual environments. New rendering algorithms have been presented that reach a photorealistic quality at interactive speeds when rendering 3D models by using images of real objects and some additional shape information (i.e. a geometric proxy). While these methods have emphasized the rendering speed and quality, they generally require extensive preprocessing in order to obtain accurately calibrated images and geometric proxies of the target objects. Moreover, most of these algorithms require user interaction for the camera calibration and image registration part or need the use of expensive equipment such as calibrated gantries and 3D scanners. In this paper we present a method to calibrate and extract a set of key-frames from a video sequence that contain enough three dimensional information to be used as the reference views for an image based reconstruction algorithm. More specifically, the goal is to recover the 3D geometry of a scene from the 2D projections obtained from the digital images of multiple reference views, taking into account the motion of the camera. Moreover, since to show the object from different perspectives, selfocclusions are constantly present increasing the difficulty of the problem. Inspired in [1] and [2], we present a different novel approach based on a divide and conquer strategy to fully calibrate a long sequence of images with a high degree of feature occlusions such as video sequences of objects in rotating platforms. The complete sequence is automatically divided into subsequences and, in each of them, a set of key-frames is selected and calibrated using an improved version of the algorithm presented in [5], recovering both camera parameters and structure of the scene. When the different subsequences have been successfully calibrated a merging process groups them into a single set of cameras and reconstructed features of the scene. A final non-linear optimization is performed in order to reduce the overall reprojection error. One advantage of the presented approach is that it allows to recover an Euclidean reconstruction of the scene without any initial solution or prior information, which is one of the drawbacks of most of the existing methods. Another important feature is that the entire calibration process is based on solving linear systems using the SVD decomposition algorithm. The knowledge of the geometric meaning and rank properties of the different transformations represented by the matrices of the process allows to enforce a valid Euclidean reconstruction. The proposed solution is designed to be versatile in respect to the input data allowing the use of (1) automatically tracked video sequences, (2) manually tracked sequences which usually contain less frames or (3) a set of still images with features and correspondences manually selected. Here on, we assume that the input data is given as a sequence of images and a list of features in each image and the correspondences with the rest of the frames. SEQUENCE FRAGMENTATION The fragmentation algorithm starts with the set of features of frame i=0 and keeps track of them in the subsequent frames until one of the following is satisfied: • A minimum of key frames has been selected. • The end of the sequence is reached. • More than a user selected percentage of the original features are lost. When this occurs, a new subsequence is created as the set of key-frames starting from the first frame to the last keyframe before the ending condition has been triggered. At the same time, each frame is tested against the last found key-frame for a planar homography fit in order to determine how much three dimensional variation is present. Typically the homography error is small when there is little variation in the camera positions between the two frames. We use a RANSAC based approach to determine the percentage of inliers of the 2D homography between the two frames [3]. If this value is below a user selected threshold, it means that there exists some significant camera motion not modeled by the 2D homography, and the frame is marked as a key-frame. Otherwise, the frame is discarded and we proceed to the next frame. To guarantee connectivity between the different subsequences so they can be merged, the last key-frame of a subsequence is the first key-frame of the following one. FRAGMENT CALIBRATION Once a subsequence has been determined, we proceed to perform a complete metric calibration by extracting the measurements that appear in all the frames of the fragment into a measurement matrix W. The presented calibration solution is a stratified reconstruction based on linear factorization algorithms with a non linear optimization process to reduce the overall reprojection error. Moreover, a robust statistical 3D analysis based on RANSAC [3] is used to improve the quality of the reconstructions. Inlier determination Due to limitations and errors of the tracking algorithms, not all the features contained in the initial full measurement matrix are suitable to be used for the reconstruction. Therefore an initial filtering based on a RANSAC-type random sampling approach is needed in order to extract the set of inlying measures. Using this method we randomly select sets of four features, which is the minimum amount required to obtain a projective reconstruction. A solution for the projection matrices P is calculated for each set and the rest of the points are obtained as a least square solution for P and the 2D measurements. Then all the reconstructed points can be classified into inliers or outliers depending on the reprojection error. The solution that presents the largest number of inliers is kept. However, the proposed projective reconstruction algorithm is a robust but slow iterative approach and it is not suitable to be used multiple times as required by the RANSAC filtering. To accelerate the selection of the best set of inliers an closed-form affine reconstruction [3],[4] is performed based on the randomly selected sets. Once an initial set of inliers has been determined, an improved projective reconstruction is computed by iteratively reevaluating the set of inliers and calculating a new projective solution with the new set of inliers until the number of inlying measures remains constant. Usually this robust estimate converges in less that ten iterations, allowing the use of the most costly projective reconstruction algorithm. Projective Reconstruction The projective factorization method is a generalization of the factorization method which was first developed in [4], for the orthographic and the paraperspective projection models respectively. It provides a more general framework to recover shape and motion from multiple view images. Let Xj = (Xj, Yj, Zj , 1)T, j = 1, . . . , n, be the unknown homogeneous 3D point vectors, Pi, i = 1, . . . , m the unknown 3 x 4 image projections, and xij = (uij, vij , 1) the measured homogeneous image point vectors. We call projective depths the non-zero scale factors λij relating the world points and its projections	3d modeling;3d scanner;algorithm;augmented reality;calibration (statistics);cholesky decomposition;computer science;digital image;homography (computer vision);ip fragmentation;image registration;iteration;iterative method;key frame;linear programming;linear system;mathematical optimization;nonlinear programming;nonlinear system;orthographic projection;preprocessor;random sample consensus;randomness;rendering (computer graphics);reprojection error;sampling (signal processing);simulation;singular value decomposition;virtual reality headset	Miguel Sainz;Antonio Susín;Nader Bagherzadeh	2003		10.1109/ICIP.2003.1246962	iterative reconstruction;multidimensional signal processing;computer vision;augmented reality;divide and conquer algorithms;calibration;camera resectioning;computer science;theoretical computer science;computer graphics (images)	Vision	52.79638828933802	-50.09416135151939	29643
b13985f6b12d44ad627e40f7fa426aec582cd785	rank selection in nonnegative matrix factorization using minimum description length		Nonnegative matrix factorization (NMF) is primarily a linear dimensionality reduction technique that factorizes a nonnegative data matrix into two smaller nonnegative matrices: one that represents the basis of the new subspace and the second that holds the coefficients of all the data points in that new space. In principle, the nonnegativity constraint forces the representation to be sparse and parts based. Instead of extracting holistic features from the data, real parts are extracted that should be significantly easier to interpret and analyze. The size of the new subspace selects how many features will be extracted from the data. An effective choice should minimize the noise while extracting the key features. We propose a mechanism for selecting the subspace size by using a minimum description length technique. We demonstrate that our technique provides plausible estimates for real data as well as accurately predicting the known size of synthetic data. We provide an implementation of our code in a Matlab format.		Steven Squires;Adam Prügel-Bennett;Mahesan Niranjan	2017	Neural Computation	10.1162/neco_a_00980	nonnegative matrix	AI	27.844858738674898	-38.33774025693066	29649
eedfadd06568fb7bbc5649a94df158d6669df262	implementation of an electronic system to monitor the thermoregulatory capacity of honeybee colonies in hives with open-screened bottom boards	precision beekeeping;honey bee;arduino;monitoring;humidity;temperature	Electronic systems are not widely used for measuring biological variables in beehives despite the importance that honeybees have for both the environment and humans. A better understanding of bee colonies is needed in order to prevent certain dangers that threaten the bee population. In this study, we have developed an electronic system based on the Arduino Open Hardware platform, to which we have added temperature and humidity sensors to adapt the system to the particular conditions of beehive management. The system has been used to record changes in temperature and humidity inside hives and assess bees’ thermoregulation adaptability within colonies in hives with opened-screened bottom boards; an interesting management tool for controlling the harmful Varroa destructor mite as compared to beehives with conventional closed bottom boards. The results revealed that bee colonies were able to thermoregulate in hives with open anti-Varroa bottom boards to the same degree as those in conventional bottom board hives even under winter conditions in a mediterranean climate, thus indicating that there are no additional risks associated with the use of open-screened bottom board models. 2015 Elsevier B.V. All rights reserved.	arduino;destructor (computer programming);projection screen;sensor	Víctor Sánchez;Sergio Gil-Lebrero;José M. Flores;Francisco J. Quiles;Manuel Ortiz;Juan-Jesús Luna-Rodríguez	2015	Computers and Electronics in Agriculture	10.1016/j.compag.2015.10.018	temperature;computer science;engineering;humidity;arduino;ecology	DB	35.894475349558526	-35.67917718738771	29665
d2c9d269de31de43bf7e5b03ea1480e4156889ae	video synopsis for ir imagery considering video as a 3d data cuboid		Video synopsis is a way to transform a recorded video into a temporal compact representation. Surveillance videos generally contain huge amount of recorded data as there are a lot of inherent spatio-temporal redundancies in the form of segments having no activities; browsing and retrieval of such huge data has always remained an inconvenient job. We present an approach to video synopsis for IR imagery in which considered video is mapped into a temporal compact and chronologically analogous way by removing these inherent spatio-temporal redundancies significantly. A group of frames of video sequence is taken to form a 3D data cuboid with X, Y and T axes, this cuboid is re-represented as stack of contiguous (X-T) slices. With the help of Canny’s edge detection and Hough transform-based line detection, contents of these slices are analysed and segments having spatio-temporal redundancy are eliminated. Hence, recorded video is dynamically summarized on the basis of its content.	cuboid;video synopsis	Nikhil Kumar;Ashish Kumar;Neeta Kandpal	2016		10.1007/978-981-10-2104-6_21	computer vision;multimedia;computer graphics (images)	Vision	39.517585620926816	-50.755563020623185	29672
d95e4603e41ea5c326b2b0ffe51e29dcdf4c26f3	robust mean shift tracking based on refined appearance model and online update		In this paper, a robust mean shift tracking algorithm based on refined appearance model (RAM) and online update strategy is proposed. The main idea of the proposed algorithm is to construct a more accurate appearance model to improve tracking precision and design an online update strategy to adjust to the appearance variation. At the beginning of the tracking, the simple mean shift tracking algorithm is applied on the first few frames to collect a set of target templates, which contains both foreground and background of the target. During the model construction, simple linear iterative clustering (SLIC) algorithm is exploited to obtain the superpixels of the target templates, and the superpixels are further clustered to distinguish the foreground from background. A weighted vector is then obtained based on the classified foreground from background, which is utilized to modify the kernel histogram appearance model. The following frames are processed based on the mean shift tracking algorithm with the modified appearance model, and the stable tracking results with no occlusion will be selected to update the appearance model. The concrete operation of model update is the same as model construction. Experiment results on some challenging test sequences indicate that the proposed algorithm can well cope with both appearance variation and background change to obtain a robust tracking performance. A further comprehensive experiment on OTB2013 demonstrates that the proposed tracking algorithm outperforms the state-of-the-art works in most cases.	mean shift	Wangsheng Yu;Zhiqiang Hou;Xiaohua Tian;Dan Hu	2015		10.1007/978-3-662-48558-3_12	computer vision;machine learning;control theory	Vision	42.89191300986159	-49.27769305606077	29717
2427ca038004a470de346ab89de3e7ca17c9726f	providing guidance for maintenance operations using automatic markerless augmented reality system	maintenance operations paper based documentation augmented instruction point based tracker 3d particle filter tracking failure camera pose problem 3d model recognition tracking problem geometric features collision free trajectories assembly disassembly sequence disassembly planning module untextured 3d triangle meshes 3d parts offlne phase automatic markerless augmented reality system;three dimensional displays augmented reality feature extraction cameras electronic mail real time systems planning;computational geometry;solid modelling assembly planning augmented reality computational geometry maintenance engineering mesh generation particle filtering numerical methods production engineering computing;maintenance engineering;production engineering computing;assembly planning;augmented reality;mesh generation;solid modelling;particle filtering numerical methods	A real-time Augmented Reality system that assists in maintenance is presented (Figure 1). The novelty of the proposed system relies on its ability of being a complete framework that is self supplied. All the data is extracted automatically during an offlne phase from a collection of 3D parts that are provided as untextured 3D triangle meshes. Our disassembly planning module computes the assembly/disassembly sequence finding collision-free trajectories [1]. Moreover, some basic geometric features, such as edges and junctions, are extracted to address the tracking problem. Additionally, these geometric features let us perform the 3D model recognition, which solves the first camera pose problem and performs the recovery in case of tracking failure [3]. During the online phase, these geometric features are used to build a markerless tracking that updates continuously the camera pose from a monocular image. This markerless tracking combines an edge tracker, a point based tracker and a 3D particle filter to offer a robust 3D tracking against undesirable conditions [2]. Once the camera pose has been computed, the system offers an augmented instruction, indicating how to assemble/disassemble the next part. Thus, the proposed AR system is a complete framework that runs in real-time and tries to facilitate the work of workers, replacing the paper-based documentation.	augmented reality;bmc remedy action request system;correctness (computer science);disassembler;documentation;experiment;graphics processing unit;hgnc;particle filter;real-time locating system;real-time transcription;triangle mesh;usability;visual inspection	Hugo Álvarez;Iker Aguinaga;Diego Borro	2011	2013 IEEE Virtual Reality (VR)	10.1109/VR.2013.6549440	maintenance engineering;mesh generation;computer vision;augmented reality;simulation;computational geometry;computer science;computer graphics (images)	Vision	48.653556907811954	-43.811696350350296	29800
7ad5a8e5855b9ed30468d3c90ec8c4da93bece4a	multidimensional observation plans inducing nondegeneracy of information matrixes of regression models	matrix plan;linear regression model;regression model;least square;information matrix;method of least squares;complete orthogonal system of functions	Linear regression models for the response, which depends on several controlled variables, are considered. The method of constructing such a plan of observations that provides the nondegeneracy of the information matrix of the regression model is substantiated.		Aivars Lorencs	2010	Automatic Control and Computer Sciences	10.3103/S0146411610020021	segmented regression;principal component regression;total least squares;simple linear regression;econometrics;mathematical optimization;proper linear model;local regression;computer science;linear regression;linear predictor function;bayesian multivariate linear regression;linear model;polynomial regression;mathematics;linear probability model;partial least squares regression;design matrix;least squares;regression analysis;linear least squares;nonlinear regression;statistics	AI	30.070131011103392	-24.136573143865967	29806
3220ee78ec1499fcd395e5cb212ee62b55bd1856	in2i: unsupervised multi-image-to-image translation using generative adversarial networks		In unsupervised image-to-image translation, the goal is to learn the mapping between an input image and an output image using a set of unpaired training images. In this paper, we propose an extension of the unsupervised image-to-image translation problem to multiple input setting. Given a set of paired images from multiple modalities, a transformation is learned to translate the input into a specified domain. For this purpose, we introduce a Generative Adversarial Network (GAN) based framework along with a multi-modal generator structure and a new loss term, latent consistency loss. Through various experiments we show that leveraging multiple inputs generally improves the visual quality of the translated images. Moreover, we show that the proposed method outperforms current state-of-the-art unsupervised image-to-image translation methods.		Pramuditha Perera;Mahdi Abavisani;Vishal M. Patel	2018	2018 24th International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2018.8545464	machine learning;generative grammar;artificial intelligence;task analysis;modalities;semantics;feature extraction;pattern recognition;image translation;computer science;multi-image;image fusion	Vision	24.70799341801494	-49.33188838925759	29860
a6671a79b3636bd3d4a783099a890af9e103efdb	integrating spatial layout of object parts into classification without pairwise terms - application to fast body parts estimation from depth images		Object recognition or human pose estimation methods often resort to a decomposition into a collection of parts. This local representation has significant advantages, especially in case of occlusions and when the “object” is non-rigid. Detection and recognition requires modeling the appearance of the different object parts as well as their spatial layout. The latter can be complex and requires the minimization of complex energy functions, which is prohibitive in most real world applications and therefore often omitted. However, ignoring the spatial layout puts all the burden on the classifier, whose only available information is local appearance. We propose a new method to integrate the spatial layout into the parts classification without costly pairwise terms. We present an application to body parts classification for human pose estimation. As a second contribution, we introduce edge features from gray images as a complement to the well known depth features used for body parts classification from Kinect data.	computation;computational complexity theory;computer vision;emoticon;grayscale;kinect;lh (complexity);least squares;outline of object recognition;randomized algorithm;robot unicorn attack;whole earth 'lectronic link	Mingyuan Jiu;Christian Wolf;Atilla Baskurt	2013			computer vision;machine learning;pattern recognition;mathematics	ML	50.16321156383767	-48.444016168759894	29874
f0196793f4fe4b455fedf82336f490ee5e43c552	a study on development of visual navigation system based on neural network learning	neural networks;jang hee lee;hoon kang;vol 2 no 1;autonomous mobile robot;한국지능시스템학회;international journal of fuzzy logic and intelligent systems vol 2 no 1;navigation;suk young shin;korean institute of intelligent systems;vision;a study on development of visual navigation system based on neural network learning;yang jun you	"""It has been integrated into several navigation systems. This paper shows that system recognizes difficult indoor roads without any specific marks such as painted guide line or tape. In this method the robot navigates with visual sensors, which uses visual information to navigate itself along the read. The Neural Network System was used to learn driving pattern and decide where to move. In this paper, I will present a vision-based process for AMR(Autonomous Mobile Robot) that is able to navigate on the indoor read with simple computation. We used a single USB-type web camera to construct smaller and cheaper navigation system instead of expensive CCD camera."	artificial neural network;machine vision	Suk-Young Shin;Jang-Hee Lee;Yang-Jun You;Hoon Kang	2002	Int. J. Fuzzy Logic and Intelligent Systems	10.5391/IJFIS.2002.2.1.001	computer vision;simulation;engineering;artificial intelligence;mobile robot navigation	Robotics	47.2908765762582	-36.71801882306792	29897
4a751b1d49f2f96cdbb2443b313052df7f68b83e	gaze-based image retrieval system using dual eye-trackers	google;eye tracking content based image retrieval;image database;usa councils;monitoring;image database gaze based image retrieval system dual eye trackers dual monitor setup eye tracking device;visual databases image retrieval;eye tracking;content based image retrieval;image retrieval monitoring tracking usa councils google;tracking;visual databases;image retrieval	In this paper we present a novel gaze-based image retrieval application. The application is designed to be run on a dual monitor setup with a separate eye tracking device dedicated to each monitor. A source image is displayed on one monitor and the retrieved images are displayed on the second monitor. The system is completely gaze controlled. The user selects one or more objects or regions in the source image by fixating on them. The system then retrieves images containing similar objects from an image database. These are displayed in a grid on the second monitor. The user can then fixate on one of these images to select it as the new source image and the process can be repeated until a satisfactory image is found.	eye tracking;image retrieval;multi-monitor;tracking system	James Coddington;Junxia Xu;Srinivas Sridharan;Manjeet Rege;Reynold J. Bailey	2012	2012 IEEE International Conference on Emerging Signal Processing Applications	10.1109/ESPA.2012.6152440	computer vision;visual word;image processing;image retrieval;computer science;multimedia;automatic image annotation;information retrieval	Vision	47.541108249762935	-43.725113647874835	29904
1fbe05463498b910f20d46b8254f44ff366a735e	agwan: a generative model for labelled, weighted graphs		Real-world graphs or networks tend to exhibit a well-known set of properties, such as heavy-tailed degree distributions, clustering and community formation. Much effort has been directed into creating realistic and tractable models for unlabelled graphs, which has yielded insights into graph structure and evolution. Recently, attention has moved to creating models for labelled graphs: many real-world graphs are labelled with both discrete and numeric attributes. In this paper, we present AGWAN (Attribute Graphs: Weighted and Numeric), a generative model for random graphs with discrete labels and weighted edges. The model is easily generalised to edges labelled with an arbitrary number of numeric attributes. We include algorithms for fitting the parameters of the AGWAN model to real-world graphs and for generating random graphs from the model. Using real-world directed and undirected graphs as input, we compare our approach to state-of-the-art random labelled graph generators and draw conclusions about the contribution of discrete vertex labels and edge weights to graph structure.	algorithm;centrality;cluster analysis;cobham's thesis;directed graph;generative model;google map maker;graph (discrete mathematics);graph labeling;markov chain;markov random field;mixture model;random graph;synthetic intelligence;vertex (computer graphics);windows legacy audio components	Michael Davis;Weiru Liu;Paul C. Miller;Ruth F. Hunter;Frank Kee	2013		10.1007/978-3-319-08407-7_12	1-planar graph;block graph;random regular graph;pathwidth;random graph;split graph;combinatorics;discrete mathematics;cograph;universal graph;interval graph;graph product;dense graph;computer science;pancyclic graph;network model;machine learning;symmetric graph;trapezoid graph;mathematics;maximal independent set;modular decomposition;partial k-tree;chordal graph;indifference graph;line graph	ML	25.464497528811695	-28.15822801389329	29924
47a9c4ea83050113771cc5846c245b6300ac1d02	a clustering approach to optimize online dictionary learning	de noising;learning algorithms;pattern clustering;optimisation;image coding;complexity theory;training;low level image processing;keywords clustering;representative dictionaries clustering approach online dictionary learning optimization low level image processing tasks image denoising image inpainting image sparse coding image representation dictionary reconstruction image encoding;offline;clustering online dictionary learning;conference paper;pattern clustering encoding image coding learning artificial intelligence optimisation;online dictionaries;inpainting;online dictionary learning;clustering;image reconstruction;noise reduction;dictionaries;clustering approach;clustering algorithms;dictionaries clustering algorithms complexity theory noise reduction image reconstruction encoding training;dictionary learning;optimization;learning artificial intelligence;off line dictionaries;encoding;sparse coding;on line setting;image encoding	Dictionary learning has emerged as a powerful tool for low level image processing tasks such as denoising and inpainting, as well as sparse coding and representation of images. While there has been extensive work on the development of online and offline dictionary learning algorithms to perform the aforementioned tasks, the problem of choosing an appropriate dictionary size is not as widely addressed. In this paper, we introduce a new scheme to reduce and optimize dictionary size in an online setting by synthesizing new atoms from multiple previous ones. We show that this method performs as well as existing offline and online dictionary learning algorithms in terms of representation accuracy while achieving significant speedup in dictionary reconstruction and image encoding times. Our method not only helps in choosing smaller and more representative dictionaries, but also enables learning of more incoherent ones.	algorithm;cluster analysis;dictionary;image processing;inpainting;machine learning;neural coding;noise reduction;online and offline;sparse matrix;speedup	Nikhil S. Rao;Fatih Murat Porikli	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288126	computer vision;k-svd;computer science;theoretical computer science;machine learning;pattern recognition;cluster analysis	Vision	25.135720258474308	-44.221776434209964	29983
3c2b1df09450144259bf6501df3da014785b3640	rigidity checking of 3d point correspondences under perspective projection	motion estimation object recognition image matching parameter estimation;object recognition;perspective projection;image matching;nonlinear parameter estimation;motion estimation;3d object recognition;linear algorithm rigidity checking 3d point correspondences perspective projection two dimensional views rigid configuration 3d object recognition correspondence matching structure from motion estimate 3d recovery equations matching condition;parameter estimation;point correspondences;structure from motion;rigidity checking;layout motion estimation parameter estimation organizing object recognition differential equations image matching computer errors iterative algorithms iterative methods	"""An algorithm is described which rapidly veri es the potential rigidity of three dimensional point correspondences from a pair of two dimensional views under perspective projection. The output of the algorithm is a simple yes or no answer to the question \Could these corresponding points from two views be the projection of a rigid con guration?"""" Potential applications include 3D object recognition from a single previous view and correspondence matching for stereo or motion over widely separated views. Our analysis begins with the observation that it is often the case that two views cannot provide an accurate structure-frommotion estimate because of ambiguity and ill-conditioning. However, it is argued that an accurate yes/no answer to the rigidity question is possible and experimental results support this assertion with as few as six pairs of corresponding points over a wide range of scene structures and viewing geometries. Rigidity checking veri es point correspondences by using 3D recovery equations as a matching condition. The proposed algorithm improves upon other methods that fall under this approach because it works with as few as six corresponding points under full perspective projection, handles correspondences from widely separated views, makes full use of the disparity of the correspondences, and is integrated with a linear algorithm for 3D recovery due to Kontsevich. The rigidity decision is based on the residual error of an integrated pair of linear and nonlinear structure-from-motion estimators. Results are given for experiments with synthetic and real image data. A complete implementation of this algorithm is being made publicly available."""	3d projection;3d single-object recognition;algorithm;assertion (software development);binocular disparity;condition number;correspondence problem;experiment;naruto shippuden: clash of ninja revolution 3;nonlinear system;outline of object recognition;structure from motion;synthetic intelligence	Daniel P. McReynolds;David G. Lowe	1996	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.546255	computer vision;mathematical optimization;structure from motion;perspective;computer science;cognitive neuroscience of visual object recognition;motion estimation;mathematics;geometry;estimation theory	Vision	52.97698194858826	-50.56908377994738	30045
c1cdf41e545e2dbc5fdd008e90164c5b5ad356d7	a multisensor architecture providing location-based services for smartphones	probabilistic techniques;802 11;image processing;localization;smartphones;sift	This paper introduces a multisensor architecture to fuse data acquired from different sensors available in commodity smartphones in order to build accurate locationbased services, and pursuing a good balance between accuracy and performance. Using scale invariant features from the images captured using the smartphone camera, we perform a matching process against previously obtained images to determine the current location of the device. Several refinements are introduced to improve the performance and the scalability of our proposal. Location fingerprinting, based on IEEE 802.11, will be used to determine a cluster of physical points, or zone, where the device seems to be according to the received signal strength. In this way, we will reduce the number of images to analyze to those contained in the tentative zone. Additionally, accelerometers will also be considered in order to improve the system performance, by means of a motion estimator. This set of techniques enables a wide range of location-based applications.	algorithmic efficiency;augmented reality;computation;experiment;fingerprint (computing);location-based service;response time (technology);scalability;sensor;smartphone	Antonio Jesus Ruiz Ruiz;Óscar Cánovas Reverte;Pedro E. López-de-Teruel	2013	MONET	10.1007/s11036-012-0423-x	embedded system;computer vision;simulation;internationalization and localization;image processing;computer science;scale-invariant feature transform;computer security	Mobile	44.747641165675724	-39.07097194160405	30079
ce82ebda83a4faddf1ab609d1310528bdf0122c6	classification of fault type on loop-configuration transmission system using support vector machine		This paper proposed to applied Support vector machine (SVM) algorithm for classified the fault type on the 500 kV transmission systems with connected in loop configuration. The fault signal was simulated using ATPDraw/EMTP program at frequency 200 kHz. The fault detection was analyzing the high frequency component by discrete wavelet transform (DWT). For the first stage, the coefficient of DWT was used for the fault detection. After the fault can be detected, the fault classification will be identified using SVM algorithm. The maximum coefficient from wavelet transform was used as input pattern of SVM to classify the type of fault. The input pattern of SVM consists of 4 input; maximum coefficient of DWT in all phase current and zero sequence current. For the SVM process, the fault classification used the five model of SVM because each model is working in parallel to avoid mistake (or error). In addition, the same input in five model were simultaneously used while the output of each models is differently according to specification of model. The overall result of 2160 case studies data can be summarized that the fault classification using SVM algorithm is highly satisfactory.	algorithm;coefficient;discrete wavelet transform;fault detection and isolation;support vector machine	Bancha Sreewirote;Atthapol Ngaopitakkul	2017	2017 6th IIAI International Congress on Advanced Applied Informatics (IIAI-AAI)	10.1109/IIAI-AAI.2017.203	wavelet transform;support vector machine;structured support vector machine;symmetrical components;transmission system;fault detection and isolation;discrete wavelet transform;emtp;computer science;artificial intelligence;pattern recognition	ML	36.78965969838625	-31.392119605203703	30084
d9f184e5519365f21fe807c38635f7f6e9ac0ed8	modeling rating data with nonlinear cub models	latent variables;transition probability;cub models;ordinal data;eurobarometer;decision process;rating data	A general statistical model for ordinal or rating data, which includes some existing approaches as special cases, is proposed. The focus is on the CUB models and a new class of models, called Nonlinear CUB, which generalize CUB. In the framework of the Nonlinear CUB models, it is possible to express a transition probability, i.e. the probability of increasing one rating point at a given step of the decision process. Transition probabilities and the related transition plots are able to describe the state of mind of the respondents about the response scale used to express judgments. Unlike classical CUB, the Nonlinear CUB models are able to model decision processes with non-constant transition probabilities.		Marica Manisera;Paola Zuccolotto	2014	Computational Statistics & Data Analysis	10.1016/j.csda.2014.04.001	latent variable;econometrics;markov chain;data mining;mathematics;ordinal data;statistics	ML	27.576608110474325	-25.740861337786214	30260
8d60be31ed288fcfec18995d910d9e23a994c90d	semisupervised hyperspectral image classification via neighborhood graph learning	hsi data set semisupervised hyperspectral image classification neighborhood graph learning semisupervised learning technique smoothness assumption remotely sensing hyperspectral image classification binary classifier semisupervised neural network;measurement;neural networks;training;semisupervised learning ssl aerial image analysis hyperspectral image hsi classification neural networks;remote sensing hyperspectral imaging image classification image processing learning artificial intelligence;laplace equations;hyperspectral imaging;training hyperspectral imaging neural networks measurement laplace equations	In problems where labeled data are scarce, semisupervised learning (SSL) techniques are an attractive framework that can exploit both labeled and unlabeled data. These approaches typically rely on a smoothness assumption such that examples that are similar in input space should also be similar in label space. In many domains, such as remotely sensed hyperspectral image (HSI) classification, the data violate this assumption. In response, we propose a general method by which a neighborhood graph used in SSL is learned using binary classifiers that are trained to predict whether a pair of pixels shares the same label. Working within the framework of semisupervised neural networks (SSNNs), we show that our approach improves on the performance of the SSNN on two HSI data sets.	algorithm;artificial neural network;dataspaces;feature learning;horizontal situation indicator;machine learning;pixel;scalability;semi-supervised learning;signal-to-noise ratio;unsupervised learning	Daniel Jiwoong Im;Graham W. Taylor	2015	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2015.2438227	computer vision;computer science;hyperspectral imaging;machine learning;pattern recognition;artificial neural network;physics;measurement;remote sensing	ML	28.951052767224397	-43.74071436583821	30277
18d5f0747a23706a344f1d15b032ea22795324fa	multi-modal auto-encoders as joint estimators for robotics scene understanding		We explore the capabilities of Auto-Encoders to fuse the information available from cameras and depth sensors, and to reconstruct missing data, for scene understanding tasks. In particular we consider three input modalities: RGB images; depth images; and semantic label information. We seek to generate complete scene segmentations and depth maps, given images and partial and/or noisy depth and semantic data. We formulate this objective of reconstructing one or more types of scene data using a Multi-modal stacked Auto-Encoder. We show that suitably designed Multi-modal Auto-Encoders can solve the depth estimation and the semantic segmentation problems simultaneously, in the partial or even complete absence of some of the input modalities. We demonstrate our method using the outdoor dataset KITTI that includes LIDAR and stereo cameras. Our results show that as a means to estimate depth from a single image, our method is comparable to the state-of-the-art, and can run in real time (i.e., less than 40ms per frame). But we also show that our method has a significant advantage over other methods in that it can seamlessly use additional data that may be available, such as a sparse point-cloud and/or incomplete coarse semantic labels.	autostereogram;depth map;encoder;missing data;modal logic;point cloud;robotics;sensor;sparse matrix;stereo cameras	Cesar Cadena;Anthony R. Dick;Ian D. Reid	2016		10.15607/RSS.2016.XII.041	computer vision;simulation;machine learning	Robotics	28.085621768234404	-49.43962872489556	30290
52943e2f944943e6c8f01373be09ce4f00062dbb	speech recognition based on student's t-distribution derived from total bayesian framework	loi student;bayesian framework;bayes estimation;bayesian prediction;prediccion;tecnologia electronica telecomunicaciones;total bayesian framework vbec;classification;ley student;estimacion bayes;automatic recognition;reconocimiento voz;speech recognition;reconnaissance parole;student distribution;tecnologias;grupo a;prediction;clasificacion;reconocimiento automatico;student s t distribution;reconnaissance automatique;estimation bayes	We introduce a robust classification method based on the Bayesian predictive distribution (Bayesian Predictive Classification, referred to as BPC) for speech recognition. We and others have recently proposed a total Bayesian framework named Variational Bayesian Estimation and Clustering for speech recognition (VBEC). VBEC includes the practical computation of approximate posterior distributions that are essential for BPC, based on variational Bayes (VB). BPC using VB posterior distributions (VB-BPC) provides an analytical solution for the predictive distribution as the Student's t-distribution, which can mitigate the over-training effects by marginalizing the model parameters of an output distribution. We address the sparse data problem in speech recognition, and show experimentally that VB-BPC is robust against data sparseness.	speech recognition	Shinji Watanabe;Atsushi Nakamura	2006	IEICE Transactions	10.1093/ietisy/e89-d.3.970	speech recognition;bayesian experimental design;computer science;machine learning;student's t-distribution;pattern recognition;bayesian linear regression;bayesian hierarchical modeling;bayesian statistics;statistics	Vision	31.893982617763225	-24.053643865802318	30306
d1cddfb23f46d61ec83c33aa9656765b960c70fd	active recognition and pose estimation of household objects in clutter	robot sensing systems;object recognition;feature extraction three dimensional displays robot sensing systems estimation object recognition cameras;estimation;three dimensional displays;feature extraction;next best view planning process active object recognition pose estimation system household objects clutter sparse feature model dense point cloud model feature matching rgb d sensor turtlebot;robot vision image matching image sensors pose estimation;cameras;conference proceeding	This paper presents an active object recognition and pose estimation system for household objects in a highly cluttered environment. A sparse feature model, augmented with the characteristics of features when observed from different viewpoints is used for recognition and pose estimation while a dense point cloud model is used for storing geometry. This strategy makes it possible to accurately predict the expected information available during the Next-Best-View planning process as both the visibility as well as the likelihood of feature matching can be considered simultaneously. Experimental evaluations of the active object recognition and pose estimation with an RGB-D sensor mounted on a Turtlebot are presented.	3d pose estimation;active object;clutter;feature model;observable;outline of object recognition;pipeline (computing);point cloud;robot;sparse matrix;turtle (robot)	Kanzhi Wu;Ravindra Ranasinghe;Gamini Dissanayake	2015	2015 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2015.7139782	computer vision;estimation;pose;3d pose estimation;feature extraction;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;articulated body pose estimation;3d single-object recognition;statistics	Robotics	50.95201678058198	-41.30446228285198	30345
b25ec064018cc8ed89f7bf2e10b3e9fef49f0c31	data association using empty convex polygonal regions in ekf-slam	lasers;pattern clustering;kernel;adaptive thresholding;slam robots data structures geometry kalman filters mobile robots pattern clustering sensor fusion;spatial data;occupancy grid;kalman filters;geometry;semantics;mobile robots;data representation;data association;adaptive threshold data association empty convex polygonal regions ekf slam data representation semantic reasoning occupancy grid data clustering;data clustering;data structures;feature extraction;simultaneous localization and mapping;clustering algorithms;sensor fusion;spatial configuration;slam robots;convex polygon;feature extraction simultaneous localization and mapping lasers clustering algorithms semantics kernel	This paper proposes a new framework for data association to solve the problem of SLAM. The proposed framework has specific relevance to range scanner based EKF-SLAM. The resulting data representation enables semantic reasoning on a spatial level which reduces the misassociation of closely spaced data from different spatial configurations through the use of convex polygons to represent data from similar spatial configurations. The data representation is especially effective for association when revisiting previously mapped regions efficiently. The spatial data representation also builds an occupancy grid for the entire map. We also provide a means of clustering range scan data using an adaptive threshold to be able to divide data at various ranges into clusters and dense data clustering to get more accurate data.	algorithm;closing (morphology);cluster analysis;correspondence problem;data (computing);extended kalman filter;iteration;on the fly;relevance;simultaneous localization and mapping;vertex (geometry)	Gururaj Kosuru;Satish Pedduri;K. Madhava Krishna	2010	2010 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2010.5723430	computer vision;computer science;machine learning;pattern recognition;mathematics;semantics;cluster analysis	Robotics	49.90801111593885	-42.689984837229574	30421
2af500d2de35764208c7b647e64411789a12b173	graph-based automatic consistent image mosaicking	graph theory;image segmentation;map construction;robot navigation;baseline approach view;mobile robots;global optimization image mosaicking robot navigation;local optimization;navigation;navigation cartography graph theory image segmentation mobile robots;global optimization graph based automatic consistent image mosaicking robot navigation map construction local optimization baseline approach view;image mosaicking;cartography;robotics and automation navigation computer science robotic assembly image sequences robot sensing systems image generation optimization methods photography computer vision;graph based automatic consistent image mosaicking;global optimization;article	Consistent image mosaicking is a potentially useful tool for robot navigation and map construction. This paper presents an automatic algorithm to generate consistent image mosaicking. During the robot processing, local optimization based on the related previous images is used for every newly added image on the baseline approach view. As soon as a loop is detected, a global optimization method is activated to generate a globally consistent image mosaicking. This method is very efficient in computation and storage saving	algorithm;baseline (configuration management);computation;global optimization;image stitching;mathematical optimization;robotic mapping	Pifu Zhang;Evangelos E. Milios;Jason Jianjun Gu	2004	2004 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2004.1521840	mobile robot;computer vision;navigation;simulation;computer science;local search;graph theory;machine learning;image segmentation;global optimization	Robotics	53.722170944625596	-42.924124042370465	30479
a0a70c43d02f3ebfba6b68f766361df1908066eb	texture simulation and implementation based on matlab and simulink	simulink;engineering;software;digital signal processing;computer graphics hardware clouds noise generators field programmable gate arrays working environment noise materials science and technology fires computer languages sun;generators;mathematics computing;optics;system modeling tool;system modeling;computer graphics;perlin noise;simulation;texture mapping;xilinx company;texture simulation;graphical textures;image texture;procedural texture;fixed point;system generator;mathematical model;simulation computer graphics field programmable gate arrays image texture mathematics computing noise;texture implementation;floating point;computer science;field programmable gate arrays;graphical textures texture simulation texture implementation matlab simulink texture mapping perlin noise system modeling tool system generator xilinx company fpga hardware;engineering electrical electronic;computer science theory methods;matlab;fpga hardware;noise;hardware	In this paper, procedural texture mapping based on Perlin noise is firstly implemented and simulated in Matlab. And then the design is converted from float-point to fix-point in Simulink. Using the system modeling tool, System Generator from Xilinx company, the noise function can be directly mapped into FPGA hardware. From the experimental results, various graphical textures can be implemented in real time and with realistic effects in FPGA hardware.	algorithm;computer simulation;field-programmable gate array;integrated development environment;matlab;perlin noise;procedural texture;simulink;systems design;systems modeling;texture mapping	Fuming Sun;Jing Liang;Xiaoling Li;Qin Wang	2009	2009 Fifth International Conference on Image and Graphics	10.1109/ICIG.2009.107	image texture;texture mapping;perlin noise;computer vision;simulation;systems modeling;computer science;floating point;noise;theoretical computer science;digital signal processing;procedural texture;mathematical model;fixed point;computer graphics;field-programmable gate array;computer graphics (images)	Robotics	43.272980185857286	-33.205598424797095	30483
305d6fb3da7daa42fcfde1ff619a340c87673968	optimizing multiple kernel learning for the classification of uav data	image classification;multiple kernel learning mkl;unmanned aerial vehicles uavs;support vector machines svms;informal settlements	Unmanned Aerial Vehicles (UAVs) are capable of providing high-quality orthoimagery and 3D information in the form of point clouds at a relatively low cost. Their increasing popularity stresses the necessity of understanding which algorithms are especially suited for processing the data obtained from UAVs. The features that are extracted from the point cloud and imagery have different statistical characteristics and can be considered as heterogeneous, which motivates the use of Multiple Kernel Learning (MKL) for classification problems. In this paper, we illustrate the utility of applying MKL for the classification of heterogeneous features obtained from UAV data through a case study of an informal settlement in Kigali, Rwanda. Results indicate that MKL can achieve a classification accuracy of 90.6%, a 5.2% increase over a standard single-kernel Support Vector Machine (SVM). A comparison of seven MKL methods indicates that linearly-weighted kernel combinations based on simple heuristics are competitive with respect to computationally-complex, non-linear kernel combination methods. We further underline the importance of utilizing appropriate feature grouping strategies for MKL, which has not been directly addressed in the literature, and we propose a novel, automated feature grouping method that achieves a high classification accuracy for various MKL methods.	algorithm;asch conformity experiments;computational complexity theory;experiment;heuristic (computer science);iterative method;linear separability;math kernel library;multiple kernel learning;nonlinear system;norm (social);optimizing compiler;orthophoto;point cloud;random forest;support vector machine;usb;unmanned aerial vehicle;xslt/muenchian grouping	Caroline M. Gevaert;Claudio Persello;George Vosselman	2016	Remote Sensing	10.3390/rs8121025	contextual image classification;machine learning;pattern recognition;data mining	AI	28.869220318828688	-44.99756363071242	30613
6a7225b5389d7f8562e6dadbfbe93b68bf19360d	ransac-based darces: a new approach to fast automatic registration of partially overlapping range images	range data;3d imaging;time complexity;random processes image registration distance measurement computational complexity search problems;computer vision;distance measurement;image edge detection image segmentation computer graphics data mining feature extraction pattern analysis robustness image recognition world wide web;computational complexity;local features;range image;image registration;random processes;registration;search problems;random sample consensus ransac based darces fast automatic image registration partially overlapping range images 3d registration time complexity data aligned rigidity constrained exhaustive search;exhaustive search	ÐIn this paper, we propose a new method, the RANSAC-based DARCES method, which can solve the partially overlapping 3D registration problem without any initial estimation. For the noiseless case, the basic algorithm of our method can guarantee that the solution it finds is the true one, and its time complexity can be shown to be relatively low. An extra characteristic is that our method can be used even for the case that there are no local features in the 3D data sets. Index TermsÐComputer vision, range data, range image, registration, 3D imaging.	3d reconstruction;algorithm;image registration;random sample consensus;range imaging;time complexity	Chu-Song Chen;Yi-Ping Hung;Jen-Bo Cheng	1999	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.809117	time complexity;stereoscopy;stochastic process;computer vision;ransac;computer science;image registration;machine learning;pattern recognition;brute-force search;computational complexity theory	Vision	49.627069163885	-51.08842016438696	30714
0225867389c9d61696d35376d2d198865d39a020	gesture recognition using auto-regressive coefficients of higher-order local auto-correlation features	high dimensional feature vectors;hidden markov model based recognizer;image recognition;time varying;sign language recognition;autoregressive coefficients;image motion analysis;image segmentation;high dimensionality;hidden markov model;time window;auto regressive;time series;correlation methods;data mining;autoregressive model;higher order;discriminant analysis;feature vector;motion recognition;handicapped aids;a priori knowledge;hidden markov models;hidden markov model based recognizer gesture recognition autoregressive coefficients higher order local auto correlation features motion recognition time varying images time differential images autoregressive model high dimensional feature vectors discriminant analysis;autoregressive processes;image motion analysis gesture recognition autoregressive processes correlation methods hidden markov models feature extraction;feature extraction;robustness;humans;higher order local auto correlation features;time varying images;lighting;time differential images;gesture recognition;autocorrelation;autocorrelation image recognition data mining hidden markov models robustness humans image segmentation feature extraction lighting handicapped aids	We propose an efficient method for motion recognition from time-varying images. The method extracts higher-order local auto-correlation (HLAC) features from time differential images in a time series. An auto-regressive model is applied to each dimension of the HLAC features and AR coefficients are calculated to achieve efficient extraction of information over a time range within a (time) window. These features become high dimensional feature vectors, so we use discriminant analysis to obtain effective less-dimensional features. These features are learned by the hidden Markov model (HMM) based recognizer to cope with non-uniformity of the speed of motions. We applied this method to gesture recognition and obtained good results. Because HLAC features are location-invariant, our method is robust to position changes of the objects (humans) in the image and need no segmentation of the objects. In addition, since the method extracts features from time differential images, it is also robust to changes of background and illumination condition. The method does not necessitate any a priori knowledge about objects in images; therefore, it may be applied to various applications of motion recognition, such as lip-reading, sign language recognition, and so forth.	autocorrelation;circuit complexity;coefficient;computation;experiment;feature extraction;feature vector;finite-state machine;gesture recognition;hidden markov model;information extraction;linear discriminant analysis;markov chain;object detection;real-time computing;time series	Tatsuya Ishihara;Nobuyuki Otsu	2004	Sixth IEEE International Conference on Automatic Face and Gesture Recognition, 2004. Proceedings.	10.1109/AFGR.2004.1301596	computer vision;speech recognition;computer science;machine learning;pattern recognition;autoregressive model;hidden markov model;statistics	Vision	38.90727766359244	-48.10770531977098	30857
36e689b5c17589d1c844bfc48d967363cc8560c0	integration of vehicle and lane detection for forward collision warning system	vehicle detection;distance measurement;estimation;roads;vehicles;cameras;real time systems	The capability of automated vehicles with intelligent on-board systems aimed at improving traffic safety has been in constant advance. This paper proposes an integrated module of lane and vehicle detection for a forward collision warning system that can be embedded in an autonomous driving system operating in real time. Integration of lane and vehicle detection provides more precise information for driving environments by using geometric consideration of the roads, and achieves synergistic effects for rejecting false alarms by adjusting ground constraints. Also, once the vehicle is localized in ego-lane, it is able to estimate the distance between the front and ego-vehicle. The experimental results in this paper, conducted on a real road dataset, verify the applicability of the proposed work for advanced driver assistance systems.	autonomous car;embedded system;on-board data handling;synergy	Huieun Kim;Youngwan Lee;Taekang Woo;Hakil Kim	2016	2016 IEEE 6th International Conference on Consumer Electronics - Berlin (ICCE-Berlin)	10.1109/ICCE-Berlin.2016.7684703	embedded system;simulation;engineering;computer security	Robotics	50.4688853454752	-35.78461401071718	30865
11d80211f76d4bc6764001e9f7b658304d7b2840	multiple appearance models for face tracking in surveillance videos	video surveillance;video signal processing;real time;face tracking;video indexing;surveillance videos lighting face detection robustness face recognition histograms indexing particle filters skin;face recognition;particle filter;filtering theory;video surveillance face recognition filtering theory video signal processing;frequency 3 ghz multiple appearance models face tracking automated video surveillance systems face recognition video indexing background clutter ambient illumination variations face pose changes difference of gaussian filters	Face tracking is a key component for automated video surveillance systems. It supports and enhances tasks such as face recognition and video indexing. Face tracking in surveillance scenarios is a challenging problem due to ambient illumination variations, face pose changes, occlusions, and background clutter. We present an algorithm for tracking faces in surveillance video based on a particle filter mechanism using multiple appearance models for robust representation of the face. We propose color based appearance model complemented by an edge based appearance model using the Difference of Gaussian (DOG) filters. We demonstrate that combined appearance models are more robust in handling the face and scene variations than a single appearance model. For example, color template appearance model is better in handling pose variations but they deteriorate against illumination variations. Similarly, an edge based model is robust in handling illumination variations but they fail in handling substantial pose changes. Hence, a combined model is more robust in handling pose and illumination changes than either one of them by itself. We show how the algorithm performs on a real surveillance scenario where the face undergoes various pose and illumination changes. The algorithm runs in real-time at 20 fps on a standard 3.0 GHz desktop PC.	algorithm;closed-circuit television;clutter;color;difference of gaussians;facial motion capture;facial recognition system;illumination (image);particle filter;real-time clock;real-time computing;tracking system	Gurumurthy Swaminathan;Vijendran G. Venkoparao;Saad Bedros	2007	2007 IEEE Conference on Advanced Video and Signal Based Surveillance	10.1109/AVSS.2007.4425341	facial recognition system;computer vision;face detection;facial motion capture;speech recognition;particle filter;computer science;video tracking;three-dimensional face recognition;computer graphics (images)	Vision	41.46612826716729	-49.54568004029812	30879
41882b6ad93ea03daa2d9a74aeb5039f98a23f56	track score and target existence	clutter;probability;track score;quality measurement;false track discrimination;tracking filters;data association;its filter track score probability automatic target tracking false track discrimination quality measurement multihypothesis tracking mht integrated track splitting;its;integrated track splitting;estimation;multihypothesis tracking;automatic target tracking;target tracking radar tracking history state estimation trajectory clutter object detection radar detection laboratories probability;quality measures;target tracking;mht;tracking filters clutter probability target tracking;tracking;estimation tracking data association mht its;its filter	Automatic target tracking in clutter initiates and updates both true tracks and false tracks. True tracks follow targets, and false tracks do not. False track discrimination is the procedure which confirms (vast majority of) true tracks, and terminates (vast majority of) false tracks. False track discrimination requires a measure of track quality to distinguish between true tracks and false tracks in a statistical sense. This paper compares two powerful track quality measures; the track score, as used in multi hypothesis tracking (MHT) and the probability of target existence, as used in integrated track splitting (ITS) filter. Both theoretical and simulation comparisons are presented in a single target tracking situation	clutter;experiment;mhtml;markov chain;simulation	Darko Musicki	2006	2006 9th International Conference on Information Fusion	10.1109/ICIF.2006.301762	computer vision;simulation;geography;pattern recognition	Vision	44.76500285681012	-49.5004098677464	30910
2b0e1f563917592cc7b8f2015b93292ae1949cdd	dual low-rank pursuit: learning salient features for saliency detection	image segmentation;object detection feature extraction learning artificial intelligence;object segmentation;visualization;prediction methods;feature extraction learning artificial intelligence object detection;visual attention feature learning saliency detection sparsity and low rankness;feature extraction;期刊论文;feature extraction visualization dictionaries object segmentation image segmentation prediction methods videos;dictionaries;feature learning saliency detection sparsity and low rankness visual attention;dual low rank pursuit visual world human eye fixations dlrp method saliency aware feature transformation learning supervision information discriminative bases sparsity pursuit framework embedded high level information supervised learning process bottom up based saliency detection methods top down based saliency detection methods;top down based saliency detection methods dual low rank pursuit visual world human eye fixations dlrp method saliency aware feature transformation learning supervision information discriminative bases sparsity pursuit framework embedded high level information supervised learning process bottom up based saliency detection methods;videos	Saliency detection is an important procedure for machines to understand visual world as humans do. In this paper, we consider a specific saliency detection problem of predicting human eye fixations when they freely view natural images, and propose a novel dual low-rank pursuit (DLRP) method. DLRP learns saliency-aware feature transformations by utilizing available supervision information and constructs discriminative bases for effectively detecting human fixation points under the popular low-rank and sparsity-pursuit framework. Benefiting from the embedded high-level information in the supervised learning process, DLRP is able to predict fixations accurately without performing the expensive object segmentation as in the previous works. Comprehensive experiments clearly show the superiority of the proposed DLRP method over the established state-of-the-art methods. We also empirically demonstrate that DLRP provides stronger generalization performance across different data sets and inherits the advantages of both the bottom-up- and top-down-based saliency detection methods.	base;benchmark (computing);dual;embedded system;embedding;evaluation;experiment;feature learning;generalization (psychology);high- and low-level;sensor;sparse matrix;supervised learning;top-down and bottom-up design;benefit;biologic segmentation;cell transformation	Congyan Lang;Jiashi Feng;Songhe Feng;Jingdong Wang;Shuicheng Yan	2016	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2015.2513393	computer vision;visualization;feature extraction;computer science;kadir–brady saliency detector;machine learning;pattern recognition;image segmentation	ML	26.480506585423754	-46.18414310371622	30930
8f60d9a441aecae0ecf775dbf43d05422528d0d0	adaptive pixel neighborhood definition for the classification of hyperspectral images with support vector machines and composite kernel	kernel;composite kernel;image resolution;support vector machines;maximum likelihood;training;kernel function;hyperspectral sensors;statistical method;indexing terms;vector median value;adaptive pixel neighborhood definition;pixel wise classification;training set;aviris hyperspectral data;support vectors machines;geophysical signal processing;feature extraction;remote sensing;pixel hyperspectral imaging support vector machines support vector machine classification kernel information filtering information filters data mining statistical analysis robustness;area filter;pixel;area self complementary filter;spectral information;hyperspectral data;support vector machines feature extraction filtering theory geophysical signal processing image resolution;standard statistical method;standard statistical method adaptive pixel neighborhood definition hyperspectral image classification support vector machines composite kernel pixel wise classification training set spatial information spectral information area filter vector median value aviris hyperspectral data;support vector machine;area self complementary filter support vectors machines spatial information hyperspectral data kernel function;hyperspectral imaging;classification accuracy;hyperspectral image classification;hyperspectral image;filtering theory;spatial information	The pixel-wise classification of hyperspectral images with a reduced training set is addressed. The joint use of the spectral and the spatial information is investigated. The spectral information simply consists of the spectral value of each pixel. For the spatial information, we use an area filter to simplify the image and extract consistent connected components. These components are used to define an adaptive neighborhood for each pixel of the image. The vector median value of each component is defined as a spatial feature for the classification. support vector machines are used for the classification and a composite kernel is used to combine both the spatial and the spectral information. Experiments are conducted on AVIRIS hyperspectral data. The proposed approach provides significant improvements in terms of classification accuracy when compared with a standard statistical method (maximum likelihood) and with a SVM classifier using the spectral information alone. Robustness with respect to the size of the training set is also investigated.	connected component (graph theory);kernel (operating system);pixel;support vector machine;test set	Mathieu Fauvel;Jocelyn Chanussot;Jon Atli Benediktsson	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4712147	support vector machine;computer vision;computer science;hyperspectral imaging;machine learning;pattern recognition;mathematics	Vision	30.88837091985341	-44.05485311407332	30952
3258dd8a7a1ff46e40eac3492f7685b7268be4ef	dimension reduction for model-based clustering via mixtures of multivariate $$t$$ t -distributions	dimension reduction;thesis;62h30;mclust teigen model based clustering dimension reduction multivariate t mixtures;mixture models;model based clustering	Dimension Reduction for Model-Based Clustering via Mixtures of Multivariate t-Distributions Katherine Morris Advisor University of Guelph, 2012 Prof. Paul D. McNicholas We introduce a dimension reduction method for model-based clustering obtained from a finite mixture of t-distributions. This approach is based on existing work on reducing dimensionality in the case of finite Gaussian mixtures. The method relies on identifying a reduced subspace of the data by considering how much group means and group covariances vary. This subspace contains linear combinations of the original data, which are ordered by importance via the associated eigenvalues. Observations can be projected onto the subspace and the resulting set of variables captures most of the clustering structure available in the data. The approach is illustrated using simulated and real data.	binary prefix;cluster analysis;dimensionality reduction	Katherine Morris;Paul D. McNicholas;Luca Scrucca	2013	Adv. Data Analysis and Classification	10.1007/s11634-013-0137-3	correlation clustering;econometrics;fuzzy clustering;computer science;machine learning;mixture model;mathematics;cluster analysis;statistics;dimensionality reduction	ML	30.396147600426882	-28.92163703445839	30954
57eed1c18d6d003eb0027bc84a8970002568337c	a fast blink detector using canonical correlation analysis	histograms detectors face correlation feature extraction classification algorithms accuracy;eye;classification algorithm;video streaming;correlation methods;temporal information;smile detection blink detector canonical correlation analysis blinking involuntary action image capture temporal information continuous video stream classification algorithm blink detection;statistical analysis;canonical correlation analysis;statistical analysis correlation methods eye object detection;object detection	Blinking is an involuntary action performed by people to keep their eyes moist but this becomes a problem when capturing an image. This problem becomes more pronounced when a flash is being used for image capture. Some of the algorithms that are present in the literature today are quite complex. Also, the detection accuracy of the present algorithms in literature is heavily dependent on temporal information. Temporal information is derived from the continuous video stream, but in the case where a flash is used for capture of an Image, temporal information may not be available. In this paper we propose a simple classification algorithm that is based on CCA (Canonical Correlation Analysis) to perform fast Blink detection for computationally constrained devices.	algorithm;streaming media	Mithun Uliyar;Soumik Ukil	2012	2012 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2012.6161725	computer vision;canonical correlation;speech recognition;computer science;pattern recognition;statistics	Vision	37.8075404296893	-50.364828644779095	30969
5dc71eaadc3a18aef56820837cc6fbe02c44b5bc	multi-level fusion of graph based discriminant analysis for hyperspectral image classification		Based on the graph-embedding framework, sparse graph-based discriminant analysis (SGDA), collaborative graph-based discriminant analysis (CGDA) and low rankness graph based discriminant analysis (LGDA) have been proposed with different graph construction. However, due to the inherent characteristics of ℓ 1-norm, ℓ 2-norm and nuclear-norm, single graph may be not optimal in capturing global and local structure of the data. In this paper, a multi-level fusion strategy is proposed in combining the three graph construction methods: 1) multiple graphs-based discriminant analysis (MGDA) in feature level with adaptive weights; 2) decision level fusion with D-S theory (GDA-DS), followed by a typical support vector machine (SVM) classification. Experimental results on three hyperspectral images datasets demonstrate that results with the fused strategy prevails with better classification performance.	computer vision;gnome-db;graph embedding;linear discriminant analysis;sparse matrix;support vector machine	Fubiao Feng;Qiong Ran;Wei Li	2016	Multimedia Tools and Applications	10.1007/s11042-016-4183-7	machine learning;pattern recognition;data mining;optimal discriminant analysis;multiple discriminant analysis	AI	25.205385602122735	-42.590986009462696	30980
e2b63f9e8794cbe45f8a9f6a33b6ab84d7974d4b	enhanced discriminant linear regression classification for face recognition	databases;training;linear regression;probes;image reconstruction face face recognition training probes databases linear regression;face recognition;image reconstruction;face	Linear Discriminant regression classification (L-DRC) embeds the fisher criterion into the linear regression classification (LRC) and can achieve more robust classification performance for face recognition. In this paper, we propose an enhanced discriminant linear regression classification (EDLRC) algorithm to further improve the discriminant power of LDRC. When calculating the between-class reconstruction error (BCRE), only those classes that are more easily to be misclassified into are considered. After maximizing the ratio of BCRE and within-class reconstruction error (WCRE), the obtained projection matrix in EDLRC is more effective than the projection matrix in LDRC, which is verified by extensive experiments.	algorithm;discriminant;experiment;facial recognition system;return loss;statistical classification	Xiaochao Qu;Hyoung Joong Kim	2014	2014 IEEE Ninth International Conference on Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP)	10.1109/ISSNIP.2014.6827696	speech recognition;machine learning;pattern recognition;mathematics;linear discriminant analysis	Vision	25.817613247586273	-42.31695918735185	30982
e60a7fc13b41c76f45107e03ceb850e413c7b4b0	train here, deploy there: robust segmentation in unseen domains		Semantic Segmentation methods play a key role in today’s Autonomous Driving research, since they provide a global understanding of the traffic scene for upper-level tasks like navigation. However, main research efforts are being put on enlarging deep architectures to achieve marginal accuracy boosts in existing datasets, forgetting that these algorithms must be deployed in a real vehicle with images that were not seen during training. On the other hand, achieving robustness in any domain is not an easy task, since deep networks are prone to overfitting even with thousands of training images. In this paper, we study in a systematic way what is the gap between the concepts of “accuracy” and “robustness”. A comprehensive set of experiments demonstrates the relevance of using data augmentation to yield models that can produce robust semantic segmentation outputs in any domain. Our results suggest that the existing domain gap can be significantly reduced when appropriate augmentation techniques regarding geometry (position and shape) and texture (color and illumination) are applied. In addition, the proposed training process results in better calibrated models, which is of special relevance to assess the robustness of current systems.	algorithm;color;convolutional neural network;experiment;illumination (image);marginal model;numerical analysis;overfitting;relevance;robustness (computer science);texture mapping	Eduardo Romera;Luis Miguel Bergasa;Jose M. Alvarez;Mohan Trivedi	2018	2018 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2018.8500561	robustness (computer science);overfitting;task analysis;semantics;image segmentation;machine learning;forgetting;data modeling;segmentation;artificial intelligence;computer science	Vision	28.464914257255476	-50.34606336131625	31074
7e5a3fbe86f4c24c71527ddcf623100a2292ae69	robust incremental subspace learning for object tracking	subspace learning;object tracking	In this paper, we introduce a novel incremental subspace based object tracking algorithm. The two major contributions of our work are the Robust PCA based occlusion handling scheme and revised incremental PCA algorithm. The occlusion handling scheme fully makes use of the merits of Robust PCA and achieves promising results in occlusion, clutter, noisy and other complex situations for the object tracking task. Besides, the introduction of incremental PCA facilitates the subspace updating process and possesses several benefits compared with traditional R-SVD based updating methods. The experiments show that our proposed algorithm is efficient and effective to cope with common object tracking tasks, especially with strong robustness due to the introduction of Robust PCA.	algorithm;clutter;experiment;pattern recognition;robust random early detection;singular value decomposition	Gang Yu;Zhiwei Hu;Hongtao Lu	2009		10.1007/978-3-642-10677-4_93	computer vision;computer science;machine learning;video tracking;pattern recognition	Vision	33.63067610550907	-47.15583866326983	31081
e5cf0321ca447dfc52bc0ee639389896173e76c4	stochastic ica contrast maximisation using oja's nonlinear pca algorithm		Independent Component Analysis (ICA) is an important extension of linear Principal Component Analysis (PCA). PCA performs a data transformation to provide independence to second order, that is, decorrelation. ICA transforms data to provide approximate independence up to and beyond second order yielding transformed data with fully factorable probability densities. The linear ICA transformation has been applied to the classical statistical signal-processing problem of Blind Separation of Sources (BSS), that is, separating unknown original source signals from a mixture whose mode of mixing is undetermined. In this paper it is shown that Oja's Nonlinear PCA algorithm performs a general stochastic online adaptive ICA. This analysis is corroborated with three simulations. The first separates unknown mixtures of original natural images, which have sub-Gaussian densities, the second separates linear mixtures of natural speech whose densities are super-Gaussian. Finally unknown mixtures of original images, which have both sub- and super-Gaussian densities are separated.	approximation algorithm;blind signal separation;decorrelation;erkki oja;independent computing architecture;independent component analysis;natural language;nonlinear system;normal statistical distribution;principal component analysis;simulation;statistical signal processing;visually impaired persons;density;mixture	Mark A. Girolami;Colin Fyfe	1997	International journal of neural systems	10.1142/S0129065797000586	machine learning;pattern recognition;mathematics;statistics	ML	30.89147629752124	-32.723732340150825	31099
c4c66578a8ec100861bee5a16b1c9c7d28be95e6	identification of a specific person using color, height, and gait features for a person following robot	gait feature;multi feature person identification;mobile service robots	This paper describes a person identification method for mobile service robots using image and range data. Person identification is a necessary function in order for mobile service robots to locate the target person for those services. Among various sensory features, image-based appearance features have often been used for person identification. They are, however, not effective in severe illumination environments such as a strong backlight. Therefore, we use two illumination-independent features, height and gait, in addition to appearance features for a more robust identification. To this end, we have developed a new method of extracting the gait feature (step length and speed), based on amaximum likelihood estimation of supporting leg positions in accumulated range data. We combine these features and use an online boosting approach to create the specific person classifier. It allows the robot to identify the specific person robustly even in a severe illumination environment. We tested our multi-feature person identification method, combined with a range data-based person tracker, in a specific person following scenario to demonstrate the effectiveness of this method. © 2016 Elsevier B.V. All rights reserved.	adaptive histogram equalization;backlight;color;gradient;mean shift;randomized algorithm;robot	Kenji Koide;Jun Miura	2016	Robotics and Autonomous Systems	10.1016/j.robot.2016.07.004	computer vision;simulation	AI	45.957449152376135	-43.777605125512395	31119
fcca9dd000b23bc0641153bec3bf958184e54ade	on using high-definition body worn cameras for face recognition from a distance	video surveillance;hd video;surveillance;face database;wavelet transforms;qa75 electronic computers computer science;fisherfaces;wavelet transform;face recognition;eigenfaces;law enforcement;standard definition;close range;high definition;qa76 computer software	Recognition of human faces from a distance is highly desirable for law-enforcement. This paper evaluates the use of low-cost, high-definition (HD) body worn video cameras for face recognition from a distance. A comparison of HD vs. Standard-definition (SD) video for face recognition from a distance is presented. HD and SD videos of 20 subjects were acquired in different conditions and at varying distances. The evaluation uses three benchmark algorithms: Eigenfaces, Fisherfaces and Wavelet Transforms. The study indicates when gallery and probe images consist of faces captured from a distance, HD video result in better recognition accuracy, compared to SD video. This scenario resembles real-life conditions of video surveillance and law-enforcement activities. However, at a close range, face data obtained from SD video result in similar, if not better recognition accuracy than using HD face data of the same range.		Wasseem Al-Obaydy;Harin Sellahewa	2011		10.1007/978-3-642-19530-3_18	computer vision;speech recognition;computer science;video tracking;three-dimensional face recognition;computer graphics (images)	Vision	37.9715915276094	-50.62902113211391	31125
320589d1b2de9ae046d1b88a534db231e9592855	design of smart video surveillance system for indoor and outdoor scenes		Smart video surveillance of indoor and outdoor scenes is a challenging task for modern surveillance systems. Different imaging conditions like bad illumination, adverse weather, etc., makes the surveillance process difficult. Recently, researchers have proposed smart surveillance systems with additional features for more accurate monitoring of events, but not much attention is paid to improve the system such that the monitoring process consumes as minimum resources as possible. In this paper, we propose a novel surveillance system that enhances visibility in adverse weather conditions and summarizes the captured videos automatically to reduce storage space. As the summarization process is based on the events in a scene, video interpretation becomes fast and easy. We propose perceptual features that can be used for more meaningful and robust summarization of the video than the existing summarization algorithms. We test the system for both indoor and outdoor scenes and show that the system works well even with multiple moving objects and complex motions.	algorithm;closed-circuit television;human visual system model;smart tv	Himanshu Kumar;Saumik Bhattacharya;Sinnu Susan Thomas;Sumana Gupta;K. S. Venkatesh	2017	2017 22nd International Conference on Digital Signal Processing (DSP)	10.1109/ICDSP.2017.8096120	visibility;computer vision;simulation;automatic summarization;artificial intelligence;computer science	Robotics	41.03327822847645	-44.71421408468834	31155
22d7d28c5c9fa027dae05bd22f7df3a5fd27a399	distributed robust subspace recovery		We study Robust Subspace Recovery (RSR) in distributed settings. We consider a huge dataset in an ad hoc network without a central processor, where each node has access only to one chunk of the dataset. We assume that part of the whole dataset lies around a low-dimensional subspace and the other part is composed of outliers that lie away from that subspace. The goal is to recover the underlying subspace for the whole dataset, without transferring the data itself between the nodes. We apply the Consensus Based Gradient method for the Geometric Median Subspace algorithm for RSR. We propose an iterative solution for the local dual minimization problem and establish its r-linear convergence. We also explain how to distributedly implement the Reaper and Fast Median Subspace algorithms for RSR. We demonstrate the competitive performance of our algorithms for both synthetic and real data.	algorithm;central processing unit;fast fourier transform;geometric median;gradient method;hoc (programming language);iterative method;rate of convergence;synthetic intelligence	Vahan Huroyan;Gilad Lerman	2017	CoRR	10.1137/17M1131659	distributed algorithm;mathematics;geometric median;mathematical optimization;outlier;gradient method;subspace topology;convergence (routing);wireless ad hoc network	ML	26.584843962863427	-36.5662404875555	31266
40b65bf60593d84addde606288209ad0d45fb05f	statistical inference in mapping and localization for mobile robots	conjunto independiente;modelizacion;navegacion;robot movil;modelo markov oculto;representation graphique;analisis estadistico;odometer;variable independante;modele markov cache;variable aleatoire;independent set;data gathering;mobile robot;hidden markov model;laser;localization;occupancy grid;variable aleatoria;robot navigation;intelligence artificielle;localizacion;robotics;aprendizaje probabilidades;probabilistic approach;odometre;grid;modelisation;captador medida;navigation;measurement sensor;ensemble independant;capteur mesure;localisation;posterior distribution;statistical analysis;robot mobile;rejilla;enfoque probabilista;approche probabiliste;graphical representation;indoor environment;probability distribution;analyse statistique;echantillonnage importance;random variable;ley a posteriori;statistical inference;robotica;grille;grafo curva;ensemble aleatoire;apprentissage probabilites;artificial intelligence;variable independiente;robotique;inteligencia artificial;importance sampling;indoor installation;instalacion interior;modeling;loi a posteriori;installation interieure;random set;graphics;moving robot;probability learning;independent variable;odometro;conjunto aleatorio	In this paper we tackle the problem of providing a mobile robot with the ability to build a map of its environment using data gathered during navigation. The data correspond to the locations visited by the robot, obtained through a noisy odometer, and the distances to obstacles from each location, obtained from a noisy laser sensor. The map is represented as an occupancy grid. In this paper, we represent the process using a Graphical Representation based on a statistical structure resembling a Hidden Markov model. We determine the probability distributions involved in this Graphical Representation using a Motion Model, a Perception model, and a set of independent Bernoulli random variables associated with the cells in the occupancy grid forming the map. Our formulation of the problem leads naturally to the estimation of the posterior distribution over the space of possible maps given the data. We exploit a particular factorization of this distribution that allows us to implement an Importance Sampling algorithm. We show the results obtained by this algorithm when applied to a data set obtained by a robot navigating inside an office building type of indoor environment.	algorithm;bernoulli polynomials;gibbs sampling;graphical user interface;ground truth;hidden markov model;importance sampling;internationalization and localization;map;markov chain;mobile robot;odometry;probabilistic automaton;robotics;simultaneous localization and mapping	Anita Araneda;Alvaro Soto	2004		10.1007/978-3-540-30498-2_54	variables;probability distribution;mobile robot;random variable;navigation;statistical inference;simulation;systems modeling;independent set;internationalization and localization;laser;importance sampling;computer science;graphics;artificial intelligence;machine learning;occupancy grid mapping;mathematics;posterior probability;robotics;grid;odometer;hidden markov model;statistics;data collection	Robotics	43.754583696309496	-28.924352169165896	31268
b0689c46695e6a0066d90e96543161470e4cc22f	image denoising method based on a deep convolution neural network		Image denoising is still a challenging problem in image processing. The authors propose a novel image denoising method based on a deep convolution neural network (DCNN). Different from other learning-based methods, the authors design a DCNN to achieve the noise image. Thus, the latent clear image can be achieved by separating the noise image from the contaminated image. At the training stage, the gradient clipping scheme is employed to prevent gradient explosions and enables the network to converge quickly. Experimental results demonstrate that the proposed denoising method can achieve a better performance compared with the state-of-the-art denoising methods. Also, the results indicate that the denoising method has the ability of suppressing different noises with different noise levels by means of one single denoising model.	artificial neural network;convolution;noise reduction	Fu Zhang;Nian Cai;Jixiu Wu;Guandong Cen;Han Wang;Xindu Chen	2018	IET Image Processing	10.1049/iet-ipr.2017.0389	image processing;kernel (image processing);artificial intelligence;computer vision;convolutional neural network;mathematics;pattern recognition;noise reduction	ML	24.99145938652429	-51.41430316189442	31379
21377314a6d9bd1e441fce30312cd1bcba23096c	human pose estimation method based on single depth image		Many of current human pose estimation methods based on depth images require training stage. However, the training stage costs huge work on making samples. And many methods for human pose occlusion condition cannot work well. In this study, a novel approach to estimate human pose with a depth image called model-based recursive matching (MRM) is introduced. A human skeleton model with customised parameters is created based on T-pose to fit different body types. The authors use depth image and 3D point cloud corresponding to input. In contrast to previous work, the proposed method avoids training step and can give an accurate estimation in the case of the human occlusion condition. They demonstrate the method by comparing to the method Kinect offered by using random forest on 20 human poses. And the ground truth of coordinates of pose joint is made by the motion capture system. The result shows that the proposed method not only works well on the general human pose but also can deal with human occlusion better. And the authorsu0027 method can be also applied to the disabled people and other creatures.	3d pose estimation	Qingqiang Wu;Guanghua Xu;Min Li;Longting Chen;Xin Zhang;Jun Xie	2018	IET Computer Vision	10.1049/iet-cvi.2017.0536	creatures;computer vision;point cloud;artificial intelligence;recursion;motion capture;mathematics;pose;pattern recognition;ground truth;random forest	Vision	50.44597292360594	-45.2193661886756	31380
ea3c28625487d191b15f1f5202c31348fb411454	an auto-focus sharpness function for stereo image pairs	focusing;histograms;3d imaging;stereo camera systems;computation time autofocus sharpness function stereo image pairs user intervention passive autofocusing focal positions stereo camera systems 3d applications focal positions left images right images sharpness accuracy;joints;accuracy;auto focus;computational complexity;cameras joints focusing histograms conferences real time systems accuracy;stereo image processing;stereo image processing computational complexity;sharpness function for stereo image pairs;3d image capture;cameras;conferences;3d image capture stereo camera systems auto focus sharpness function for stereo image pairs;real time systems	Auto-focusing is a key feature of modern camera systems that ensures a focused image is captured without the need for any user intervention. Passive auto-focusing is achieved based on a measure of sharpness or a sharpness function that is extracted from a series of images at different focal positions. With the rapid growth of stereo camera systems for 3D applications, this paper introduces a sharpness function when a series of stereo image pairs are captured at different focal positions. This function is defined based on the joint information between the left and right images. A comparison is carried out between the introduced sharpness function and the conventional sharpness functions when considering the left and right images separately in terms of sharpness accuracy and computation time.	computation;focal (programming language);stereo camera;time complexity	Mohammad T. Rahman;Nasser Kehtarnavaz;Siamak Yousefi	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6116631	stereoscopy;computer vision;autofocus;computer science;histogram;mathematics;accuracy and precision;computational complexity theory;statistics;computer graphics (images)	Vision	50.80460766349633	-48.64722761662917	31392
9dd63ec3e9a84c48cd71beebe18a30bb74ee35ad	visual navigation with street view image matching		The vision based navigation approach is a key to success for the driving assistance technology. In this work, we presents a visual navigation assistance system based on the geographic information of the vehicle and image matching between the online and pre-established data. With the rough GPS coordinates, we utilize the image retrieval algorithms to find the most similar image in the panoramic image database. The searching results are then compared with the input image for feature matching to find the landmarks in the panoramic image. By using the 360◦ field-of-view of the panoramic images, the camera’s heading can be calculated by the matching results. Finally, the landmark information is identified by the markers on the Google map as visual guidance and assistance.	algorithm;course (navigation);geographic coordinate system;google street view;image registration;image retrieval;machine vision;pattern recognition;real-time clock;wearable technology	Chih-Hung Hsu;Huei-Yung Lin	2016		10.5220/0005674405830589	computer vision	Vision	50.92202173485665	-43.658495600492955	31481
b184650985d07ffab26efc130fd29ab4425e2b95	low-rank regularized multi-view inverse-covariance estimation for visual sentiment distribution prediction				Anan Liu;Yingdi Shi;Peiguang Jing;Jing Liu;Yuting Su	2018	J. Visual Communication and Image Representation	10.1016/j.jvcir.2018.11.006		Vision	25.588752585671386	-44.54309555011373	31548
e7bcc787ccd5fbfc42d493e7753b7d86a1b44c10	interpolation models with multiple hyperparameters	design model;spline;generalization error;bayesian inference;regression;convexity;regularizer;neuronal spike;bayesian model	A traditional interpolation model is characterized by the choice of reg-ularizer applied to the interpolant, and the choice of noise model. Typically , the regularizer has a single regularization constant , and the noise model has a single parameter. The ratio == alone is responsible for determining globally all these attributes of the interpolant: its`complexity', ``exibility', `smoothness', `characteristic scale length', and`characteristic amplitude'. We suggest that interpolation models should be able to capture more than just one avour of simplicity and complexity. We describe Bayesian models in which the interpolant has a smoothness that varies spatially. We emphasize the importance, in practical implementation, of the concept of`conditional convexity' when designing models with many hyperparameters. We apply the new models to the interpolation of neuronal spike data and demonstrate a substantial improvement in generalization error.	bayesian network;generalization error;hardware random number generator;interpolation	David J. C. MacKay;Ryo Takeuchi	1998	Statistics and Computing	10.1023/A:1008862908404	econometrics;mathematical optimization;machine learning;mathematics;bayesian inference;statistics	ML	28.80008606400752	-27.06648852283767	31609
e91e68fbde0f46242ce9a4eee9c33e442de60d39	a rotation test to verify latent structure	microarray data;projection pursuit;latent variable;statistical test;statistical significance;independent component analysis;multivariate regression	We consider here how to tell whether a latent variable that has been estimated in a multivariate regression context might be real. Often a followup investigation will find a real physical variable that corresponds closely to the latent variable and that will settle the issue. If no such variable is uncovered then a statistical test for whether the variable is real is desirable. It is well known that a Gaussian latent variable cannot be identified in the presence of Gaussian background noise. But a nonGaussian variable can be distinguished from such noise. Instead of testing based on the magnitude of the latent variable, we test based on a measure of its non-Gaussianity, via a projection pursuit criterion. To judge the statistical significance of the observed criterion, we introduce a test based on uniform random rotations of the data, taken in a space orthogonal to the measured regression variables. For the microarray data of the AGEMAP consortium, we confirm the presence of some latent variables in the expression values for 8 of the 16 tissue types considered.	general linear model;latent variable;microarray	Patrick O. Perry;Art B. Owen	2010	Journal of Machine Learning Research	10.1145/1756006.1756024	latent class model;latent variable;projection pursuit;independent component analysis;structural equation modeling;microarray analysis techniques;econometrics;multivariate statistics;statistical hypothesis testing;computer science;machine learning;pattern recognition;mathematics;statistical significance;partial least squares regression;probabilistic latent semantic analysis;local independence;latent variable model;regression analysis;statistics	ML	28.614023495779055	-24.88924047061932	31625
7db799612b1ed3f9cacdf930170653d1fbd1648c	visual tracking and segmentation using appearance and spatial information of patches	k nearest neighbors;nearest neighbor searches;histograms;image segmentation;trees mathematics image segmentation robot vision target tracking;k d tree;mean shift;robotics;trees mathematics;robot vision;image edge detection;computational complexity;visual segmentation;pixel;object tracking;classification algorithms;patches;k nearest neighbor;k d trees;k d trees visual tracking visual segmentation spatial information patches object tracking robotics k nearest neighbors;target tracking;visual tracking;spatial information;image segmentation target tracking robotics and automation yagi uda antennas computational complexity classification tree analysis histograms usa councils nearest neighbor searches video sequences	Object tracking and segmentation find a wide range of applications in robotics. Tracking and segmentation are difficult in cluttered and dynamic backgrounds. We propose a tracking and segmentation algorithm in which tracking and segmentation are performed consecutively. We separate input images into disjoint patches using an efficient oversegmentation algorithm. Objects and their background are described by bags of patches. We classify the patches in a new frame by searching k nearest neighbors. K-d trees are constructed using these patches to reduce computational complexity. Target location is estimated coarsely by running the mean-shift algorithm. Based on the estimated locations, we classify the patches again using appearance and spatial information. This strategy out-performs direct segmentation of patches based on appearance information only. Experimental results show that the proposed algorithm provides good performance on difficult sequences with clutter.	bottom-up parsing;clutter;computational complexity theory;k-nearest neighbors algorithm;mean shift;robotics	Junqiu Wang;Yasushi Yagi	2010	2010 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2010.5509303	computer vision;computer science;machine learning;pattern recognition;robotics;scale-space segmentation;k-nearest neighbors algorithm	Robotics	43.78954570234423	-50.088513867992404	31631
2abf783168d5f8a9f3ed85f5aa54a6a914bdf61c	empirical risk minimization for probabilistic grammars: sample complexity and hardness of learning	approximate algorithm;structural risk minimization;statistical model;empirical risk minimization;expectation maximization;computational linguistic;natural language;sample complexity;probabilistic grammar	Probabilistic grammars are generative statistical models that are useful for compositional and sequential structures. They are used ubiquitously in computational linguistics. We present a framework, reminiscent of structural risk minimization, for empirical risk minimization of probabilistic grammars using the log-loss. We derive sample complexity bounds in this framework that apply both to the supervised setting and the unsupervised setting. By making assumptions about the underlying distribution that are appropriate for natural language scenarios, we are able to derive distribution-dependent sample complexity bounds for probabilistic grammars. We also give simple algorithms for carrying out empirical risk minimization using this framework in both the supervised and unsupervised settings. In the unsupervised case, we show that the problem of minimizing empirical risk is NP-hard. We therefore suggest an approximate algorithm, similar to expectation-maximization, to minimize the empirical risk.	approximation algorithm;computation;computational linguistics;empirical risk minimization;expectation–maximization algorithm;generative model;np-hardness;natural language;probabilistic turing machine;probabilistic automaton;sample complexity;statistical model;structural risk minimization	Shay B. Cohen;Noah A. Smith	2012	Computational Linguistics	10.1162/COLI_a_00092	statistical model;probabilistic analysis of algorithms;empirical risk minimization;structural risk minimization;expectation–maximization algorithm;computer science;machine learning;pattern recognition;data mining;natural language	ML	25.531529927276456	-30.35650831276603	31671
c6b03af4e73eecd0a58ff7645f3373b8bd9a651c	group detection at camera handoff for collecting people appearance in multi-camera systems	moving object;video surveillance;cameras labeling video surveillance geometry layout object detection us department of transportation multimedia systems forensics information retrieval;information retrieval;geometry;layout;multimedia systems;epipolar geometry;map estimation;us department of transportation;cameras;labeling;forensics;object detection	Logging information on moving objects is crucial in video surveillance systems. Distributed multi-camera systems can provide the appearance of objects/people from different viewpoints and at different resolutions, allowing a more complete and precise logging of the information. This is achieved through consistent labeling to correlate collected information of the same person. This paper proposes a novel approach to consistent labeling also capable to fully characterize groups of people and to manage miss segmentations. The ground-plane homography and the epipolar geometry are automatically learned and exploited to warp objects' principal axes between overlapped cameras. A MAP estimator that exploits two contributions (forward and backward) is used to choose the most probable label configuration to be assigned at the handoff of a new object. Extensive experiments demonstrate the accuracy of the proposed method in detecting single and simultaneous handoffs, miss segmentations, and groups.	closed-circuit television;epipolar geometry;experiment;homography (computer vision);sensor	Simone Calderara;Rita Cucchiara;Andrea Prati	2006	2006 IEEE International Conference on Video and Signal Based Surveillance	10.1109/AVSS.2006.55	layout;computer vision;labeling theory;simulation;computer science;forensic science;epipolar geometry;computer graphics (images)	Vision	43.05271817714272	-47.347980646074184	31714
50397b2abd5d605356d4c08ba3e0ca8ea7f0b869	relabel mixture models via modal clustering	62g05;modal clustering;kernel density estimation;62h30;62f15;finite mixture models;label switching;em algorithm;bayesian analysis	ABSTRACTEffectively solving the label switching problem is critical for both Bayesian and Frequentist mixture model analyses. In this article, a new relabeling method is proposed by extending a recently developed modal clustering algorithm. First, the posterior distribution is estimated by a kernel density from permuted MCMC or bootstrap samples of parameters. Second, a modal EM algorithm is used to find the m! symmetric modes of the KDE. Finally, samples that ascend to the same mode are assigned the same label. Simulations and real data applications demonstrate that the new method provides more accurate estimates than many existing relabeling methods.	cluster analysis;mixture model;modal logic	Qiang Zheng;Weixin Yao	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2015.1089287	kernel density estimation;econometrics;expectation–maximization algorithm;bayesian probability;label switching;mathematics;statistics	ML	30.823274931496545	-25.640401853466084	31727
1507a04f374f8793c64c991320186ef44bb244a8	local visual homing by matched-filter descent in image distances	distance measure;image database;natural images;it value;optical flow;matched filter	In natural images, the distance measure between two images taken at different locations rises smoothly with increasing distance between the locations. This fact can be exploited for local visual homing where the task is to reach a goal location that is characterized by a snapshot image: descending in the image distance will lead the agent to the goal location. To compute an estimate of the spatial gradient in the distance measure, its value must be sampled at three noncollinear points. An animal or robot would have to insert exploratory movements into its home trajectory to collect these samples. Here we suggest a method based on the matched-filter concept that allows one to estimate the gradient without exploratory movements. Two matched filters – optical flow fields resulting from translatory movements in the horizontal plane – are used to predict two images in perpendicular directions from the current location. We investigate the relation to differential flow methods applied to the local homing problem and show that the matched-filter approach produces reliable homing behavior on image databases. Two alternative methods that only require a single matched filter are suggested. The matched-filter concept is also applied to derive a home-vector equation for a Fourier-based parameter method.	database;distance;gradient;matched filter;missile guidance;movement;optical flow;population parameter;robot;sampling - surgical action;smoothing;snapshot (computer storage);system of linear equations	Ralf Möller;Andrew Vardy	2006	Biological Cybernetics	10.1007/s00422-006-0095-3	computer vision;simulation;computer science;machine learning;optical flow;mathematics;matched filter	Vision	48.61464491960391	-37.80069657562637	31872
3f904cf3fcd563274ab237e7f944bfb16ae0468a	hd: efficient hand detection and tracking	skin;computational modeling;image color analysis;high definition video;lighting;cameras;tracking	Automated hand detection is useful for applications requiring reliable hand posture and hand gesture processing. Such applications include human-computer/human-robot interfaces for rehabilitation, serious games, or non-invasive medical diagnosis. Hence, in this paper, we focus on the design and development of a robust and fast hand detection and tracking (HD) system. The design of our HD system involved the study of the human skin colour and people's foreground properties, in order to merge these information for an efficient hand detection and tracking. Experiments have been carried out in real-world environment and have demonstrated the excellent performance of our HD system.	background subtraction;human–robot interaction;natural user interface;poor posture;sensor;thresholding (image processing)	Joanna Isabelle Olszewska;Cleveland Rouge;Sohil Shaikh	2016	2016 Federated Conference on Computer Science and Information Systems (FedCSIS)	10.15439/2016F445	computer vision;simulation;computer science;lighting;tracking;skin;computational model;computer graphics (images)	Robotics	46.36588037275219	-43.794168161637046	31874
00b4211786fceda9d4aaa781f56b08ffb24ef03d	on the uncertainty of information retrieval in associative memories	decoding vectors associative memory uncertainty hamming distance context;information retrieval;decoder uncertainty information retrieval associative memory system memory machines decision processes emotions natural association paradigm associative retrieval sequence reconstruction problem;content addressable storage;information retrieval content addressable storage	We (people) are memory machines. Our decision processes, emotions and interactions with the world around us are based on and driven by associations to our memories. This natural association paradigm will become critical in future memory systems, namely, the key question will not be “How do I store more information?” but rather, “Do I have the relevant information? How do I retrieve it?” The focus of this paper is to make a first step in this direction. We define and solve a very basic problem in associative retrieval. Given a word W, the words in the memory that are t-associated with W are the words in the ball of radius t around W. In general, given a set of words, say W, X and Y, the words that are t-associated with {W, X, Y} are those in the memory that are within distance t from all the three words. Our main goal is to study the maximum size of the t-associated set as a function of the number of input words and the minimum distance of the words in memory - we call this value the uncertainty of an associative memory. We derive the uncertainty of the associative memory that consists of all the binary vectors with an arbitrary number of input words. In addition, we study the retrieval problem, namely, how do we get the t-associated set given the inputs? We note that this paradigm is a generalization of the sequences reconstruction problem that was proposed by Leven-shtein (2001). In this model, a word is transmitted over multiple channels. A decoder receives all the channel outputs and decodes the transmitted word. Levenshtein computed the minimum number of channels that guarantee a successful decoder - this value happens to be the uncertainty of an associative memory with two input words.	algorithm;codec;content-addressable memory;hamming distance;ibm notes;information retrieval;interaction;lester the unlikely;programming paradigm;reconstruction conjecture;vii	Eitan Yaakobi;Jehoshua Bruck	2012	2012 IEEE International Symposium on Information Theory Proceedings	10.1109/ISIT.2012.6283016	computer science;theoretical computer science;machine learning;content-addressable memory;memory map	Theory	36.2025938114094	-27.471630640235432	31905
29e7910b5fca82f8ac4670fcc350798690b0031d	novel dominant plant detection algorithm for image sequence	estimation theory;image motion analysis;image sequences fitting image reconstruction three dimensional displays robustness cameras estimation;natural scenes approximation theory computational geometry estimation theory image motion analysis image reconstruction image sequences iterative methods;computational geometry;three dimensional;approximation theory;iterative methods;image reconstruction;local surface approximation geometric feature dominant plane detection progressive structure dominant plane fitting algorithms motion algorithm real scene reconstruction image sequences three dimensional points information least median square estimation ransac theory lmedsq scene surface reconstructive points outdoor scenarios dense 3d point cloud reconstruction algorithm image sequence;algorithms;natural scenes;image sequences	Dominant plane is an important geometric feature and can be used in a wide range of applications. In this paper, we develop a robust technology for dominant plane detection using the progressive structure from motion and dominant plane fitting algorithms. Firstly, we propose a progressive structure from motion algorithm to reconstruct the real scene from the image sequences, and obtain the information of three-dimensional points in the scene. In the following, based on the least median square (LMedSq) estimation and ransac theory, we present a novel plane fitting algorithm to find the dominant plane near the scene surface from the reconstructive points. Experimental results from different outdoor scenarios show that the proposed reconstruction algorithm obtains dense 3D point cloud, and achieves satisfactory recovery from image sequence. Then, the plane fitting algorithm accurately detects the dominant plane region from the reconstructive points. The detected plane is the approximation for local surface and meets the actual scene. These tests verify that the proposed method is effective, accurate and robust.	algorithm;approximation;autonomous robot;handheld game console;mobile robot;point cloud;random sample consensus;structure from motion	Baojie Fan;Yingkui Du	2013	2013 Seventh International Conference on Image and Graphics	10.1109/ICIG.2013.68	iterative reconstruction;three-dimensional space;computer vision;mathematical optimization;feature detection;computational geometry;mathematics;geometry;iterative method;image plane;estimation theory;approximation theory	Vision	52.727234926939865	-49.5355640909344	31974
51a455d65b1adac24db587fda29cf3052c22c36e	surface reconstruction from point cloud of human body by clustering	human body model;body surface;upright standing posture;point cloud;arbitrary posture;surface reconstruction;posture estimation.;estimated posture;point cloud data;body section;human body	This paper proposes a method in which the body surface is reconstructed from a point cloud (set of points) on the human body acquired by a device such as a laser range finder. The method constructs the surface appropriately for the human body by clustering the point cloud at body sections such as the chest and the upper arm. The clustering is performed as follows. A simple human model composed of cylinders, ellipsoidal columns, and hemispheres is assumed. The posture is estimated by matching the human body model to the point cloud data. Based on the estimated posture, the point cloud is assigned to body sections. Since the posture is inferred from the point cloud, the method is effective not only for an upright standing posture, but for point cloud data obtained from an arbitrary posture. Experiments were performed to apply the method to simulation data and measured data, and it was shown that a realistic surface could be reconstructed. The surface can be reconstructed for postures other than the upright standing posture. The effectiveness of the proposed method is thus verified. © 2006 Wiley Periodicals, Inc. Syst Comp Jpn, 37(11): 44–56, 2006; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/ scj.20230	cluster analysis;column (database);concave function;digi-comp i;experiment;html;hidden surface determination;john d. wiley;norm (social);point cloud;poor posture;simulation;synthetic data;system of measurement	Takuya Funatomi;Isao Moro;Shinobu Mizuta;Michihiko Minoh	2006	Systems and Computers in Japan	10.1002/scj.20230	computer vision;simulation;computer science;point cloud;computer graphics (images)	Robotics	49.8538476761967	-42.25504257558453	31981
9d64a53f4630f7c3d144b32297a4233b484e269b	development of robot self-identification based on visuomotor prediction	clutter;robot vision clutter feature extraction image motion analysis motion control regression analysis;image motion analysis;motion control;robot vision;feature extraction;regression analysis;motion selective visual feature robot self identification visuomotor prediction visual location cluttered visual image statistical predictor linear regression visual feature extraction proprioceptive input r 2 statistics robot body parts biologically plausible visual motion processing model orientation selective visual feature;visualization feature extraction robot kinematics computer architecture joints biological system modeling	We propose a developmental method that enables a robot to identify visual locations associated with its own body from a cluttered visual image based on the concept of visuomotor predictors. A set of statistical predictors are trained by linear regression to predict the visual features at each visual location from proprioceptive input. By measuring each predictor's predictability using the R2 statistics, the algorithm can determine which visual locations correspond to the robot's body parts. Visual features are extracted using biologically plausible visual motion processing models. We demonstrate that while both orientation selective and motion selective visual features can be used for self-identification, motion selective features are more robust to changes in appearance.	algorithm;kerrison predictor;robot;statistical model	Tao Zhou;Piotr Dudek;Bertram E. Shi	2012	2012 IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL)	10.1109/DevLrn.2012.6400878	computer vision;structure from motion;computer science;machine learning;pattern recognition;visual servoing	Robotics	46.095055444982385	-48.89035280873373	32017
09e22b5cb75809c2d0d50fdcbd9d1800705817f5	a probabilistic approach to concurrent mapping and localization for mobile robots	tecnologia industrial tecnologia mecanica;bayes rule;map building;mobile robot;localization;grupo de excelencia;mobile robots;probabilistic approach;maximum likelihood estimation;concurrent mapping and localization;navigation;large scale;positioning;maximum likelihood estimate;expectation maximization;indoor environment;mobile robot navigation;mapping;tecnologias;probabilistic reasoning	This paper addresses the problem of building large-scale geometric maps of indoor environments with mobile robots. It poses the map building problem as a constrained, probabilistic maximum-likelihood estimation problem. It then devises a practical algorithm for generating the most likely map from data, along with the most likely path taken by the robot. Experimental results in cyclic environments of size up to 80 by 25 meter illustrate the appropriateness of the approach.	algorithm;map;mobile robot;norm (social);simultaneous localization and mapping	Sebastian Thrun;Wolfram Burgard;Dieter Fox	1998	Machine Learning	10.1023/A:1007436523611	mobile robot;computer vision;simulation;computer science;machine learning;maximum likelihood;statistics	Robotics	52.55277080073787	-34.637972631902024	32021
177bc509dd0c7b8d388bb47403f28d6228c14b5c	deep learning face representation from predicting 10,000 classes	learning artificial intelligence face recognition feature extraction image classification image representation;lfw deep learning face representation feature representation deep hidden identity feature face verification deepid multiclass face identification training hidden layer neuron activation deep convolutional network face classifier face identities feature extraction hierarchy deep convnets compact identity related feature extraction complementary representation overcomplete representation face verification;face verification;deep learning;face feature extraction neurons training joints bayes methods biological neural networks;deep learning face verification	This paper proposes to learn a set of high-level feature representations through deep learning, referred to as Deep hidden IDentity features (DeepID), for face verification. We argue that DeepID can be effectively learned through challenging multi-class face identification tasks, whilst they can be generalized to other tasks (such as verification) and new identities unseen in the training set. Moreover, the generalization capability of DeepID increases as more face classes are to be predicted at training. DeepID features are taken from the last hidden layer neuron activations of deep convolutional networks (ConvNets). When learned as classifiers to recognize about 10, 000 face identities in the training set and configured to keep reducing the neuron numbers along the feature extraction hierarchy, these deep ConvNets gradually form compact identity-related features in the top layers with only a small number of hidden neurons. The proposed features are extracted from various face regions to form complementary and over-complete representations. Any state-of-the-art classifiers can be learned based on these high-level representations for face verification. 97:45% verification accuracy on LFW is achieved with only weakly aligned faces.	bayesian network;computer vision;convolutional neural network;deep learning;feature extraction;gaussian process;hidden variable theory;high- and low-level;human reliability;kernel method;multi-source;neuron;test set;while	Yi Sun;Xiaogang Wang;Xiaoou Tang	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.244	computer vision;speech recognition;computer science;machine learning;pattern recognition;deep learning	Vision	25.1978833820833	-50.06709728049509	32060
4de49115bd6ea876231837822afca6c653cf1033	analysis of skeletal shape trajectories for person re-identification		In this paper, we are interested in people re-identification using skeleton information provided by a consumer RGB-D sensor. We perform the modelling and the analysis of human motion by focusing on 3D human joints given by skeletons. In fact, the motion dynamic is modeled by projecting skeleton information on Grassmann manifold. Moreover, in order to define the identity of a test trajectory, we compare it against a labeled trajectory database while using an unsupervised similarity assessment procedure. Indeed, the main contribution of this work resides in the introduced distance that combines temporal information as well as global and local geometrical ones. Realized experiments on standard datasets prove that the proposed method performs accurately even though it does not assume any prior knowledge.	experiment;kinesiology;machine learning;manifold regularization;unsupervised learning	Amani Elaoud;Walid Barhoumi;Hassen Drira;Ezzeddine Zagrouba	2017		10.1007/978-3-319-70353-4_12	grassmannian;computer vision;artificial intelligence;skeleton (computer programming);pattern recognition;computer science;rgb color model;trajectory	Vision	36.11563091370538	-49.761854592208586	32102
17c4a65e466fb3958a386743a4afafdf8e7202ec	vision-based localization using a central catadioptric vision system	vision system;monte carlo localization;local algorithm;omnidirectional image;indoor environment	We present an appearance-based localization algorithm for an indoor environment that is inspired by human’s localization and navigation capabilities. Our localization approach integrates the Monte-Carlo localization technique with an omnidirectional image matching algorithm. The approach yields robust localization outcome with reasonable accuracy even when operating in a large map with sparse reference images.	algorithm;image registration;internationalization and localization;monte carlo localization;odometry;robot;sparse matrix	Mana Saedan;Chee Wang Lim;Marcelo H. Ang	2006		10.1007/978-3-540-77457-0_37	monte carlo localization;computer vision;simulation;machine vision;computer science;artificial intelligence	Robotics	52.85894123532194	-39.15084559894435	32114
8e545806a5721977c90d5ab12b6ca08b686a6330	on the asymptotic bias of the diffusion-based distributed pareto optimization	pareto optimization;distributed optimization;performance analysis;diffusion strategies;asymptotic bias	We revisit the asymptotic bias analysis of the distributed Pareto optimization algorithm developed based on the diffusion strategies. We propose an alternative way to analyze the asymptotic bias of this algorithm at small step-sizes and show that the asymptotic bias descends to zero with a linear dependence on the largest step-size parameter when this parameter is sufficiently small. In addition, through the proposed analytic approach, we provide an expression for the small-step-size asymptotic bias when a condition assumed jointly on the combination matrices and the step-sizes does not strictly hold. This is a likely scenario in practice, which has not been considered in the original paper that introduced the algorithm. Our methodology provides new insights into the inner workings of the diffusion Pareto optimization algorithm while being considerably less involved than the small-step-size asymptotic bias analysis presented in the original work. This is because we take advantage of the special eigenstructure of the composite combination matrix used in the algorithm without calling for any eigenspace decomposition or matrix inversion.	algorithm;asymptote;mathematical optimization;pareto efficiency	Reza Arablouei;Kutluyil Dogançay;Stefan Werner;Yih-Fang Huang	2017	Signal Processing	10.1016/j.sigpro.2016.05.023	econometrics;mathematical optimization;multi-objective optimization;mathematics;statistics	ML	28.364068265578343	-27.119694914006054	32124
2e7783c306bffb82081dd9af7c0593859937e411	lidar based off-road negative obstacle detection and analysis	detectors;velocity;all terrain vehicles;sensor lidar based off road negative obstacle detection autonomous unmanned ground vehicle ugv off road terrain topography off road environments point cloud 3d laser range finder geometry based method negative obstacle detector nodr support vector machine algorithm svm algorithm;support vector machines geometry laser ranging optical radar remotely operated vehicles;support vector machines;mounts;real time;geometry;hazards;terrain;self operation;laser radar;remotely operated vehicles;laser ranging;classification;topography;offroad traffic;avoidance;ground level;ground vehicles;barriers;symposia;navigation;optical radar;laser radar vehicles support vector machines robot sensing systems lasers three dimensional displays equations;algorithms;unmanned;planning;paths;detection and identification systems;autonomous land vehicles;vector analysis	In order for an autonomous unmanned ground vehicle (UGV) to drive in off-road terrain at high speeds, it must analyze and understand its surrounding terrain in realtime: it must know where it intends to go, where are the hazards, and many details of the topography of the terrain. Much research has been done in the way of obstacle avoidance, terrain classification, and path planning, but still so few UGV systems can accurately traverse off-road environments at high speeds autonomously. One of the most dangerous hazards found off-road are negative obstacles, mainly because they are so difficult to detect. We present algorithms that analyze the terrain using a point cloud produced by a 3D laser range finder, then attempt to classify the negative obstacles using both a geometry-based method we call the Negative Obstacle DetectoR (NODR) as well as a support vector machine (SVM) algorithm. The terrain is analyzed with respect to a large UGV with the sensor mounted up high as well as a small UGV with the sensor mounted low to the ground.	algorithm;angularjs;autonomous car;autonomous robot;benchmark (computing);experiment;motion planning;negative feedback;obstacle avoidance;point cloud;refresh rate;scan line;support vector machine;traverse;topography;unmanned aerial vehicle	Jacoby Larson;Mohan Manubhai Trivedi	2011	2011 14th International IEEE Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2011.6083105	computer vision;simulation;geography;remote sensing	Robotics	49.5477244331625	-36.01381934070512	32171
81cc843b3bfad58b8392511ff836eac891602bd3	minibatch gibbs sampling on large graphical models		Gibbs sampling is the de facto Markov chain Monte Carlo method used for inference and learning on large scale graphical models. For complicated factor graphs with lots of factors, the performance of Gibbs sampling can be limited by the computational cost of executing a single update step of the Markov chain. This cost is proportional to the degree of the graph, the number of factors adjacent to each variable. In this paper, we show how this cost can be reduced by using minibatching: subsampling the factors to form an estimate of their sum. We introduce several minibatched variants of Gibbs, show that they can be made unbiased, prove bounds on their convergence rates, and show that under some conditions they can result in asymptotic single-update-runtime speedups over plain Gibbs sampling.	algorithm;algorithmic efficiency;asymptotic computational complexity;big data;chroma subsampling;computation;computational complexity theory;factor graph;gibbs sampling;graph (discrete mathematics);graphical model;iteration;markov chain monte carlo;monte carlo method;rate of convergence;scalability	Christopher De Sa;Vincent Chin-Hung Chen;Wing Wong	2018			mathematical optimization;factor graph;mathematics;markov chain monte carlo;inference;markov chain;gibbs sampling;graph;graphical model;convergence (routing)	ML	26.70883502253528	-28.666240214296067	32455
530be28d4488de22beed27a25d2c4ef17e43eb4c	parameter sensitive detectors	detectors;parameter sensitive detectors;profile face detection;biological system modeling;vehicle detection;joints;image sensors;profile face detection parameter sensitive detectors object detection classifiers a priori partitioning;function space;a priori partitioning;face recognition;false positive rate;shape;human body;pedestrian detection;parameter space;detection rate;pattern classification;classifiers;humans;technical report;vehicles;parameter estimation;face detection;pattern classification face recognition image sensors object detection;object detection;face detection detectors humans object detection parameter estimation vehicle detection biological system modeling shape vehicles joints	Object detection can be challenging when the object class exhibits large variations. One commonly-used strategy is to first partition the space of possible object variations and then train separate classifiers for each portion. However, with continuous spaces the partitions tend to be arbitrary since there are no natural boundaries (for example, consider the continuous range of human body poses). In this paper, a new formulation is proposed, where the detectors themselves are associated with continuous parameters, and reside in a parameterized function space. There are two advantages of this strategy. First, a-priori partitioning of the parameter space is not needed; the detectors themselves are in a parameterized space. Second, the underlying parameters for object variations can be learned from training data in an unsupervised manner. In profile face detection experiments, at a fixed false alarm number of 90, our method attains a detection rate of 75% vs. 70% for the method of Viola-Jones. In hand shape detection, at a false positive rate of 0.1%, our method achieves a detection rate of 99.5% vs. 98% for partition based methods. In pedestrian detection, our method reduces the miss detection rate by a factor of three at a false positive rate of 1%, compared with the method of Dalal-Triggs.	camera resectioning;experiment;face detection;jones calculus;object detection;pedestrian detection;sensor;unsupervised learning;weak value	Quan Yuan;Ashwin Thangali;Vitaly Ablavsky;Stan Sclaroff	2007	2007 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2007.383153	facial recognition system;computer vision;detector;face detection;human body;function space;shape;false positive rate;computer science;technical report;machine learning;pattern recognition;image sensor;mathematics;parameter space;estimation theory;statistics	Vision	35.1419946787658	-44.45949287709562	32477
08fad455bf0f8dd96d4285d5fa4a53f1166cfeb0	video classification using semantic concept co-occurrences	training semantics detectors context linear programming image edge detection support vector machines;video signal processing image classification image matching image representation integer programming;classification;gmcp;clique co occurrence semantic concept classification gmcp;mbip video classification semantic concept cooccurrences complex videos semantic attributes commonly termed concept generalized maximum clique problem gmcp semantic cooccurrence pattern class representation mixed binary integer programming;co occurrence;clique;semantic concept	We address the problem of classifying complex videos based on their content. A typical approach to this problem is performing the classification using semantic attributes, commonly termed concepts, which occur in the video. In this paper, we propose a contextual approach to video classification based on Generalized Maximum Clique Problem (GMCP) which uses the co-occurrence of concepts as the context model. To be more specific, we propose to represent a class based on the co-occurrence of its concepts and classify a video based on matching its semantic co-occurrence pattern to each class representation. We perform the matching using GMCP which finds the strongest clique of co-occurring concepts in a video. We argue that, in principal, the co-occurrence of concepts yields a richer representation of a video compared to most of the current approaches. Additionally, we propose a novel optimal solution to GMCP based on Mixed Binary Integer Programming (MBIP). The evaluations show our approach, which opens new opportunities for further research in this direction, outperforms several well established video classification methods.	baseline (configuration management);clique (graph theory);clique problem;integer programming;linear programming	Shayan Modiri Assari;Amir Roshan Zamir;Mubarak Shah	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.324	clique;co-occurrence;biological classification;theoretical computer science;machine learning;pattern recognition;mathematics	Vision	26.254047321221048	-45.99386169976909	32490
a108bb7dec4279a7c6e38c4de6786cc1b2cfa06c	high-precision synchronization of video cameras using a single binary light source	light emitting diodes;ground truth;computer hardware;video;cameras;light sources	1School of Information Science and Engineering, Fudan University, Shanghai, China 2School of Computer Science, Fudan University, Shanghai, China Abstract Camera synchronization is necessary for multi-camera appl ications. In this paper, we propose a simple and yet effective approach termed Random On-Off Light Sourc e (ROOLS ) to synchronize video sequences. It uses a single light source such as an LED to generate a rando m binary valued signal which is captured by the video cameras. The captured binary valued sequences a re then matched and the temporal offset of the cameras is computed up to sub-frame interval precision. We have tested the proposed method on synchronizing video sequences captured under a variety of illu mination conditions and the results are verified against the ground truth provided by an LED array clock. The m ain contribution of the proposed method is that it reliably achieves high precision synchronization a t a low cost of only adding a simple light source. In addition, it is suited for synchronization in both labora t ry and outdoor environments.	computer science;ground truth;information science;randomness	Qi Zhao;Yan Qiu Chen	2009	J. Electronic Imaging	10.1117/1.3247860	computer vision;video;ground truth;computer science;three-ccd camera;light-emitting diode	Vision	51.7629210299043	-42.24408885294335	32533
4b4ac923e28c2f1b9d281d1da3136539ab226937	real-time online adaption for robust instrument tracking and pose estimation		We propose a novel method for instrument tracking in Retinal Microsurgery (RM) which is apt to withstand the challenges of RM visual sequences in terms of varying illumination conditions and blur. At the same time, the method is general enough to deal with different background and tool appearances. The proposed approach relies on two random forests to, respectively, track the surgery tool and estimate its 2D pose. Robustness to photometric distortions and blur is provided by a specific online refinement stage of the offline trained forest, which makes our method also capable of generalizing to unseen backgrounds and tools. In addition, a peculiar framework for merging together the predictions of tracking and pose is employed to improve the overall accuracy. Remarkable advantages in terms of accuracy over the state-of-the-art are shown on two benchmarks.	3d pose estimation;real-time transcription	Nicola Rieke;David Joseph Tan;Federico Tombari;Josué Page Vizcaíno;Chiara Amat di San Filippo;Abouzar Eslami;Nassir Navab	2016		10.1007/978-3-319-46720-7_49	computer vision;tracking system	Robotics	41.94406444835279	-47.40582092372086	32547
f116780641a52401ff30af8d389f9b5655b64af3	informed sampling for asymptotically optimal path planning		"""Anytime almost-surely asymptotically optimal planners, such as RRT*, incrementally find paths to every state in the search domain. This is inefficient once an initial solution is found, as then only states that can provide a <italic> better</italic> solution need to be considered. Exact knowledge of these states requires solving the problem but can be approximated with heuristics. This paper formally defines these sets of states and demonstrates how they can be used to analyze arbitrary planning problems. It uses the well-known <inline-formula><tex-math notation=""""LaTeX"""">$L^2$ </tex-math></inline-formula> norm (i.e., Euclidean distance) to analyze minimum-path-length problems and shows that existing approaches decrease in effectiveness <italic>factorially</italic> (i.e., faster than exponentially) with state dimension. It presents a method to address this curse of dimensionality by <italic>directly</italic> sampling the prolate hyperspheroids (i.e., symmetric <inline-formula><tex-math notation=""""LaTeX"""">$n$</tex-math></inline-formula> -dimensional ellipses) that define the <inline-formula><tex-math notation=""""LaTeX"""">$L^2$</tex-math></inline-formula> <italic>informed</italic> set. The importance of this direct informed sampling technique is demonstrated with Informed RRT*. This extension of RRT* has less theoretical dependence on state dimension and problem size than existing techniques and allows for <italic>linear</italic> convergence on some problems. It is shown experimentally to find better solutions faster than existing techniques on both abstract planning problems and HERB, a two-arm manipulation robot."""	analysis of algorithms;anytime algorithm;approximation algorithm;asymptotically optimal algorithm;curse of dimensionality;euclidean distance;experiment;heuristic (computer science);motion planning;rapidly-exploring random tree;sampling (signal processing);whole earth 'lectronic link	Jonathan D. Gammell;Timothy D. Barfoot;Siddhartha S. Srinivasa	2018	IEEE Transactions on Robotics	10.1109/TRO.2018.2830331	control theory;mathematical optimization;euclidean distance;rate of convergence;asymptotically optimal algorithm;heuristics;curse of dimensionality;mathematics;probabilistic logic;motion planning;sampling (statistics)	AI	51.51185200642658	-23.966363691542295	32643
1d36ec54bdaa6ad5c457fcd80c7f48996e6f22ee	fpga-based architecture for extended associative memories and its application in image recognition	image recognition;reconfigurable logic;hardware architecture;extended associative memories	In this paper, an efficient FPGA-based architecture for Extended Associative Memories (EAM) focused on the classification stage of an image recognition system for real-time applications is presented. The EAM training phase is only used during the generation of associative memory, completed this task, this module is disconnected from the system; for this reason the hardware architecture of this module was designed for optimize the FPGA resource usage. On the other hand, the EAM can be part of a system requiring working in real time, such a perception system for a mobile robot or a personal identification system; for this reason, the hardware architecture of EAM classification phase was designed for obtaining high processing speeds. Experimental results show high performance of our proposal when altered versions of the images used to train the memory are presented.	computer vision;field-programmable gate array	Enrique Guzmán-Ramírez;Ignacio Arroyo-Fernández;Carlos González-Rojas;Jesús Linares-Flores;Oleksiy B. Pogrebnyak	2012		10.1007/978-3-642-37807-2_17	computer vision;computer science;artificial intelligence;theoretical computer science;hardware architecture	Robotics	44.854883084975555	-34.80958543036409	32734
9d67cc54ba016208ed10f4045445b65cf9361d5b	an akaike information criterion for multiple event mixture cure models	akaike information criterion;model selection;mixture cure model;em algorithm;competing risks	We derive the proper form of the Akaike information criterion for variable selection for mixture cure models, which are often fit via the expectation-maximization algorithm. Separate covariate sets may be used in the mixture components. The selection criteria are applicable to survival models for right-censored data with multiple competing risks and allow for the presence of an insusceptible group. The method is illustrated on credit loan data, with pre-payment and default as events and maturity as the insusceptible case and is used in a simulation study.	akaike information criterion;capability maturity model;censoring (statistics);expectation–maximization algorithm;feature selection;simulation	Lore Dirick;Gerda Claeskens;Bart Baesens	2015	European Journal of Operational Research	10.1016/j.ejor.2014.08.038	econometrics;determining the number of clusters in a data set;akaike information criterion;expectation–maximization algorithm;pattern recognition;mathematics;bayesian information criterion;model selection;statistics	ML	30.844369290997758	-24.110878355817235	32741
a8fd30515d5bdebfb82d5cb43e01f088ef8dd999	fpga implementation of image processing for real-time robot vision system		This paper presents a real-time robot vision system integrating adequate image processing and pan-tilt motion control which is implemented by FPGA reconfigurable logic device. The digital image processing acquired by CMOS image sensor is performed on the embedded FPGA board and Linux real-time video communication module. The integrated robot vision system aims to achieve a suitable platform available from both hardware and software. In addition, we also present spatial recognition by edge detection and tracking function of moving objects by determining color which are necessary for autonomous mobile robots.	autonomous robot;binocular disparity;cmos;digital image processing;edge detection;embedded system;field-programmable gate array;image sensor;linux;logic gate;mobile robot;real-time clock;real-time locating system;real-time transcription;reconfigurable computing;service robot;smoothing	Hayato Hagiwara;Kenichi Asami;Mochimitsu Komori	2011		10.1007/978-3-642-24106-2_18	embedded system;computer vision;machine vision	Robotics	44.85129945724194	-35.39227434610783	32751
a89d23f5e57cdb509571df499ec6c2d44d502866	comparison of early stopping criteria for neural-network-based subpixel classification	geophysical image processing;vegetation mapping;stopping criteria;neural nets;validation data set;training;mean squared error reduction;image classification;neural network learning rate;subpixel land cover classification;remote sensing protocols error analysis satellites robustness neural networks multilayer perceptrons bayesian methods geography;network training protocol;subpixel classification early stopping criterion neural network;artificial neural networks;training iteration;remote sensing;satellites;pixel;subpixel classification;mean square error methods;early stopping criterion;terrain mapping;remote sensing early stopping criteria spectral mixture problems network training protocol subpixel land cover classification mean squared error reduction validation data set training iteration neural network learning rate;spectral mixture problems;terrain mapping geophysical image processing image classification mean square error methods neural nets;data models;neural network;early stopping criteria	A neural-network-based subpixel classification is one of the most commonly used approaches to address spectral mixture problems. Neural-network subpixel-classification performance is directly related to the network-training protocols used. This letter examined early stopping criteria for network training of subpixel land-cover classification. A new stopping criterion is proposed that is based on the reduction of mean squared error (MSE) for a validation data set. We obtained excellent results by stopping the network training when the reduction of MSE between training iterations became marginal. Furthermore, the neural-network learning rate can be used as a threshold value to identify the stopping point. The approach appeared to be robust for both simulation data and actual remote-sensing data. Use of this criterion outperformed two other commonly used stopping criteria: a predefined number of training iterations and a cross-validation approach.	artificial neural network;cross-validation (statistics);early stopping;iteration;marginal model;mean squared error;pixel;simulation	Yang Shao;Gregory N. Taff;Stephen J. Walsh	2011	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2010.2052782	data modeling;contextual image classification;early stopping;computer science;machine learning;pattern recognition;data mining;artificial neural network;satellite;pixel;statistics	Vision	31.558448872564938	-42.81732267232065	32840
546b1aea260a82bb430a997106042e9a6317ade3	a factorization method for affine structure from line correspondences	affine camera;one dimensional camera;motion estimation;cameras shape image segmentation streaming media robot vision systems matrix decomposition motion estimation closed form solution robustness linearity;image reconstruction motion estimation;factorization method;image reconstruction;affine shape affine structure line correspondences factorization method recovery of shape motion affine cameras multi step factorization;structure from motion;line correspondence	A family of structure from motion algorithms called the factorization method has been recently developed from the orthographic projection model to the afJine camera model [23, i6, 181, All these algorithms are limited to handling only point features of the image stream. W e propose in this paper an algorithm for the recovery of shape and motion from line correspondences by the factorization method with the afJine camera. Instead of one step factorization for points, a multi-step factorization method is developed for lines based on the decomposition of the whole shape and m80tion into three separate substructures. Each of these substructures can then be linearly solved by factorizing the appropriate measurement matrices. I t is also established that a f i n e shape and motion with uncnlibrated a f i n e cameras can be achieved with a t least seven lines over three views, which extends the preuious results of Koenderink and Van Doorn [Q] for points to lines.	algorithm;orthographic projection;structure from motion	Long Quan;Takeo Kanade	1996		10.1109/CVPR.1996.517164	iterative reconstruction;affine space;computer vision;mathematical optimization;structure from motion;affine coordinate system;affine involution;computer science;affine plane;affine geometry of curves;affine hull;motion estimation;affine transformation;harris affine region detector;mathematics;geometry;affine shape adaptation;motion field;affine combination;hessian affine region detector	Vision	53.53326633843357	-50.83404487828235	32868
934d392767774d318414aab89f2146bed5d4cae7	clustering and dimensionality reduction on riemannian manifolds	eigenvalues and eigenfunctions;data segmentation;2d motion segmentation;pattern clustering;image motion analysis;generalizations;frequency modulation;local nonlinear dimensionality reduction algorithms;image segmentation;diffusion tensor images;manifolds;automatic segmentation;image segmentation clustering algorithms computer vision euclidean distance principal component analysis motion segmentation diffusion tensor imaging geophysics computing null space extraterrestrial measurements;geometry;null space;euclidean distance;matrix algebra;data representation;riemannian manifolds;computer vision;data clustering;diffusion tensor imaging segmentation;motion segmentation;riemannian metric;diffusion tensor imaging segmentation riemannian manifolds data clustering data representation local nonlinear dimensionality reduction algorithms generalizations riemannian spaces riemannian metric data segmentation 2d motion segmentation;geophysics computing;nonlinear dimensionality reduction;image representation;principal component analysis;riemannian manifold;pattern clustering generalisation artificial intelligence geometry image motion analysis image representation image segmentation matrix algebra;clustering algorithms;riemannian spaces;generalisation artificial intelligence;diffusion tensor imaging;extraterrestrial measurements;dimensional reduction;clustered data	We propose a novel algorithm for clustering data sampled from multiple submanifolds of a Riemannian manifold. First, we learn a representation of the data using generalizations of local nonlinear dimensionality reduction algorithms from Euclidean to Riemannian spaces. Such generalizations exploit geometric properties of the Riemannian space, particularly its Riemannian metric. Then, assuming that the data points from different groups are separated, we show that the null space of a matrix built from the local representation gives the segmentation of the data. Our method is computationally simple and performs automatic segmentation without requiring user initialization. We present results on 2-D motion segmentation and diffusion tensor imaging segmentation.	algorithm;cluster analysis;data point;kernel (linear algebra);nonlinear dimensionality reduction	Alvina Goh;René Vidal	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587422	isometry;exponential map;fundamental theorem of riemannian geometry;statistical manifold;mathematical optimization;topology;computer science;machine learning;mathematics;geometry;cluster analysis;information geometry;pseudo-riemannian manifold	Vision	27.633663558675643	-40.409860525817486	32889
5e29d2df884d5c48a9876b70606398d6f2f5b60a	classification of stellar spectral data using svm	dimension reduction;principle component analysis;support vector machine	In this paper a new technique is developed on stellar spectral classification. Because stellar spectral data sets are usually extremely noisy, wavelet de-noising method is proposed to reduce noise first. Then the support vector machines (SVM) is used for the classification. Experimental results show that in most cases, there will be a better performance using this composite classifier than using SVM with principle component analysis data dimension reduction technique.	dimensionality reduction;experiment;principal component analysis;stellar (payment network);stellar classification;support vector machine;wavelet	Fei Xing;Ping Guo	2004		10.1007/978-3-540-28647-9_101	support vector machine;computer science;machine learning;pattern recognition;data mining;mathematics;relevance vector machine;structured support vector machine;dimensionality reduction;principal component analysis	ML	29.22826932332271	-41.6348066170765	32931
69ff6c51ff42c7451dd6e2817bd9b34eceeb3b7c	lasso and equivalent quadratic penalized models		The least absolute shrinkage and selection operator (lasso) and ridge regression produce usually different estimates although input, loss function and parameterization of the penalty are identical. In this paper we look for ridge and lasso models with identical solution set. It turns out, that the lasso model with shrink vector λ and a quadratic penalized model with shrink matrix as outer product of λ with itself are equivalent, in the sense that they have equal solutions. To achieve this, we have to restrict the estimates to be positive. This doesn’t limit the area of application since we can decompose every estimate in a positive and negative part. The resulting problem can be solved with a non negative least square algorithm and may benefit from algorithms with high numerically accuracy. This model can also deal with mixtures of ridge and lasso penalties like the elastic net, leading to a continuous solution path as a function of the mixture proportions. Beside this quadratic penalized model, an augmented regression model with positive bounded estimates is developed which is also equivalent to the lasso model, but is probably faster to solve.	algorithm;elastic net regularization;lasso;least squares;loss function;numerical analysis;outer product	Stefan Hummelsheim	2013	CoRR		econometrics;mathematical optimization;mathematics;elastic net regularization;statistics	Vision	29.59613420842025	-25.991273184020578	32949
8cc85b8aadb96d22e40ccc56f763869bd12c3080	deep specialized network for illuminant estimation		Illuminant estimation to achieve color constancy is an illposed problem. Searching the large hypothesis space for an accurate illuminant estimation is hard due to the ambiguities of unknown reflections and local patch appearances. In this work, we propose a novel Deep Specialized Network (DS-Net) that is adaptive to diverse local regions for estimating robust local illuminants. This is achieved through a new convolutional network architecture with two interacting sub-networks, i.e. an hypotheses network (HypNet) and a selection network (SelNet). In particular, HypNet generates multiple illuminant hypotheses that inherently capture different modes of illuminants with its unique two-branch structure. SelNet then adaptively picks for confident estimations from these plausible hypotheses. Extensive experiments on the two largest color constancy benchmark datasets show that the proposed ‘hypothesis selection’ approach is effective to overcome erroneous estimation. Through the synergy of HypNet and SelNet, our approach outperforms state-of-the-art methods such as [1–3].	benchmark (computing);experiment;high- and low-level;interaction;network architecture;performance;reflection (computer graphics);synergy	Wu Shi;Chen Change Loy;Xiaoou Tang	2016		10.1007/978-3-319-46493-0_23	computer science;artificial intelligence;machine learning;network architecture;standard illuminant;color constancy	Vision	28.83831584843151	-49.913503453722285	32970
66087d782b4099859fe8923c5e7b18a25c78c2ce	a new perspective on object tracking based on byy and five action circling	best harmony;a5 paradigm;object tracking;five action circling;ying yang philosophy	In this paper, we present a new perspective on object tracking based on Bayesian Ying-Yang learning theory and five action circling. During the tracking procedure, the A5 circling must be kept well balanced. Each action should be neither too weak to sustain the system nor too loaded to jam the circling. For object tracking, the A5 paradigm is explained as follow: i) object acquirement and initialization; ii) object representation and description; iii) hypothesis measurement; iv) optimization and estimation; v) object assessment and location. Our extensive experiments show that the proposed novel framework performs robustly in a large variety of image sequences. Additional, a new insight is given on the Bayesian Ying-Yang system, best harmony learning, and five action circling theory from a perspective of object tracking system.		Zhenyu Wang;Peng Liu	2011		10.1007/978-3-642-31919-8_52	psychology;computer vision;artificial intelligence;communication	Vision	34.47831335673977	-47.85714241241434	33011
2ac73f90ca71240342ae54a0dc8c47f2f3d12f85	measure locally, reason globally: occlusion-sensitive articulated pose estimation	legged locomotion;biological system modeling;local likelihood;space exploration;null;kinematics;learning activities;tree graphs;graphical models;belief propagation;human body;tree structure;graphical model;inference algorithms;humans;computer science;biological system modeling humans kinematics graphical models tree graphs space exploration computer science inference algorithms legged locomotion belief propagation;pose estimation	Part-based tree-structured models have been widely used for 2D articulated human pose-estimation. These approaches admit efficient inference algorithms while capturing the important kinematic constraints of the human body as a graphical model. These methods often fail however when multiple body parts fit the same image region resulting in global pose estimates that poorly explain the overall image evidence. Attempts to solve this problem have focused on the use of strong prior models that are limited to learned activities such as walking. We argue that the problem actually lies with the image observations and not with the prior. In particular, image evidence for each body part is estimated independently of other parts without regard to self-occlusion. To address this we introduce occlusion-sensitive local likelihoods that approximate the global image likelihood using per-pixel hidden binary variables that encode the occlusion relationships between parts. This occlusion reasoning introduces interactions between non-adjacent body parts creating loops in the underlying graphical model. We deal with this using an extension of an approximate belief propagation algorithm (PAMPAS). The algorithm recovers the real-valued 2D pose of the body in the presence of occlusions, does not require strong priors over body pose and does a quantitatively better job of explaining image evidence than previous methods.	3d pose estimation;approximation algorithm;belief propagation;cobham's thesis;encode;graph (discrete mathematics);graphical model;hidden surface determination;interaction;pixel;software propagation	Leonid Sigal;Michael J. Black	2006	2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)	10.1109/CVPR.2006.180	computer vision;simulation;computer science;machine learning;articulated body pose estimation;mathematics;geometry;graphical model;statistics	Vision	46.52931468125796	-51.61036435124704	33027
6134c520eeb42c8a0f4f4d25e94f1ed5b734a8de	simulation-based inference for simultaneous processes on regular lattices	indirect inference;probability theory and statistics;robust estimator;maximum likelihood;monte carlo study;robust estimation;quadrant process;yule walker estimator;asymptotic properties;spatio temporal process;temporal processing;sannolikhetsteori och statistik	The article proposes a simulation-based inferential method for simultaneous processes defined on a regular lattice. The focus is on spatio-temporal processes with a simultaneous component, that is such that contemporaneous spatial neighbors are potential explanatory variables in the model. The new method has the advantage of being simpler to implement than maximum likelihood and allows us to propose a robust estimator. We give asymptotic properties, present a Monte Carlo study and an illustrative example.	inferential theory of learning;least squares;monte carlo method;robustification;robustness (computer science);s-plus;simulation;stochastic process	Xavier de Luna;Marc G. Genton	2002	Statistics and Computing	10.1023/A:1014830401806	robust statistics;econometrics;machine learning;mathematics;maximum likelihood;statistics	ML	28.354073230975395	-25.422550870821954	33039
3bb87c7229b1234d2a8da1e8936b4707da7d6ce7	reactive obstacle avoidance for multicopter uavs via evaluation of depth maps		Reacting to unforeseen obstacles is a major issue in the field of autonomous navigation. In the context of Unmanned Aerial Vehicles, an “obstacle” is any object that stands between the UAV and its desired position (waypoint). Therefore, obstacle detection can be reduced to the problem of assessing the visibility of the waypoint from the point of view of the drone. In this work, data acquired from an onboard depth camera are used to describe the visibility of the target waypoint in a qualitative framework, and to plan a new route when obstacles are detected.	map;obstacle avoidance;unmanned aerial vehicle	Luca Di Stefano;Eliseo Clementini;Enrico Stagnini	2017		10.1007/978-3-319-63946-8_10	machine learning;visibility;artificial intelligence;computer vision;motion planning;computer science;obstacle;waypoint;obstacle avoidance;depth map	Robotics	51.32996697315344	-35.908541686737415	33120
7f6e82b700bd35392a3e2c188911cb069a44aec5	motion segmentation via a sparsity constraint	image motion analysis image segmentation matrix algebra video signal processing;video sequences;computer vision;motion segmentation sparsity constraint intelligent transportation systems feature point trajectory video sequence correlation matrix sparse affinity matrix information theoretic principles nonzero elements feature point trajectories spectral clustering;motion segmentation sparse matrices trajectory computer vision video sequences computational modeling correlation;motion segmentation;computational modeling;trajectory;motion segmentation affinity based method residual sorting sparsity constraint;correlation;sparsity constraint motion segmentation affinity based method residual sorting;sparse matrices	Motion segmentation is an important task for intelligent transportation systems. In this paper, inspired by the fact that a feature point trajectory can be sparsely represented as a combination of several feature point trajectories that share coherent transformations, an efficient and effective motion segmentation method with a sparsity constraint is proposed. Specifically, we first propose an accumulated scheme to efficiently integrate motion information from all the frames of a video sequence to construct a correlation matrix. Then, a sparse affinity matrix is built on the correlation matrix by using information-theoretic principles, where the nonzero elements in the same row of the sparse affinity matrix correspond to the feature point trajectories more likely belonging to the same motion. Thereafter, a segment and merge procedure is proposed to effectively estimate the number of motions via the sparse affinity matrix. Finally, by applying spectral clustering on the sparse affinity matrix, different motions in the video sequence are accurately segmented based on the estimated number of motions. Experimental results on the Hopkins 155 and the 62-clip datasets demonstrate that the proposed method achieves superior performance compared with several state-of-the-art methods.	affinity analysis;cluster analysis;coherence (physics);experiment;information theory;processor affinity;sparse matrix;spectral clustering;tree accumulation	Taotao Lai;Hanzi Wang;Yan Yan;Tat-Jun Chin;Wanlei Zhao	2017	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2016.2596296	computer vision;mathematical optimization;sparse matrix;trajectory;machine learning;motion estimation;mathematics;scale-space segmentation;computational model;correlation	Vision	28.120088176745757	-40.886189264140775	33242
38ce19dbe337a6bf7ec4c1a62e5a2732f29e87c2	3d single point imaging technology for tracking multiple fish		Image based tracking like video tracking has shown potential in aquaculture behavioural studies in past decade. Image based tracking is allowing to have higher spatial and temporal resolution in compared to most conventional methods such as hand scoring, tagging and telemetry. They also permit to have more information about the environment rather than other methods. Most studies about trajectory are based on tracking in two dimensional (2D) environments; however, organisms are mostly included in three dimensional (3D) environments which influence ecological interactions extensively. Furthermore, in 2D image analysis, occlusion of fish is a frequent problem for analysis of fish tracking and ultimately their behaviour. Recently, new hardware based on single point 3D imaging technology have been developed which can provide 3D single points in real-time by combining a colour video camera, infrared video camera with an infrared projector. The main objective of this study was to develop a multiple fish tracking system in 3D space based on the current available 3D imaging technology. Developed system could accurately (98%) track multiple Tilapia (Oreochromis niloticus) which was freely swimming in an aquarium. This study contributes to feasibility of new sensors to monitor fish behaviours in 3D space.	3d reconstruction;fingerprint (computing);image analysis;imaging technology;interaction;kinect;network processor;range imaging;real-time clock;real-time locating system;sensor;tracking system;video projector;video tracking	Mohammadmehdi Saberioon;Petr Císar;Jan Urban	2016		10.5220/0005634001150121	computer science;tracking system;temporal resolution;video tracking;imaging technology;computer vision;artificial intelligence;video camera	Visualization	45.71212598985814	-42.2933760069559	33250
3c94d26997a312628d4f59c8c49bb6fd87ea7fb3	foam phantom development for artificial vertebrae used for surgical training	phantoms;artificial segments foam phantom development surgical training kyphoplasty vertebroplasty patient safety augmented reality simulator artificial vertebral segments realistic haptic feedback needle insertions formalin fixed vertebral specimens customized polyurethane blocks mechanical parameters specific foam phantom insertion force measurement parametric model needle insertion bone specimen measurements comprehensible characteristic parameters;biomechanics;orthopaedics;surgery biomechanics biomedical materials biomedical measurement bone cellular biophysics learning artificial intelligence neurophysiology orthopaedics phantoms physiological models polymers;polyurethane foam;artificial organs biocompatible materials biomechanical phenomena general surgery humans models anatomic spine;polyurethane foam artificial vertebrae kyphoplasty;artificial vertebrae;bone;surgery;force needles bones phantoms force measurement surgery geometry;neurophysiology;kyphoplasty;learning artificial intelligence;polymers;physiological models;cellular biophysics;biomedical measurement;biomedical materials	Currently the surgical training of kyphoplasty and vertebroplasty is performed on patients or specimens. To improve patient safety, a project was initiated to develop an Augmented Reality simulator for the surgical training of these interventions. Artificial vertebral segments should be integrated to provide realistic haptic feedback. To reach this, resulting forces during needle insertions (trans- and extrapedicular) into formalin-fixed vertebral specimens were measured. The same insertion procedure was also performed on six customized polyurethane blocks with varying mechanical parameters. Based on the results of these measurements, a specific foam phantom was generated and the insertion force measured. Additionally a parametric model for the needle insertion into bone was designed calculating three characteristic parameters for all insertion measurements. The resulting insertion force for the foam phantom was comparable to the specimen measurements and the parametric model provided comprehensible characteristic parameters. Based on the resulting force during needle insertion into human vertebrae, a possible foam recipe for manufacturing artificial segments was found. Furthermore, the parametric model provides characteristic parameters for the assessment of phantoms as well as the development of its production process.	augmented reality;biological specimen;bone tissue;bone structure of spine;clinical act of insertion;customize;formalin;haptic device component;haptic technology;imaging phantom;insertion mutation;kyphoplasty;parametric model;patients;phantom reference;phantoms, imaging;polyurethanes;simulators;vertebroplasty	David Fuerst;Daniel Stephan;Peter Augat;Andreas Schrempf	2012	2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2012.6347306	medicine;pathology;polymer;biomechanics;biological engineering;physiology;neurophysiology;surgery	Robotics	39.86016330855553	-37.849035503234674	33252
a432ee5977443b5c29001f4bd10c6303cc364d4d	polysemious visual representation based on feature aggregation for large scale image applications	polysemious representation;max pooling;large scale;feature aggregation	Multiple image features and multiple semantic concepts from the images have intrinsic and complex relations. These relations influence the effectiveness of image semantic analysis methods, especially on the large scale problems. In this paper, a framework of generating polysemious image representation through three levels of feature aggregation is proposed. In the codebook level aggregation, visual dictionaries are learned for each feature type, and each image feature can be reconstructed with this dictionary. In the semantic level aggregation, the multiple concept distributions are learned with each feature codebook by using the improved local anchor embedding. Then the polysemious representation for for single feature type can be established after this level. In the multiple feature level aggregation, final image polysemious representation is obtained through multiple feature fusion with a weighted pooling approach. Through the proposed framework, multiple feature fusion and multiple semantic descriptions are both achieved in an integrated way. Experimental evaluations on large scale image dataset validate the effectiveness of the proposed method.	codebook;dictionary;feature (computer vision);semantic analysis (compilers);semantic mapper	Xinhang Song;Shuqiang Jiang;Shuhui Wang;Liang Li;Qingming Huang	2014	Multimedia Tools and Applications	10.1007/s11042-014-1975-5	feature detection;machine learning;pattern recognition;data mining;feature	Vision	26.78542366165149	-46.494694848223055	33315
a5ae7fe2bb268adf0c1cd8e3377f478fca5e4529	exemplar hidden markov models for classification of facial expressions in videos	kernel;support vector machines;video signal processing emotion recognition face recognition hidden markov models image classification matrix algebra support vector machines time series video databases;hidden markov models videos kernel probabilistic logic computational modeling support vector machines probability distribution;conference paper;computational modeling;hidden markov models;probability distribution;probabilistic logic;spatiotemporal features exemplar hidden markov models hmm dynamic events temporal segments facial expression recognition variable length expression sequences vector representation summary statistics image level features feedtum datasets am fed datasets oulu casia datasets ck datasets expression datasets kernel matrix probabilistic kernel discriminative power generative models classification performance time series svm support vector machines discriminative classifier;videos	Facial expressions are dynamic events comprised of meaningful temporal segments. A common approach to facial expression recognition in video is to first convert variable-length expression sequences into a vector representation by computing summary statistics of image-level features or of spatio-temporal features. These representations are then passed to a discriminative classifier such as a support vector machines (SVM). However, these approaches don't fully exploit the temporal dynamics of facial expressions. Hidden Markov Models (HMMs), provide a method for modeling variable-length expression time-series. Although HMMs have been explored in the past for expression classification, they are rarely used since classification performance is often lower than discriminative approaches, which may be attributed to the challenges of estimating generative models. This paper explores an approach for combining the modeling strength of HMMs with the discriminative power of SVMs via a model-based similarity framework. Each example is first instantiated into an Exemplar-HMM model. A probabilistic kernel is then used to compute a kernel matrix, to be used along with an SVM classifier. This paper proposes that dynamical models such as HMMs are advantageous for the facial expression problem space, when employed in a discriminative, exemplar-based classification framework. The approach yields state-of-the-art results on both posed (CK+ and OULU-CASIA) and spontaneous (FEEDTUM and AM-FED) expression datasets highlighting the performance advantages of the approach.	discriminative model;expression problem;facial recognition system;generative model;hidden markov model;markov chain;pattern recognition;problem domain;spontaneous order;support vector machine;time series	Karan Sikka;Abhinav Dhall;Marian Stewart Bartlett	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2015.7301350	probability distribution;support vector machine;kernel;computer science;machine learning;pattern recognition;data mining;mathematics;probabilistic logic;computational model;hidden markov model;discriminative model	Vision	35.31345211950911	-47.907446114439836	33330
5889b05c3a41c541e29ea6ba816aff3be0f4e41c	codebook design for vector quantization based on a kernel fuzzy learning algorithm	codebook design;learning algorithm;data compression;discrete cosine transform;fuzzy clustering;vector quantization;general methods;principle component analysis;vector quantizer;linde buzo gray;kernel fuzzy learning	Vector quantization (VQ) is an efficient technique for data compression and has been successfully used in various applications. The methods most commonly used to generate a codebook are the Linde, Buzo, Gray (LBG) algorithm, fuzzy vector quantization (FVQ) algorithm, Kekre‘s Fast Codebook Generation (KFCG) algorithm, discrete cosine transform based (DCT-based) codebook generation method, and k-principle component analysis (K-PCA) algorithm. However, if the separation boundaries in codebook generation are nonlinear, their performance can degrade fast. In this paper, we present a kernel fuzzy learning (KFL) algorithm, which takes advantages of the distance kernel trick and the gradient-based fuzzy clustering method, to create a codebook automatically. Experiments with real data show that the proposed algorithm is more efficient in its performance compared to that of the LBG, FVQ, KFCG, and DCT-based method, and to the K-PCA algorithm.	algorithmic learning theory;cluster analysis;codebook;data compression;discrete cosine transform;fuzzy clustering;gradient;kernel (operating system);kernel method;linde–buzo–gray algorithm;location-based game;nonlinear system;peak signal-to-noise ratio;principal component analysis;vector quantization	Zongbo Xie;Jiuchao Feng	2011	CSSP	10.1007/s00034-011-9271-3	data compression;fuzzy clustering;computer science;theoretical computer science;machine learning;discrete cosine transform;pattern recognition;mathematics;linde–buzo–gray algorithm;vector quantization;principal component analysis	ML	29.946699626796896	-39.90232026761046	33370
a39dea3a4efc9521b16383e0bdd4c02edb5cb9cd	example-based visual object counting for complex background with a local low-rank constraint		Visual object counting (VOC) is important in many real-world applications. Our previous work approximated sparsity-constrain example-based VOC (ASE-VOC) works well with insufficient training data. It assumes that image patches share the similar local geometry with counterpart density maps, and then the density map of the image patch can be estimated by preserving such geometry. However. ASE-VOC has a weak constraint for data structure and experiments reveal that the performance of ASE-VOC degrades when facing with complex background. To solve this problem, we proposed a novel local low-rank constrained example-based VOC (LLRE-VOC) method. Because local low-rank constraint can choose the samples belonging to the subspace that lies closest to the test samples. Even with complicated data structure, LLRE-VOC can guarantee the patches selected share similar structure with input patch. Extensive experiments conducted on public benchmarks demonstrate the superior performance of our proposed LLRE-VOC method.	adaptive server enterprise;approximation algorithm;benchmark (computing);data structure;experiment;map;sparse matrix	X. L. Huang;Y. X. Zou;Yue Wang	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952441	iterative reconstruction;artificial intelligence;visualization;pattern recognition;machine learning;computer science;training set;subspace topology;data structure	Vision	29.364814679566823	-47.85275837703056	33440
88778be8e9845d36e4690141ff33e57497de3d97	geometry-aware neighborhood search for learning local models for image superresolution	manifolds clustering algorithms computational modeling image resolution data models training adaptation models;biological patents;biomedical journals;text mining;europe pubmed central;citation search;citation networks;nearest neighbor search clustering patch manifolds;research articles;abstracts;open access;life sciences;clinical guidelines;pattern clustering computer vision differential geometry graph theory image reconstruction image resolution inverse problems learning artificial intelligence;geometry aware neighborhood search geodesic distance based subset selection soft clustering spectral clustering geometry driven overlapping cluster replicator graph clustering method out of sample extension agnn search adaptive scheme adaptive geometry driven nearest neighbor search input test sample reconstruction dissimilarity metric euclidean distance k means algorithm computer vision applications inverse problem sparse image model image superresolution learning local model;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Local learning of sparse image models has proved to be very effective to solve inverse problems in many computer vision applications. To learn such models, the data samples are often clustered using the K-means algorithm with the Euclidean distance as a dissimilarity metric. However, the Euclidean distance may not always be a good dissimilarity measure for comparing data samples lying on a manifold. In this paper, we propose two algorithms for determining a local subset of training samples from which a good local model can be computed for reconstructing a given input test sample, where we consider the underlying geometry of the data. The first algorithm, called adaptive geometry-driven nearest neighbor search (AGNN), is an adaptive scheme, which can be seen as an out-of-sample extension of the replicator graph clustering method for local model learning. The second method, called geometry-driven overlapping clusters (GOCs), is a less complex nonadaptive alternative for training subset selection. The proposed AGNN and GOC methods are evaluated in image superresolution and shown to outperform spectral clustering, soft clustering, and geodesic distance-based subset selection in most settings.	algorithm;assumed;autostereogram;circuit restoration;computer vision;distance (graph theory);euclidean distance;graph - visual representation;iterative method;k-means clustering;nearest neighbor search;single linkage cluster analysis;sparse matrix;spectral clustering;subgroup;super-resolution imaging;telling untruths;manifold;negative regulation of notch signaling pathway involved in somitogenesis;statistical cluster	Júlio César Ferreira;Elif Vural;Christine Guillemot	2016	IEEE Transactions on Image Processing	10.1109/TIP.2016.2522303	computer vision;text mining;medical research;computer science;machine learning;pattern recognition;data mining;mathematics;statistics	Vision	27.88700988525548	-39.68914430606288	33499
c5aeb96be4c04b2fd6904e63940b59edf20dcb97	multiple kernel learning with data augmentation		The motivations of multiple kernel learning (MKL) approach are to increase kernel expressiveness capacity and to avoid the expensive grid search over a wide spectrum of kernels. A large amount of work has been proposed to improve the MKL in terms of the computational cost and the sparsity of the solution. However, these studies still either require an expensive grid search on the model parameters or scale unsatisfactorily with the numbers of kernels and training samples. In this paper, we address these issues by conjoining MKL, Stochastic Gradient Descent (SGD) framework, and data augmentation technique. The pathway of our proposed method is developed as follows. We first develop a maximum-aposteriori (MAP) view for MKL under a probabilistic setting and described in a graphical model. This view allows us to develop data augmentation technique to make the inference for finding the optimal parameters feasible, as opposed to traditional approach of training MKL via convex optimization techniques. As a result, we can use the standard SGD framework to learn weight matrix and extend the model to support online learning. We validate our method on several benchmark datasets in both batch and online settings. The experimental results show that our proposed method can learn the parameters in a principled way to eliminate the expensive grid search while gaining a significant computational speedup comparing with the state-of-the-art baselines.	algorithmic efficiency;benchmark (computing);cobham's thesis;convex optimization;convolutional neural network;gene regulatory network;graphical model;kernel (operating system);math kernel library;mathematical optimization;multiple kernel learning;optimization problem;scalability;sparse matrix;speedup;stochastic gradient descent	Khanh Nguyen;Trung Le;Vu Nguyen;Tu Dinh Nguyen;Dinh Q. Phung	2016			machine learning;pattern recognition	AI	25.614286364589724	-33.82556866326249	33545
5c789cd7f565156147477835443ddabf3296938c	adaptive maximum-likelihood-like estimation in linear models. i. consistency	modelizacion;maximum likelihood;maximum vraisemblance;linear regression;modelisation;consistent estimator;maximization of kernel estimates of densities of residuals;regresion lineal;adaptive estimator;estimacion adaptativa;modeling;regression lineaire;consistency;maxima verosimilitud;estimateur convergent;adaptive estimation;estimador convergente;estimation adaptative		linear model;processor consistency	Jan Ámos Vísek	1992	Kybernetika		econometrics;mathematical optimization;systems modeling;linear regression;mathematics;maximum likelihood;consistency;consistent estimator;statistics	NLP	32.54557074162648	-24.009051093231218	33567
1b6c8128db1dc224701429c0637c553a15a9e767	cooperative control of uavs for localization of intermittently emitting mobile targets	cooperative control;detectors;decentralization;aircraft control;unmanned aerial vehicles robustness radio frequency rf signals distributed control logic time measurement aerospace simulation hardware testing;radiofrequency;platforms;unmanned aerial vehicle;localization;simulation;emission;drones;robust control;remotely operated vehicles;searching;unmanned aerial vehicles uavs;cooperative;flight control systems;robust control aircraft control decentralised control object detection remotely operated vehicles;radio frequency;decentralised control;remotely piloted vehicles;moving targets;coast guard research;target localization;unmanned aerial vehicles uavs cooperative distributed control localization;decision theory;decision logic cooperative control mobile target localization unmanned aerial vehicle radio frequency signal emission decentralized control architecture rudimentary sensor;decentralized control;radio frequency signal emission;multiple targets;flight testing;decentralized control architecture;surface targets;mobile target localization;radio signals;distributed control;decision logic;rudimentary sensor;object detection	Compared with a single platform, cooperative autonomous unmanned aerial vehicles (UAVs) offer efficiency and robustness in performing complex tasks. Focusing on ground mobile targets that intermittently emit radio frequency signals, this paper presents a decentralized control architecture for multiple UAVs, equipped only with rudimentary sensors, to search, detect, and locate targets over large areas. The proposed architecture has in its core a decision logic which governs the state of operation for each UAV based on sensor readings and communicated data. To support the findings, extensive simulation results are presented, focusing primarily on two success measures that the UAVs seek to minimize: overall time to search for a group of targets and the final target localization error achieved. The results of the simulations have provided support for hardware flight tests.	aerial photography;algorithm;american osteopathic association;angle of arrival;autonomous robot;consensus dynamics;distributed control system;drug vehicle;emitter device component;estimated;internationalization and localization;kalman filter;lambert's cosine law;loss function;radio frequency;reading (activity);requirement;scheduling (computing);scheduling - hl7 publishing domain;scientific publication;simulation;unmanned aerial vehicle;sensor (device);triangulation	Daniel J. Pack;Pedro DeLima;Gregory J. Toussaint;George York	2009	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2008.2010865	simulation;computer science;control theory;radio frequency	Robotics	52.75791036195886	-33.23333493562333	33605
20021f1770ca5c977efd63249f8fcd77a634f0d2	spatially cost-sensitive active learning	hyperspectral imagery;spatial data;active learning;land cover classification;spatial distribution;ground truth	In active learning, one attempts to maximize classifier performance for a given number of labeled training points by allowing the active learning algorithm to choose which points should be labeled. Typically, when the active learner requests labels for the selected points, it assumes that all points require the same amount of effort to label and that the cost of labeling a point is independent of other selected points. In spatially distributed data such as hyperspectral imagery for land-cover classification, the act of labeling a point (i.e., determining the land-type) may involve physically traveling to a location and determining ground truth. In this case, both assumptions about label acquisition costs made by traditional active learning are broken, since costs will depend on physical locations and accessibility of all the visited points. This paper formulates and analyzes the novel problem of performing active learning on spatial data where label acquisition costs are proportional to distance traveled.	accessibility;active learning (machine learning);algorithm;gis applications;ground truth;iteration;label printer applicator;money	Alexander Liu;Goo Jun;Joydeep Ghosh	2009		10.1137/1.9781611972795.70	computer vision;ground truth;hyperspectral imaging;pattern recognition;spatial analysis;active learning;remote sensing	ML	32.30190135308077	-40.20379906957523	33606
92f3088780001455523ac3c6f1cec1d58c80d6a5	a stick-breaking likelihood for categorical data analysis with latent gaussian models		Problem: Parameter learning is difficult since the marginal likelihood contains an intractable integral, which arises due to the non-conjugacy between the likelihood and Gaussian prior on the latent variables. Solution: We propose a novel stick-breaking likelihood for categorical data analysis and derive tractable and accurate lower bounds for the marginal likelihood. Our results demonstrate that the proposed stick-breaking model effectively captures correlation and is well suited to the analysis of categorical data.	categorical variable;cobham's thesis;latent variable;marginal model	Mohammad Emtiyaz Khan;Shakir Mohamed;Benjamin M. Marlin;Kevin P. Murphy	2012			econometrics;categorical variable;pattern recognition;mathematics;statistics	ML	27.777236425388214	-29.700718377394693	33607
a110602d15b1c98459071501483366a56cfb59d5	online video text detection with markov decision process		Online video text detection is important in many applications, such as real-time translator and wearable camera system for visually-impaired. Existing methods for video text detection perform unsatisfactorily mainly because of the inferior text detection result and insufficient utilization of spatial and temporal information. Besides, the majority of them work in offline mode. In this paper, we propose an online video text detection method which works nearly in real time. We detect texts in each frame using a EAST based text detector, and formulate the online text tracking problem as decision making in Markov Decision Processes (MDPs). The similarity function in tracking stage can be learned by reinforcement learning. Besides, text detection and tracking are naturally unified by state transactions in the MDP. Extensive experiments on three benchmark datasets, ICDAR 2015, Minetto, and Youtube Video Text, verify the effectiveness of our method.	airplane mode;benchmark (computing);experiment;international conference on document analysis and recognition;markov chain;markov decision process;online and offline;performance;real-time clock;reinforcement learning;scalability;similarity measure;video clip	Xue-Hang Yang;Fei Yin;Cheng-Lin Liu	2018	2018 13th IAPR International Workshop on Document Analysis Systems (DAS)	10.1109/DAS.2018.20	real-time computing;machine learning;wearable computer;feature extraction;reinforcement learning;computer science;markov decision process;markov process;artificial intelligence	AI	34.340041346464595	-50.276697669623154	33630
0e3a90e491ccdb8be0d77a54843dba0de39f9765	feature based object recognition using statistical occlusion models with one-to-one correspondence	synthetic aperture radar;statistical models;bayes methods;statistical occlusion models;object recognition;synthetic aperture radar imagery;partially occluded;bayesian framework;bayesian methods;data mining;probability;spatial correlation;feature extraction;layout;statistical model;shape;image resolution;object model	In this paper we present a new Bayesian framework for partially occluded object recognition with one-to-one correspondence. We introduce two different statistical models for occlusion: One model assumes that each feature in the model can be occluded independent of whether any other features are occluded, whereas the second model uses spatially correlated occlusion to represent the extent of occlusion. Using these models, the object recognition problem reduces to finding the object hypothesis with largest generalized likelihood We develop fast algorithms for finding the optimal one-to-one correspondence between scene features and object model features to compute the generalized likelihood. We evaluate our algorithms using examples extracted from synthetic aperture radar imagery, and illustrate the performance advantages of our approach over alternative algorithms proposed by others	one-to-one (data model);outline of object recognition	Zhengrong Ying;David A. Castañón	2001		10.1109/ICCV.2001.10093	statistical model;computer vision;computer science;machine learning;pattern recognition;mathematics;statistics	Vision	45.393800899045836	-51.600525949710686	33637
2da029370083be80fc6ea968c5eaa03acc6c48fe	consensus-based distributed linear support vector machines	online algorithm;support vector machines;convex optimization;sensor network;sensor networks;optimization;support vector machine;training algorithm	This paper develops algorithms to train linear support vector machines (SVMs) when training data are distributed across different nodes and their communication to a centralized node is prohibited due to, for example, communication overhead or privacy reasons. To accomplish this goal, the centralized linear SVM problem is cast as the solution of coupled decentralized convex optimization subproblems with consensus constraints on the parameters defining the classifier. Using the method of multipliers, distributed training algorithms are derived that do not exchange elements from the training set among nodes. The communications overhead of the novel approach is fixed and fully determined by the topology of the network instead of being determined by the size of the training sets as it is the case for existing incremental approaches. An online algorithm where data arrive sequentially to the nodes is also developed. Simulated tests illustrate the performance of the algorithms.	centralized computing;convex optimization;mathematical optimization;online algorithm;overhead (computing);support vector machine;test set	Pedro A. Forero;Alfonso Cano;Georgios B. Giannakis	2010		10.1145/1791212.1791218	embedded system;support vector machine;mathematical optimization;convex optimization;wireless sensor network;computer science;machine learning;data mining	ML	31.785220935583624	-35.119989605709506	33639
06fc643a48464fd8280d11c1189f49afacbd43a4	semantic mapping using object-class segmentation of rgb-d images	belief networks;object recognition;image segmentation;slam robots belief networks cameras decision making image colour analysis image segmentation image sensors image texture object recognition path planning;path planning;training;semantics;image sensors;image texture;accuracy;image segmentation image color analysis simultaneous localization and mapping semantics training decision trees accuracy;image color analysis;image colour analysis;simultaneous localization and mapping;decision trees;slam robots;cameras;object class segmentation method semantic mapping object class segmentation rgb d images task planning task execution unstructured environments rgb d cameras simultaneous localization and mapping random decision forests dense depth measurements scale invariance object recognition method probabilistic segmentation voxel based 3d map bayesian framework	For task planning and execution in unstructured environments, a robot needs the ability to recognize and localize relevant objects. When this information is made persistent in a semantic map, it can be used, e. g., to communicate with humans. In this paper, we propose a novel approach to learning such maps. Our approach registers measurements of RGB-D cameras by means of simultaneous localization and mapping. We employ random decision forests to segment object classes in images and exploit dense depth measurements to obtain scale-invariance. Our object recognition method integrates shape and texture seamlessly. The probabilistic segmentation from multiple views is filtered in a voxel-based 3D map using a Bayesian framework. We report on the quality of our object-class segmentation method and demonstrate the benefits in accuracy when fusing multiple views in a semantic map.	experiment;graphics processing unit;image gradient;image segmentation;outline of object recognition;persistent data structure;pixel;robot;robotics;semantic mapper;simultaneous localization and mapping;voxel	Jörg Stückler;Nenad Biresev;Sven Behnke	2012	2012 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2012.6385983	image texture;computer vision;computer science;cognitive neuroscience of visual object recognition;machine learning;decision tree;pattern recognition;image sensor;semantics;motion planning;accuracy and precision;image segmentation;simultaneous localization and mapping	Robotics	49.4197511496005	-39.13252274716777	33684
01e3666804c425bc87ca14a2bba3295dca7e8202	rapid weak-perspective structure from motion with missing data	minimisation;approximate algorithm;solid modelling computer vision graphics processing units image reconstruction minimisation parameter estimation;three dimensional displays cameras graphics processing unit vectors matrix decomposition real time systems approximation algorithms;approximation algorithms;real time;optimization algorithm weak perspective structure missing data computer vision gpu powered version motion algorithm bmvc2008 parameter minimization reprojection error real time 3d reconstruction;three dimensional;computer vision;vectors;matrix decomposition;three dimensional displays;image reconstruction;graphics processing units;graphic processing unit;missing data;parameter estimation;graphics processing unit;optimal algorithm;structure from motion;3d reconstruction;parameter optimization;cameras;solid modelling;real time systems	3D reconstruction with missing data has been a very challenging computer vision task since the late 90s. This paper proposes the GPU-powered version of our weak-perspective Structure from Motion algorithm published in BMVC2008. Although this method is iterative, it is very rapid since all substeps in each iteration minimize the parameters optimally with respect to the reprojection error. We demonstrate here in both synthetic and real tests that the use of the GPU significantly reduces the time demand of the algorithm. Real-time 3D reconstruction is possible if the parameters of the optimization algorithm are set properly.	3d projection;3d reconstruction;algorithm;computer vision;dhrystone;graphics processing unit;iteration;least squares;mathematical optimization;missing data;online and offline;parallel programming model;real-time clock;real-time computing;real-time transcription;reprojection error;structure from motion;test data;virtual reality headset	Csaba Kazó;Levente Hajder	2011	2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)	10.1109/ICCVW.2011.6130283	3d reconstruction;iterative reconstruction;three-dimensional space;computer vision;minimisation;mathematical optimization;structure from motion;missing data;computer science;mathematics;estimation theory;matrix decomposition;approximation algorithm;statistics;computer graphics (images)	Vision	52.269102722799914	-48.573478217770564	33719
d266e78d54087568ad1110a5a6a24ce6e31368f3	a simple and real-time moving object detection invariant to cast shadow	object detection image motion analysis image sensors lighting;surveillance moving object detection background subtraction shadow;shadow invariant features real time moving object detection cast shadow shadow effects illumination changes static camera declining sun elongated shadow illumination condition;feature extraction;sun;robustness;lighting;probabilistic logic;lighting feature extraction robustness object detection real time systems probabilistic logic sun;object detection;real time systems	We present a moving objection detection method robust to shadow effects and illumination changes on a static camera. This study addresses the effect of the declining sun at evening, which produces elongated shadow of objects. The illumination condition of the evening time is more rapidly changed. These two factors can affect the quality of the moving object detection. We deal with these artifacts by fusing the several illumination or shadow invariant features. The preliminary results of this study are validated qualitatively on two real datasets. Also, the proposed algorithm achieves the real-time performance.	algorithm;illumination (image);object detection;real-time clock	Tae Hyun Oh;In-So Kweon	2014	2014 11th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)	10.1109/URAI.2014.7057496	computer vision;feature extraction;computer science;lighting;probabilistic logic;robustness	Robotics	43.830101504020675	-46.06846353405828	33723
7f68a5429f150f9eb7550308bb47a363f2989cb3	multiple-facial action unit recognition by shared feature learning and semantic relation modeling	gold accuracy databases face recognition feature extraction bayes methods hidden markov models;learning artificial intelligence bayes methods directed graphs face recognition feature extraction inference mechanisms;denver intensity of spontaneous facial actions database multiple facial action unit recognition shared feature learning semantic relation modeling feature labels target labels multitask feature learning method action unit recognition task division bayesian network co existent mutual exclusive semantic relations facial images bayesian network learning probabilistic inference extended cohn kanade database;action unit recognition;multi task learning action unit recognition;multi task learning	In this paper, we propose multiple facial action unit recognition by modeling their relations from both features and target labels. First, a multi-task feature learning method is adopted to divide action unit recognition tasks into several groups, and then learn the shared features for each group. Second, a Bayesian network is used to model the co-existent and mutual-exclusive semantic relations among action units from the target labels of facial images. After that, the learned Bayesian network employs the recognition results of the multi-task learning, and realizes multiple facial action recognition by probabilistic inference. Experiments on the extended Cohn-Kanade database and the Denver Intensity of Spontaneous Facial Actions database demonstrate the effectiveness of our approach.	bayesian network;computer multitasking;exclusive relationship (programming);feature learning;feature vector;multi-task learning;ontology components;spontaneous order	Yachen Zhu;Shangfei Wang;Lihua Yue;Qiang Ji	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.293	multi-task learning;computer vision;feature;computer science;machine learning;pattern recognition	Vision	34.89373097376577	-47.30537100474129	33757
4ad0126efabdf1a2bcab6c0f2579ff72265f324f	principal component analysis with missing data and its application to object modeling	minimisation;sequences;errors;least squares approximations;least mean square methods;normal measurement matrix;image segmentation;object modeling;measurement;least squares method;distance measure;weighted least square;weighting functions;least mean square methods minimization methods matrices virtual reality image segmentation machine vision modeling;transformations;frames;virtual reality;real range images;minimization methods;matrix algebra;model tests;motion;integration;multiple views;computer vision;toy house model;pcamd;regions;observation based modeling;quaternion representation;visible regions;matrices;matrices mathematics;redundancy;shape;mathematical models;multiple view merging;factor analysis;range image;machine vision;toy house model principal component analysis missing data object modeling observation based modeling virtual reality environment range images pcamd weighted least square minimization problem visible regions normal measurement matrix surface normals distance measurement matrix multiple view merging wls problems quaternion representation rotation matrices translation vectors synthetic data real range images statistically optimal object model redundancy;principal component analysis;surface normals;observation;range images;distance measurement matrix;sequences mathematics;algorithms;rotation matrices;virtual reality environment;surfaces;digital simulation least squares approximations minimisation matrix algebra virtual reality image segmentation computer vision modelling;missing data;statistically optimal object model;translation vectors;segmented;synthetic data;approach;rotation;modeling;wls problems;images;digital simulation;problem solving;noise;object model;weighted least square minimization problem	Observation-based modeling can reduce the cost and effort of model constructions for tasks such as virtual reality environment. In this paper object modeling from a sequence of range images has been formulated as a problem of principal component analysis with missing data (PCAMD), which can be generalized as a weighted least square (WLS) minimization problem. After all visible regions appeared over the whole sequence are segmented and tracked, a normal measurement matrix of surface normals and a distance measurement matrix of normal distances to the origin are constructed respectively. These two measurement matrices, with possibly many missing elements due to occlusion and mismatching, enable us to formulate multiple view merging as a combination of two WLS problems. The solution to the first WLS problem, which employs the quaternion representation of the rotation matrix, yields sutj'ace normals and rotation matrices. Subsequently the normal distances and translation vectors are computed by solving the second WLS problem. Experiments using synthetic data and real range images show that our approach is robust against noise and mismatch because it produces a statistically optimal object model by muking use of redundancy from multiple views. A toy house model from a sequence of real range images is presented.	experiment;isometric projection;least squares;missing data;normal (geometry);principal component analysis;synthetic data;virtual reality	Harry Shum;Katsushi Ikeuchi;Raj Reddy	1994		10.1109/CVPR.1994.323882	computer vision;mathematical optimization;object model;machine vision;computer science;machine learning;mathematics;geometry;virtual reality;statistics	Vision	53.064554309902526	-50.65724957063897	33767
b1de2b64bbe90bf39af2551877208a6b4dc0dae8	a real-time velocity estimation using motion blur in air hockey	off the shelf cameras real time velocity estimation motion blur air hockey puck velocity estimation low resolution image puck detection fourier transform motion direction extraction air hockey table;object detection cameras fourier transforms image motion analysis image resolution image restoration	In this paper a real-time approach for the air hockey puck's velocity estimation based on motion blur is presented. In the proposed approach first a low resolution image is used to detect the puck and then the Fourier transform is used on the full resolution image to extract the direction of the motion. This hierarchical approach provides faster processing speed. The roundness of the ball and the length of the blur are finally used in order to estimate the speed. The proposed approach is implemented on an actual air hockey table using two different cameras, one with 30 and the other one with 60 frames per second. The obtained results show promising performance on velocity estimation using off-the-shelf cameras.	gaussian blur;image resolution;real-time clock;real-time locating system;velocity (software development)	Shayan Rezvankhah;Amir Ali Bagherzadeh;Hadi Moradi;Babak Nadjar Araabi	2012	2012 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2012.6491223	computer vision;computer science;optics;computer graphics (images)	Robotics	44.99290835373404	-42.39325537930123	33791
4f9ec1b8028429cb4a53371b5eedd03b3947239f	grasp exploration for 3d object shape representation using probabilistic map	center of mass;motion tracking;shape representation;probability distribution;frame of reference;coordinate system	In this work, the representation of 3D object shape acquired from grasp exploration is presented. Electromagnetic motion tracking sensors are attached to the fingertips for object contour following, acquiring thus the 3D points to represent the object shape using a probabilistic volumetric map. The object-centric representation is adopted by finding the centroid through 3D moment invariants. The occupancy of each individual voxel in the map is assumed to be independent from the other voxels occupancy, and the posterior information is obtained by using the Bayes’ rule, thus achieving the probability distribution on the occupations percentage for each voxel. The conversion of the probabilistic map in Cartesian space to the spherical coordinate system for visualization with more resolution in an egocentric view is proposed.	experiment;object point;point cloud;sensor;shape context;stereopsis;voxel	Diego R. Faria;Ricardo Martins;Jorge Dias	2010		10.1007/978-3-642-11628-5_23	probability distribution;center of mass;frame of reference;computer vision;coordinate system;mathematics;geometry;engineering drawing;quantum mechanics	Robotics	51.039048988210546	-40.502858465845726	33816
12ae599756603f0b79d395ba88083866de5e5238	tracking articulated hand motion with eigen dynamics analysis	image motion analysis;articulated hand motion tracking;stochastic linear dynamic system;sequential monte carlo method;bayes methods;image sequence analysis;bayes procedures;eigen dynamics analysis;likelihood edge;computer vision;motion capture;dynamic bayesian network;machine vision;principal component analysis;linear dynamical system;image sequence;hand tracking;switching linear dynamic system;data gloves;tracking motion analysis electronic design automation and methodology data gloves stochastic systems bayesian methods image analysis image motion analysis image sequence analysis image sequences;object detection computer vision bayes methods monte carlo methods principal component analysis image sequences image motion analysis data gloves;computer input output;iterative closest point algorithm;monte carlo methods;object detection;dynamic analysis;sequential monte carlo method articulated hand motion tracking eigen dynamics analysis stochastic linear dynamic system dynamic bayesian network image sequence principal component analysis switching linear dynamic system iterative closest point algorithm likelihood edge;image sequences	This paper introduces the concept of eigen-dynamics and proposes an eigen dynamics analysis (EDA) method to learn the dynamics of natural hand motion from labelled sets of motion captured with a data glove. The result is parameterized with a high-order stochastic linear dynamic system (LDS) consisting of five lower-order LDS. Each corresponding to one eigen-dynamics. Based on the EDA model, we construct a dynamic Bayesian network (DBN) to analyze the generative process of a image sequence of natural hand motion. Using the DBN, a hand tracking system is implemented. Experiments on both synthesized and real-world data demonstrate the robustness and effectiveness of these techniques.	approximation algorithm;asynchrony (computer programming);biconnected component;color histogram;computer vision;converge;dynamic bayesian network;dynamical system;eigen (c++ library);experiment;hidden surface determination;iccv;internet information services;iteration;nonlinear system;rough set;sampling (signal processing);tracking system;wired glove	Hanning Zhou;Thomas S. Huang	2003		10.1109/ICCV.2003.1238472	linear dynamical system;computer vision;motion capture;simulation;machine vision;computer science;machine learning;dynamic program analysis;dynamic bayesian network;monte carlo method;principal component analysis	Vision	45.791770408120996	-49.25694564594768	33818
48fb8e7cbd700bf4bdca42befb9795d64d8f0d1b	a visual tracking algorithm for real time people detection	experimental tests;probability;stochastic approach;real time;real time people detection;real time processing;layout;maximum likelihood estimation;blob overlapping visual tracking algorithm real time people detection stochastic approach maximum a posteriori probability;stochastic processes;visual tracking algorithm;feature extraction;image sequence;intelligent systems;robustness;image analysis;people tracking;humans;blob overlapping;tracking maximum likelihood estimation object detection probability stochastic processes;visual tracking;feature extraction cameras layout humans stochastic processes image sequences image analysis algorithm design and analysis robustness intelligent systems;maximum a posteriori probability;algorithm design and analysis;cameras;tracking;object detection;image sequences	In this paper we present a multi-people-tracking algorithm which is able to detect and track humans in complex situations with varying light conditions, high frame rate, and real time processing. We propose a stochastic approach for foreground people tracking based on the evaluation of the maximum a posteriori probability (MAP). The algorithm evaluates geometrical information on the blob overlapping and does not require the feature extraction to track the single object. Experimental tests have been carried out on soccer image sequence in which some players enter into the camera view and remain for some time.	algorithm;feature extraction;real-time computing	Tiziana D'Orazio;Marco Leo;Paolo Spagnolo;Pier Luigi Mazzeo;Nicola Mosca;Massimiliano Nitti	2007	Eighth International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS '07)	10.1109/WIAMIS.2007.14	layout;algorithm design;computer vision;image analysis;eye tracking;feature extraction;computer science;machine learning;pattern recognition;probability;tracking;maximum likelihood;statistics;robustness	Vision	44.25301170131352	-48.00413633719601	33857
07bbffb1d04d252a471c3a40653849b1c8200ede	an auxiliary variational method	boltzmann equation;modele graphe non oriente;learning;aproximacion campo medio;ecuacion boltzmann;mean field;mixture model;inferencia;graphical model;teoria mezcla;approximate inference;approximation champ moyen;variational method;methode variationnelle auxiliere;mean field approximation;mixture theory;theorie melange;lower bound;inference;equation boltzmann	Variational methods have proved popular and effective for inference and learning in intractable graphical models. An attractive feature of the approaches based on the Kullback-Leibler divergence is a rigorous lower bound on the normalization constants in undirected models. In the suggested work we explore the idea of using auxiliary variables to improve on the lower bound of standard mean field methods. Our approach forms a more powerful class of approximations than any structured mean field technique. Furthermore, the existing lower bounds of the variational mixture models could be seen as computationally expensive special cases of our method. A byproduct of our work is an efficient way to calculate a set of mixture coefficients for any set of tractable distributions that principally improves on a flat combination.	analysis of algorithms;approximation;calculus of variations;cobham's thesis;coefficient;generative grammar;graph (discrete mathematics);graphical model;kullback–leibler divergence;mixture model;numerical analysis;variational method (quantum mechanics);variational principle	Felix V. Agakov;David Barber	2004		10.1007/978-3-540-30499-9_86	mathematical optimization;mean field theory;machine learning;calculus;mathematics;statistics	ML	26.841452868971484	-29.953045795969665	33881
446ff496eb5217f7d9eaad6163a997e8913238ad	optimization of speeded-up robust feature algorithm for hardware implementation		Speeded-Up Robust Feature (SURF) is a widely-used robust local gradient feature detection and description algorithm. The algorithm itself can be implemented easily on general-purpose processors. However, the software implementation of SURF cannot achieve a performance high enough to meet the practical real-time requirements. And what is more, the huge data storage and the floating point operation of SURF algorithm make it hard and onerous to design and verify corresponding hardware implementation. This paper customized a SURF algorithm for hardware implementation, which combined several optimization methods in previous literature and three approaches (named Word Length Reduction (WLR), Low Bits Abandon(LBA), and Sampling Radius Reduction (SRR)). The computation operations of the simplified and optimized SURF (P-SURF) were reduced by 50% compared with the original SURF. At the same time, the Recall and Precision of the SURF feature descriptor are only dropped by 0.31 on average in the typical testing set, which are within an acceptable accuracy range. P-SURF has been implemented on hardware using TSMC 65 nm process, and the architecture of the whole system mainly contains four modules, including Integral Image Generator, IPoint Detector, IPoint Orientation Assigner, and IPoint Feature Vector Extractor. The chip size is 3.4 × 4 mm2. The power usage is less than 220mW according to the Synopsys Prime time while extracting IPoints in a video input of VGA (640 × 480) 172 fps operating at 200 MHz. The performance is better than the results reported in literature.	algorithm;central processing unit;coefficient;computation;computer data storage;computer vision;flops;feature detection (computer vision);feature detection (web development);feature extraction;feature vector;general-purpose macro processor;gradient;logical block addressing;mathematical optimization;precision and recall;randomness extractor;real-time clock;requirement;sas;simpl;sampling (signal processing);speeded up robust features;video graphics array;visual descriptor;wholesale line rental	Shanshan Cai;Leibo Liu;Shouyi Yin;Renyan Zhou;Weilong Zhang;Shaojun Wei	2013	Science China Information Sciences	10.1007/s11432-013-4946-y	computer vision;real-time computing;computer hardware;computer science;theoretical computer science;algorithm	Arch	41.03558758504941	-51.59211857290902	33904
02fa7ef6fabbbed55ac7476b8e14e166b4a037b5	free-form planar curve tracking using related points	kalman filters;curve fitting;image sequences;object tracking;kalman filtering technique;camera;curve fitting models;discrete steady-state kalman filter;field of view;fitting algorithms;free form object tracking;free-form planar curve tracking;image sequence	Tracking free form objects by fitting curve models to their boundaries in real-time is not feasible due to the computational burden of fitting algorithms. In this paper, we propose to do fitting only for certain frames in an image sequence and fill in the missing ones using Kalman filtering technique. An algorithm is presented to track a free-form shaped object, moving along an unknown trajectory, within the camera's field of view (FOV). A discrete steady-state Kalman filter is used to estimate the future positions and orientation of the target object. Kalman filter uses the “related points” extracted from the decomposition of implicit polynomials of target's boundary curves and measured position of target's centroid. Related points undergo the same motion with the curve, hence could be used to estimate the orientation of the target. The resulting algorithm is verified with simulations.	algorithm;computation;field of view in video games;kalman filter;polynomial;real-time computing;real-time locating system;simulation;steady state	Burak Yöndem;Mustafa Unel;Aytül Erçil	2005	2005 13th European Signal Processing Conference		computer vision;mathematical optimization;invariant extended kalman filter;fast kalman filter;control theory;mathematics;extended kalman filter;moving horizon estimation;alpha beta filter	Robotics	48.74564584894	-46.82571088613574	33924
a82010bd43c61a484d68615d1779572d059ee8db	representing a global map for a mobile robot with relational local maps from sensory data	relational local maps;robot sensing systems;sensor systems;mobile robot;edge detection;camera model;motion stereo global map representation edge detection robot vision computerised navigation computerised pattern recognition computer vision mobile robot relational local maps sensory data object centered coordinate system camera model;mobile robots robot kinematics layout sonar navigation robot sensing systems intelligent sensors roads control engineering sensor systems robot vision systems;mobile robots;control engineering;computerised pattern recognition;layout;reference point;computer vision;motion stereo;object centered coordinate system;robot vision;roads;mobile robots computer vision computerised navigation computerised pattern recognition;sensory data;sonar navigation;robot vision systems;global map representation;intelligent sensors;coordinate system;robot kinematics;computerised navigation	This paper proposes a method for representing a global map for a mobile robot by using the descriptions of local maps and their relation. Sensor maps viewed at different locations close to each other are transferred into a local map represented in the object centered coordinate system. First, the 3-D information of the edges on the floor is obtained at each sensor map (a view) by assuming the camera model and the flatness of the floor. A reliable feature is selected as a reference in the local map on which other edges are mapped. During the motion of the robot, the local map is updated by a motion stereo method until the current reference point disappears from a view. Farther edges should be represented in other local map when the robot approaches to them since they cannot been located as precisely as closer edges can. Finally, the relation between local maps is described in the global map. The method is tested in an indoor scene and the experimental results are shown.	image sensor;map;mobile robot	Minoru Asada;Yasuhito Fukui;Saburo Tsuji	1988		10.1109/ICPR.1988.28282	mobile robot;computer vision;simulation;computer science;quasi-open map;computer graphics (images)	Robotics	51.58167371081503	-36.73842731560927	33928
5b0d31f0dfed5d94c97eedfc32b7d7d8eea21d21	a local spectral method for graphs: with applications to improving graph partitions and exploring data graphs locally	optimal solution;personalized pagerank;information network;semi supervised learning;laplacian matrix;data analysis;graph partitioning;machine learning;seed set;region of interest;linear time;spectral method;scientific computing;spectral graph partitioning;local spectral algorithms;constrained optimization problem;empirical evaluation;data structure;eigenvectors	The second eigenvalue of the Laplacian matrix and its associated eigenvector are fundamental features of an undirected graph, and as such they have found widespread use in scientific computing, machine learning, and data analysis. In many applications, however, graphs that arise have several local regions of interest, and the second eigenvector will typically fail to provide information fine-tuned to each local region. In this paper, we introduce a locally-biased analogue of the second eigenvector, and we demonstrate its usefulness at highlighting local properties of data graphs in a semi-supervised manner. To do so, we first view the second eigenvector as the solution to a constrained optimization problem, and we incorporate the local information as an additional constraint; we then characterize the optimal solution to this new problem and show that it can be interpreted as a generalization of a Personalized PageRank vector; and finally, as a consequence, we show that the solution can be computed in nearly-linear time. In addition, we show that this locally-biased vector can be used to compute an approximation to the best partition near an input seed set in a manner analogous to the way in which the second eigenvector of the Laplacian can be used to obtain an approximation to the best partition in the entire input graph. Such a primitive is useful for identifying and refining clusters locally, as it allows us to focus on a local region of interest in a semi-supervised manner. Finally, we provide a detailed empirical evaluation of our method by showing how it can applied to finding locally-biased sparse cuts around an input vertex seed set in social and information networks.	approximation;computational science;constrained optimization;constraint (mathematics);graph (discrete mathematics);laplacian matrix;machine learning;mathematical optimization;optimization problem;pagerank;region of interest;semi-supervised learning;semiconductor industry;sparse matrix;spectral method;time complexity	Michael W. Mahoney;Lorenzo Orecchia;Nisheeth K. Vishnoi	2012	Journal of Machine Learning Research		time complexity;mathematical optimization;combinatorics;laplacian matrix;data structure;eigenvalues and eigenvectors;computer science;graph partition;machine learning;mathematics;data analysis;spectral method;region of interest	ML	28.96045133663618	-37.098311362266855	33983
a60146c458adfe9207f015d7a77cb7dfb54f744f	understanding dynamic social grouping behaviors of pedestrians	reliability;trajectory;image edge detection;heuristic algorithms;etin dynamic social grouping behavior pedestrian video social group recognition evolving tracklet interaction network;video signal processing image recognition pedestrians;target tracking;tracklet interaction network dynamic social grouping behavior pedestrian social groups;trajectory videos target tracking heuristic algorithms reliability image edge detection;videos	There have been many studies in the literature on social group recognition of crowds of pedestrians. However, most of these studies have approached the problem from a static point of view. A study on the dynamic property of social groups among people over time can provide significant insight into human behaviors and events. Inspired by sociological models of human collective behavior, in this work, we present a framework for characterizing hierarchical social groups based on evolving tracklet interaction network (ETIN) where the tracklets of pedestrians are represented as nodes and the their grouping behaviors are captured by the edges with associated weights. We use non-overlapping snapshots of the interaction network and develop the framework for a unified dynamic group identification and tracklet association. The approach is evaluated quantitatively and qualitatively on videos of pedestrian scenes where manually labeled ground-truth is given. The results of our approach are consistent to human-perceived dynamic social groups of the crowd. The performance analysis of our method shows that the approach is scalable and it provides situational awareness in a real-world scenarios.	degree (graph theory);degree of a polynomial;experiment;ground truth;interaction network;maximal set;multidimensional digital pre-distortion;scalability;sensor;social network;unified framework	Linan Feng;Bir Bhanu	2015	IEEE Journal of Selected Topics in Signal Processing	10.1109/JSTSP.2014.2365765	computer vision;simulation;trajectory;reliability;statistics	HCI	37.834830659634925	-45.78089827313465	34039
5e3183be84236000d362f67cfe851a7a1f9fe16b	sequential monte carlo for bayesian matching of objects with occlusions	reconnaissance visage;bayesian framework;contexto atestado;object recognition;vision ordenador;analisis escena;analyse scene;convergence;metodo monte carlo;occlusion;image matching;bayes methods;analisis forma;methode monte carlo;oclusion;statistical models in pattern recognition;reconnaissance objet;statistical model;computer vision;monte carlo methods bayesian methods shape predictive models pattern recognition deformable models graphical models sampling methods layout convergence;face recognition;cluttered environment;image representation;pattern matching;monte carlo method;modele statistique;pattern recognition;algorithms artificial intelligence bayes theorem image enhancement image interpretation computer assisted information storage and retrieval models biological models statistical monte carlo method pattern recognition automated subtraction technique;vision ordinateur;modelo estadistico;pattern analysis;concordance forme;reconnaissance forme;environnement encombre;reconocimiento patron;monte carlo simulation;monte carlo simulation object recognition statistical models in pattern recognition;monte carlo methods;convergence monte carlo methods bayes methods image matching image representation;appariement image;analyse forme;conditional prior model sequential monte carlo bayesian matching occlusion model fiducial features sequential matching particle representation batch algorithms convergence difficulties abnormal appearance unimodal target distribution;bayesian model;sequential monte carlo;scene analysis	We consider the problem of locating instances of a known object in a novel scene by matching the fiducial features of the object. The appearance of the features and the shape of the object are modeled separately and combined in a Bayesian framework. In this paper, we present a novel matching scheme based on sequential Monte Carlo, in which the features are matched sequentially, utilizing the information about the locations of previously matched features to constrain the task. The particle representation of hypotheses about the object position allow matching in multimodal and cluttered environments, where batch algorithms may have convergence difficulties. The proposed method requires no initialization or predetermined matching order, as the sequence can be started from any feature. We also utilize a Bayesian model to deal with features that are not detected due to occlusions or abnormal appearance. In our experiments, the proposed matching system shows promising results, with performance equal to batch approaches when the target distribution is unimodal, while surpassing traditional methods under multimodal conditions. Using the occlusion model, the object can be localized from only a few visible features, with the nonvisible parts predicted from the conditional prior model.	active appearance model;algorithm;bayesian network;computation;estimated;experiment;fiducial marker;gabor filter;gibbs sampling;handling (psychology);large;leucaena pulverulenta;mixture model;monte carlo method;multimodal interaction;obstruction;part dosing unit;particle filter;pattern matching;physical object;pixel;pose (computer vision);sampling (signal processing);sampling - surgical action;small	Toni Tamminen;Jouko Lampinen	2006	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2006.128	facial recognition system;computer vision;computer science;machine learning;pattern recognition;mathematics;statistics;monte carlo method	Vision	45.79728781995807	-50.53861757021931	34042
9829731d9799a2990973d367b17ea16bb6203d8d	joint alignment of multiple generalized point sets with anisotropic positional uncertainty based on expectation maximization		Alignment of multiple point sets is an essential problem in medical imaging and computer-assisted surgery. For example, aligning multiple point sets into one common coordinate frame is a prerequisite for statistical shape modelling (SSM). In this paper, we first formally formulate the multiple generalized point cloud registration problem in a probabilistic manner. Not only positional but also the orientational information is utilized in the registration. All the observed generalized point sets to be registered are considered to be realizations of underlyinng unknown hybrid mixture models (HMMs). By (i) utilizing more enriched information, i.e. orientational information or normal vectors (ii) treating all point sets equally, our registration algorithm is more robust to outliers and does not bias towards any point set. Assuming that the positional and orientational data are co-independent, the probability density function (PDF) of an observed hybrid point is the multiplication of Gaussian and Fisher distributions. Notably, the positional error vector is assumed to obey a multivariate Gaussian distribution to accommodate anisotropic noise. Expectation maxmization (EM) framework is utilized to jointly estimate the parameters. In the E-step, the posteriors between points and underlying mixture model components are computed. In the M-step, the constrained optimization problem of the rigid transformation matrix is re-formulated as an unconstrained one using the Rodrigues Formula of a rotation matrix. Extensive experiments are conducted on CT data of a femur bone model to compare the proposed algorithm with the state-of-the-art registration methods. The experimental results demonstrate the algorithm's better accuracy, robustness to noise and outliers, and faster convergence speed.	constrained optimization;constraint (mathematics);expectation–maximization algorithm;experiment;fisher information;mathematical optimization;medical imaging;mixture model;optimization problem;point cloud;point set registration;portable document format;transformation matrix	Zhe Min;Max Q.-H. Meng	2018	2018 International Conference on 3D Vision (3DV)	10.1109/3DV.2018.00029	mixture model;rotation matrix;point cloud;expectation–maximization algorithm;outlier;rodrigues' rotation formula;algorithm;gaussian;computer science;multivariate normal distribution	Vision	50.26546969507844	-51.58363013453365	34076
4f669e0c3e1144f14830eb6922c95d55310aaeb5	the dependent dirichlet process mixture of objects for detection-free tracking and object modeling		This paper explores how to find, track, and learn models of arbitrary objects in a video without a predefined method for object detection. We present a model that localizes objects via unsupervised tracking while learning a representation of each object, avoiding the need for pre-built detectors. Our model uses a dependent Dirichlet process mixture to capture the uncertainty in the number and appearance of objects and requires only spatial and color video data that can be efficiently extracted via frame differencing. We give two inference algorithms for use in both online and offline settings, and use them to perform accurate detection-free tracking on multiple real videos. We demonstrate our method in difficult detection scenarios involving occlusions and appearance shifts, on videos containing a large number of objects, and on a recent human-tracking benchmark where we show performance comparable to state of the art detector-based methods.	algorithm;autoregressive integrated moving average;benchmark (computing);general-purpose modeling;internationalization and localization;object detection;on the fly;online and offline;sensor;unsupervised learning	Willie Neiswanger;Frank D. Wood;Eric P. Xing	2014			computer vision;computer science;machine learning;video tracking;pattern recognition	Vision	44.525352775093005	-48.003130022629584	34122
d46d17bbccb07ddff53ec1add7ec56e48d861045	rough neural fault classification of power system signals	classifier fusion;wavelet analysis;standard deviation;high voltage direct current;rough membership;knowledge based fault recognition;rough neuron;classification;rough neural network;power system faults;time frequency representation;power system;feature extraction;classification system;membership function;classification accuracy;classify fusion;artificial neural network;neural network	This paper proposes an approach to classify faults that commonly occur in a High Voltage Direct Current (HVDC) power system. These faults are distributed throughout the entire HVDC system. The most recently published techniques for power system fault classification are the wavelet analysis, two-dimensional time-frequency representation for feature extraction and conventional artificial neural networks for fault type identification. The main limitation of these systems is that they are commonly designed to focus on a group of faults involved in a specific area of a power system. This paper introduces a framework for fault classification that covers a wider range of faults. The proposed fault classification framework has been initiated and developed in the context of the HVDC power system at Manitoba Hydro, which uses what is known as the Transcan system to record and archive fault events in files. Each fault file includes the most active signals (there are 23 of them) in the power system. Testing the proposed framework for fault classification is based on fault files collected and classified manually over a period of two years. The fault classification framework presented in this paper introduces the use of the rough membership function in the design of a neural fault classification system. A rough membership function makes it possible to distinguish similar feature values and measures the degree of overlap between a set of experimental values and a set of values representing a standard (e.g., set of values typically associated with a known fault). In addition to fault classification using rough neural networks, the proposed framework includes what is known as a linear mean and standard deviation classifier. The proposed framework also includes a classifier fusion technique as a means of increasing the fault classification accuracy.	archive;artificial neural network;feature extraction;linear classifier;rough set;time–frequency representation;transcranial direct-current stimulation;wavelet	Liting Han;James F. Peters	2008	Trans. Rough Sets	10.1007/978-3-540-85064-9_17	wavelet;fault coverage;membership function;feature extraction;biological classification;fault indicator;computer science;stuck-at fault;machine learning;pattern recognition;time–frequency representation;data mining;fault model;electric power system;standard deviation;artificial neural network	EDA	36.75033504613872	-31.522325348447406	34128
57412e2966a04c106657c926bcfdcb5c3842444d	camera and microphone array for 3d audiovisual face data collection	high resolution;microphone arrays cameras shape speech image reconstruction hardware calibration face recognition humans databases;data collection;dynamic facial expression video;multimedia application;indexing terms;3d shape reconstruction 3d audiovisual face data collection microphone array camera dynamic facial expression video synchronization calibration facial marker tracking;camera microphone array;facial marker tracking;face recognition;microphone arrays calibration cameras face recognition image reconstruction;microphone array;synchronization;image reconstruction;3d audiovisual face data collection;face modeling;3d shape reconstruction camera microphone array 3d face model;microphone arrays;facial expression;camera calibration;3d shape reconstruction;calibration;3d face model;cameras;camera	This paper proposes a novel camera/microphone array system capable of capturing dynamic facial expression video with synchronized speech and reconstructing realistic 3D face models from the data. Both hardware and software issues including camera calibration, video/audio synchronization, facial marker tracking and 3D shape reconstruction are considered. To our best knowledge, this system is the first camera/microphone array system that is able to capture high-resolution facial expression video with synchronized speech. The system can be used to collect dynamic 3D audiovisual face data for many multimedia applications.	camera resectioning;image resolution;microphone	Yuxiao Hu;Hao Tang;Thomas S. Huang	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518071	iterative reconstruction;facial recognition system;synchronization;computer vision;camera auto-calibration;calibration;camera resectioning;speech recognition;index term;image resolution;computer science;facial expression;statistics;data collection;computer graphics (images)	Visualization	47.861390580775485	-45.223297377224746	34148
ca00dafc19e9e8b279d2af6c7ed362ae18bf47ee	robust object tracking via sparsity-based collaborative model	target tracking histograms vectors robustness image reconstruction adaptation models collaboration;histograms;computer graphics;collaboration;image classification;vectors;image reconstruction;object tracking computer graphics image classification;object tracking;robustness;target tracking;adaptation models;drift problem sparsity based collaborative model robust object tracking algorithm drastic appearance change robust appearance model holistic templates local representations sparsity based discriminative classifier sparsity based generative model s dc module sgm module histogram based method occlusion handing scheme update scheme	In this paper we propose a robust object tracking algorithm using a collaborative model. As the main challenge for object tracking is to account for drastic appearance change, we propose a robust appearance model that exploits both holistic templates and local representations. We develop a sparsity-based discriminative classifier (SD-C) and a sparsity-based generative model (SGM). In the S-DC module, we introduce an effective method to compute the confidence value that assigns more weights to the foreground than the background. In the SGM module, we propose a novel histogram-based method that takes the spatial information of each patch into consideration with an occlusion handing scheme. Furthermore, the update scheme considers both the latest observations and the original template, thereby enabling the tracker to deal with appearance change effectively and alleviate the drift problem. Numerous experiments on various challenging videos demonstrate that the proposed tracker performs favorably against several state-of-the-art algorithms.	algorithm;discriminative model;effective method;experiment;generative model;holism;ibm notes;internet information services;lu decomposition;pattern recognition;second generation multiplex;sparse matrix;yang	Wei Zhong;Huchuan Lu;Ming-Hsuan Yang	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247882	iterative reconstruction;computer vision;contextual image classification;computer science;machine learning;video tracking;pattern recognition;histogram;computer graphics;robustness;collaboration	Vision	33.68736154852054	-47.70489505465854	34207
74156a11c2997517061df5629be78428e1f09cbd	preparatory coordination of head, eyes and hands: experimental study at intersections	radio frequency;feature extraction;predictive models;head;vehicles;iris;data models	Drivers use some combination of head, eye and hand movements to perform varying number of tasks from driving related to non-driving secondary tasks. Furthermore, the combinations may vary depending on the task performed. It is important to model and understand these variations in order to build predictive systems, explore driving styles, detect activities, etc. This study, therefore, introduces a framework to model the spatio-temporal movements of head, eyes and hands given naturalistic driving data of looking-in at the driver for any events or tasks performed of interest. As a use case, we explore the temporal coordination of the modalities on data of drivers executing maneuvers at stop-controlled intersections; the maneuvers executed are go straight, turn left and turn right. In sequentially increasing time windows, by training classifiers which have the ability to provide discriminative quality of its input variable, the experimental study at intersections shows which type of, when and how long distinguishable preparatory movements occur in the range of a few milliseconds to a few seconds.	device driver;experiment;microsoft windows	Sujitha Martin;Akshay Rangesh;Eshed Ohn-Bar;Mohan Manubhai Trivedi	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7900057	data modeling;computer vision;simulation;feature extraction;computer science;machine learning;predictive modelling;head;radio frequency	Robotics	37.527734657272866	-43.27668105724692	34210
9a7af2f96a77a922a04cfb472b5565816f03fd2f	end-to-end learning of keypoint detector and descriptor for pose invariant 3d matching		Finding correspondences between images or 3D scans is at the heart of many computer vision and image retrieval applications and is often enabled by matching local keypoint descriptors. Various learning approaches have been applied in the past to different stages of the matching pipeline, considering detection, description, or metric learning objectives. These objectives were typically addressed separately and most previous work has focused on image data. This paper proposes an end-to-end learning framework for keypoint detection and its representation (descriptor) for 3D depth maps or 3D scans, where the two can be jointly optimized towards task-specific objectives without a need for separate annotations. We employ a Siamese architecture augmented by a sampling layer and a novel score loss function which in turn affects the selection of region proposals. The positive and negative examples are obtained automatically by sampling corresponding region proposals based on their consistency with known 3D pose labels. Matching experiments with depth data on multiple benchmark datasets demonstrate the efficacy of the proposed approach, showing significant improvements over state-of-the-art methods.		Georgios Georgakis;Srikrishna Karanam;Ziyan Wu;Jan Ernst;Jana Kosecka	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00210	task analysis;computer vision;feature extraction;architecture;sampling (statistics);artificial intelligence;image retrieval;computer science;invariant (mathematics);pattern recognition;end-to-end principle;3-dimensional matching	Vision	30.809128091744657	-49.20347328190016	34261
99170bce7bfad44045b4889f9ecaf7a8671d9f8f	human motion analysis under actual sports game situations - sequential multi-decay motion history image matching		This paper proposes a sequential multi-decay motion history image matching with the aim of analyzing human motions captured in actual game situations without subjecting people to any intrusive measures. The motion history image (MHI) is a wellknown motion representation method, which can be used without foreground detection. In MHIs, pixels on which motion is detected have large pixel values. As time elapses following the latest motion detection, the values decrease according to a decay parameter. Two improvements were made to enable MHI-based template matching to be applied to motion analysis; introducing a template MHI sequence matching process that enables analysis of the temporal development of motions and extending MHIs to include multiple decay parameters. Due to the MHI sequence, a reference motion includes target motions of various speeds. Since the appropriate decay parameter varies with motion speed, no one predefined decay parameter can be the best one. These improvements enable our method to effectively analyze human motions in actual game situations. Experiments carried out indoors with capturing of 3D motion data and outdoors under real games situations verified the effectiveness of the proposed method.	computer performance;image registration;pixel;template matching	Dan Mikami;Toshitaka Kimura;Koji Kadota;Harumi Kawamura;Akira Kojima	2013			computer vision;simulation;multimedia	Vision	44.9056702737567	-45.52996298655016	34329
dd21d0da1bc642132793171dde47ac697078b561	learning single-view 3d reconstruction with limited pose supervision		It is expensive to label images with 3D structure or precise camera pose. Yet, this is precisely the kind of annotation required to train single-view 3D reconstruction models. In contrast, unlabeled images or images with just category labels are easy to acquire, but few current models can use this weak supervision. We present a unified framework that can combine both types of supervision: a small amount of camera pose annotations are used to enforce pose-invariance and view-point consistency, and unlabeled images combined with an adversarial loss are used to enforce the realism of rendered, generated models. We use this unified framework to measure the impact of each form of supervision in three paradigms: semi-supervised, multi-task, and transfer learning. We show that with a combination of these ideas, we can train single-view reconstruction models that improve up to 7 points in performance (AP) when using only 1% pose annotated training data.	3d reconstruction;cvpr;generative adversarial networks;nips;transformer;yang	Guandao Yang;Yin Cui;Serge J. Belongie;Bharath Hariharan	2018		10.1007/978-3-030-01267-0_6	machine learning;transfer of learning;artificial intelligence;computer vision;3d reconstruction;computer science;training set;annotation	Vision	27.263204952080528	-49.731267691647396	34383
b694a99b30923bdcf5a743e34517ca876a63d1b5	gait recognition using motion trajectory analysis		Gait recognition has received significant attention in the recent years due to its applications in numerous fields of computer vision, particularly in automated person identification in visual surveillance and monitoring systems. In this paper, we propose a novel algorithm for gait recognition using spatio-temporal motion characteristics of a person. The proposed algorithm consists of four steps. First, motion features are extracted from video sequence which are used to generate a codebook in the second step. In a third step, the local descriptors are encoded using Fisher vector encoding. Finally, the encoded features are classified using linear Support Vector Machine (SVM). The performance of the proposed algorithm is evaluated and compared with state-of-the-art on two widely used gait databases TUM GAID and CASIA-A. The recognition results demonstrate the effectiveness of the proposed algorithm.		Muhammad Hassan Khan;Frédéric Li;Muhammad Shahid Farid;Marcin Grzegorzek	2017		10.1007/978-3-319-59162-9_8	computer science;support vector machine;artificial intelligence;pattern recognition;computer vision;codebook;gait;trajectory	Vision	36.204098112479215	-50.532132491338935	34403
7152324b22e33e4f7e98b2b798900d9b86c0be73	an improved icp registration algorithm with a weight-bootstrap scheme	measurement;estimation;statistical analysis approximation theory image registration iterative methods optimisation;three dimensional displays;transforms;optimization;approximation methods;iterative closest point algorithm;improved icp registration algorithm tangent distance pair matching confidence iterative bootstrapped quadratic approximation method optimization process observation error point sets iterative closet point registration method weight bootstrap scheme;iterative closest point algorithm transforms approximation methods three dimensional displays optimization estimation measurement	In this paper, we propose a variant of the iterative closet point (ICP) registration method by introducing a novel weight-bootstrap scheme. The rigid transform between two point sets can be estimated, as long as proper correspondences are given. The accuracy of the estimated transform is mainly determined by the goodness of these matches. However, the confidence of the correspondences is weakened by the observation error. Aiming to address this challenge, the proposed solution parameterizes the pair matching confidence and improves the optimization process. Specifically, we adopt the tangent distance as error metric and introduce an iterative bootstrapped quadratic approximation method to increase the registration accuracy. Compared to the existing methods, experiments show that our method can produce a more accurate transform estimation in 2D case, and yield an acceptable result in 3D case.	algorithm;approximation;experiment;global optimization;image registration;iterative method;mathematical optimization;point set registration;thresholding (image processing)	Fei Guo;Yifeng He;Ling Guan	2015	2015 IEEE 17th International Workshop on Multimedia Signal Processing (MMSP)	10.1109/MMSP.2015.7340874	mathematical optimization;estimation;discrete mathematics;mathematics;iterative closest point;measurement;statistics	Vision	50.55765999815763	-51.30802380485129	34451
1539c1766511953fc86e4b8522502b8ff94bf5ad	computer graphic modeling and simulation of human musculoskeletal system for biomechanical research	musculoskeletal model;surgical simulation;modeling and simulation;musculoskeletal system;musculoskeletal;virtual human;biomechanics;bone fracture;computer graphic;human model;research and development;medical image;geometric model	In this paper, the human musculoskeletal model based on the medical images of the Korean standard male and the computer graphics based simulation technology with biomechanical analyses are presented. The virtual human model consists of three components. The model component includes the development of anatomically accurate geometric models with physiologically relevant material data. The analysis component includes various types of mechanical. In the simulation component, task-oriented graphic simulation would be performed for virtual evaluation, test, measurement, design, and planning. Three biomechanical analyses with graphic simulation using the virtual Korean model were performed; the surgical simulation of bone fracture surgery, the biomechanical evaluation of surgical implant in knee, and the graphic visualization and simulation of tennis play. In conclusion, the developed virtual Korean model of the musculoskeletal system would have lots of potentiality for biomechanical research and development in various fields, such as medical industry, automobile, ergonomics, or nuclear industry.	simulation	Yoon Hyuk Kim	2007		10.1007/978-3-540-73321-8_16	simulation;engineering;biological engineering;mechanical engineering	Robotics	39.64038994945312	-38.59428806419219	34468
707aad47dd5f74ea5236ca472d4c78cc08d225fa	automatic fall detection of human in video using combination of features	analytical models;software;legged locomotion;hazards;feature extraction;adaptation models;cameras	The problem of automatically fall detection of older people living alone is a popular research topic since falls are one of the major health hazards among the aging population aged 65 and above and the population of them in China is more than 100 million. In this paper, we present an automatic human fall detection framework based on video surveillance which can improve safety of elders in indoor environments. First, a vision component was used to detect and extract moving people in videos from static cameras. Then, we combine Histograms of Oriented Gradients(HOG),Local Binary Pattern(LBP)and feature extracted by the Deep Learning Framework Caffe to form a new augmented feature and the feature is named HLC. We use HLC to represent a person's motion state in a frame of a video sequence. Because the process of fall is a sequence of movements, we use HLC features which were extracted from continuous frames of a video sequence to implement the fall detection. With the help of the HLC feature, we achieve an average fall detection result of 93.7% sensitivity and 92.0% specificity on three different datasets.	closed-circuit television;deep learning;hazard (computer architecture);sensitivity and specificity	Kun Wang;Guitao Cao;Dan Meng;Weiting Chen;Wenming Cao	2016	2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2016.7822694	computer vision;simulation;speech recognition;feature extraction;hazard;computer science;machine learning	Vision	39.95755185574096	-45.29167104575965	34478
066000d44d6691d27202896691f08b27117918b9	vision-based analysis of small groups in pedestrian crowds	trajectory videos target tracking humans legged locomotion clustering algorithms;hierarchical clustering;cluster algorithm;image recognition;pattern clustering;bottom up hierarchical clustering;bottom up;pedestrian safety;automated crowd analysis;legged locomotion;poison control;real time situation awareness vision based analysis pedestrian crowd dynamics multiobject tracking sociological model human collective behavior small group detection bottom up hierarchical clustering generalized symmetric hausdortf distance pairwise proximity pairwise velocity real world pedestrian scene human coded ground truth substantial statistical agreement human perceived small group structure crowd structure automated crowd analysis pedestrian group;pedestrian detection and tracking;sociological model;injury prevention;real time situation awareness;real time;safety literature;generalized symmetric hausdortf distance;small group detection;crowd structure;traffic safety;statistical analysis image recognition object tracking pattern clustering pedestrians;injury control;substantial statistical agreement;home safety;collective behavior;real world pedestrian scene;injury research;pedestrian groups;pedestrians;safety abstracts;trajectory;human factors;statistical analysis;automatic detection;occupational safety;object tracking;pedestrian detection;safety;situation awareness;crowd dynamics;clustering algorithms;hausdorff distance;safety research;human coded ground truth;pedestrian crowd dynamics;accident prevention;violence prevention;ground truth;pedestrian group;humans;bicycle safety;crowd dynamics pedestrian detection and tracking pedestrian groups;target tracking;human collective behavior;vision based analysis;poisoning prevention;falls;multiobject tracking;ergonomics;human perceived small group structure;pairwise velocity;suicide prevention;videos;pairwise proximity	Building upon state-of-the-art algorithms for pedestrian detection and multi-object tracking, and inspired by sociological models of human collective behavior, we automatically detect small groups of individuals who are traveling together. These groups are discovered by bottom-up hierarchical clustering using a generalized, symmetric Hausdorff distance defined with respect to pairwise proximity and velocity. We validate our results quantitatively and qualitatively on videos of real-world pedestrian scenes. Where human-coded ground truth is available, we find substantial statistical agreement between our results and the human-perceived small group structure of the crowd. Results from our automated crowd analysis also reveal interesting patterns governing the shape of pedestrian groups. These discoveries complement current research in crowd dynamics, and may provide insights to improve evacuation planning and real-time situation awareness during public disturbances.	algorithm;awareness;bottom-up parsing;cluster analysis;complement system proteins;computer vision;crowd simulation;experiment;ground truth;hausdorff dimension;hierarchical clustering;inspiration function;numerous;pedestrian detection;rem sleep behavior disorder;real-time clock;real-time transcription;social sciences;social force model;statistical model;united states public health service;velocity (software development);xslt/muenchian grouping;algorithm;statistical cluster;travel;videocassette	Weina Ge;Robert T. Collins;Barry Ruback	2012	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2011.176	situation awareness;hausdorff distance;computer vision;simulation;ground truth;computer science;suicide prevention;human factors and ergonomics;trajectory;injury prevention;machine learning;collective behavior;video tracking;top-down and bottom-up design;hierarchical clustering;cluster analysis;computer security;statistics	Vision	38.102574076744695	-45.8052817329794	34501
fd66585cc95230c00fd9aa522670cfc49f4d4731	suggestions for a biologically inspired spiking retina using order-based coding	image processing systems biologically inspired spiking retina order based coding hardware vision systems spike based biological image processing address event representation image reconstruction;vision system;eye;image coding;image processing;image coding biomimetics eye;convolution;comunicacion de congreso;environmental conditions;biologically inspired spiking retina;address event representation;visualization;retina neurons image coding hardware machine vision image processing signal design signal generators fires image reconstruction;retina;image reconstruction;pixel;hardware vision systems;neurons;order based coding;spike based biological image processing;image processing systems;encoding;biomimetics;hardware	"""This paper discusses some new suggestions for designing hardware vision systems that take inspiration from spike-based biological image processing. The key idea is to modify already existing Address Event Representation (AER) designs so that there is a periodic reset signal that can be generated every time some predefined proportion of """"neurons"""" has emitted a spike. Each """"neuron"""" only emits at most one spike per processing cycle, but the most strongly activated neurons fire first, ensuring that information about the image is transmitted with maximum efficiency. Simulations demonstrate that this sort of design can allow image reconstruction under conditions where only a few percent of the units have emitted a spike. This means that the reset signal can be triggered at high rates allowing images to be processed at very high clock rates but in a way that could automatically adjust to variations in environmental conditions including scene contrast."""	clock rate;image processing;iterative reconstruction;neuron;retina display	Simon J. Thorpe;Adrien Brilhault;José Antonio Pérez-Carrasco	2010	Proceedings of 2010 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2010.5537898	iterative reconstruction;biomimetics;computer vision;visualization;image processing;computer science;theoretical computer science;machine learning;convolution;pixel;encoding	Arch	45.4205254931148	-32.802420195227484	34523
ec58dd7e7aee71a43037c7da9bfe733339f2eb52	bootstrapping descriptors for non-euclidean data		For data carrying a non-Euclidean geometric structure it is natural to perform statistics via geometric descriptors. Typical candidates are means, geodesics, or more generally, lower dimensional subspaces, which carry specific structure. Asymptotic theory for such descriptors is slowly unfolding and its application to statistical testing usually requires one more step: Assessing the distribution of such descriptors. To this end, one may use the bootstrap that has proven to be a very successful tool to extract inferential information from small samples. In this communication we review asymptotics for descriptors of manifold valued data and study a non-parametric bootstrap test that aims at a high power, also under the alternative.		Benjamin Eltzner;Stephan Huckemann	2017		10.1007/978-3-319-68445-1_2	manifold;asymptotic analysis;geodesic;linear subspace;applied mathematics;statistical hypothesis testing;bootstrapping;non-euclidean geometry;bootstrapping (electronics);mathematics	ML	35.14500681860811	-25.749144652512072	34534
033e38c62c76370bd79e3ce3811eb292a48018e9	affine 3-d reconstruction from two projective images of independently translating planes	three dimensional displays transmission line matrix methods layout cameras symmetric matrices calibration computer science geometry image reconstruction information resources;fundamental matrices 3 d reconstruction projective images independently translating planes multi body scene homograph matrices affine calibration;image reconstruction;3 dimensional;calibration image reconstruction;calibration	Consider two views of a multi-body scene consisting of k planar bodies moving in pure translation one relative to the other. We show that the fundamental matrices, one per body, live in a 3-dimensional subspace, which when represented as a step-3 extensor is the common transversal on the collection of extensors defined by the homography matricesH1; :::; Hk of the moving planes. We show that as much as five bodies are necessary for recovering the common transversal from the homography matrices, from which we show how to recover the fundamental matrices and the affine calibration between the two cameras.	fundamental matrix (computer vision);homography (computer vision)	Lior Wolf;Amnon Shashua	2001		10.1109/ICCV.2001.937630	iterative reconstruction;three-dimensional space;computer vision;calibration;topology;mathematics;geometry	Vision	53.726658635725734	-50.82638216896627	34547
4244fd47bd8d73e0a36aa374cf002699ea689a0d	large scale visual classification via learned dictionaries and sparse representation	image resolution;visual classification;image database;image classification;large scale;dictionary learning;k nearest neighbor;sparse representation	We address the large scale visual classification problem. The approach is based on sparse and redundant representations over trained dictionaries. The proposed algorithm firstly trains dictionaries using the images of every visual category, one category has one dictionary. In this paper, we choose the K-SVD algorithm to train the visual category dictionary. Given a set of training images from a category, the K-SVD algorithm seeks the dictionary that leads to the best representation for each image in this set, under strict sparsity constraints. For testing images, the traditional classification method under the large scale condition is the k-nearest-neighbor method. And in our method, the category result is through the reconstruction residual using different dictionaries. To get the most effective dictionaries, we explore the large scale image database from the Internet [2] and design experiments on a nearly 1.6 million tiny images on the middle semantic level defined based on Word-Net. We compare the image classification performance under different image resolutions and k-nearest-neighbor parameters. The experimental results demonstrate that the proposed algorithm outperforms k-nearest-neighbor in two aspects: 1) the discriminative capability for large scale visual classification task, and 2) the average running time of classifying one image.	dictionary;sparse approximation	Zhenyong Fu;Hongtao Lu;Nan Deng;Nengbin Cai	2010		10.1007/978-3-642-16530-6_38	contextual image classification;image resolution;k-svd;computer science;machine learning;pattern recognition;sparse approximation;data mining;k-nearest neighbors algorithm	Vision	28.49893063054216	-46.169634658448	34568
0de7c0674e44d0bc5e19bb8860c463a0fcd2645a	logical dp matching for detecting similar subsequence	dynamic program;sequential pattern	A logical dynamic programming (DP) matching algorithm is proposed for extracting similar subpatterns from two sequential patterns. In the proposed algorithm, local similarity between two patterns is measured by a logical function, called support. The DP matching with the support can extract all similar subpatterns simultaneously while compensating nonlinear fluctuation. The performance of the proposed algorithm was evaluated qualitatively and quantitatively via an experiment of extracting motion primitives, i.e., common subpatterns in gesture patterns of different classes.		Seiichi Uchida;Akihiro Mori;Ryo Kurazume;Rin-ichiro Taniguchi;Tsutomu Hasegawa	2007		10.1007/978-3-540-76386-4_59	discrete mathematics;computer science;pattern recognition;mathematics;algorithm	NLP	38.290463398977685	-49.75340049248456	34590
61ae6e3447ee2476e3d82b5c904030434b607313	machine condition classification using deterioration feature extraction and anomaly determination	daubechies wavelet;t test;mechanical engineering computing;engine condition;neural nets;data normalization;temperature sensors;machine condition monitoring;lvq neural network;mother wavelet;deterioration feature extraction;temperature sensor;failure state;pressure sensor;wavelet transforms;learning vector quantization neural network;condition classification;wavelet transform;engines;condition monitoring;machine condition classification;feature extraction;equipment status assessment;singular point elimination;failure mechanical;pattern classification;production equipment;anomaly determination;deviation value acquisition;pressure sensors;normal state;lvq classification system;learning artificial intelligence;vector quantisation;engine deterioration;prognostics;machine condition diagnostics;wavelet selection;artificial neural network;learning vector quantization;neural network	Condition classification has been widely used for assessing equipment status for machine condition monitoring and diagnostics. An engine was fitted with one temperature and two pressure sensors to study the machine conditions in prognostics with an added abnormal state, in addition to the conventional normal and failure states. This work enables a better classification capability in order to predict deterioration in the engine. Information related to three deterioration processes was collected, and preprocessed using singular point elimination, deviation value acquisition, and data normalization. Wavelet transforms were used to extract deterioration features with different mother wavelets. The mother wavelets were selected using tests to optimize the wavelet selection. The deterioration was related to the amount of anomaly, with the abnormal states defined to distinguish the functional from the failure states. A Learning Vector Quantization (LVQ) neural network was used to classify the machine conditions, including normal, abnormal, and failure states. The results showed that the deterioration features defined using the Daubechies wavelet (db8) most strongly correlated with the original signal, so that the classification accuracy based on the deterioration features was greatly improved. The LVQ classification system had good accuracy for machine condition classification, and was adaptable to various engine conditions.	anomaly detection;artificial neural network;daubechies wavelet;elegant degradation;feature extraction;feature vector;learning vector quantization;sensor;stationary wavelet transform	Dongxiang Jiang;Chao Liu	2011	IEEE Transactions on Reliability	10.1109/TR.2011.2104433	speech recognition;engineering;pressure sensor;machine learning;pattern recognition;artificial neural network;wavelet transform	ML	36.94558654405075	-31.187176172177654	34606
135bd77060725da6737aaccb57587531119de5ea	the boolean column and column-row matrix decompositions	cur decomposition;boolean decompositions;matrix decompositions;cx decomposition;tecnologia electronica telecomunicaciones;data mining;approximation;matrix decomposition;experimental evaluation;tecnologias;grupo a	Matrix decompositions are used for many data mining purposes. One of these purposes is to find a concise but interpretable representation of a given data matrix. Different decomposition formulations have been proposed for this task, many of which assume a certain property of the input data (e.g., nonnegativity) and aim at preserving that property in the decomposition. In this paper we propose new decomposition formulations for binary matrices, namely the Boolean CX and CUR decompositions. They are natural combinations of two previously presented decomposition formulations. We consider also two subproblems of these decompositions and present a rigorous theoretical study of the subproblems. We give algorithms for the decompositions and for the subproblems, and study their performance via extensive experimental evaluation. We show that even simple algorithms can give accurate and intuitive decompositions of real data, thus demonstrating the power and usefulness of the proposed decompositions.	algorithm;data mining;nikon cx format	Pauli Miettinen	2008	Data Mining and Knowledge Discovery	10.1007/s10618-008-0107-0	combinatorics;discrete mathematics;approximation;data mining;mathematics;matrix decomposition;algorithm	ML	28.752016178968184	-35.80534073147794	34625
88212348d3b64fce9eb50b11a389ca6997f7e1b9	improving acoustic vehicle classification by information fusion	journal;pattern classification;information fusion;bayesian decision fusion	We present an information fusion approach for ground vehicle classification based on the emitted acoustic signal. Many acoustic factors can contribute to the classification accuracy of working ground vehicles. Classification relying on a single feature set may lose some useful information if its underlying sound production model is not comprehensive. To improve classification accuracy, we consider an information fusion diagram, in which various aspects of an acoustic signature are taken into account and emphasized separately by two different feature extraction methods. The first set of features aims to represent internal sound production, and a number of harmonic components are extracted to characterize the factors related to the vehicle’s resonance. The second set of features is extracted based on a computationally effective discriminatory analysis, and a group of key frequency components are selected by mutual information, accounting for the sound production from the vehicle’s exterior parts. In correspondence with this structure, we further put forward a modified Bayesian fusion algorithm, which takes advantage of matching each specific feature set with its favored classifier. To assess the proposed approach, experiments are carried out based on a data set containing acoustic signals from different types of vehicles. Results indicate that the fusion approach can effectively increase classification accuracy compared to that achieved using each individual features set alone. The Bayesian-based decision level fusion is found to be improved than a feature level fusion approach.	acoustic cryptanalysis;algorithm;bayesian network;data acquisition;diagram;experiment;feature extraction;feature selection;mutual information;resonance;sensor;signal-to-noise ratio;software deployment;velocity (software development)	Baofeng Guo;Mark S. Nixon;Thyagaraju Damarla	2011	Pattern Analysis and Applications	10.1007/s10044-011-0202-5	speech recognition;pattern recognition;data mining	AI	37.383564910355354	-32.46589700181844	34768
6ed2b15d4d883e48046b82ff56d10424c81bcad3	video jitter analysis for automatic bootleg detection	image motion analysis;computer forensics;video signal processing;image classification;correlation methods;jitter trajectory correlation cameras tracking motion pictures feature extraction;correlation distribution video jitter analysis automatic bootleg detection recaptured video video forensics scene jitter classification planar surface imaging plane high frequency 2d motion field interframe motion trajectory local feature motion global feature motion 2 level wavelet decomposition normalised cross correlation matrix high frequency component feature trajectory tracking;wavelet transforms;wavelet transforms computer forensics correlation methods image classification image motion analysis jitter object detection object tracking video signal processing;object tracking;jitter;object detection	This paper presents a novel technique for the automatic detection of recaptured videos with applications to video forensics. The proposed technique uses scene jitter as a cue for classification: when recapturing planar surfaces approximately parallel to the imaging plane, any added motion due to jitter will result in approximately uniform high-frequency 2D motion fields. The inter-frame motion trajectories are retrieved with feature tracking techniques, while local and global feature motion are decoupled through a 2-level wavelet decomposition. A normalised cross-correlation matrix is then populated with the similarities between the high-frequency components of the tracked features' trajectories. The correlation distribution is then compared with trained models for classification. Experiments with original and recaptured standard datasets show the validity of the proposed technique.	bsd;cross-correlation;data compression;high- and low-level;image processing;motion estimation;population;sampling (signal processing);wavelet	Marco Visentini Scarzanella;Pier Luigi Dragotti	2012	2012 IEEE 14th International Workshop on Multimedia Signal Processing (MMSP)	10.1109/MMSP.2012.6343423	computer vision;contextual image classification;speech recognition;jitter;telecommunications;quarter-pixel motion;computer science;video tracking;motion estimation;computer forensics;wavelet transform;computer graphics (images)	Vision	48.08751943006795	-46.53986799045859	34827
550858b7f5efaca2ebed8f3969cb89017bdb739f	“wii using only ‘we’”: using background subtraction and human pose recognition to eliminate game controllers	active sensor;principal component analysis computer games human computer interaction object recognition pose estimation;wii;object recognition;appearance based algorithm wii background subtraction human pose recognition game controller elimination human computer interface video gaming systems human silhouettes extraction adaptive local principal component analysis;human computer interaction;adaptive local principal component analysis;human computer interfaces human pose recognition computer vision for game systems;human computer interfaces;game controller elimination;video game;computer vision;human pose recognition;computer vision for game systems;image reconstruction;principal component analysis;games;background subtraction;human silhouettes extraction;video gaming systems;humans;lighting;virtual environment;computer games;appearance based algorithm;games humans lighting principal component analysis real time systems image reconstruction cameras;cameras;human computer interface;real time systems;pose estimation	We propose a novel human-computer interface for video gaming systems. Unlike current systems that use active sensors (e.g. Kinect), in the proposed approach, the human-poses are captured by a single camera and used to control the course of the game. This eliminates the need for traditional hand-held controllers and allows for the player to be immersed into the virtual environment of the game. The success of the approach is achieve through a robust extraction of human silhouettes and by a simple but effective human-pose recognizer. The system employs an adaptive framework for background subtraction using Adaptive Local Principal Component Analysis (ALPCA), which is able to handle sudden as well as gradual changes in background. Next, the extracted human silhouettes are used to train a appearance-based algorithm that can recognize the human poses. The excellent results reported here justify the elimination of active sensors and game controllers for many traditional games.	algorithm;background subtraction;finite-state machine;game controller;human–computer interaction;kinect;mobile device;principal component analysis;sensor;traditional game;virtual reality;wii	Yuanqiang Dong;Daniel Conrad;Guilherme N. DeSouza	2011	2011 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2011.5980310	iterative reconstruction;games;computer vision;simulation;pose;background subtraction;computer science;virtual machine;cognitive neuroscience of visual object recognition;lighting;multimedia;principal component analysis	Robotics	46.35271637029086	-44.01956393454818	34863
50b651d8ec0286397a23232f6347ed8efaf3510d	mixtures of equispaced normal distributions and their use for testing symmetry with univariate data	bootstrap method;density estimation;expectation maximization algorithm;skewness	Given a random sample of observations, mixtures of normal densities are often used to estimate the unknown continuous distribution from which the data come. The use of this semi-parametric framework is proposed for testing symmetry about an unknown value. More precisely, it is shown how the null hypothesis of symmetry may be formulated in terms of a normal mixture model, with weights about the center of symmetry constrained to be equal one another. The resulting model is nested in a more general unconstrained one, with the same number of mixture components and free weights. Therefore, after having maximized the constrained and unconstrained log-likelihoods, by means of the Expectation-Maximization algorithm, symmetry is tested against skewness through a likelihood ratio statistic with p -value computed by using a parametric bootstrap method. The behavior of this mixture-based test is studied through a Monte Carlo simulation, where the proposed test is compared with the traditional one, based on the third standardized moment, and with the non-parametric triples test. An illustrative example is also given which is based on real data.		Silvia Bacci;Francesco Bartolucci	2014	Computational Statistics & Data Analysis	10.1016/j.csda.2013.01.015	econometrics;mathematical optimization;skewness;density estimation;expectation–maximization algorithm;mathematics;statistics	ML	30.780243160571608	-24.091005851570248	34867
d18f4ed20738fc283e9203989101d032b45ae304	bayesian algorithms for simultaneous structure from motion estimation of multiple independently moving objects	moving object;cluster algorithm;pattern clustering;analyse amas;decomposition valeur singuliere;deteccion blanco;estimation mouvement;image processing;bayes methods;methode bayes;estimacion movimiento;singular value decomposition;bayesian methods motion estimation image sequences clustering algorithms layout image segmentation image motion analysis digital cameras partitioning algorithms optical filters;procesamiento imagen;motion estimation;blanco movil;traitement image;detection cible;algorithme;algorithm;detection objet;cluster analysis;posterior distribution;sequential importance sampling;image sequence;echantillonnage importance;cible mobile;analisis cluster;secuencia imagen;decomposicion valor singular;real image sequence bayesian algorithm motion estimation multiple independently moving object simultaneous structure monocular image sequence sequential importance sampling technique empirical posterior distribution object motion feature separation parameter singular value decomposition clustering algorithm synthetic image sequence;importance sampling;algorithms artificial intelligence cluster analysis computer graphics computer simulation humans image enhancement image interpretation computer assisted imaging three dimensional information storage and retrieval models biological models statistical movement numerical analysis computer assisted pattern recognition automated reproducibility of results sensitivity and specificity signal processing computer assisted subtraction technique video recording walking;pattern clustering motion estimation bayes methods image sequences singular value decomposition;target detection;structure from motion;moving target;sequence image;object detection;image sequences;algoritmo	The problem of simultaneous structure from motion estimation for multiple independently moving objects from a monocular image sequence is addressed. Two Bayesian algorithms are presented for solving this problem using the sequential importance sampling (SIS) technique. The empirical posterior distribution of object motion and feature separation parameters is approximated by weighted samples. The first algorithm addresses the problem when only two moving objects are present. A singular value decomposition (SVD)-based sample clustering algorithm is shown to be capable of separating samples related to different objects. A pair of SIS procedures is used to track the posterior distribution of the motion parameters. In the second algorithm, a balancing step is added into the SIS procedure to preserve samples of low weights so that all objects have enough samples to propagate empirical motion distributions. By using the proposed algorithms, the relative motions of all the moving objects with respect to the camera can be simultaneously estimated. Both algorithms have been tested on synthetic and real- image sequences. Improved results have been achieved.	addresses (publication format);approximation algorithm;bayesian network;cluster analysis;equilibrium;importance sampling;mimo;motion estimation;pdgfb wt allele;particle filter;petrosal sinus sampling;physical object;sampling (signal processing);scanning probe microscopes (device);singular value decomposition;structure from motion;synthetic intelligence;weight;biologic segmentation;statistical cluster	Gang Qian;Rama Chellappa;Qinfen Zheng	2005	IEEE Transactions on Image Processing	10.1109/TIP.2004.837551	computer vision;structure from motion;image processing;importance sampling;computer science;machine learning;pattern recognition;motion estimation;mathematics;posterior probability;cluster analysis;singular value decomposition;statistics	Vision	46.646031217597475	-50.49143481019082	34870
cebacd8a70a1348b3406102db94175bbbc182251	iot edge device based key frame extraction for face in video recognition		Following the development of computing and communication technologies, the idea of Internet of Things (IoT) has been realized not only at research level but also at application level. Among various IoT-related application fields, biometrics applications, especially face recognition, are widely applied in video-based surveillance, access control, law enforcement and many other scenarios. In this paper, we introduce a Face in Video Recognition (FivR) framework which performs real-time key-frame extraction on IoT edge devices, then conduct face recognition using the extracted key-frames on the Cloud back-end. With our key-frame extraction engine, we are able to reduce the data volume hence dramatically relief the processing pressure of the cloud back-end. Our experimental results show with IoT edge device acceleration, it is possible to implement face in video recognition application without introducing the middle-ware or cloud-let layer, while still achieving real-time processing speed.	access control;automatic vectorization;biometrics;central processing unit;computation;edge device;facial recognition system;graphics processing unit;half-precision floating-point format;high- and low-level;ibm notes;internet of things;key frame;mathematical optimization;real-time clock;real-time transcription;warez	Xuan Qi;Chen Liu;Stephanie Schuckers	2018	2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)	10.1109/CCGRID.2018.00087	real-time computing;cloud computing;edge device;access control;facial recognition system;acceleration;key frame;law enforcement;biometrics;computer science	EDA	42.7397385016649	-37.0019455934964	34880
127423fe44cdf3d9ea5a17e88564f12c72107b15	unsupervised 3d object discovery and categorization for mobile robots		We present a method for mobile robots to learn the concept of o bjects and categorize them without supervision using 3D point clouds f rom a laser scanner as input. In particular, we address the challenges of categori zing objects discovered in different scans without knowing the number of categories. The un derlying object discovery algorithm finds objects per scan and gives them loc allyonsistent labels. To associate these object labels across all scans, we introd uceclass graph which encodes the relationship among local object class labels. O ur algorithm finds the mapping from local class labels to global category labels by inferring on this graph and uses this mapping to assign the final category label to the discovered objects. We demonstrate on real data our alogrithm’s ability to discove r and categorize objects without supervision.	algorithm;batch processing;categorization;class diagram;cluster analysis;inner class;microsoft outlook for mac;mobile robot;online and offline;online machine learning;outline of object recognition;point cloud;seamless3d;smoothing;supervised learning	Jiwon Shin;Rudolph Triebel;Roland Siegwart	2011		10.1007/978-3-319-29363-9_4	computer vision;machine learning;pattern recognition	ML	30.948697723562322	-48.259598563509556	34895
34ab397bc042833bf59ec68a6a9c300f6910b0e9	invariant image classification using triple-correlation-based neural networks	spectral domain;image recognition;reconocimiento imagen;triple correlation based neural networks;learning algorithm;spatial domain;neural networks;correlations;neural nets;additive noise;image classification;algorithme apprentissage;correlation methods;data mining;noise robustness;multi layer neural network;neural nets image recognition correlation methods spectral analysis;artificial neural networks;reseaux neuronaux;third order correlations;efficient implementation;2d gray scale images;clustering;image representation;feature extraction;reconnaissance image;image classification neural networks image recognition image representation feature extraction noise robustness additive noise artificial neural networks data mining multi layer neural network;binary images invariant image classification triple correlation based neural networks 2d gray scale images third order correlations clustering spatial domain spectral domain;binary images;simulation study;invariant image classification;correlation;spectral analysis;neural network	Triple-correlation-based neural networks are introduced and used in this paper for invariant classification of 2D gray scale images. Third-order correlations of an image are appropriately clustered, in spatial or spectral domain, to generate an equivalent image representation that is invariant with respect to translation, rotation, and dilation. An efficient implementation scheme is also proposed, which is robust to distortions, insensitive to additive noise, and classifies the original image using adequate neural network architectures applied directly to 2D image representations. Third-order neural networks are shown to be a specific category of triple-correlation-based networks, applied either to binary or gray-scale images. A simulation study is given, which illustrates the theoretical developments, using synthetic and real image data.	additive white gaussian noise;architecture as topic;artificial neural network;biological neural networks;computer vision;dilation (morphology);distortion;grayscale;location-based service;neural network simulation;norm (social);pathological dilatation;synthetic intelligence;triple correlation;utility functions on indivisible goods	Anastasios Delopoulos;Andreas Tirakis;Stefanos D. Kollias	1994	IEEE transactions on neural networks	10.1109/72.286911	computer vision;contextual image classification;binary image;feature extraction;computer science;machine learning;pattern recognition;mathematics;cluster analysis;correlation;artificial neural network	Vision	30.62828182110231	-45.45406269112087	34929
c078e563b6adec02472637b0f740836906b1117c	a novel sparse representation classification face recognition based on deep learning	image recognition;face recognition;machine learning;deep learning;feature extraction;classification algorithms;face;lighting	The existing face recognition under pose and illumination variations is a challenging problem. A novel sparse recognition face recognition algorithm based on deep learning is presented in this paper. The deep learning network extracted global and local information, the deep learning network adopted the supervised Convolution restricted Boltzmann machine. The features extracted could recover the face image and reduce intraidentity variances, while maintaining discriminativeness between identities. The algorithm obtained the feature by the deep network and realized fast sparse classification by smoothed l0 norm. Experimental results on FERET face database show that the proposed algorithm can improve recognition rate and recognition speed when dealing with various conditions such as pose variation.	algorithm;convolution;deep learning;feret (facial recognition technology);facial recognition system;pattern recognition;restricted boltzmann machine;smoothing;sparse approximation;sparse matrix	Jun-Ying Zeng;Yikui Zhai;Jun-Ying Gan	2015	2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)	10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.345	face;computer vision;feature;feature extraction;computer science;machine learning;pattern recognition;three-dimensional face recognition;lighting;deep learning;3d single-object recognition	Vision	28.553256526018696	-47.66427362963877	34932
472fb6117bac49c227f42796d30e86f636ab165d	a spherical robot-centered representation for urban navigation	vision based navigation;3d;urban environment;spherical image augmentation;saliency map;urban navigation;trajectory cameras robustness vehicles estimation pixel navigation;generic method;spherical saliency map;image based registration technique;mobile robots;robotics;urban environments;robust estimation;robust outlier rejection;computer vision;navigation;robot vision;trajectory;3d odometry;general methods;estimation;image representation;image registration;pixel;robot vision image registration image representation mobile robots;prelearned global spherical memory spherical robot centered representation urban navigation generic method vision based navigation urban environments spherical image augmentation spherical saliency map image based registration technique robust outlier rejection;spherical robot centered representation;robustness;prelearned global spherical memory;vehicles;2d;andrew comport s homepage;augmented reality;visual servoing;cameras;tracking;pose estimation	This paper describes a generic method for vision-based navigation in real urban environments. The proposed approach relies on a representation of the scene based on spherical images augmented with depth information and a spherical saliency map, both constructed in a learning phase. Saliency maps are built by analyzing useful information of points which best condition spherical projections constraints in the image. During navigation, an image-based registration technique combined with robust outlier rejection is used to precisely locate the vehicle. The main objective of this work is to improve computational time by better representing and selecting information from the reference sphere and current image without degrading matching. It will be shown that by using this pre-learned global spherical memory no error is accumulated along the trajectory and the vehicle can be precisely located without drift.	computation;image sensor;map;pixel;real-time transcription;rejection sampling;sampling (signal processing);simultaneous localization and mapping;time complexity	Maxime Meilland;Andrew I. Comport;Patrick Rives	2010	2010 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2010.5650380	mobile robot;computer vision;augmented reality;estimation;navigation;2d computer graphics;simulation;pose;computer science;image registration;trajectory;tracking;robotics;visual servoing;pixel;robustness;computer graphics (images)	Robotics	52.4806301440245	-39.729476571477186	35084
3cac9ea58db2288eb267bb65034dcece9e998be8	predicting human activities using spatio-temporal structure of interest points	hough voting;random forest;action prediction	Early recognition and prediction of human activities are of great importance in video surveillance, e.g., by recognizing a criminal activity at its beginning stage, it is possible to avoid unfortunate outcomes. We address early activity recognition by developing a Spatial-Temporal Implicit Shape Model (STISM), which characterizes the space-time structure of the sparse local features extracted from a video. The early recognition of human activities is accomplished by pattern matching through STISM. To enable efficient and robust matching, we propose a new random forest structure, called multi-class balanced random forest, which makes a good trade-off between the balance of the trees and the discriminative abilities. The prediction is done simultaneously for multiple classes, which saves both the memory and computational cost. The experiments show that our algorithm significantly outperforms the state of the arts for the human activity prediction problem.	activity recognition;algorithm;closed-circuit television;computation;computational complexity theory;experiment;implicit shape model;interest point detection;pattern matching;random forest;sparse matrix	Gang Yu;Junsong Yuan;Zicheng Liu	2012		10.1145/2393347.2396380	random forest;computer vision;simulation;computer science;artificial intelligence;machine learning;pattern recognition	Vision	32.77393343629083	-49.79445507384565	35087
3f0df1274e01a77bf5d62cb044e8e696dd156c3a	how to think about grasping systems - basis grasps and variation budgets		In unstructured environments, grasping systems should cope with a wide range of object and environment variations, across size, shape and pose, friction and mass, visual occlusions and shadows, robot control inaccuracy, and many other factors. This paper proposes a framework for analyzing the sources of variations in grasping tasks as a way to understand grasping system performance. The concomitant design approach starts with a collection of basis grasps, each a specific arrangement of the fingers on a specific object. Next, we use motion sequences, sensing, and passive mechanics to make these grasps robust to variations in objects, sensing, and control. We then analyze each grasp’s robustness to local variation to determine the basin of attraction, the range of variation it can tolerate while still achieving a good grasp. Finally, we treat this basin of attraction as a variation budget that can be distributed across subsystems to inform system tradeoffs between object variation, perception errors, and robot inaccuracies. The principle advantage is that within the context of specific grasps, the effects of local variations can be understood and quantified, and therefore compared across disparate approaches. 1 Grasping Systems & Variation Creating versatile grasping capabilities is a longstanding challenge in robotics. Although robots grasp effectively in structured factories, they need to be more versatile Leif P. Jentoft RightHand Robotics Inc., Allston, MA, USA. e-mail: leif@righthandrobotics.com Qian Wan Harvard School of Engineering and Applied Sciences, Cambridge, MA, USA. e-mail: qwan@seas.harvard.edu Robert D. Howe Harvard School of Engineering and Applied Sciences, Cambridge, MA, USA. e-mail: howe@seas.harvard.edu	email;local variable;robot control;robotics;robustness (computer science);smart environment	Leif P. Jentoft;Qian Wan;Robert D. Howe	2015		10.1007/978-3-319-51532-8_22	machine learning;robustness (computer science);perception;robot control;artificial intelligence;grasp;attraction;computer science;control theory	Robotics	48.20515185631645	-30.53441588419587	35091
4f3cbc40131fa817eb119e262452ccc415cd403a	a framework for augmented reality using non-central catadioptric cameras	texturized objects augmented reality noncentral catadioptric cameras noncentral catadioptric imaging devices virtual object perspective camera spherical mirror stanford bunny object;mirrors;non central catadioptric cameras;noncentral catadioptric cameras;perspective camera;spherical mirror;image texture;forward projection;virtual object;texturized objects;forward projection augmented reality non central catadioptric cameras;three dimensional displays;three dimensional displays cameras lighting mirrors calibration augmented reality;noncentral catadioptric imaging devices;lighting;image texture augmented reality;augmented reality;calibration;cameras;stanford bunny object	"""In this article we propose a framework for the application of augmented reality to non-central catadioptric imaging devices. Considering a virtual object in the world with known 3D coordinates, the goal is to project this object into the image of a non-central catadioptric camera. We propose a solution to this problem which allows us to project texturized objects to the image in realtime, up to 20 fps: projection of 3D segments to the image, occlusions, illumination and shading. To the best of our knowledge this is the first time that this problem is addressed (all state-of-the-art methods are derived for central camera systems). In our experiments, we used a non-central catadioptric camera formed with a perspective camera and a spherical mirror. To test the proposed approach, we define a cube with texturized faces where each of the main steps of the framework is evaluated. To conclude, we used the proposed framework to project to the image the Stanford """"bunny"""" object."""	augmented reality;cuda;central processing unit;computation;dbpedia;distortion;experiment;geforce 700 series;geforce 8 series;graphics processing unit;image plane;laptop;shading;stanford bunny;texture mapping;time complexity	Tiago Dias;Pedro Miraldo;Nuno Gonçalves	2015	2015 IEEE International Conference on Autonomous Robot Systems and Competitions	10.1109/ICARSC.2015.31	computer vision;geography;optics;computer graphics (images)	Vision	53.675012541938685	-45.859770621537265	35092
042244e0e85b6d19ab16a5db62f678360215933c	embedded system for fall detection using body-worn accelerometer and depth sensor	assistive technologies embedded system fall detection body worn accelerometer depth sensor accelerometric data depth maps real time processing motion data low cost pandaboard platform fall event person movement system performance camera views image sequences kinect sensors;feature extraction cameras accelerometers computational efficiency floors embedded systems real time systems;embedded systems;object detection accelerometers assisted living body sensor networks cameras embedded systems image motion analysis image sensors image sequences;feature extraction;fall detection embedded systems assistive technologies;computational efficiency;accelerometers;cameras;floors;real time systems	This paper presents an embedded system for fall detection using accelerometric data and depth maps. A real-time processing of motion data and depth maps is realized on a low-cost PandaBoard platform. In order to achieve detection of human falls with low computational cost the system performs a depth-based inferring about the fall event when person's movement is above some preset threshold. The performance of the system has been evaluated on our publicly available dataset consisting of synchronized depth maps and motion data. To investigate the detection accuracy in depth maps from different camera views the image sequences were simultaneously recorded by two Kinect sensors, where one of them was placed in the front of the scene, whereas the second one was located on the ceiling. The motion data were acquired by a body-worn accelerometer and transmitted wirelessly to the processing unit, responsible for both synchronization and recording or processing of the data.	algorithmic efficiency;computation;depth map;embedded system;kinect;pandaboard;range imaging;real-time clock;sensor	Michal Kepski;Bogdan Kwolek	2015	2015 IEEE 8th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)	10.1109/IDAACS.2015.7341404	embedded system;computer vision;real-time computing;feature extraction;computer science;machine learning;accelerometer	Robotics	44.91254530687034	-37.19205546106812	35140
3d3621522a1b99e192aae30f9f2acec09cff26f0	a visual-numeric approach to clustering and anomaly detection for trajectory data	trajectory clustering;tendency;surveillance;technology;scenes;anomaly detection;computer science software engineering;event detection;mit trajectory dataset;science technology;clusivat hierarchical clustering;space;computer science;tracking	This paper proposes a novel application of Visual Assessment of Tendency (VAT)-based hierarchical clustering algorithms (VAT, iVAT, and clusiVAT) for trajectory analysis. We introduce a new clustering based anomaly detection framework named iVAT+ and clusiVAT+ and use it for trajectory anomaly detection. This approach is based on partitioning the VAT-generated Minimum Spanning Tree based on an efficient thresholding scheme. The trajectories are classified as normal or anomalous based on the number of paths in the clusters. On synthetic datasets with fixed and variable numbers of clusters and anomalies, we achieve 98 % classification accuracy. Our two-stage clusiVAT method is applied to 26,039 trajectories of vehicles and pedestrians from a parking lot scene from the real life MIT trajectories dataset. The first stage clusters the trajectories ignoring directionality. The second stage divides the clusters obtained from the first stage by considering trajectory direction. We show that our novel two-stage clusiVAT approach can produce natural and informative trajectory clusters on this real life dataset while finding representative anomalies.	activity recognition;algorithm;anomaly detection;cluster analysis;course (navigation);hierarchical clustering;high- and low-level;high-level programming language;information;minimum spanning tree;real life;statistical model;synthetic data;synthetic intelligence;thresholding (image processing);tracking system	Dheeraj Kumar;James C. Bezdek;Sutharshan Rajasegarar;Christopher Leckie;Marimuthu Palaniswami	2015	The Visual Computer	10.1007/s00371-015-1192-x	computer vision;anomaly detection;computer science;machine learning;space;data mining;tracking;quantum mechanics;technology	ML	38.67946091771114	-46.3748735479518	35192
cb7dfaa79f7ebbf437db295f15e67c88ca53f763	probabilistic model checking of robots deployed in extreme environments		Robots are increasingly used to carry out critical missions in extreme environments that are hazardous for humans. This requires a high degree of operational autonomy under uncertain conditions, and poses new challenges for assuring the robot’s safety and reliability. In this paper, we develop a framework for probabilistic model checking on a layered Markov model to verify the safety and reliability requirements of such robots, both at pre-mission stage and during runtime. Two novel estimators based on conservative Bayesian inference and imprecise probability model with sets of priors are introduced to learn the unknown transition parameters from operational data. We demonstrate our approach using data from a real-world deployment of unmanned underwater vehicles in extreme environments.		Xingyu Zhao;Valentin Robu;David Flynn;Fateme Dinmohammadi;Michael Fisher;Matt Webster	2019	CoRR		artificial intelligence;software deployment;machine learning;robot;imprecise probability;prior probability;markov model;computer science;bayesian inference;statistical model	AI	49.12508550855037	-25.215952805202065	35195
8317e3f116ea79f0c7c9412b41efd9a12282eed6	an eye for an eye: a single camera gaze-replacement method	databases;video sequence;single camera gaze replacement method;teleconferencing;eye;eyes tracking;video cameras eye image sequences object detection teleconferencing;videoconference;training;video conference system;video conference;runtime;artificial neural networks;eyes;video cameras;pixel;cameras eyes videoconference teleconferencing head face detection object detection infrared detectors iris eyelids;ghosting artifact single camera gaze replacement method video conference system eyes tracking video sequence;face;head;face detection;iris;infrared detectors;cameras;object detection;eyelids;ghosting artifact;image sequences	The camera in video conference systems is typically positioned above, or below, the screen, causing the gaze of the users to appear misplaced. We propose an effective solution to this problem that is based on replacing the eyes of the user. This replacement, when done accurately, is enough to achieve a natural looking video. At an initialization stage the user is asked to look straight at the camera. We store these frames, then track the eyes accurately in the video sequence and replace the eyes, taking care of illumination and ghosting artifacts. We have tested the system on a large number of videos demonstrating the effectiveness of the proposed solution.	artifact (software development);care-of address;rollover (key);video clip	Lior Wolf;Ziv Freund;Shai Avidan	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2010.5540133	computer vision;computer science;multimedia;videoconferencing;artificial neural network;computer graphics (images)	Vision	46.51748695278973	-44.676879648878064	35212
c9c62fdd5909ca2c5e74bbaa4f64cc702c523f7e	an integrated design of a smartphone interface with multiple image processing methods to reduce shining light from a digital projector	face face detection skin image color analysis instruction sets user interfaces;skin;image color analysis;projection screen smartphone interface integrated design multiple image processing methods shining light reduction digital projector open system platforms speaker face tracking background subtraction technique skin color detection face detection function eyes detection method shoulder line detection method speed tracking black mask position;skin color detection interface smartphone projector mask face detection;face;face detection;user interfaces edge detection face recognition gaze tracking image colour analysis object detection object tracking optical projectors smart phones;user interfaces;instruction sets	In this paper we propose a smartphone to be used in open system platforms that combines with a projector to produce a black mask which when used together cover and track the speaker's face to reduce the strong light from the projector shining toward the speaker's eyes. This design uses the background subtraction to first eliminate the background of the captured image and then detect the skin color. In order to track the location of the speaker, this design also utilizes the face detection function to track the coordinates of the speaker's face. This design also uses the projection screen blocks to automatically calibrate the mask position based on the resulting distance calculation from the projector to the projection screen.		Ying-Wen Bai;Tine-Hsueh Lin	2015	2015 IEEE 5th International Conference on Consumer Electronics - Berlin (ICCE-Berlin)	10.1109/TCE.2016.7514669	face;computer vision;face detection;object-class detection;computer science;operating system;instruction set;multimedia;skin;user interface;computer graphics (images)	Visualization	46.41916454333604	-43.57560588515308	35257
68adba7757fb064f739582a663f35e3180e7a925	anomaly detection and localization: a novel two-phase framework based on trajectory-level characteristics		Detecting and locating anomalies defined as unusual and irregular behaviors are important for public security in surveillance videos. In this paper, we propose a novel feature called Point Trajectory-based Histogram of Optical Flow (PT-HOF) to better capture the fine-grained spatial and temporal information along the point trajectory in crowd scenes. By encoding the extracted features through an unsupervised autoencoder network, the high-level representation features are used to build a Gaussian Mixture Model for estimating the anomaly likelihood of each trajectory. Furthermore, the consistency motion object (CMO) is constructed by clustering similar point trajectories in a local region to analyze the spatial structure of trajectories, which can improve the accuracy of anomaly localization. Experiments on two benchmark datasets demonstrate the advantage of the proposed algorithm by comparing with state-of-the-art methods.		Kun Zhao;Bin Liu;Weihai Li;Nenghai Yu;Zhiqiang Liu	2018	2018 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)	10.1109/ICMEW.2018.8551517	mixture model;autoencoder;computer vision;anomaly detection;cluster analysis;artificial intelligence;computer science;pattern recognition;histogram;trajectory;optical flow	Vision	35.47268587869906	-49.618732834802984	35397
636aff53e4b58080244abb8adabf10854cd580e4	fuzzy shape recognition for robot repositioning	robot sensing systems;repositioning;electrical capacitance tomography;fuzzy set theory shape recognition repositioning autonomous navigation mobile robots fuzzy internal representation position control uncertainty handling;mobile robot;uncertainty;path planning;shape recognition;motion estimation;mobile robots;uncertainty handling;shape measurement;fuzzy internal representation;fuzzy set theory;electrical capacitance tomography shape measurement navigation electronic switching systems context uncertainty robot sensing systems motion estimation dead reckoning clustering algorithms;uncertainty handling mobile robots navigation path planning pattern recognition fuzzy set theory position control;navigation;position control;pattern recognition;clustering algorithms;electronic switching systems;autonomous navigation;dead reckoning;context	In this communication we address the problem of shape recognition in the context of autonomous navigation of mobile robots. We present an association procedure which is able to establish the correspondence between elements of a learned fuzzy internal representation of the environment and currently perceived objects. Using this procedure, a mobile robot is able to periodically decrease the uncertainty affecting its position.	autonomous robot;mobile robot	Stefan Rolfes;Maria-João Rendas	1998		10.1109/ICPR.1998.712054	mobile robot;computer vision;computer science;control theory;mobile robot navigation	Robotics	50.955723878269275	-37.308634502820084	35408
56972aa9f9d3cce7c77d402602bc8f3af94d57c9	team annieway's autonomous system	autonomous vehicle;laser scanner;state machine;software structure;autonomic system	This paper reports on AnnieWAY, an autonomous vehicle that is capable of driving through urban scenarios and that has successfully entered the finals of the DARPA Urban Challenge 2007 competition. After describing the main challenges imposed and the major hardware components, we outline the underlying software structure and focus on selected algorithms. A recent laser scanner plays the prominent role in the perception of the environment. It measures range and reflectivity for each pixel. While the former is used to provide 3D scene geometry, the latter allows robust lane marker detection. Mission and maneuver selection is conducted via a concurrent hierarchical state machine that specifically ascertains behavior in accordance with California traffic rules. We conclude with a report of the results achieved during the competition.	algorithm;autonomous robot;autonomous system (internet);computer;darpa grand challenge (2007);finite-state machine;pixel;reflection coefficient;synergy;uml state machine	Christoph Stiller;Sören Kammel;Benjamin Pitzer;Julius Ziegler;Moritz Werling;Tobias Gindele;Daniel Jagszent	2008		10.1007/978-3-540-78157-8_19	embedded system;computer vision;simulation;engineering	Robotics	53.22532961202127	-31.432289647968844	35416
074ed969f3abdfab78b16b9ecc2a68cc208ea455	a software demonstrator for cognitive image processing using the associative memory chip		This paper presents the design of a software demonstrator to be used in conjunction an embedded system for real-time pattern matching. The demonstrator was designed to verify the proper hardware operation and to calculate the various constants used, thus the operations on the underlying model are bit-accurate. The embedded hardware is based on systems that have been developed for use in the field of High Energy Physics (HEP) and, in particular, in the trigger system of the ATLAS Experiment. The algorithm which is implemented is based on the learning process of the human vision and acts as an edge detector. The demonstrator is using the Qt application framework and the underlying model is written in C++. This separation allows the application to be used as an image viewer or as a command line tool. The latter allows the fast and efficient use of the application for the parallel processing of multiple images, the generation of Pattern Banks and the calculation of the constants used in the hardware.	atlas;algorithm;application framework;autoassociative memory;c++;command-line interface;content-addressable memory;edge detection;embedded system;graphical user interface;heterogeneous element processor;image processing;image viewer;opencv;parallel computing;pattern matching;qt (software);real-time clock	S. Gkaitatzis;Calliope-Louisa Sotiropoulou;Pierluigi Luciano;Paola Giannetti;Kostas Kordas	2017	2017 6th International Conference on Modern Circuits and Systems Technologies (MOCAST)	10.1109/MOCAST.2017.7937613	embedded system;computer hardware;computer science;theoretical computer science	EDA	44.445810531060815	-33.047582227172796	35454
455b0126f3b2605d2c7aaac998a32212faad7488	constraint-based probabilistic modeling for statistical abduction	abduction;modelizacion;base de connaissances;generic model;variable independante;modelo generativo;loi conjointe;loi conditionnelle;aprendizaje probabilidades;logical programming;ley condicional;probabilistic approach;modele generatif;modelisation;grammaire cf;probabilistic model;constraint;abduccion;programmation logique;context free grammar;enfoque probabilista;approche probabiliste;gramatica independiente;estimacion parametro;ley conjunta;algorithme em;generative model;apprentissage probabilites;base conocimiento;variable independiente;algoritmo em;parameter estimation;estimation parametre;probabilistic logic;logique probabiliste;em algorithm;programacion logica;modeling;conditional distribution;discriminative model;probability learning;independent variable;joint distribution;knowledge base	We introduce a new framework for logic-based probabilistic modeling called constraint-based probabilistic modeling which defines CBPMs (constraint-based probabilistic models) , i.e. conditional joint distributions P(⋅∣KB) over independent propositional variables constrained by a knowledge base KB consisting of clauses. We first prove that generative models such as PCFGs and discriminative models such as CRFs have equivalent CBPMs as long as they are discrete. We then prove that CBPMs in infinite domains exist which give existentially closed logical consequences of KB probability one. Finally we derive an EM algorithm for the parameter learning of CBPMs and apply it to statistical abduction.	discriminative model;expectation–maximization algorithm;generative model;kilobyte;knowledge base	Taisuke Sato;Masakazu Ishihata;Katsumi Inoue	2010	Machine Learning	10.1007/s10994-010-5206-7	variables;conditional probability distribution;statistical model;knowledge base;systems modeling;expectation–maximization algorithm;computer science;artificial intelligence;machine learning;mathematics;probabilistic logic;constraint;joint probability distribution;context-free grammar;estimation theory;generative model;algorithm;discriminative model;statistics	AI	25.001698024826446	-27.0409900852802	35488
945ef646679b6c575d3bbef9c6fc0a9629ac1b62	learning a structured dictionary for video-based face recognition	dictionaries videos face face recognition image reconstruction optimization training;video signal processing approximation theory face recognition image reconstruction minimisation;reconstruction error minimization structured dictionary learning video based face recognition invariant structural information low rank approximation;training;face recognition;image reconstruction;dictionaries;face;optimization;videos	In this paper, we propose a structured dictionary learning framework for video-based face recognition. We discover the invariant structural information from different videos of each subject. Specifically, we employ dictionary learning and low-rank approximation to preserve the invariant structure of face images in videos. The learned dictionary is both discriminative and reconstructive. Thus, we not only minimize the reconstruction error of all the face images but also encourage a sub-dictionary to represent the corresponding subject from different videos. Moreover, by introducing the low-rank approximation, the proposed method is able to discover invariant structured information from different videos of the same subject. To this end, an efficient alternating algorithm is employed to learn our structured dictionary. Extensive experiments on three video-based face recognition databases show that our approach outperforms several state-of-the-art methods.	algorithm;authorization;benchmark (computing);database;dictionary;experiment;facial recognition system;low-rank approximation;machine learning;mathematical optimization	Hongyu Xu;Jingjing Zheng;Azadeh Alavi;Rama Chellappa	2016	2016 IEEE Winter Conference on Applications of Computer Vision (WACV)	10.1109/WACV.2016.7477689	iterative reconstruction;facial recognition system;face;computer vision;object-class detection;computer science;machine learning;pattern recognition;geometry	Vision	25.431025530545913	-44.137384207493284	35580
69998355746058ff492805c72e41fcec8032c123	appearance-based multiple fish tracking for collective motion analysis	trajectory;estimation;position measurement;parameter estimation;target tracking;cameras	We propose a visual tracking method for dense fish schools in which occlusions occur frequently. Although much progress has been made for tracking multiple objects in video images, it is challenging to track individuals in highly dense groups. For occluded fishes, estimation of their positions and directions is difficult. However, if we know the number of fishes in a local area, we can accurately estimate their states by matching all of the combinations of possible parameters on the basis of our appearance model. We apply the idea to track multiple fishes in a school. Experimental results show that multiple fishes are practically tracked with our method compared to a well-known tracking method, and the average difference is less than 4%b of the mean body length of the school.	algorithm;bl (logic);collective motion;matching (graph theory);video tracking	Kei Terayama;Koki Hongo;Hitoshi Habe;Masa-Aki Sakagami	2015	2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)	10.1109/ACPR.2015.7486526	computer vision;simulation;geography;communication	Vision	47.27978913669562	-46.45209221653502	35627
62655d4899419e4be38c3dd2b803caab056fcdd2	a hybrid sparsity and distance-based discrimination detector for hyperspectral images		Hyperspectral target detection is an approach which tries to locate targets in a hyperspectral image on the condition of given targets spectrum. Many classical target detectors are based on the linear mixing model (LMM) and sparsity model. The LMM has a poor performance in dealing with the spectral variability. Therefore, more studies focus on the sparsity-based detectors, most of which are based on residual reconstruction. Owing to the fact that the impure dictionary for the test pixel weakens the detection performance and the discrimination ability of residual function has direct influence on the detecting accuracy, the dictionary purity and discriminative residual function are two most important factors affecting the accuracy of sparsity-based target detectors. In order to obtain more purified dictionary and discriminative residual function, this paper proposes a novel sparsity-based detector named the hybrid sparsity and distance-based discrimination (HSDD) detector for target detection in hyperspectral imagery. The residual function is constrained by the discrimination information during the dictionary construction, which enhances the dictionary purification. Only background samples are used to construct the dictionary because it is easier to remove the target pixel than to select it on the condition that majority of pixels are the background pixels. Hence, a purification process is applied for background training samples in order to construct an effective competition between the residual term and discriminative term. Extensive experimental results with four hyperspectral data sets demonstrate that the proposed HSDD algorithm has a better performance than the state-of-the-art algorithms.	algorithm;data dictionary;experiment;heart rate variability;kullback–leibler divergence;pixel;pure function;purification of quantum state;purify;receiver operating characteristic;sensor;sparse matrix	Xiaoqiang Lu;Wuxia Zhang;Xuelong Li	2018	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2017.2767068	discriminative model;pixel;iterative reconstruction;mathematics;computer vision;residual;artificial intelligence;detector;object detection;hyperspectral imaging;sparse approximation	Vision	29.711281753193155	-43.807837978670605	35741
5227d87e478f4ec8149ef37814c36bdb9e91c3ca	a linear generalized hough transform and its parallel implementation	processing element;generalized hough transform;picture processing;voting object detection object recognition pattern matching parallel algorithms shape image edge detection;ais 4000 vision processor linear generalized hough transform parallel implementation linear numeric pattern partially occluded objects simd array processors;transforms;transforms parallel algorithms pattern recognition picture processing;pattern recognition;parallel implementation;parallel algorithms	This paper presents a Linear Generalized Hough Transform (LIGHT) in which a linea^ numeric pattem is used to replace the single peak in the original Generalized Hough Transform (GHT). The LIGHT is more capable of detecting partially occluded objects. Moreover. it is well-suited for parallelization, especially on SIMD array processors. Several sequential and parallel LIGHT algorithms have been developed. Preliminary results for parallel implementation were obtained from an SZMD one-dimensional m a y processor (the AIS-4OOO Vision Processor with 5 12 processing elements).	algorithm;central processing unit;generalised hough transform;parallel computing;simd;sensor	Zheng Li;B. G. Yao;F. Tong	1991		10.1109/CVPR.1991.139776	hough transform;computer vision;parallel computing;computer science;theoretical computer science;parallel algorithm	Vision	44.297072725617355	-35.569849640159454	35767
9e814f2ed94ca848ea8995f4d6a60050e0f5333a	scale-awareness of light field camera based visual odometry		We propose a novel direct visual odometry algorithm for micro-lens-array-based light field cameras. The algorithm calculates a detailed, semi-dense 3D point cloud of its environment. This is achieved by establishing probabilistic depth hypotheses based on stereo observations between the micro images of different recordings. Tracking is performed in a coarse-to-fine process, working directly on the recorded raw images. The tracking accounts for changing lighting conditions and utilizes a linear motion model to be more robust. A novel scale optimization framework is proposed. It estimates the scene scale, on the basis of keyframes, and optimizes the scale of the entire trajectory by filtering over multiple estimates. The method is tested based on a versatile dataset consisting of challenging indoor and outdoor sequences and is compared to state-of-the-art monocular and stereo approaches. The algorithm shows the ability to recover the absolute scale of the scene and significantly outperforms state-of-the-art monocular algorithms with respect to scale drifts.	algorithm;key frame;light field;mathematical optimization;pareto efficiency;point cloud;real life;semiconductor industry;simultaneous localization and mapping;visual odometry	Niclas Zeller;Franz Quint;Uwe Stilla	2018		10.1007/978-3-030-01237-3_44	point cloud;computer vision;computer science;linear motion;artificial intelligence;absolute scale;monocular;visual odometry;trajectory;light field;light-field camera	Vision	52.8112610355816	-45.66999711163868	35773
2148a906b30147621f98f08c0bd1ce100f9a7870	svd-based screening for the graphical lasso		The graphical lasso is the most popular approach to estimating the inverse covariance matrix of highdimension data. It iteratively estimates each row and column of the matrix in a round-robin style until convergence. However, the graphical lasso is infeasible due to its high computation cost for large size of datasets. This paper proposes Sting, a fast approach to the graphical lasso. In order to reduce the computation cost, it efficiently identifies blocks in the estimated matrix that have nonzero elements before entering the iterations by exploiting the singular value decomposition of data matrix. In addition, it selectively updates elements of the estimated matrix expected to have nonzero values. Theoretically, it guarantees to converge to the same result as the original algorithm of the graphical lasso. Experiments show that our approach is faster than existing approaches.	algorithm;artificial intelligence;coefficient;computation;converge;experiment;graphical user interface;iteration;lasso;round-robin scheduling;sting;singular value decomposition;the matrix	Yasuhiro Fujiwara;Naoki Marumo;Mathieu Blondel;Koh Takeuchi;Hideaki Kim;Tomoharu Iwata;Naonori Ueda	2017		10.24963/ijcai.2017/233	lasso (statistics);machine learning;artificial intelligence;computer science;singular value decomposition	ML	25.48101295257765	-35.06631905281198	35799
b9740f921f6e2be2e70657ed903d788934870049	distance sensor fusion for obstacle detection at night based on kinect sensors		This paper introduces a vehicle safety system for moving backwards during night time. The system consists of two RGB-D cameras supported by two computers connected via Gigabit Ethernet. The cameras are mounted on the back side of the vehicle at different altitudes and tilt angles allowing detection of objects behind the vehicle and defects of the road surface. The safety system triggers an alarm if an object appears in dangerous proximity to the car. The dangerous proximity is decided according to the mass of the vehicle and its speed. Obstacles are tracked to predict if ones may become dangerous in the near future. Experiments have shown that the system is able to detect obstacles and holes on the road pavement when there is not enough light for common cameras.	kinect;sensor	Alexander Filonenko;Danilo Cáceres Hernández;Andrey Vavilin;Taeho Kim;Kang-Hyun Jo	2015		10.1007/978-3-319-22186-1_13	computer vision;simulation	Robotics	45.52614492285319	-41.44171425684859	35842
56ed51f00b194f3695f791f641d252a24d373d49	mono-vision based moving object detection in complex traffic scenes		Vision-based dynamic objects motion segmentation can significantly help to understand the context around vehicles, and furthermore improve road traffic safety and autonomous navigation. Therefore, moving object detection in complex traffic scene becomes an inevitable issue for ADAS and autonomous vehicles. In this paper, we propose an approach that combines different multiple views geometry constraints to achieve moving objects detection using only a monocular camera. Self-assigned weights are estimated online moderating the contribution of each constraint. Such a combination enhances the detection performance in degenerated situations. According to the experimental results, the proposed approach provides accurate moving objects detections in dynamic traffic scenarios with large camera motions.	architecture design and assessment system;autonomous robot;coefficient;constraint logic programming;deep learning;object detection;optical flow;pixel;sensor	Vincent Frémont;Sergio Alberto Rodriguez Florez;Bihao Wang	2017	2017 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2017.7995857	vehicle dynamics;object detection;computer vision;object-class detection;road traffic safety;monocular;computer science;artificial intelligence	Robotics	42.20262314377752	-45.311588529933715	35845
8fbfe4cf21d2e28e29d5e0d99b58cdc99208dc85	simplified markov random fields for efficient semantic labeling of 3d point clouds	support vector machines;training;markov random fields;markov processes feature extraction;radio frequency;vectors;object category simplified markov random fields semantic labeling 3d point cloud classification simplified markov networks point wise classification random forest support vector machines robust neighborhood filtering geometric statistics feature extraction;feature extraction;robustness;markov processes;markov random fields radio frequency support vector machines robustness feature extraction training vectors	In this paper, we focus on 3D point cloud classification by assigning semantic labels to each point in the scene. We propose to use simplified Markov networks to model the contextual relations between points, where the node potentials are calculated from point-wise classification results using off-the-shelf classifiers, such as Random Forest and Support Vector Machines, and the edge potentials are set by physical distance between points. Our experimental results show that this approach yields comparable if not better results with improved speed compared with state-of-the-art methods. We also propose a novel robust neighborhood filtering method to exclude outliers in the neighborhood of points, in order to reduce noise in local geometric statistics when extracting features and also to reduce number of false edges when constructing Markov networks. We show that applying robust neighborhood filtering improves the results when classifying point clouds with more object categories.	interaction;markov chain;markov random field;point cloud;random forest;sensor;support vector machine	Yan Lu;Christopher Rasmussen	2012	2012 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2012.6386039	support vector machine;markov chain;maximum-entropy markov model;markov kernel;feature extraction;computer science;machine learning;pattern recognition;data mining;mathematics;markov algorithm;markov process;markov model;radio frequency;robustness;variable-order markov model	Robotics	43.50436430785802	-51.60060212481925	35872
c549604b6a4afd28895a812dab6e5bce684a476c	object predetection based on kernel parametric distribution fitting	image recognition;multimodal distribution fitting;rkhs space;rkhs space image object predetection kernel parametric distribution fitting multimodal distribution fitting pattern recognition;kernel parametric distribution fitting;image object predetection;object detection image recognition;pattern recognition;kernel object detection detection algorithms pattern recognition image analysis degradation algorithm design and analysis support vector machines road transportation pixel;object detection	Multimodal distribution fitting is an important task in pattern recognition. For instance, the predetection which is the preliminary stage that limits image areas to be processed in the detection stage amounts to the modeling of a multimodal distribution. Different techniques are available for such modeling. We propose a pros and cons analysis of multimodal distribution fitting techniques convenient for object predetection in images. This analysis leads us to propose efficient and accurate variants over the previously proposed techniques as shown by our experiments. These variants are based on parametric distribution fitting in the RKHS space induced by a positive definite kernel	algorithm;cholesky decomposition;curve fitting;experiment;incomplete cholesky factorization;kernel (operating system);multimodal interaction;pattern recognition;test set	Jean-Philippe Tarel;Sabri Boughorbel	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.883	computer vision;computer science;machine learning;pattern recognition;mathematics	Vision	31.460480305227126	-41.66693531865273	35875
2da1b52b294dbdc305926c0472eb0688589b4588	3d shape matching using collinearity constraint	shape computer science iterative closest point algorithm cameras robot vision systems robotics and automation iterative algorithms educational institutions robustness laser modes;image matching;statistical model;computer vision;statistical analysis;shape matching;range image;comparative study;statistical model 3d shape matching collinearity constraint iterative closest point;statistical analysis image matching computer vision	In this paper, a novel algorithm is proposed to carry out automatic 3D shape matching with 3D shapes represented as sets of points. After the possible matches between the 3D shapes have been determined by the tradition ICP criterion, the novel approach employs the collinearity constraint to eliminate false matches based on a statistical model. A comparative study based on real range images has shown that the proposed algorithm is accurate, robust and efficient for the automatic matching of overlapping 3D shapes.	algorithm;experiment;statistical model	Yonghuai Liu;Longzhuang Li;Baogang Wei	2004	IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004	10.1109/ROBOT.2004.1307402	active shape model;statistical model;computer vision;mathematical optimization;active appearance model;machine learning;comparative research;mathematics;statistics	Robotics	49.816932878310425	-50.818082775136006	35908
4399737d08b1fd818f9dee2992a8c0fb57e72c4d	sensor fusion and surrounding environment mapping for a mobile robot using a mixed extended kalman filter	nonlinear filters;kalman filters;mobile robots;robot sensing systems kalman filters silicon mobile robots polynomials;navigation;slam robots kalman filters mobile robots navigation nonlinear filters sensor fusion;sensor fusion;slam robots;robot navigation sensor fusion surrounding environment mapping mixed extended kalman filter mobile robot localization unknown environment ekf on board sensor out of board sensor sensor defect polynomial model online algorithm environment map building fusing algorithm	In this work the localization of a mobile robot in an unknown environment is faced. A new version of the Extended Kalman Filter (EKF) is presented. The proposed EKF uses both measurements provided by robot on board and out of board sensors in order to emphasize the qualities and overcome the defects of such sensors. Moreover assuming a polynomial model for the robot surrounding environment bounds, an online algorithm able to build a map of this environment is presented. The proposed algorithms are tested in a numerical way contrasting them with a classical Extended Kalman Filter based only on the out of board sensors and with a fusing algorithm related only on the on board sensors.	channel (communications);experiment;extended kalman filter;mobile robot;numerical analysis;online algorithm;online and offline;polynomial;reflection mapping;robotic mapping;sensor;workspace	Luigi D'Alfonso;Antonio Grano;Pietro Muraca;Paolo Pugliese	2013	2013 10th IEEE International Conference on Control and Automation (ICCA)	10.1109/ICCA.2013.6565004	kalman filter;control engineering;mobile robot;computer vision;navigation;computer science;engineering;control theory;sensor fusion;moving horizon estimation;mobile robot navigation;simultaneous localization and mapping	Robotics	53.30506930363346	-34.2079039623927	35950
64251ddc77d81f1b3a2548c1c6b9e8d3b5ece02c	generative-model-based tracking by cluster analysis of image differences	moving object;generic model;image differencing;cluster analysis;generative model;cluster model;em algorithm;tracking	The EM algorithm is used to track moving objects as clusters of pixels significantly different from the corresponding pixels in a reference image. The underlying cluster model is Gaussian in image space, but not in grey-level difference distribution. The generative model is used to derive criteria for the elimination and merging of clusters, while simple heuristics are used for the initialisation and splitting of clusters. The system is competitive with other tracking algorithms based on image differencing.	cluster analysis;generative model	Arthur E. C. Pece	2002	Robotics and Autonomous Systems	10.1016/S0921-8890(02)00203-8	computer vision;expectation–maximization algorithm;computer science;machine learning;pattern recognition;tracking;cluster analysis;generative model	Robotics	46.338397163608796	-50.7731258515803	35975
24539a503e4f1686aa0daa775e9b50cc97953888	expectation propagation learning of finite beta-liouville mixtures for spatio-temporal object recognition	human robot interaction;planning;nlp	In this paper, we develop an efficient approach for the learning of finite Beta-Liouville mixture models. Unlike existing approaches, our is based on expectation propagation for parameters estimation and can select automatically the appropriate number of mixture components. We provide a coherent and unified learning framework to learn the complexity of the deployed mixture models and all the involved model parameters. We illustrate the performance of our learning algorithm with artificial data and a real application namely spatio-temporal objects (or dynamic events) recognition which has significant potential to be used in interactive systems or robotics. In particular, we highlight three of the most common spatio-temporal objects which involving facial expression, human activities and hand gesture. Our experiments results show the merits of the proposed approach.	algorithm;coherence (physics);estimation theory;expectation propagation;experiment;mixture model;outline of object recognition;robotics;software propagation	Wentao Fan;Nizar Bouguila	2013		10.1145/2493525.2493531	computer vision;computer science;artificial intelligence;machine learning	AI	36.742153493698694	-41.62254074271423	36018
0b2c68e26d4457725067c591423e525ad0cfd100	using calibrated camera for euclidean path modeling	unsupervised learning;video surveillance;image registration machine vision camera calibration euclidean path modeling;indexing terms;image sensors;video surveillance computer vision image registration image sensors image sequences monitoring object detection unsupervised learning;computer vision;monitoring;machine vision;cameras layout testing object detection prototypes computer vision legged locomotion monitoring video surveillance satellites;image registration;computer vision system calibrated camera euclidean path modeling activity monitoring multicamera video surveillance system unusual object behavior detection unsupervised training phase input sequence registration satellite imagery;satellite imagery;camera calibration;euclidean path modeling;object detection;image sequences	In this paper, we address the issue of Euclidean path modeling in a single camera for activity monitoring in a multi-camera video surveillance system. The paper proposes to use calibrated cameras to detect unusual object behavior. During the unsupervised training phase, after metric rectifying the input trajectories, the input sequences are registered to the satellite imagery and prototype path models are constructed. During the testing phase, using our simple yet efficient similarity measures, we seek a relation between the input trajectories derived from a sequence and the prototype path models. Real-world pedestrian sequences are used to demonstrate the practicality of the proposed method.	aerial photography;bird's-eye view;closed-circuit television;image rectification;prototype;rectifier;unsupervised learning;velocity (software development)	Imran N. Junejo;Hassan Foroosh	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379282	unsupervised learning;computer vision;camera resectioning;simulation;index term;machine vision;computer science;image registration;image sensor	Robotics	50.074575790931256	-40.76411312046521	36091
ce2ce671970f2be4043ef9266941a49a0400b41c	human action recognition based on skeleton splitting	skeleton;human action recognition;diffusion tensor fields	Human action recognition, defined as the understanding of the human basic actions from video streams, has a long history in the area of computer vision and pattern recognition because it can be used for various applications. We propose a novel human action recognition methodology by extracting the human skeletal features and separating them into several human body parts such as face, torso, and limbs to efficiently visualize and analyze the motion of human body parts. Our proposed human action recognition system consists of two steps: (i) automatic skeletal feature extraction and splitting by measuring the similarity between neighbor pixels in the space of diffusion tensor fields, and (ii) human action recognition by using multiple kernel based Support Vector Machine. Experimental results on a set of test database show that our proposed method is very efficient and effective to recognize the actions using few parameters. 2013 Elsevier Ltd. All rights reserved.	computer vision;feature extraction;pattern recognition;pixel;streaming media;support vector machine	Sang Min Yoon;Arjan Kuijper	2013	Expert Syst. Appl.	10.1016/j.eswa.2013.06.024	computer vision;feature;computer science;artificial intelligence;skeleton	Vision	37.68982612270122	-49.09729861215782	36106
5cd35d673e8b0b512e2b998d692fc3c030214f0f	led: localization-quality estimation embedded detector		Classification subnetwork and box regression subnetwork are essential components in deep networks for object detection. However, we observe a contradiction that before NMS, some better localized detections do not correspond to higher classification confidences, and vice versa. This contradiction exists because classification confidences can not fully reflect the localization-quality (loc-quality) of each detection. In this work, we propose the Localization-quality Estimation embedded Detector abbreviated as LED, and a corresponding detection pipeline. In this detection pipeline, we first propose an accurate loc-quality estimation method for each detection, then combine the loc-quality with the corresponding classification confidence during inference to make each detection more reasonable and accurate. For efficiency, LED is designed as an one-stage network. Extensive experiments are conducted on Pascal VOC 2007 and KITTI car detection datasets to demonstrate the effectiveness of LED.		Shiquan Zhang;Xu Zhao;Liangji Fang;Haiping Fei;Haitao Song	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451206	object detection;feature extraction;artificial intelligence;detector;versa;pattern recognition;inference;subnetwork;computer science	Vision	30.536878016209865	-50.66783177522413	36122
0bab4bc0bf27be86a5a769198517639dbe4053ac	real-time 3d reconstruction using a combination of point-based and volumetric fusion		Real-time 3D reconstruction using low-cost commodity sensors like Kinect or Xtion has been successfully applied in a wide range of fields like augmented reality, robotic teleoperation, and medical diagnosis. Due to the assumption of static scene, popular 3D reconstruction technologies such as KinectFusion and KinFu, find truthful reconstruction with fast motion camera or segmenting a moving object to be a challenge. In this paper, we propose a weighted iterative closest point (ICP) algorithm that uses both depth and RGB information to enhance the stability of camera tracking. Additionally, a GPU-based region growing method that combines depth, normal and intensity level as similarity criteria, is also applied to segment foreground moving objects accurately. For real-time processing and GPU memory efficiency, we also design a combination of point-based and volumetric representation to reconstruct moving objects and static scene, respectively. Both qualitative and quantitative results show that our proposed method improves real-time 3D reconstruction on the performance of camera tracking and segmentation of moving objects with reduced computational complexity.		Zhengyu Xia;Joohee Kim;Young Soo Park	2018	2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2018.8594061	3d reconstruction;artificial intelligence;computer vision;computer science;rgb color model;pose;augmented reality;iterative closest point;teleoperation;region growing;segmentation	Robotics	53.62298319149737	-46.28496811458137	36127
1020636ac1af75f2c9668f5d1990a3978bc6e94e	marginal marginalised particle filter	gaussian processes;kalman filters;monte carlo methods;particle filtering (numerical methods);gaussian sum description;kalman filtering;monte carlo simulation;marginal marginalised particle filter;probability densities;state trajectories	This paper deals with filters that combine the analytical Kalman filtering and the Monte Carlo simulation based particle filtering. Since the particles are related to the state trajectories from the initial time up to the current time rather than to the state at the last time only, these filters cannot be directly used in fusion of probability densities of the last state. Therefore, marginalisation of the outdated parts of the state trajectories is proposed in the paper. In order to obtain a reproducible probability density, at least theoretically, the Gaussian sum description of random variables is also newly considered in the problem formulation.	kalman filter;marginal model;monte carlo method;particle filter;simulation	Jirí Ajgl;Miroslav Simandl	2013	2013 American Control Conference		kalman filter;monte carlo localization;econometrics;mathematical optimization;ensemble kalman filter;hybrid monte carlo;particle filter;markov chain monte carlo;fast kalman filter;gaussian process;mathematics;extended kalman filter;statistics;monte carlo method	Robotics	38.36635173183681	-26.037010684500743	36174
01aeae41bb2d49740826b06bc8668372c3fe778d	sparse gaussian process regression via l1 penalization	large dataset;gaussian process regression;prediction accuracy;matrix approximation;gaussian process;approaches to learning	To handle massive data, a variety of sparse Gaussian Process (GP) methods have been proposed to reduce the computational cost. Many of them essentially map the large dataset into a small set of basis points. A common approach to learn these basis points is evidence maximization. Nevertheless, evidence maximization may lead to overfitting and cause a high computational cost. In this paper, we propose a novel sparse GP regression approach, GPLasso, that explicitly represents the trade-off between its approximation quality and the model sparsity. GPLasso minimizes a `1-penalized KL divergence between the exact and sparse GP posterior processes. Optimizing this convex cost function leads to sparse GP parameters. Furthermore, we use incomplete Cholesky factorization to obtain low-rank matrix approximations to speed up the optimization procedure. Experimental results on synthetic and real data demonstrate that, compared with several state-of-the-art sparse GP methods and a direct low-rank matrix approximation method, GPLasso achieves a significantly improved trade-off between prediction accuracy and computational cost.	algorithmic efficiency;approximation;basis function;cholesky decomposition;computation;expectation–maximization algorithm;gaussian process;incomplete cholesky factorization;kriging;kullback–leibler divergence;loss function;mathematical optimization;optimizing compiler;overfitting;penalty method;singular value decomposition;sparse matrix;synthetic intelligence	Feng Yan;Yuan Qi	2010			mathematical optimization;machine learning;sparse approximation;gaussian process;mathematics;kriging;statistics	ML	26.439230501007255	-33.98204119176465	36198
d895e9194715e2c1dc7db1c95d1f9814bd35d6e6	3d face and motion estimation from sparse points using adaptive bracketed minimization	software;model reconstruction;media technology;powell s multidimensional minimization;hardware and architecture;camera pose;computer networks and communications;gradient descent;article	This paper presents a novel method for estimating camera motion and reconstructing human face from a video sequence. The coarse-to-fine method is applied via combining the concepts of Powell’s minimization with gradient descent. Sparse points defining the human face in every frame are tracked using the active appearance model. The case of occluded points, even for self-occlusion, does not pose a problem in the proposed method. Robustness in the presence of noise and 3D accuracy using this method is also demonstrated. Examples of face reconstruction using other methods including trifocal tensor, Powell’s minimization, and gradient descent are also compared to the proposed method. Experiments on both synthetic and real faces are presented and analyzed. Also, different camera movement paths are illustrated. All real-world experiments used an off-the-shelf digital camera carried by a human walking without using any dolly to demonstrate the robustness and practicality of the proposed method.	active appearance model;conjugate gradient method;deep down;digital camera;experiment;gradient descent;hidden surface determination;image noise;maxima and minima;motion estimation;powell's method;sparse matrix;synthetic intelligence;time complexity;transformation matrix;trifocal tensor	Varin Chouvatut;Suthep Madarasmi;Mihran Tuceryan	2011	Multimedia Tools and Applications	10.1007/s11042-011-0925-8	gradient descent;computer vision;simulation;computer science;machine learning;computer graphics (images)	Vision	52.381727754106095	-48.50696635367554	36268
b3c7e7fdfb29364aaf50f684648370734c60f619	structural similarity-based object tracking in video sequences	histograms;colour histogram;video sequence;filtering;edge histogram structural similarity measurement object tracking video sequence bhattacharyya distance likelihood function particle filter colour histogram;bhattacharyya distance;image segmentation;video signal processing;radar tracking;particle measurements;video sequences;distortion measurement;qa75 electronic computers computer science;edge histogram;video sequences radar tracking histograms particle measurements particle tracking robustness particle filters target tracking filtering distortion measurement;particle filter;object tracking;particle filtering;structural similarity measurement;robustness;particle filtering similarity measure object tracking video sequences;particle tracking;particle filters;target tracking;video signal processing image segmentation object detection particle filtering numerical methods;similarity measure;likelihood function;structural similarity;object detection;particle filtering numerical methods	This paper addresses the problem of object tracking in video sequences. The use of a structural similarity measure for tracking is proposed. The measure reflects the distance between two images by comparing their structural and spatial characteristics and has shown to be robust to illumination and contrast changes. As a result it guarantees robustness of the tracking process under changes in the environment. The previously used Bhattacharyya distance is not robust to such changes. Additionally, when a tracker is run with the Bhattacharyya distance, histograms should be calculated in order to find the likelihood function of the measurements. With the new function there is no need to calculate histograms. A particle filter (PF) is implemented where this measure is used for computing the distance between the reference and current frame. The algorithm performance has been tested and evaluated over real-world video sequences, and has been shown to outperform methods based on colour and edge histograms	algorithm;microsoft edge;particle filter;robustness (computer science);similarity measure;structural similarity	Artur Loza;Lyudmila Mihaylova;Cedric Nishan Canagarajah;David R. Bull	2006	2006 9th International Conference on Information Fusion	10.1109/ICIF.2006.301574	computer vision;speech recognition;pattern recognition;mathematics	Vision	43.532287793438286	-51.06258770188173	36270
fed7316ab794af3a51b7129ecaffbbb52e5a4d81	profile parameters of wheelset detection for high speed freight train	high speed cameras	Because of freight train, in China, transports goods on railway freight line throughout the country, it does not depart from or return to engine shed during a long phase, thus we cannot monitor the quality of wheel set effectively. This paper provides a system which uses leaser and high speed camera, applies no-contact light section technology to get precise wheel set profile parameters. The paper employs clamping-track method to avoid complex railway ballast modification project. And detailed descript an improved image-tracking algorithm to extract central line from profile curve. For getting one pixel width and continuous line of the profile curve, uses local gray maximum points as direction control points to direct tracking direction. The results based on practical experiment show the system adapted to detection environment of high speed and high vibration, and it can effectively detect the wheelset geometric parameters with high accuracy. The system fills the gaps in wheel set detection for freight train in main line and has an enlightening function on monitoring the quality of wheel set.© (2012) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.		Kai Yang;Li Ma;Xiaorong Gao;Li Wang	2012		10.1117/12.952474	simulation;engineering;transport engineering;forensic engineering	Vision	39.85709695276108	-34.31787102440898	36321
4aa9242695055286d8d300a589b0623e6de895c8	dynamic objects tracking with a mobile robot using passive uhf rfid tags	robot sensing systems rfid tags estimation navigation tracking;scitos g5 mobile robot passive uhf rfid tags ultra high frequency radio frequency identification mobile robots sensing characteristics reader environmental effects tag density tag reflection tag diffraction tag absorption dynamic objects tracking mobile agent signal strength positions estimates bayesian framework two stage dynamic motion model dual particle filter object dynamic motion;uhf devices bayes methods mobile robots motion control object tracking particle filtering numerical methods position control radiofrequency identification sensors signal processing	Recent research deals more and more with the application of ultra high frequency (UHF) radio-frequency identification (RFID) on mobile robots. However, the sensing characteristics between the reader and the tag (i.e. detections and signal strength) are challenging to model due to the influence of environmental effects (e.g. tag density, reflection, diffraction, or absorption). In this paper, we address the problem of dynamic objects tracking with a mobile agent using the signal strength from UHF RFID tags attached to objects. Our solution estimates the positions of RFID tags under a Bayesian framework. More precisely, we combine a two stage dynamic motion model with the dual particle filter, to capture the dynamic motion of the object and to quickly recover from failures in tracking. This approach is then tested on a Scitos G5 mobile robot through various experiments.	algorithm;experiment;heuristic;information gain in decision trees;kullback–leibler divergence;mathematical model;mobile agent;mobile robot;monte carlo localization;particle filter;radio frequency;radio-frequency identification;sensor;service robot;travis ci;ultra high frequency	Ran Liu;Goran Huskic;Andreas Zell	2014	2014 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2014.6943161	embedded system;computer vision	Robotics	53.21948037627545	-33.95822543878287	36378
ae299fad29ba650fbf1e14c7c95ba8ae32e095f0	person re-identification by robust canonical correlation analysis	rcca person re identification robust canonical correlation analysis people matching surveillance cameras pose change nonoverlapping cameras rocca coherent subspace training set data covariance matrix estimation shrinkage estimation smoothing technique regularized cca;covariance matrices correlation cameras image color analysis robustness estimation measurement;measurement;surveillance canonical correlation analysis cca covariance estimation person re identification subspace;estimation;covariance matrices;image color analysis;robustness;correlation;surveillance cameras covariance matrices image matching pose estimation;cameras	Person re-identification is the task to match people in surveillance cameras at different time and location. Due to significant view and pose change across non-overlapping cameras, directly matching data from different views is a challenging issue to solve. In this letter, we propose a robust canonical correlation analysis (ROCCA) to match people from different views in a coherent subspace. Given a small training set as in most re-identification problems, direct application of canonical correlation analysis (CCA) may lead to poor performance due to the inaccuracy in estimating the data covariance matrices. The proposed ROCCA with shrinkage estimation and smoothing technique is simple to implement and can robustly estimate the data covariance matrices with limited training samples. Experimental results on two publicly available datasets show that the proposed ROCCA outperforms regularized CCA (RCCA), and achieves state-of-the-art matching results for person re-identification as compared to the most recent methods.	closed-circuit television;coherence (physics);computation;regularized meshless method;smoothing;test set;transformation matrix	Le An;Songfan Yang;Bir Bhanu	2015	IEEE Signal Processing Letters	10.1109/LSP.2015.2390222	econometrics;estimation;pattern recognition;mathematics;correlation;measurement;statistics;robustness	Vision	42.645098486791944	-51.406265983526524	36420
2baccfc49eed16f0c018a9e96a5e2884f8e83ac0	automatic soccer player tracking in single camera with robust occlusion handling using attribute matching		This paper presents an automatic method to track soccer players in soccer video recorded from a single camera where the occurrence of pan-tilt-zoom can take place. The automatic object tracking is intended to support texture extraction in a free viewpoint video authoring application for soccer video. To ensure that the identity of the tracked object can be correctly obtained, background segmentation is performed and automatically removes commercial billboards whenever it overlaps with the soccer player. Next, object tracking is performed by an attribute matching algorithm for all objects in the temporal domain to find and maintain the correlation of the detected objects. The attribute matching process finds the best match between two objects in different frames according to their pre-determined attributes: position, size, dominant color and motion information. Utilizing these attributes, the experimental results show that the tracking process can handle occlusion problems such as occlusion involving more than three objects and occluded objects with similar color and moving direction, as well as correctly identify objects in the presence of camera movements. key words: free viewpoint, attribute matching, automatic object tracking, soccer video	algorithm;pattern matching	M. S. Houari Sabirin;Hiroshi Sankoh;Sei Naito	2015	IEICE Transactions		computer vision;simulation;multimedia	Vision	42.600523315070774	-47.21779716821704	36422
5ef4029d9b5bac38527796ac3427f64f45072f6c	pedestrian trajectory prediction in large infrastructures - a long-term approach based on path planning		This paper presents a pedestrian trajectory prediction technique. Its mail novelty is that it does not require any previous observation or knowledge of pedestrian trajectories, thus making it useful for autonomous surveillance applications. The prediction requires only a set of possible goals, a map of the scenario and the initial position of the pedestrian. Then, it uses two different path planing algorithms to find the possible routes and transforms the similarity between observed and planned routes into probabilities. Finally, it applies a motion model to obtain a time-stamped predicted trajectory. The system has been used in combination with a pedestrian detection and tracking system for real-world tests as well as a simulation software for a large number of	approximation algorithm;autonomous robot;cybernetics;experiment;fast multipole method;kalman filter;linear algebra;map;motion planning;on-board data handling;pedestrian detection;real-time clock;robotics;scenario testing;simulation software;tracking system	Mario Garzón;David Garzón-Ramos;Antonio Barrientos;Jaime del Cerro	2016		10.5220/0005983303810389	computer vision;simulation;transport engineering	Robotics	53.523418859746826	-34.30112328303514	36445
e5acdb0246b33d33c2a34a4a23faaf21e2f9b924	an efficient non-negative matrix-factorization-based approach to collaborative filtering for recommender systems	nmf based cf models;tikhonov regularization collaborative filtering cf non negative matrix factorization nmf recommender system single element based approach;training;low computational complexity;non negative matrix factorization nmf;rsnmf;recommender systems collaborative filtering computational complexity matrix decomposition;collaborative filtering cf;low computational complexity nonnegative matrix factorization based approach collaborative filtering recommender systems nmf based cf models extreme sparsity target rating matrix matrix manipulation single element based approach nonnegative update process feature matrices nonnegative negative single element based update rules regularized single element based nmf rsnmf;accuracy;nonnegative update process;computational modeling;nonnegative matrix factorization based approach;nonnegative negative single element based update rules;recommender system;collaborative filtering;computational modeling training accuracy sparse matrices algorithm design and analysis informatics computational complexity;matrix decomposition;computational complexity;feature matrices;regularized single element based nmf;informatics;target rating matrix;single element based approach;tikhonov regularization;extreme sparsity;recommender systems;sparse matrices;algorithm design and analysis;matrix manipulation	Matrix-factorization (MF)-based approaches prove to be highly accurate and scalable in addressing collaborative filtering (CF) problems. During the MF process, the non-negativity, which ensures good representativeness of the learnt model, is critically important. However, current non-negative MF (NMF) models are mostly designed for problems in computer vision, while CF problems differ from them due to their extreme sparsity of the target rating-matrix. Currently available NMF-based CF models are based on matrix manipulation and lack practicability for industrial use. In this work, we focus on developing an NMF-based CF model with a single-element-based approach. The idea is to investigate the non-negative update process depending on each involved feature rather than on the whole feature matrices. With the non-negative single-element-based update rules, we subsequently integrate the Tikhonov regularizing terms, and propose the regularized single-element-based NMF (RSNMF) model. RSNMF is especially suitable for solving CF problems subject to the constraint of non-negativity. The experiments on large industrial datasets show high accuracy and low-computational complexity achieved by RSNMF.	automatic taxonomy construction;coefficient;collaborative filtering;computation;computational complexity theory;computer vision;experiment;loss function;negative base;negativity (quantum mechanics);non-negative matrix factorization;recommender system;scalability;sparse matrix	Xin Luo;Mengchu Zhou;Yunni Xia;Qingsheng Zhu	2014	IEEE Transactions on Industrial Informatics	10.1109/TII.2014.2308433	algorithm design;sparse matrix;computer science;collaborative filtering;machine learning;pattern recognition;data mining;accuracy and precision;matrix decomposition;computational complexity theory;informatics;computational model;tikhonov regularization;recommender system	Vision	27.57242120889668	-36.11831358388397	36463
8cbecfd79f44643fa589f7c110942ed3acfe4883	a gpu-supported high-level programming language for image processing	data parallel;nvidia geforce gtx 280;high level languages;paper;mobile device;image processing;programming language;video signal processing;data parallelism;video signal processing graphics processing units high level languages language translation parallel architectures;language translation;video processing;language translator gpu supported high level programming language image processing video processing applications general purpose computers mobile devices cuda high level video processing library ravioli;graphics processing unit kernel libraries image resolution gray scale image color analysis;cuda;parallel architectures;graphics processing units;nvidia	Real-time image/video processing applications are now in demand with the advance of general purpose computers and mobile devices. However, programmers have to handle the digital images, and be aware of the resolutions and pixels. This makes image processing programming unintuitive. On the other hand, image/video processing typically has data parallelisms, and the performance gains are expected on GPUs. CUDA is developed for GPUs but writing the image/video processing programs efficiently with CUDA needs many CUDA-specific operations. They are not the essence of image/video processing and bother programmers. We have proposed a high-level video processing library RaVioli for solving this problem. RaVioli allows programmers to be unaware of resolutions, but there are some restrictions for the programming. Hence, this paper proposes a more intuitive programming language for image/video processing and a translator for the language. By using the translator, programmers can benefit from GPUs without the knowledge about both the GPU architecture and the CUDA APIs, and achieve performance gains.	cuda;computer;digital image;graphics processing unit;high- and low-level;high-level programming language;image processing;mobile device;pixel;programmer;real-time transcription;video processing	Ami Ono;Katsuhiko Kondo;Takafumi Inaba;Tomoaki Tsumura;Hiroshi Matsuo	2011	2011 Seventh International Conference on Signal Image Technology & Internet-Based Systems	10.1109/SITIS.2011.66	parallel computing;image processing;computer science;theoretical computer science;operating system;mobile device;data parallelism;video processing;programming language;high-level programming language;computer graphics (images)	Robotics	42.451684101865126	-35.54403680814434	36570
54f6ac5e2e605560c016e7a439dff8a65a3e1286	exact and stable recovery of pairwise interaction tensors		Tensor completion from incomplete observations is a problem of significant practical interest. However, it is unlikely that there exists an efficient algorithm with provable guarantee to recover a general tensor from a limited number of observations. In this paper, we study the recovery algorithm for pairwise interaction tensors, which has recently gained considerable attention for modeling multiple attribute data due to its simplicity and effectiveness. Specifically, in the absence of noise, we show that one can exactly recover a pairwise interaction tensor by solving a constrained convex program which minimizes the weighted sum of nuclear norms of matrices from O(nr log(n)) observations. For the noisy cases, we also prove error bounds for a constrained convex program for recovering the tensors. Our experiments on the synthetic dataset demonstrate that the recovery performance of our algorithm agrees well with the theory. In addition, we apply our algorithm on a temporal collaborative filtering task and obtain state-of-the-art results.	algorithm;collaborative filtering;convex optimization;dhrystone;experiment;mathematical optimization;provable prime;scalability;simulation;time complexity;weight function	Shouyuan Chen;Michael R. Lyu;Irwin King;Zenglin Xu	2013			mathematical optimization;combinatorics;machine learning;mathematics;statistics	ML	26.892948600455647	-35.77329983611438	36721
ed74363c54ef59e99a983e85f02be9d263f66ffb	is gender classification across ethnicity feasible using discriminant functions?	video surveillance;heterogeneous databases;video surveillance face recognition gender issues image classification principal component analysis;image classification;face databases principal component analysis accuracy;linear discriminate analysis;discriminant function;discriminant analysis;gender issues;face recognition;linear discriminant function;principal component analysis;video surveillance gender classification gender recognition principal component analysis linear discriminant analysis subclass discriminant analysis heterogeneous database pca lda pca sda pca svm cross ethnicity variations face recognition;generalization capability;principal component	Over the years, automatic gender recognition has been used in many applications. However, limited research has been done on analyzing gender recognition across ethnicity scenario. This research aims at studying the performance of discriminant functions including Principal Component Analysis, Linear Discriminant Analysis and Subclass Discriminant Analysis with the availability of limited training database and unseen ethnicity variations. The experiments are performed on a heterogeneous database of 8112 images that includes variations in illumination, expression, minor pose and ethnicity. Contrary to existing literature, the results show that PCA provides comparable but slightly better performance compared to PCA+LDA, PCA+SDA and PCA+SVM. The results also suggest that linear discriminant functions provide good generalization capability even with limited number of training samples, principal components and with cross-ethnicity variations.	algorithm;experiment;heterogeneous database system;linear classifier;linear discriminant analysis;nonlinear system;performance evaluation;principal component analysis;regular expression;statistical classification	Tejas I. Dhamecha;Anush Sankaran;Richa Singh;Mayank Vatsa	2011	2011 International Joint Conference on Biometrics (IJCB)	10.1109/IJCB.2011.6117524	facial recognition system;computer science;machine learning;pattern recognition;data mining;optimal discriminant analysis;linear discriminant analysis;multiple discriminant analysis;principal component analysis	Vision	27.57942508310322	-45.06417340122667	36742
fed80968050532af3e69253665e941dab2937673	a high-dimensional two-sample test for the mean using random subspaces	test about the mean;probability theory and statistics;two sample problem;statistik;gene expression data;mathematical statistics;gene set testing;random subspace;high dimensional data;statistics;matematisk statistik;sannolikhetsteori och statistik	A common problem in genetics is that of testing whether a set of highly dependent gene expressions differ between two populations, typically in a high-dimensional setting where the data dimension is larger than the sample size. Most high-dimensional tests for the equality of two mean vectors rely on naive diagonal or trace estimators of the covariance matrix, ignoring dependences between variables. A test using random subspaces is proposed, which offers higher power when the variables are dependent and is invariant under linear transformations of the marginal distributions. The p-values for the test are obtained using permutations. The test does not rely on assumptions about normality or the structure of the covariance matrix. It is shown by simulation that the new test has higher power than competing tests in realistic settings motivated by microarray gene expression data. Computational aspects of high-dimensional permutation tests are also discussed and an efficient R implementation of the proposed test is provided.		Måns Thulin	2014	Computational Statistics & Data Analysis	10.1016/j.csda.2013.12.003	econometrics;discrete mathematics;mathematical statistics;mathematics;normality test;statistics;clustering high-dimensional data	ML	28.893232693310246	-24.504262782505545	36750
433303978f5d65887f3e4baaaf3cfa7901562dc8	extrinsic calibration of a multi-beam lidar system with improved intrinsic laser parameters using v-shaped planes and infrared images	calibration laser radar cameras laser modes measurement by laser beam robot vision systems;optical scanners;image sensors;pnp algorithm extrinsic calibration multibeam lidar system intrinsic laser parameters v shaped planes infrared images laser pixel correspondences multilayer laser scanner ir image sensor camera nonlinear algorithm;optical radar;infrared imaging;optical scanners calibration cameras image sensors infrared imaging optical radar;3 d reconstruction lidar camera system infrared imaging multi layer laser scanner sensor calibration;calibration;cameras	In this paper we present a novel method for the calibration of LiDAR-camera systems. The method uses infrared (IR) images to establish laser-pixel correspondences between a multi-layer laser scanner and an IR image sensor. Based on the established correspondences, the intrinsic parameters of the LiDAR system and of the camera as well as the extrinsic parameters are adjusted. The proposed method works in a two-stage manner. The linear estimation of the extrinsic parameters are first obtained using an efficient PnP algorithm, while in the second stage the extrinsic and intrinsic parameters are optimized using a nonlinear algorithm. Experiments show promising results.	algorithm;camera resectioning;experiment;image sensor;layer (electronics);legacy plug and play;nonlinear system;pixel	Po-Sen Huang;Wen-Bin Hong;Hsiang-Jen Chien;Chia-Yen Chen	2013	IVMSP 2013	10.1109/IVMSPW.2013.6611921	computer vision;geography;optics;remote sensing	Vision	53.52566197227626	-43.7887541494676	36751
0bf8aa1006957ccbc18e4bd6624a7028157a58ff	swim stroke analytic: front crawl pulling pose classification		In this work, we automatically distinguish the efficient high elbow pose from dropping one in pulling phase of front crawl stroke in front view amateurly recorded videos. This task is challenging due to the aquatic environment and missing depth information. We predict the pull's efficiency through multiclass svm and random forest classifiers given arms key positions and angles as the feature set. We evaluate our approach over a labeled dataset of video frames taken from 25 members of masters' swim club at Ryerson University with different levels of expertise and physiological characteristics. Our results show the effectiveness of our approach with random forest classifier, yielding 67% accuracy.		Hossein Fani;Amin Mirlohi;Hawre Hosseini;Rainer Herperst	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451756	support vector machine;task analysis;feature extraction;pose;random forest;pattern recognition;computer science;artificial intelligence;front crawl	Vision	35.921815802777836	-49.44409871686346	36782
f11dbdeacd2d125cc81dc3a77c1ba0c9c4f64966	consensus skeleton for non-rigid space-time registration	space time;regisration and deformation;point cloud;i 3 5 computer graphics computational geometry and object modeling object representation	We introduce the notion of consensus skeletons for non-rigid space-time registration of a deforming shape. Instead of basing the registration on point features, which are local and sensitive to noise, we adopt the curve skeleton of the shape as a global and descriptive feature for the task. Our method uses no template and only assumes that the skeletal structure of the captured shape remains largely consistent over time. Such an assumption is generally weaker than those relying on large overlap of point features between successive frames, allowing for more sparse acquisition across time. Building our registration framework on top of the low-dimensional skeletontime structure avoids heavy processing of dense point or volumetric data, while skeleton consensusization provides robust handling of incompatibilities between per-frame skeletons. To register point clouds from all frames, we deform them by their skeletons, mirroring the skeleton registration process, to jump-start a non-rigid ICP. We present results for non-rigid space-time registration under sparse and noisy spatio-temporal sampling, including cases where data was captured from only a single view.	consensus (computer science);disk mirroring;point cloud;sampling (signal processing);sparse matrix	Qian Zheng;Andrei Sharf;Andrea Tagliasacchi;Baoquan Chen;Hao Zhang;Alla Sheffer;Daniel Cohen-Or	2010	Comput. Graph. Forum	10.1111/j.1467-8659.2009.01633.x	computer vision;computer science;space time;point cloud;mathematics;geometry;topological skeleton;computer graphics (images)	Vision	52.51826933719561	-48.14792658603566	36793
031d369e402a1cfbd1afeee772fe59ece86cd056	status self-validation of sensor arrays using gray forecasting model and bootstrap method	statistical analysis bootstrapping fault diagnosis forecasting theory gas sensors measurement uncertainty reliability sensor arrays signal reconstruction;measurement uncertainty;redundancy;estimation;monitoring;status self validation bootstrap method dynamic uncertainty gray forecasting model sensor array;sensor arrays measurement uncertainty monitoring estimation redundancy;sensor arrays;gray forecasting model trust mechanism metal oxide gas sensor array experimental system signal reconstruction metabolism method fault free signal discrimination fdir scheme dynamic measurement uncertainty estimation data validation fault detection isolation and recovery scheme gm 1 1 reliability monitoring status self validation bootstrap method	The reliability monitoring of sensor arrays is a challenging and critical issue that directly influences the performance of a measurement and control system. In this paper, a novel strategy based on gray forecasting model GM(1,1) coupled with the bootstrap method is proposed for status self-validation of sensor arrays. The proposed strategy focuses on fault detection, isolation, and recovery (FDIR), data validation and dynamic measurement uncertainty estimation of the sensor arrays. The FDIR scheme can effectively detect and isolate sensor abrupt faults and simultaneously accomplish fault recovery with high accuracy and good timeliness. Furthermore, the proposed FDIR scheme has the advantage of discriminating between fault-free signals with sudden changes and undoubted abrupt faults through the trust mechanism. The model GM(1,1) is updated continuously by a metabolism method to improve the adaptivity of the strategy for reliability monitoring. After signal reconstruction, the data validation and dynamic measurement uncertainty can be evaluated by the bootstrap method without any prior information about measurands. A real metal-oxide gas sensor array experimental system is designed to verify the excellent performance of the proposed strategy. The experimental results demonstrate that the proposed approach is capable of conducting the status self-validation of sensor arrays effectively and improving the reliability of sensor arrays in engineering applications.	algorithm;bootstrapping (statistics);control system;data validation;experiment;experimental system;fault detection and isolation;kerrison predictor;portable document format;real-time clock;sensor web;signal reconstruction	Yinsheng Chen;Jingli Yang;Yonghui Xu;Shouda Jiang;Xiaodong Liu;Qi Wang	2016	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2016.2540942	econometrics;estimation;electronic engineering;engineering;mathematics;redundancy;statistics;measurement uncertainty	Robotics	37.546627386016866	-29.068218845390465	36805
a544cd2a7340c5965a81df36fc5d12c5c73ee4e4	the penalty term of exponentially embedded family is estimated mutual information		The penalty term plays an important role in model order selection rules. The Exponentially Embedded Families (EEF) is consistent and effective in model order selection. In this paper we show that the EEF penalty term can be viewed as estimated mutual information (MI) between unknown parameters and received data from Bayesian viewpoints. The finding is a result of an important relationship between Kullback-Leibler Divergence (KLD), signal-to-noise ratio (SNR) and MI in estimation/detection of random signals, which is also introduced.	download;embedded system;kullback–leibler divergence;model selection;mutual information;resultant;selection rule;signal-to-noise ratio;vagueness	Zhenghan Zhu;Steven Kay	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952937	artificial intelligence;pointwise mutual information;probability density function;divergence;pattern recognition;statistics;mutual information;exponential growth;mathematics;bayesian probability;signal-to-noise ratio	Robotics	33.03597020876681	-26.859294566428233	36824
9a5c8d136f2c9e2684884d82cce5cb18b0de0e40	human action recognition from depth videos using pool of multiple projections with greedy selection		Depth-based action recognition has been attracting the attention of researchers because of the advantages of depth cameras over standard RGB cameras. One of these advantages is that depth data can provide richer information from multiple projections. In particular, multiple projections can be used to extract discriminative motion patterns that would not be discernible from one fixed projection. However, high computational costs have meant that recent studies have exploited only a small number of projections, such as front, side, and top. Thus, a large number of projections, which may be useful for discriminating actions, are discarded. In this paper, we propose an efficient method to exploit pools of multiple projections for recognizing actions in depth videos. First, we project 3D data onto multiple 2D-planes from different viewpoints sampled on a geodesic dome to obtain a large number of projections. Then, we train and test action classifiers independently for each projection. To reduce the computational cost, we propose a greedy method to select a small yet robust combination of projections. The idea is that best complementary projections will be considered first when searching for optimal combination. We conducted extensive experiments to verify the effectiveness of our method on three challenging benchmarks: MSR Action 3D, MSR Gesture 3D, and 3D Action Pairs. The experimental results show that our method outperforms other state-of-the-art methods while using a small number of projections. key words: action recognition, depth sequences, multiple projections, greedy method	2d-plus-depth;action potential;algorithmic efficiency;computation;experiment;exploit (computer security);greedy algorithm	Chien-Quang Le;Sang Phan Le;Thanh Duc Ngo;Duy-Dinh Le;Shin'ichi Satoh;Duc Anh Duong	2016	IEICE Transactions		computer vision;greedy algorithm;computer science;machine learning;pattern recognition;mathematics;algorithm	Vision	33.27413071164712	-49.63372727554951	36832
99a37a522c531715cdc8d76117cd2e98707aed74	omnidirectional dso: direct sparse odometry with fisheye cameras		We propose a novel real-time direct monocular visual odometry for omnidirectional cameras. Our method extends direct sparse odometry by using the unified omnidirectional model as a projection function, which can be applied to fisheye cameras with a field-of-view (FoV) well above 180$^\circ$ . This formulation allows for using the full area of the input image even with strong distortion, while most existing visual odometry methods can only use a rectified and cropped part of it. Model parameters within an active keyframe window are jointly optimized, including the intrinsic/extrinsic camera parameters, three-dimensional position of points, and affine brightness parameters. Thanks to the wide FoV, image overlap between frames becomes bigger and points are more spatially distributed. Our results demonstrate that our method provides increased accuracy and robustness over state-of-the-art visual odometry algorithms.	algorithm;camera resectioning;distortion;fisheye;key frame;library (computing);omnidirectional camera;real-time clock;rectifier;sparse matrix;visual odometry	Hidenobu Matsuki;Lukas von Stumberg;Vladyslav C. Usenko;J&#x00F6;rg St&#x00FC;ckler;Daniel Cremers	2018	IEEE Robotics and Automation Letters	10.1109/LRA.2018.2855443	pattern recognition;brightness;odometry;artificial intelligence;simultaneous localization and mapping;distortion;visual odometry;monocular;computer science;omnidirectional antenna;affine transformation	Robotics	52.883401386651244	-45.81675994208634	36849
b551ed135a16477cc6a0ff6680e6cec6e09bf6a9	real-time road tracking using templates matching	template matching;real time		real-time transcription;template matching	Miguel Ángel Sotelo;Alexander Zelinsky;Francisco Javier Rodríguez;Luis Miguel Bergasa	1999			computer vision;control engineering;template;artificial intelligence;computer science;template matching	Vision	50.94331671796865	-42.736991733383604	36879
6b011aa54aeabae8ac172a0cf0dd4333d1bfd327	supervised algorithm selection for flow and other computer vision problems	ucl;discovery;theses;conference proceedings;digital web resources;ucl discovery;open access;ucl library;book chapters;open access repository;ucl research	Motion estimation is one of the core problems of computer vision. Given two or more frames from a video sequence, the goal is to find the temporal correspondence for one or more points from the sequence. For dense motion estimation, or optical flow, a dense correspondence field is sought between the pair of frames. A standard approach to optical flow involves constructing an energy function and then using some optimization scheme to find its minimum. These energy functions are hand designed to work well generally, with the intention that the global minimum corresponds to the ground truth temporal correspondence. As an alternative to these heuristic energy functions we aim to assess the quality of existing algorithms directly from training data. We show that the addition of an offline training phase can improve the quality of motion estimation. For optical flow, decisions such as which algorithm to use and when to trust its accuracy, can all be learned from training data. Generating ground truth optical flow data is a difficult and time consuming process. We propose the use of synthetic data for training and present a new dataset for optical flow evaluation and a tool for generating an unlimited quantity of ground truth correspondence data. We use this method for generating data to synthesize depth images for the problem of depth image super-resolution and show that it is superior to real data. We present results for optical flow confidence estimation with improved performance on a standard benchmark dataset. Using a similar feature representation, we extend this work to occlusion region detection and present state of the art results for challenging real scenes. Finally, given a set of different algorithms we treat optical flow estimation as the problem of choosing the best algorithm from this set for a given pixel. However, posing algorithm selection as a standard classification problem assumes that class labels are disjoint. For each training example it is assumed that there is only one class label that correctly describes it, and that all other labels are equally bad. To overcome this, we propose a novel example dependent cost-sensitive learning algorithm based on decision trees where each label is instead a vector representing a data point’s affinity for each of the algorithms. We show that this new algorithm has improved accuracy compared to other classification baselines on several computer vision problems.	algorithm selection;benchmark (computing);computer vision;data point;decision tree;ground truth;heuristic;image processing;mathematical optimization;maxima and minima;motion estimation;online and offline;optical flow;pixel;processor affinity;super-resolution imaging;synthetic data	Oisin Mac Aodha	2014			computer science;artificial intelligence;machine learning;data mining	Vision	28.194733338213958	-48.658038343997084	36885
f53da61849dd92220339e70dff372de256b10134	robust tracking-by-detection using a selection and completion mechanism		It is challenging to track a target continuously in videos with long-term occlusion, or objects which leave then re-enter a scene. Existing tracking algorithms combined with onlinetrained object detectors perform unreliably in complex conditions, and can only provide discontinuous trajectories with jumps in position when the object is occluded. This paper proposes a novel framework of tracking-by-detection using selection and completion to solve the abovementioned problems. It has two components, tracking and trajectory completion. An offline-trained object detector can localize objects in the same category as the object being tracked. The object detector is based on a highly accurate deep learning model. The object selector determines which object should be used to re-initialize a traditional tracker. As the object selector is trained online, it allows the framework to be adaptable. During completion, a predictive non-linear autoregressive neural network completes any discontinuous trajectory. The tracking component is an online real-time algorithm, and the completion part is an after-theevent mechanism. Quantitative experiments show a significant improvement in robustness over prior state-of- the-art methods.	algorithm;artificial neural network;autoregressive model;deep learning;experiment;hidden surface determination;network model;nonlinear system;online and offline;real-time clock;sensor	Ruochen Fan;Fang-Lue Zhang;Min Zhang;Ralph R. Martin	2017	Computational Visual Media	10.1007/s41095-017-0083-7	computer vision;robustness (computer science);pattern recognition;artificial neural network;video tracking;deep learning;autoregressive model;detector;artificial intelligence;trajectory;computer science;viola–jones object detection framework	Robotics	43.12218179828769	-47.96194485788599	36913
6ec09bad57cc81a71ef7596f57e94ee13b380ae3	video summarization via semantic attended networks		The goal of video summarization is to distill a raw video into a more compact form without losing much semantic information. However, previous methods mainly consider the diversity and representation interestingness of the obtained summary, and they seldom pay sufficient attention to semantic information of resulting frame set, especially the long temporal range semantics. To explicitly address this issue, we propose a novel technique which is able to extract the most semantically relevant video segments (i.e., valid for a long term temporal duration) and assemble them into an informative summary. To this end, we develop a semantic attended video summarization network (SASUM) which consists of a frame selector and video descriptor to select an appropriate number of video shots by minimizing the distance between the generated description sentence of the summarized video and the human annotated text of the original video. Extensive experiments show that our method achieves a superior performance gain over previous methods on two benchmark datasets.	automatic summarization;benchmark (computing);digital media;digital video;experiment;information;uncompressed video	Huawei Wei;Bingbing Ni;Yichao Yan;Huanyu Yu;Xiaokang Yang;Chen Yao	2018			artificial intelligence;automatic summarization;machine learning;computer science	AI	33.17885887347408	-50.57605019697914	37087
40fc2803d3df9759a3853b040588a7d9be9542d2	perception-prediction-control architecture for ip pan-tilt-zoom camera through interacting multiple models	interacting multiple models tracking pan tilt zoom camera ptz delay latency prediction zoom control;target tracking cameras delays predictive models kalman filters three dimensional displays	IP Pan-Tilt-Zoom cameras (IP PTZ) are now common in videosurveillance areas as they are easy to deploy and can take high resolution pictures of targets in a large field of view thanks to their pan-tilt and zoom capabilities. However the closer the view is, the higher is the risk to lose your target. Furthermore, off-the-shelf cameras used in large videosurveillance areas present important motion delays. In this paper, we suggest a new motion control architecture that manages tracking and zoom delays by an Interacting Multiple Models analysis of the target motion, increasing tracking performances and robustness.	algorithm;computation;display lag;experiment;ip camera;image resolution;interrupt;kalman filter;pan–tilt–zoom camera;performance;radar tracker;time complexity	Pierrick Paillet;Romaric Audigier;Frédéric Lerasle;Quoc-Cuong Pham	2014	2014 International Conference on Computer Vision Theory and Applications (VISAPP)	10.5220/0004674103140324	computer vision;simulation;computer graphics (images)	Vision	49.6125842176114	-43.26110959387536	37111
c5dce665fb9da2f430c352d591e855001eabd3ff	supporting urban search & rescue mission planning through visualization-based analysis	and texture;color;shadowing;datavetenskap datalogi;computer graphics i 3 7;computer science;three dimensional graphics and realism;shading	We propose a visualization system for incident commanders in urban search & rescue scenarios that supports access path planning for post-disaster structures. Utilizing point cloud data acquired from unmanned robots, we provide methods for assessment of automatically generated paths. As data uncertainty and a priori unknown information make fully automated systems impractical, we present a set of viable access paths, based on varying risk factors, in a 3D environment combined with the visual analysis tools enabling informed decisions and trade-offs. Based on these decisions, a responder is guided along the path by the incident commander, who can interactively annotate and reevaluate the acquired point cloud to react to the dynamics of the situation. We describe design considerations for our system, technical realizations, and discuss the results of an expert evaluation.	computation;feedback;future search;integrated circuit;interactivity;monte carlo method;motion planning;point cloud;robot;sampling (signal processing);scientific visualization;technical standard;unmanned aerial vehicle;usability;utility	Alexander Bock;Alexander Kleiner;Jonas Lundberg;Timo Ropinski	2014		10.2312/vmv.20141275	computer vision;shading;simulation;computer science;computer graphics;statistics;computer graphics (images)	HCI	53.14702692508238	-26.12676691683132	37151
f714e99c022279a87e2db2ff003bfb4308df7e11	handling pedestrians in crosswalks using deep neural networks in the iara autonomous car		In this work, we propose a subsystem to handle pedestrians in crosswalks using deep neural networks for the IARA autonomous car, which relies on camera and LIDAR data fusion. Crosswalks’ positions were manually annotated in IARA’s map. Pedestrians are detected in the camera image using a convolutional neural network (CNN). Then, pedestrians’ positions in the map are obtained by fusing their positions in the image with the LIDAR point cloud. Subsequently, if a pedestrian position is inside the crosswalk area, the crosswalk is set as busy. Finally, a busy crosswalk message is published to the High-Level Decision Maker subsystem. This subsystem selects the car’s behavior according to the crosswalk condition and propagates this decision down through the control pipeline, in order to make the car drive correctly through the crosswalk area. The Pedestrian Handler subsystem was evaluated on IARA, which was driven autonomously for various laps along a real and complex circuit with various crosswalks. In all passages through crosswalks, the Pedestrian Handler dealt with pedestrians as expected, i.e., without any human intervention.	artificial neural network;autonomous car;autonomous robot;convolutional neural network;deep learning;interrupt handler;neural networks;point cloud;scream tracker;velocity (software development)	Ranik Guidolini;Shivalingrao Mamle Desai;Luan F. R. Jesus;Vinicius B. Cardoso;Claudine Badue;Thiago Oliveira-Santos	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489397	point cloud;computer vision;convolutional neural network;artificial neural network;machine learning;computer science;pedestrian;lidar;sensor fusion;artificial intelligence	Robotics	46.48326911270973	-40.255639546191745	37163
ee07abde799d82bc75c896c00a9212f095a1a69e	a statistical approach for person verification using human behavioral patterns	signal image and speech processing;biometrics;pattern recognition;image processing and computer vision	We propose a person verification method using behavioral patterns of human upper body motion. Behavioral patterns are represented by three-dimensional features obtained from a time-of-flight camera. We take a statistical approach to model the behavioral patterns using Gaussian mixture models (GMM) and support vector machines. We employ the maximum likelihood linear regression adaptation method to estimate GMM parameters with a limited amount of data. Experimental results show that it reduced by 28.6% the relative equal error rates from a system using the maximum likelihood estimation with 25 samples per subject. We also demonstrate that the proposed approach is robust against variations in body motion over time.	behavioral pattern;google map maker;mixture model;support vector machine;time-of-flight camera	Felipe Gómez-Caballero;Takahiro Shinozaki;Sadaoki Furui;Koichi Shinoda	2013	EURASIP J. Image and Video Processing	10.1186/1687-5281-2013-44	computer vision;speech recognition;computer science;archaeology;pattern recognition;biometrics	ML	39.709785641529706	-48.94037123905847	37255
a04c8ecd7472a5e93bba143bff6f7a05587d3b43	reaching and grasping novel objects: using neural dynamics to integrate and organize scene and object perception with movement generation	kuka lightweight arm robotic grasping object perception movement generation neural dynamics architecture perception action loop scene exploration pose estimation shape classification kinect sensor schunk dextrous hand;shape grasping estimation robot kinematics robot sensing systems discrete fourier transforms;shape recognition dexterous manipulators image classification neural net architecture object recognition pose estimation robot vision	We present a neural dynamics architecture for robotic grasping of novel objects. It closes the perception-action loop by integrating perceptual processes such as scene exploration, pose estimation, and shape classification with movement generation to reach and grasp a target object. Inspired by theories of human embodied cognition, this is achieved by interconnected dynamical systems, whose dynamical instabilities mark the discrete events of the grasping process. The architecture perceives the scene through a Kinect sensor and executes the grasp with a Schunk Dextrous Hand attached to a Kuka light weight arm.	dynamical system;embodied cognition;kinect;robot;theory	Guido Knips;Stephan K. U. Zibner;Hendrik Reimann;Irina Popova;Gregor Schöner	2014	4th International Conference on Development and Learning and on Epigenetic Robotics	10.1109/DEVLRN.2014.6982999	computer vision;simulation;engineering;communication	Robotics	49.555472743912574	-31.07446089655221	37279
2c87eb314051162c525cdacec32595c89b25c20f	estimating semi-parametric missing values with iterative imputation	dataset distribution;iterative imputation method;semi parametric iterative imputation algorithm siia;algorithms;missing values	In this paper, the author designs an efficient method for imputing iteratively missing target values with semiparametric kernel regression imputation, known as the semi-parametric iterative imputation algorithm (SIIA). While there is little prior knowledge on the datasets, the proposed iterative imputation method, which impute each missing value several times until the algorithms converges in each model, utilize a substantially useful amount of information. Additionally, this information includes occurrences involving missing values as well as capturing the real dataset distribution easier than the parametric or nonparametric imputation techniques. Experimental results show that the author’s imputation methods outperform the existing methods in terms of imputation accuracy, in particular in the situation with high missing ratio. comparing to other methods. Missing values imputation is to find an efficient way to “guess” the missing values (imputation) based on other information in datasets. One advantage of this approach is that missing values treatment is independent of the learning algorithm used. That allows users to select the most suitable imputation method for their applications. Commonly used imputation methods for missing values include parametric regression imputation methods and non-parametric regression imputations. However, there are other relations within real world data, and both parametric imputation method and non-parametric imputation method are not adequate to capture the relations. That is, we know a part of relation DOI: 10.4018/jdwm.2010070101 2 International Journal of Data Warehousing and Mining, 6(3), 1-10, July-September 2010 Copyright © 2010, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. between independent variables (condition attributes) and dependent variable (target attribute), e.g., we can regard this relation as parametric model, but we have no knowledge on the relation between other independent variables and dependent variable, e.g., we can take it as nonparametric model. However, combining these two parts, it is difficult for us to consider the compound relation with parametric model or nonparametric model. Moreover, the case is very general in real application. In this paper, we regard the relation containing two models as semi-parametric model or partial parametric model. In real application, semi-parametric model is natural than non-parametric model because users can always know some information but no all on the datasets, such some parameters in the datasets. To model this semi-parametric relation, in this paper, we design an efficient semi-parametric iterative imputation method (SIIA) that takes into account the advantages of parametric models and pure non-parametric models so as to overcome their certain shortcomings for each single model. In the left parts, we will first review the existing literatures for dealing with missing values. And then we design the iterative imputation methods which can impute missing values with kernel method or even in the dataset with high missing ratio. After that, we will demonstrate our proposed methods with all kinds of experiments. Finally, we will conclusion our works and put forward our future work.	algorithm;experiment;geo-imputation;iteration;iterative method;kernel method;mathematical model;missing data;parametric model;parametric polymorphism;semiconductor industry;semiparametric model	Shichao Zhang	2010	IJDWM	10.4018/jdwm.2010070101	missing data;computer science;data mining;imputation;statistics	ML	28.30033164178193	-35.26550622729831	37343
a8c09281c01a76e5ea78600ce2379605138a76cb	a nonlinear framework of delayed particle smoothing method for vehicle localization under non-gaussian environment	non gaussian noise;particle filter;ensemble kalman filter;vehicle localization;fixed delay smoothing	In this paper, a novel nonlinear framework of smoothing method, non-Gaussian delayed particle smoother (nGDPS), is proposed, which enables vehicle state estimation (VSE) with high accuracy taking into account the non-Gaussianity of the measurement and process noises. Within the proposed method, the multivariate Student's t-distribution is adopted in order to compute the probability distribution function (PDF) related to the process and measurement noises, which are assumed to be non-Gaussian distributed. A computation approach based on Ensemble Kalman Filter (EnKF) is designed to cope with the mean and the covariance matrix of the proposal non-Gaussian distribution. A delayed Gibbs sampling algorithm, which incorporates smoothing of the sampled trajectories over a fixed-delay, is proposed to deal with the sample degeneracy of particles. The performance is investigated based on the real-world data, which is collected by low-cost on-board vehicle sensors. The comparison study based on the real-world experiments and the statistical analysis demonstrates that the proposed nGDPS has significant improvement on the vehicle state accuracy and outperforms the existing filtering and smoothing methods.	algorithm;assumed;computation (action);degeneracy (graph theory);ensemble kalman filter;entity name part qualifier - adopted;experiment;gibbs sampling;nonlinear system;normal statistical distribution;on-board data handling;particle filter;portable document format;sampling (signal processing);sampling - surgical action;smoothing (statistical technique);time complexity;vancomycin susceptible enterococcus;non-t, non-b childhood acute lymphoblastic leukemia;sensor (device);t-distribution	Zhu Xiao;Vincent Havyarimana;Tong Li;Dong Wang	2016		10.3390/s16050692	mathematical optimization;ensemble kalman filter;particle filter;computer science;control theory;extended kalman filter;statistics;smoothing	AI	40.43697129560431	-27.00169270256473	37379
d97df0e92f763f5465d8f092b8ea83b497712661	pedestrian tracking using single-row laser range scanners	cross section;background subtraction;coordinate system	In this research, we propose a novel method of monitoring and tracking pedestrians in wide and open area, such as shopping mall and exhibition hall, using a number of laser range scanners (LD-A). LD-A, produced by IBEO Lasertechnik, is a single-row type laser range scanner with a high profiling rate of lOHz and wide viewing angle of 270 degree. LD-As are set on the ground doing horizontal scanning, so that horizontal cross sections of the surroundings, containing moving legs of pedestrians as well as still objects, are obtained in a rectangular coordinate system of real dimension. Each LD-A is controlled by a client computer, which gathers laser data, extracts the data of moving objects by background subtraction, and sends them to a server computer through network. The server computer synchronizes all client computers, integrates the data of moving objects from client computers to a global coordinate system, and tracks trajectories by identifying the pattern of moving legs. An experiment is conducted in Tokyo BigSite to monitor the pedestrian's movement in an exhibition hall, where three LD-As are used, and trajectories of more than 50 pedestrians are tracked simultaneously.	aac-ld;background subtraction;client (computing);computer;cross section (geometry);server (computing);viewing angle	Huijing Zhao;Ryosuke Shibasaki;Nobuaki Ishihara	2002			computer vision;coordinate system;artificial intelligence;computer science;cartesian coordinate system;data extraction;viewing angle;client;laser;server;background subtraction	Robotics	49.13532493688499	-41.95892159504704	37410
d11b8269c5a5f973b27a8420763725439e7916fb	heterogeneous face recognition based on super resolution reconstruction by adaptive multi-dictionary learning		The heterogeneous face recognition algorithm is based on super resolution reconstruction and two-dimensional marginal fisher analysis. In this paper, a super resolution reconstruction algorithm by adaptive multi-dictionary learning is adopted. Compared with the traditional global dictionary learning, this algorithm spends less time on dictionary training and image reconstruction to a great extent. Firstly, a sketch is transformed to a photo by eigenface algorithm. Secondly, super resolution reconstruction by improved adaptive multi-dictionary learning is used to reconstruct the synthesized photo, which is able to enhance the quality of synthesized photo effectively. Finally, the synthesized photo is recognized by two-dimensional marginal fisher analysis. We demonstrate these ideas in practice and show how they lead to faster operation speed and ideal recognition rate.	dictionary;facial recognition system;machine learning;super-resolution imaging	Jingjing Zhao;Yiming Mao;Qi Fang;Zhicheng Liang;Fumeng Yang;Shu Zhan	2015		10.1007/978-3-319-25417-3_18	computer vision;pattern recognition	Vision	29.568608761879045	-46.42029899751602	37453
592494839cf7910760e82fc2b3a15ca640505e84	learning hierarchical 3d kernel descriptors for rgb-d action recognition	rgb d action;kernel descriptor;action recognition	Human action recognition is an important and challenging task due to intra-class variation and complexity of actions which is caused by diverse style and duration in performed action. Previous works mostly concentrate on either depth or RGB data to build an understanding about the shape and movement cues in videos but fail to simultaneously utilize rich information in both channels. In this paper we study the problem of RGB-D action recognition from both RGB and depth sequences using kernel descriptors. Kernel descriptors provide an unified and elegant framework to turn pixel-level attributes into descriptive information about the performed actions in video. We show how using simple kernel descriptors over pixel attributes in video sequences achieves a great success compared to the state-of-the-art complex methods. Following the success of kernel descriptors (Bo, et al., 2010) on object recognition task, we put forward the claim that using 3D kernel descriptors could be an effective way to project the low-level features on 3D patches into a powerful structure which can effectively describe the scene. We build our system upon the 3D Gradient kernel descriptor and construct a hierarchical framework by employing efficient match kernel (EMK) (Bo, and Sminchisescu, 2009) and hierarchical kernel descriptors (HKD) as higher levels to abstract the mid-level features for classification. Through extensive experiments we demonstrate the proposed approach achieves superior performance on four standard RGB-D sequences benchmarks. © 2015 Elsevier Inc. All rights reserved.	benchmark (computing);call of duty: black ops;experiment;gradient;high- and low-level;kernel (operating system);outline of object recognition;pixel	Yu Kong;Behnam Satarboroujeni;Yun Fu	2016	Computer Vision and Image Understanding	10.1016/j.cviu.2015.10.001	computer vision;kernel method;string kernel;kernel embedding of distributions;radial basis function kernel;machine learning;pattern recognition;mathematics;tree kernel;polynomial kernel	Vision	29.95517227297722	-51.7844331353368	37528
23133099c004ce9641ef9cce1d9a5e373f4a36ea	a robust approach for ego-motion estimation using a mobile stereo platform	closed form solution;motion estimation;optical flow	We propose a robust approach to the problem of ego-motion estimation using a mobile stereo platform. Stereo is computed for every frame obtaining 3D points of the environment. In addition optical flow establishes correspondences between points in successive frames. A smoothness motion constraint is applied in order to detect flow vectors which are inconsistent with the current ego-motion. The optimal rotation and translation between the resulting clouds of static points is then computed using a closed form solution based on unit quaternions, providing the motion between the current and previous frame. Stabilization and a better estimation are achieved computing the observed motion between the current frame and many frames in the past in a multi-frame fashion. The integration of successive movements allows the reconstruction of the travelled path. Experimental results with sequences covering more than 1.5 kilometers of travelled distance are presented and compared with GPS and odometry.	computation;global positioning system;motion compensation;motion estimation;odometry;optical flow	Hernán Badino	2004		10.1007/978-3-540-69866-1_15	computer vision;simulation;geography;geodesy;motion estimation	Vision	52.1222188433059	-46.339353595442	37551
3b714b28471bb912a65c7c7262366862d6eb34fd	a least squares formulation for canonical correlation analysis	multi dimensional;least squares problem;canonical correlation analysis;high dimensional data;least square	Canonical Correlation Analysis (CCA) is a well-known technique for finding the correlations between two sets of multi-dimensional variables. It projects both sets of variables into a lower-dimensional space in which they are maximally correlated. CCA is commonly applied for supervised dimensionality reduction, in which one of the multi-dimensional variables is derived from the class label. It has been shown that CCA can be formulated as a least squares problem in the binaryclass case. However, their relationship in the more general setting remains unclear. In this paper, we show that, under a mild condition which tends to hold for high-dimensional data, CCA in multi-label classifications can be formulated as a least squares problem. Based on this equivalence relationship, we propose several CCA extensions including sparse CCA using 1-norm regularization. Experiments on multi-label data sets confirm the established equivalence relationship. Results also demonstrate the effectiveness of the proposed CCA extensions.	dimensionality reduction;least squares;multi-label classification;sparse matrix;turing completeness	Liang Sun;Shuiwang Ji;Jieping Ye	2008		10.1145/1390156.1390285	econometrics;canonical correspondence analysis;canonical correlation;combinatorics;computer science;machine learning;mathematics;least squares;statistics;clustering high-dimensional data	ML	26.475734302374306	-37.222189612346654	37574
a791269782c34a8c7aefe960bbb925f6581f3003	grid-based scan-to-map matching for accurate 2d map building	normal distribution;kullback leibler divergence;journal article;scan matching;mapping;grid map	This paper presents a grid-based scan-to-map matching technique for accurate 2D map building. At every acquisition of a new scan, the proposed technique matches the new scan to the previous scan similarly to the conventional techniques, but further corrects the error by matching the new scan to the globally defined map. In order to achieve best scan-to-map matching at each acquisition, the map is represented as a grid map with multiple normal distributions (NDs) in each cell, which is one contribution of this paper. Additionally, the new scan is also represented by NDs, developing a novel ND-to-ND matching technique. This ND-to-ND matching technique has significant potential in the enhancement of the global matching as well as the computational efficiency. Experimental results first show that the proposed technique accumulates very small errors after consecutive matchings and identifies that the scans are matched better to the map with the multi-ND representation than one ND representation. The proposed technique is then tested in a number of large indoor environments, including public domain datasets and the applicability to real world problems is demonstrated.	computation;map matching;matching (graph theory);norsk data	Kunjin Ryu;Lakshitha Dantanarayana;Tomonari Furukawa;Gamini Dissanayake	2016	Advanced Robotics	10.1080/01691864.2015.1124025	normal distribution;computer vision;mathematical optimization;machine learning;mathematics;kullback–leibler divergence;grid reference	Robotics	48.22043996647056	-51.91953260145996	37600
6c7c9464c15a922a96376606005f118dbaea0610	a principal varying-coefficient model for quantile regression: joint variable selection and dimension reduction		A principal varying-coefficient model for quantile regression based on regression splines estimation is proposed. Convergence rate and local asymptotics for the coefficient functions are then derived. Furthermore, penalization is used to obtain joint variable selection and dimension reduction in quantile varying-coefficient models. A group coordinate descent algorithm is adopted for a computationally efficient implementation. Simulations are carried out to investigate the finite sample performance and an application on a real data set is presented.	coefficient;dimensionality reduction;feature selection	Weihua Zhao;Xuejun Jiang;Heng Lian	2018	Computational Statistics & Data Analysis	10.1016/j.csda.2018.05.021	rate of convergence;multivariate adaptive regression splines;econometrics;statistics;dimensionality reduction;quantile;feature selection;coordinate descent;quantile regression;mathematics;asymptotic distribution	ML	29.64146660306378	-25.54365867991247	37609
1a536ab895939c8b761fb24eda192f7fc79a5813	fast and accurate image classification with histogram based features and additive kernel svm	histograms;kernel;standards;support vector machines;polynomials;additives;accuracy;remote sensing additive kernels support vector machines histogram intersection kernel chi square kernel histogram based features;kernel histograms support vector machines accuracy additives standards polynomials;support vector machines image classification operating system kernels remote sensing;additive kernel support vector machine classification kernel based image classification methods critical classification problems remote sensing additive kernel function histogram based feature representations histogram intersection kernel chi square kernel classification phase classification accuracy computational time	Kernel-based image classification methods rely on the considered kernel functions that can be chosen with respect to prior information on the adopted features. In remote sensing, histogram features have recently gained an increasing interest due to their capability to address several critical classification problems (e.g., the problem of curse of dimensionality) when appropriate kernels and classifiers are selected. In view of that, in this paper we introduce in remote sensing additive kernels in the context of support vector machine classification (AK-SVM), which are suitable kernels for histogram based feature representations. In particular, we investigate the Histogram Intersection kernel and the chi-square kernel within the AK-SVM. Moreover, we present fast implementations of the AK-SVM to significantly speed up the classification phase of the SVM. Experimental results show the effectiveness of the AK-SVM in terms of classification accuracy and computational time when compared to SVMs with standard kernels.	computation;computer vision;curse of dimensionality;kernel (operating system);reed–solomon error correction;speedup;support vector machine;time complexity;utility functions on indivisible goods	Begüm Demir;Lorenzo Bruzzone	2015	2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2015.7326280	kernel;support vector machine;least squares support vector machine;kernel method;kernel;string kernel;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;food additive;histogram matching;machine learning;pattern recognition;data mining;graph kernel;histogram;mathematics;accuracy and precision;bag-of-words model in computer vision;tree kernel;kernel;variable kernel density estimation;polynomial kernel;statistics;polynomial;kernel smoother	ML	30.69835624386684	-42.538599680158335	37611
f88d9eb4a545caa7ab460a11ee035e5c47dadf82	3d gait recognition based on functional pca on kendall's shape space		In this paper we propose a novel gait recognition approach from animated 3D skeletal data. Our approach is based on two disparate ideas from Shape Analysis and Functional Data Analysis (FDA) for a joint geometric-functional analysis. That is, skeletal sequences are viewed as time-parametrized trajectories on the Kendall's shape space when scaling, translation and rotation variations are filtered out from fixed-time 3D skeletons. A Riemannian Functional Principal Component Analysis (RFPCA) is carried out on our manifold-valued trajectories in order to build a new basis of principal functions, termed EigenTrajectories. Thus, each trajectory, could be projected into the eigenbasis which give rise to a compact signature, or EigenScores. The latter is fed to pre-trained ‘One-vs-All’ SVM classifiers for identity recognition and authentication. Based on the geometry of the underlying shape space, tools for re-sampling and synchronizing trajectories are naturally derived to apply the proposed variant of FPCA. We have conducted experiments on a subset of the CMU dataset. Our approach shows promising results compared to the state-of-the-art when a compact and robust signature is considered.	authentication;autistic disorder;experiment;functional data analysis;functional principal component analysis;gait analysis;generic drugs;image scaling;international conference on functional programming;projections and predictions;sampling (signal processing);shape analysis (digital geometry);silo (dataset);skeleton;subgroup;test scaling;manifold	Nadia Hosni;Hassen Drira;Faten Chaieb;Boulbaba Ben Amor	2018	2018 24th International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2018.8545040	support vector machine;riemannian geometry;artificial intelligence;scaling;principal component analysis;shape analysis (digital geometry);pattern recognition;functional principal component analysis;functional data analysis;trajectory;computer science	Vision	37.38719290643804	-50.49755655393894	37624
a58c828831cf27eac8fe65b80042ffc93b040dbf	motion key-frames extraction based on amplitude of distance characteristic curve	key frames extraction;distance features;amplitude;insert and merge;pca	The key frames extraction technique extracts key postures to describe the original motion sequence, which has been widely used in motion compression, motion retrieval, motion edition and so on. In this paper, we propose a method based on the amplitude of curve to find key frames in a motion captured sequence. First we select a group of joint distance features to represent the motion and adopt the Principal Component Analysis (PCA) method to obtain the one dimension principal component as a features curve which will be used. Then we gain the initial key-frames by extracting the local optimum points in the curve. At last, we get the final key frames by inserting frames based on the amplitude of the curve and merging key frames too close. A number of experimental examples demonstrate that our method is practicable and efficient not only in the visual performance but also in the aspect of the compression ratio and error rate.	bit error rate;data compression ratio;key frame;local optimum;principal component analysis	Qiang Zhang;Xiang Xue;Dongsheng Zhou;Xiaopeng Wei	2014	Int. J. Comput. Intell. Syst.	10.1080/18756891.2013.859873	computer vision;quarter-pixel motion;machine learning;pattern recognition;motion estimation;mathematics;amplitude;principal component analysis	Vision	37.00097899639098	-52.01617379859298	37630
23edf233a4493f547f715a6124306a15997e2506	multiple scaled person re-identification framework for hd video surveillance application		Person re-identification is an important problem in auto- mated video surveillance. It remains challenging in terms of extraction of reliable and distinctive features, and matching of the features under differ- ent camera views. In this paper, we propose a novel re-identification strat- egy for person re-identification based on multiple image scaled framework. Specifically, global features and local features are extracted separately in different image scales. These two-scaled processing are constructed in a cascaded system. We use semi-supervised SVM to obtain a similarity func- tion for global features and a similarity function combining the spatial con- straint and salience weight for local features. Experiments are conducted on two datasets: ETHZ and our dataset with high resolution. Experimen- tal results demonstrate that the proposed method outperforms the con- ventional method in terms of both accuracy and efficiency.	closed-circuit television	Hua Yang;Xinyu Wang;Wenqi Ma;Hang Su;Ji Zhu	2015		10.1007/978-3-662-48570-5_21	computer vision;simulation;geography;data mining	Vision	34.45062624586235	-51.739888761222815	37664
20b848e56595802e16b866e387ff7087ea7ca24e	object reidentification in real world scenarios across multiple non-overlapping cameras		In a world where surveillance cameras are at every street corner, there is a growing need for synergy among cameras as well as the automation of the data analysis process. This paper deals with the problem of reidentification of objects in a set of multiple cameras inputs without any prior knowledge of the cameras distribution or coverage. The proposed approach is robust to change of scale, lighting conditions, noise and viewpoints among cameras, as well as object rotation and unpredictable trajectories. Both novel and traditional features are extracted from the object. Light and noise invariance is achieved using textural features such as oriented gradients, color ratio and color saliency. A probabilistic framework is used incorporating the different features into a human probabilistic model. Experimental results show that textural features improve the reidentification rate and the robustness of the recognition process compared with other state-of-the-art algorithms.	algorithm;closed-circuit television;gradient;image noise;statistical model;synergy	Guy Berdugo;Omri Soceanu;Yair Moshe;Dmitry Rudoy;Itsik Dvir	2010	2010 18th European Signal Processing Conference		computer vision;geography;pattern recognition;computer graphics (images)	Vision	43.14410439109338	-46.58321533875963	37685
4bf676fabec54dbbccf8fe486693328ca21d8168	capturing the dynamics of pedestrian traffic using a machine vision system	frames per second;2 dimensional;machine vision;image sequence	We developed a machine vision system to automatically capture the dynamics of pedestrians under four different traffic scenarios. By considering the overhead view of each pedestrian as a digital object, the system processes the image sequences to track the pedestrians. Considering the perspective effect of the camera lens and the projected area of the hallway at the top-view scene, the distance of each tracked object from its original position to its current position is approximated every video frame. Using the approximated distance and the video frame rate (30 frames per second), the respective velocity and acceleration of each tracked object are later derived. The quantified motion characteristics of the pedestrians are displayed by the system through 2-dimensional graphs of the kinematics of motion. The system also outputs video images of the pedestrians with superimposed markers for tracking. These visual markers were used to visually describe and quantify the behavior of the pedestrians under different traffic scenarios.	approximation algorithm;bird's-eye view;frame language;machine vision;overhead (computing);velocity (software development);virtual artifact	Louie Vincent A. Ngoho;Jaderick P. Pabico	2015	CoRR		computer vision;two-dimensional space;simulation;machine vision;computer science;frame rate;computer graphics (images)	Vision	48.59466246449112	-42.60245548897214	37693
f2260902b97f12dc7745e02fc7cb0aae6cc61cb4	truncation error compensation in kernel machines	time series forecasting;least mean squares methods;support vector machines;time series;online learning;kernel finite wordlength effects time series analysis vectors equations prediction algorithms haptic interfaces;kernel machine;time series error compensation learning artificial intelligence least mean squares methods regression analysis support vector machines;online learning kernel machine time series prediction time series forecasting;error compensation;regression analysis;learning artificial intelligence;finite storage buffer truncation error compensation kernel machines time series data prediction time series data analysis intelligent systems unreliable data communication kernel based regression algorithms nonlinear relationship prediction smooth delta corrected kernel least mean square algorithm sdc klms algorithm linear time;time series prediction	The analysis and prediction of time series data has played an important role for intelligent systems used in the area of cybernetics and human-machine interaction. Time series prediction is especially important in the case of unreliable communication of data acquired by intelligent systems. Computationally efficient kernel based regression algorithms have allowed for the prediction of non-linear relationships within time series data. In this paper, we present the smooth delta corrected kernel least mean square (SDC-KLMS) algorithm. The SDC-KLMS scales in linear time with the number of samples stored, hence making it computationally efficient. We present a theoretical motivation for our algorithm and we experimentally show how our approach overcomes a limitation imposed by the use of a finite storage buffer. Experiments with simulated, benchmark, and real world data were conducted to verify the accuracy of our algorithm.	algorithm;benchmark (computing);computational complexity theory;cybernetics;data buffer;error detection and correction;experiment;human–computer interaction;kernel (operating system);kernel method;kerrison predictor;mean squared error;nonlinear system;smart data compression;smoothing;time complexity;time series;truncation error	Jason P. Rhinelander;Peter Xiaoping Liu	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.325	kernel;principal component regression;least squares support vector machine;kernel method;econometrics;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;computer science;machine learning;time series;pattern recognition;variable kernel density estimation;polynomial kernel;statistics	Robotics	31.856076142370558	-33.44877534688429	37738
c26694c4cf634afa39d0d3f2a05ca14efec17641	pedestrian segmentation in infrared images based on circular shortest path	histograms;image segmentation;cost function;segmentation infrared pedestrian circular shortest path;contour extraction infrared pedestrian image segmentation algorithm circular shortest path saliency mapping gray thresholding human shape feature regional polar transformation human shape coefficient cost function;infrared imagery;shortest path algorithms;pedestrians;shape;pedestrians feature extraction image segmentation infrared imaging;feature extraction;intelligent vehicles;transportation;image analysis;detection and identification systems;image segmentation shape histograms cost function transportation feature extraction	A novel infrared pedestrian segmentation algorithm based on the circular shortest path is proposed. The foreground containing pedestrians is estimated by saliency mapping and gray thresholding. In the foreground area, the human shape feature is introduced by a regional polar transformation. By adding the human shape coefficient to the object term of the cost function, the extracted contour can fit the human shape well while excluding most false alarms. The proposed algorithm performs better in areas with low contrast, and obtains good segmentation quantitatively and qualitatively.	algorithm;coefficient;experiment;loss function;region of interest;shortest path problem;thresholding (image processing)	Xiangzhi Bai;Fugen Zhou	2016	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2016.2516342	computer vision;transport;image analysis;feature extraction;shape;computer science;machine learning;pattern recognition;histogram;image segmentation;shortest path problem	Vision	43.08057878223898	-45.40933013400995	37750
f9d2a91b101d4ef54ffe0649cc1af5846d66a9d2	moderately clipped lasso	clipped lasso;variable selection;mcp;high dimension;lasso	The least absolute shrinkage and selection operator (LASSO) has been widely used in high-dimensional linear regression models. However, it is known that the LASSO selects too many noisy variables. In this paper, we propose a new estimator, the moderately clipped LASSO (MCL), that deletes noisy variables successively without sacrificing prediction accuracy much. Various numerical studies are done to illustrate superiority of the MCL over other competitors.	lasso	Sunghoon Kwon;Sangin Lee;Yongdai Kim	2015	Computational Statistics & Data Analysis	10.1016/j.csda.2015.07.001	machine learning;lasso;pattern recognition;mathematics;feature selection;elastic net regularization;statistics	ML	24.844356652556943	-35.21087993599729	37779
093bd73706a7d49c48a9f303ce0418aa15fcecab	asymptotic justification of bandlimited interpolation of graph signals for semi-supervised learning	asymptotics graph signal processing semi supervised learning interpolation;interpolation;convergence;bandwidth convergence signal processing interpolation semisupervised learning laplace equations data models;low density separation problem asymptotic justification bandlimited interpolation graph signals semi supervised learning graph based methods;laplace equations;signal processing;source separation graph theory interpolation learning artificial intelligence;bandwidth;semisupervised learning;data models	Graph-based methods play an important role in unsupervised and semi-supervised learning tasks by taking into account the underlying geometry of the data set. In this paper, we consider a statistical setting for semi-supervised learning and provide a formal justification of the recently introduced framework of bandlimited interpolation of graph signals. Our analysis leads to the interpretation that, given enough labeled data, this method is very closely related to a constrained low density separation problem as the number of data points tends to infinity. We demonstrate the practical utility of our results through simple experiments.	bandlimiting;data point;experiment;interpolation;semi-supervised learning;semiconductor industry;supervised learning;unsupervised learning	Aamir Anis;Aly El Gamal;Amir Salman Avestimehr;Antonio Ortega	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7179015	spline interpolation;semi-supervised learning;data modeling;mathematical optimization;bilinear interpolation;convergence;interpolation;computer science;machine learning;signal processing;pattern recognition;mathematics;bandwidth;statistics	Vision	28.69765161570353	-37.67843139695176	37820
4fcd979b447d98b0c96a9854ae348fc0abdb2442	adaptive behavior navigation of a mobile robot	unsupervised learning;obstacle avoidance adaptive behavior navigation mobile robot neural network model reactive behavioral navigation competitive neural network control strategy sensor information learning operation reinforcement learning linear velocities angular velocities learning control;learning control;mobile robot;path planning;reinforcement learning;robot navigation;mobile robots;adaptive behavior;indexing terms;neurocontrollers unsupervised learning mobile robots path planning sensor fusion recurrent neural nets;obstacle avoidance;navigation mobile robots robot sensing systems infrared sensors neural networks learning robot control humans control systems psychology;neurocontrollers;recurrent neural nets;sensor fusion;neural network model;control strategy;neural network	This paper describes a neural network model for the reactive behavioral navigation of a mobile robot. From the information received through the sensors the robot can elicit one of several behaviors (e.g. stop, avoid, stroll, wall following), through a competitive neural network. The robot is able to develop a control strategy depending on sensor information and learning operation. Reinforcement learning improves the navigation of the robot by adapting the eligibility of the behaviors and determining the linear and angular robot velocities.	adaptive behavior;angularjs;artificial neural network;automated planning and scheduling;autonomy;control theory;mobile robot;network model;nomad 200;oracle fusion architecture;organizing (structure);performance;reinforcement learning;robotic mapping;self-organization;sensor;simulation	Eduardo Zalama Casanova;Jaime Gómez García-Bermejo;M. Paul;José R. Perán	2002	IEEE Trans. Systems, Man, and Cybernetics, Part A	10.1109/3468.995537	mobile robot;robot learning;computer vision;avm navigator;computer science;artificial intelligence;social robot;machine learning;robot control;mobile robot navigation;artificial neural network	Robotics	50.56517626359851	-29.208469395557383	37977
f63fe90030f6a85ac340ef24343e9b12a0e0916a	global symbolic maps from local navigation	real time;navigation system;terrain mapping;symbolic representation	In order to navigate autonomously, most robot systems are provided with some sort of global terrain map. To make storage practical, these maps usually have a high-level symbolic representation of the terrain. The robot’s symbolic map is then used to plan a local path. This paper describes a system which uses the reverse (and perhaps more natural) process. This system processes local sensor data in such a way as to allow efficient, reactive local navigation. A byproduct of this navigation process is an abstraction of the terrain information which forms a global symbolic terrain map of the terrain through which the robot has passed. Since this map is in the same format as that used by the local navigation system, the map is easy for the system to use, augment, or correct. Compared with the data from which the maps are created, the maps are very space efficient, and can be modified, or used for navigation in real-time. Experiments with this system, both in simulation, and with a real robot operating in natural terrain, are described.	high- and low-level;map;mental representation;real-time clock;real-time computing;robot;simulation	David P. Miller;Marc G. Slack	1991			computer vision;simulation;computer science;mobile robot navigation	Robotics	52.445795079122405	-31.42248084363376	37997
ceee695eba7f72af8948eee618b0523b75e9a184	road obstacle classification with attention windows	road obstacle classification;object detection driver information systems image classification image sensors learning artificial intelligence;multiple sensors;learning algorithm;training;image classification;laser radar;driver support;image sensors;driver support road obstacle classification road obstacle detection attention windows learning system multiple sensors competition based learning algorithm image descriptors intentional data mislabeling;learning system;image descriptors;three dimensional displays;radar imaging;laser radar radar imaging cameras vehicle detection sensor systems radar detection road vehicles object detection intelligent sensors image analysis;vehicles;learning artificial intelligence;classification accuracy;driver information systems;attention windows;cameras;competition based learning algorithm;intentional data mislabeling;object detection;road obstacle detection	A learning system for detection and classification of road obstacles, such as vehicles and non-vehicles, is proposed which utilizes information from multiple sensors. An advanced range sensor guides a selection of candidate images provided by the camera for subsequent analysis. A competition based learning algorithm is used to distinguish between representations of different obstacles. High classification accuracy is demonstrated in a realistic variety of driving conditions in the presence of intentional data mislabeling in the two-class setup with state-of-art image descriptors.	algorithm;computer vision;image sensor;microsoft windows;mobile operating system;on-board data handling;radar;vii;visual descriptor	Danil V. Prokhorov	2010	2010 IEEE Intelligent Vehicles Symposium	10.1109/IVS.2010.5548053	computer vision;simulation;geography;machine learning	Robotics	48.596088103409876	-36.50128463315022	37999
23fc3119c451a55715b8f042e7bb6454357db5da	projective registration with difference decomposition	piecewise projective transformations;image recognition;computer vision application software target tracking computer errors laboratories prototypes image recognition surveillance computer graphics image coding;image coding;image segmentation;performance evaluation;application software;surveillance;computer graphics;prototypes;performance;motion estimation;image region registration;computer vision image registration image segmentation performance evaluation motion estimation;nonplanar object motion modeling;computer vision;difference decomposition;image registration;nonlinear transformations;linear transformation;prototype implementation;image transformations;target tracking;projective registration;prototype implementation projective registration difference decomposition image region registration performance image transformations nonlinear transformations piecewise projective transformations nonplanar object motion modeling;computer errors	Current methods for registering image regions perform well for simple transformations or the large image regions. In this paper, we present a new method that is better able to handle small image regions as they deform with non-linear transformations. We introduce a novel approach to solving the registration problem. The method is a generalization of previous methods and can better handle non-linear transforms. Although the methods are general, we focus on projective transformations and introduce transformations for modeling the motions of non-planar objects. We conclude with examples from our prototype implementation.	non-deterministic turing machine;nonlinear system;prototype	Michael Gleicher	1997		10.1109/CVPR.1997.609345	computer vision;application software;performance;computer science;image registration;theoretical computer science;motion estimation;prototype;linear map;image segmentation;computer graphics;computer graphics (images)	Vision	51.16435188386805	-49.30345878821261	38016
caea0a276a6ce22c2607e18a3f725ee7937caf1c	human tracking using a far-infrared sensor array and a thermo-spatial sensitive histogram		We propose a human body tracking method using a farinfrared sensor array. A far-infrared sensor array captures the spatial distribution of temperature as a low-resolution image. Since it is difficult to identify a person from the low-resolution thermal image, we can avoid privacy issues. Therefore, it is expected to be applied for the analysis of human behaviors in various places. However, it is difficult to accurately track humans because of the lack of information sufficient to describe the feature of the target human body in the low-resolution thermal image. In order to solve this problem, we propose a thermo-spatial sensitive histogram suitable to represent the target in the low-resolution thermal image. Unlike the conventional histograms, in case of the thermo-spatial sensitive histogram, a voting value is weighted depending on the distance to the target’s position and the difference from the target’s temperature. This histogram allows the accurate tracking by representing the target with multiple histograms and reducing the influence of the background pixels. Based on this histogram, the proposed method tracks humans robustly to occlusions, pose variations, and background clutters. We demonstrate the effectiveness of the method through an experiment using various image sequences.	experiment;gesture recognition;pixel;poor posture;privacy;sampling (signal processing)	Takashi Hosono;Tomokazu Takahashi;Daisuke Deguchi;Ichiro Ide;Hiroshi Murase;Tomoyoshi Aizawa;Masato Kawade	2014		10.1007/978-3-319-16631-5_20	pattern recognition;artificial intelligence;spatial distribution;computer science;computer vision;pixel;sensor array;histogram;far infrared	Vision	45.10125373580627	-43.889614150283805	38041
36c9731f24e5daa42c1e2c6c68258567dfa78a0a	movement tracking in terrain conditions accelerated with cuda	video cameras object tracking parallel architectures real time systems;image processing;algorytmy;computer architecture;visualization;analiza obrazu;obrazy cyfrowe;graphics processing units;algorithms;image analysis;graphics processing units matlab tracking cameras image processing computer architecture visualization;digital images;matlab;cuda technology movement tracking terrain conditions video cameras leaves fluttering grass waving smoke fog image processing method real time system;cameras;tracking	The paper presents a solution to the problem of movement tracking in images acquired from video cameras monitoring outside terrain. The solution is resistant to such adverse factors as: leaves fluttering, grass waving, smoke or fog, movement of clouds etc. The presented solution is based on well known image processing methods, nevertheless the key was the use of an appropriate conduct procedure. In order to obtain a real-time system the CUDA technology was involved.	cuda;image processing;real-time computing;real-time transcription	Piotr Sklodowski;Witold Zorski	2014	2014 Federated Conference on Computer Science and Information Systems	10.15439/2014F282	computer vision;image analysis;simulation;visualization;image processing;computer science;tracking;digital image;computer graphics (images)	Vision	44.32108822049643	-42.06497223522652	38082
67d0e75d7f64570e11e93d24fb4a820b1cb5a1bd	robust active perception via data-association aware belief space planning		We develop a belief space planning (BSP) approach that advances the state of the art by incorporating reasoning about data association (DA) within planning, while considering additional sources of uncertainty. Existing BSP approaches typically assume data association is given and perfect, an assumption that can be harder to justify while operating, in presence of localization uncertainty, in ambiguous and perceptually aliased environments. In contrast, our data association aware belief space planning (DA-BSP) approach explicitly reasons about DA within belief evolution, and as such can better accommodate these challenging real world scenarios. In particular, we show that due to perceptual aliasing, the posterior belief becomes a mixture of probability distribution functions, and design cost functions that measure the expected level of ambiguity and posterior uncertainty. Using these and standard costs (e.g. control penalty, distance to goal) within the objective function, yields a general framework that reliably represents action impact, and in particular, capable of active disambiguation. Our approach is thus applicable to robust active perception and autonomous navigation in perceptually aliased environments. We demonstrate key aspects in basic and realistic simulations.	active galactic nucleus;aliasing;automated planning and scheduling;autonomous robot;correspondence problem;language localisation;loss function;np-completeness;norm (social);occam's razor;optimization problem;scalability;simulation;thresholding (image processing);unified framework;word-sense disambiguation	Shashank Pathak;Antony Thomas;Asaf Feniger;Vadim Indelman	2016	CoRR		computer vision;belief structure;artificial intelligence;mathematics	AI	33.151921130153234	-34.14962249189805	38183
f889650c6b90992a7d3e0a7116a63216491ec7f3	scene segmentation assisted by stereo vision	3d;image color analysis geometry clustering algorithms stereo vision three dimensional displays image segmentation algorithm design and analysis;cluster algorithm;stereo image processing computational geometry image colour analysis image reconstruction image segmentation;image segmentation;image processing;scene segmentation;stereo vision systems;geometry;computational geometry;color information;three dimensional;three dimensional displays;image color analysis;image colour analysis;image reconstruction;stereo image processing;stereo vision;clustering algorithms;stereo;3d scene segmentation stereo;3d geometry;framed scene;3d structure;color information scene segmentation stereo vision systems 3d reconstruction 3d geometry framed scene;algorithm design;algorithm design and analysis;3d reconstruction;scene analysis	Stereo vision systems for 3D reconstruction have been deeply studied and are nowadays capable to provide a reasonably accurate estimate of the 3D geometry of a framed scene. They are commonly used to merely extract the 3D structure of the scene. However, a great variety of applications is not interested in the geometry itself, but rather in scene analysis operations, among which scene segmentation is a very important one. Classically, scene segmentation has been tackled by means of color information only, but it turns out to be a badly conditioned image processing operation which remains very challenging. This paper proposes a new framework for scene segmentation where color information is assisted by 3D geometry data, obtained by stereo vision techniques. This approach resembles in some way what happens inside our brain, where the two different views coming from the eyes are used to recognize the various object in the scene and by exploiting a pair of images instead of just one allows to greatly improve the segmentation quality and robustness. Clearly the performance of the approach is dependent on the specific stereo vision algorithm used in order to extract the geometry information. This paper investigates which stereo vision algorithms are best suited to this kind of analysis. Experimental results confirm the effectiveness of the proposed framework and allow to properly rank stereo vision systems on the basis of their performances when applied to the scene segmentation problem.	3d reconstruction;algorithm;computer vision;image processing;performance;scene graph;stereopsis	Carlo Dal Mutto;Pietro Zanuttigh;Guido M. Cortelazzo;Stefano Mattoccia	2011	2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission	10.1109/3DIMPVT.2011.16	computer stereo vision;stereo cameras;computer vision;computer science;multimedia;computer graphics (images)	Vision	52.18419591896764	-51.54972930600316	38244
4b1ea90855057d320c5da3cffdfac0e1bc86a3d5	fusion of geometric models from vls overlapping profiles	random sample consensus;vls fusion overlapping profiles ransac geometric model;fusion;random sampling;ransac;local planes;laser ranging;maximum likelihood estimation;maximum a posteriori estimation;fitting;3d model;numerical analysis;estimation;overlapping profiles;three dimensional displays;feature extraction;clouds;vehicle based laser scanning;solid modeling;map estimation;line segment extraction;geometric model;laser scanning;geometric models;profile indicators;vls;sensor fusion;three dimensional displays clouds solid modeling laser modes fitting object oriented modeling estimation;solid modelling feature extraction laser ranging maximum likelihood estimation sensor fusion;laser modes;object oriented modeling;trial and error strategy;solid modelling;trial and error strategy geometric models vehicle based laser scanning overlapping profiles line segment extraction random sample consensus local planes profile indicators maximum a posteriori estimation	This study advances a new method for fusion of geometric models from the overlapping profiles collected by vehicle-based laser scanning (VLS), which is developed as a novel mapping technique. The schematic starts from line segment extraction based on random sample consensus (RANSAC) for each profile, and the resulted line segments are grouped. Then, the grouped line segments are fitted to local planes, and the local planes involved in different profile indicators are fused based on maximum a posteriori (MAP) estimation. Finally, the achieved 3D models are refined under the trial-and-error strategy. The mean distances from the fused local planes to the related inlier points are all less than 0.3 m, and numerical analysis based on the real-measured VLS data has validated the new method.	3d modeling;numerical analysis;random sample consensus;schematic;von luschan's chromatic scale	Yi Lin;Juha Hyyppä	2010	2010 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2010.5650072	laser scanning;sampling;computer vision;estimation;ransac;simulation;fusion;feature extraction;numerical analysis;computer science;maximum a posteriori estimation;geometric modeling;pattern recognition;sensor fusion;maximum likelihood;solid modeling;statistics	Robotics	53.49554936354108	-43.77826884464831	38282
103a4bc6f1e5e45a5008ed9a2fece3bdea28f2de	a vision based system for attitude estimation of uavs	vision system;histograms;uav;aircraft control;image segmentation;010202 biological mathematics;unmanned aerial vehicle;remotely operated vehicles aircraft control computer vision image colour analysis;remotely operated vehicles;computer vision;brightness;machine vision unmanned aerial vehicles aircraft position measurement monitoring lenses mirrors sun awards and recognition committee australia;080106 image processing;image color analysis;image colour analysis;unmanned aerial vehicle vision based system attitude estimation uav image intensity image color analysis aircraft attitude;machine vision;970109 expanding knowledge in engineering;pixel;image intensity;vision based system;attitude estimation;970106 expanding knowledge in the biological sciences;170112 sensory processes perception and performance;aircraft attitude;aircraft	This paper describes a technique for estimating the attitude of a UAV by monitoring the visual horizon. An algorithm is developed that makes the best use of color and intensity information in an image to determine the position and orientation of the horizon, and infer the aircraft's attitude. The technique is accurate, reliable, and fully capable of real-time operation. Furthermore, it can be incorporated into any existing vision system, irrespective of the way in which the environment is imaged (e.g. through lenses or mirrors).	algorithm;algorithmic efficiency;camera resectioning;color space;effective method;error detection and correction;mathematical optimization;pixel;real-time clock;real-time computing;thresholding (image processing);unmanned aerial vehicle	Saul Thurrowgood;Dean Soccol;Richard James Donald Moore;Daniel Peter Bland;Mandyam V. Srinivasan	2009	2009 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2009.5354041	remotely operated underwater vehicle;computer vision;simulation;machine vision;attitude indicator;computer science;engineering;histogram;image segmentation;brightness;pixel;remote sensing	Robotics	49.57116419871913	-40.38587634339031	38311
06c358aa12b6530825f76e8ea394ba471a21c6cb	computer-based training system for simulating wrist arthroscopy	minimally invasive;wrist;real time;training;wrist arthroscopy;virtual reality;biomedical imaging;force feedback;computational modeling;collision detection;haptic interface computer based training system wrist arthroscopy bones 3d virtual representation real time collision detection force feedback device;solid modeling;computer based training system;computer based training;real time collision detection;surgery;bones 3d virtual representation;biomedical education;haptic interfaces;convex hull;force feedback device;computer simulation;computational modeling computer simulation wrist minimally invasive surgery costs pain virtual prototyping bones object detection force feedback;virtual reality biomedical education computer based training haptic interfaces;haptic interface;medical students	The minimally invasive approach of arthroscopy means less pain and faster recovery time for patients compared to open surgery. However, it implies a high difficulty of performance. In this paper, a functional prototype of a computer-based training simulator for wrist arthroscopy is introduced. A 3-D virtual representation of the bones constituting the wrist of a patient is shown. Objects are modeled using the convex hull approaches and an algorithm to simulate real time collision detection during the training on the operation is presented. In addition, a force feedback device is used as a haptic interface with the computer simulation system. This leads in the development of a low cost system that is used by trainees with the same benefits as professional devices. In this regard, the wrist arthroscopy can be simulated and medical students can learn the basic skills required with safety, flexibility and less cost.	algorithm;collision detection;computer performance;computer simulation;convex hull;haptic technology;prototype	Fadi Yaacoub;Yskandar Hamam;Antoine Abche	2008	2008 21st IEEE International Symposium on Computer-Based Medical Systems	10.1109/CBMS.2008.67	computer simulation;medical imaging;simulation;computer science;artificial intelligence;virtual reality;haptic technology;surgery	Robotics	39.55335459828312	-37.950164485514385	38321
486ee9d420fd1eba1006221d3d61931563bc4115	detach and adapt: learning cross-domain disentangled deep representation		While representation learning aims to derive interpretable features for describing visual data, representation disentanglement further results in such features so that particular image attributes can be identified and manipulated. However, one cannot easily address this task without observing ground truth annotation for the training data. To address this problem, we propose a novel deep learning model of Cross-Domain Representation Disentangler (CDRD). By observing fully annotated source-domain data and unlabeled target-domain data of interest, our model bridges the information across data domains and transfers the attribute information accordingly. Thus, cross-domain feature disentanglement and adaptation can be jointly performed. In the experiments, we provide qualitative results to verify our disentanglement capability. Moreover, we further confirm that our model can be applied for solving classification tasks of unsupervised domain adaptation, and performs favorably against state-of-the-art image disentanglement and translation methods.		Yen-Cheng Liu;Yu-Ying Yeh;Tzu-Chien Fu;Sheng-De Wang;Wei-Chen Chiu;Yu-Chiang Frank Wang	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00924	visualization;artificial neural network;machine learning;artificial intelligence;pattern recognition;deep learning;task analysis;computer science;data domain;ground truth;feature learning;annotation	Vision	24.633697010768454	-49.19641964683051	38337
55a6222d73bf517ec81453c60b5c469ce715d031	multi-gpu based camera network system keeps privacy using growing neural gas	video surveillance;multigpu architecture multigpu based camera network system privacy multicamera surveillance system self organizing neural networks user gestures digital home graphic processor units multiple acquisition devices video frequency growing neural gas models image sequences multicamera network multisource data parallel processing;graphics processing unit neurons cameras instruction sets computer architecture surveillance acceleration;gpu;multiview video processing;multi core gng gpu camera networks privacy multiview video processing visual surveillance cuda;cuda;visual surveillance;self organising feature maps;graphics processing units;video surveillance graphics processing units image sequences multiprocessing systems self organising feature maps;multiprocessing systems;camera networks;gng;privacy;multi core;image sequences	In this work we present a multi-camera surveillance system based on the use of self-organizing neural networks to represent events in video. The objectives include: identifying and tracking persons or objects in the scene or the interpretation of user gestures for interaction with services, devices and systems implemented in the digital home. Additionally, the system process several tasks in parallel using GPUs (Graphic Processor Units). Addressing multiple vision tasks of various levels such as segmentation, representation or characterization, analysis and monitoring of the movement to allow the construction of a robust representation of their environment and interpret the elements of the scene.	graphics processing unit;neural gas	Sergio Orts;José García Rodríguez;Vicente Morell;Jorge Azorín López;Juan Manuel García Chamizo	2012		10.1109/IJCNN.2012.6252805	multi-core processor;embedded system;computer vision;computer science;privacy;computer graphics (images)	Vision	44.76126907726325	-36.795033245175624	38360
7da261ca2cff495bd7744504797c4c3d5b8b9b4c	particle phd filter based multiple human tracking using online group-structured dictionary learning		An enhanced sequential Monte Carlo probability hypothesis density (PHD) filter-based multiple human tracking system is presented. The proposed system mainly exploits two concepts: a novel adaptive gating technique and an online group-structured dictionary learning strategy. Conventional PHD filtering methods preset the target birth intensity and the gating threshold for selecting real observations for the PHD update. This often yields inefficiency in false positives and missed detections in a cluttered environment. To address this issue, a measurement-driven mechanism based on a novel adaptive gating method is proposed to adaptively update the gating sizes. This yields an accurate approach to discriminate between survival and residual measurements by reducing the clutter inferences. In addition, online group-structured dictionary learning with a maximum voting method is used to robustly estimate the target birth intensity. It enables the new-born targets to be automatically detected from noisy sensor measurements. To improve the adaptability of our group-structured dictionary to appearance and illumination changes, we employ the simultaneous code word optimization algorithm for the dictionary update stage. Experimental results demonstrate our proposed method achieves the best performance amongst state-of-the-art random finite set-based methods, and the second best online tracker ranked on the leaderboard of latest MOT17 challenge.	algorithm;clutter;code word;dictionary;machine learning;mathematical optimization;monte carlo method;sensor;tracking system	Zeyu Fu;Pengming Feng;Federico Angelini;Jonathon A. Chambers;Syed Mohsen Naqvi	2018	IEEE Access	10.1109/ACCESS.2018.2816805	residual;filter (signal processing);computer science;distributed computing;adaptability;code word;noise measurement;tracking system;particle filter;gating;artificial intelligence;pattern recognition	Vision	42.62245227863019	-48.79285708599007	38409
6f2933d3e88fab47decfb6373891c0df802eaf95	accurate fall detection by nine-axis imu sensor		Fall related injuries are a central problem for the elderly people, therefore many automated fall detectors have been developed. But prevalent methods are neither practical nor poor in accuracy. This paper proposes a novel fall detection algorithm using accelerometers, gyroscopes and magnetometers. In our study, we divide human activities into two categories: lying posture and no-lying posture. We assume that a lying posture is detected after falls. The proposed algorithm has three steps: quaternion Kalman filter, posture recognition, activity intensity analysis. The data is obtained by using nine-axial inertial measurement unit attached on the waist. Using the quaternion Kalman filer the system can obtain body's posture vectors measured in the frame of reference of the ground. The body's posture vectors include Euler angles, quaternion, acceleration. The Euler angles are used to determine the lying posture or no-lying posture. The quaternion and acceleration are used to analyze activity intensity when lying posture are detected. The proposed method features low computational cost and real-time response, in addition has a nice accuracy and convenient in detect falls.	algorithm;algorithmic efficiency;apache axis;computation;emoticon;euler characteristic;kalman filter;netapp filer;poor posture;real-time clock;sensor	Yuanzhong Yan;Yongsheng Ou	2017	2017 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2017.8324524	computer vision;acceleration;euler angles;quaternion;control theory;frame of reference;accelerometer;inertial measurement unit;kalman filter;engineering;artificial intelligence;gyroscope	Robotics	44.87306375499513	-44.61487787792589	38489
2662e0fd7ef5e7a9a66cb2bff9aa5438647d5ae2	application of principal component analysis in weighted stacking of seismic data		Optimal stacking of multiple data sets plays a significant role in many scientific domains. The quality of stacking will affect the signal-to-noise ratio and amplitude fidelity of the stacked image. In seismic data processing, the similarity-weighted stacking makes use of the local similarity between each trace and a reference trace as the weight to stack the flattened prestack seismic data after normal moveout correction. The traditional reference trace is an approximated zero-offset trace that is calculated from a direct arithmetic mean of the data matrix along the spatial direction. However, in the case that the data matrix contains abnormal misaligned trace, erratic, and non-Gaussian random noise, the accuracy of the approximated zero-offset trace would be greatly affected, and thereby further influence the quality of stacking. We propose a novel weighted stacking method that is based on principal component analysis. The principal components of the data matrix, namely, the useful signals, are extracted based on a low-rank decomposition method by solving an optimization problem with a low-rank constraint. The optimization problem is solved via a common singular value decomposition algorithm. The low-rank decomposition of the data matrix will alleviate the influence of abnormal trace, erratic, and non-Gaussian random noise, and thus will be more robust than the traditional alternatives. We use both synthetic and field data examples to show the successful performance of the proposed approach.	approximation algorithm;constrained optimization;constraint (mathematics);independent component analysis;low-rank approximation;mathematical optimization;noise (electronics);optimization problem;principal component analysis;signal-to-noise ratio;singular value decomposition;stacking;synthetic intelligence	Jianyong Xie;Wei Chen;Dong Zhang;Shaohuan Zu;Yangkang Chen	2017	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2017.2703611	decomposition method (constraint satisfaction);mathematics;normal moveout;principal component analysis;mathematical optimization;data set;matrix decomposition;singular value decomposition;optimization problem;signal-to-noise ratio	Vision	26.686502103589277	-37.832379917709204	38523
55c6181ad66a1086badffb379502d7b062037bbb	descriptor ensemble: an unsupervised approach to descriptor fusion in the homography space		With the aim to improve the performance of feature matching, we present an unsupervised approach to fuse various local descriptors in the space of homographies. Inspired by the observation that the homographies of correct feature correspondences vary smoothly along the spatial domain, our approach stands on the unsupervised nature of feature matching, and can select a good descriptor for matching each feature point. Specifically, the homography space serves as the common domain, in which a correspondence obtained by any descriptor is considered as a point, for integrating various heterogeneous descriptors. Both geometric coherence and spatial continuity among correspondences are considered via computing their geodesic distances in the space. In this way, mutual verification across different descriptors is allowed, and correct correspondences will be highlighted with a high degree of consistency (i.e., short geodesic distances here). It follows that oneclass SVM can be applied to identifying these correct correspondences, and boosts the performance of feature matching. The proposed approach is comprehensively compared with the state-of-the-art approaches, and evaluated on four benchmarks of image matching. The promising results manifest its effectiveness.	baseline (configuration management);benchmark (computing);computer vision;data descriptor;distance (graph theory);feature (computer vision);feature model;homography (computer vision);image processing;image registration;motion estimation;outline of object recognition;precision and recall;scott continuity;sensor;smoothing;unsupervised learning	Yuan-Ting Hu;Yen-Yu Lin;Hsin-Yi Chen;Kuang-Jui Hsu;Bing-Yu Chen	2014	CoRR		computer vision;machine learning;pattern recognition;mathematics	Vision	34.587867270398306	-51.728466812074835	38547
6253c9a7ddb7aefd72c798735f86bcdca532f6d0	weakly supervised motion segmentation with particle matching	particle matching;cerebral palsy;motion segmentation;computerized diagnosis;tracking	Motion segmentation refers to the task of segmenting moving objects subject to their motion in order to distinguish and track them in a video. This is a challenging task in situations where different objects share similar movement patterns, or in cases where one object is occluded by others in part of the scene. In such cases, unsupervised motion segmentation fails and additional information is needed to boost the performance. Based on a formulation of the clustering task as an optimization problem using a multi-labeled Markov Random Field, we develop a semi-supervised motion segmentation algorithm by setting up a framework for incorporating prior knowledge into the segmentation algorithm. Prior knowledge is given in the form of manually labelling trajectories that belong to the various objects in one or more frames of the video. Clearly, one wishes to limit the amount of manual labelling in order for the algorithm to be as autonomous as possible. Towards that end, we propose a particle matching procedure that extends the prior knowledge by automatically matching particles in frames over which fast motion or occlusion occur. The performance of the proposed method is studied through a variety of experiments on videos involving fast and complicated motion, occlusion and re-appearance, and low quality film. The qualitative and quantitative results confirm reliable performance on the types of applications our method is designed for. © 2015 Elsevier Inc. All rights reserved.	algorithm;autonomous robot;cluster analysis;experiment;hidden surface determination;markov chain;markov random field;mathematical optimization;optimization problem;semiconductor industry	Hodjat Rahmati;Ralf Dragon;Ole Morten Aamo;Lars Adde;Øyvind Stavdahl;Luc Van Gool	2015	Computer Vision and Image Understanding	10.1016/j.cviu.2015.07.004	computer vision;simulation;computer science;machine learning;motion estimation;block-matching algorithm;tracking;scale-space segmentation	Vision	42.832037873551904	-47.38706665796651	38580
549878bd26efad0a18a3d4035bf91a37f4e18b67	user performance for vehicle recognition with visual and infrared sensors from an unmanned aerial vehicle		In many situations it is important to detect and recognize people and vehicles. In this study the purpose was to examine human performance to detect and recognize vehicles on the ground from synthetic video sequences captured from a simulated unmanned aerial vehicle. A visual and an infrared sensor was used on an unmanned aerial vehicle with camera scan rate of the field of view on the ground relative to the ground of either 8 m/s or 12 m/s. The results from this study demonstrated that performance was affected by type of sensor, camera scan rate and type of vehicle. Subjects performed worse with infrared than with visual sensor and increased camera scan rate caused more errors. Also, the results show that recognition performance varied between 67 and 100% depending on type of vehicle. Recognition of specific vehicles was also affected negatively by interference from vehicles of similar appearance. Consequently, a vehicle with unique appearance within the set was easier to recognize.	aerial photography;sensor;unmanned aerial vehicle	Patrik Lif;Fredrik Näsström;Fredrik Bissmarck;Jonas Allvar	2018		10.1007/978-3-319-91238-7_25	infrared;human–computer interaction;horizontal scan rate;computer vision;computer science;field of view;artificial intelligence	Robotics	45.22353575944984	-43.03314058067122	38592
695eb3cd27dc125836c951ba5ada1cb1db999627	matching cad model and image features for robot navigation and inspection of an aircraft	ptz camera;feature matching;computer aided inspection;edcircle;image analysis;hough transform	This paper focuses on the navigation of a moving robot equipped with cameras, moving around an aircraft to perform inspection of different types of items (probes, doors, etc.). Matching CAD model and image features is useful to provide meaningful features for localization and inspection tasks. In our approach two primitive sets are matched using a similarity function. The similarity scores are injected in the edges of a bipartite graph. A best-match search procedure in bipartite graph guarantees the uniqueness of the match solution. The method provides good matching results even when the location of the robot with respect to the aircraft is badly estimated. Inspection approaches on static ports and air inlet vent are presented.	3d modeling;computer-aided design;mega man zx;robot;robotic mapping;similarity measure	Igor Jovancevic;Ilisio Viana;Jean-José Orteu;Thierry Sentenac;Stanislas Larnier	2016		10.5220/0005756303590366	hough transform;computer vision;image analysis;simulation;computer science	Robotics	49.91353140689574	-40.776943370385496	38635
66b4481b9cd6f9b02c1309639727465358d51c4b	multiple faults diagnosis in motion system based on svm	fault diagnosisconnection-related faults � svmgaussian kernelmotion system	In order to strengthen the safety and to monitor the working states of numerical control system real time, a framework was developed to diagnose multi-kinds of potential connection-related faults in the system. This framework considered that the number of fault data is small and the map from position to applied torque is a complex nonlinear function, and adopted the method of support vector machine to decide whether fault states had occurred or would occur. Position signal and the torque monitoring one in this system were used to train the parameters of support vector machine where Gaussian function was employed as nonlinear kernel. The faults mentioned in this research were diagnosed from a simple decision function real time, where the parameters were from the trained results. The framework and a trained decision function were applied to an X–Y motion platform, where the process included data acquisition, training of support vector machine and fault diagnosis. Results validate that three working states including two kinds of faults can be diagnosed easily at the same time, and the method can be used to numerical control system feasibly.	control system;cryptanalysis of the lorenz cipher;data acquisition;motion simulator;motion system;nc (complexity);nonlinear system;numerical analysis;real-time computing;support vector machine	Jinzhuang Xiao;Hongrui Wang;Xin-Cai Yang;Zheng Gao	2012	Int. J. Machine Learning & Cybernetics	10.1007/s13042-011-0035-y	control engineering;real-time computing;engineering;machine learning	ML	36.66582943853188	-30.558388458343057	38643
e756d023fd777182694c2c50027682c4f0fc9a82	sampling feature points for contour tracking with graphics hardware	graphics hardware	We present in this paper a GPU-accelerated algorithm for sampling contour points and normals from a generic CAD model of a 3D object, in order to aid contour-based real-time tracking algorithms. The procedure achieves fast computation rates for generic meshes consisting of polyhedral, non-convex as well as smooth surfaces. This method is part of a general purpose, multi-camera and multi-target framework, supporting rigid and articulated objects, in order to achieve a high degree of generality for different tracking scenarios.	3d modeling;algorithm;computation;computer-aided design;graphics hardware;graphics processing unit;jacobian matrix and determinant;normal (geometry);polygon mesh;polyhedron;real-time clock;real-time locating system;sampling (signal processing);video card	Erwin Roth;Giorgio Panin;Alois Knoll	2008			computer vision;computer graphics (images);artificial intelligence;computer science;computation;polygon mesh;sampling (statistics);generality;theoretical computer science;graphics hardware	Vision	53.44465243267522	-47.58989776451816	38664
180ad879ef2f3090310e11cfdbda576e3aafc02e	bayesian mixed membership models for soft clustering and classification	monte carlo markov chain;generic model;bayesian approach;latent variable;posterior distribution;national long term care survey	The paper describes and applies a fully Bayesian approach to soft clustering and classification using mixed membership models. Our model structure has assumptions on four levels: population, subject, latent variable, and sampling scheme. Population level assumptions describe the general structure of the population that is common to all subjects. Subject level assumptions specify the distribution of observable responses given individual membership scores. Membership scores are usually unknown and hence we can also view them as latent variables, treating them as either fixed or random in the model. Finally, the last level of assumptions specifies the number of distinct observed characteristics and the number of replications for each characteristic. We illustrate the flexibility and utility of the general model through two applications using data from: (i) the National Long Term Care Survey where we explore types of disability; (ii) semantic decompositions of abstracts and bibliographies from articles published in The Proceedings of the National Academy of Sciences. In the first application we use a Monte Carlo Markov chain implementation for sampling from the posterior distribution. In the second application, because of the size and complexity of the data base, we use a variational approximation to the posterior. We also include a guide to other applications of mixed membership modeling.	academy;approximation;bibliographic index;calculus of variations;cluster analysis;database;latent variable;markov chain monte carlo;monte carlo method;numerical analysis;observable;population;sampling (signal processing);unsupervised learning	Elena A. Erosheva;Stephen E. Fienberg	2004		10.1007/3-540-28084-7_2	econometrics;variable-order bayesian network;gibbs sampling;markov chain monte carlo;causal markov condition;pattern recognition;mathematics;bayesian linear regression;bayesian hierarchical modeling;bayesian statistics;statistics	ML	30.9792956099889	-24.66897941848937	38680
addb60c760304830224b6d747c439e9bd685a0d4	a novel vehicle reversing speed control based on obstacle detection and sparse representation	vehicles cameras velocity control stereo vision labeling calibration vectors;backing driving;vehicle reversing control binocular camera system obstacle detection obstacle tracking and recognition particle filter sparse representation;performance evaluation sparse representation vehicle safety method binocular camera obstacle detection binocular camera obstacle segmentation obstacle tracking obstacle recognition vehicle reversing speed control algorithm disparity computation triangulation object distance information vehicle rear particle filter obstacle identification electronic throttle opening automatic braking dodge sport utility vehicle;stereoscopic cameras;proximity detectors;期刊论文;detection and identification systems;speed control;velocity control automobiles automotive electronics braking cameras computer vision image representation image segmentation object detection object recognition particle filtering numerical methods road safety sparse matrices	In this paper we present a vehicle safety method for reversing speed control based on obstacle detection and sparse representation. The proposed system consists of three main steps, namely, binocular camera obstacle detection and segmentation, obstacle tracking and recognition, and vehicle reversing speed control algorithm. First of all, a binocular camera system is used to detect obstacles as a vehicle is reversing. Using disparity computation and triangulation, we can get all objects' distance information in the rear of a vehicle. Second, the framework of particle filter and sparse representation are used to track and identify the main obstacles such as human or animal bodies, vehicles, or any other objects. Finally, the vehicle reversing speed control algorithm, which controls the electronic throttle opening and automatic braking prior to collisions, makes the reversing control safer and more reliable. This system has been field tested on a Dodge sport utility vehicle by which the final performance evaluation demonstrates the validity of the proposed vehicle reversing speed control.	algorithm;binocular disparity;binocular vision;computation;effective method;experiment;particle filter;performance evaluation;reversing: secrets of reverse engineering;simulation;sparse approximation;sparse matrix	Zutao Zhang;Hong A Xu;Zhifeng Chao;Xiaopei Li;Chunbai Wang	2015	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2014.2360337	control engineering;computer vision;simulation;computer science;engineering;artificial intelligence;electronic speed control	Robotics	48.71297047812881	-42.35260069020265	38699
9dacba44ef8d093f44865f76d26ab078a89cef96	a driver support system to prevent traffic accidents caused by optical illusions	road traffic computer vision driver information systems feature extraction human computer interaction learning artificial intelligence object detection road accidents;machine learning driver support system traffic accident prevention optical illusions automatic assistance system visual illusion detection image feature identification speeded up robust features technique surf technique;image features optical illusions traffic accidents feedback alert system pattern matching;vehicles optical imaging visualization cameras optical filters brightness optical feedback	Optical illusions during night driving have a possibility to induce serious traffic accidents. This study, therefore, aims to build an automatic assistance system to detect the visual illusions and to alert drivers to potential dangers. In simulation studies, the proposed support system was able to decide the correct positional relationship between two automobiles from tail lamps and silhouette images. The Speeded-Up Robust Features (SURF) technique, which can identify image features, was also applied to actual driving scenes with external noises (e.g., streetlights, traffic signs, etc.). In future works, machine learning based on the image features could realize a robust assistance system.	machine learning;simulation;speeded up robust features	Koji Kashihara	2014	2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2014.6974482	computer vision;simulation	Robotics	41.57250442136473	-43.93372601353558	38748
90d09dc2aabe6dbe0f1c066ccda7c22dd53ff7d8	generic dynamic environment perception using smart mobile devices	monocular vision;obstacle detection;mobile device;occupancy grid;advanced driving assistance system	The driving environment is complex and dynamic, and the attention of the driver is continuously challenged, therefore computer based assistance achieved by processing image and sensor data may increase traffic safety. While active sensors and stereovision have the advantage of obtaining 3D data directly, monocular vision is easy to set up, and can benefit from the increasing computational power of smart mobile devices, and from the fact that almost all of them come with an embedded camera. Several driving assistance application are available for mobile devices, but they are mostly targeted for simple scenarios and a limited range of obstacle shapes and poses. This paper presents a technique for generic, shape independent real-time obstacle detection for mobile devices, based on a dynamic, free form 3D representation of the environment: the particle based occupancy grid. Images acquired in real time from the smart mobile device's camera are processed by removing the perspective effect and segmenting the resulted bird-eye view image to identify candidate obstacle areas, which are then used to update the occupancy grid. The occupancy grid tracked cells are grouped into obstacles depicted as cuboids having position, size, orientation and speed. The easy to set up system is able to reliably detect most obstacles in urban traffic, and its measurement accuracy is comparable to a stereovision system.	computation;cuboid;embedded system;embedding;generic drugs;mobile device;mobile operating system;preparation;real-time clock;smart device;stereopsis;vision, monocular;sensor (device)	Radu Danescu;Razvan Itu;Andra Petrovai	2016		10.3390/s16101721	embedded system;computer vision;simulation;computer science;monocular vision;occupancy grid mapping;mobile device	Robotics	49.82707505268378	-41.33720669913239	38754
44a49b58933affd96eb078c066c0404d2a8d6b61	integrated video object tracking with applications in trajectory-based event detection	video object;tracking system;video analysis;kalman filter;event detection;application integration;data association;probabilistic data association;adaptive particle sampling;trajectory analysis;intelligent surveillance;traffic monitoring;tracking	This work presents an automated and integrated framework that robustly tracks multiple targets for video-based event detection applications. Integrating the advantages of adaptive particle sampling and mathematical tractability of Kalman filtering, the proposed tracking system achieves both high tracking accuracy and computational simplicity. Occlusion and segmentation error cases are analyzed and resolved by constructing measurement candidates via adaptive particle sampling and an enhanced version of probabilistic data association. Also, we integrate the initial occlusion handling module in the tracking system to backtrack and correct the object trajectories. The reliable tracking results can serve as the foundation for automatic event detection. We also demonstrate event detection by classifying the trajectories of the tracked objects from both traffic monitoring and human surveillance applications. The experimental results have shown that the proposed tracking mechanism can solve the occlusion and segmentation error problems effectively and the events can be detected with high accuracy.		Hsu-Yung Cheng;Jenq-Neng Hwang	2011	J. Visual Communication and Image Representation	10.1016/j.jvcir.2011.07.001	kalman filter;computer vision;real-time computing;simulation;tracking system;computer science;video tracking;tracking	Vision	43.919161380289495	-47.2775770701309	38828
c5a01e8ffa78431fcdd8ec1e22f0e820c830565b	pac-bayesian analysis of co-clustering and beyond	bayesian analysis	We derive PAC-Bayesian generalization bounds for supervis d and unsupervised learning models based on clustering, such as co-clustering, matrix tri-fac torization, graphical models, graph clustering, and pairwise clustering. 1 We begin with the analysis of co-clustering, which is a widel y used approach to the analysis of data matrices. We distinguish am ong two tasks in matrix data analysis: discriminative prediction of the missing entries in data ma trices and estimation of the joint probability distribution of row and column variables in co-occur rence matrices. We derive PAC-Bayesian generalization bounds for the expected out-of-sample perf ormance of co-clustering-based solutions for these two tasks. The analysis yields regularization ter ms that were absent in the previous formulations of co-clustering. The bounds suggest that the exp cted performance of co-clustering is governed by a trade-off between its empirical performance a d the mutual information preserved by the cluster variables on row and column IDs. We derive an iter ative projection algorithm for finding a local optimum of this trade-off for discriminative predic tion tasks. This algorithm achieved stateof-the-art performance in the MovieLens collaborative filt ering task. Our co-clustering model can also be seen as matrix tri-factorization and the results pro vide generalization bounds, regularization terms, and new algorithms for this form of matrix factorizat on. The analysis of co-clustering is extended to tree-shaped gr aphical models, which can be used to analyze high dimensional tensors. According to the bound s, the generalization abilities of treeshaped graphical models depend on a trade-off between their empirical data fit and the mutual information that is propagated up the tree levels. We also formulate weighted graph clustering as a prediction pr blem: given a subset of edge weights we analyze the ability of graph clustering to predic t the remaining edge weights. The analysis of co-clustering easily extends to this problem an d suggests that graph clustering should optimize the trade-off between empirical data fit and the mut ual information that clusters preserve on graph nodes.	algorithm;bayesian network;biclustering;cluster analysis;consistency model;exptime;graphical model;graphical user interface;local optimum;movielens;mutual information;perf (linux);triangular function;unsupervised learning	Yevgeny Seldin;Naftali Tishby	2010	Journal of Machine Learning Research		correlation clustering;bayesian probability;computer science;machine learning;pattern recognition;mathematics;cluster analysis;statistics	ML	27.868107028102507	-31.888719797514007	38832
ac283fc4f470f31f942115e2ccd6030223c13d53	sensorless intelligent classifier of tool condition in a cnc milling machine using a som supervised neural network	wear;monitoring;breakage	Industry has monitoring systems to determine the tool condition and to ensure quality. This paper presents an intelligent classification system which determines the status of cutters in a CNC milling machine. The tool states are detected through the analysis of the cutting forces drawn from the spindle motors currents. A wavelet transformation was used in order to compress the data and to optimise the classifier structure. Then a supervised SOM neural network is responsible for carrying out the classification of the signal. Achieving a reliability of 95%, the system is capable of detecting breakage and a worn cutter.	artificial neural network	Georgina Mota-Valtierra;Luis Alfonso Franco-Gasca;Gilberto Herrera Ruiz	2011	IJAISC	10.1504/IJAISC.2011.042710	machine learning;wear	AI	36.63614116981924	-30.753287477558782	38854
589af1a1668af2b2f837ebe2c308075939484494	recognition and relocation of 3-dimensional objects using stereo robot vision system	software architecture;three dimensional;feature extraction;3 dimensional	An important task in sterco robot vision is to determine the three-dimcnsional locations of objects. The real location of an object is determined from the disparity of matched p i n t s in sterco image pairs. This papcr dcpicts a new sterw correspondence techniquc to match planes extracted from stcreo image pairs. The stereo image pairs are taken from the lateral stereo camera model of eye-on-hand configuration with a camera on the end-cffcctor of the robot. Stcreo Robot Vision Systcm (SRVS) which finds out thc three-dimensional locations of objects is described. Its software architecture consists of five major steps: stereo camcra modclling, feature extraction, sterco image matching, depth dctcrmination, and interpolation. In ordcr to realize a knowledge-bascd robot which has the capabilities of cognition and problem solving, we make the robot recognize objects and rclocate them in the spccificd order using the three-dimensional locations obtaincd from the SRVS. Wc show results of the SRVS with the proposcd matching algorithm and the relocation work of thc robot.	algorithm;binocular disparity;cognition;error analysis (mathematics);feature extraction;image registration;interpolation;lateral thinking;order by;problem solving;relocation (computing);robot;robotics;software architecture;stereo camera	Kyoung Mihn Do;Chee-Woo Kang;Kwae-Hi Lee	1990			computer vision;mathematics;artificial intelligence;interpolation;machine vision;computer stereo vision;software architecture;feature extraction;robot;relocation;stereo camera	Robotics	50.31935703551728	-38.96809348229847	38929
4d4a38c63569732d3fbfc5138085162f01b0e1d0	a neurodynamic optimization approach to constrained sparsity maximization based on alternative objective functions	minimisation;optimal solution;alternative objective functions;one layer recurrent neural network neurodynamic optimization approach constrained sparsity maximization alternative objective functions l0 norm minimization problem np hard;compressed sensing;one layer recurrent neural network;constrained minimization;neurodynamic optimization approach;objective function;approximation theory;sparse matrices approximation theory minimisation recurrent neural nets;laplace equations;artificial neural networks;l0 norm minimization problem;laplace equations artificial neural networks robustness;phase diagram;robustness;recurrent neural nets;recurrent neural network;constrained sparsity maximization;np hard;sparse matrices	In recent years, constrained sparsity maximization problems received tremendous attention in the context of compressive sensing. Because the formulated constrained L0 norm minimization problem is NP-hard, constrained L1 norm minimization is usually used to compute approximate sparse solutions. In this paper, we introduce several alternative objective functions, such as weighted L1 norm, Laplacian, hyperbolic secant, and Gaussian functions, as approximations of the L0 norm. A one-layer recurrent neural network is applied to compute the optimal solutions to the reformulated constrained minimization problems subject to equality constraints. Simulation results in terms of time responses, phase diagrams, and tabular data are provided to demonstrate the superior performance of the proposed neurodynamic optimization approach to constrained sparsity maximization based on the problem reformulations.	approximation algorithm;artificial neural network;compressed sensing;entropy maximization;expectation–maximization algorithm;mathematical optimization;np-hardness;phase diagram;recurrent neural network;secant method;simulation;sparse matrix;t-norm;table (information);taxicab geometry	Zhishan Guo;Jun Wang	2010	The 2010 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2010.5596553	minimisation;mathematical optimization;constrained optimization;combinatorics;sparse matrix;computer science;recurrent neural network;machine learning;np-hard;mathematics;compressed sensing;phase diagram;artificial neural network;robustness;approximation theory	ML	26.58441657625847	-38.96394519369862	38958
b3856022904a3192368326b798c74514ffc18869	real-time facial expression recognition based on features' positions and dimensions	mouth;facial expression recognition;feature positions;sign language;image matching;real time;facial feature extraction;automatic generation;computer vision;feature dimensions;eyes;face recognition;statistical analysis;image colour analysis;feature extraction;colour matching real time systems facial expression recognition feature positions feature dimensions facial feature extraction image matching statistical analysis computer vision;face recognition facial features position measurement statistics hardware mouth eyes user interfaces graphics facial animation;facial animation;position measurement;statistics;facial features;man machine interface;facial expression;colour matching;processing speed;user interfaces;graphics;hardware;real time systems	This paper describes a method of real-time facial expression recognition which is based on automatic measurement of the facial features' dimension and the positional relationship between them. The method is composed of two parts, the facial feature extraction using matching techniques and the Jacial expression recognition using statistics of position and dimension of the features. The method is implemented in an experimental hardware qstem and the performance is evaluated. The extraction rale of the facial-area, the mouth and the eyes are about IOO%, 96% and 90%, respectively, and the recognition rates of facial expression such as normal, angry, surprise, smile and sad expression are 54%, 89%. 86%, 53% and 7 I %, respectively, for a specijc person. The whole processing speed is about 15 frames/second. Finally, we touch on some applications such as man-machine inredace, automatic generation of facial CG animation and sign language translation using facial expression recognition techniques	ampersand;computer animation;feature extraction;jumbo frame;optical character recognition;real-time clock;real-time transcription;speech recognition	Histoshi Sako;Anthony V. W. Smith	1996		10.1109/ICPR.1996.547025	facial recognition system;human–machine interface;computer vision;speech recognition;computer facial animation;sign language;feature extraction;computer science;graphics;pattern recognition;three-dimensional face recognition;user interface;facial expression;statistics	Vision	39.58157472408252	-49.43370366678364	38960
e84c1f93cf99de23426d4d7d9dc55ada674e816a	what you saw is not what you get: domain adaptation using asymmetric kernel transforms	object model transfer;asymmetric kernel transform;image recognition;object recognition;kernel;imaging condition;measurement;supervised learning;support vector machines;visual domain adaptation;training;object recognition model;visualization;real world application;nonlinear transformation;object recognition image recognition learning artificial intelligence;transforms;linear transformation;kernel adaptation models training visualization measurement support vector machines transforms;symmetric transformation;learning artificial intelligence;adaptation models;kernel space;object model;imaging condition asymmetric kernel transform real world application visual domain adaptation object model transfer supervised learning nonlinear transformation kernel space symmetric transformation object recognition model	In real-world applications, “what you saw” during training is often not “what you get” during deployment: the distribution and even the type and dimensionality of features can change from one dataset to the next. In this paper, we address the problem of visual domain adaptation for transferring object models from one dataset or visual domain to another. We introduce ARC-t, a flexible model for supervised learning of non-linear transformations between domains. Our method is based on a novel theoretical result demonstrating that such transformations can be learned in kernel space. Unlike existing work, our model is not restricted to symmetric transformations, nor to features of the same type and dimensionality, making it applicable to a significantly wider set of adaptation scenarios than previous methods. Furthermore, the method can be applied to categories that were not available during training. We demonstrate the ability of our method to adapt object recognition models under a variety of situations, such as differing imaging conditions, feature types and codebooks.	codebook;domain adaptation;kernel (operating system);nonlinear system;outline of object recognition;software deployment;supervised learning;user space;visual basic[.net]	Brian Kulis;Kate Saenko;Trevor Darrell	2011	CVPR 2011	10.1109/CVPR.2011.5995702	support vector machine;computer vision;kernel;visualization;object model;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;linear map;supervised learning;measurement	Vision	25.187740784604284	-47.76171783691944	39009
51ab4022a10fda7324dcae2729ddd117d58d2b87	robust capped norm nonnegative matrix factorization: capped norm nmf	robust nonnegative matrix factorization;robust clustering;capped norm	As an important matrix factorization model, Nonnegative Matrix Factorization (NMF) has been widely used in information retrieval and data mining research. Standard Nonnegative Matrix Factorization is known to use the Frobenius norm to calculate the residual, making it sensitive to noises and outliers. It is desirable to use robust NMF models for practical applications, in which usually there are many data outliers. It has been studied that the 2,1, or 1-norm can be used for robust NMF formulations to deal with data outliers. However, these alternatives still suffer from the extreme data outliers. In this paper, we present a novel robust capped norm orthogonal Nonnegative Matrix Factorization model, which utilizes the capped norm for the objective to handle these extreme outliers. Meanwhile, we derive a new efficient optimization algorithm to solve the proposed non-convex non-smooth objective. Extensive experiments on both synthetic and real datasets show our proposed new robust NMF method consistently outperforms related approaches.	algorithm;data mining;experiment;information retrieval;mathematical optimization;matrix multiplication;non-negative matrix factorization;robustness (computer science);synthetic intelligence	Hongchang Gao;Feiping Nie;Tom Weidong Cai;Heng Huang	2015		10.1145/2806416.2806568	mathematical optimization;machine learning	ML	26.09489737653703	-38.04823123287228	39027
f9789fd018b30a59940ed43ffe33bbc225a48266	a multisensor technique for gesture recognition through intelligent skeletal pose analysis	sensor systems;pose estimation computer vision gesture recognition image fusion learning artificial intelligence object tracking;estimation;estimation intelligent sensors real time systems gesture recognition tracking sensor systems;touchless display interaction multisensor technique intelligent skeletal pose analysis smart sensor technology computer vision techniques human hand tracking finger movement tracking finger occlusion computer vision gesture recognition artificial hand pose estimation sensor generated skeletal pose ground truth;user evaluation depth sensors gesture recognition multisensor occlusion pose estimation;gesture recognition;intelligent sensors;tracking;real time systems	Recent advances in smart sensor technology and computer vision techniques have made the tracking of unmarked human hand and finger movements possible with high accuracy and at sampling rates of over 120 Hz. However, these new sensors also present challenges for real-time gesture recognition due to the frequent occlusion of fingers by other parts of the hand. We present a novel multisensor technique that improves the pose estimation accuracy during real-time computer vision gesture recognition. A classifier is trained offline, using a premeasured artificial hand, to learn which hand positions and orientations are likely to be associated with higher pose estimation error. During run-time, our algorithm uses the prebuilt classifier to select the best sensor-generated skeletal pose at each time step, which leads to a fused sequence of optimal poses over time. The artificial hand used to establish the ground truth is configured in a number of commonly used hand poses such as pinches and taps. Experimental results demonstrate that this new technique can reduce total pose estimation error by over 30% compared with using a single sensor, while still maintaining real-time performance. Our evaluations also demonstrate that our approach significantly outperforms many other alternative approaches such as weighted averaging of hand poses. An analysis of our classifier performance shows that the offline training time is insignificant, and our configuration achieves about 90.8% optimality for the dataset used. Our method effectively increases the robustness of touchless display interactions, especially in high-occlusion situations by analyzing skeletal poses from multiple views.	3d pose estimation;algorithm;computer vision;gesture recognition;ground truth;interaction;online and offline;real-time clock;real-time locating system;sampling (signal processing);sensor;smart transducer;statistical classification	Nathaniel Rossol;L. Irene Cheng;Anup Basu	2016	IEEE Transactions on Human-Machine Systems	10.1109/THMS.2015.2467212	computer vision;estimation;speech recognition;3d pose estimation;computer science;gesture recognition;articulated body pose estimation;tracking;statistics;intelligent sensor	Vision	38.79071736217245	-42.763488634725604	39153
b4a137868151223f73a96a7505f4c9f7a4f40d6a	does dirichlet prior smoothing solve the shannon entropy estimation problem?		The Dirichlet prior is widely used in estimating discrete distributions and functionals of discrete distributions. In terms of Shannon entropy estimation, one approach is to plug-in the Dirichlet prior smoothed distribution into the entropy functional, while the other one is to calculate the Bayes estimator for entropy under the Dirichlet prior for squared error, which is the conditional expectation. We show that in general they do not improve over the maximum likelihood estimator, which plugs-in the empirical distribution into the entropy functional. No matter how we tune the parameters in the Dirichlet prior, this approach cannot achieve the minimax rates in entropy estimation, as recently characterized by Jiao, Venkat, Han, and Weissman [1], and Wu and Yang [2]. The performance of the minimax rate-optimal estimator with n samples is essentially at least as good as that of the Dirichlet smoothed entropy estimators with n ln n samples. We harness the theory of approximation using positive linear operators for analyzing the bias of plug-in estimators for general functionals under arbitrary statistical models, thereby further consolidating the interplay between these two fields, which was thoroughly exploited by Jiao, Venkat, Han, and Weissman [3] in estimating various functionals of discrete distributions. We establish new results in approximation theory, and apply them to analyze the bias of the Dirichlet prior smoothed plug-in entropy estimator. This interplay between bias analysis and approximation theory is of relevance and consequence far beyond the specific problem setting in this paper.	approximation theory;entropy estimation;estimation theory;han unification;minimax;plug-in (computing);relevance;shannon (unit);smoothing;statistical model;yang	Jiantao Jiao;Kartik Venkat;Yanjun Han;Tsachy Weissman	2015	2015 IEEE International Symposium on Information Theory (ISIT)	10.1109/TIT.2017.2733537		ML	28.57423700135242	-28.183438472758887	39164
698530d3d339da5f6eb1dfe0d68949e9658af707	learning emergent tasks for an autonomous mobile robot	robot sensing systems;heuristic programming learning artificial intelligence mobile robots neural nets;ahc;eec esprit 2483 panorama project;emergent task learning;upm robuter;neural nets;mobile robot;surveillance;reinforcement learning;heuristic programming;adaptive heuristic critic;eec esprit 2483 panorama project emergent task learning autonomous mobile robot reinforcement learning algorithm neural network topology ahc adaptive heuristic critic fusion supervisor surveillance opmor simulation environment mobile platform upm robuter;mobile robots;event detection;autonomous mobile robot;reinforcement learning algorithm;vectors;mobile platform;upm;neural network topology;fusion supervisor;learning artificial intelligence;autonomous learning;simulation environment;mobile robots robot sensing systems robot kinematics discrete event simulation event detection vectors;robot kinematics;neural network;opmor;discrete event simulation	We present an implementation of a reinforcement learning algorithm trough the use of a special neural network topo-logy, the AHC (Adaptive Heuristic Critic). The AHC is used as a fusion supervisor of primitive behaviors in order to execute more complex robot behaviors, for example go to goal, surveillance or follow a path. The fusion supervisor is part of an architecture for the execution of mobile robot tasks which are composed of several primitive behaviors which act in a simultaneous or concurrent fashion. The architecture allows for learning to take place at the execution level, it incorporates the experience gained in executing primitive behaviors as well as the overall task. The implementation of this autonomous learning approach has been tested within OPMOR, a simulation environment for mobile robots and with our mobile platform, the UPM Robuter. Both, simulated and actual results are presented. The performance of the AHC neural network is adequate. Portions of this work has been implemented within the EEC ESPRIT 2483 PANORAMA Project.	algorithm;artificial neural network;autonomous robot;emergence;experiment;goto;heuristic;mobile operating system;mobile robot;reinforcement learning;self-organizing map;simulation;teuvo kohonen;topo;undefined behavior	D. Gachet;Miguel Angel Salichs;Luis Moreno;J. R. Pimentel	1994		10.1109/IROS.1994.407378	mobile robot;simulation;computer science;engineering;artificial intelligence;machine learning;artificial neural network	Robotics	50.18018964555034	-29.384733284165637	39170
a659b5e7f91d59c079f725b60a8a920239276e00	solarspire: querying temporal solar imagery by content	image recognition;image segmentation astronomical techniques solar flares astronomy computing image retrieval content based retrieval visual databases image sequences feature extraction image recognition;image segmentation;time series;astronomy computing;satellite images solarspire sun temporal solar imagery content based retrieval image sequence archives solar flares spatio temporal behavior bright spots dark spots relative intensity temporally persistent objects query web based interface size time series;feature extraction;image sequence;solar flares;satellite image;astronomical techniques;image segmentation content based retrieval image databases brightness satellites information retrieval eyes contracts nasa sun;content based retrieval;solar flare;image sequences;visual databases;image retrieval	In this paper, we describe a novel content-based retrieval application which permits astrophysicists to search large image sequence archives for solar phenomenon, such as solar flares, based on the spatio-temporal behavior of the solar phenomenon. Specifically, images are preprocessed to identify bright and dark spots based on their relative intensity with respect to their neighboring regions. Temporally persistent objects are then extracted from the collection of spots, and their spatio-temporal behavior represented as intensity and size time series. Users define a query in terms of a model of spatio-temporal behaviors through a Web-based interface. The stored intensity and size time series are searched, and series segments that match the specified specified spatio-temporal behavior are returned. The benchmark results based on 2500 satellite images show that the proposed methodology demonstrated better than 85% accuracy on a solar phenomenon previously identified by astrophysicists.		Matthew L. Hill;Vittorio Castelli;Chung-Sheng Li;Yuan-Chi Chang;Lawrence D. Bergman;John R. Smith;Barbara J. Thompson	2001		10.1109/ICIP.2001.959175	solar flare;computer vision;image retrieval;computer science;information retrieval	Vision	39.09360386705899	-50.7208184486746	39187
ba6d4cf28ce7b023fab6a62d2cb38b4fbfc91064	a floor and obstacle height map for 3d navigation of a humanoid robot	robot sensing systems;humanoid robot;3d navigation;legged locomotion;intelligent robots;data interpretation;biped robot;path planning;working environment noise;occupancy grid;humanoid robot 3d perception and navigation obstacle avoidance;orbital robotics;noise measurement;3d perception and navigation;navigation;obstacle avoidance;humanoid robots;stereo vision;navigation humanoid robots legged locomotion robot sensing systems working environment noise noise measurement intelligent robots orbital robotics grid computing intelligent systems;intelligent systems;autonomous navigation;3 dimensional;collision avoidance;grid computing	With the development of biped robots, systems became able to navigate in a 3 dimensional world, walking up and down stairs, or climbing over small obstacles. We present a method for obtaining a labeled 2.5D grid map of the robot's surroundings. Each cell is marked either as floor or obstacle and contains a value telling the height of the floor or obstacle. Such height maps are useful for path planning and collision avoidance. The method uses a novel combination of a 3D occupancy grid for robust sensor data interpretation and a 2.5D height map for fine resolution floor values. We evaluate our approach using stereo vision on the humanoid robot QRIO and show the advantages over previous methods. Experimental results from navigation runs on an obstacle course demonstrate the ability of the method to generate detailed maps for autonomous navigation.	2.5d;autonomous robot;heightmap;humanoid robot;map;motion planning;qrio;stereopsis	Jens-Steffen Gutmann;Masaki Fukuchi;Masahiro Fujita	2005	Proceedings of the 2005 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2005.1570257	computer vision;simulation;intelligent decision support system;computer science;humanoid robot;artificial intelligence;mobile robot navigation	Robotics	51.7910859370338	-37.094097525655464	39273
dacf8a074d95152dbd4b7e3544579ac9a039dcb0	rao-blackwellized point mass filter for reliable state estimation	probability;state estimation approximation theory particle filtering numerical methods probability;state estimation;approximation theory;sine wave tracking reliable state estimation rao blackwellized point mass filter rb pmf rao blackwellized marginal particle filter rb mpf point mass approximation posterior distribution reliable tracking probability events;rao blackwellized point mass filters filters state estimation;particle filtering numerical methods	We present a Rao-Blackwellized point mass filter (RB-PMF) as a deterministic counterpart of the Rao-Blackwellized marginal particle filter (RB-MPF). The main advantage of the proposed filter is its deterministic nature that results in the same estimate for repeated runs over the same data. Moreover, the point mass approximation offers more reliable representation of the tails of the posterior distribution. This results in more reliable tracking of low probability events, which is demonstrated on a simulated example of sine wave tracking. Due to Rao-Blackwellization, the proposed filter is capable to estimate problems of higher dimensions than the original point mass filter. This is demonstrated on estimation of a demanding five dimensional problem.	approximation;marginal model;mass effect trilogy;particle filter;tails	Václav Smídl;Matej Gasperin	2013	Proceedings of the 16th International Conference on Information Fusion		mathematical optimization;control theory;mathematics;statistics	Robotics	38.65493952961032	-26.238589528077593	39280
c1e4420ddc71c4962e0ba26287293a25a774fb6e	learning depth from single monocular images	image features;supervised learning;markov random field;local features;discriminative training;ground truth;depth estimation	We consider the task of depth estimation from a single monocular image. We take a supervised learning approach to this problem, in which we begin by collecting a training set of monocular images (of unstructured outdoor environments which include forests, trees, buildings, etc.) and their corresponding ground-truth depthmaps. Then, we apply supervised learning to predict the depthmap as a function of the image. Depth estimation is a challenging problem, since local features alone are insufficient to estimate depth at a point, and one needs to consider the global context of the image. Our model uses a discriminatively-trained Markov Random Field (MRF) that incorporates multiscale localand global-image features, and models both depths at individual points as well as the relation between depths at different points. We show that, even on unstructured scenes, our algorithm is frequently able to recover fairly accurate depthmaps.	algorithm;depth perception;discriminative model;ground truth;heightmap;linear programming;markov chain;markov random field;multiscale modeling;spatial scale;supervised learning;test set	Ashutosh Saxena;Sung H. Chung;Andrew Y. Ng	2005			computer vision;ground truth;computer science;machine learning;pattern recognition;mathematics;supervised learning;feature	Vision	29.901218864664582	-49.74381762681941	39297
00470cde9003ed5e69cf00987eda2412f579ad7e	lfnet: a novel bidirectional recurrent convolutional neural network for light-field image super-resolution		The low spatial resolution of light-field image poses significant difficulties in exploiting its advantage. To mitigate the dependency of accurate depth or disparity information as priors for light-field image super-resolution, we propose an implicitly multi-scale fusion scheme to accumulate contextual information from multiple scales for super-resolution reconstruction. The implicitly multi-scale fusion scheme is then incorporated into bidirectional recurrent convolutional neural network, which aims to iteratively model spatial relations between horizontally or vertically adjacent sub-aperture images of light-field data. Within the network, the recurrent convolutions are modified to be more effective and flexible in modeling the spatial correlations between neighboring views. A horizontal sub-network and a vertical sub-network of the same network structure are ensembled for final outputs via stacked generalization. Experimental results on synthetic and real-world data sets demonstrate that the proposed method outperforms other state-of-the-art methods by a large margin in peak signal-to-noise ratio and gray-scale structural similarity indexes, which also achieves superior quality for human visual systems. Furthermore, the proposed method can enhance the performance of light field applications such as depth estimation.	artificial neural network;binocular disparity;biological neural networks;circuit restoration;convolution;convolutional neural network;ensemble learning;generalization (psychology);grayscale;image restoration;index;light field;linear-feedback shift register;numerous;peak signal-to-noise ratio;recurrent neural network;structural similarity;subnetwork;super-resolution imaging;synthetic data;synthetic intelligence;anatomical layer;emotional dependency	Yunlong Wang;Fei Liu;Kunbo Zhang;Guangqi Hou;Zhenan Sun;Tieniu Tan	2018	IEEE Transactions on Image Processing	10.1109/TIP.2018.2834819	convolutional neural network;iterative reconstruction;fold (higher-order function);computer vision;structural similarity;light field;artificial intelligence;prior probability;mathematics;spatial relation;image resolution;pattern recognition	Vision	25.268262772683627	-51.36823832203218	39389
130662798b00fa78bfa5baae04b611def85a33ac	local histogram matching for efficient optical flow computation applied to velocity estimation on pocket drones	liverpool;velocity measurement image sequences mobile robots robot vision;repository;university;velocity control loop local histogram matching optical flow computation efficiency velocity estimation pocket drones autonomous flight stm32f4 microprocessor stereo camera edge histograms local optical flow subpixel flow determination time horizon adaptation velocity measurements;image edge detection histograms optical sensors optical imaging drones cameras estimation	Autonomous flight of pocket drones is challenging due to the severe limitations on on-board energy, sensing, and processing power. However, tiny drones have great potential as their small size allows maneuvering through narrow spaces while their small weight provides significant safety advantages. This paper presents a computationally efficient algorithm for determining optical flow, which can be run on an STM32F4 microprocessor (168 MHz) of a 4 gram stereo-camera. The optical flow algorithm is based on edge histograms. We propose a matching scheme to determine local optical flow. Moreover, the method allows for sub-pixel flow determination based on time horizon adaptation. We demonstrate velocity measurements in flight and use it within a velocity control-loop on a pocket drone.	algorithm;algorithmic efficiency;computation;histogram matching;matching (graph theory);maximum flow problem;microprocessor;on-board data handling;optical flow;pixel;stereo camera;unmanned aerial vehicle;velocity (software development)	Kimberly McGuire;Guido C. H. E. de Croon;C. De Wagter;Bart D W Remes;Karl Tuyls;Hilbert J. Kappen	2016	2016 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2016.7487496	computer vision;simulation;engineering;computer graphics (images)	Robotics	49.59408273381841	-36.30727556009966	39466
a0be50fd50930e53daf4b9968252e2fc3646f8fc	segmentation over detection by coupled global and local sparse representations	latent mask;training set;unique latent object mask;global bounding-box-level mask;local patch-level mask;mask property;local sparse representation;pascal voc object segmentation;object mask;unified optimization problem;sparse representation	Motivated by the rising performances of object detection algorithms, we investigate how to further precisely segment out objects within the output bounding boxes. The task is formulated as a unified optimization problem, pursuing a unique latent object mask in nonparametric manner. For a given test image, the objects are first detected by detectors. Then for each detected bounding box, the objects of the same category along with their object masks are extracted from the training set. The latent mask of the object within the bounding box is inferred based on three objectives: 1) the latent mask should be coherent, subject to sparse errors caused by within-category diversities, with the global bounding-box-level mask inferred by sparse representation over the bounding boxes of the same category within the training set; 2) the latent mask should be coherent with local patch-level mask inferred by sparse representation of the individual patch over all spatially nearby (handling local deformations) patches of the same category in the training set; and 3) mask property within each sufficiently small super-pixel should be consistent. All these three objectives are integrated into a unified optimization problem, and finally the sparse representation coefficients and the latent mask are alternately optimized based on Lasso optimization and smooth approximation followed by Accelerated Proximal Gradient method, respectively. Extensive experiments on the Pascal VOC object segmentation datasets, VOC2007 and VOC2010, show that our proposed algorithm achieves competitive results with the state-ofthe-art learning based algorithms, and is superior over other detection based object segmentation algorithms.		Wei Xia;Zheng Song;Jiashi Feng;Loong Fah Cheong;Shuicheng Yan	2012		10.1007/978-3-642-33715-4_48	theoretical computer science;machine learning;pattern recognition;mathematics	Vision	32.585703358567734	-46.539662552194066	39479
b99588bd0393a60a0c627970ab6cab7338d08ca6	semantic segmentation based on iterative contraction and merging		The state-of-the-art models for semantic image segmentation usually contain a convolutional neural network (CNN) and a conditional random field (CRF). As a predictor, existing CNN techniques can generate a dense prediction result but may generate obvious boundary errors at the same time. As a refinement model, CRF improves the CNN outcomes by forcing the consistency of local labels. However, the use of CRF may cause fragmentation effect around object boundaries. In this paper, we propose the use of a so-called iterative contraction and merging (ICM) process to facilitate the semantic segmentation process. Guided by the high-level information from CNN, the ICM process is used as a tool to grow image segments in a bottom-up way and to produce more accurate outcomes in an iterative way. The ICM process can faithfully preserve the boundary information and maintain the consistency of local labels. Our experimental results demonstrate that the performance of the proposed approach is comparable to the state-of-the-art models but with more accurate boundaries.	artificial neural network;bottom-up parsing;conditional random field;convolutional neural network;fragmentation (computing);high- and low-level;image segmentation;iterated conditional modes;iteration;iterative method;kerrison predictor;refinement (computing)	Tzu-Hao Yang;Jia-Hao Syu;Sheng-Jyh Wang	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296488	convolutional neural network;artificial intelligence;image segmentation;semantics;forcing (mathematics);computer science;conditional random field;contraction (grammar);pattern recognition;fragmentation (computing);segmentation	Vision	25.947960155616055	-51.794512951191194	39486
3ca16b82435f1501f871123e982124b5960b30be	detect globally, label locally: learning accurate 6-dof object pose estimation by joint segmentation and coordinate regression		Coordinate regression has established itself as one of the most successful current trends in model-based 6 degree of freedom (6-DOF) object pose estimation from a single image. The underlying idea is to train a system that can regress the three-dimensional coordinates of an object, given an input RGB or RGB-D image and known object geometry, followed by a robust procedure such as RANSAC to optimize the object pose. These coordinate regression based approaches exhibit state-of-the-art performance by using pixel-level cues to model the probability distribution of object parts within the image. However, they fail to capture global information at the object level to learn accurate foreground/background segmentation. In this letter, we show that combining global features for object segmentation and local features for coordinate regression results in pixel-accurate object boundary detections and consequently a substantial reduction in outliers and an increase in overall performance. We propose a deep architecture with an instance-level object segmentation network that exploits global image information for object/background segmentation and a pixel-level classification network for coordinate regression based on local features. We evaluate our approach on the standard ground-truth 6-DOF pose estimation benchmarks and show that our joint approach to accurate object segmentation and coordinate regression results in the state-of-the-art performance on both RGB and RGB-D 6-DOF pose estimation.	3d pose estimation;autostereogram;ground truth;pixel;random sample consensus;sensor;biologic segmentation	Apurv Nigam;Adrian Penate-Sanchez;Lourdes Agapito	2018	IEEE Robotics and Automation Letters	10.1109/LRA.2018.2858446	degrees of freedom (statistics);ransac;robustness (computer science);rgb color model;image segmentation;outlier;computer vision;engineering;control theory;architecture;pose;artificial intelligence	Robotics	30.327063891269848	-49.10943177447169	39489
236bc416b7c65bae8568ead184f6a30737278370	face detection of ubiquitous surveillance images for biometric security from an image enhancement perspective		Security methods based on biometrics have been gaining importance increasingly in the last few years due to recent advances in biometrics technology and its reliability and efficiency in real world applications. Also, several major security disasters that occurred in the last decade have given a new momentum to this research area. The successful development of biometric security applications cannot only minimise such threats but may also help in preventing them from happening on a global scale. Biometric security methods take into account humans’ unique physical or behavioural traits that help to identify them based on their intrinsic characteristics. However, there are a number of issues related to biometric security, in particular with regard to the poor visibility of the images produced by surveillance cameras that need to be addressed. In this paper, we address this issue by proposing an integrated image enhancement approach for face detection. The proposed approach is based on contrast enhancement and colour balancing methods. The contrast enhancement method is used to improve the contrast, while the colour balancing method helps to achieve a balanced colour. Importantly, in the colour balancing method, a new process for colour cast adjustment is introduced which relies on statistical calculation. It can adjust the colour cast and maintain the luminance of the whole image at the same level. We evaluate the performance of the proposed approach by applying three face detection methods (skin colour based face detection, feature based face detection and image based face detection) to surveillance images before and after enhancement using the proposed approach. The results show a significant improvement in face detection when the proposed approach was applied.		Kashif Iqbal;Michael O. Odetayo;Anne E. James	2014	J. Ambient Intelligence and Humanized Computing	10.1007/s12652-012-0134-y	computer vision;face detection;simulation;computer security	Vision	40.91508128518858	-45.33264908164635	39490
d849b5322f61388436822dbf1ef0f3009f28811f	joint categorization of objects and rooms for mobile robots	robot vision image colour analysis image representation learning artificial intelligence mobile robots;object categorization nyu2 dataset representative synthetic samples hk human knowledge learning phase object object relations object room relations rgbd images crf conditional random field model inference processes pgm probabilistic graphical models contextual relations robotic applications mobile robots room categorization;training;random variables;feature extraction;robots;ontologies;probabilistic logic;training robots random variables feature extraction probabilistic logic context modeling ontologies;context modeling	In general, the problems of objects' and rooms' categorizations for robotic applications have been addressed separately. The current trend is, however, towards a joint modelling of both issues in order to leverage their mutual contextual relations: object → room (e.g. the detection of a microwave indicates that the room is likely to be a kitchen), and room → object (e.g. if the robot is in a bathroom, it is probable to find a toilet). Probabilistic Graphical Models (PGMs) are typically employed to conveniently cope with such relations, relying on inference processes to hypothesize about objects' and rooms' categories. In this work we present a Conditional Random Field (CRF) model, a particular type of PGM, to jointly categorize objects and rooms from RGBD images exploiting object-object and object-room relations. The learning phase of the proposed CRF uses Human Knowledge (HK) to eliminate the necessity of gathering real training data. Concretely, HK is acquired through elicitation and codified into an ontology, which is exploited to effortless generate an arbitrary number of representative synthetic samples for training. The performance of the proposed CRF model has been assessed using the NYU2 dataset, achieving a success of ~ 70% categorizing both, objects and rooms.	categorization;conditional random field;graphical model;high- and low-level;microwave;mobile robot;semantic reasoner;synthetic data;telecommuting;workspace	José-Raúl Ruiz-Sarmiento;Cipriano Galindo;Javier González	2015	2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2015.7353720	robot;random variable;computer vision;feature extraction;computer science;ontology;artificial intelligence;machine learning;context model;probabilistic logic	Robotics	30.70251104199894	-47.869046187571335	39503
642bb5022352a634c3ad16a78f189e120d16cfbc	a theory of minimal 3d point to 3d plane registration and its generalization	minimal solution;correspondence problem;point to plane registration;3d to 3d registration;pose estimation	Registration of 3D data is a key problem in many applications in computer vision, computer graphics and robotics. This paper provides a family of minimal solutions for the 3D-to-3D registration problem in which the 3D data are represented as points and planes. Such scenarios occur frequently when a 3D sensor provides 3D points and our goal is to register them to a 3D object represented by a set of planes. In order to compute the 6 degrees-of-freedom transformation between the sensor and the object, we need at least six points on three or more planes. We systematically investigate and develop pose estimation algorithms for several configurations, including all minimal configurations, that arise from the distribution of points on planes. We also identify the degenerate configurations in such registrations. The underlying algebraic equations used in many registration problems are the same and we show that many 2D-to-3D and 3D-to-3D pose estimation/registration algorithms involving points, lines, and planes can be mapped to the proposed framework. We validate our theory in simulations as well as in three real-world applications: registration of a robotic arm with an object using a contact sensor, registration of planar city models with 3D point clouds obtained using multi-view reconstruction, and registration between depth maps generated by a Kinect sensor.	3d pose estimation;algebraic equation;algorithm;computer graphics;computer vision;kinect;map;point cloud;robot;robotic arm;robotics;simulation	Srikumar Ramalingam;Yuichi Taguchi	2012	International Journal of Computer Vision	10.1007/s11263-012-0576-x	computer vision;mathematical optimization;pose;computer science;mathematics;geometry;correspondence problem	Vision	53.57473408335932	-48.5239941092829	39527
b4db5a7d7f07b8fc2d4202102fe9890a7b880ae5	vision-based multi-robot simultaneous localization and mapping	simultaneous localization and mapping robot kinematics robot vision systems cameras mobile robots robot localization layout image databases robot sensing systems information technology;robot localization;robot sensing systems;image databases;information technology;mobile robots;layout;camera motion;simultaneous localization and mapping;robot vision systems;cameras;robot kinematics;pose estimation	In this paper we present a vision-based approach for the multi-robot Simultaneous Localization and Mapping (SLAM) problem. We study the case of a team of robots equipped with a single camera and collaborating in the same worksite. We propose to calculate the location of the robots by using a collection of sparse views of the planar surface on which these robots are moving. The camera motions are estimated using inter-image homographies computed from the matching of overhead transformed views. Results of map generated from the estimated robot locations are presented.	curve fitting;gaussian blur;homography (computer vision);map;overhead (computing);robot;simultaneous localization and mapping;sparse matrix	Hassan Hajjdiab;Robert Laganière	2004	First Canadian Conference on Computer and Robot Vision, 2004. Proceedings.	10.1109/CCCRV.2004.1301439	layout;mobile robot;embedded system;computer vision;simulation;pose;computer science;artificial intelligence;robot control;information technology;mobile robot navigation;robot kinematics;robot calibration;simultaneous localization and mapping	Robotics	52.24090391554858	-39.66988706456857	39541
e908252457334380eb72ef8d93cb80128c970b87	binary nonnegative matrix factorization applied to semi-conductor wafer test sets	semi-conductor wafer processing;wafer test data;appropriate algorithm;hidden cause;binary-valued data set;binary nonnegative matrix factorization;single wafer;probabilistic extension;elementary failure pattern;semi-conductor wafer test sets;probabilistic superposition;continuous-valued elementary pattern;non negative matrix factorization;nonnegative matrix factorization;optimization problem	semi-conductor wafer processing;wafer test data;appropriate algorithm;hidden cause;binary-valued data set;binary nonnegative matrix factorization;single wafer;probabilistic extension;elementary failure pattern;semi-conductor wafer test sets;probabilistic superposition;continuous-valued elementary pattern;non negative matrix factorization;nonnegative matrix factorization;optimization problem	non-negative matrix factorization;wafer testing	Reinhard Schachtner;Gerhard Pöppel;Elmar Wolfgang Lang	2009		10.1007/978-3-642-00599-2_89	mathematical optimization;discrete mathematics;theoretical computer science;mathematics	ML	35.043932028447465	-29.04788805256247	39545
5a2b4d68aba7d1b28c728e27cbd508648dfe1f25	robot navigation in corridor environments using a sketch floor map	path planning;computational geometry;robot navigation;mobile robots;navigation;sensor aliasing robot navigation system sketch floor map metric information navigational instructions augmented topological map multiple hypotheses robots location landmark matching failures corridors structure voronoi diagram path planning;navigation floors robot sensing systems data mining humans information science feature extraction sensor phenomena and characterization character generation;path planning navigation computational geometry mobile robots	This paper presents a new robot navigation system that can operate on a sketch floor map provided by a user. This sketch map is similar to floor plans as shown at the entrance of buildings, which does not contain accurate metric information and details such as obstacles. The system enables a user to give navigational instructions to a robot by interactively providing a floor map and pointing out goal positions on the map. Since metric information is unavailable, navigation is done using an augmented topological map which described the structure of the corridors extracted from a given floor map. Multiple hypotheses of the robot’s location are maintained and updated during navigation in order to cope with sensor aliasing and landmark-matching failures due to factors such as unknown obstacles inside the corridors.	aliasing;interactivity;robot;robotic mapping	Vachirasuk Setalaphruk;Atsushi Ueno;Izuru Kume;Yasuyuki Kono;Masatsugu Kidode	2003		10.1109/CIRA.2003.1222240	turn-by-turn navigation;mobile robot;computer vision;navigation;simulation;computational geometry;computer science;artificial intelligence;motion planning;mobile robot navigation	Robotics	51.49574593444965	-36.81773807161298	39580
48b71b09267b74676ee27b5fda38b9b4c10e5305	real-time head tracker using color, stereovision and ellipse fitting in a particle filter	mobile camera;real time;color histogram;face tracking;color distribution;particle filter;action recognition;image sequence;mobile agent;ellipse fitting	This paper proposes the use of a particle filter combined with color, depth information and shape features as an efficient and effective way to deal with tracking a head on the basis of image stream coming from a mobile stereovision camera. The head is modeled in the 2D image domain by an ellipse. The color distribution within interior of the ellipse is represented by a color histogram. The color his togram is dynamically updated over time. The length of the ellipse’s minor axis is determined on the basis of depth information. The particles representing the candidate ellipses are weighted in each time step in respect of intensity gradient near the edge of the ellipse and matching score of the col or histograms representing the interior of an ellipse surrounding the tracked object and currently analyzed one. The proposed algorithm can track a head reliably in cases of temporal occlusions as well as vary ing illumination conditi ons by dealing with multiple hypotheses for the pose. Experimental results obtained on long image sequences show the feasibility of ourapproach to perform tracking a head undergoing complex changes of shape and appearance against a varying background. The tracker has been evaluated in experiments consisting in face tracking with a real mobile agent.	algorithm;apache axis;color depth;color histogram;curve fitting;experiment;gradient;mobile agent;particle filter;real-time transcription;robot;robotics;stereopsis	Bogdan Kwolek	2004	Informatica, Lith. Acad. Sci.		color histogram;computer vision;facial motion capture;color normalization;particle filter;computer science;mobile agent;mathematics;computer graphics (images)	Vision	48.41838539343274	-43.3710560838684	39664
4dd07a34d709c4eb505046fa4c76e35eee18fefc	fusion of local manifold learning methods	manifolds signal processing algorithms vectors educational institutions learning systems laplace equations materials;sensor fusion learning artificial intelligence matrix algebra optimisation;low dimensional embedding local manifold learning method fusion geometric intuitions geometric information fusion manifold structure local tangent coordinates local algorithms selection matrix global functional optimization based algorithm;manifold learning dimensionality reduction	Different local manifold learning methods are developed based on different geometric intuitions and each method only learns partial information of the true geometric structure of the underlying manifold. In this letter, we introduce a novel method to fuse the geometric information learned from local manifold learning algorithms to discover the underlying manifold structure more faithfully. We first use local tangent coordinates to compute the local objects from different local algorithms, then utilize the selection matrix to connect the local objects with a global functional and finally develop an alternating optimization-based algorithm to discover the low-dimensional embedding. Experiments on synthetic as well as real datasets demonstrate the effectiveness of our proposed method.	algorithm;align (company);machine learning;mathematical optimization;nonlinear dimensionality reduction;synthetic intelligence	Xianglei Xing;Kejun Wang;Zhuowen Lv;Yu Zhou;Sidan Du	2015	IEEE Signal Processing Letters	10.1109/LSP.2014.2360842	local tangent space alignment;mathematical optimization;machine learning;mathematics;geometry;manifold alignment	ML	27.551277782261327	-40.095945858226976	39669
5a894f57431f889291666e7ae349c3bbcab8e1ef	fusion strategies for minimizing sensing-level uncertainty in manipulator control	humanoid robot;tecnologia industrial tecnologia mecanica;tecnologia electronica telecomunicaciones;degree of freedom;soft computing;robot manipulator;parameter selection;controller design;synthetic data;sensor fusion;tecnologias;grupo a;simulation model;covariance matrix;neural network	Humanoid robotic applications require robot to act and behave like human being. Following soft computing like approach human being can think, decide and control himself in unstructured dynamic surroundings, where a great degree of uncertainty exists in the information obtained through sensory organs. In the robotics domain also, one of the key issues in extracting useful knowledge from sensory data is that of coping with information as well as sensory uncertainty at various levels. In this paper a generalized fusion based hybrid classifier (ANN-FDD-FFA) has been developed and applied for validating on generated synthetic data from observation model as well as from real hardware robot. The fusion goal, selected here, is primarily to minimize uncertainties in robotic manipulation tasks that are based on internal (joint sensors) as well as external (vision camera) sensory information. The effectiveness of present methodology has been extensively studied with a specially configured experimental robot having five degrees of freedom and a simulated model of a vision guided manipulator. In the present investigation main uncertainty handling approach includes weighted parameter selection (of geometric fusion) by a trained neural network that is not available in standard manipulator robotic controller designs. These approaches in hybrid configuration has significantly reduce the uncertainty at different levels for faster and more accurate manipulator control as demonstrated here through rigorous simulations and experimentations.	algorithm;artificial neural network;data applied;definition;dimensionality reduction;humanoid robot;information needs;iteration;network architecture;nonlinear system;performance;recursion;repeatability;robot;robotics;rough set;sensor;simulation;singlet fission;soft computing;synthetic data;uncertainty principle;xbox live vision	Gora Chand Nandi;Debjani Mitra	2005	Journal of Intelligent and Robotic Systems	10.1007/s10846-005-2964-8	control engineering;covariance matrix;simulation;computer science;engineering;humanoid robot;artificial intelligence;machine learning;simulation modeling;mobile manipulator;control theory;sensor fusion;soft computing;degrees of freedom;artificial neural network;synthetic data	Robotics	49.04063838891482	-27.955970997271553	39684
17d28a56bea45b0d189cbf8fef0d6c22de86ef06	optimal high-dynamic-range image acquisition for humanoid robots	object recognition;robot vision;humanoid robots;image representation;cameras;pose estimation	Humanoid robots should be able to visually recognize objects and estimate their 6D pose in real environmental conditions with their limited sensor capabilities. In order to achieve these visual skills, it is necessary to establish an optimal visual transducer connecting the scene layout with the internal representations of objects and places. This visual transducer should capture the noiseless visual manifold of the scene with high-dynamic-range in an efficient manner. Our endeavor is to develop such a visual transducer using the widespread LDR cameras in humanoid robots. In our previous work, the noiseless acquisition of continuous images [1] and the improved radio-metric calibration [2] already enabled the humanoid robots to attain the desired visual manifold in terms of quality. However, since the radiance range of the scene can be very wide, the required amount of exposures to capture the visual manifold (robustly without radiance inconsistencies) turns impractically large in terms of scope, granularity and acquisition time. In this article, a method for estimating the minimal amount of exposures and their particular integration times is presented. This method integrates our previous work in order to synthesize HDR images with the minimal amount of exposures while ensuring the high quality of the resulting image. Conclusively, the minimal exposure set provides performance improvements without quality trade-off. Experimental evaluation is presented with the humanoid robots ARMAR-III a, b [3].	algorithm;archive;display resolution;dynamic range;feature extraction;high-dynamic-range imaging;humanoid robot;ldraw;metric;range imaging;robustness (computer science);sampling (signal processing);transducer;vii	David Israel Gonzalez-Aguirre;Tamim Asfour;Rüdiger Dillmann	2013	2013 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2013.6696721	computer vision;simulation;pose;computer science;humanoid robot;artificial intelligence;cognitive neuroscience of visual object recognition	Robotics	52.90485086046828	-45.27747377444833	39686
0bca15c021e69572ebd8c72ca685a1abcd25e6e7	bayesian estimation of causal direction in acyclic structural equation models with individual-specific confounder variables and non-gaussian distributions	structural equation models;estimation of causal direction;latent confounding variables;non gaussianity;bayesian networks	Several existing methods have been shown to consistently estimate causal direction assuming linear or some form of nonlinear relationship and no latent confounders. However, the estimation results could be distorted if either assumption is violated. We develop an approach to determining the possible causal direction between two observed variables when latent confounding variables are present. We first propose a new linear non-Gaussian acyclic structural equation model with individual-specific effects that are sometimes the source of confounding. Thus, modeling individual-specific effects as latent variables allows latent confounding to be considered. We then propose an empirical Bayesian approach for estimating possible causal direction using the new model. We demonstrate the effectiveness of our method using artificial and real-world data.	causal filter;causality;coefficient;directed acyclic graph;experiment;latent variable;latent variable model;model selection;nonlinear system;structural equation modeling	Shohei Shimizu;Kenneth Bollen	2014	Journal of Machine Learning Research		latent class model;structural equation modeling;econometrics;computer science;machine learning;pattern recognition;bayesian network;mathematics;non-gaussianity;latent variable model;statistics	ML	28.120655773083815	-25.933479866006667	39693
0d10f800db996e05c6fa358ca81579e928c7c0dd	rgb-d object recognition using the knowledge transferred from relevant rgb images		The availability of depth images provides a new possibility to solve the challenging object recognition problem. However, when there is not enough labeled data, we cannot learn a discriminative classifier even using depth information. To solve this problem, we extend LCCRRD method by kernel trick. First, we construct two RGB classifiers with all labeled RGB images from source and target domain. The significant samples for both classifier are boosted and the non-significant ones are inhibited by exploiting the relationship between two domains. In this process, the knowledge of source RGB classifier can be transferred to target RGB classifier effectively. Then to improve the performance of RGB-D classifier by applying the knowledge from source domain, the predicted results of RGB-D classifier are made consistent to target RGB classifier. Furthermore all the parameters are optimized in a unified objective function. Experiments on four cross-domain dataset pairs shows that our approach is indeed effective and promising.	outline of object recognition	Depeng Gao;Rui Wu;Jiafeng Liu;Qingcheng Huang;Xianglong Tang;Peng Liu	2017		10.1007/978-3-319-70136-3_68	labeled data;artificial intelligence;discriminative model;kernel method;computer science;pattern recognition;cognitive neuroscience of visual object recognition;classifier (linguistics);rgb color model;computer vision	Vision	25.600213298838696	-47.758685968486596	39696
4817c736a9c020946c83ba5c62963e77a1ce2f82	autonomous sonar navigation in indoor, unknown and unstructured environments	map building;mobile robot;path planning;mobile robots;office environments autonomous sonar navigation unstructured environments unknown environments indoor environments bootstrapping problem concurrent localisation map building heuristic multiple hypothesis data association framework mobile robot roamer;data association;sonar mobile robots path planning computerised navigation;indoor environment;sonar navigation mobile robots robot sensing systems research and development error correction fasteners robot vision systems cameras buildings indoor environments;computerised navigation;sonar	whose positions are known in advance. These beacons are A mobile robot operating autonomously in unknown, unstructured environments has to be able to map its environment while at the same time determining its own position accurately within this environment. This paper presents an approach where the bootstrapping problem of concurrent localisation and map building is solved by estimating the respective errors introduced by each of the processes and correcting them accordingly. The success of this approach also hinges on the ability to determine which measurement originates from which feature. A heuristic multiDle hvDothesis data assoobserved by accurate, reliable sensors such as laser range scanners and cameras. However, in many applications, such as in homes or offices, the structuring of the robot’s environment is undesirable. Therefore the robot should be able to navigate in unknown, unstructured environments without any artificial help. It might be conceivable that in some cases some rudimentary map information might be available, such as an architect’s plan of a building. However, completely autonomous operation requires that the robot is capable of constructing and maintaining its own map in a dynamic environment. Therefore this work will concentrate on the case where the robot has no a priori map.	autonomous robot;heuristic;map;mobile robot;sonar (symantec);sensor	Wolfgang D. Rencken	1994		10.1109/IROS.1994.407440	mobile robot;computer vision;simulation;computer science;engineering;artificial intelligence;mobile robot navigation	Robotics	53.09883416329666	-34.313948213756134	39705
f1ded1b8cbddc3a8cc0f585957f1834c5c2e0311	an approach for fast and parallel video processing on apache hadoop clusters	libraries;computers;face detection tracking motion detection computers algorithm design and analysis programming libraries;video processing;motion detection and tracking hadoop mapreduce video processing face detection;video surveillance data handling face recognition image motion analysis object detection object tracking parallel programming public domain software video signal processing;motion detection and tracking;mapreduce;face detection;hadoop;programming;motion detection;algorithm design and analysis;tracking;video analysis fast video processing parallel video processing apache hadoop clusters mapreduce based clusters large scale video data handling processing time reduction face detection algorithm motion detection algorithm motion tracking algorithm smart city video surveillance video sites satellite image processing	This paper proposes an approach for fast and parallel video processing on MapReduce-based clusters such as Apache Hadoop. By utilizing clusters, the approach is able to handle large-scale of video data and the processing time can be significantly reduced. Technique details of performing video analysis on clusters are revealed, including method of porting typical video processing algorithms designed for a single computer to the proposed system. As case studies, face detection and motion detection and tracking algorithms have been implemented on clusters. Performance experiments on an Apache Hadoop cluster of six computers show that the system is able to reduce the running time of the two implemented algorithms to below 25% of that of a single computer. The applications of the system include smart city video surveillance, services provided by video sites and satellite image processing.	algorithm;apache hadoop;closed-circuit television;computer;experiment;face detection;image processing;mapreduce;smart city;streaming media;time complexity;video content analysis;video processing	Hanlin Tan;Lidong Chen	2014	2014 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2014.6890135	algorithm design;computer vision;programming;face detection;real-time computing;object-class detection;computer science;video tracking;tracking;video processing;video post-processing;computer graphics (images)	Visualization	43.138496236722915	-36.64500912612408	39756
aa1129780cc496918085cd0603a774345c353c54	evolutionary cost-sensitive discriminative learning with application to vision and olfaction	prototypes;learning systems;face recognition;principal component analysis;face;optimization;access control	In the design of machine learning models, one often assumes the same loss, which, however, may not hold in cost-sensitive learning scenarios. In a face-recognition-based access control system, misclassifying a stranger as a house owner and allowing entry may result in a more serious financial loss than misclassifying a house owner as a stranger and not allowing entry. That is, different types of recognition mistakes may lead to different losses, and therefore should be treated carefully. It is expected that a cost-sensitive learning mechanism can reduce the total loss when given a cost matrix that quantifies how severe one type of mistake is against another one. However, in many realistic applications, the cost matrix is unknown and unclear to users. Motivated by these concerns, in this paper, we propose an evolutionary cost-sensitive discriminative learning (ECSDL) method, with the following merits: 1) it addresses the definition of cost matrix in cost-sensitive learning without human intervention; 2) an evolutionary backtracking search algorithm is derived for the NP-hard cost matrix optimization; and 3) a cost-sensitive discriminative subspace is found, where the between-class separability and within-class compactness are well achieved, such that recognition becomes easier. Experiments in a variety of cost-sensitive vision and olfaction classification tasks demonstrate the efficiency and effectiveness of the proposed ECSDL approach.	access control;backtracking;control system;convex optimization;deep learning;discriminative model;experiment;facial recognition system;gradient;kernel method;linear separability;machine learning;mathematical optimization;np-hardness;nonlinear system;search algorithm;semidefinite programming;total loss;wordnet	Lei Zhang;David Zhang	2017	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2016.2631878	facial recognition system;face;computer vision;computer science;engineering;artificial intelligence;access control;machine learning;prototype;principal component analysis	ML	24.826698428334986	-44.08142833315037	39789
1d2cf7a5e5e801d3efcbe6daedbeb17b650b782a	sparse representation based approach for rgb-d hand gesture recognition	depth image;sparse coding;gesture recognition;attribute constraint	In this paper, we present a new algorithm for RGB-D hand gesture recognition by using multi-attribute sparse representation enforced with group constraints. Firstly, the hand region is segmented from the background according to the depth information. Then, we process all gesture-performing hand region images with PCA to reduce the feature dimension. To obtain a more accurate and discriminative representation, a multi-attribute sparse representation is employed for hand gesture recognition from different view angles. The multiple attributes for a gesture image can be represented by individual binary matrices to indicate the group properties for each gesture. Then, these attribute matrices are incorporated into the formulation of l1-minimization in the sparse coding framework. Finally, the effectiveness and robustness of the proposed method are demonstrated through experiments on a public RGB-D hand gesture dataset.		Te-Feng Su;Chin-Yun Fan;Meng-Hsuan Lin;Shang-Hong Lai	2015		10.1007/978-3-319-24078-7_57	computer vision;computer science;machine learning;pattern recognition;gesture recognition;neural coding	Vision	35.49670314417549	-51.26433191546831	39868
f22da19b3684d2049012996e54bc81dce99303b5	robust classification with convolutional prototype learning		Convolutional neural networks (CNNs) have been widely used for image classification. Despite its high accuracies, CNN has been shown to be easily fooled by some adversarial examples, indicating that CNN is not robust enough for pattern classification. In this paper, we argue that the lack of robustness for CNN is caused by the softmax layer, which is a totally discriminative model and based on the assumption of closed world (i.e., with a fixed number of categories). To improve the robustness, we propose a novel learning framework called convolutional prototype learning (CPL). The advantage of using prototypes is that it can well handle the open world recognition problem and therefore improve the robustness. Under the framework of CPL, we design multiple classification criteria to train the network. Moreover, a prototype loss (PL) is proposed as a regularization to improve the intra-class compactness of the feature representation, which can be viewed as a generative model based on the Gaussian assumption of different classes. Experiments on several datasets demonstrate that CPL can achieve comparable or even better results than traditional CNN, and from the robustness perspective, CPL shows great advantages for both the rejection and incremental category learning tasks.		Hong-Ming Yang;Xu-Yao Zhang;Fei Yin;Cheng-Lin Liu	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00366	robustness (computer science);discriminative model;convolutional neural network;machine learning;feature extraction;generative model;artificial intelligence;pattern recognition;contextual image classification;computer science;concept learning;softmax function	Vision	25.110474599268763	-48.65245435494214	39874
42bb241681c4bec1fa36211a204fa0dc8158e5ff	localizing objects while learning their appearance	model specification;large scale;conditional random field	Learning a new object class from cluttered training images is very challenging when the location of object instances is unknown. Previous works generally require objects covering a large portion of the images. We present a novel approach that can cope with extensive clutter as well as large scale and appearance variations between object instances. To make this possible we propose a conditional random field that starts from generic knowledge and then progressively adapts to the new class. Our approach simultaneously localizes object instances while learning an appearance model specific for the class. We demonstrate this on the challenging Pascal VOC 2007 dataset. Furthermore, our method enables to train any state-of-the-art object detector in a weakly supervised fashion, although it would normally require object location annotations.	clutter;conditional random field;instance (computer science);internationalization and localization;supervised learning	Thomas Deselaers;Bogdan Alexe;Vittorio Ferrari	2010		10.1007/978-3-642-15561-1_33	computer vision;method;object model;computer science;artificial intelligence;machine learning;specification;conditional random field	Vision	31.499579971198557	-48.85806982451909	39939
83b4c9ef3ac11c4a3a264e77c31ea6daf50e4c87	investigation of structure – biodegradability relationships in polychlorinated biphenyls using self-organising maps	actividad microbiana;medio ambiente;biodegradabilite;self organizing maps;microbial activity;biphenyle polychloro;polychlorobiphenyl;aspergillus niger;activite microbienne;carte autoorganisatrice;artificial intelligent;bifenilo policloro;environment;autoorganizacion;biodegradability;self organization;environmental hazards;environnement;biodegradabilidad;autoorganisation;self organising map;polychlorinated biphenyl	Polychlorinated biphenyls (PCBs) represent a significant, long-term environmental hazard. Persistent in the environment, accumulative in many species, and toxic, they are a serious pollutant at many industrial sites. The most common methods for environmental remediation – thermal or photochemical treatment – are often an unsatisfactory option if PCBs are present in chemically-contaminated sites, since these methods may convert PCBs into even more toxic chemicals such as dioxins. Microbial degradation offers a safer and more Environmentally-friendly alternative, but susceptibility to microbial degradation varies widely among members of the PCB family, and the relationship of structure to degradation rates is poorly understood. This paper discusses the use of a Self-Organising Map (SOM) to rationalise and predict degradation data for PCBs under the action of Aspergillus Niger. A SOM is shown to be able to predict biodegradability to within 25% of the experimental values for three quarters of a set of 44 PCBs. It appears that prediction of the biodegradability of dichloro-PCBs may be more difficult than prediction for other types of PCB.	elegant degradation;persistent world;printed circuit board;self-organizing map	H. M. Cartwright	2002	Neural Computing & Applications	10.1007/s005210200013	self-organization;self-organizing map;computer science;artificial intelligence;natural environment;biodegradation	ML	35.70725887326576	-35.850802841464116	39984
3da6c362ec8f21cc46f785a1ca1cd02d1698d754	tracking in object action space	parametric gestures;action recognition;tracking;pose estimation	1077-3142/$ see front matter 2013 Elsevier Inc. A http://dx.doi.org/10.1016/j.cviu.2013.02.002 q This paper has been recommended for acceptance ⇑ Corresponding author. E-mail address: vok@m-tech.aau.dk (V. Krüger). In this paper we focus on the joint problem of tracking humans and recognizing human action in scenarios such as a kitchen scenario or a scenario where a robot cooperates with a human, e.g., for a manufacturing task. In these scenarios, the human directly interacts with objects physically by using/manipulating them or by, e.g., pointing at them such as in ‘‘Give me that. . .’’. To recognize these types of human actions is difficult because (a) they ought to be recognized independent of scene parameters such as viewing direction and (b) the actions are parametric, where the parameters are either object-dependent or as, e.g., in the case of a pointing direction convey important information. One common way to achieve recognition is by using 3D human body tracking followed by action recognition based on the captured tracking data. For the kind of scenarios considered here we would like to argue that 3D body tracking and action recognition should be seen as an intertwined problem that is primed by the objects on which the actions are applied. In this paper, we are looking at human body tracking and action recognition from a object-driven perspective. Instead of the space of human body poses we consider the space of the object affordances, i.e., the space of possible actions that are applied on a given object. This way, 3D body tracking reduces to action tracking in the object (and context) primed parameter space of the object affordances. This reduces the highdimensional joint-space to a low-dimensional action space. In our approach, we use parametric hidden Markov models to represent parametric movements; particle filtering is used to track in the space of action parameters. We demonstrate its effectiveness on synthetic and on real image sequences using humanupper body single arm actions that involve objects. 2013 Elsevier Inc. All rights reserved.	action algebra;action potential;approximation algorithm;care-of address;coat of arms;comment (computer programming);computation;concatenation;emoticon;exemplification;experiment;hidden markov model;human–robot interaction;identifier;language primitive;markov chain;online and offline;particle filter;robot;scheme;stochastic grammar;synthetic intelligence;viewing cone	Volker Krüger;Dennis Herzog	2013	Computer Vision and Image Understanding	10.1016/j.cviu.2013.02.002	computer vision;simulation;pose;computer science;mathematics;tracking	Vision	37.03538611492288	-42.58263195143347	39991
443ad3317d158dff467714b71bc73fc8ba59101b	embedded vision system for mobile robot navigation	vision system;processing element;image features;digital signal processors;digital signal processing;image processing;personal computer;stereo vision based navigation algorithm;block matching operations;image matching;path planning;systolic array processor;systolic array;visual navigation;mobile robots;indexing terms;image processing functions;embedded vision system;acceleration;navigation;mobile service;robot vision;large scale integration;energy consumption;10 w embedded vision system mobile robot navigation image processing functions digital signal processor spatial filtering feature extraction block matching operations systolic array processor correlation based image matching stereo vision based navigation algorithm;machine vision;feature extraction;10 w;mobile robot navigation;stereo vision;spatial filtering;digital signal processor;block matching;power consumption;robot vision feature extraction image matching mobile robots path planning;processing speed;correlation based image matching;low power consumption;machine vision mobile robots navigation image processing acceleration energy consumption large scale integration hardware digital signal processors digital signal processing;hardware	We developed an embedded vision system that can accelerate the basic image processing functions for mobile robot navigation with compact hardware featuring low power consumption. The system is composed of a digital signal processor (DSP) and a dedicated LSI for low-level image processing, specifically for spatial filtering, feature extraction, and block matching operations. The image processing LSI has a dedicated systolic array processor consisting of 64 processing elements to accelerate basic block operations for image feature calculation and correlation-based image matching. The power consumption is only 10 W, about one-seventh that of a typical Pentium 4 personal computer, but the processing speed for correlation matching is roughly three times faster than such a system. With this vision system, we implemented a stereo-vision based navigation algorithm on our mobile service robot and performed a visual navigation experiment in a building hallway	algorithm;application-specific integrated circuit;array processing;basic block;digital signal processor;embedded system;feature (computer vision);feature extraction;field-programmable gate array;high- and low-level;image processing;image registration;machine vision;mobile robot;pentium 4;personal computer;robotic mapping;service robot;signal processing;spatial anti-aliasing;stereopsis;systolic array;vector processor	Naoyuki Sawasaki;Manabu Nakao;Yoshinobu Yamamoto;Keiju Okabayashi	2006	Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.	10.1109/ROBOT.2006.1642108	embedded system;computer vision;digital signal processor;feature detection;template matching;machine vision;image processing;computer science;artificial intelligence;digital image processing;mobile robot navigation;computer graphics (images)	Robotics	44.418752672811046	-35.59902294896011	40030
bd205857decfaea7ef60b0097298a15b297c98e7	conditional gans for multi-illuminant color constancy: revolution or yet another approach?		Non-uniform and multi-illuminant color constancy are important tasks, the solution of which will allow to discard information about lighting conditions in the image. Non-uniform illumination and shadows distort colors of real-world objects and mostly do not contain valuable information. Thus, many computer vision and image processing techniques would benefit from automatic discarding of this information at the pre-processing step. In this work we propose novel view on this classical problem via generative end-to-end algorithm, namely image conditioned Generative Adversarial Network. We also demonstrate the potential of the given approach for joint shadow detection and removal. Forced by the lack of training data, we render the largest existing shadow removal dataset and make it publicly available. It consists of approximately 6,000 pairs of wide field of view synthetic images with and without shadows.	algorithm;color;computer vision;distortion;end-to-end principle;image processing;preprocessor;synthetic intelligence;yet another	O V Sidorov	2018	CoRR		image processing;generative grammar;yet another;pattern recognition;standard illuminant;computer science;artificial intelligence;training set;field of view;color constancy;shadow	Vision	26.36491271849463	-49.93789149185884	40038
2b9632180ce981c5bddc5bda174ca4568ed648d0	unwrapping the eye for visible-spectrum gaze tracking on wearable devices	gaze orientation eye unwrapping visible spectrum gaze tracking wearable devices interference minimization light sources nonactive illumination methods iris contour location camera projection eye image 3d eye model three step robust circle fitting procedure;eye;human computer interaction;iris cameras robustness feature extraction solid modeling transforms calibration;solid modelling assisted living curve fitting eye gesture recognition human computer interaction iris recognition lighting object tracking;iris recognition;assisted living;feature extraction;solid modeling;object tracking;transforms;robustness;lighting;curve fitting;iris;calibration;gesture recognition;cameras;solid modelling	Wearable devices with gaze tracking can assist users in many daily-life tasks. When used for extended periods of time, it is desirable that such devices do not employ active illumination for safety reasons and to minimize interference from other light sources such as the sun. Most non active-illumination methods for gaze tracking attempt to locate the iris contour by fitting an ellipse. Although the camera projection causes the iris to appear as an ellipse in the eye image, it is actually a circle on the eye surface. Instead of searching for an ellipse in the eye image, the method proposed in this paper searches for a circle on the eye surface. To this end, the method calibrates a three-dimensional eye model based on the location of the corners of the eye. Using the 3D eye model, an input image is first transformed so that the eye's spherical surface is warped into a plane, thus “unwrapping” the eye. The iris circle is then detected on the unwrapped image by a three-step robust circle-fitting procedure. The location of the circle corresponds to the gaze orientation on the outside image. The method is fast to calibrate and runs in realtime. Extensive experimentation on embedded hardware and comparisons with alternative methods demonstrate the effectiveness of the proposed solution.	algorithm;embedded system;experiment;eye tracking;interference (communication);real-time computing;test-and-set;the circle (file system);wearable computer;wearable technology	Bernardo Rodrigues Pires;Michael Devyver;Akihiro Tsukada;Takeo Kanade	2013	2013 IEEE Workshop on Applications of Computer Vision (WACV)	10.1109/WACV.2013.6475042	computer vision;calibration;feature extraction;computer science;video tracking;gesture recognition;iris recognition;lighting;solid modeling;robustness;curve fitting;computer graphics (images)	Vision	47.53747366124244	-44.47533702813118	40098
da125896d1d5d46bdd6ee816befd6ad0e6779be9	binary associative memories applied to gray level pattern recalling	layer by layer;memoire associative;image segmentation;intelligence artificielle;imagen nivel gris;reconstruction image;reconstruccion imagen;image reconstruction;image niveau gris;segmentation image;courbe niveau;associative memory;memoria asociativa;artificial intelligence;inteligencia artificial;curva nivel;grey level image;contour line	In this paper we show how a binary memory can be used to recall gray- level patterns. Given a set of gray-level patterns to be first memorized: 1) Decompose each pattern into a set of binary patterns, and 2) Build a binary associative memory (one matrix for each binary layer) with each training pattern set (by layers). A given pattern or a distorted version of it is recalled in three steps: 1) Decomposition of the pattern by layers into its binary patterns, 2) Recovering of each one of its binary components, layer by layer also, and 3) Reconstruction of the pattern from the binary patterns already recalled in step 2. Conditions for perfect recall of a pattern either from the fundamental set or from a distorted version of one them are also given. Experiments are also provided.		Juan Humberto Sossa Azuela;Ricardo Barrón;Francisco Cuevas de la Rosa;Carlos Aguilar Ibáñez;Héctor Cortés	2004		10.1007/978-3-540-30498-2_66	iterative reconstruction;layer by layer;computer vision;computer science;artificial intelligence;mathematics;binary pattern;image segmentation;algorithm;contour line	Vision	34.79364201801082	-39.84456886734188	40117
fc1c58b94fda9be82cfb87f2f873760709ba638f	hyperspectral image classification based on deep auto-encoder and hidden markov random field		Hyperspectral Image (HSI) classification is one of the most persistent issue in remote sensing field. Recently, deep learning has attracted attention in HSI Classification field due to its accuracy and stronger generalization. This paper proposes a new spectral-spatial HSI classification approach developed on the deep learning concept of stacked-auto-encoders (SAE) based deep feature extraction and hidden Markov random field based segmentation. Specifically, First the SAE model is implemented as a spectral information-based classifier to extract the deep spectral features. Second, spatial information is obtained by using effective Hidden Markov random field (HMRF) based segmentation technique. Finally, maximum voting based criteria is employed to merge the extracted spectral and spatial information, which results in the precise spectral-spatial HSI classification. The characterization of the HSI with spectral spatial features results into more comprehensive analysis of HSI and to a more accurate classification. In general, use of spectral information resulted from the SAE process and spatial information by means of HMRF based segmentation and merging of spectral and spatial information by means of maximum voting based criteria, has a significant effect on the accuracy of the HSI classification. Experiments on real diverse hyperspectral data sets with different contexts and resolutions acquired by AVIRIS and ROSIS sensors show the accuracy of the proposed method and confirms that results of the proposed classification approach are comparable to several recently proposed HSI classification techniques.	algorithm;autoencoder;computer vision;deep learning;encoder;experiment;feature extraction;hidden markov random field;horizontal situation indicator;lr parser;logistic regression;markov chain;pixel;selective area epitaxy;sensor;stellar classification;whole earth 'lectronic link	Atif Mughees;Linmi Tao	2017	2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2017.8393336	support vector machine;hidden markov random field;deep learning;artificial intelligence;spatial analysis;machine learning;image segmentation;feature extraction;computer science;hidden markov model;contextual image classification	AI	30.78134294892594	-44.56659063503314	40137
14e911297321783cbb9b2719ff3cc9333e6d4650	data mining for activity extraction in video data	data mining	The exploration of large video data is a task which is now possible because of the advances made on object detection and tracking. Data mining techniques such as clustering are typically employed. Such techniques have mainly been applied for segmentation/indexation of video but knowledge extraction of the activity contained in the video has been only partially addressed. In this paper we present how video information is processed with the ultimate aim to achieve knowledge discovery of people activity in the video. First, objects of interest are detected in real time. Then, in an off-line process, we aim to perform knowledge discovery at two stages: 1) finding the main trajectory patterns of people in the video. 2) finding patterns of interaction between people and contextual objects in the scene. An agglomerative hierarchical clustering is employed at each stage. We present results obtained on real videos of the Torino metro (Italy).	cluster analysis;data mining;hierarchical clustering;interaction;object detection;online and offline;prototype;real-time computing	Jose Luis Patino;Etienne Corvée;François Brémond;Monique Thonnat	2008			computer vision;computer science;data science;video tracking;data mining	ML	38.586519133411194	-46.13739248265422	40198
4386d4ce3dae75351ccf62194803cf6b7f7bf670	parallel medical image reconstruction: from graphics processing units (gpu) to grids	distributed memory;paper;image processing;computed tomography;graphics processing units gpu;pet;cell processor;parallel programming;parallel and distributed computing;positron emission tomography;medical image reconstruction;positron emission tomography pet;cuda;medical image;image reconstruction;list mode osem algorithm;nvidia;graphic processing unit;cost effectiveness;medicine;parallel machines;parallel architecture;product quality;parallel architecture comparison;parallel programs;tomography;shared memory multiprocessor	We present and compare a variety of parallelization approaches for a real-world case study on modern parallel and distributed computer architectures. Our case study is a production-quality, time-intensive algorithm for medical image reconstruction used in computer tomography (PET). We parallelize this algorithm for the main kinds of contemporary parallel architectures: shared-memory multiprocessors, distributed-memory clusters, graphics processing units (GPU) using the CUDA framework, the Cell processor and, finally, how various architectures can be accessed in a distributed Grid environment. The main contribution of the paper, besides the parallelization approaches, is their systematic comparison regarding four important criteria: performance, programming comfort, accessibility, and cost-effectiveness. We report results of experiments on particular parallel machines of different architectures that confirm the findings of our systematic comparison.	accessibility;algorithm;blue (queue management algorithm);ct scan;cuda;cell (microprocessor);central processing unit;cognitive dimensions of notations;computer architecture;computer graphics;distributed memory;experiment;graphics hardware;graphics processing unit;iterative reconstruction;medical imaging;multi-core processor;openmp;parallel computing;polyethylene terephthalate;race condition;shared memory;tomography	Maraike Schellmann;Sergei Gorlatch;Dominik Meiländer;Thomas Kösters;Klaus P. Schäfers;Frank Wübbeling;Martin Burger	2010	The Journal of Supercomputing	10.1007/s11227-010-0397-z	parallel computing;distributed memory;image processing;computer science;theoretical computer science;tomography;pet;computer graphics (images)	HPC	41.50248431886845	-30.948418079288917	40216
3c4d30d674ef1e2f80cc18d8dc6b8281b760a3f5	seeing signs of danger: attention-accelerated hazmat label detection		Rescue robots and similar vehicles must recognize various visual objects. Some are of particular interest and must be reliably recognized, for example, hazard signs. Hazmat labels and other intentionally placed signs of danger are typically attached to walls, containers, or vehicles, in locations where they attract attention. These backgrounds typically are of relatively simple structure (though not guaranteed to be plain) while the labels have saturated colors and high contrasts. We provide a new dataset that contains such images and a novel hazmat detection method. It includes an attentional preselection, which exploits the salient design and placement of the labels to locate them, followed by a SIFT-based classification that determines the concrete label type. The results show substantial speed improvements and accuracy gains over the traditional method without an attention stage.	active vision;color;computation;emoticon;hazard (computer architecture);object detection;optical flow;outline of object recognition;robot;scale-invariant feature transform;software bug;speedup;statistical classification;top-down and bottom-up design;visual objects	Mahmoud A. Mohamed;Jan Tünnermann;Bärbel Mertsching	2018	2018 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)	10.1109/SSRR.2018.8468639	salient;computer vision;artificial intelligence;computer science;exploit;visual objects;scale-invariant feature transform;rescue robot	Robotics	39.449259829472055	-43.537348020926814	40254
79af8fb8a9fd93d0b3ef83ac4f6d2822ecd5965b	a method for large-scale l 1 -regularized logistic regression		Logistic regression with 1 regularization has been proposed as a promising method for feature selection in classification problems. Several specialized solution methods have been proposed for 1-regularized logistic regression problems (LRPs). However, existing methods do not scale well to large problems that arise in many practical settings. In this paper we describe an efficient interior-point method for solving 1-regularized LRPs. Small problems with up to a thousand or so features and examples can be solved in seconds on a PC. A variation on the basic method, that uses a preconditioned conjugate gradient method to compute the search step, can solve large sparse problems, with a million features and examples (e.g., the 20 Newsgroups data set), in a few tens of minutes, on a PC. Numerical experiments show that our method outperforms standard methods for solving convex optimization problems as well as other methods specifically designed for 1regularized LRPs. Introduction Logistic regression Let x ∈ R denote a vector of feature variables, and b ∈ {−1,+1} denote the associated binary output. In the logistic model, the conditional probability of b, given x, has the form Prob(b|x) = 1/(1 + exp ( −b(w x + v) ) ). The parameters of this model are v ∈ R (the intercept) and w ∈ R (the weight vector). Suppose we are given a set of training or observed examples, (xi, bi) ∈ R × {−1,+1}, i = 1, . . . , m, assumed to be independent samples from a distribution. The model parameters w and v can be found by maximum likelihood estimation from the observed examples. The maximum likelihood estimate minimizes the average loss lavg(v, w) = (1/m) m ∑	computational complexity theory;conjugate gradient method;convex optimization;exptime;emoticon;experiment;feature selection;interior point method;least squares;logistic regression;mathematical optimization;matrix regularization;numerical linear algebra;preconditioner;series acceleration;sparse matrix;visual intercept	Kwangmoo Koh;Seung-Jean Kim;Stephen P. Boyd	2007			mathematical optimization;machine learning;statistics	ML	25.447129300209134	-33.781097872077	40337
94563aecc8ddf0b7e46bb83d4440f622a36ad28a	non-linear extension of generalized hyperplane approximation		A non-linear extension of generalized hyperplane approximation (GHA) method is introduced in this letter. Although GHA achieved a high-confidence result in motion parameter estimation by utilizing the supervised learning scheme in histogram of oriented gradient (HOG) feature space, it still has unstable convergence range because it approximates the non-linear function of regression from the feature space to the motion parameter space as a linear plane. To extend GHA into a non-linear regression for larger convergence range, we derive theoretical equations and verify this extension’s effectiveness and efficiency over GHA by experimental results. key words: non-linear regression, feature augmentation, generalized hyperplane approximation, learning-based motion parameter estimation, object tracking	approximation;bibo stability;control theory;estimation theory;feature vector;generalized hebbian algorithm;gradient;linear function;nonlinear system;supervised learning	Hyun-Chul Choi	2016	IEICE Transactions		computer vision;mathematical optimization;half-space;computer science;machine learning;video tracking;pattern recognition;mathematics;nonlinear regression;statistics	ML	29.78424909573988	-40.50084875555314	40386
4660d85e8f77be65bcc41b37985a8dd19924fed7	virtual top view: towards real-time agregation of videos to monitor large areas		Currently, large areas are continuously monitored by camera networks, whereas an overall situation assessment within a reasonable time is a crucial requirement. In this paper, we propose our Virtual Top View (VTV) approach that provides a clear, concise and direct interpretation of on-field activities in real-time preserving the spatial relationship and, technically, employs planar homography to aggregate the Virtual Top View out of multiple, individual video streams. With an increasing number of cameras or size of the monitored area, the aggregation process slows down. Therefore, we develop acceleration methods (autogenerated warp maps) to achieve a real-time aggregation within large camera networks. Finally, we evaluate the performance and demonstrate our approach in an intra-logistics environment.	real-time transcription	Hagen Borstell;Saira Saleem Pathan;Michael Soffner;Klaus Richter	2013		10.1007/978-3-642-40246-3_68	database	Vision	38.87516284206999	-44.37686422357023	40408
778c9f88839eb26129427e1b8633caa4bd4d275e	pose pooling kernels for sub-category recognition	detectors;kernel;training data;face recognition;birds;vectors;pose estimation face recognition learning artificial intelligence pattern classification;feature extraction;pattern classification;head;learning artificial intelligence;kernel based learning methods pose pooling kernels subcategory recognition super category landmarks volumetric models morphable models faces pose keypoint configurations poselet based pose normalization nearest neighbor methods;kernel birds vectors training data detectors feature extraction head;pose estimation	The ability to normalize pose based on super-category landmarks can significantly improve models of individual categories when training data are limited. Previous methods have considered the use of volumetric or morphable models for faces and for certain classes of articulated objects. We consider methods which impose fewer representational assumptions on categories of interest, and exploit contemporary detection schemes which consider the ensemble of responses of detectors trained for specific pose-keypoint configurations. We develop representations for poselet-based pose normalization using both explicit warping and implicit pooling as mechanisms. Our method defines a pose normalized similarity or kernel function that is suitable for nearest-neighbor or kernel-based learning methods.	3d computer graphics;database normalization;ibm notes;k-nearest neighbors algorithm;kernel (operating system);nearest-neighbor interpolation;normalization (image processing);sensor	Ning Zhang;Ryan Farrell;Trevor Darrell	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6248364	facial recognition system;computer vision;training set;detector;kernel;pose;3d pose estimation;feature extraction;computer science;machine learning;pattern recognition;head	Vision	30.167319060048854	-47.3101231158363	40410
437581a02bae8a177dae727f4ecd0dbe2eb6c77d	segmentation and tracking of static and moving objects in video surveillance scenarios	moving object;video surveillance;image motion analysis;monocular video sequences;tracking system;image segmentation;real time object tracking system;multi level;radiation detectors;video surveillance scenarios;static camera;mean shift;image classification;indexing terms;moving foreground;motion segmentation;estimation;registers;modified mean shift tracking algorithm;pixel;object tracking;video surveillance object detection real time systems image segmentation motion analysis pixel video sequences cameras layout motion estimation;multilevel foreground segmentation;modified mean shift tracking algorithm video surveillance scenarios real time object tracking system monocular video sequences static camera pixel based foreground detection system foreground object tracking multilevel foreground segmentation moving foreground static segmentation;target tracking;foreground segmentation;pixel based foreground detection system;foreground object tracking;mean shift foreground segmentation tracking multi level;cameras;tracking;object detection;static segmentation;real time systems;video surveillance cameras image classification image motion analysis image segmentation object detection target tracking	In this paper we present a real-time object tracking system for monocular video sequences with static camera. The workflow is based on a pixel-based foreground detection system followed by foreground object tracking. The foreground detection method performs the segmentation in three levels: Moving Foreground, Static Foreground and Background level. The tracking uses the foreground segmentation for identifying the tracked objects, but minimizes the reliance on the foreground segmentation, using a modified Mean Shift tracking algorithm. Combining this tracking system with the Multi-Level foreground segmentation, we have improved the tracking results using the classification in static or moving objects. The system solves successfully a high percentage of the moving objects occlusions, and most of the occlusions between static and moving objects.	algorithm;closed-circuit television;mean shift;pixel;real-time clock;tracking system	Jaime Gallego;Montse Pardàs;José Luis Landabaso	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4712355	computer vision;estimation;contextual image classification;index term;background subtraction;mean-shift;tracking system;computer science;video tracking;tracking;image segmentation;processor register;particle detector;pixel;statistics	Robotics	44.58796999240059	-45.689229684354345	40461
51ffc215f7c6c5644c323a5c45591576906e9c1f	a simple calibration for upper limb motion tracking and reconstruction	arm joint length estimation inertial sensor based upper limb motion tracking inertial sensor based upper limb motion reconstruction calibration method position trajectory reconstruction elbow joints wrist joints shoulder joint kinect captured pseudoground truth;elbow;wrist;joints;trajectory;vectors;joints calibration elbow vectors wrist trajectory shoulder;shoulder;calibration;sensors biomechanics biomedical measurement calibration motion compensation	This paper extends the work of inertial sensor based upper limb motion tracking by introducing a simple calibration method to automatically construct a global reference frame and estimate arm length. The method has effectively eliminated the requirement of manually aligning the sensors' local reference frames when multiple sensors are used to track the movements of the individual arm segments. The capacity of arm length estimation also makes it possible to reconstruct position trajectories of the elbow and the wrist joints in a reference frame with the shoulder joint as the origin. Verification of the algorithm has been done by comparing the estimated arm length with the Kinect captured pseudo ground truth. Effectiveness of the algorithm can be observed by visualizing the reconstructed position trajectories of the arm joints.	activities of daily living (activity);arm span;articular system;calibration;elbow joint structure;frame (physical object);ground truth;imagery;joint structure of shoulder region;kinect;less than;movement;pseudo brand of pseudoephedrine;reference frame (video);upper extremity;upper arm;verification of theories;wrist joint;algorithm;sensor (device)	Yan Wang;James Y. Xu;Xiaoxu Wu;Gregory J. Pottie;William J. Kaiser	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6944963	calibration;simulation;engineering;trajectory;physics;anatomy;surgery	Robotics	49.060264011136155	-45.42798152637629	40479
656e7c7739e3f334d4f275c71499485501aabc44	a two-step methodology for human pose estimation increasing the accuracy and reducing the amount of learning samples dramatically		In this paper, we present a two-step methodology to improve existing human pose estimation methods from a single depth image. Instead of learning the direct mapping from the depth image to the 3D pose, we first estimate the orientation of the standing person seen by the camera and then use this information to dynamically select a pose estimation model suited for this particular orientation. We evaluated our method on a public dataset of realistic depth images with precise ground truth joints location. Our experiments show that our method decreases the error of a state-of-the-art pose estimation method by 30%, or reduces the size of the needed learning set by a factor larger than 10.	3d pose estimation;experiment;ground truth	Samir Azrour;Sébastien Piérard;Pierre Geurts;Marc Van Droogenbroeck	2017		10.1007/978-3-319-70353-4_1	computer science;artificial intelligence;computer vision;pattern recognition;ground truth;pose	Vision	50.51902053912348	-44.98504881109362	40502
fd834c21a855b1a77b0338e4da99edb8b4267658	parameter estimation of unknown rigid objects moving freely in non-gravity field by stereo vision	estimation method;robot vision;weightlessness unknown rigid objects nongravity field stereo vision physical parameter estimation;aerospace robotics;stereo image processing;stereo vision;parameter estimation stereo vision cameras orbital robotics angular velocity motion estimation lenses computational intelligence computer simulation space missions;parameter estimation;computer simulation;stereo image processing aerospace robotics robot vision parameter estimation	In this paper, we discuss the estimation method of physical parameters of an unknown rigid object which moves freely in nongravity field by using stereo vision. The physical parameters cannot be estimated by only observing the change of the position and orientation of the object. A known object is shot to the unknown target object, then the collision happens. The collision changes the movement of unknown and known objects. The physical parameter estimation method, based on the image data of two objects before and after the collision, is proposed. The effectiveness of the proposed method is evaluated by computer simulations.	estimation theory;stereopsis	Fumitoshi Matsuno;Tetsuya Sawada	1998		10.1109/IROS.1998.724795	computer stereo vision;computer simulation;stereo cameras;computer vision;simulation;computer science;stereopsis;estimation theory;statistics;computer graphics (images)	Robotics	53.03693256432745	-40.457723176661176	40535
0a0899ba3cb39eabe779bd73468f5e601c950457	ica-based probabilistic local appearance models	joint distribution ica based probabilistic local appearance models image modeling object detection object localization object appearance model independent component analysis distance sensitive histograming spatial dependencies nonrigid objects modeling accuracy spatial relationships modeling detection performance cluttered scenes image retrieval;probability;spatial dependence;feature extraction statistical analysis probability object detection;histograms independent component analysis object detection layout availability stochastic processes computational complexity microcomputers focusing image retrieval;independent component analysis;feature vector;statistical analysis;feature extraction;spatial relationships;image modeling;object detection	This paper proposes a novel image modeling scheme for object detection and localization. Object appearance is modeled by the joint distribution of k-tuple salient point feature vectors which are factorized component-wise after an independent component analysis (ICA). Also, we propose a distance-sensitive histograming technique for capturing spatial dependencies. The advantages over existing techniques include the ability to model non-rigid objects (at the expense of modeling accuracy) and the flexibility in modeling spatial relationships. Experiments show that ICA does improve modeling accuracy and detection performance. Experiments in object detection in cluttered scenes have demonstrated promising results.	experiment;independent computing architecture;independent component analysis;object detection	Xiang Sean Zhou;Baback Moghaddam;Thomas S. Huang	2001		10.1109/ICIP.2001.958978	spatial relation;independent component analysis;computer vision;object-class detection;feature vector;spatial dependence;feature extraction;computer science;machine learning;pattern recognition;probability;mathematics;statistics	Vision	44.77978851218077	-51.688036853357055	40572
ee368f0e6bcacc1fc97ca736f1c8b56e7aa7f7d7	linear cyclic pursuit based prediction of personal space violation in surveillance video	lucas kanade tracking algorithm linear cyclic pursuit based prediction personal space violation surveillance video human interaction single calibrated camera 2 5d coordinate system;video surveillance;video cameras;estimation vehicles nickel;target tracking;video surveillance calibration target tracking video cameras;short term prediction human interaction linear cyclic pursuit;calibration	Analysis of human interaction in a social gathering is of high interest in security and surveillance applications. It is also of psychological interest to study the interaction to get a better understanding of the participant behavior. This paper is an attempt to explore and analyze interactions among the individuals from a single calibrated camera. We are particularly interested in trajectory prediction. These predicted trajectories of individuals are then used in predicting personal space violation. Each individual, represented by a feature point in a 2.5D coordinate system, is tracked using Lucas-Kanade tracking algorithm. We use the linear cyclic pursuit framework to model this point motion. This model is used for short-term prediction of individual trajectory. We demonstrate these ideas on different types of datasets.	2.5d;algorithm;closed-circuit television;interaction;kanade–lucas–tomasi feature tracker	Neha Bhargava;Subhasis Chaudhuri;Guna Seetharaman	2013	2013 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)	10.1109/AIPR.2013.6749324	computer vision;simulation;geography;video tracking;computer graphics (images)	Vision	39.18153834949319	-45.75584574685208	40583
b5b450a7fa63def80236fb8ab944eb5d5f1e22ff	on mutual information-based control of range sensing robots for mapping applications	laser rangefinder mutual information based control range sensing robots information content spatial realization mapping robot occupancy grid map binary bayesian filter narrow beam based sensor model robot control omnidirectional ground robot;bayes methods;laser ranging;robots;robots bayes methods laser ranging;robot sensing systems mutual information random variables measurement by laser beam bayes methods entropy	In this paper we examine the correlation between the information content and the spatial realization of range measurements taken by a mapping robot. To do so, we consider the task of constructing an occupancy grid map with a binary Bayesian filter. Using a narrow beam-based sensor model (versus an additive white Gaussian noise model), we prove that any controller tasked to maximize a mutual information reward function is eventually attracted to unexplored space. This intuitive behavior is derived solely from the geometric dependencies of the occupancy grid mapping algorithm and the monotonie properties of mutual information. Since it is a function of both the robot's position and the uncertainty of the surrounding cells, mutual information encodes geometric relationships that are fundamental to robot control, thus yielding geometrically relevant reward surfaces on which the robot can navigate. Lastly, we present the results of two experiments employing an omnidirectional ground robot equipped with a laser rangefinder.	additive white gaussian noise;algorithm;bayesian network;experiment;integrated development environment;motion planning;mutual information;reinforcement learning;robot control;self-information;utility functions on indivisible goods	Brian J. Julian;Sertac Karaman;Daniela Rus	2013	2013 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2013.6697102	robot;control engineering;monte carlo localization;computer vision;simulation;computer science;engineering;artificial intelligence;occupancy grid mapping	Robotics	51.96065804107467	-33.42850586231419	40606
9514a2bda166ddc50ffd2626a51f65e82866b25c	probabilistic modeling of human movements for intention inference		Inference of human intention may be an essential step towards understanding human actions and is hence important for realizing efficient human-robot interaction. In this paper, we propose the Intention-Driven Dynamics Model (IDDM), a latent variable model for inferring unknown human intentions. We train the model based on observed human movements/actions. We introduce an efficient approximate inference algorithm to infer the human’s intention from an ongoing movement. We verify the feasibility of the IDDM in two scenarios, i.e., target inference in robot table tennis and action recognition for interactive humanoid robots. In both tasks, the IDDM achieves substantial improvements over state-of-the-art regression and classification.	approximation algorithm;humanoid robot;human–robot interaction;latent variable model	Zhikun Wang;Marc Peter Deisenroth;Heni Ben Amor;David Vogt;Bernhard Schölkopf;Jan Peters	2012		10.15607/RSS.2012.VIII.055	approximate inference;artificial intelligence;humanoid robot;computer science;machine learning;latent variable model;probabilistic logic;inference	Robotics	49.109112133766565	-28.231889857439104	40612
773f9be0297a293f83dadfcf51fd2841a604350c	a dynamic approach to the recognition of 3d facial expressions and their temporal models	facial expression recognition;image motion analysis;image segmentation;temporal dynamics;hidden markov model;training;emotion recognition;bu 4dfe database;three dimensional;3d motion based feature;face recognition;hidden markov models;solid modelling emotion recognition face recognition feature extraction hidden markov models image motion analysis;three dimensional displays;feature extraction;image sequence;bu 4dfe database 3d facial expression recognition temporal model 3d motion based feature 3d facial geometry sequence feature selection method hidden markov model;3d facial expression recognition;3d facial geometry sequence;feature selection;facial expression;feature extraction three dimensional displays hidden markov models image sequences face recognition training image segmentation;temporal model;solid modelling;feature selection method;image sequences	In this paper we propose a method that exploits 3D motion-based features between frames of 3D facial geometry sequences for dynamic facial expression recognition. An expressive sequence is modeled to contain an onset followed by an apex and an offset. Feature selection methods are applied in order to extract features for each of the onset and offset segments of the expression. These features are then used to train a Hidden Markov Model in order to model the full temporal dynamics of the expression. The proposed fully automatic system was tested in a subset of the BU-4DFE database for the recognition of happiness, anger and surprise. Comparisons with a similar system based on the motion extracted from facial intensity images was also performed. The attained results suggest that the use of the 3D information does indeed improve the recognition accuracy when compared to the 2D data.	apex (geometry);experiment;facial recognition system;feature selection;hidden markov model;markov chain;onset (audio)	Georgia Sandbach;Stefanos P. Zafeiriou;Maja Pantic;Daniel Rueckert	2011	Face and Gesture 2011	10.1109/FG.2011.5771434	computer vision;speech recognition;computer science;pattern recognition	Vision	37.71222819346255	-49.843937396988395	40623
56ee4c2dfde20de4b8109ed39e8cd425bfe8302f	leveraging orientation knowledge to enhance human pose estimation methods	3d;depth camera;orientation;machine learning;human pose estimation	Predicting accurately and in real-time 3D body joint positions from a depth image is the cornerstone for many safety, biomedical, and entertainment applications. Despite the high quality of the depth images, the accuracy of existing human pose estimation methods from single depth images remains insufficient for some applications. In order to enhance the accuracy, we suggest to leverage a rough orientation estimation to dynamically select a 3D joint position prediction model specialized for this orientation. This orientation estimation can be obtained in real-time either from the image itself, or from any other clue like tracking. We demonstrate the merits of this general principle on a pose estimation method similar to the one used with Kinect cameras. Our results show that the accuracy is improved by up to 45.1 %, with respect to a method using the same model for all orientations.	3d pose estimation;display resolution;kinect;real-time clock;rough set	Samir Azrour;Sébastien Piérard;Marc Van Droogenbroeck	2016		10.1007/978-3-319-41778-3_8	computer vision;pose;3d pose estimation;computer science;machine learning;pattern recognition;articulated body pose estimation;orientation;3d computer graphics	Vision	50.757003327806096	-45.00590272061935	40642
0cb8e4a3efdf2a312323091668be171f4a8738c2	scheme for evaluation and reduction of motion artifacts in mobile vision systems	vision system;motion artifact	Artifacts like motion blur are a common problem for vision systems on mobile robots, especially when operating under low light conditions or when using high-resolution sensors. In this contribution we present a scheme for estimating the degree of motion artifacts, especially motion blur, present in a stream of individual camera images. A single quality estimate is derived per frame using data from an inertial measurement unit. Considering limited image processing capacity of resource-constrained mobile robots, we show a number of data processing strategies that are based upon the idea of congestion control by adaptive image rejection.	algorithm;gaussian blur;image processing;image resolution;image response;load (computing);mobile robot;network congestion;rejection sampling;sensor	Christoph Walter;Felix Penzlin;Norbert Elkmann	2009			computer vision;structure from motion;computer science;artificial intelligence	Robotics	51.25550601237501	-41.69157435939078	40648
8a85230fca6c77a7c9ed1d8d76decb03f4ed0a44	model selection for topic models via spectral decomposition		Correctly choosing the number of topics plays an important role in successfully applying topic models to real world applications. Following the latest tensor decomposition framework by Anandkumar et al., we make the first attempt to provide theoretical analysis on the number of topics under Latent Dirichlet Allocation model. With mild conditions, our method provides accessible information on the number of topics, which includes both upper and lower bounds. Experimental results on synthetic datasets demonstrate that our proposed bounds are correct and tight. Furthermore, using Gaussian Mixture Model as an example, we show that our methodology can be easily generalized for analyzing the number of mixture components in other mixture models.	latent dirichlet allocation;mixture model;model selection;synthetic intelligence;topic model	Dehua Cheng;Xinran He;Yan Liu	2015			latent dirichlet allocation;dynamic topic model;machine learning;pattern recognition;mathematics;statistics	ML	27.633470291338636	-30.99566553263344	40729
73e7ae0314a9646dca51e447f68ebb43dbe5a884	collaborative filtering in a non-uniform world: learning with the weighted trace norm	matrix completion;science learning;collaborative filtering	We show that matrix completion with tracenorm regularization can be significantly hurt when entries of the matrix are sampled nonuniformly. We introduce a weighted version of the trace-norm regularizer that works well also with non-uniform sampling. Our experimental results demonstrate that the weighted trace-norm regularization indeed yields significant gains on the (highly non-uniformly sampled) Netflix dataset.	collaborative filtering;manifold regularization;matrix regularization;nonuniform sampling;sampling (signal processing);the matrix	Ruslan Salakhutdinov;Nathan Srebro	2010			mathematical optimization;computer science;collaborative filtering;machine learning;data mining;mathematics	ML	25.25711335712076	-35.26414870614506	40764
ea078f38852eeff00f1a78e4e20b33fd158f9547	real-time modeling of vascular flow for angiography simulation	minimally invasive;real time;acute stroke;blood pressure;interventional neuroradiology;distribution pattern;blood flow;contrast agent	Interventional neuroradiology is a growing field of minimally invasive therapies that includes embolization of aneurysms and arteriovenous malformations, carotid angioplasty and carotid stenting, and acute stroke therapy. Treatment is performed using image-guided instrument navigation through the patient's vasculature and requires intricate combination of visual and tactile coordination. In this paper we present a series of techniques for real-time high-fidelity simulation of angiographic studies. We focus in particular on the computation and visualization of blood flow and blood pressure distribution patterns, mixing of blood and contrast agent, and high-fidelity simulation of fluoroscopic images.	acm siggraph;aclarubicin;aneurysm;bone structure of spine;cardiology discipline;cerebrovascular accident;computation;computation (action);congenital abnormality;contrast media;diagnostic radiologic examination;ehlers-danlos syndrome;imagery;interventional neuroradiology;lecture notes in computer science;liquid substance;medical ultrasound;parkinson disease;patients;radiology;real-time clock;real-time locating system;real-time transcription;roentgen rays;simulation;simulators;springer (tank);structure of anatomic arteriovenous anastomosis;structure of vertebral artery;therapeutic procedure;ultrasonography, doppler, transcranial;volume	Xunlei Wu;Jérémie Allard;Stephane Cotin	2007	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-540-75757-3_68	radiology;medicine;pathology;blood flow;blood pressure;surgery	Robotics	39.708135873405176	-38.84029334134887	40796
685bfd75d1a625ed4c102ad4a42135f3ce7daad8	sparsity induced similarity measure for label propagation	unlabeled data;nearest neighbor searches;label propagation;graph based semi supervised learning;sparsity induced similarity measure;scene classification;sparse linear combination;euclidean distance;usa councils;semi supervised learning;training data;vectors;sparse decomposition;pixel;pattern recognition learning artificial intelligence;semisupervised learning nearest neighbor searches euclidean distance training data gain measurement layout kernel phase estimation phase measurement pattern recognition;linear programming;bag of words approach;pattern recognition;learning artificial intelligence;bag of words;similarity measure;scene classification sparsity induced similarity measure label propagation graph based semi supervised learning sparse linear combination sparse decomposition bag of words approach;object model	Graph-based semi-supervised learning has gained considerable interests in the past several years thanks to its effectiveness in combining labeled and unlabeled data through label propagation for better object modeling and classification. A critical issue in constructing a graph is the weight assignment where the weight of an edge specifies the similarity between two data points. In this paper, we present a novel technique to measure the similarities among data points by decomposing each data point as an L1 sparse linear combination of the rest of the data points. The main idea is that the coefficients in such a sparse decomposition reflect the point's neighborhood structure thus providing better similarity measures among the decomposed data point and the rest of the data points. The proposed approach is evaluated on four commonly-used data sets and the experimental results show that the proposed Sparsity Induced Similarity (SIS) measure significantly improves label propagation performance. As an application of the SIS-based label propagation, we show that the SIS measure can be used to improve the Bag-of-Words approach for scene classification.	bag-of-words model in computer vision;coefficient;data point;semi-supervised learning;semiconductor industry;similarity measure;software propagation;sparse matrix;supervised learning	Hong Cheng;Zicheng Liu;Jie Yang	2009	2009 IEEE 12th International Conference on Computer Vision	10.1109/ICCV.2009.5459267	training set;object model;computer science;linear programming;bag-of-words model;machine learning;pattern recognition;data mining;euclidean distance;mathematics;pixel	Vision	27.786133326777364	-43.162424860971434	40811
78c2a503cce428082453fa186b2a20a41c9d75d7	multiple imputation in principal component analysis	point estimation;bootstrap;62g09;principal component analysis;procrustes rotation;62h25;multiple imputation;missing values;em algorithm	The available methods to handle missing values in principal component analysis only provide point estimates of the parameters (axes and components) and estimates of the missing values. To take into account the variability due to missing values a multiple imputation method is proposed. First a method to generate multiple imputed data sets from a principal component analysis model is defined. Then, two ways to visualize the uncertainty due to missing values onto the principal component analysis results are described. The first one consists in projecting the imputed data sets onto a reference configuration as supplementary elements to assess the stability of the individuals (respectively of the variables). The second one consists in performing a principal component analysis on each imputed data set and fitting each obtained configuration onto the reference one with Procrustes rotation. The latter strategy allows to assess the variability of the principal component analysis parameters induced by the missing values. The methodology is then evaluated from a real data set.	geo-imputation;missing data;principal component analysis;spatial variability	Julie Josse;Jérôme Pagès;François Husson	2011	Adv. Data Analysis and Classification	10.1007/s11634-011-0086-7	econometrics;expectation–maximization algorithm;missing data;point estimation;data mining;mathematics;imputation;multiple correspondence analysis;statistics;principal component analysis	ML	29.548339270047435	-25.030161242416593	40833
7c1c7a2de15d950e74ff671020b12bf2c0d12b93	a new strategy for structural health monitoring based on structural destroyed mode and data correlation	deflected curve;structural safety monitoring ssm;damage indices;structural destroyed mode;structural health monitoring shm	This paper provides a new strategy for study of Structural Health Monitoring, and discusses establishment of structural safety monitoring system based on the structural destroyed modes and data correlation. Two concepts, structural destroyed mode and data correlation, are given and discussed. The structural destroyed mode refers to the pattern of certain structural state or situation that some kind of failure occurred. The study on data correlation focuses on the relations existed between different sensors and locations. A structural safety monitoring system for a simply supported beam including several damage indices is built, and the numerical experiment shows it works effectively.	numerical analysis;sensor	Simeng Liu;Liangliang Zhang	2012	Intelligent Automation & Soft Computing	10.1080/10798587.2012.10643276	reliability engineering;machine learning;beam (structure);artificial intelligence;computer science;structural health monitoring;safety monitoring;correlation	AI	37.851576132828846	-29.257560454997446	40850
d53a081b6e92a0c654fff88bdb8065ac53e73f20	robust virtual scan for obstacle detection in urban environments	sorted array based acceleration method robust virtual scan intelligent vehicle environmental sensing 3d light detection and ranging sensor 3d lidar sensor dense point cloud vehicle detection vehicle tracking 3d point cloud compression free space representation obstacle representation occupancy grid map slam steep ramps road curbs overhung barrier gates complex urban environment data structure basic vscan matrix bvsm road filtering obstacle detection method;optical radar collision avoidance data structures image filtering image representation intelligent transportation systems object detection;roads three dimensional displays automotive electronics robustness urban areas laser radar two dimensional displays	Obstacle detection is an essential technique for intelligent vehicles. Environmental sensing especially plays a vital role to achieve accurate obstacle detection. Unlike classical 2D scan, emerging 3D Light Detection and Ranging (LiDAR) sensors can scan dense point cloud at one time, which represents detailed information of urban environments. The downside of obstacle detection using 3D LiDAR, on the other hand, is its computational cost posed by a large amount of 3D data. The virtual scan (VScan), first introduced by Petrovskaya et al. [1] for efficient vehicle detection and tracking, is a 2D compression of 3D point cloud to represent free space, obstacles and unknown areas. To overcome the computational problem of obstacle detection using 3D LiDAR, therefore, VScan is suitable. In addition, it can bridge across new-born 3D LiDAR sensors and many matured applications based on 2D scan, including occupancy grid map, SLAM, planning, detection, and tracking, due to its 2D representation of 3D point cloud. A key challenge to VScan for intelligent vehicles is that we must improve robustness of VScan in complex urban environments. For example, steep ramps with large slope, low curbs along the road, and overhung barrier gates at the entrance often make VScan mis-behave. In this paper, we present a robust VScan generation method for intelligent vehicles driving in complex urban environments. Our method uses a new data structure, called basic VScan matrix (BVSM), to represent 3D point cloud around the own vehicle. We also develop (i) a simultaneous road filtering and obstacle detection method that works on top of BVSM to generate robust VScan generation, and (ii) a sorted array based acceleration method to perform the VScan generation in real-time. Experimental results from field testing, where steep ramps, barrier gates, and curbs are contained, demonstrate that our method improves the robustness and the speed of VScan generation as compared to traditional methods. The computational cost of generating VScan with 2000 beams is limited within 15ms. The result of obstacle detection on the road with large slopes is stable in the range of 60m with a few number of false positives.	algorithmic efficiency;automated planning and scheduling;computational problem;data structure;hereditary property;point cloud;real-time clock;sensor;simultaneous localization and mapping;software architecture analysis method;sorted array;sorting	Mengwen He;Eijiro Takeuchi;Yoshiki Ninomiya;Shinpei Kato	2016	2016 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2016.7535461	computer vision;simulation;geography;remote sensing	Robotics	52.361821976661176	-36.44268199881762	40880
3bc2f294f9b914ff20810647f026925c96d55646	geospatial correspondences for multimodal registration		The growing availability of very high resolution (<;1 m/pixel) satellite and aerial images has opened up unprecedented opportunities to monitor and analyze the evolution of land-cover and land-use across the world. To do so, images of the same geographical areas acquired at different times and, potentially, with different sensors must be efficiently parsed to update maps and detect land-cover changes. However, a naϊve transfer of ground truth labels from one location in the source image to the corresponding location in the target image is generally not feasible, as these images are often only loosely registered (with up to ± 50m of non-uniform errors). Furthermore, land-cover changes in an area over time must be taken into account for an accurate ground truth transfer. To tackle these challenges, we propose a mid-level sensor-invariant representation that encodes image regions in terms of the spatial distribution of their spectral neighbors. We incorporate this representation in a Markov Random Field to simultaneously account for nonlinear mis-registrations and enforce locality priors to find matches between multi-sensor images. We show how our approach can be used to assist in several multimodal land-cover update and change detection problems.	aerial photography;ground truth;image registration;image resolution;locality of reference;map;markov chain;markov random field;multimodal interaction;nonlinear system;parsing;pixel;sensor	Diego Marcos;Raffay Hamid;Devis Tuia	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.550	computer vision;geospatial analysis;artificial intelligence;computer science	Vision	47.185434576677174	-50.221186121229685	40921
9d3a319ca777d845639faed11bad229a134f03c3	implicit regularization in variational bayesian matrix factorization	matrix factorization;variational bayesian	Matrix factorization into the product of lowrank matrices induces non-identifiability, i.e., the mapping between the target matrix and factorized matrices is not one-to-one. In this paper, we theoretically investigate the influence of non-identifiability on Bayesian matrix factorization. More specifically, we show that a variational Bayesian method involves regularization effect even when the prior is non-informative, which is intrinsically different from the maximum a posteriori approach. We also extend our analysis to empirical Bayes scenarios where hyperparameters are also learned from data.	calculus of variations;conjugate variables;information;matrix regularization;multimodal interaction;one-to-one (data model);statistical model;variational principle	Shinichi Nakajima;Masashi Sugiyama	2010			mathematical optimization;combinatorics;eigendecomposition of a matrix;incomplete cholesky factorization;incomplete lu factorization;computer science;pattern recognition;mathematics;matrix decomposition;non-negative matrix factorization	ML	28.19507537022472	-32.84451296243256	41007
0b9269625b0546cafc37f75c431fef6ef3e71342	a unified approach combining photometric and geometric information for pose estimation	cost function;gold standard;ground truth;synthetic data;pose estimation	In this paper, we present a novel approach for the relative pose estimation problem from point correspondences extracted from image pairs. Unlike classical algorithms, such as the Gold Standard algorithm, the proposed approach ensures that the matched points are photo-consistent throughout the pose estimation process. In fact, common algorithms use the photometric information to extract the feature points and to establish the 2D point correspondences. Then, they focus on minimizing, in a non-linear scheme, geometric distances between the projection of reconstructed 3D points and the coordinates of the extracted image points without taking the photometric information into account. The approach we propose in this paper merges geometric and photometric information in a unified cost function for the final non-linear minimization. This allows us to achieve results with higher precision and also with higher convergence frequency. Extensive experiments with ground truth on synthetic data show the superiority of the proposed approach in terms of robustness and precision. The simulation results have been confirmed by several tests on real image data.	3d pose estimation;algorithm;baseline (configuration management);correspondence problem;estimation theory;experiment;ground truth;loss function;nonlinear system;reprojection error;simulation;synthetic data;virtual reality headset	Pierre Fite Georgel;Selim Benhimane;Nassir Navab	2008		10.5244/C.22.14	computer vision;econometrics;pose;3d pose estimation;ground truth;gold standard;computer science;mathematics;statistics;synthetic data	Vision	53.36454604565049	-49.70870358921563	41118
9af5d0d7d6106863629ea0a643ffa05f934e0ee7	svo: semidirect visual odometry for monocular and multicamera systems	robot vision simultaneous localization and mapping slam;semidirect visual odometry high frequency texture probabilistic depth estimation algorithm feature based methods image intensity gradients svo monocular systems multicamera systems;cameras feature extraction robustness tracking visualization real time systems optimization;image texture sensor fusion;visualization;feature extraction;robustness;optimization;cameras;tracking;real time systems	Direct methods for visual odometry (VO) have gained popularity for their capability to exploit information from all intensity gradients in the image. However, low computational speed as well as missing guarantees for optimality and consistency are limiting factors of direct methods, in which established feature-based methods succeed instead. Based on these considerations, we propose a semidirect VO (SVO) that uses direct methods to track and triangulate pixels that are characterized by high image gradients, but relies on proven feature-based methods for joint optimization of structure and motion. Together with a robust probabilistic depth estimation algorithm, this enables us to efficiently track pixels lying on weak corners and edges in environments with little or high-frequency texture. We further demonstrate that the algorithm can easily be extended to multiple cameras, to track edges, to include motion priors, and to enable the use of very large field of view cameras, such as fisheye and catadioptric ones. Experimental evaluation on benchmark datasets shows that the algorithm is significantly faster than the state of the art while achieving highly competitive accuracy.	algorithm;benchmark (computing);euro-vo;fisheye;gradient;mathematical optimization;pixel;sparse voxel octree;visual odometry	Christian Forster;Zichao Zhang;Michael Gassner;Manuel Werlberger;Davide Scaramuzza	2017	IEEE Transactions on Robotics	10.1109/TRO.2016.2623335	computer vision;simulation;visualization;feature extraction;computer science;tracking;robustness;computer graphics (images)	Vision	52.300899461137305	-47.19063688697892	41121
9096b9dd014ae99fc5e14e96222178432c9f3299	special issue: dynamic data-driven applications systems (dddas) concepts in signal processing		Dynamic Data Driven Applications Systems (DDDAS) is a transformative framework for incorporating evolving data into a dynamic system to adapt to operational conditions, and recursively to steer its measurement components that generate such data. Signal processing techniques are applied to big data analysis to spawn developments in designs, algorithms, architectures, and applications. DDDAS is based on four interactive concepts: applications modeling, mathematical and statistical processing, measurement systems, and systems software design, as shown in Fig. 1. Rooted in control theory, many of the prominent developments include methods of signal processing to determine the relevant data for model updates. These model updates arise from statistical data analysis. New insights have been developed for measurement systems with unobservable data, such as new architectures to emulate data collections for missing data. To complement the modeling and statistical analysis, software methods are paramount for realtime applications. With recent advances from Very Large Scale Integration (VLSI) and high performance computing (HPC), implementations of DDDAS concepts are realizable. The DDDAS framework has spawned numerous applications such as environment analysis (e.g. weather); robotic systems (e.g., unmanned aerial vehicle, unmanned ground vehicle (UAV/ UGV) coordination); image processing (e.g., target tracking), and embedded computing (e.g., hardware/software designs). This special issue brings together DDDAS examples for the readers. More information can be found at www.1dddas.org. The special issue received numerous submissions and two were selected based on the implementation of the entire DDDAS framework presented in Fig. 1. The first was S. Sarkar, et al., BDeep Learning for Automated Occlusion Edge Detection in RGB-D Frames^ (10.1007/s11265-016-1209-3). The authors addressed the challenge of real-time video tracking when objects are occluded. Using Deep Convolutional Neural Networks (CNN) algorithms from computer vision, they demonstrate robust occlusion edge determination via DDDAS processing of heterogeneous measurements from visual cameras, depth modeling, and motionmodels. The system level performance application was executed in experiments using parallel computing software with Graphical Processor Units (GPUs) from the Compute Unified Device Architecture (CUDA) reducing the false alarms from occlusion detection. The second was R. Wu, et al., BA Container-based Elastic Cloud Architecture for Pseudo Real-time Exploitation of Wide Area Motion Imagery (WAMI) Stream^ (10.1007/ s11265-016-1206-6)^. The authors addressed the difficulty of using large scale data as demonstrated with WAMI. The paper highlights software implementation solutions by using a cloud-based approach to decompose the imagery and enhance multi-target tracking. Their DDDAS approach included the Pseudo Real-time Exploitation of Sub-Area (PRESA) framework which divides the data into spatial regions for parallel processing of video tracking algorithms. Key to the approach was the stitching of tracks from the spatial regions. The results demonstrate that the virtual-machine container approached improved the processing frame rate, object detection, and tracking accuracy, as compared to approaches using the Hadoop method. * Erik Blasch erik.blasch@gmail.com		Erik Blasch;Shashi Phoha	2017	Signal Processing Systems	10.1007/s11265-017-1253-7	theoretical computer science;real-time computing;signal processing;computer science;dynamic data	Robotics	45.977206908658424	-37.773833839966585	41128
41ea49891cd4c42f537daa3a7c1cd76c23e839a2	hyperspectral image classification via fusing correlation coefficient and joint sparse representation		The joint sparse representation (JSR)-based classifier assumes that pixels in a local window can be jointly and sparsely represented by a dictionary constructed by the training samples. The class label of each pixel can be decided according to the representation residual. However, once the local window of each pixel includes pixels from different classes, the performance of the JSR classifier may be seriously decreased. Since correlation coefficient (CC) is able to measure the spectral similarity among different pixels efficiently, this letter proposes a new classification method via fusing CC and JSR, which attempts to use the within-class similarity between training and test samples while decreasing the between-class interference. First, the CCs among the training and test samples are calculated. Then, the JSR-based classifier is used to obtain the representation residuals of different pixels. Finally, a regularization parameter  $\lambda $  is introduced to achieve the balance between the JSR and the CC. Experimental results obtained on the Indian Pines data set demonstrate the competitive performance of the proposed approach with respect to other widely used classifiers.	coefficient;dictionary;experiment;horizontal situation indicator;interference (communication);java community process;pixel;sparse approximation;sparse matrix;window function	Bing Tu;Xiaofei Zhang;Xudong Kang;Guoyun Zhang;Jinping Wang;Jianhui Wu	2018	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2017.2787338	computer vision;support vector machine;mathematics;pixel;residual;hyperspectral imaging;regularization (mathematics);contextual image classification;sparse approximation;correlation coefficient;pattern recognition;artificial intelligence	Vision	29.85276281012912	-44.0149247039774	41158
3c0bf6d589cd781de37e486d72c9b3218ba5e9ba	weakly supervised learning of deep metrics for stereo reconstruction		Deep-learning metrics have recently demonstrated extremely good performance to match image patches for stereo reconstruction. However, training such metrics requires large amount of labeled stereo images, which can be difficult or costly to collect for certain applications (consider, for example, satellite stereo imaging). The main contribution of our work is a new weakly supervised method for learning deep metrics from unlabeled stereo images, given coarse information about the scenes and the optical system. Our method alternatively optimizes the metric with a standard stochastic gradient descent, and applies stereo constraints to regularize its prediction. Experiments on reference data-sets show that, for a given network architecture, training with this new method without ground-truth produces a metric with performance as good as state-of-the-art baselines trained with the said ground-truth. This work has three practical implications. Firstly, it helps to overcome limitations of training sets, in particular noisy ground truth. Secondly it allows to use much more training data during learning. Thirdly, it allows to tune deep metric for a particular stereo system, even if ground truth is not available.	algorithm;artificial neural network;correspondence problem;deep learning;esa;expect;experiment;graphics processing unit;ground truth;network architecture;overhead (computing);planetary scanner;scott continuity;similarity measure;stochastic gradient descent;supervised learning;teaching method;tesla (microarchitecture)	Stepan Tulyakov;Anton Ivanov;François Fleuret	2017	2017 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2017.150	artificial intelligence;computer vision;machine learning;iterative reconstruction;supervised learning;ground truth;network architecture;satellite;computer science;stochastic gradient descent;training set;pattern recognition;stereo imaging	Vision	27.558000282152555	-49.637430938354065	41288
323de58872f9ecf383a6e2a4e4357267c9eb42a1	unobtrusive fall detection at home using kinect sensor	fall detection;depth image and point cloud processing	The existing CCD-camera based systems for fall detection require time for installation and camera calibration. They do not preserve the privacy adequately and are unable to operate in low lighting conditions. In this paper we show how to achieve automatic fall detection using only depth images. The point cloud corresponding to floor is delineated automatically using v-disparity images and Hough transform. The ground plane is extracted by the RANSAC algorithm. The detection of the person takes place on the basis of the updated on-line depth reference images. Fall detection is achieved using a classifier trained on features representing the extracted person both in depth images and in point clouds. All fall events were recognized correctly on an image set consisting of 312 images of which 110 contained the human falls. The images were acquired by two Kinect sensors placed at two different locations.	algorithm;binocular disparity;camera resectioning;charge-coupled device;hough transform;kinect;online and offline;point cloud;random sample consensus;sensor;statistical classification	Michal Kepski;Bogdan Kwolek	2013		10.1007/978-3-642-40261-6_55	computer vision;computer science;computer graphics (images)	Vision	46.504644077095385	-42.99737202217212	41324
2a0e053f1b66f002492628bf161d568d4b64d9c9	adaptive multifeature visual tracking in a probability-hypothesis-density filtering framework	scale invariant feature;color histogram;probability hypothesis density;feature reliability	Probability hypothesis density (PHD) based trackers have enjoyed growing popularity in recent years, particularly in the field of nonlinear non-Gaussian visual tracking scenarios. These visual trackers can be degraded by a variety of factors, including changes of illumination, occlusion, poor image contrast, shape and appearance variation, clutter and other unmodeled changes of tracked objects. In this paper, for enhancing the performance of PHD based trackers, both scale invariant feature and color distribution feature are used as descriptors of targets of interest. To adaptively adjust the weights of each feature in the tracking process, a confidence measure, i.e., a quantitative measure for the spatial uncertainty of each feature is incorporated into the multifeature tracking algorithm. Experimental results show that the proposed multifeature tracker can improve the reliability and robustness of state estimation and the number estimation in tracking a variable number of objects of varying scales even when background region was similar to the object's appearance. & 2013 Elsevier B.V. All rights reserved.	algorithm;bittorrent tracker;cluster analysis;clutter;color histogram;dynamical system;nonlinear system;scale-invariant feature transform;sensor;video tracking;weight function	Jingjing Wu;Shiqiang Hu;Yang Wang	2013	Signal Processing	10.1016/j.sigpro.2013.04.016	color histogram;computer vision;computer science;machine learning;pattern recognition;mathematics;feature	Vision	43.00640176216501	-49.675766272074505	41359
f59cc71a8ebdb4de13dfa897393ea2f5eee9c037	unified camera tamper detection based on edge and object information	biological patents;camera tamper detection;biomedical journals;text mining;europe pubmed central;citation search;covered event;adaptive threshold;edge disappearance rate;defocused event;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;full text;moved event;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	In this paper, a novel camera tamper detection algorithm is proposed to detect three types of tamper attacks: covered, moved and defocused. The edge disappearance rate is defined in order to measure the amount of edge pixels that disappear in the current frame from the background frame while excluding edges in the foreground. Tamper attacks are detected if the difference between the edge disappearance rate and its temporal average is larger than an adaptive threshold reflecting the environmental conditions of the cameras. The performance of the proposed algorithm is evaluated for short video sequences with three types of tamper attacks and for 24-h video sequences without tamper attacks; the algorithm is shown to achieve acceptable levels of detection and false alarm rates for all types of tamper attacks in real environments.	algorithm;bluetooth;chamaecyparis lawsoniana;closed-circuit television;large;peg10 gene;physical object;pixel;relocation of home or business;video clip;algorithm	Gil-beom Lee;Myeong-jin Lee;Jongtae Lim	2015		10.3390/s150510315	text mining;medical research;computer science;bioinformatics;data mining;world wide web;computer security	Vision	40.8870654837209	-45.75545555162603	41414
8a8471f7ad114e553e83c0b3d9c6d8891d466085	algorithms for super-resolution of images and videos based on learning methods. (algorithmes de super-résolution d'images et de vidéos basés sur des méthodes d'apprentissage)		With super-resolution (SR) we refer to a class of techniques that enhance the spatial resolution of images and videos. SR algorithms can be of two kinds: multi-frame methods, where multiple low-resolution images are aggregated to form a unique highresolution image, and single-image methods, that aim at upscaling a single image. This thesis focuses on developing theory and algorithms for the single-image SR problem. In particular, we adopt the so called example-based approach, where the output image is estimated with machine learning techniques, by using the information contained in a dictionary of image examples . The examples consist in image patches, which are either extracted from external images or derived from the input image itself. For both kinds of dictionary, we design novel SR algorithms, with new upscaling and dictionary construction procedures, and compare them to state-of-the-art methods. The results achieved are shown to be very competitive both in terms of visual quality of the superresolved images and computational complexity. We then apply our designed algorithms to the video upscaling case, where the goal is to enlarge the resolution of an entire video sequence. The algorithms, opportunely adapted to deal with this case, are also analyzed in the coding context. The analysis conducted shows that, in speci c cases, SR can also be an e ective tool for video compression, thus opening new interesting perspectives.		Marco Bevilacqua	2014				Vision	28.776278911349056	-48.33457751676914	41429
187b607e839371da3b75a8b0fe4a514dc7ebe7ad	a novel fragments-based similarity measurement algorithm for visual tracking	visual tracking;appearance model;similarity measure	Various adaptive appearance models have been proposed to deal with the challenges in tracking objects such as occlusions, illumination changes, background clutter, and pose variation. In this paper, first, we present a novel Fragments-based Similarity Measurement algorithm for object tracking in video sequence. Both the target and the reference are divided by multiple fragments of the same size. Then, we find the similarity of each fragment with the overlapped smaller patches by comparing the average intensity value of the patches. The accuracy of the tracking results can be improved by adjusting the size of the patches. Finally we incorporate the global similarity measurement using two kinds of distances between them. This method encodes the color and the spatial information so that it can track non-rigid objects under complex scene. We use this coarse-to-fine method to get a balance between the accuracy and the computational cost. Extensive experiments are conducted to verify the efficiency and the reliability of our proposed algorithm in the realistic videos.	algorithm;algorithmic efficiency;clutter;color;computation;experiment;patch (computing)	Jun Shang;Chuanbo Chen;Hu Liang;He Tang;Mudar Sarem	2014	JCP	10.4304/jcp.9.9.2167-2172	computer vision;active appearance model;simulation;eye tracking;computer science;pattern recognition;mathematics	Vision	43.199392656202114	-50.08688148386074	41457
5102a61d92c7cdc75dd72e06dc58faad44b3f21a	global registration of 3d lidar point clouds based on scene features: application to structured environments		Acquiring 3D data with LiDAR systems involves scanning multiple scenes from different points of view. In actual systems, the ICP algorithm (Iterative Closest Point) is commonly used to register the acquired point clouds together to form a unique one. However, this method faces local minima issues and often needs a coarse initial alignment to converge to the optimum. This paper develops a new method for registration adapted to indoor environments and based on structure priors of such scenes. Our method works without odometric data or physical targets. The rotation and translation of the rigid transformation are computed separately, using, respectively, the Gaussian image of the point clouds and a correlation of histograms. To evaluate our algorithm on challenging registration cases, two datasets were acquired and are available for comparison with other methods online. The evaluation of our algorithm on four datasets against six existing methods shows that the proposed method is more robust against sampling and scene complexity. Moreover, the time performances enable a real-time implementation. Data Set: https://liris.cnrs.fr/3d-registration/ Data Set License: ODC Attribute License	algorithm;computation;converge;high-level programming language;iterative closest point;iterative method;maxima and minima;orthogonal defect classification;pascal;performance;point cloud;preprocessor;real-time computing;real-time transcription;real-time web;sampling (signal processing);sensor;time complexity	Julia Sánchez;Florence Denis;Paul Checchin;Florent Dupont;Laurent Trassoudaine	2017	Remote Sensing	10.3390/rs9101014	remote sensing;point cloud;geology;computer vision;rigid transformation;artificial intelligence;iterative closest point;gaussian surface;sampling (statistics);maxima and minima;lidar;histogram	Vision	53.41739691999392	-43.4814942492064	41468
4b25f45cccbd53f51597dc63bdc42ab9d5a0f716	social interaction discovery by statistical analysis of f-formations.	social interaction;statistical analysis	We present a novel approach for detecting social interactions in a crowded scene by employing solely visual cues. The detection of social interactions in unconstrained scenarios is a valuable and important task, especially for surveillance purposes. Our proposal is inspired by the social signaling literature, and in particular it considers the sociological notion of F-formation. A F-formation is a set of possible configurations in space that people may assume while participating in a social interaction. Our system takes as input the positions of the people in a scene and their (head) orientations; then, employing a voting strategy based on the Hough transform, it recognizes F-formations and the individuals associated with them. Experiments on simulations and real data promote our idea.	algorithm;ground truth;hough transform;interaction;orientation (graph theory);sensor;simulation;synthetic data	Marco Cristani;Loris Bazzani;Giulia Paggetti;Andrea Fossati;Diego Tosato;Alessio Del Bue;Gloria Menegaz;Vittorio Murino	2011		10.5244/C.25.23	computer vision;simulation;computer science;machine learning;statistics	Vision	39.773263412353295	-46.06738291737281	41475
87edac0e4ee175de18caca5f63f40d416c252dac	improving person detection using synthetic training data	synthetic training data;detectors;haar features;supervised learning;training;complex real world scenes;video sequences;histograms of oriented gradients;synthetic training samples;training data;3d model;three dimensional displays;cameras three dimensional displays detectors hair solid modeling training data training;person detection;solid modeling;object detection haar transforms image sequences learning artificial intelligence;video sequences person detection synthetic training data complex real world scenes supervised learning haar features histogram of oriented gradients features;learning artificial intelligence;haar transforms;synthetic training samples person detection 3d model;cameras;object detection;histogram of oriented gradients features;image sequences;hair	Person detection in complex real-world scenes is a challenging problem. State-of-the-art methods typically use supervised learning relying on significant amounts of training data to achieve good detection results. However, labeling training data is tedious, expensive, and error-prone. This paper presents a novel method to improve detection performance by supplementing real-world data with synthetically generated training data. We consider the case of detecting people in crowded scenes within an AdaBoost-framework employing Haar and Histogram-of-Oriented-Gradients (HOG) features. Our evaluations on real-world video sequences of crowded scenes with significant occlusions show that the combination of real and synthetic training data significantly improves overall detection results.	adaboost;anomaly detection;cognitive dimensions of notations;haar wavelet;histogram of oriented gradients;image gradient;sensor;supervised learning;synthetic intelligence	Jie Yu;Dirk Farin;Christof Kruger;Bernt Schiele	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5650143	computer vision;training set;detector;speech recognition;haar-like features;computer science;machine learning;solid modeling;supervised learning	Vision	36.89090162340358	-46.90503772273643	41547
637de43801f26fab8f567787485c57ab92273ce5	mask-aware photorealistic face attribute manipulation		The task of face attribute manipulation has found increasing applications, but still remains challenging with the requirement of editing the attributes of a face image while preserving its unique details. In this paper, we choose to combine the Variational AutoEncoder (VAE) and Generative Adversarial Network (GAN) for photorealistic image generation. We propose an effective method to modify a modest amount of pixels in the feature maps of an encoder, changing the attribute strength continuously without hindering global information. Our training objectives of VAE and GAN are reinforced by the supervision of face recognition loss and cycle consistency loss for faithful preservation of face details. Moreover, we generate facial masks to enforce background consistency, which allows our training to focus on manipulating the foreground face rather than background. Experimental results demonstrate our method, called Mask-Adversarial AutoEncoder (M-AAE), can generate high-quality images with changing attributes and outperforms prior methods in detail preservation.	arm accredited engineer;autoencoder;case preservation;effective method;encoder;experiment;facial recognition system;glossary of computer graphics;map;pixel;variational principle	Ruoqi Sun;Chen Huang;Jianping Shi;Lizhuang Ma	2018	CoRR		pixel;artificial intelligence;pattern recognition;encoder;autoencoder;computer science;facial recognition system;effective method	Vision	25.94542084357152	-50.4115310730174	41721
80ea60ec20cf76ed2e57df58b31303c2a6278150	multi-scale descriptor for robust and fast camera motion estimation	estimated camera motion multiscale descriptor fast 6d camera motion estimation system robust camera motion estimation system msd time consuming step computational burden multiscale gradient information computational efηciency integral images intensity gradient;motion estimation me descriptor;motion estimation;motion estimation robustness cameras feature extraction vectors computer vision estimation;motion estimation cameras;cameras	This letter presents a robust and fast 6D camera motion estimation system that combines a FAST detector with a Multi-scale Descriptor (MSD). For a robust and fast camera motion estimation system, the descriptor is an especially important component, as it is the most time consuming step in motion estimation. Furthermore, the properties of the descriptor significantly affect the precision of the estimated motion. In this letter, we present a descriptor that requires a low computational burden and offers high precision for robust and fast motion estimation. The descriptor provides high precision through the use of multi-scale gradient information and computational efficiency by employing 4 integral images and an intensity gradient. We conclude this letter with an evaluation of the proposed motion estimation in comparison with different types of descriptors in practical situations. The results show that the proposed motion estimation provides good performance over previous methods in terms of processing time and precision of estimated camera motion.	computation;gradient descent;motion estimation;time complexity	Chang Hun Sung;Myung Jin Chung	2013	IEEE Signal Processing Letters	10.1109/LSP.2013.2264672	computer vision;simulation;quarter-pixel motion;computer science;motion estimation;control theory;motion field	Vision	47.86390433487131	-46.588322170546334	41780
ae9e3b3bc85868fb5c2fe41233b954691a4d6ab5	an information-theoretic-based evolutionary approach for the dynamic search path planning problem	search problems autonomous aerial vehicles genetic algorithms mobile robots path planning;dynamic search path planning genetic algorithms unmanned aerial vehicle information theory;myopic heuristics information theoretic based evolutionary approach dynamic search path planning problem open loop model anticipated feedback action outcomes observations objective function conditional observation probability independence expected system entropy lateness travel discovery time;entropy path planning sensors uncertainty vehicle dynamics linear programming search problems	A new information-theoretic-based evolutionary approach is proposed to solve the dynamic search path planning problem. Path planning is achieved using an open-loop model with anticipated feedback while dynamically capturing incoming new requests and real action outcomes/observations as exogenous events, to timely adjust search path plans using coevolution. The approach takes advantage of objective function separability and conditional observation probability independence to efficiently minimize expected system entropy, lateness and travel/discovery time respectively. Computational results clearly show the value of the approach in comparison to a myopic heuristics over various problem instances.	bayesian network;centralized computing;computation;heuristic (computer science);information theory;iterative and incremental development;linear separability;loss function;motion planning;optimization problem;path (variable);search problem;self-propelled particles;unmanned aerial vehicle	Mohamed Barkaoui;Jean Berger;Abdeslem Boukhtouta	2014	2014 International Conference on Advanced Logistics and Transport (ICALT)	10.1109/ICAdLT.2014.6864073	mathematical optimization;simulation;any-angle path planning;machine learning	Robotics	52.928943221658635	-25.177521055553136	41800
03e8130103751db21775495e14f040a09db3636e	beyond log-concavity: provable guarantees for sampling multi-modal distributions using simulated tempering langevin monte carlo		A key task in Bayesian machine learning is sampling from distributions that are only specified up to a partition function (i.e., constant of proportionality). One prevalent example of this is sampling posteriors in parametric distributions, such as latent-variable generative models. However sampling (even very approximately) can be #P-hard. Classical results (going back to [BÉ85]) on sampling focus on log-concave distributions, and show a natural Markov chain called Langevin diffusion mixes in polynomial time. However, all log-concave distributions are uni-modal, while in practice it is very common for the distribution of interest to have multiple modes. In this case, Langevin diffusion suffers from torpid mixing. We address this problem by combining Langevin diffusion with simulated tempering. The result is a Markov chain that mixes more rapidly by transitioning between different temperatures of the distribution. We analyze this Markov chain for a mixture of (strongly) log-concave distributions of the same shape. In particular, our technique applies to the canonical multi-modal distribution: a mixture of gaussians (of equal variance). Our algorithm efficiently samples from these distributions given only access to the gradient of the log-pdf. To the best of our knowledge, this is the first result that proves fast mixing for multimodal distributions in this setting. A summary of this paper and related materials are available at http://tiny.cc/ glr17. Note that the proof has been improved from the original version of the paper at http://www.arxiv.org/abs/1710.02736.	algorithm;bayesian network;best, worst and average case;cluster analysis;cobham's thesis;concave function;cryptocurrency tumbler;generative model;gradient;heuristic (computer science);importance sampling;ising model;latent variable;machine learning;markov chain;mixture model;modal logic;monte carlo method;multimodal interaction;p (complexity);parallel tempering;particle filter;partition function (mathematics);polynomial;portable document format;provable security;sampling (signal processing);sharp-p;simulated annealing;state space;time complexity;tiny c compiler;topic model	Rong Ge;Holden Lee;Andrej Risteski	2018			mixture model;dynamic monte carlo method;mathematical optimization;mathematics;monte carlo method in statistical physics;monte carlo method;parallel tempering;sampling (statistics);parametric statistics;markov chain	ML	27.145078263782878	-28.84296528052551	41831
135469bb76a36d0ecea794e09ea8d350b6ca09b4	minimum-entropy estimation in semi-parametric models	metodo cuadrado menor;evaluation performance;methode moindre carre;sample size;entropia;performance evaluation;methode parametrique;least squares method;maximum likelihood;estimation robuste;efficiency;metodo parametrico;estimacion m;evaluacion prestacion;semi parametric models;parametric method;maximum vraisemblance;outlier;robust estimation;entropy estimation;observacion aberrante;parametric estimation;consistent estimator;outliers;robustesse;entropie;least square;estimacion parametro;semi parametric model;estimation m;observation aberrante;robustness;entropy;parameter estimation;estimation parametre;reseau neuronal;estimacion adaptativa;m estimation;red neuronal;maxima verosimilitud;adaptive estimation;estimation adaptative;neural network;robustez	In regression problems where the density f of the errors is not known, maximum likelihood is unapplicable, and the use of alternative techniques like least squares or robust M -estimation generally implies inefficient estimation of the parameters. The search for adaptive estimators, that is, estimators that remain asymptotically efficient independently of the knowledge of f , has received a lot of attention, see in particular (Stein, 1956; Stone, 1975; Bickel, 1982) and the review paper (Manski, 1984). The paper considers a minimum-entropy parametric estimator that minimizes an estimate of the entropy of the distribution of the residuals. A first construction connects the method with the Stone-Bickel approach, where the estimation is decomposed into two steps. Then we consider a direct approach that does not involve any preliminary √ n-consistent estimator. Some results are given that illustrate the good performance of minimum-entropy estimation for reasonable sample sizes when compared to standard methods, in particular concerning robustness in the presence of outliers.	entropy estimation;least squares;semiconductor industry	Eric Wolsztynski;Eric Thierry;Luc Pronzato	2005	Signal Processing	10.1016/j.sigpro.2004.11.028	econometrics;entropy;mathematical optimization;outlier;mathematics;least squares;artificial neural network;statistics	ML	32.35578196553907	-24.66680324384969	41840
56360890208d8e53283229bc1eb981f881ceba22	consistent least squares fitting of ellipsoids	metodo cuadrado menor;methode moindre carre;analisis numerico;sample size;measurement error;least squares method;circles;ajustement;regression model;analyse numerique;fitting;hyperplanes;modelo regresion;numerical analysis;estimation erreur;consistent estimator;error estimation;regression estimator;modele regression;least square;estimacion error;ordinary least square;estimacion parametro;hyperspheres;non linearite;no linealidad;nonlinearity;parameter estimation;estimation parametre;ajuste;sista;variance;variancia	A parameter estimation problem for ellipsoid fitting in the presence of measurement errors is considered. The ordinary least squares estimator is inconsistent, and due to the nonlinearity of the model, the orthogonal regression estimator is inconsistent as well, i.e., these estimators do not converge to the true value of the parameters, as the sample size tends to infinity.A consistent estimator is proposed, based on a proper correction of the ordinary least squares estimator. The correction is explicitly given in terms of the true value of the noise variance.	algorithm;approximation;computation;computer simulation;converge;curve fitting;ellipsoid method;estimation theory;linear least squares (mathematics);national fund for scientific research;non-linear least squares;nonlinear system;ordinary least squares;pl/i;quadratic equation;total least squares	Ivan Markovsky;Alexander Kukush;Sabine Van Huffel	2004	Numerische Mathematik	10.1007/s00211-004-0526-9	generalized least squares;efficient estimator;minimum mean square error;total least squares;minimax estimator;simple linear regression;econometrics;mathematical optimization;james–stein estimator;minimum-variance unbiased estimator;estimator;stein's unbiased risk estimate;ordinary least squares;nonlinear system;trimmed estimator;newey–west estimator;mathematics;mean squared error;non-linear least squares;bias of an estimator;consistent estimator;least squares;linear least squares;statistics	ML	33.40739985452908	-24.19108375662844	41910
9d2ccd39c8f87bf7b0b40f10ad4185872269c816	global estimation in constrained environments	search and rescue;mobile sensors;sequential monte carlo method;optimal estimation;aerial robotics;search and rescue robots;estimation;variance reduction techniques;robot motion planning;motion planning;global optimization;estimation error;importance sampling	This article considers the optimal estimation of the state of a dynamic observable using a mobile sensor. The main goal is to compute a sensor trajectory which minimizes the estimation error over a given time horizon taking into account the uncertainty in the observable dynamics and its sensing and respecting the constraints of the workspace. The main contribution is a methodology for handling arbitrary dynamics, noise models and environment constraints in a global optimization framework. It is based on sequential Monte Carlo methods and sampling-based motion planning. Three variance reduction techniques–utility sampling, shuffling, and pruning–based on importance sampling, are proposed to speed-up convergence. The developed framework is applied to two typical scenarios: a simple vehicle operating in a planar polygonal obstacle environment; a simulated helicopter searching for a moving target in a 3-D terrain.	global optimization;importance sampling;mathematical optimization;monte carlo method;motion planning;observable;sampling (signal processing);variance reduction;workspace	Marin Kobilarov;Jerrold E. Marsden;Gaurav S. Sukhatme	2012	I. J. Robotics Res.	10.1177/0278364911423558	optimal estimation;computer vision;mathematical optimization;estimation;simulation;importance sampling;engineering;mathematics;motion planning;statistics;global optimization	Robotics	52.112684589792025	-25.74130527204025	41930
685b819bc077d862ad2969aa86a5da47dd7a4138	performance evaluation of 3d keypoint detectors for time-of-flight depth data	detectors;shape;three dimensional displays;feature extraction;robots;cameras	In object recognition techniques, specially feature-based methods, a fundamental step is to extract keypoints which are distinct and considerably interesting in the image. There are many different keypoint detectors already available, each with its own specific use and results vary enormously. It is widely agreed that evaluation of feature detectors is important. To our knowledge there is no comparative study for the performance of keypoint detectors for only depth data from Time-Of-Flight (ToF) camera. As ToF sensors are cheap and extensively used for robotic applications, especially sensors with low sensor noise like Swiss Ranger SR-4k which give only depth data, need arises for this comparative study. A meticulous acquisition of different household object's depth data has been achieved using a Cartesian robot. The pose information from the robot has been used for more accurate evaluation. Different keypoints valid for only depth data are extracted and their repeatability is evaluated. A Comparative study has also been done on standard RGB-D datasets using the new metrics we have defined, to test the correctness of our approach with state of the art approaches which have used RGB-D data.	cartesian coordinate robot;correctness (computer science);experiment;harris affine region detector;image noise;object detection;outline of object recognition;performance evaluation;repeatability;sensor;switzerland;time-of-flight camera;ranger	Vijaya K. Ghorpade;Paul Checchin;Laurent Malaterre;Laurent Trassoudaine	2016	2016 14th International Conference on Control, Automation, Robotics and Vision (ICARCV)	10.1109/ICARCV.2016.7838686	robot;computer vision;detector;simulation;feature extraction;shape;computer science;artificial intelligence;data mining	Robotics	52.37285367182778	-40.055957191900376	41965
dd49b5edbf48eaa46fcffd101c66da050b62da96	conditional random fields for contextual human motion recognition	conditional independence;maximum entropy markov models;image motion analysis;monocular video sequences;hidden markov model;image sequence analysis;conditional models;markov random fields;convex optimization;dynamic program;multiclass logistic regression;logistic regression;markov random field;conditional model;markov model;hidden markov models;humans character generation artificial intelligence chromium image recognition inference algorithms hidden markov models feature extraction optical filters gold;human motion;markov processes image motion analysis image sequences;conditional random field;graphical model;feature selection;optimization;optimization markov random fields discriminative models hidden markov models human motion recognition multiclass logistic regression feature selection conditional models;markov processes;human motion recognition;discriminative conditional random field;human activity;maximum entropy;discriminative model;discriminative models;image sequences;maximum entropy markov models human motion recognition monocular video sequences discriminative conditional random field	We present algorithms for recognizing human motion in monocular video sequences, based on discriminative conditional random field (CRF) and maximum entropy Markov models (MEMM). Existing approaches to this problem typically use generative (joint) structures like the hidden Markov model (HMM). Therefore they have to make simplifying, often unrealistic assumptions on the conditional independence of observations given the motion class labels and cannot accommodate overlapping features or long term contextual dependencies in the observation sequence. In contrast, conditional models like the CRFs seamlessly represent contextual dependencies, support efficient, exact inference using dynamic programming, and their parameters can be trained using convex optimization. We introduce conditional graphical models as complementary tools for human motion recognition and present an extensive set of experiments that show how these typically outperform HMMs in classifying not only diverse human activities like walking, jumping. running, picking or dancing, but also for discriminating among subtle motion styles like normal walk and wander walk	conditional random field	Cristian Sminchisescu;Atul Kanaujia;Zhiguo Li;Dimitris N. Metaxas	2005		10.1109/ICCV.2005.59	computer science;machine learning;pattern recognition;mathematics;hidden markov model;discriminative model;statistics	Vision	35.05769753732666	-47.44720438325991	41969
4610f6d45605f0edefe4671e5a2531b586056629	multi moving people detection from binocular sequences	moving object;temporal difference;cameras motion estimation humans video sequences data mining flowcharts acoustic signal detection signal processing algorithms radio access networks automation;moving object segmentation multimoving people detection binocular sequences motion estimation cross camera correspondence background subtraction temporal difference depth estimation human detection;outdoor sequences;image segmentation;distance measure;video signal processing;cameras motion estimation humans object detection video sequences data mining flowcharts radio access networks automation educational institutions;noisy background;motion estimation;video sequences;data mining;feature space;hierarchical strategy;stereo pair;multimoving people detection;cross camera correspondence;motion estimation object detection stereo image processing image sequences;human detection results multi moving people detection binocular sequences video sequences motion estimation method cross camera correspondence stereo pair background subtraction temporal difference depth estimation distance measure hierarchical strategy indoor sequences outdoor sequences multiple partially occluded persons noisy background;human detection results;multiple partially occluded persons;human detection;stereo image processing;video signal processing object detection motion estimation image segmentation image sequences;background subtraction;acoustic signal detection;moving object segmentation;humans;motion estimation method;depth estimation;signal processing algorithms;multi moving people detection;flowcharts;cameras;object detection;radio access networks;image sequences;indoor sequences;binocular sequences;automation	A novel approach for detection of multiple moving objects from binocular video sequences is reported. First an efficient motion estimation method is applied to sequences acquired from each camera. The motion estimation is then used to obtain cross camera correspondence between the stereo pair. Next, background subtraction is achieved by fusion of temporal difference and depth estimation. Finally moving foregrounds are further segmented into moving object according to a distance measure defined in a 2.5D feature space, which is done in a hierarchical strategy. The proposed approach has been tested on several indoor and outdoor sequences. Preliminary experiments have shown that the new approach can robustly detect multiple partially occluded moving persons in a noisy background. Representative human detection results are presented.	2.5d;background subtraction;binocular vision;experiment;feature vector;motion estimation;temporal difference learning	Yang Ran;Qinfen Zheng	2003		10.1109/ICASSP.2003.1199101	temporal difference learning;computer vision;feature vector;background subtraction;flowchart;computer science;automation;pattern recognition;motion estimation;image segmentation;computer graphics (images)	Vision	43.92789534485197	-47.92354640167713	41982
47c68c67f4d151504d43e26ff0a8521ccb9b9fd6	sudden global spatial-temporal change detection and its applications	cellular nonlinear network;cnn um;fast event detection in flows;analogic algorithm;video shot change detection;temporal change	We are watching the news on TV: the change of the background tells us when a new story begins. A glance at the clock and we can clearly see what time it is. These are special spatial-temporal episodes caused by ballistic eye movements and sudden optical changes. In this paper we give a useful definition for generalized sudden global change events, present its main properties and give an algorithm to recognize them in any video-flow. The proposed algorithm is implemented on a standard Cellular Nonlinear Network Universal Machine (CNN-UM). The processing time of the detection is roughly one millisecond on the ACE4k CNN-UM chip in the Aladdin environment. Therefore it can serve as a common function in any online, real-time spatial-temporal algorithm. The sudden global change event can be used to define the instant when the time-dependent part of that algorithm, e.g., homotopy, has to be initialized. The first steps of the multimedia processing, such as searching for key frames — for compression, browsing or indexing — can be helped by the proposed algorithm as a robust video shot change detection method.		Dávid Bálya	2003	Journal of Circuits, Systems, and Computers	10.1142/S0218126603001173	embedded system;computer vision;simulation;telecommunications;computer science;machine learning;algorithm;computer graphics (images)	EDA	39.20647555573875	-46.64438324996485	41986
f242b6dcdfe499253650efe6806143ae829db72e	a simple on-road object segmentation approach in its system	obstacle detection;image segmentation automated highways image resolution;image segmentation;image resolution;sensors;intelligent transport system;edge detection;object segmentation object detection pixel intelligent transportation systems image segmentation layout computer vision road transportation intelligent sensors image edge detection;intelligent transportation system;automated highways;segmentation;filling;data mining;object segmentation;interest pixels function onroad object segmentation approach its system intelligent transportation system object detection algorithm transportation scene images;segmentation intelligent transportation system object detection interest pixels;pixel;transportation;vehicles;interest pixels;object detection	In ITS(Intelligent Transportation System), on-road object detection algorithm is one of the most important research fields and obstacle segmentation is a key factor in obstacle detection approaches. In this paper, a simple and fast segmentation approach is proposed for on-road object in ITS. Firstly, a simple method is applied to detect the interest pixels in the transportation scene images by the defined interest pixels function and a single strategy is applied to reduce the redundant computation in the process of computation gray mean of pixels in squared window; Secondly, all the detected interest pixels neighbored each other are grouped to object interest region; after that, the non-interest pixels enclosed by interest region are changed as interest pixels; Lastly, the binarized images can be easily segmented applying simple edge detection algorithms or contour tracing approaches. Several images captured from ITS system is used to test the proposed algorithm, the result indicate that the approach is valid and feasible.	algorithm;computation;edge detection;grayscale;object detection;pixel;preprocessor	Yongquan Xia;Jun Zhi;Min Huang;Weili Li;Rui Ma	2009	2009 International Asia Conference on Informatics in Control, Automation and Robotics	10.1109/CAR.2009.21	computer vision;transport;intelligent transportation system;simulation;edge detection;image resolution;computer science;sensor;image segmentation;segmentation;pixel;computer graphics (images)	Robotics	43.31954636139925	-44.5925812596457	42015
75512a0c23866cc2f0bcf349b770f0fb2ce17e57	ordinal depth supervision for 3d human pose estimation		Our ability to train end-to-end systems for 3D human pose estimation from single images is currently constrained by the limited availability of 3D annotations for natural images. Most datasets are captured using Motion Capture (MoCap) systems in a studio setting and it is difficult to reach the variability of 2D human pose datasets, like MPII or LSP. To alleviate the need for accurate 3D ground truth, we propose to use a weaker supervision signal provided by the ordinal depths of human joints. This information can be acquired by human annotators for a wide range of images and poses. We showcase the effectiveness and flexibility of training Convolutional Networks (ConvNets) with these ordinal relations in different settings, always achieving competitive performance with ConvNets trained with accurate 3D joint coordinates. Additionally, to demonstrate the potential of the approach, we augment the popular LSP and MPII datasets with ordinal depth annotations. This extension allows us to present quantitative and qualitative evaluation in non-studio conditions. Simultaneously, these ordinal annotations can be easily incorporated in the training procedure of typical ConvNets for 3D human pose. Through this inclusion we achieve new state-of-the-art performance for the relevant benchmarks and validate the effectiveness of ordinal depth supervision for 3D human pose.		Georgios Pavlakos;Xiaowei Zhou;Kostas Daniilidis	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00763	iterative reconstruction;artificial intelligence;machine learning;ordinal number;motion capture;pattern recognition;computer science;benchmark (computing);pose;ground truth	Vision	27.55489859664958	-49.67502070645568	42029
c27d3a6ef5a1523d843afb2c3b537b83e3b72d04	nighttime foreground pedestrian detection based on three-dimensional voxel surface model	near-infrared stereo network camera;nighttime foreground pedestrian detection;voxel surface model	Pedestrian detection is among the most frequently-used preprocessing tasks in many surveillance application fields, from low-level people counting to high-level scene understanding. Even though many approaches perform well in the daytime with sufficient illumination, pedestrian detection at night is still a critical and challenging problem for video surveillance systems. To respond to this need, in this paper, we provide an affordable solution with a near-infrared stereo network camera, as well as a novel three-dimensional foreground pedestrian detection model. Specifically, instead of using an expensive thermal camera, we build a near-infrared stereo vision system with two calibrated network cameras and near-infrared lamps. The core of the system is a novel voxel surface model, which is able to estimate the dynamic changes of three-dimensional geometric information of the surveillance scene and to segment and locate foreground pedestrians in real time. A free update policy for unknown points is designed for model updating, and the extracted shadow of the pedestrian is adopted to remove foreground false alarms. To evaluate the performance of the proposed model, the system is deployed in several nighttime surveillance scenes. Experimental results demonstrate that our method is capable of nighttime pedestrian segmentation and detection in real time under heavy occlusion. In addition, the qualitative and quantitative comparison results show that our work outperforms classical background subtraction approaches and a recent RGB-D method, as well as achieving comparable performance with the state-of-the-art deep learning pedestrian detection method even with a much lower hardware cost.	algorithm;attempt;background subtraction;binocular disparity;binocular vision;chimeric antigen receptor;closed-circuit television;deep learning;entity name part qualifier - adopted;experiment;extraction;gaussian blur;graphics processing unit;hearing loss, high-frequency;hidden surface determination;high- and low-level;ip camera;image resolution;lamp, infrared;mobile operating system;networking cables;pedestrian detection;preprocessor;real life;router (computing);scientific publication;silo (dataset);stereopsis;voxel	Jing Li;Fangbing Zhang;Lisong Wei;Tao Yang;Zhaoyang Lu	2017		10.3390/s17102354	voxel;deep learning;rgb color model;engineering;computer vision;shadow;background subtraction;artificial intelligence;pedestrian detection	AI	50.70943936475371	-44.80211931897239	42150
331b60de90ca24259deac6a9e25f17c5aebd0e14	inadequacy of interval estimates corresponding to variational bayesian approximations		In this paper we investigate the properties of the covariance matrices associated with variational Bayesian approximations, based on data from mixture models, and compare them with the true covariance matrices, corresponding to Fisher information matrices. It is shown that the covariance matrices from the variational Bayes approximations are normally ‘too small’ compared with those for the maximum likelihood estimator, so that resulting interval estimates for the parameters will be unrealistically narrow, especially if the components of the mixture model are not well separated.	approximation;calculus of variations;fisher information;local convergence;mixture model;numerical analysis;refinement (computing);time complexity;variational principle	Bo Wang;D. M. Titterington	2005			mathematical optimization;econometrics;fisher information;mixture model;computer science;maximum likelihood;covariance;bayes' theorem;matrix (mathematics);bayesian probability	ML	30.06729896690449	-27.150643380538845	42205
e4b07d38b5f93aa6938a25118f4dfe19a1a99d4f	hand-gesture recognition system by using microwave doppler sensors	microwave devices;sensors;home appliances;microwave imaging;multiple command;object detection gesture recognition image motion analysis microwave detectors;home appliance control;doppler effect;microwave doppler sensors;real time operation hand gesture recognition system microwave doppler sensor moving object detection human being appliance control method microwave doppler sensor hand movement prototype system;sensors doppler effect home appliances microwave circuits microwave imaging gesture recognition microwave devices;microwave circuits;lighting free microwave doppler sensors gesture detection home appliance control multiple command;lighting free;gesture detection;gesture recognition	A microwave doppler sensor is a very popular device for detecting moving objects such as human beings. This paper proposes a new appliance-control method using microwave Doppler sensors. The user moves his/her hand back and forth in front of the target appliance for the proposed system. The system recognizes 1) two types of gesture, 2) the direction of hand movement, and 3) the number of hand movements. A prototype system with two light fixtures was developed. The gesture was detected within two seconds in real-time operation. A total of 200 on/off commands were experimentally evaluated. As a result, 85% of the gestures were correctly detected and 3% were misdetected with incorrect appliance selection. The remaining 15% generated no response.	dos;embedded system;experiment;gesture recognition;microwave;prototype;real-time transcription;sensor;software appliance	Shigeo Kaneda;Yusuke Kubota;Tomohito Kurokawa;Takeshi Furuhata	2015	2015 IEEE 39th Annual Computer Software and Applications Conference	10.1109/COMPSAC.2015.230	embedded system;computer vision;doppler effect;computer science;engineering;sensor;gesture recognition	Robotics	45.67747360922471	-41.608630830738456	42266
279d88049454bc3edf836f459e56838d4ae6d7de	a general framework for consistency of principal component analysis	random matrix;spike model;pca;high dimension low sample size	A general asymptotic framework is developed for studying consistency properties of principal component analysis (PCA). Our framework includes several previously studied domains of asymptotics as special cases and allows one to investigate interesting connections and transitions among the various domains. More importantly, it enables us to investigate asymptotic scenarios that have not been considered before, and gain new insights into the consistency, subspace consistency and strong inconsistency regions of PCA and the boundaries among them. We also establish the corresponding convergence rate within each region. Under general spike covariance models, the dimension (or number of variables) discourages the consistency of PCA, while the sample size and spike information (the relative size of the population eigenvalues) encourage PCA consistency. Our framework nicely illustrates the relationship among these three types of information in terms of dimension, sample size and spike size, and rigorously characterizes how their relationships affect PCA consistency.	consistency model;depth perception;principal component analysis;rate of convergence	Dan Shen;Haipeng Shen;J. S. Marron	2016	Journal of Machine Learning Research		econometrics;computer science;random matrix;pattern recognition;mathematics;statistics;principal component analysis	ML	27.031145019661754	-33.708672902875655	42314
45f0d5ee5b7238aabade80cb9098b09dcf0131ec	real-time object tracking with movels and affine transformations	autonomous software agents;object recognition;image motion analysis;mathematics;active contour;object shape;perspective transformations;real time object tracking;video signal processing;software agent;real time;video analysis;active contours;motion;frames sequence;computer vision;software agents;image motion analysis optical tracking software agents real time systems transforms video signal processing image sequences;shape;optical tracking;affine transformation;object tracking;motion real time object tracking movels affine transformations on line video analysis perspective transformations object shape autonomous software agents active contour frames sequence;transforms;shape object detection active contours lighting object recognition computer vision mathematics algorithm design and analysis software agents digital images;movels;affine transformations;lighting;on line video analysis;digital images;algorithm design and analysis;object detection;real time systems;image sequences	In this paper we present a new real-time algorithm for object tracking by on-line video analysis. This algorithm is able to track an object even in presence of substantial perspective transformations and to detect changes in the object shape due to changes in its structure. The core of the system is a chain made of autonomous software agents (MOVels), which acts like an active contour on the sequence of frames.		Giancarlo Iannizzotto;Lorenzo Vita	2000		10.1109/ICIP.2000.900958	computer vision;method;simulation;object model;computer science;software agent;video tracking;affine transformation;computer graphics (images)	Vision	48.53764671159559	-48.3378434008632	42377
310883abab96bd5a018482e97290b68a30245def	collaborative exploration for map construction	robot sensing systems;laser ranging mobile robots multi robot systems path planning;filtering;collaborative work;virtual tether;uncertainty;ground truth pose estimates;path planning;kalman filters;collaboration;map construction;mobile robots;laser ranging;visual landmarks;data uncertainty;map learning;collaborative exploration;machine learning;collaboration robot sensing systems computer science machine learning uncertainty sonar measurements computational efficiency kalman filters filtering collaborative work;cooperative localization;multi robot systems;position estimation;ground truth;visual landmarks collaborative exploration map construction map learning ground truth pose estimates uncertainty cooperative localization virtual tether position estimation;computer science;computational efficiency;sonar measurements;pose estimation	We consider the problem of map learning while maintaining ground-truth pose estimates. Map learning is important in tasks that requir ea model of the envir onmentor some of its featur es. As a robot collects data, uncertainty about its position accumulates and corrupts its knowledge of the positions from which observations are taken. We addr essthis problem by employing cooperative localization; that is, deploying a se cond r obot to observe the other as it explor es, thereby establishing a virtual tether, and enabling an accurate estimate of the robot's p osition while it c onstructs the map. This pap er presents our approach to this pr oblem in the context of learning a set of visual landmarks useful for pose estimation. In addition to developing a formalism and conc ept,we validate our results experimentally and present quantitative results demonstrating the performance of the method.	conditional (computer programming);experiment;ground truth;mahdiyar;robot;second level address translation;semantics (computer science)	Ioannis M. Rekleitis;Robert Sim;Gregory Dudek;Evangelos E. Milios	2001		10.1109/CIRA.2001.1013215	filter;kalman filter;mobile robot;robot learning;computer vision;simulation;pose;uncertainty;ground truth;computer science;artificial intelligence;machine learning;motion planning;collaboration	Robotics	52.79655596760178	-35.05059543953765	42410
2920e56c0804c16cdc14e49cba313b421fb88ba2	self-calibration of the intrinsic parameters of cameras for active vision systems	optical distortion;lens distortion;focusing;task performance;image motion analysis;trajectories of features self calibration lens distortion computer vision intrinsic parameters cameras active vision systems camera motions optical flow field;image processing;trajectories of features;optical computing;computer vision;camera motions;camera motion;machine vision;active vision systems;lenses;self calibration;optical flow;cameras machine vision optical distortion robot vision systems calibration lenses image motion analysis head focusing optical computing;head;optical flow field;robot vision systems;intrinsic parameters;calibration;image processing computer vision calibration cameras;cameras;active vision	A new technique for the calibration of the intrinsic parameters of cameras for active vision systems is presented. By making deliberate camera motions, the intrinsics of the cameras can be calibrated based either on the positional difference of optical flow field (PDOFF) or the trajectories of features (TOF) on the image plane. A way to detect the distortion of a camera lens is also presented. The calibration method is simple, fast, reliable, and very easy to combine with task performing processes. The performance of the technique is illustrated. The method can be used for general calibration of camera intrinsics. >	active vision;camera resectioning	Fenglei Du;Michael Brady	1993		10.1109/CVPR.1993.341087	distortion;smart camera;computer vision;camera auto-calibration;calibration;active vision;machine vision;image processing;computer science;optical flow;lens;optical computing;head;computer graphics (images)	Vision	52.07586100842463	-41.196237502796855	42488
f1df5f74477658df45814743f2e605656e8ac16e	see the forest for the trees: joint spatial and temporal recurrent neural networks for video-based person re-identification		Surveillance cameras have been widely used in different scenes. Accordingly, a demanding need is to recognize a person under different cameras, which is called person re-identification. This topic has gained increasing interests in computer vision recently. However, less attention has been paid to video-based approaches, compared with image-based ones. Two steps are usually involved in previous approaches, namely feature learning and metric learning. But most of the existing approaches only focus on either feature learning or metric learning. Meanwhile, many of them do not take full use of the temporal and spatial information. In this paper, we concentrate on video-based person re-identification and build an end-to-end deep neural network architecture to jointly learn features and metrics. The proposed method can automatically pick out the most discriminative frames in a given video by a temporal attention model. Moreover, it integrates the surrounding information at each location by a spatial recurrent model when measuring the similarity with another pedestrian video. That is, our method handles spatial and temporal information simultaneously in a unified manner. The carefully designed experiments on three public datasets show the effectiveness of each component of the proposed deep network, performing better in comparison with the state-of-the-art methods.	artificial neural network;color;computer vision;deep learning;end-to-end principle;experiment;feature learning;network architecture;recurrent neural network;supercomputer;the forest	Zhen Zhou;Yan Huang;Wei Wang;Liang Wang;Tieniu Tan	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.717	computer vision;discriminative model;spatial analysis;feature extraction;computer science;pattern recognition;architecture;artificial neural network;machine learning;recurrent neural network;feature learning;artificial intelligence	Vision	28.278329092153804	-51.66674124947428	42500
146ac20dc8bac67cc175aa3a78e471423e8616a7	feature extraction and target classification of side-scan sonar images	kernel;support vector machines;acoustics;training;security;autonomous systems;sonar	Side-scan sonar technology has been used over the last three decades for underwater surveying and imaging. Application areas of side-scan sonar include archaeology, security and defence, seabed classification, and environmental surveying. In recent years the use of autonomous underwater systems has allowed for automatic collection of data. Along with automatic collection of data comes the need to automatically detect what information is important. Automatic target recognition can allow for efficient task planning and autonomous system deployment for security and defence applications. Support Vector Machines (SVMs) are proven general purpose methods for pattern classification. They provide maximum margin classification that does not over fit to training data. It is generally accepted that the choice of kernel function allows for domain specific information to be leveraged in the classification system. In this paper it is shown that for target classification in side-scan sonar, extra feature extraction and data engineering can result in better classification performance compared to parameter optimization alone.	automatic target recognition;autonomous robot;autonomous system (internet);feature extraction;information engineering;mathematical optimization;sonar (symantec);software deployment;statistical classification;support vector machine;system deployment	Jason P. Rhinelander	2016	2016 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2016.7850074	computer vision;speech recognition;engineering;data mining	AI	41.73903130906306	-37.711231493386705	42538
3d4a0ceac2bcce4c7684434068f99b5cf1498717	a semi-supervised framework for mapping data to the intrinsic manifold	stiefel manifold;manifold learning;semi supervised learning;scattered data;iterative methods;independent component analysis manifolds laplace equations pattern recognition scattering parameters principal component analysis machine learning computer vision pattern analysis kernel;euclidean space;scattered data manifold semisupervised learning data mapping intrinsic parameter manifold manifold learning iterative neighborhood average method anchor points diffusion close formed manifold stiefel manifold;data handling learning artificial intelligence iterative methods;data handling;learning artificial intelligence;orthogonal group;averaging method	This paper presents a novel scheme for manifold learning. Different from the previous work reducing data to Euclidean space which cannot handle the looped manifold well, we map the scattered data to its intrinsic parameter manifold by semisupervised learning. Given a set of partially labeled points, the map to a specified parameter manifold is computed by an iterative neighborhood average method called anchor points diffusion procedure (APD). We explore this idea on the most frequently used close formed manifolds, Stiefel manifolds whose special cases include hyper sphere and orthogonal group. The experiments show that APD can recover the underlying intrinsic parameters of points on scattered data manifold successfully.	alexander horned sphere;algorithm;auditory processing disorder;camera resectioning;computer graphics;experiment;interpolation;intrinsic dimension;iterative method;nonlinear dimensionality reduction;semi-supervised learning;semiconductor industry;supervised learning	Haifeng Gong;Chunhong Pan;Hanqing Lu;Songde Ma	2005	Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1	10.1109/ICCV.2005.18	semi-supervised learning;local tangent space alignment;statistical manifold;mathematical optimization;stiefel manifold;invariant manifold;orthogonal group;computer science;euclidean space;atlas;machine learning;group method of data handling;hilbert manifold;pattern recognition;mathematics;iterative method;nonlinear dimensionality reduction;simplicial manifold;closed manifold;pseudo-riemannian manifold;manifold alignment	Vision	27.569736979406336	-40.424369063297945	42629
af1f326ebc520d451763337026e5dbd7881a6196	probability and common-sense: tandem towards robust robotic object recognition in ambient assisted living		The suitable operation of mobile robots when providing Ambient Assisted Living (AAL) services calls for robust object recognition capabilities. Probabilistic Graphical Models (PGMs) have become the de-facto choice in recognition systems aiming to efficiently exploit contextual relations among objects, also dealing with the uncertainty inherent to the robot workspace. However, these models can perform in an incoherent way when operating in a long-term fashion out of the laboratory, e.g. while recognizing objects in peculiar configurations or belonging to new types. In this work we propose a recognition system that resorts to PGMs and common-sense knowledge, represented in the form of an ontology, to detect those inconsistencies and learn from them. The utilization of the ontology carries additional advantages, e.g. the possibility to verbalize the robot’s knowledge. A primary demonstration of the system capabilities has been carried out with very promising results.	atm adaptation layer;coherence (physics);graphical model;mobile robot;outline of object recognition;workspace	José-Raúl Ruiz-Sarmiento;Cipriano Galindo;Javier González	2016		10.1007/978-3-319-48799-1_1	computer vision;simulation;speech recognition	AI	30.5707082952004	-48.443531939299035	42640
92e28c35aa02eab88564126f1f31971dcc8b2a50	kernel-based optimized feature vectors selection and discriminant analysis for face recognition	kernel based fisher discriminant analysis;optimisation;fisher linear discriminant analysis;kernel;fisherface;optimized subset;geometry;recognition accuracy;matrix algebra;data mining;feature space;data distribution;discriminant analysis;feature vector;fisher discriminant analysis;face recognition vectors performance analysis lighting linear discriminant analysis optimization methods kernel data mining feature extraction computational complexity;nonlinear discriminant features extraction;face recognition;statistical analysis;vectors;matrix algebra face recognition statistical analysis optimisation geometry computational complexity feature extraction;computational complexity;feature extraction;performance analysis;lighting;facial expression variation;facial expression;face image;linear discriminant analysis;computational complexity kernel based optimized feature vectors selection face recognition face image data distribution facial expression variation optimized subset nonlinear discriminant features extraction kernel based fisher discriminant analysis fisherface recognition accuracy;optimization methods;kernel based optimized feature vectors selection	In practice, face image data distribution is very complex because of pose, illumination and facial expression variation, so it is inadequate to describe it just by Fisherface or Fisher linear discriminant analysis (FLDA). In the paper a method is presented for face recognition using kernel-based optimized feature vectors selection and discriminant analysis. The kernel trick is used to select an optimized subset from the data and form a subspace into the feature space that can capture the structure of the entire data into the feature space according to geometric consideration. Then all the data are projected into this subspace and FLDA is performed in this subspace to extract nonlinear discriminant features of the data for face recognition. Another similar analysis method is kernel-based Fisher discriminant analysis (KFDA), which transforms all the data into the feature space and FLDA is performed in the feature space. The proposed method is compared with Fisherface and KFDA on two benchmarks, and experimental results demonstrate that it outperforms Fisherface and can give the same recognition accuracy as KFDA, but its computational complexity is reduced against KFDA.	facial recognition system;kernel (operating system);linear discriminant analysis	Qingshan Liu;Rui Huang;Hanqing Lu;Songde Ma	2002		10.1109/ICPR.2002.1048314	kernel fisher discriminant analysis;speech recognition;feature vector;computer science;machine learning;pattern recognition;mathematics;linear discriminant analysis	Vision	27.471781705211445	-42.70265728584661	42692
2784bcf4e93a54131753fc56f87b6c956419db80	joint dictionary learning for multispectral change detection	biological patents;biomedical journals;text mining;sensors;europe pubmed central;automatic threshold selection change detection joint dictionary learning multitemporal remote sensing;training;citation search;citation networks;learning systems;multitemporal remote sensing automatic threshold selection change detection joint dictionary learning;compressed sensing feature extraction feature selection geophysical image processing hyperspectral imaging image coding image reconstruction image representation remote sensing;research articles;abstracts;feature extraction;image reconstruction;dictionaries image reconstruction remote sensing learning systems training sensors feature extraction;remote sensing;open access;dictionaries;life sciences;clinical guidelines;joint dictionary learning multispectral change detection remote sensing technology radiometric value variations spectral signature spectral information sparse coding method image pixel reconstruction query image pair reconstruction error minimization automatic threshold selection strategy sparse representation problem enhanced thematic mapper china;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Change detection is one of the most important applications of remote sensing technology. It is a challenging task due to the obvious variations in the radiometric value of spectral signature and the limited capability of utilizing spectral information. In this paper, an improved sparse coding method for change detection is proposed. The intuition of the proposed method is that unchanged pixels in different images can be well reconstructed by the joint dictionary, which corresponds to knowledge of unchanged pixels, while changed pixels cannot. First, a query image pair is projected onto the joint dictionary to constitute the knowledge of unchanged pixels. Then reconstruction error is obtained to discriminate between the changed and unchanged pixels in the different images. To select the proper thresholds for determining changed regions, an automatic threshold selection strategy is presented by minimizing the reconstruction errors of the changed pixels. Adequate experiments on multispectral data have been tested, and the experimental results compared with the state-of-the-art methods prove the superiority of the proposed method. Contributions of the proposed method can be summarized as follows: 1) joint dictionary learning is proposed to explore the intrinsic information of different images for change detection. In this case, change detection can be transformed as a sparse representation problem. To the authors’ knowledge, few publications utilize joint learning dictionary in change detection; 2) an automatic threshold selection strategy is presented, which minimizes the reconstruction errors of the changed pixels without the prior assumption of the spectral signature. As a result, the threshold value provided by the proposed method can adapt to different data due to the characteristic of joint dictionary learning; and 3) the proposed method makes no prior assumption of the modeling and the handling of the spectral signature, which can be adapted to different data.	dictionary [publication type];eclipse;experiment;handling (psychology);intuition;metric;machine learning;multispectral image;neural coding;pixel;projections and predictions;question (inquiry);real life;sparse approximation;sparse matrix	Xiaoqiang Lu;Yuan Yuan;Xiangtao Zheng	2017	IEEE Transactions on Cybernetics	10.1109/TCYB.2016.2531179	iterative reconstruction;computer vision;text mining;feature extraction;k-svd;computer science;sensor;machine learning;pattern recognition;data mining	Vision	30.20995145187136	-44.4416214178472	42733
52f94745da4f7e10a4a72263874dc75b499c1518	novel sonar salient feature structure for extended kalman filter-based simultaneous localization and mapping of mobile robots	wheeled robots;mobile robot;sonars;measurement uncertainty;data association;feature maps;simultaneous localization and mapping;extended kalman filter;home navigation	Not all line or point features capable of being extracted by sonar sensors from a cluttered home environment are useful for simultaneous localization and mapping (SLAM) of a mobile robot. This is due to unfavorable conditions such as environmental ambiguity and sonar measurement uncertainty. We present a novel sonar feature structure suitable for a cluttered environment and the extended Kalman filter (EKF)-based SLAM scheme. The key concept is to extract circle feature clouds on salient convex objects by sonar data association called convex saliency circling. The centroid of each circle cloud, called a sonar salient feature, is used as a natural landmark for EKF-based SLAM. By investigating the environmental inherent feature locality, cylindrical objects are augmented conveniently at the weak SLAM-able area as a natural supplementary saliency to achieve consistent SLAM performance. Experimental results demonstrate the validity and robustness of the proposed sonar salient feature structure for EKF-based SLAM. © Koninklijke Brill NV, Leiden and The Robotics Society of Japan, 2012	augmented reality;correspondence problem;extended kalman filter;locality of reference;mobile robot;nl (complexity);nv network;robotics;sonar (symantec);sensor;simultaneous localization and mapping	Se-Jin Lee;Dong-Woo Cho;Jae-Bok Song	2012	Advanced Robotics	10.1163/156855312X633093	mobile robot;computer vision;simulation;computer science;artificial intelligence;machine learning;extended kalman filter;measurement uncertainty;simultaneous localization and mapping	Robotics	52.64866903025006	-39.18339794293943	42737
40cfabc5a8605203e2861749f97a219be4eb000a	frontal gait recognition from incomplete sequences using rgb-d camera	depth feature extraction frontal gait recognition incomplete sequences rgb d cameras hierarchical classification strategy red green blue depth cameras airport security check points metal detector gate surveillance zone anthropometric feature based classification motion feature extraction;surveillance airports feature extraction gait analysis image classification image colour analysis image motion analysis image sensors image sequences object recognition;feature extraction gait recognition cameras joints videos legged locomotion	Frontal gait recognition using partial cycle information has not received significant attention to date in spite of its many potential applications. In this paper, we propose a hierarchical classification strategy that combines front and back view features captured by RGB-D (Red Green Blue - Depth) cameras. Airport security check points are considered as a typical application scenario, where two depth cameras mounted on top of a metal detector gate positioned beyond a yellow line, respectively, record front and back views of a subject as he goes through the check-in process. Due to the short distance of the surveillance zone between the yellow line and point of exit, it is often not possible to capture a full gait cycle independently from the front view or back view. An initial stage of anthropometric feature-based classification followed by motion feature extraction from the front view is used to restrict the potential set of matched subjects. A final classification is then applied on this reduced set of subjects using depth features extracted from the back view. The method is computationally efficient with a much higher rate of accuracy compared with existing gait recognition approaches.	airport security;algorithmic efficiency;anthropometry;comparison and contrast of classification schemes in linguistics and metadata;experiment;feature extraction;gait analysis;kinect;software development kit	Pratik Chattopadhyay;Shamik Sural;Jayanta Mukherjee	2014	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2014.2352114	computer vision;simulation;feature extraction;pattern recognition;feature	Vision	39.29794777271897	-44.73347984895708	42770
b0b57f31dd858c45c476e3156212208744a923c1	non-asymptotic adaptive prediction in functional linear models	functional linear regression;62j05;62g08;model selection on random bases;minimax rate;penalized contrast estimator;functional principal component analysis;mean squared prediction error	Functional linear regression has recently attracted considerable interest. Many works focus on asymptotic inference. In this paper we consider in a non asymptotic framework a simple estimation procedure based on functional Principal Regression. It revolves in the minimization of a least square contrast coupled with a classical projection on the space spanned by the m first empirical eigenvectors of the covariance operator of the functional sample. The novelty of our approach is to select automatically the crucial dimension m by minimization of a penalized least square contrast. Our method is based on model selection tools. Yet, since this kind of methods consists usually in projecting onto known non-random spaces, we need to adapt it to empirical eigenbasis made of data-dependent – hence random – vectors. The resulting estimator is fully adaptive and is shown to verify an oracle inequality for the risk associated to the prediction error and to attain optimal minimax rates of convergence over a certain class of ellipsoids. Our strategy of model selection is finally compared numerically with cross-validation. AMS subject classification: Primary, 62J05; Secondary, 62G08.	asymptote;cross-validation (statistics);data dependency;linear model;minimax;model selection;numerical analysis;randomness;social inequality;vhdl-ams	Élodie Brunel;André Mas;Angelina Roche	2016	J. Multivariate Analysis	10.1016/j.jmva.2015.09.008	functional principal component analysis;econometrics;mathematical optimization;mean squared prediction error;mathematics;statistics	ML	29.258059078949064	-25.531277001176015	42779
1de78b4369610d76727c000db074a0823128cd33	multi-target visual tracking with aerial robots	target tracking trajectory cameras robot vision systems;greedy algorithm multitarget visual tracking aerial robots mobile target tracking robot trajectory constant factor approximation combinatorial optimization problem maximum group coverage problem;trajectory control aerospace robotics approximation theory combinatorial mathematics multi robot systems object detection optimisation robot vision	We study the problem of tracking mobile targets using a team of aerial robots. Each robot carries a camera to detect targets moving on the ground. The overall goal is to plan for the trajectories of the robots in order to track the most number of targets, and accurately estimate the target locations using the images. The two objectives can conflict since a robot may fly to a higher altitude and potentially cover a larger number of targets at the expense of accuracy. We start by showing that k ≥ 3 robots may not be able to track all n targets while maintaining a constant factor approximation of the optimal quality of tracking at all times. Next, we study the problem of choosing robot trajectories to maximize either the number of targets tracked or the quality of tracking. We formulate this problem as the weighted version of a combinatorial optimization problem known as the Maximum Group Coverage (MGC) problem. A greedy algorithm yields a 1/2 approximation for the weighted MGC problem. Finally, we evaluate the algorithm and the sensing model through simulations and preliminary experiments.	aerial photography;approximation;biocybernetics;combinatorial optimization;cybernetics;experiment;greedy algorithm;inter-process communication;mathematical optimization;optimization problem;overhead (computing);pipelines;robot;simulation;video tracking	Pratap Tokekar;Volkan Isler;Antonio Franchi	2014	2014 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2014.6942986	computer vision;mathematical optimization;simulation;engineering	Robotics	53.54525656975809	-26.51743480285825	42798
f03ac6efb844bbd128bb11533793c946e7555c10	remote sensing, gis and cellular automata for urban growth simulation		Cities are complex spatial systems and modeling their dynamics of growth using traditional modeling techniques is a challenging task. Cellular automata (CA) have been widely used for modeling urban growth because of their computational simplicity, their explicit representation of time and space and their ability to generate complex patterns from the interaction of simple components of the system using simple rules. Integrating GIS tools and remote sensing data with CA has the potential to provide realistic simulation of the future urban growth of cities. The proposed approach is applied to model the growth of the City of Montreal. Land use/land cover maps derived from Landsat data acquired in 1975 and 1990 were used to train a CA model which was then used to project the land use in 2005. A comparison of the projected and actual land uses for 2005 is presented and discussed.		Munira Al-Ageili;Malek Mouhoub;Joseph M. Piwowar	2017	Computer and Information Science	10.5539/cis.v10n4p38	land use;remote sensing;cellular automaton;land cover;computer science	Robotics	40.833855419455276	-29.377871120207843	42807
8df939b1ec3d963a2414f882a1c456cd917cc5cd	greedy learning of graphical models with small girth	graph theory;nodes;theorems;greedy algorithms;graphical models greedy algorithms correlation entropy silicon markov processes complexity theory;graphs;computational complexity markov graph learning discrete probability distributions naive greedy algorithm prediction performance improvement forward backward greedy algorithm deletion step recursive greedy algorithm forward steps two level process greedy iterations inner loop analytical process empirical process small girth graph nodes;iterative methods;statistical distributions;computational complexity;computation science;algorithms;markov processes;statistical distributions computational complexity graph theory greedy algorithms iterative methods markov processes	This paper develops two new greedy algorithms for learning the Markov graph of discrete probability distributions, from samples thereof. For finding the neighborhood of a node (i.e. variable), the simple, naive greedy algorithm iteratively adds the new node that gives the biggest improvement in prediction performance over the existing set. While fast to implement, this can yield incorrect graphs when there are many short cycles, as now the single node that gives the best prediction can be outside the neighborhood. Our new algorithms get around this in two different ways. The forward-backward greedy algorithm includes a deletion step, which goes back and prunes incorrect nodes that may have initially been added. The recursive greedy algorithm uses forward steps in a two-level process, running greedy iterations in an inner loop, but only including the final node. We show, both analytically and empirically, that these algorithms can learn graphs with small girth which other algorithms - both greedy, and those based on convex optimization - cannot.	convex optimization;girth (graph theory);graphical model;greedy algorithm;inner loop;iteration;markov chain;markov random field;mathematical optimization;recursion	Avik Ray;Sujay Sanghavi;Sanjay Shakkottai	2012	2012 50th Annual Allerton Conference on Communication, Control, and Computing (Allerton)	10.1109/Allerton.2012.6483471	greedy randomized adaptive search procedure;mathematical optimization;combinatorics;greedy algorithm;machine learning;mathematics;greedy coloring	ML	25.272589235096884	-28.861936515011582	42907
caa578487b2156021f16e629e73bd339e752497b	transmission rate control for molecular communication among biological nanomachines	drugs;mathematical expression;biological nanomachine;drug delivery;drug delivery systems;positive feedback;receivers throughput molecular communication drugs drug delivery optimization numerical models;molecular communication;transmission rate control;feedback based control molecular communication biological nanomachines transmission rate control flow control;positive feedback transmission rate control molecular communication biological nanomachine aqueous environment optimization problem mathematical expression optimal transmission rate autonomous bionanomachine negative feedback;receivers;optimization problem;biological nanomachines;nanobiotechnology biomedical materials drug delivery systems molecular biophysics molecular communication telecommunication;negative feedback;autonomous bionanomachine;molecular biophysics;feedback based control;optimization;numerical models;flow control;optimal transmission rate;aqueous environment;biomedical materials;throughput;nanobiotechnology;molecular communication telecommunication	In this paper, we discuss issues concerned with transmission rate control in molecular communication, an emerging communication paradigm for bio-nanomachines in an aqueous environment. In molecular communication, a group of bio-nanomachines acting as senders transmit molecules, the molecules propagate in the environment, and another group of bio-nanomachines acting as receivers chemically react to the molecules propagating in the environment. In the model of molecular communication considered in this paper, senders may transmit molecules at a high rate to accelerate the receiver reactions or to increase the throughput. However, if the senders transmit molecules faster than the receivers react, the excess molecules remain in the environment and eventually degrade or diffuse away, which results in loss of molecules or degradation in efficiency. Such a potential issue associated with throughput and efficiency is in this paper discussed as an optimization problem. A mathematical expression for an upper-bound on the throughput and efficiency is first derived to provide an insight into the impact of model parameters. The optimal transmission rates that maximize the throughput and efficiency are then numerically calculated and presented, and throughput and efficiency are shown to be in trade-off relationships in a wide range of transmission rates. Further, two classes of feedback-based transmission rate control schemes are designed for autonomous bio-nanomachines to dynamically control their transmission rates, respectively based on negative and positive feedback from the receivers. The numerical evaluation of the two transmission rate control schemes is then shown to provide useful guidelines for application developers to satisfy their design goals.	autonomous robot;british informatics olympiad;control theory;elegant degradation;mathematical optimization;nanorobotics;numerical analysis;offset binary;optimization problem;positive feedback;programming paradigm;throughput	Tadashi Nakano;Yutaka Okaie;Athanasios V. Vasilakos	2013	IEEE Journal on Selected Areas in Communications	10.1109/JSAC.2013.SUP2.12130016	optimization problem;throughput;positive feedback;telecommunications;computer science;expression;flow control;negative feedback;nanobiotechnology;molecular biophysics	Networks	41.17510623147163	-28.49205664712729	42925
32bfa6cfa99cfeb2a274e6a7091bf9edbee39b77	vehicle detection and tracking based on optical field		Vehicle detection and tracking are of great significance on computer vision and practical applications. The main task of it is to pick out vehicles from the images in a realtime video and tag them so as to achieve goals like traffic flow calculation and the driving direction estimate. In this essay, we chose Horn-Schunck method based on optical field to detect the vehicles. Without knowing any background information, this method can precisely process the video in a real time, picking out the statistics of the moving car and count them. The algorithm used in the essay can achieve the goal of vehicle detection and tracking, calculating and show vehicle flow precisely and avoid the interference of pedestrians and other irrelevant factors.	algorithm;computer vision;experiment;horn clause;horn–schunck method;interference (communication);optical flow;real-time computing;relevance	Zhenyu Guo;Ziqi Zhou;Xiaoting Sun	2017	2017 International Conference on Security, Pattern Analysis, and Cybernetics (SPAC)	10.1109/SPAC.2017.8304352	filter (signal processing);optical field;optical filter;traffic flow;computer vision;interference (wave propagation);artificial intelligence;computer science;optical flow	Robotics	43.00071116344643	-43.764137204005905	42989
d5fcb07691eb7bd25117c96e20bf5e9689e96672	necessary conditions to attain performance bounds on structure and motion estimates of rigid objects	motion analysis;minimum variance;cramer rao lower bounds;estimation problems;planar surface patches;motion estimation surface texture sea surface maximum likelihood estimation computer vision analysis of variance parameter estimation noise level motion analysis image analysis;analytic conditions;computational geometry;motion estimation;surface texture;maximum likelihood estimation;3d space;computer vision;motion estimates;sea surface;parameter estimation problems;maximum likelihood estimate;noise level;rigid objects;mean square error;analysis of variance;mean square error methods;crlb;structure estimates;performance bounds;image analysis;computational geometry motion estimation parameter estimation mean square error methods;parameter estimation;surface patch;polyhedral objects;structure and motion;surface patch performance bounds structure estimates motion estimates rigid objects analytic conditions maximum likelihood estimate minimum variance estimation problems computer vision parameter estimation 3d structure cramer rao lower bounds crlb mean square error parameter estimation problems noise level polyhedral objects planar surface patches 3d space;3d structure	Analytic conditions that are necessary for the maximum likelihood estimate to become asymptotically unbiased and attain minimum variance are derived for estimation problems in computer vision. In particular, problems of estimating the parameters that describe the 3D structure of rigid objects or their motion are investigated. It is common practice to compute Cramer-Rao lower bounds (CRLB) to approximate the mean-square error in parameter estimation problems, but the CRLB is not guaranteed to be a tight bound and typically underestimates the true mean-square error. The necessary conditions for the Cramer-Rao lower bound to be a good approximation of the mean-squareerror are derived. The tightness of the bound depends on the noise level, the number of pixels on the surface of the object, and the texture of the surface. We examine our analytical results experimentally using polyhedral objects that consist of planar surface patches with various textures that move in 3D space. We provide necessary conditions for the CRLB to be attained that depend on the size, texture, and noise level of the surface patch.		Margrit Betke;Eran Naftali;Nicholas C. Makris	2001		10.1109/CVPR.2001.990996	mathematical optimization;image analysis;computational geometry;mathematics;geometry;maximum likelihood;statistics	Vision	52.92754930291104	-51.429227295576595	43091
584317724a57f30b7e25904eae95cbb2a111874b	gibbs sampling in factorized continuous-time markov processes	continuous time;bayesian network;time scale;continuous time markov process;gibbs sampling;compact representation;exact sampling	A central task in many applications is reasoning about processes that change over continuous time. Continuous-Time Bayesian Networks is a general compact representation language for multi-component continuous-time processes. However, exact inference in such processes is exponential in the number of components, and thus infeasible for most models of interest. Here we develop a novel Gibbs sampling procedure for multi-component processes. This procedure iteratively samples a trajectory for one of the components given the remaining ones. We show how to performexactsampling that adapts to the natural time scale of the sampled process. Moreover, we show that this sampling procedure naturally exploits the structure of the network to reduce the computational cost of each step. This procedure is the first that can provide asymptotically unbiased approximation in such processes.	algorithmic efficiency;approximation;bayesian network;computation;gibbs sampling;markov property;time complexity	Tal El-Hay;Nir Friedman;Raz Kupferman	2008			econometrics;mathematical optimization;gibbs sampling;computer science;slice sampling;machine learning;bayesian network;mathematics;statistics	ML	26.705296992280537	-28.355680360575587	43116
0cb13ae80e181545e1fa79a0a55d07ed2b39ba65	element rearrangement for tensor-based subspace learning	tensile stress iterative algorithms convergence data compression principal component analysis linear discriminant analysis vectors asia contacts earth;convergence;tensile stress;data compression;subspace learning;iterative algorithms;earth;tensor based subspace learning;data compression element rearrangement tensor based subspace learning mode k flattened matrix iterative algorithm integer optimization problem convergence expectation maximization algorithm unsupervised subspace learning supervised subspace learning;contacts;unsupervised subspace learning;iterative algorithm;optimization problem;iterative methods;element rearrangement;vectors;principal component analysis;tensors expectation maximisation algorithm iterative methods learning artificial intelligence;expectation maximization algorithm;integer optimization problem;earth mover s distance;learning artificial intelligence;supervised subspace learning;classification accuracy;mode k flattened matrix;linear discriminant analysis;dimensional reduction;asia;tensors;expectation maximisation algorithm	The success of tensor-based subspace learning depends heavily on reducing correlations along the column vectors of the mode-k flattened matrix. In this work, we study the problem of rearranging elements within a tensor in order to maximize these correlations, so that information redundancy in tensor data can be more extensively removed by existing tensor-based dimensionality reduction algorithms. An efficient iterative algorithm is proposed to tackle this essentially integer optimization problem. In each step, the tensor structure is refined with a spatially-constrained Earth Mover's Distance procedure that incrementally rearranges tensors to become more similar to their low rank approximations, which have high correlation among features along certain tensor dimensions. Monotonic convergence of the algorithm is proven using an auxiliary function analogous to that used for proving convergence of the Expectation-Maximization algorithm. In addition, we present an extension of the algorithm for conducting supervised subspace learning with tensor data. Experiments in both unsupervised and supervised subspace learning demonstrate the effectiveness of our proposed algorithms in improving data compression performance and classification accuracy.	approximation;approximation algorithm;data compression;dimensionality reduction;expectation–maximization algorithm;iterative method;linear programming;mathematical optimization;motion estimation;non-monotonic logic;optical flow;optimization problem;redundancy (information theory);supervised learning;unsupervised learning	Shuicheng Yan;Dong Xu;Stephen Lin;Thomas S. Huang;Shih-Fu Chang	2007	2007 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2007.382984	mathematical optimization;computer science;machine learning;pattern recognition;mathematics;iterative method;linear discriminant analysis;tensor product network;multilinear subspace learning;statistics	Vision	27.007387228272876	-40.78979465323268	43128
cf2437040db5c35e3bbbf0d780c50d682936e6de	crossing road monitoring system based on adaptive decision for illegal situation	pedestrian safety;poison control;smart system;injury prevention;illegal crossing road;adaptive decision;intelligent traffic monitoring;safety literature;line detection;traffic safety;injury control;home safety;injury research;monitoring system;safety abstracts;human factors;automatic detection;occupational safety;safety;safety research;accident prevention;violence prevention;hough transform;long range;bicycle safety;traffic monitoring;close range;lane line detection;poisoning prevention;falls;ergonomics;suicide prevention	Automatically detecting and tracking crossing lane line vehicles in the illegal situation is an important part of E-police system. However, till now, there is few research works reported for it. In this paper, a novel crossing road monitoring system containing two cameras is built, which is composed of several steps. The long-range camera is used for lane lines extraction by improved multiple Hough transforms, and illegal vehicle is detected according to the distance between vehicle center and lane lines. Then, the illegal vehicle is tracked and license plate is captured by the close-range camera. The records used for police checking contain the video of illegal process and license plate. The proposed system has been used by the traffic administration bureau. The results on actual application show that this novel system can be used for E-police with high detection accuracy.	silk road	Tao Gao;Zhengguang Liu;Shiguo Lian;Shihong Yue;Jun Zhang	2011	Appl. Soft Comput.	10.1016/j.asoc.2010.05.027	hough transform;simulation;computer science;suicide prevention;human factors and ergonomics;injury prevention;smart system;computer security	Robotics	43.138656571137524	-42.86710666891423	43130
280502e30fd998480c8e0ffe0391392d482ecc53	remember and transfer what you have learned - recognizing composite activities based on activity spotting	image recognition;mirrors;image motion analysis;machine learning technique;activity spotting;training;joints;learning artificial intelligence image motion analysis image recognition;training data activity spotting activity recognition approach machine learning technique;training data;real world application;hidden markov models;machine learning;hidden markov models training data training mirrors data models knowledge transfer joints;transfer learning;knowledge transfer;activity recognition approach;learning artificial intelligence;data models;activity recognition	Activity recognition approaches have shown to enable good performance for a wide variety of applications. Most approaches rely on machine learning techniques requiring significant amounts of training data for each application. Consequently they have to be retrained for each new application limiting the real-world applicability of today's activity recognition methods. This paper explores the possibility to transfer learned knowledge from one application to others thereby significantly reducing the required training data for new applications. To achieve this transferability the paper proposes a new layered activity recognition approach that lends itself to transfer knowledge across applications. Besides allowing to transfer knowledge across applications this layered approach also shows improved recognition performance both of composite activities as well as of activity events.	activity recognition;conditional random field;defense in depth (computing);graphical model;heart rate variability;high-level programming language;machine learning;meronomy;spatial variability;top-down and bottom-up design	Ulf Blanke;Bernt Schiele	2010	International Symposium on Wearable Computers (ISWC) 2010	10.1109/ISWC.2010.5665869	data modeling;computer vision;training set;speech recognition;transfer of learning;computer science;machine learning;pattern recognition;activity recognition	Arch	26.647540582803977	-47.8525864791929	43148
79d598289f59caf4d2744cd7da9532d654070b4b	dupl-vr: deep unsupervised progressive learning for vehicle re-identification		Vehicle re-identification (Re-ID) is a search for the similar vehicles in a multi-camera network usually having non-overlapping field-of-views. Supervised approaches have been used mostly for re-ID problem but they have certain limitations when it comes to real life scenarios. To cope with these limitations unsupervised learning techniques can be used. Unsupervised techniques have been successfully applied in the field of person re-identification. Having this in mind, this paper presents an unsupervised approach to solve the vehicle re-ID problem by training a base network architecture with a self-paced progressive unsupervised learning architecture which has not been applied to solve the vehicle re-ID problem. The algorithm has been extensively analyzed over two large available benchmark datasets VeRi and VehicleID for vehicle re-ID with image-to-image and cross-camera search strategies and the approach achieved better performance in most of the standard evaluation metrics when compared with the existing state-of-the-art supervised approaches.		Raja Muhammad Saad Bashir;Muhammad Shahzad;Muhammad Moazam Fraz	2018		10.1007/978-3-030-03801-4_26	computer science;artificial intelligence;pattern recognition;network architecture;architecture;unsupervised learning	AI	29.646005489073843	-51.056222182319466	43186
6b525709375eea472f7b88da419416207a9dae04	design for an intelligent surveillance system based on system-on-a-programmable-chip platform	software;surveillance labeling artificial intelligence computer architecture hardware target tracking software;surveillance;system on a programmable chip foreground detection object tracking;sopc processor intelligent surveillance system system on a programmable chip platform digital surveillance system moving objects adaptive search method object boundary box detection error multiple background maintenance accelerator object labeling;computer architecture;artificial intelligence;target tracking;video surveillance object detection programmable circuits system on chip target tracking;labeling;hardware	The digital surveillance system becomes more and more popular in recent years. The systems attempt to raise amount of high resolution cameras, consequently those systems stupendously increase the computational load on central server. As in the intelligent object recognition processing flow, the technique on tracking multiple targets, such as tracking group of people through occlusion, is still challenging. In this paper, we present a platform-based System-on-a-Programmable-Chip (SOPC) surveillance system. We discuss the behavior of the moving objects with adaptive search method. With this system, we can track the moving people in successive frame by object boundary box and velocity without color cues or appearance model. The proposed system can still solve the problem on the detection error even the foreground is similar to the background. The overall architectures of the intelligent surveillance system consist of three parts: multiple background maintenance (MBM) accelerator, object labeling and an SOPC processor.	computer and network surveillance;embedded system;image resolution;interaction;mbm (file format);outline of object recognition;server (computing);system on a chip;velocity (software development)	Tsung-Han Tsai;Chih-Hao Chang	2015	2015 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2015.7169080	embedded system;computer vision;labeling theory;real-time computing;tracking system;computer science;video tracking	Robotics	44.52539944726857	-42.986011172675134	43205
b8f62d697c07b1894747c9b14975359abfa199a9	enhanced codebook model for real-time background subtraction		The CodeBook is one of the popular real-time background models for moving object detection in a video. However, for some of the complex scenes, it does not achieve satisfactory results due to the lack of an automatic parameters estimation mechanism. In this paper, we present an improved CodeBook model, which is robust in sudden illumination changes and quasi-periodic motions. The major contributions of the paper are a robust statistical parameter estimation method, a controlled adaptation procedure, a simple, but effective technique to suppress shadows and a novel block based approach to utilize the local spatial information. The proposed model was tested on numerous complex scenes and results shows a significant performance improvement over standard model.	background subtraction;codebook;real-time clock	Munir Shah;Jeremiah D. Deng;Brendon J. Woodford	2011		10.1007/978-3-642-24965-5_51	statistical parameter;computer science;pattern recognition;mixture model;object detection;spatial analysis;performance improvement;codebook;computer vision;artificial intelligence;background subtraction	Vision	43.294117273425016	-49.27123813528246	43222
1487e13720eed5a50426a23b584437b362a63682	expectation-maximization bernoulli-gaussian approximate message passing	gaussian noise;regularized least squares;compressed sensing;bayes methods;em framework expectation maximization bernoulli gaussian approximate message passing compressed sensing signal distribution marginal distribution bayesian variation zero mean gaussian noise signal reconstruction signal parameter noise parameter bg amp algorithm;expectation maximization;noise measurement bayesian methods noise message passing compressed sensing manganese sensors;gaussian approximation;signal reconstruction bayes methods compressed sensing expectation maximisation algorithm gaussian noise message passing;message passing;signal reconstruction;numerical experiment;expectation maximisation algorithm	The approximate message passing (AMP) algorithm originally proposed by Donoho, Maleki, and Montanari yields a computationally attractive solution to the usual ℓ1-regularized least-squares problem faced in compressed sensing, whose solution is known to be robust to the signal distribution. When the signal is drawn i.i.d from a marginal distribution that is not least-favorable, better performance can be attained using a Bayesian variation of AMP. The latter, however, assumes that the distribution is perfectly known. In this paper, we navigate the space between these two extremes by modeling the signal as i.i.d Bernoulli-Gaussian (BG) with unknown prior sparsity, mean, and variance, and the noise as zero-mean Gaussian with unknown variance, and we simultaneously reconstruct the signal while learning the prior signal and noise parameters. To accomplish this task, we embed the BG-AMP algorithm within an expectation-maximization (EM) framework. Numerical experiments confirm the excellent performance of our proposed EM-BG-AMP on a range of signal types.12	approximation algorithm;bernoulli polynomials;compressed sensing;expectation–maximization algorithm;experiment;job control (unix);least squares;marginal model;message passing;numerical method;operational amplifier;sparse matrix	Jeremy P. Vila;Philip Schniter	2011	2011 Conference Record of the Forty Fifth Asilomar Conference on Signals, Systems and Computers (ASILOMAR)	10.1109/ACSSC.2011.6190117	gaussian noise;mathematical optimization;signal transfer function;machine learning;mathematics;statistics	ML	30.76891314200854	-28.651052595203666	43248
466544ccb7e7846d020cbeee4e9d35dd380c8e3a	fast nearest neighbor search in se(3) for sampling-based motion planning		Nearest neighbor searching is a fundamental building block of most sampling-based motion planners. We present a novel method for fast exact nearest neighbor searching in SE(3)—the 6 dimensional space that represents rotations and translations in 3 dimensions. SE(3) is commonly used when planning the motions of rigid body robots. Our approach starts by projecting a 4-dimensional cube onto the 3-sphere that is created by the unit quaternion representation of rotations in the rotational group SO(3). We then use 4 kd-trees to efficiently partition the projected faces (and their negatives). We propose efficient methods to handle the recursion pruning checks that arise with this kd-tree splitting approach, discuss splitting strategies that support dynamic data sets, and extend this approach to SE(3) by incorporating translations. We integrate our approach into RRT and RRT* and demonstrate the fast performance and efficient scaling of our nearest neighbor search as the tree size increases.	approximation algorithm;dimensionality reduction;dynamic data;image scaling;motion planning;nearest neighbor search;nearest-neighbor interpolation;ompl;opml;quaternions and spatial rotation;rapidly-exploring random tree;recursion;robot;sampling (signal processing);search algorithm	Jeffrey Ichnowski;Ron Alterovitz	2014		10.1007/978-3-319-16595-0_12	nearest-neighbor chain algorithm;best bin first;machine learning;pattern recognition;nearest neighbor search	Robotics	51.6086809864862	-24.168240199047073	43256
359253add888f0e85cb6271b7776f3bbba867dbe	pomdp manipulation via trajectory optimization	robot sensing systems;trajectory optimization robot sensing systems uncertainty computational modeling;uncertainty;willow garage pr2 platform pomdp manipulation trajectory optimization object manipulation force feedback environmental model partially observable markov decision process high dimensional continuous state discontinuous contacts high dimensional continuous space trajectory cost model based method motion generation optimal trajectory pomdp solver pomdp formulation sample based approach sampled model temporal abstraction macro action temporal action kuka arm;computational modeling;trajectory optimization;trajectory control force feedback manipulators markov processes motion control optimisation	Efficient object manipulation based only on force feedback typically requires a plan of actively contact-seeking actions to reduce uncertainty over the true environmental model. In principle, that problem could be formulated as a full partially observable Markov decision process (POMDP) whose observations are sensed forces indicating the presence/absence of contacts with objects. Such a naive application leads to a very large POMDP with high-dimensional continuous state, action and observation spaces. Solving such large POMDPs is practically prohibitive. In other words, we are facing three challenging problems: 1) uncertainty over discontinuous contacts with objects; 2) high-dimensional continuous spaces; 3) optimization for not only trajectory cost but also execution time. As trajectory optimization is a powerful model-based method for motion generation, it can handle the last two issues effectively by computing locally optimal trajectories. This paper aims to integrate advantages of trajectory optimization into existing POMDP solvers. The full POMDP formulation is solved using sample-based approaches, where each sampled model is quickly evaluated via trajectory optimization instead of simulating a large number of rollouts. To further accelerate the solver, we propose to integrate temporal abstraction, i.e. macro actions or temporal actions, into the POMDP model. We demonstrate the proposed method on a simulated 7 DoF KUKA arm and a physical Willow Garage PR2 platform. The results show that our proposed method could effectively seek contacts in complex scenarios, and achieve near-optimal performance of path planing.	haptic technology;local optimum;markov chain;mathematical optimization;partially observable markov decision process;partially observable system;planning;robot;run time (program lifecycle phase);simulation;solver;sparse matrix;synergy;trajectory optimization;willow	Ngo Anh Vien;Marc Toussaint	2015	2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2015.7353381	control engineering;trajectory optimization;mathematical optimization;simulation;uncertainty;computer science;computational model;statistics	Robotics	51.27444711032189	-25.547659070071536	43276
2ab9edf7022f8475d5f55d553eb7d1cfe20e667d	spatial prediction of hydrogen sulfide in sewers with a modified gaussian process combined mutual information	hydrogen sulfide spatial concentration hydrogen sulfide spatial prediction modified gaussian process mutual information based strategy data driven machine learning model gravity sewer system covariance function sensor measurements np hard combinatorial sensor selection problem;computational modeling;vectors;covariance matrices;mutual information;predictive models;entropy;data models predictive models covariance matrices computational modeling vectors entropy mutual information;wastewater treatment combinatorial mathematics computational complexity data handling environmental science computing gaussian processes learning artificial intelligence sewage treatment;conference proceeding;data models	This paper proposes a data driven machine learning model for spatial prediction of hydrogen sulfide (H2S) in a gravity sewer system. The gaseous H2S in the overhead of the gravity sewer is modelled using a Gaussian Process with a new covariance function due to constraints of sewer boundaries. The covariance function is proposed based on the distance between two locations computed along the lengths of the sewer network. A mutual information based strategy is used to choose the best k sensor measurements and their locations from among n potential sensor observations and their locations. This provably NP-hard combinatorial sensor selection problem is addressed by maximizing the mutual information between the selected locations and the locations that are not selected or do not have any sensor deployments. A proof-of-concept study was carried out comparing the spatial prediction of H2S with a complex model currently used by Sydney Water. The proposed approach is shown to be effective in both modelling and predicting the H2S spatial concentrations in sewers as well as identifying optimal number of H2S sensors and their locations for a required level of prediction accuracy.	gaussian process;hydrogen;machine learning;mutual information;np-hardness;named pipe;overhead (computing);selection algorithm;sensor	Linh V. Nguyen;Sarath Kodagoda;Ravindra Ranasinghe;Gamini Dissanayake;Heriberto Bustamante;Dammika Vitanage;Tung Nguyen	2014	2014 13th International Conference on Control Automation Robotics & Vision (ICARCV)	10.1109/ICARCV.2014.7064464	data modeling;entropy;computer science;engineering;artificial intelligence;machine learning;data mining;predictive modelling;mutual information;computational model;statistics	AI	45.975282374426364	-27.136038156202254	43281
7dbc24dfc2dd4f9640958790f08b377be121e5a9	a latent variable model approach to pmi-based word embeddings		Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods. This paper proposes a new generative model, a dynamic version of the log-linear topic model of Mnih and Hinton (2007). The methodological novelty is to use the prior to compute closed form expressions for word statistics. This provides a theoretical justification for nonlinear models like PMI, word2vec, and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic embeddings contain linear algebraic structure that allows solution of word analogies, as shown by Mikolov et al. (2013a) and many subsequent papers. Experimental support is provided for the generative model assumptions, the most important of which is that latent word vectors are fairly uniformly dispersed in space.	generative model;latent variable model;log-linear model;microsoft word for mac;nonlinear system;topic model;word embedding;word2vec	Sanjeev Arora;Yuanzhi Li;Yingyu Liang;Tengyu Ma;Andrej Risteski	2016	Transactions of the Association for Computational Linguistics	10.1162/tacl_a_00106	natural language processing;machine learning;pattern recognition;mathematics	NLP	27.597054799564624	-30.89983992059496	43299
2232416778783616736149c870a69beb13cda743	face recognition in a meeting room	data collection;face recognition;face recognition interactive systems lighting speech pattern recognition laboratories humans head electronic switching systems read only memory;constraint handling face recognition lighting feature extraction;local features;feature extraction;constraint handling;lighting;facial expression;spatial constraints face recognition meeting room human face identification input image quality illumination head poses changing facial expressions occlusion dynamic space warping local features	In this paper, weinvestigaterecognition of humanfaces in a meetingroom. The major challenges of identifying humanfacesin this environmentincludelow quality of input images,poor illumination,unrestrictedheadposesand continuouslychangingfacial expressionsandocclusion.In order to addresstheseproblemswe proposea novel algorithm, DynamicSpaceWarping (DSW).Thebasic idea of the algorithm is to combinelocal features under certain spatial constraints. We compare DSWwith the eigenface approachondatacollectedfromvariousmeetings.Wehave testedboth front and profile face imagesand imageswith two stagesof occlusion.Theexperimentalresultsindicate thattheDSWapproachoutperformstheeigenfaceapproach in bothcases.	algorithm;eigenface;facial recognition system	Ralph Gross;Jie Yang;Alexander H. Waibel	2000		10.1109/AFGR.2000.840649	facial recognition system;computer vision;face detection;speech recognition;feature extraction;computer science;three-dimensional face recognition;lighting;facial expression;face hallucination;statistics;data collection	Vision	39.89143142233343	-49.78063077638717	43310
041a09a9db9a318596b72d041a832b05100b0ddf	place recognition with convnet landmarks: viewpoint-robust, condition-robust, training-free		Place recognition has long been an incompletely solved problem in that all approaches involve significant compromises. Current methods address many but never all of the critical challenges of place recognition – viewpoint-invariance, condition-invariance and minimizing training requirements. Here we present an approach that adapts state-of-the-art object proposal techniques to identify potential landmarks within an image for place recognition. We use the astonishing power of convolutional neural network features to identify matching landmark proposals between images to perform place recognition over extreme appearance and viewpoint variations. Our system does not require any form of training, all components are generic enough to be used off-the-shelf. We present a range of challenging experiments in varied viewpoint and environmental conditions. We demonstrate superior performance to current state-of-theart techniques. Furthermore, by building on existing and widely used recognition frameworks, this approach provides a highly compatible place recognition system with the potential for easy integration of other techniques such as object detection and semantic scene interpretation.	artificial neural network;convolutional neural network;experiment;object detection;requirement;viewpoint	Niko Sünderhauf;Sareh Shirazi;Adam Jacobson;Feras Dayoub;Edward Pepperell;Ben Upcroft;Michael Milford	2015		10.15607/RSS.2015.XI.022	computer vision;engineering;artificial intelligence;machine learning;3d single-object recognition	Vision	30.807695152533668	-49.498477615532266	43319
fc0ffcfec9d52913c0b51e323fba1b5a5483d8bc	parallel feature extraction and heterogeneous object-detection for multi-camera driver assistance systems	object detection driver information systems feature extraction field programmable gate arrays image classification image sensors;program processors feature extraction vehicles computer architecture object detection cameras clocks;opencv parallel feature extraction heterogeneous object detection multicamera driver assistance systems flexible architecture image based feature detection object classification fpga driver assistance systems detection speed classification speed object detection system cascaded classifiers viola jones detector	We present a flexible architecture for image-based feature detection and object classification on an FPGA. This architecture is tailored to the requirements of future driver assistance systems, which will make it necessary to detect a wide range of different object types in multi-camera systems requiring highly efficient hardware. In contrast to other designs, which typically address a specific object type or only accelerate early processing steps, the proposed pipeline offers different operation modes to switch resources for either detection or classification speed. In addition, the architecture can incorporate heterogeneous processors for different feature types. The design is tailored to support any object detection system using weak features and cascaded classifiers. For evaluation, a classic Viola Jones Detector is implemented being fully compatible with OpenCV.	awareness;central processing unit;distortion;feature detection (computer vision);feature detection (web development);feature extraction;field-programmable gate array;haar wavelet;machine learning;microsoft windows;multiple encryption;object detection;object type (object-oriented programming);opencv;requirement;throughput;whole earth 'lectronic link	Stefan Wonneberger;Peter Mühlfellner;Pedro Ceriotti;Thorsten Graf;Rolf Ernst	2015	2015 25th International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2015.7293975	embedded system;computer vision;feature detection;real-time computing;object-class detection;computer science;viola–jones object detection framework;feature	EDA	43.404251204166485	-36.56560902662524	43367
e7fa6c1e1f0276398c27c5c6334900a6b9491b20	estimation of partially linear regression models under the partial consistency property		In this paper, utilizing recent theoretical results in high dimensional statistical modeling, we propose a model-free yet computationally simple approach to estimate the partially linear model $Y=X\beta+g(Z)+\varepsilon$. Motivated by the partial consistency phenomena, we propose to model $g(Z)$ via incidental parameters. Based on partitioning the support of $Z$, a simple local average is used to estimate the response surface. The proposed method seeks to strike a balance between computation burden and efficiency of the estimators while minimizing model bias. Computationally this approach only involves least squares. We show that given the inconsistent estimator of $g(Z)$, a root $n$ consistent estimator of parametric component $\beta$ of the partially linear model can be obtained with little cost in efficiency. Moreover, conditional on the $\beta$ estimates, an optimal estimator of $g(Z)$ can then be obtained using classic nonparametric methods. The statistical inference problem regarding $\beta$ and a two-population nonparametric testing problem regarding $g(Z)$ are considered. Our results show that the behavior of test statistics are satisfactory. To assess the performance of our method in comparison with other methods, three simulation studies are conducted and a real dataset about risk factors of birth weights is analyzed.		Xia Cui;Ying Lu;Heng Peng	2017	Computational Statistics & Data Analysis	10.1016/j.csda.2017.05.004	econometrics;mathematical optimization;mathematics;statistics	ML	29.079116846318325	-24.48052456118232	43376
d1a295949364536aebf96953ad87ea7a41b3fd8f	live demonstration: the “davis” dynamic and active-pixel vision sensor	davis machine vision frame based computer vision neuromorphic event driven vision photodiode cmos image sensor international imager sensor workshop vlsi dynamic and active pixel vision sensor;photodiodes cmos image sensors computer vision;robot sensing systems voltage control machine vision cameras visualization software algorithms very large scale integration	This demonstration will show the features of the Dynamic and Active-Pixel Vision Sensor (DAVIS) reported at the VLSI Symposium and the International Imager Sensor Workshop in 2013. This sensor concurrently outputs conventional CMOS image sensor frames and sparse, low-latency dynamic vision sensor events from the same pixels, sharing the same photodiodes. The setup will allow visitors to explore the advantages of combining of fast and computationally-efficient neuromorphic event-driven vision with the existing body of methods for frame-based computer and machine vision.	cmos;event-driven programming;image sensor;machine vision;neuromorphic engineering;sparse matrix;very-large-scale integration	Christian Brandli;Raphael Berner;Minhao Yang;Shih-Chii Liu;V. Villeneuva;Tobi Delbrück	2014	2014 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2014.6865163	embedded system;stereo cameras;computer vision;electronic engineering;machine vision;computer science;image sensor	Arch	45.51781888098831	-34.18750904017673	43388
cece2663db622ac35d3aca7e9b3bed705def117e	feature-based tracking on a multi-omnidirectional camera dataset	databases;video surveillance;tracking system;legged locomotion;surveillance;ambient intelligence;interest points;image sequence analysis;bomni dataset;feature based tracking;video surveillance object tracking video cameras;ambient assisted living;interest point matching;omnidirectional camera;aal dataset;video cameras;image color analysis;multiomnidirectional camera dataset;foreground background segmentation;omnidirectional cameras;object tracking;foreground background segmentation feature based tracking multiomnidirectional camera dataset surveillance ambient intelligence applications bomni dataset bounding box interest point matching;cameras databases target tracking educational institutions image color analysis lighting legged locomotion;lighting;target tracking;ambient intelligence applications;aal dataset video surveillance object tracking omnidirectional cameras image sequence analysis;cameras;foreground background;bounding box	Omnidirectional cameras have a lot of potential for surveillance and ambient intelligence applications, since they provide increased coverage with fewer cameras. We introduce the new BOMNI dataset, collected with two omnidirectional cameras simultaneously. The dataset contains single subject and multi-subject interaction scenarios, as well as actions relevant for ambient assisted living, such as falling down. We describe evaluation protocols on this dataset, and provide benchmarking baseline results for two tracking systems based on bounding box and interest point matching after foreground-background segmentation, respectively.	ambient intelligence;baseline (configuration management);foreground-background;minimum bounding box;omnidirectional camera;tracking system	Bans Evrim Demiröz;Ismail Ari;Orhan Eroglu;Albert Ali Salah;Lale Akarun	2012	2012 5th International Symposium on Communications, Control and Signal Processing	10.1109/ISCCSP.2012.6217867	computer vision;simulation;geography;computer graphics (images)	Vision	40.956112793126586	-47.490533412406585	43416
4f5f16386595cb01ce5f41331cbe7d53e9cc8a34	a novel image fusion scheme for robust multiple face recognition with light-field camera	detectors;video surveillance;light field camera;surveillance;wide area surveillance;pan tilt zoom;video surveillance face recognition image fusion image sensors;image fusion;image sensors;closed circuit television;face recognition;image fusion scheme lytro light field camera pan tilt zoom closed circuit television nonuniform illumination multiple pose wide area surveillance light field camera robust multiple face recognition;lytro light field camera;image fusion scheme;face;entropy;multiple pose;robust multiple face recognition;cameras;nonuniform illumination;face entropy cameras image fusion face recognition surveillance detectors	Accurate face recognition in wide area surveillance application is a challenging problem because of multiple pose, non-uniform illumination, low resolution and out-of-focus face images that are recorded with conventional surveillance cameras (Closed-Circuit TeleVision or Pan-Tilt-Zoom). In this paper, we address the problem of face recognition in wide area surveillance with a light-field camera. The main advantage of a light-field camera is that, it can provide different focus (or depth) images in a single exposure (capture) which is not possible with a conventional 2D camera. In this work, we propose a novel weighted image fusion scheme to combine different depth (or focus) images rendered by a light-field camera. The proposed image fusion scheme is not only dynamic in handling number of depth (or focus) images but also adaptive in assigning higher weights to the best focus image as compared to the out-of-focus image. Extensive experiments are carried out on our newly acquired face dataset captured using Lytro light-field camera to bring out the merits and demerits of the proposed weighted image fusion scheme for face recognition in wide area surveillance applications.	closed-circuit television;experiment;facial recognition system;image fusion;image resolution;light field	Ramachandra Raghavendra;Kiran B. Raja;Bian Yang;Christoph Busch	2013	Proceedings of the 16th International Conference on Information Fusion		smart camera;computer vision;camera auto-calibration;simulation;geography;three-dimensional face recognition;computer graphics (images)	Vision	43.18791916859741	-50.36586883992898	43430
e3911ed72d78fd7b9bdec2c80644f4d73a219f6f	local perception maps for autonomous robot navigation	robot sensing systems;sensor phenomena and characterization;ultrasonic data;neural networks;neural nets;mobile robot;ultrasonic measurement;autonomous robot navigation;ultrasonic imaging;specular reflections;local perception maps;local navigation local perception maps autonomous robot navigation mobile robotics system navigation free space representations ultrasonic data neural network ranging error minimisation specular reflections sensory data redundancy;mobile robots;orbital robotics;autonomic system;ranging error minimisation;navigation orbital robotics mobile robots neural networks robot sensing systems ultrasonic imaging sensor phenomena and characterization sonar buildings reflection;navigation;specular reflection;redundancy;local navigation;mobile robotics system;free space representations;a priori information;measurement errors mobile robots computerised navigation navigation ultrasonic measurement neural nets redundancy;sensory data redundancy;reflection;autonomous robot;buildings;measurement errors;neural network;computerised navigation;sonar	This paper describes a mobile robotics system that was implemented and is capable of building instantaneous representations of the free space available, and use those representations to perform navigation. The free space representations are done by means of perception maps specially designed to hold and combine ultrasonic data. The maps are built by neural network appropriately trained to minimise undesired ranging errors due to specular reflections that is achieved by taking advantage of the redundancy of sensorial data. The concept of local navigation is developed as a new navigation approach, and is based on full independence on the environment, relying purely on sensorial perception. Local motion is generated according to simple generic behaviour descriptions: the local navigation strategies. The system which was implemented guarantees a safe local motion throughout the environment, requiring no a priori information nor any pre-defined path to follow. An adequate navigation architecture integrates the local navigation module within a framework of other several modules for more complete navigation tasks. The results obtained are quite promising in pointing the way to a truly autonomous system.	autonomous robot;map;robotic mapping	Vítor Santos;João G. M. Gonçalves;F. Vas	1996		10.1109/IROS.1996.571058	control engineering;turn-by-turn navigation;mobile robot;computer vision;specular reflection;simulation;computer science;engineering;mobile robot navigation;artificial neural network	Robotics	52.46380988812836	-33.135450755874096	43533
badb95dbdfb3f044a46d7ba0ee69dba929c511b1	yet another gaze detector: an embodied calibration free system for the icub robot	robot sensing systems;robot vision gaze tracking humanoid robots human robot interaction image resolution;estimation;feature extraction;face;gaze detector icub robot robot embodied vision vga image resolution humanoid robot gaze tracking method gaze recognition embodied calibration;face feature extraction estimation robot sensing systems calibration;calibration	The recognition of gaze as for example mutual gaze plays an important role in social interaction. Previous research shows that already infants are capable of detecting mutual gaze. Such abilities are relevant for robots to learn from interaction, for example detecting when the robot is being addressed. Although various gaze tracking methods have been proposed, few seem to be openly available for robotic platforms such as iCub. In this paper we will describe a gaze tracking system for humanoid robots that is completely based on freely available libraries and data sets. Our system is able to estimate horizontal and vertical gaze directions using low resolution VGA images from robot embodied vision at 30 frames per second. For this purpose we developed a pupil detection algorithm combining existing approaches to increase noise robustness. Our method combines positions of the face and eye features as well as context features such as eyelid correlates and thus does not rely on fixed head orientations. An evaluation on the iCub robot shows that our method is able to estimate mutual gaze with 96% accuracy at 8° tolerance and one meter distance to the robot. The results further support that mutual gaze detection yields higher accuracy in an embodied setup compared to other configurations.	algorithm;centrality;eye tracking;graphics display resolution;humanoid robot;human–robot interaction;icub;image resolution;library (computing);multi-user;pixel;real-time computing;sensor;support vector machine;tracking system;video graphics array;yet another	Lars Schillingmann;Yukie Nagai	2015	2015 IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids)	10.1109/HUMANOIDS.2015.7363515	face;computer vision;estimation;calibration;simulation;feature extraction;computer science;social robot	Robotics	53.03770972747316	-41.78571759393224	43587
9795c4cba1ebe2300f55ebeba33d5ba2a7fabb0e	statistical analysis of complex and spatially dependent data: a review of object oriented spatial statistics	spatial regression models with differential regularization;object oriented data analysis;bagging voronoi algorithm;kriging for object data	We review recent advances in Object Oriented Spatial Statistics, a system of ideas, algorithms and methods that allows the analysis of high dimensional and complex data when their spatial dependence is an important issue. At the intersection of different disciplines – including mathematics, statistics, computer science and engineering – Object Oriented Spatial Statistics provides the right perspective to address key problems in varied contexts, from Earth and life sciences to urban planning. We illustrate a few paradigmatic methods applied to problems of prediction, classification and smoothing, giving emphasis to the key ideas Object Oriented Spatial Statistics relies upon.	algorithm;computer science;smoothing;spatial analysis;spatial reference system;statistical classification	Alessandra Menafoglio;Piercesare Secchi	2017	European Journal of Operational Research	10.1016/j.ejor.2016.09.061	computer science;data science;machine learning;data mining;statistics	ML	31.968582799987697	-35.9545188268787	43644
11df25b4e074b7610ec304a8733fa47625d9faca	collaborative neighbor representation based classification using l2-minimization approach	journal;collaborative representation;face recognition;pattern classification;sparse representation;nearest neighbor representation	0167-8655/$ see front matter 2012 Elsevier B.V. A http://dx.doi.org/10.1016/j.patrec.2012.09.024 ⇑ Corresponding author. E-mail address: zhangyi@scu.edu.cn (Z. Yi). Applications of sparse signal representation in image processing and pattern recognition have attracted a great deal of attention. Sparse representation based classification (SRC) methods emphasizes on sparse representation computed by l1-minimization to exploit the underlying sparsity in the problem domain, and argued the importance of sparse representation that improved the discrimination to achieve robust and accurate classification results. Recently, many studies have shown the role of collaborative representation (CR) in SRC, which actually improved the classification accuracy. In this paper, we proposed a novel collaborative neighbor representation method for multi-class classification based on l2-minimization approach with the assumption of locally linear embedding (LLE). The proposed method represents a test sample over the dictionary by automatically choosing optimal nearest basis spanned in the same linear subspace as of test sample. The proposed representation method achieves competitive classification accuracy via optimal neighbor representation having discriminative learning power. Extensive experiments on real-world face and digit databases are performed to analyze the performance of the proposed method against SRC methods. Result clearly shows that the proposed method achieves competitive results for face recognition and pattern classification, and is significantly much faster and comparably accurate than SRC based classification methods. 2012 Elsevier B.V. All rights reserved.	algorithmic efficiency;arc diagram;coefficient;computational complexity theory;database;dictionary;experiment;facial recognition system;image processing;multiclass classification;nonlinear dimensionality reduction;pattern recognition;problem domain;sample rate conversion;semiconductor research corporation;set redundancy compression;signal-to-noise ratio;sparse approximation;sparse matrix;statistical classification	Waqas Jadoon;Zhang Yi;Lei Zhang	2013	Pattern Recognition Letters	10.1016/j.patrec.2012.09.024	facial recognition system;computer science;machine learning;pattern recognition;sparse approximation;data mining	AI	26.667980460482212	-42.2641362445884	43653
51ad74020bc240d42caa54c9a4bfb6e5054b102b	stochastic reasoning for structural pattern recognition: an example from image-based uav navigation	uav;navigation;expectation maximization;decision rationales;structural pattern recognition	This paper reports on the statistical embedding of a structural pattern recognition system into the autonomous navigation of an unmanned aerial vehicle (UAV). A rule-based system is used for the recognition of visual landmarks such as bridges in aerial views. In principle, rule-based systems can be designed and coded with no training data at hand, but a sound interpretation and utilization of the achieved results needs statistical inference and representative data sets of sufficient coverages. Flying a UAV with an experimental system is expensive, risky, and legally questionable. Therefore, we chose a virtual globe as a camera simulator providing arbitrary amounts of training and test data. The expected positions of landmarks in the aerial views are modeled by mixture models representing inliers, outliers, and intermediate forms which stem from similar structures appearing frequently in the vicinity of landmarks. The parameters of the corresponding likelihood functions are estimated by the Expectation–Maximization method. Using these estimates, we carry out tests and compare the results for heuristic, pessimistic, optimistic, and Bayesian decision rationales. This performance evaluation reveals the superiority of the Bayesian approach.	structural pattern;syntactic pattern recognition;unmanned aerial vehicle	Eckart Michaelsen;Jochen Meidow	2014	Pattern Recognition	10.1016/j.patcog.2014.02.009	computer vision;navigation;simulation;expectation–maximization algorithm;computer science;artificial intelligence;machine learning;pattern recognition	Vision	37.38497098001141	-39.76211817105758	43664
628f51c41d56ca3952c2410ba924e151c4ba3e12	minimum-volume ellipsoids containing compact sets : application to parameter bounding	experimental design;elipsoide;plan experiencia;parameter bounding;estimation erreur;plan experience;ellipsoide;error estimation;estimacion error;estimacion parametro;minimum volume ellipsoid;erreur bornee;ellipsoide volume minimum;parameter estimation;estimation parametre;borne parametre;bounded error estimation;ellipsoid	Abstract   The problem of finding the minimum-volume ellipsoid containing a compact set χ is shown to be equivalent to the determination of an optimal distribution of weights over χ. Several equivalent conditions for optimality of this distribution are obtained, and used to construct algorithms guaranteed to converge to the optimum. These algorithms can be used to approximate the posterior feasible set for the parameters of a model in the bounded-error context. Linear and nonlinear illustrative examples are treated.		Luc Pronzato;Eric Walter	1994	Automatica	10.1016/0005-1098(94)90075-2	econometrics;mathematical optimization;mathematics;ellipsoid;estimation theory;design of experiments;statistics	NLP	33.47540424017803	-24.60484191382863	43682
5c54905d39128d5e5b1ab73768c5de54d4e3bcbe	sampling and estimation for (sparse) exchangeable graphs		Sparse exchangeable graphs on R + , and the associated graphex framework for sparse graphs, generalize exchangeable graphs on N, and the associated graphon framework for dense graphs. We develop the graphex framework as a tool for statistical network analysis by identifying the sampling scheme that is naturally associated with the models of the framework, and by introducing a general consistent estimator for the parameter (the graphex) underlying these models. The sampling scheme is a modification of independent vertex sampling that throws away vertices that are isolated in the sampled subgraph. The estimator is a dilation of the empirical graphon estimator, which is known to be a consistent estimator for dense exchangeable graphs; both can be understood as graph analogues to the empirical distribution in the i.i.d. sequence setting. Our results may be viewed as a generalization of consistent estimation via the empirical graphon from the dense graph regime to also include sparse graphs.	dilation (morphology);graphon;sampling (signal processing);social network analysis;sparse matrix;vertex (graph theory)	Victor Veitch;Daniel M. Roy	2016	CoRR		1-planar graph;combinatorics;discrete mathematics;dense graph;mathematics;maximal independent set;chordal graph;indifference graph;statistics	ML	26.408351636563648	-28.116633563891906	43689
b37c569fe0f71f8d9449621fbccb6701db320f04	hyperspectral image refined plant classification by graph based composite kernel		Recently, the popularity of using hyperspectral image to study and monitor plant characteristics and conditions has been increased. The use of hyperspectral image improves the breeding process and increases profits. In the case of hyperspectral data with high spectral resolution characteristics suitable for intraclass classification, this paper focuses on the application of hyperspectral image analysis in distinguishing among different plant species. Plant intraclass classification is sophisticated due to its small spectral differences. Hence, a refined hyperspectral image classification method for plant, referred as SI-GCK which uses Spectral Index (SI) to represent plant spectral, and take advantage of semi-supervised graph-based composite kernel (GCK) method to combine spectral information and spatial location of pixels for classification is presented in this paper. As a comparison, sequential floating forward selection (SFFS) is used to select spectral bands for SVM learning. Its accuracy of plant classification is nearly equal to result by means of SI, and the proposed method in this paper is better than aforementioned.	computer vision;cooperative breeding;image analysis;kernel (operating system);pixel;semiconductor industry;stepwise regression	Yanling Liu;Yingjie Zhang	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518585	support vector machine;pixel;computer vision;kernel (linear algebra);plant taxonomy;artificial intelligence;spectral bands;hyperspectral imaging;contextual image classification;computer science;spectral resolution	Vision	30.649347125082503	-44.13870401285579	43749
18b1ae62f058e499add5070b250fbbfef6e88501	a deep neural network architecture to estimate node assignment costs for the graph edit distance		The problem of finding a distance and a correspondence between a pair of graphs is commonly referred to as the Error-tolerant Graph matching problem. The Graph Edit Distance is one of the most popular approaches to solve this problem. This method needs to define a set of parameters and the cost functions aprioristically. On the other hand, in recent years, Deep Neural Networks have shown very good performance in a wide variety of domains due to their robustness and ability to solve non-linear problems. The aim of this paper is to present a model to compute the assignments costs for the Graph Edit Distance by means of a Deep Neural Network previously trained with a set of pairs of graphs properly matched. We empirically show a major improvement using our method with respect to the state-of-the-art results.	graph edit distance;network architecture	Xavier Cortés;Donatello Conte;Hubert Cardot;Francesc Serratosa	2018		10.1007/978-3-319-97785-0_31	matching (graph theory);architecture;artificial neural network;graph edit distance;machine learning;robustness (computer science);graph;artificial intelligence;computer science	Arch	26.604236746423624	-48.72418233393646	43756
76fc1f133349fb09f97bdce5c5c8d8b657691e24	vhr image change detection based on discriminative dictionary learning	change detection;image resolution;image resolution feature extraction image classification image representation;image classification;vhr remote sensing image;image representation;feature extraction;discriminative dictionary;dictionaries feature extraction training context spatial resolution matching pursuit algorithms;discriminative dictionary vhr image change detection discriminative dictionary learning very high resolution image change detection feature extraction feature classification algorithm classification task sparse representation algorithm;sparse representation;discriminative dictionary change detection vhr remote sensing image sparse representation	The difficulty of Very High Resolution (VHR) image change detection is mainly due to the low separability between the changed and unchanged class. The traditional approaches usually address the problem by solving the feature extraction and classification separately, which cannot ensure that the classification algorithm makes the best use of the features. Considering this, we propose a novel approach that combines the feature extraction and the classification task by utilizing the sparse representation algorithm with discriminative dictionary. Experiments on real data sets show that our method achieves effective results.	algorithm;data dictionary;feature extraction;linear separability;machine learning;sparse approximation;sparse matrix	Kun Ding;Chunlei Huo;Yuan Xu;Chunhong Pan	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6638311	computer vision;contextual image classification;feature detection;edge detection;image resolution;feature extraction;k-svd;computer science;machine learning;pattern recognition;sparse approximation;change detection;feature	Vision	30.347845521227462	-44.511181469246914	43767
30527f170a3995df2ef8952cf3c925606b04b476	a novel drive status monitoring system	soft shape context concept;automobiles;image processing;driver face image;steering control degree drive status monitoring system driver face image front road image sight orientation lane path orientation car heading direction soft shape context concept;drive status monitoring system;lane path orientation;head direction;front road image;monitoring system;monitoring face detection lips flowcharts road accidents image edge detection cameras eyes turning cybernetics;sight orientation;traffic engineering computing automobiles image processing object detection steering systems;traffic engineering computing;car heading direction;correlation coefficient;steering control degree;object detection;steering systems	This paper describes a novel drive status monitoring system. Two fixed cameras are used in the proposed system to capture the driver's face image and the front road image to find the driver's sight orientation, the lane path orientation, and the car heading direction, and these 3 orientation information are calculated for 2 correlation coefficients by using the soft-shape-context concept to indicate the driver's steering control degrees. Three driving situations of the safe zone, risky zone, and unconscious zone are suggested according the combined coefficient of 2 correlation coefficients. This proposed system is capable to cooperate with other pre-crash systems based on the distance information to neighbor cars.	coefficient;course (navigation);shape context	Jiann-Der Lee;Jiann-Der Li;Li-Chang Liu	2006	2006 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2006.385036	computer vision;simulation;image processing;computer science	Robotics	46.13316579354765	-41.46720050911952	43809
c6e3d1c100f120438fdb3f609691f875d422f8f9	robust face recognition via facial disguise learning	会议论文	The sparse representation based classifier (SRC) has been successfully applied to robust face recognition (FR) with various disguises. Following SRC, recently regularized robust coding (RRC) was proposed for more robustness to facial occlusion by designing a new robust representation residual term. Although RRC has achieved the leading performance, it ignores the prior knowledge embedded in facial disguises. In this paper, we proposed a novel facial disguise learning (FDL) model, in which the unknown occlusion pattern in the query image is learned using a collected disguise mask dictionary. Two learning strategies with an iterative reweighted coding algorithm, independent FDL and joint FDL, were presented in this paper. The experiments on face recognition with disguise clearly show the advantage of the proposed FDL in accuracy and efficiency.	facial recognition system	Meng Meng Yang;Linlin Shen	2014		10.1007/978-3-662-45643-9_33	three-dimensional face recognition;face hallucination	Vision	29.21922984910592	-46.93555843521476	43828
850db8e50dfc8e9c9e5efc57589c87cae510a1fa	continuous bayesian networks for the estimation of species richness		We propose a new methodology based on continuous Bayesian networks for assessing species richness. Specifically, we applied a restricted structure Bayesian network, known as tree augmented naive Bayes (TAN), regarding a set of environmental continuous predictors. First, we analysed the relationships between the response variable (called the terrestrial vertebrate species richness) and a set of environmental predictors. Second, the learnt model was used to estimate the species richness in Andalusia (Spain) and the results were depicted on a map. In addition to this, the TAN model was compared to three other methods commonly used for regression in terms of their root mean squared error. The experimental results showed that the TAN model not only was competitive from the point of view of accuracy but also managed to deal with the species richness–environment relationship, which is complex from the ecological point of view. The results highlight that landscape heterogeneity, topographical and social variables had a direct relationship with species richness while climatic variables showed more complicated relationships with the response.	bayesian network;mean squared error;naive bayes classifier;point of view (computer hardware company);terrestrial television;topography	Alejandra Maldonado;R. F. Ropero;P. A. Aguilera;Rafael Rumí;Antonio Salmerón	2015	Progress in Artificial Intelligence	10.1007/s13748-015-0067-8	statistics;machine learning;computer science;naive bayes classifier;artificial intelligence;probabilistic logic;bayesian network;mean squared error;species richness	AI	39.69910042335148	-24.459533701129445	43859
a000baa748e4b0023318820e9774b62486ac9c68	eyelid and iris tracking method with novel eye models	user state estimation eyelid tracking method iris tracking method eye models gaze estimation system complex calibration equipment calibration automatic user eyelid tracking automatic user iris tracking eyeball model eyelid shape tracking;eyelids iris estimation shape head computational modeling image edge detection;gaze tracking;iris recognition;shape recognition;state estimation;state estimation calibration gaze tracking iris recognition shape recognition;calibration	These days, useful gaze estimation system is getting expected as a new interface. However previous most gaze estimation systems require special equipment or complex calibration. In this report, we propose the method which tracks user's eyelid and iris automatically with novel eyelid model and eyeball model, aiming at useful gaze estimation as a future work. Eyelid shape tracking is also useful for more accurate iris tracking and user's state estimation.		Kimimasa Tamura;Yoshimitsu Aoki	2013	Proceedings of the 2013 IEEE/SICE International Symposium on System Integration	10.1109/SII.2013.6776739	computer vision;communication;computer graphics (images)	EDA	47.35344372092168	-44.0086348331117	43874
fb24bb7c8d1bdb617ba4458805f0556acab22059	generation method of the trigger signal for the automatic capture system to the harmful animals with intelligent image processing		Up to now, some damage by the harmful animals such as deer and wild boars are increased abruptly. Their damage such as crash between cars and harmful animals, eating all domestic vegetables are occurred at not only local area but also urban area. Under these backgrounds, an efficient and reliable capturing system for the harmful animals is requested, extremely. We aim at to develop an efficient automatic capturing system for harmful animals using image as a final product. Especially, we focus its generation method of the trigger signal for the capture devices. For the all days use without human’s monitor and operation, we propose a new generation method for the trigger signal using an intelligent image processing and a new camera device Kinect. We consider activation of the trap according to the animal’s shape and size furthermore motion by the optical flow analysis, automatically. The Kinect is equipped infrared radiation projector and its receiver camera, which can get the depth image. We construct an experimental device, which is simple and miniature of the real size and investigate detection ability. We discuss its effectiveness of the generation method for the realization of the capture system.	image processing	Fumiaki Takeda	2014		10.1007/978-3-319-07593-8_52	embedded system;computer vision;computer hardware;computer science	Robotics	45.29994447013622	-41.5387613334182	43878
90bde45c3e718d5dd27345fa78ac97a4b825b5aa	abnormal walking gait analysis using silhouette-masked flow histograms	image motion analysis;video signal processing;silhouette masked flow histograms abnormal walking gait analysis gait patterns computer vision human silhouette extraction input videos frame to frame optical flows motion metrics histogram representations eigenspace transformation walking pattern;silhouette masked flow histograms;motion metrics;computer vision;human silhouette extraction;frame to frame optical flows;feature extraction;gait analysis;abnormal walking gait analysis;eigenspace transformation;optical flow;video signal processing computer vision feature extraction image motion analysis image sequences;legged locomotion histograms optical computing image motion analysis diseases computer vision humans videos motion analysis pattern recognition;histogram representations;input videos;gait patterns;image sequences;walking pattern	Abnormalities of gait patterns can provide telltale signs of the onset or progression of certain diseases. This paper proposes a simple but effective approach to abnormal gait analysis using computer vision techniques. The proposed method starts with the extraction of human silhouettes from input videos and the computation of frame-to-frame optical flows, then motion metrics based on histogram representations of silhouette-masked flows, and finally gait analysis with eigenspace transformation. Different from current gait classification and recognition studies, the proposed method deals with another interesting problem, namely not only determining different styles of the same walking action but detecting whether or not it is deviated from usual walking pattern, which is expected as a feasible means to deduce physical conditions of people. Experimental results show its promising performance	color gradient;computation;computer vision;experiment;gait analysis;homography (computer vision);mathematical model;onset (audio);sensor;statistical classification;tag (game)	Liang Wang	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.199	computer vision;simulation;gait analysis;feature extraction;computer science;optical flow;gait;computer graphics (images)	Vision	38.467588451563536	-48.816690320752095	43914
5bca6f299530d3becccf5837ccd5bf51f18dc877	a visual positioning system for vehicle or mobile robot navigation	motion analysis;navegacion;robot movil;automatic driving;vision ordenador;tecnologia electronica telecomunicaciones;posicionamiento;localization;conduccion automatica;feature tracking;self localization;analyse mouvement;conduite automatique;localizacion;computer vision;seguimiento modelo;navigation;positioning;poursuite modele;localisation;robot mobile;mobile robot navigation;model following;positioning system;vision ordinateur;analisis movimiento;tecnologias;grupo a;moving robot;positionnement	Localization of a vehicle is a key component for driving assistance or autonomous navigation. In this work, we propose a visual positioning system (VPS) for vehicle or mobile robot navigation. Different from general landmark-based or model-based approaches, which rely on some predefined known landmarks or a priori information about the environment, no assumptions on the prior knowledge of the scene are made. A stereo-based vision system is built for both extracting feature correspondences and recovering 3-D information of the scene from image sequences. Relative positions of the camera motion are then estimated by registering the 3-D feature points from two consecutive image frames. Localization of the mobile platform is finally given by the reference to its initial position. key words: motion analysis, feature tracking, self-localization	autonomous robot;global positioning system;mobile device;mobile robot;motion estimation;robotic mapping;virtual private server	Huei-Yung Lin;Jen-Hung Lin	2006	IEICE Transactions	10.1093/ietisy/e89-d.7.2109	computer vision;navigation;simulation;internationalization and localization;computer science;artificial intelligence;mobile robot navigation	Robotics	52.533518957649356	-40.268631805104285	43930
629ae01949496449dbf7980f7d2ea7a67fe0df59	face verification across aging based on deep convolutional networks and local binary patterns	会议论文	This paper proposes a novel method to learn a set of high-level feature representations for face verification across aging. Conventional hand-crafted features are not capable to overcome aging effects. In order to obtain an accurate face representation, we apply the combination of a nine-layer deep convolutional neural network and Local Binary Pattern(LBP) histograms, both of which are essential to face recognition. On account of the need of large quantity data in deep learning methods, we train the model on the publicly available cross-age face dataset CACD (Cross-Age Celebrity Dataset), which contains more than 160000 face images of 2000 different celebrities. Experiments on the CACD and LFW (Labeled Faces in the Wild) dataset demonstrate that the proposed approach outperforms the state-of-the-art methods. In addition, hairstyle, facial expression, changes of background and occlusion provide discriminative cues to the system of face verification.	local binary patterns	Huanhuan Zhai;Chunping Liu;Husheng Dong;Yi Ji;Yun Guo;Shengrong Gong	2015		10.1007/978-3-319-23989-7_35	computer science;theoretical computer science;machine learning;pattern recognition	AI	31.004347777247947	-51.375493261659294	44067
42cd84b93db811915b4747d32bb16047a1f95053	phase based modelling of dynamic textures	static background;global motion coherence;dynamic texture modelling;phase based modelling;static background phase based modelling dynamic texture sequence spatiotemporal variations dynamic texture modelling image representation linear dynamical system global motion coherence fourier phase;image texture image representation image sequences;image texture;dynamic texture;image representation;linear dynamical system;fourier phase;spatiotemporal phenomena optical noise fires clouds motion analysis optical computing image motion analysis motion estimation physics computing coherence;dynamic texture sequence;spatiotemporal variations;image sequences	This paper presents a model of spatiotemporal variations in a dynamic texture (DT) sequence. Most recent work on DT modelling represents images in a DT sequence as the responses of a linear dynamical system (LDS) to noise. Despite its merits, this model has limitations because it attempts to model temporal variations in pixel intensities which do not take advantage of global motion coherence. We propose a model that relates texture dynamics to the variation of the Fourier phase, which captures the relationships among the motions of all pixels (i.e. global motion) within the texture, as well as the appearance of the texture. Unlike LDS, our model does not require segmentation or cropping during the training stage, which allows it to handle DT sequences containing a static background. We test the performance of this model on recognition and synthesis of DT's. Experiments with a dataset that we have compiled demonstrate that our phase based model outperforms LDS.	compiler;dynamical system;pixel	Bernard Ghanem;Narendra Ahuja	2007	2007 IEEE 11th International Conference on Computer Vision	10.1109/ICCV.2007.4409094	image texture;linear dynamical system;computer vision;simulation;computer science;mathematics;texture compression;texture filtering;computer graphics (images)	Vision	39.04574133059006	-47.48389618572676	44115
c2f47ece410cd2fe78cd6892e67af1a4025883dd	multicamera object detection and tracking with object size estimation	reliability;video surveillance;surveillance;cameras three dimensional displays surveillance estimation vehicles calibration reliability;video surveillance object detection object tracking;multicamera object detection late fusion surveillance scenes multicamera solutions object size estimation object tracking;estimation;three dimensional displays;object tracking;vehicles;calibration;cameras;object detection	A number of multi-camera solutions exist for tracking objects of interest in surveillance scenes. Generally, the approach follows the idea of either early fusion (where all cameras are used to make a decision about detection and tracking) or late fusion (where objects are detected and tracked in individual cameras independently, and then the results combined). This paper describes an early fusion approach derived from the common approach of projecting foreground mask into a common coordinate system. The described approach extends prior work to suppress false detections and automatically estimate the size of the object under tracking, thus enabling it to work in environments containing a mix of people and vehicles.	algorithm;object detection;sensor	Murray Evans;Christopher J. Osborne;James M. Ferryman	2013	2013 10th IEEE International Conference on Advanced Video and Signal Based Surveillance	10.1109/AVSS.2013.6636636	computer vision;estimation;calibration;simulation;tracking system;viola–jones object detection framework;video tracking;reliability;statistics;computer graphics (images)	Robotics	44.55023567628417	-46.1583742027425	44133
2a82bdef6f611216e63cf63b16ae14ce7c22601e	a primal-dual algorithm for data gathering based on matrix completion for wireless sensor networks	electronic mail;primal dual optimization approach data gathering novel matrix completion algorithm wireless sensor networks tractable convex optimization problem hilbert space global features local features;discrete cosine transforms;data structures;optimization;correlation;wireless sensor networks convex programming hilbert spaces matrix algebra;wireless sensor networks;optimization wireless sensor networks correlation data structures discrete cosine transforms electronic mail	In this paper, we proposed a novel matrix completion algorithm in Wireless Sensor Networks. Our contribution is twofold. First, we propose a general formulation of the problem of data gathering as a tractable convex optimization problem on the Hilbert space of data measurement matrices. The proposed criterion takes full advantage of both the global features, particularly low rank nature of data, and local features namely the sparsity and the spatio-temporal correlation in the sensors data to improve the recovery accuracy. Second, we design a new class of primal-dual optimization approach in order to optimize the resulting regularized criterion. Experiments carried out on two datasets show that the proposed algorithm outperforms state-of-the-art methods for low sampling rate and achieves a good recovery accuracy even if the sampling rate is very low.	cobham's thesis;convex optimization;diffusing update algorithm;hilbert space;mathematical optimization;optimization problem;sampling (signal processing);sensor;sparse matrix	Mohamed Ali Moussa;Yosra Marnissi;Yacine Ghamri-Doudane	2016	2016 IEEE International Conference on Communications (ICC)	10.1109/ICC.2016.7511420	mathematical optimization;discrete mathematics;wireless sensor network;data structure;computer science;theoretical computer science;mathematics;correlation	Robotics	26.515016654306415	-39.213218137929765	44224
6dcc0a6748b01bd70241efb8b7120cb48eefd0bc	fuzzy-clustering-based discriminant method of multiple quadric surfaces for noisy and sparse range data	range data;noise clustering;fuzzy clustering;fuzzy c means;stereo vision;fuzzy c varieties;shape modeling		discriminant;fuzzy clustering;sparse	Hideaki Kawano;Hiroshi Maeda;Norikazu Ikoma	2010	JACIII	10.20965/jaciii.2010.p0160	computer vision;fuzzy clustering;flame clustering;computer science;stereopsis;machine learning;pattern recognition	Robotics	33.97379972542402	-42.08807131265793	44232
b00098e314c0ec9efbafae264082ba0fd614f205	structure and kinematics triangulation with a rolling shutter stereo rig	moving object;kinematics cameras cmos image sensors geometry solid modeling biosensors nonlinear distortion computer vision nonlinear equations least squares methods;image motion analysis cameras;image motion analysis;rolling shutter cameras;shape parameters;velocity parameters;geometry;nonlinear error criterion;kinematics;computer vision;cmos image sensors;nonlinear distortion;a priori knowledge;noise level;shape;three dimensional displays;image reconstruction;solid modeling;least square;mathematical model;rolling shutter stereo rig;nonlinear error criterion rolling shutter stereo rig spatio temporal triangulation method rolling shutter cameras rigid moving objects shape parameters velocity parameters;rigid moving objects;nonlinear equations;experimental evaluation;spatio temporal triangulation method;structure and motion;least squares methods;cameras;biosensors	We describe a spatio-temporal triangulation method to be used with rolling shutter cameras. We show how a single pair of rolling shutter images enables the computation of both structure and motion of rigid moving objects. Starting from a set of point correspondences in the left and right images, we introduce the velocity and shutter characteristics in the triangulation equations. This results in a non-linear error criterion whose minimization in the least square sense provides the shape and velocity parameters. Unlike previous work on rolling shutter cameras, the constraining assumption of a-priori knowledge about the object geometry is removed and a full 3D motion model is considered. The aim of this work is thus to make the use of rolling shutter cameras of a broader interest. Experimental evaluation results confirm the feasibility of the approach.	algorithm;computation;correspondence problem;harris affine region detector;matching (graph theory);misiurewicz point;movie projector;nonlinear system;random sample consensus;structure from motion;velocity (software development)	Omar Ait-Aider;François Berry	2009	2009 IEEE 12th International Conference on Computer Vision	10.1109/ICCV.2009.5459408	iterative reconstruction;computer vision;nonlinear distortion;kinematics;a priori and a posteriori;simulation;nonlinear system;shape;mathematical model;mathematics;geometry;solid modeling;least squares;biosensor	Vision	53.115031132353685	-49.74420032081468	44289
2cb31b1a334de6763732a0e147ce30ac1eb849d4	preliminary investigation of unconstrained person identification for tabletops using soft biometrics	biometrics access control;unconstrained person identification user activities occlusion robust person identification head shape shoulder width depth image user soft biometrics extraction ceiling mounted depth camera tabletop system;skeleton;head feature extraction cameras biometrics access control shape skeleton shoulder;shape;feature extraction;shoulder;object recognition biometrics access control computer vision feature extraction;head;cameras	This paper tries to realize unconstrained person identification for tabletop systems using a ceiling-mounted depth camera that overlooks a table. We extract a user's soft biometrics from a depth image, such as the shoulder width and shape of the head that can be captured from the ceiling. We then try to achieve robust person identification by combining each of the soft biometric. Person identification from a ceiling has several advantages: a ceiling-mounted camera hardly suffers from the occlusion problem, does not interfere users' activities and can capture multiple users in one shot. In the preliminary experiment, proposed method has achieved 83% accuracy identifying 19 participants.	information sensitivity;multi-user;soft biometrics	Akira Masuda;Takuya Maekawa;Yasuo Namioka	2016	2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops)	10.1109/PERCOMW.2016.7457063	computer vision;simulation;feature extraction;shape;computer science;head;skeleton	Robotics	46.36610761358057	-43.48846528626015	44298
7e868c8d92892125fb980b5372612246fb46fcab	collaborative strategy for visual object tracking		Adaptively learning the difference between object and background, discriminative trackers are able to overcome the complex background problem in visual object tracking. However, they are not robust enough to handle the out-of-plane rotation of object, which reduces recall performance. Meanwhile, allowing individual parts certain criterion of freedom, part-based trackers can better handle the out-of-plane rotation problem. However, they are prone to be affected by complex background, leading to low precision performance. To simultaneously address both issues, we propose a collaborative strategy that makes mutual enhancement between a discriminative tracker and a part-based tracker possible to obtain better overall performance. On one hand, we use validated results from the part-based tracker to update the discriminative tracker for recall performance improvement. On the other hand, based on confident results from the discriminative tracker we adaptively update the part-based tracker for simultaneous precision performance improvement. Experiments on various challenge sequences show that our approach achieved the state-of-the-art performance, which demonstrated the effectiveness of mutual collaboration between the two trackers.	bittorrent tracker;discriminative model;experiment	Yongquan Yang;Ning Chen;Shenlu Jiang	2017	Multimedia Tools and Applications	10.1007/s11042-017-4633-x	artificial intelligence;discriminative model;computer vision;computer science;pattern recognition;performance improvement;video tracking;bittorrent tracker;human–robot interaction	Vision	33.69457467886731	-47.83626994024203	44309
e1a38c70b99fa7677d0b167260bd285513ec4048	a robust functional-data-analysis method for data recovery in multichannel sensor systems	asynchronous data;data recovery;condition monitoring;robust functional principal component analysis asynchronous data condition monitoring data recovery;nonskewed signal recovery robust functional data analysis method data recovery multichannel sensor systems condition monitoring failure prevention critical equipment sensor readings integrated systems asynchronous data sampling limited data transmission fault diagnosis fault prognosis functional principal component analysis fpca skewed distributions robust data recovery method functional data analysis reliability enhancement grand median functions sensor signal smoothing correlated signals multivariate functional regression experimental flow control loop coolant flow loop multimodular integral pressurized water reactor strongly skewed signal recovery turbofan engine data;robust functional principal component analysis;statistical distributions condition monitoring correlation methods data analysis fault diagnosis principal component analysis regression analysis sensor fusion smoothing methods;data models robustness eigenvalues and eigenfunctions predictive models bandwidth sensor systems sun	Multichannel sensor systems are widely used in condition monitoring for effective failure prevention of critical equipment or processes. However, loss of sensor readings due to malfunctions of sensors and/or communication has long been a hurdle to reliable operations of such integrated systems. Moreover, asynchronous data sampling and/or limited data transmission are usually seen in multiple sensor channels. To reliably perform fault diagnosis and prognosis in such operating environments, a data recovery method based on functional principal component analysis (FPCA) can be utilized. However, traditional FPCA methods are not robust to outliers and their capabilities are limited in recovering signals with strongly skewed distributions (i.e., lack of symmetry). This paper provides a robust data-recovery method based on functional data analysis to enhance the reliability of multichannel sensor systems. The method not only considers the possibly skewed distribution of each channel of signal trajectories, but is also capable of recovering missing data for both individual and correlated sensor channels with asynchronous data that may be sparse as well. In particular, grand median functions, rather than classical grand mean functions, are utilized for robust smoothing of sensor signals. Furthermore, the relationship between the functional scores of two correlated signals is modeled using multivariate functional regression to enhance the overall data-recovery capability. An experimental flow-control loop that mimics the operation of coolant-flow loop in a multimodular integral pressurized water reactor is used to demonstrate the effectiveness and adaptability of the proposed data-recovery method. The computational results illustrate that the proposed method is robust to outliers and more capable than the existing FPCA-based method in terms of the accuracy in recovering strongly skewed signals. In addition, turbofan engine data are also analyzed to verify the capability of the proposed method in recovering non-skewed signals.	control system;data recovery;disabled persons;dropout (neural networks);forecast of outcome;functional data analysis;functional principal component analysis;general linear model;hl7publishingsubsection <operations>;international conference on functional programming;kerrison predictor;learning to rank;mit engineering systems division;missing data;network packet;reactor (software);reactor device component;reading (activity);sampling (signal processing);sampling - surgical action;smoothing (statistical technique);sparse matrix;transmitter;yang;disease transmission;sensor (device)	Jian Sun;Haitao Liao;Belle R. Upadhyaya	2014	IEEE Transactions on Cybernetics	10.1109/TCYB.2013.2285876	real-time computing;computer science;data recovery;statistics	Robotics	35.89245944721491	-29.503732789568126	44319
7e5560eb85e6bb77c3b59afcb548263a8220ae53	hyperspectral feature selection based on mutual information and nonlinear correlation coefficient	nonlinear correlation coefficient;mutual information feature extraction hyperspectral imaging hyperspectral sensors random variables probability distribution infrared image sensors entropy multidimensional signal processing electronic mail;pattern classification correlation methods feature extraction information theory nonlinear systems;classification accuracy improvement hyperspectral feature selection mutual information multidimensional data one dimensional conditional mi component nonlinear correlation coefficient aviris 92av3c dataset;correlation methods;nonlinear correlation coefficient hypersepctral data feature selection mutual information;multi dimensional;nonlinear systems;feature extraction;numerical computation;hypersepctral data;pattern classification;mutual information;feature selection;classification accuracy;correlation coefficient;information theory	Mutual information (MI) has obvious potential for feature selection, but this has not been fully exploited in the past. In order to make numerical computation easier and more accurate, the MI of the whole multi-dimensional data can be decomposed into an amount of one-dimensional MI and one-dimensional conditional MI components. This paper reveals that using one-dimensional MI components to replace the one-dimensional conditional MI components may be problematic when the features are highly correlated, and we propose a method that using nonlinear correlation coefficient (NCC) to replace some one-dimensional MI components, which also including the conditional ones. Simulations are carried out on the AVIRIS 92AV3C dataset and the results show great potential for improvement in classification accuracy.	coefficient;computation;computer simulation;feature selection;mutual information;neural correlates of consciousness;nonlinear system;numerical analysis	Miao Zhang;Qiang Wang;Yi Shen;Bo Zhang	2009	2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2009.71	information theory;feature extraction;computer science;machine learning;pattern recognition;mathematics;mutual information;feature selection;statistics	Robotics	30.0540531949903	-42.50850860885206	44324
596d6dd19884dd869dda9a8278d3af735cadba0d	hyperspectral imagery classification with multiple regularized collaborative representations		Recent advances have shown a great potential to explore collaborative representations in hyperspectral imagery (HSI) classification, including sparse representations and joint collaborative representations. In this letter, we propose a weighted regularized collaborative representation optimized classifier (WRCROC) that makes use of multiple collaborative representations. It strikes a balance between an optimized weighted joint collaborative representation classifier, which essentially classifies a test sample to the class that minimizes the distance between the sample and its representation in the selected class, and a weighted regularized collaborative representation classifier, which actually assigns a test sample to the class that minimizes the distance between the sample and its collaborative components. The proposed WRCROC algorithm is tested on two benchmark HSI data sets. Experimental results demonstrate that the proposed algorithm performs better than existing representation-based classifiers.	algorithm;benchmark (computing);computer vision;horizontal situation indicator;sparse matrix	Xiang Chen;Shuiying Li;Jiangtao Peng	2017	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2017.2699667	support vector machine;computer vision;data set;artificial intelligence;machine learning;classifier (linguistics);hyperspectral imaging;computer science;pattern recognition	ML	29.269770165926715	-43.95990252753236	44366
2a53677037a56297163661dc1afe44fc020f3d35	convolutional neural networks for license plate detection in images		License plate detection is a challenging task when dealing with open environments and images captured from a certain distance by low-cost cameras. In this paper, we propose an approach for detecting license plates based on a convolutional neural network which models a function that produces a score for each image sub-region, allowing us to estimate the locations of the detected license plates by combining the results obtained from sparse overlapping regions. Experiments were performed on a challenging benchmark, containing 4,070 license plates in 1,829 images, captured under several weather conditions. The proposed approach achieved a precision of 0.87 and recall of 0.83, outperforming a state-of-the-art detector — a promising result, given that the experiments were performed on single images, without any kind of preprocessing or temporal integration.	artificial neural network;autostereogram;benchmark (computing);convolutional neural network;experiment;preprocessor;sensor;sparse matrix	Francisco Delmar Kurpiel;Rodrigo Minetto;Bogdan Tomoyuki Nassu	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296912	license;convolutional neural network;artificial intelligence;computer vision;task analysis;pattern recognition;computer science;preprocessor	Vision	31.232674260356607	-51.93462531335767	44577
0b2d46b28829e6833f471f590dd800f224988af1	dynamic illumination optical flow computing for sensing multiple mobile robots from a drone		In this paper, we consider a motion sense problem motivated by the International Aerial Robotics Competition Mission-7, where an aerial robot is required to provide detection and estimation about mobile vehicles. Dense optical flow computing is employed first to provide a velocity field from image sequences. Then, region growing based on the optical flow field is used to extract moving objects on the background, and motion estimation is eventually achieved while both camera and objects are moving. In addition, classical optical flow techniques do not work in the competition since there may be illumination changes, such as flashlights and reflections in the arena. To deal with this problem, the procedures of the brightness constancy relaxation and intensity normalization are combined in the optical flow algorithm. Experimental results have demonstrated the robustness against varying illumination. The proposed approach can provide motion estimation results of acceptable accuracy for several benchmark data sets and image sequences generated with micro aerial vehicles.	aerial photography;aerobot;algorithm;benchmark (computing);computation;database normalization;illumination (image);international arctic research center;linear programming relaxation;maximum flow problem;mobile robot;motion estimation;object detection;on-board data handling;optical flow;real-time clock;real-time computing;reflection (computer graphics);region growing;robotics;unmanned aerial vehicle;velocity (software development)	Shengze Cai;Yongbin Huang;Bo Ye;Chao Xu	2018	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMC.2017.2709404	robustness (computer science);computer science;normalization (statistics);optical computing;motion estimation;computer vision;region growing;mobile robot;artificial intelligence;robotics;optical flow	Robotics	52.14452750407603	-44.365461374612465	44581
2a65d7d5336b377b7f5a98855767dd48fa516c0f	fast supervised lda for discovering micro-events in large-scale video datasets	video micro events;supervised topic modeling;variational inference;video event detection	"""This paper introduces fsLDA, a fast variational inference method for supervised LDA, which overcomes the computational limitations of the original supervised LDA and enables its application in large-scale video datasets. In addition to its scalability, our method also overcomes the drawbacks of standard, unsupervised LDA for video, including its focus on dominant but often irrelevant video information (e.g. background, camera motion). As a result, experiments in the UCF11 and UCF101 datasets show that our method consistently outperforms unsupervised LDA in every metric. Furthermore, analysis shows that class-relevant topics of fsLDA lead to sparse video representations and encapsulate high-level information corresponding to parts of video events, which we denote """"micro-events""""."""	experiment;fast multipole method;high- and low-level;linear discriminant analysis;relevance;scalability;sparse matrix;unsupervised learning;variational principle	Angelos Katharopoulos;Despoina Paschalidou;Christos Diou;Anastasios Delopoulos	2016		10.1145/2964284.2967237	computer science;machine learning;pattern recognition;data mining	ML	26.440130972558478	-45.971552820747526	44613
e495b0cc13f5968da9c0ad8077a1a134e4cb4035	enhancing gait based person identification using joint sparsity model and ℓ1-norm minimization	covariate factors;gait recognition;joint sparsity model jsm;l 1;rp based dimensional reduction;l 1 norm;individual identification	We consider the problem of person identification using gait sequences under normal, carrying bag and different clothing conditions as the main concern. It has been demonstrated that Gait Energy Image (GEI) can attain a better gait recognition rate under normal conditions. However, it has been shown that GEI is not robust enough to handle the carrying bags and different clothing conditions. Instead of GEI, there are several appearance based gait features in the available literature to reduce the effect of covariate factors by keeping dynamic parts and removing the static parts of the gait features under the assumption that the carrying bags and different clothing conditions affect mostly the static parts. It is however shown in the literature that the static parts also contain valuable information and removal of certain static parts such as head by mistake thigh typed certainly decreases the recognition rate.Our main objective has been to increase the gait recognition rate on different clothing and carrying bag covariate gait sequences. Therefore instead of removing static parts, the Joint Sparsity Model (JSM) is applied to identify the carrying bags and different clothings conditions from GEI features. If a set of GEI feature vectors is submitted to JSM model then a common component and an innovations component for each GEI feature are obtained. The innovations component that has unique characteristic to each of features is considered to identify the covariate conditions. The identified covariate conditions are removed from GEI features and a novel gait feature called GEI JSM is generated. The dimension of GEI JSM is reduced using Random Projection (RP) approach and ? 1 -norm minimization technique based sparse representation is used for classification. It is demonstrated that the RP and ? 1 -norm minimization based sparse representation approach provides statistically significant better results than that of the existing individual identification approaches.	sparse matrix;taxicab geometry	Yogarajah Pratheepan;Priyanka Chaurasia;Joan Condell;Girijesh Prasad	2015	Inf. Sci.	10.1016/j.ins.2015.01.031	speech recognition;machine learning;pattern recognition;mathematics	AI	26.3883788306927	-42.92495194568028	44630
0bdbf63900bf75077b00984d757ca3f10eb372c4	kernel fisher npe for face recognition	subspace learning;neighborhood preserving embedding;linear approximation;face recognition;subspace method;local linear embedding	Neighborhood Preserving Embedding (NPE) is a subspace learning algorithm. Since NPE is a linear approximation to Locally Linear Embedding (LLE) algorithm, it has good neighborhood-preserving properties. Although NPE has been applied in many fields, it has limitations to solve recognition task. In this paper, a novel subspace method, named Kernel Fisher Neighborhood Preserving Embedding (KFNPE), is proposed. In this method, discriminant information as well as the intrinsic geometry relations of the local neighborhoods are preserved according to prior class-label information. Moreover, complex nonlinear variations of real face images are represented by nonlinear kernel mapping. Experimental results on ORL face database demonstrate the effectiveness of the proposed method.	algorithm;discriminant;facial recognition system;kernel (operating system);linear approximation;nonlinear dimensionality reduction;nonlinear system;norton power eraser;return loss	Guoqiang Wang;Zongying Ou;Fan Ou;Dianting Liu;Feng Han	2007		10.1007/978-3-540-72393-6_88	facial recognition system;speech recognition;computer science;machine learning;pattern recognition;mathematics;linear approximation	Vision	25.88792168597039	-42.20544579111582	44645
6082e47bd6d063a5066e6be41e2908b4bc381e42	deflation methods for sparse pca	optimization problem	In analogy to the PCA setting, the sparse PCA problem is often solved by iteratively alternating between two subtasks: cardinality-constrained rank-one variance maximization and matrix deflation. While the former has received a great deal of attention in the literature, the latter is seldom analyzed and is typically borrowed without justification from the PCA context. In this work, we demonstrate that the standard PCA deflation procedure is seldom appropriate for the sparse PCA setting. To rectify the situation, we first develop several deflation alternatives better suited to the cardinality-constrained context. We then reformulate the sparse PCA optimization problem to explicitly reflect the maximum additional variance objective on each round. The result is a generalized deflation procedure that typically outperforms more standard techniques on real-world datasets.	expectation–maximization algorithm;linear discriminant analysis;mathematical optimization;optimization problem;principal component analysis;sparse pca;sparse matrix	Lester W. Mackey	2008			optimization problem;econometrics;mathematical optimization;sparse pca;computer science;machine learning;mathematics	ML	26.962230388761125	-34.38791704643944	44690
e9e80247fc53c7822fedc034313c30ce76d2bb9a	a unified bayesian approach for prediction and detection using mobile sensor networks	gaussian processes;bayes methods;statistical analysis approximation theory bayes methods distributed sensors gaussian processes inference mechanisms markov processes mobile robots random processes;inference mechanisms;mobile robots;distributed sensors;approximation theory;statistical analysis;random processes;integrated nested laplace approximation unified bayesian approach binary random event prediction random scalar field prediction mobile sensor networks heterogeneous data collection heterogeneous uncertainties false detection rates measurement noises statistical correlations heterogeneous random events heterogeneous random fields latent random variables gaussian markov random field statistical inference gaussian approximation;markov processes;robot sensing systems bayesian methods approximation methods covariance matrix vectors computational modeling detectors	In this paper, we develop a unified Bayesian approach that enables the prediction of binary random events and random scalar fields from heterogeneous data collected by mobile sensor networks with different detectors and sensors. The heterogeneous uncertainties such as different false detection rates and measurement noises are taken into account. This proposed unified approach exploits the statistical correlations among heterogeneous random events and random fields via their latent random variables which are modeled by a Gaussian Markov random field. The statistical inference based on Gaussian approximation is then provided in order to predict the random events and/or scalar fields. The fully Bayesian approach based on the integrated nested Laplace approximation is also proposed to deal with the case where model parameters are not known a priori. Simulation results demonstrate the correctness and usefulness of the proposed approaches.	approximation;bayesian programming;correctness (computer science);markov chain;markov random field;sensor;simulation	Yunfei Xu;Jongeun Choi;Sarat Dass;Tapabrata Maiti	2012	2012 IEEE 51st IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2012.6426817	independent and identically distributed random variables;random variate;gaussian random field;mobile robot;stochastic process;econometrics;random field;multivariate random variable;random element;sum of normally distributed random variables;machine learning;random function;stochastic simulation;gaussian process;mathematics;markov process;statistics;variable-order markov model;approximation theory	Robotics	39.59948796358629	-25.688962342891152	44694
aa3e276120d6c4212f44224f6a2a409384232c9e	large scale audio-visual video analytics platform for forensic investigations of terroristic attacks		The forensic investigation of a terrorist attack poses a huge challenge to the investigative authorities, as several thousand hours of video footage need to be spotted. To assist law enforcement agencies (LEA) in identifying suspects and securing evidences, we present a platform which fuses information of surveillance cameras and video uploads from eyewitnesses. The platform integrates analytical modules for different input-modalities on a scalable architecture. Videos are analyzed according their acoustic and visual content. Specifically, Audio Event Detection is applied to index the content according to attack-specific acoustic concepts. Audio similarity search is utilized to identify similar video sequences recorded from different perspectives. Visual object detection and tracking are used to index the content according to relevant concepts. The heterogeneous results of the analytical modules are fused into a distributed index of visual and acoustic concepts to facilitate rapid start of investigations, following traits and investigating witness reports.	acoustic cryptanalysis;align (company);basic stamp;closed-circuit television;digital video;entry point;fundamental fysiks group;hard coding;microsoft windows;monkey's audio;object detection;scalability;server (computing);similarity search;sound card;time server;upload;video content analysis;video tracking	Alexander Schindler;Martin Boyer;Andrew Lindley;David Schreiber;Thomas Philipp	2019		10.1007/978-3-030-05716-9_9	computer science;multimedia;data mining;architecture;law enforcement;scalability;object detection;upload;analytics;nearest neighbor search	Vision	34.54050066213088	-51.46572394658071	44698
c9087a5e0079a2fd4d54a798de2c63fa8fc96fa1	a block minorization–maximization algorithm for heteroscedastic regression	convergence;parallel algorithm;biological system modeling;heteroscedastic regression;maximum likelihood estimation;log likelihood function block minorization maximization algorithm heteroscedastic regression maximum likelihood estimator ml estimator heteroscedastic regression models newton algorithms matrix multiplications big data contexts mm algorithm;parallel algorithm heteroscedastic regression minorization maximization mm algorithm maximum likelihood ml estimation;computational modeling;maximum likelihood ml estimation;big data;regression analysis big data newton method optimisation;signal processing algorithms maximum likelihood estimation algorithm design and analysis biological system modeling computational modeling big data convergence;minorization maximization mm algorithm;signal processing algorithms;algorithm design and analysis	The computation of the maximum likelihood (ML) estimator for heteroscedastic regression models is considered. The traditional Newton algorithms for the problem require matrix multiplications and inversions, which are bottlenecks in modern Big Data contexts. A new Big Data-appropriate minorization-maximization (MM) algorithm is considered for the computation of the ML estimator. The MM algorithm is proved to generate monotonically increasing sequences of likelihood values and to be convergent to a stationary point of the log-likelihood function. A distributed and parallel implementation of the MM algorithm is presented, and the MM algorithm is shown to have differing time complexity to the Newton algorithm. Simulation studies demonstrate that the MM algorithm improves upon the computation time of the Newton algorithm in some practical scenarios where the number of observations is large.	big data;computation;expectation–maximization algorithm;mm algorithm;newton;newton's method;simulation;stationary process;time complexity	Hien Duy Nguyen;Luke R. Lloyd-Jones;Geoffrey J. McLachlan	2016	IEEE Signal Processing Letters	10.1109/LSP.2016.2586180	algorithm design;econometrics;mathematical optimization;scoring algorithm;big data;convergence;ramer–douglas–peucker algorithm;expectation–maximization algorithm;computer science;mathematics;parallel algorithm;maximum likelihood;maximum likelihood sequence estimation;computational model;statistics	ML	29.3175673290843	-30.691018652957244	44752
5a62c98a801377619106f6ba87bab05fcd48e73f	stochastic majorization-minimization algorithms for large-scale optimization	surrogate functions;majorization minimization;optimization	Majorization-minimization algorithms consist of iteratively minimizing a majorizing surrogate of an objective function. Because of its simplicity and its wide applicability, this principle has been very popular in statistics and in signal processing. In this paper, we intend to make this principle scalable. We introduce a stochastic majorization-minimization scheme which is able to deal with largescale or possibly infinite data sets. When applied to convex optimization problems under suitable assumptions, we show that it achieves an expected convergence rate of O(1/ √ n) after n iterations, and of O(1/n) for strongly convex functions. Equally important, our scheme almost surely converges to stationary points for a large class of non-convex problems. We develop several efficient algorithms based on our framework. First, we propose a new stochastic proximal gradient method, which experimentally matches state-of-the-art solvers for large-scale l1logistic regression. Second, we develop an online DC programming algorithm for non-convex sparse estimation. Finally, we demonstrate the effectiveness of our approach for solving large-scale structured matrix factorization problems.	algorithm;convex function;convex optimization;experiment;iteration;mathematical optimization;optimization problem;proximal gradient method;proximal gradient methods for learning;rate of convergence;scalability;signal processing;sparse matrix;stationary process	Julien Mairal	2013			mathematical optimization;combinatorics;discrete mathematics;computer science;machine learning;mathematics;statistics	ML	25.010569984911182	-33.544501354176845	44757
91026f78039622a1058f62e17fd9c2b246393f9c	deep adaptive update of discriminant kcf for visual tracking		In order to solve the challenges of In-plane/Out-of-plane Rotation (IPR/OPR), fast motion (FM) and occlusion (OCC), a new robust visual tracking framework combining an adaptive template update strategy and tracking validity evaluation, named (AU_DKCF) is presented in this paper. Specifically, the proposed appearance discriminant models are firstly used to determine the tracking validity, and then a new adaptive template update strategy is introduced, which provides an efficient update mechanism to distinguish IPR/OPR from FM and OCC states, and furthermore, a new visual tracking framework AU_DKCF is presented, which combines object detection to distinct FM and OCC states. We implement two versions of the proposed tracker with the representations from both conventional hand-crafted and deep convolution neural networks (CNNs) based features to validate the strong compatibility of the algorithm. Experiment results demonstrate the state-of-the-art performance in tracking accuracy and speed for processing the cases of IPR/OPR, FM and OCC.		Xin Ning;Weijun Li;Weijuan Tian;Xuchi;Dongxiaoli;Zhangliping	2018		10.1007/978-3-030-04224-0_38	convolutional neural network;artificial intelligence;object detection;artificial neural network;convolution;discriminant;eye tracking;pattern recognition;computer science	Vision	31.129355414165232	-50.57408871392522	44762
1c4c5e2a93b18db65337c226c158acdb314699f1	3l fitting of higher degree implicit polynomials	databases;pictorial databases;3l fitting method;numerical stability;complex objects;object recognition;interpolation;data representations;object representations;bayes methods polynomials curve fitting surface fitting interpolation computer vision numerical stability object recognition;bayesian recognizers;bayes methods;cad;affine invariants;surface fitting;bayesian methods;potential field;data representation;polynomials;computer vision;euclidean invariants;polynomials surface fitting computer vision image analysis interpolation bayesian methods stability analysis robustness indexing databases;robot vision;indexing;implicit polynomial 2d curves;complex object structure;indexation;numerical stability analysis;stability analysis;robustness;image analysis;implicit polynomial 3d surfaces;3l fitting;free form shapes;curve fitting;distance transform;free form shapes implicit polynomial 2d curves implicit polynomial 3d surfaces higher degree implicit polynomials 3l fitting object representations data representations computer vision image analysis interpolation euclidean invariants affine invariants bayesian recognizers fitting algorithms stability analysis complex object structure pictorial databases indexing robot vision cad;curves and surfaces;higher degree implicit polynomials;implicit polynomial representation;fitting algorithms	Implicit polynomial 2D curves and 3D surfaces are potentially among the most useful object or data representations for use in computer vision and image analysis. That is because of their interpolation property, Euclidean and aane invariants, and Bayesian recog-nizers. This paper studys and compares various tting algorithms in a uniied framework of stability analysis. It presents a new robust 3L tting method that is repeatable, numerically stable and computationally fast and can be used for high degree implicit polyno-mials to capture complex object structure. With this, we lay down a foundation that enables a technology based on implicit polynomial curves and surfaces for applications to indexing into pictorial databases, robot vision, CAD for free-form shapes, etc.	algorithm;computer vision;computer-aided design;curve fitting;database;image analysis;interpolation;loop invariant;numerical stability;polynomial	Zhibin Lei;Michael M. Blane;David B. Cooper	1996		10.1109/ACV.1996.572044	computer vision;search engine indexing;von neumann stability analysis;discrete mathematics;image analysis;bayesian probability;interpolation;computer science;cognitive neuroscience of visual object recognition;cad;mathematics;geometry;external data representation;distance transform;numerical stability;robustness;polynomial;curve fitting	Vision	52.667697439440005	-51.892269347919814	44773
d23c29b721237ddd3c181a33c54394ed79158182	synchronization of coupled pulse-type hardware neuron models for cpg model	excitatory inhibitory mutual coupling;oscillation pattern;oscillations;hardware neurons rhythm mutual coupling semiconductor device modeling integrated circuit modeling organisms oscillators cmos integrated circuits cmos process;oscillators neural nets;neural nets;coupled oscillator;oscillators;mutual coupling;synchronization phenomena;locomotion rhythm;coupled pulse type hardware neuron model;coupled oscillators system;chip;synchronization;semiconductor device modeling;integrated circuit modeling;mathematical model;neurons;central pattern generator;excitatory inhibitory mutual coupling coupled pulse type hardware neuron model central pattern generator locomotion rhythm synchronization phenomena oscillation pattern coupled oscillators system	It is well known that locomotion rhythms of living organisms are generated by CPG (Central Pattern Generator). In this paper, we discuss the synchronization phenomena and oscillation patterns of the coupled oscillators system using pulse-type hardware neuron models (P-HNMs) for the purpose of constructing the CPG model. It is shown that the plural coupled P-HNMs connected by excitatory-inhibitory mutual coupling can generate various oscillation patterns. Therefore, we construct the CPG model by using the coupled P-HNMs to generate several locomotion rhythms. As a result, we show clearly that the IC chip of CPG model, which can generate the quadruped locomotion patterns, can be constructed by CMOS process.	cmos;central pattern generator;integrated circuit;neuron;printing;synaptic package manager	Ken Saito;Katsutoshi Saeki;Yoshifumi Sekine	2009	2009 International Joint Conference on Neural Networks	10.1109/IJCNN.2009.5179069	computer science;control theory;oscillation;artificial neural network	Robotics	42.56841002430948	-27.17816504164325	44809
5fba529e56026ba842ecff3eca155e2a43c07537	pose estimation of ad-hoc mobile camera networks	smart phones;cameras three dimensional displays estimation transmission line matrix methods calibration mobile communication ad hoc networks;image texture;smart phone ad hoc mobile camera networks overlapping views camera parameter estimation 3d scene camera independent world coordinate system planar surface patch low rank texture relative pose estimation low rank surface patch distributed architecture;mobile ad hoc networks;smart phones cameras image texture mobile ad hoc networks pose estimation;cameras;pose estimation	An algorithm is proposed for the pose estimation of ad-hoc mobile camera networks with overlapping views. The main challenge is to estimate camera parameters with respect to the 3D scene without any specific calibration pattern, hence allowing for a consistent, camera-independent world coordinate system. The only assumption about the scene is that it contains a planar surface patch of a low-rank texture, which is visible in at least two cameras. Such low-rank patterns are quite common in urban environments. The proposed algorithm consists of three main steps: relative pose estimation of the cameras within the network, followed by the localization of the network within the 3D scene using a low-rank surface patch, and finally the estimation of a consistent scale for the whole system. The algorithm follows a distributed architecture, hence the computing power of the participating mobile devices are efficiently used. The performance and robustness of the proposed algorithm have been analyzed on both synthetic and real data. Experimental results confirmed the relevance and applicability of the method.	3d pose estimation;algorithm;camera phone;distributed computing;hoc (programming language);homography (computer vision);image rectification;mobile device;relevance;synthetic data;synthetic intelligence	Zsolt Sánta;Zoltan Kato	2013	2013 International Conference on Digital Image Computing: Techniques and Applications (DICTA)	10.1109/DICTA.2013.6691514	image texture;computer vision;simulation;pose;3d pose estimation;computer science;computer graphics (images)	Robotics	52.90823951315681	-47.39177765429242	44829
271464de8c21585a1205ef79b641c610d4da65c7	multi-object tracking vlsi architecture using image-scan based region growing and feature matching	processing element;image storage;image recognition;object matching;image segmentation;real time tracking;surveillance;very large scale integration;image matching;real time;real time processing;image scan processing;fpga;feature matching;data mining;feature space;20 mhz multiobject tracking vlsi architecture region growing feature matching image segmentation object matching fpga asic embedded memories image scan processing feature extraction;vlsi feature extraction field programmable gate arrays image matching image segmentation integrated circuit design object detection;integrated circuit design;feature extraction;20 mhz;object tracking;vlsi;embedded memories;asic;very large scale integration image segmentation data mining image recognition field programmable gate arrays object detection feature extraction image storage robot vision systems surveillance;field programmable gate arrays;region growing;multiobject tracking;robot vision systems;object detection;vlsi architecture	This paper presents a real-time multi-object tracking architecture based on image segmentation and object matching and its FPGA/ASIC implementation. With image segmentation, we can detect all objects in the image no matter whether they are moving or not. Using image segmentation results of successive frames, we exploit object matching in a simple object feature space for tracking of objects. For both real-time processing and compact implementation, we developed a novel image-scan based region-growing segmentation architecture, which efficiently utilizes high access-bandwidth embedded memories. The structure of image-scan processing element array is at the same time exploited for object feature extraction. Using simple object features, object matching can be realized for finding the most similar object in the previous image frame. The proposed architecture is realized with modern FPGA hardware and is verified to enable real-time tracking of up to 230 objects for QVGA-size video picture at 20MHz clock frequency	application-specific integrated circuit;clock rate;embedded system;feature extraction;feature model;feature vector;field-programmable gate array;graphics display resolution;image segmentation;real-time clock;real-time locating system;region growing;very-large-scale integration;x86 memory segmentation	K. Yamaoka;Takashi Morimoto;Hidekazu Adachi;K. Awane;Tetsushi Koide;Hans Jürgen Mattausch	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1693898	computer vision;computer science;machine learning;segmentation-based object categorization;video tracking;pattern recognition;image segmentation;very-large-scale integration;scale-space segmentation;field-programmable gate array;computer graphics (images)	Robotics	44.364520045496455	-35.39842688215099	44902
93eea66a0ebc4eb057bcd4bdc03b3ca03a1948f7	a multiple hypothesis tracker for multitarget tracking with multiple simultaneous measurements	robot localization;radar tracking;algorithm design and analysis target tracking robot localization radar tracking mathematical model coordinate measuring machines;passive coherent localisation pcl data association multiframe assignment mfa multiple hypothesis tracking mht multitarget tracking over the horizon radar othr;mathematical model;target tracking;coordinate measuring machines;algorithm design and analysis	Typical multitarget tracking systems assume that in every scan there is at most one measurement for each target. In certain other systems such as over-the-horizon radar tracking, the sensor can generate resolvable multiple detections, corresponding to different measurement modes, from the same target. In this paper, we propose a new algorithm called multiple detection multiple hypothesis tracker (MD-MHT) to effectively track multiple targets in such multiple-detection systems. The challenge for this tracker, which follows the multiple hypothesis framework, is to jointly resolve the measurement origin and measurement mode uncertainties. The proposed tracker solves this data association problem via an extension to the multiframe assignment algorithm. Its performance is demonstrated on a simulated over-the-horizon-radar multitarget tracking scenario, which confirms the effectiveness of this algorithm.	algorithm;coherence (physics);correspondence problem;frame language;mhtml;molecular dynamics;multipath propagation;radar;sonar (symantec);sensor;tracking system;mdadm	Thuraiappah Sathyan;Tat-Jun Chin;Sanjeev Arulampalam;David Suter	2013	IEEE Journal of Selected Topics in Signal Processing	10.1109/JSTSP.2013.2258322	algorithm design;computer vision;radar tracker;simulation;tracking system;computer science;mathematical model;control theory;mathematics;statistics	Vision	50.22176615999823	-33.50128708572073	44929
3b63e62654136b2ff0975294d51ba8da632ed1ac	learning based reconstruction of grayscale face image from far-infrared image	video surveillance;face reconstruction;face recognition;canonical correlation analysis;image reconstruction;far infrared image;security	It is important for security surveillance systems to operate continuously for 24 hours. During the night, use of far-infrared cameras is preferable in outdoor situations due to a number of reasons. However, the person in the image is often unrecognizable. This paper attempts to estimate the face from his/her far-infrared image. The estimation is done through two phases, a holistic estimation and a patch based one. In each of these phases, a learning based approach is employed, which learns the relationship between grayscale and far-infrared face images from pairs of the images of a large number of persons. Canonical Correlation Analysis (CCA) is performed to obtain the maximum correlation in the data. Locally Linear Embedding (LLE) is performed to estimate grayscale face image. Three types of experiments were done with this method and evaluated by PSNR. These experiments show a good result in estimating face image whose face images of different expressions were included in training data.	color image;displacement mapping;experiment;grayscale;ground truth;holism;nonlinear dimensionality reduction;peak signal-to-noise ratio;pixel;refinement (computing)	Brahmastro Kresnaraman;Yoshito Mekada;Tomokazu Takahashi;Hiroshi Murase	2013	2013 2nd IAPR Asian Conference on Pattern Recognition	10.1109/ACPR.2013.74	computer vision;u-matrix;computer science;machine learning;pattern recognition	Vision	35.188607171113475	-50.64289276588003	44954
4a7316b5d9e26d3276636fd46abe5f0ccb483515	scalable kernel clustering: approximate kernel k-means		Kernel-based clustering algorithms have the ability to capture the non-linear structure in real world data. Among various kernel-based clustering algorithms, kernel k -means has gained popularity due to its simple iterative nature and ease of implementation. However, its run-time complexity and memory footprint increase quadratically in terms of the size of the data set, and hence, large data sets cannot be clustered efficiently. In this paper, we propose an approximation scheme based on randomization, called the Approximate Kernel k-means. We approximate the cluster centers using the kernel similarity between a few sampled points and all the points in the data set. We show that the proposed method achieves better clustering performance than the traditional low rank kernel approximation based clustering schemes. We also demonstrate that it’s running time and memory requirements are significantly lower than those of kernel k -means, with only a small reduction in the clustering quality on several public domain large data sets. We then employ ensemble clustering techniques to further enhance the performance of our algorithm.	approximation algorithm;approximation error;cluster analysis;column (database);computational complexity theory;computer cluster;data point;hilbert space;iterative method;k-means clustering;kernel (operating system);kilobyte;memory footprint;monte carlo method;nonlinear system;random permutation;randomness;requirement;sampling (signal processing);scalability;semiconductor industry;time complexity	Radha Chitta;Rong Jin;Timothy C. Havens;Anil K. Jain	2014	CoRR		correlation clustering;constrained clustering;kernel method;mathematical optimization;data stream clustering;string kernel;kernel embedding of distributions;radial basis function kernel;fuzzy clustering;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;mathematics;cluster analysis;tree kernel;variable kernel density estimation;polynomial kernel;kernel smoother;clustering high-dimensional data	ML	24.902438555076856	-36.72089070573653	44962
fdb5ee34d62065a1b7209e4d4215e9bcbb89ea7f	illumination insensitive robot self-localization using panoramic eigenspaces	robust method;panoramic image	We propose to use a robust method for appearance-based matching that has been shown to be insensitive to illumination and occlusion for robot self-localization. The drawback of this method is that it relies on panoramic images taken in one certain orientation, restricts the heading of the robot throughout navigation or needs additional sensors for orientation, e.g. a compass. To avoid these problems we propose a combination of the appearance-based method with odometry data. We demonstrate the robustness of the proposed self-localization against changes in illumination by experimental results obtained in the RoboCup Middle-Size scenario.	automation;btrieve;course (navigation);experiment;odometry;particle filter;robot;sensor;simultaneous localization and mapping;speedup	Gerald Steinbauer;Horst Bischof	2004		10.1007/978-3-540-32256-6_7	computer vision;simulation;computer science;computer graphics (images)	Robotics	52.695632373561594	-39.835948656139756	44983
4b1abc5b52db2ba854101b137d1fe3aed9e21274	the role of dictionary learning on sparse representation-based classification	fisher discriminative dictionary learning;metaface learning;sparse modeling representative selection;sparse representation based classification	This paper analyzes the role of dictionary selection in Sparse Representation-based Classification (SRC). While SRC introduces interesting results in the field of classification, its performance is highly limited by the number of training samples to form the classification matrix. Different studies addressed this issue by using a more compact representation of the training data in order to achieve higher classification speed and accuracy. Representative selection methods which are analyzed in this paper include Metaface dictionary learning, Fisher Discriminative Dictionary Learning (FDDL), Sparse Modeling Representative Selection (SMRS), and random selection of the training samples. The first two methods build their own dictionaries via an optimization process while the other two methods select the representatives directly from the original training samples. These methods, along with the original method which uses all training samples to form the classification matrix, were examined on two face datasets and one digit dataset. The role of feature extraction was also studied using two dimensionality reduction methods, down-sampling and random projection. The results show that the FDDL method leads to the best classification accuracy followed by the SMRS method as the second best. On the other hand, the SMRS method requires a much smaller learning time which makes it more appropriate for dynamic situations where the dictionary is regularly updated with new samples. The accuracy of the Metaface dictionary learning method was specifically less than the other two methods. As expected, using all the training samples as the dictionary resulted in the best recognition rates in all the datasets but the classification times for this approach were far larger than the required time using any of the three dictionary learning methods.	dictionary;dimensionality reduction;feature extraction;machine learning;mathematical optimization;random projection;sample rate conversion;sampling (signal processing);sparse approximation;sparse matrix;statistical classification	Soheil Shafiee;Farhad Kamangar;Vassilis Athitsos;Junzhou Huang	2013		10.1145/2504335.2504385	speech recognition;k-svd;computer science;machine learning;pattern recognition	Vision	26.130062992376335	-42.299403112399354	45023
31f6b1e159cdf5af9bbd0491b4731c0f4a5dd635	spatio-temporal consistency to detect and segment carried objects				Farnoosh Ghadiri;Robert Bergevin;Guillaume-Alexandre Bilodeau	2017			computer science;computer vision;artificial intelligence;pattern recognition	HCI	40.48904527209002	-47.733205103370565	45041
1761e10eca5b90fb1f62d83d3a2a1ff1ddf525dc	dependable dense stereo matching by both two-layer recurrent process and chaining search	reliability;image segmentation;psnr;gaussian processes;image matching;image texture;computational modeling;robot vision;robots;stereo image processing;stereo vision;gaussian noisy images dependable stereo matching algorithm two layer recurrent process chaining search disparity computation occluded regions texture less regions robotics applications intensity difference image pair pixel pixel level information region level information middlebury benchmark;robots stereo vision reliability image segmentation psnr educational institutions computational modeling;stereo image processing gaussian processes image matching image texture robot vision	Disparity computation in occluded or texture-less regions is considered to be a fundamental issue in dense stereo matching, but there is another practical issue that must be resolved before it can be used effectively in various robotics applications. This issue is the problem of intensity difference between corresponding pixels of an image pair. To tackle such problems, we present a dependable stereo matching algorithm using two-layer recurrent process and chaining search. Two-layer process integrates pixel and region-levels information through recurrent interaction. To estimate the precise disparities in occluded regions, reliable disparities in non-occluded region are propagated to occluded regions by the proposed chaining search. To test our algorithm, it was compared with two outstanding algorithms in Middlebury benchmark using Gaussian noisy images. The results validated the effectiveness of our approach.	algorithm;benchmark (computing);binocular disparity;coexist (image);computation;computer stereo vision;dependability;iteration;meltwater entrepreneurial school of technology;pixel;recurrent neural network;robotics;search algorithm	Sehyung Lee;Youngbin Park;Il Hong Suh	2012	2012 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2012.6386179	computer stereo vision;robot;image texture;stereo cameras;computer vision;simulation;template matching;peak signal-to-noise ratio;computer science;stereopsis;machine learning;reliability;gaussian process;image segmentation;computational model	Robotics	42.32826330310079	-51.0726530089765	45124
1796c3da43b9dbd1f7ef9334b3d65d96bfb481c5	image recognition with occlusions	image recognition;cost function;function approximation;global optimization	We study the problem of how to detect interesting objects appeared in a given image I Our approach is to treat it as a function approximation problem based on an over redundant basis Since the ba sis a library of image templates is over redundant there are in nitely many ways to decompose I To select the best decomposition we rst propose a global optimization procedure that considers a concave cost function derived from a weighted L norm with p This concave cost function selects as few coe cients as possible producing a sparse representation of the image and handle occlusions However it contains multiple local minima We identify all local minima so that a global optimization is possible by visiting all of them Secondly because the number of local minima grows exponentially with the number of templates we investigate a greedy L Matching Pursuit strategy	computer vision;concave function;global optimization;greedy algorithm;loss function;matching pursuit;mathematical optimization;maxima and minima;sparse approximation;sparse matrix	Tyng-Luh Liu;Michael J. Donahue;Davi Geiger;Robert A. Hummel	1996		10.1007/BFb0015566	computer vision;function approximation;computer science;machine learning;pattern recognition;global optimization	Vision	28.0544871687589	-41.65616382034073	45126
f8bd2a135ac508a3aea6d403ba45c957a6451ed7	a vlsi system architecture for optical flow computation	field programmable gate array;lucas kanade algorithm;application specific hardware;video signal processing;data flow graphs;lucas kanade;high speed optical techniques;real time processing;optical computing;video sequences;fpga;scene interpretation systems;kingston dimm ddr memory module;xupv2p fpga board;vlsi cameras data flow graphs field programmable gate arrays image sequences video signal processing;general purpose processor;data flow graph;optical imaging;pixel;xilinx virtex ii field programmable gate arrays;vlsi;ip networks;optical flow;hardware implementations;pixel plus 2 0 mega pixel camera;vlsi system architecture;field programmable gate arrays;optical computing very large scale integration computer architecture image motion analysis high speed optical techniques field programmable gate arrays video sequences cameras hardware layout;system architecture;high speed;hardware implementation;optical flow computation;cameras;data flow graphs vlsi system architecture optical flow computation video sequences camera scene interpretation systems application specific hardware lucas kanade algorithm xilinx virtex ii field programmable gate arrays fpga kingston dimm ddr memory module pixel plus 2 0 mega pixel camera xupv2p fpga board hardware implementations;camera;hardware;image sequences	The computation of optical flow in video sequences is a challenging task in most camera based scene interpretation systems. In the past, most optical flow computation algorithms has been either implemented in software running on general purpose processors or designed as an application specific hardware. However, these implementations either cannot support real-time processing requirements or result in excessive inaccuracies in the computed velocity values. In this work, we propose a efficient VLSI system architecture for computing the optical flow in video sequences using the Lucas-Kanade (L-K) algorithm. The algorithm is converted into high speed RTL implementation by exploiting the inherent paralellism in the data flow graph. Clever pipelining strategies has been used throughout the design to further improve the speedup of velocity computation. We have mapped the RTL design on a Xilinx Virtex II Field Programmable Gate Arrays (FPGA) supported with Kingston DIMM DDR memory module, and a Pixel-Plus 2.0 Mega-pixel camera on the XUPV2P FPGA board. Experimental results of our proposed design showed significant improvements in accuracy with a speedup of five times when compared with other recent hardware implementations.	algorithm;central processing unit;computation;dimm;dataflow;field-programmable gate array;kanade–lucas–tomasi feature tracker;memory module;optical flow;pipeline (computing);pixel;pixelplus;real-time clock;register-transfer level;requirement;speedup;systems architecture;velocity (software development);very-large-scale integration;virtex (fpga)	Koustav Bhattacharya;Venkataraman Mahalingam;N. Ranganathan	2009	2009 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2009.5117759	embedded system;parallel computing;computer hardware;computer science;operating system;field-programmable gate array;systems architecture	EDA	44.24920386647842	-35.29506784373511	45136
954c2a33b1a479ba60b433c9d49069c945fad392	using adaptive background subtraction into a multi-level model for traffic surveillance	adaptive background detection;video based traffic monitoring;traffic linguistic description;vehicle counting and classification;multi level image analysis	Interest in the application of computer vision techniques to automatic video-based analysis of traffic is high at present. This is due in part to the capabilities of video sensors, as well as to social demands for traffic safety. In general, these systems are cheaper and less disruptive than other kinds of devices like loop detectors for traffic monitoring. Automatic traffic surveillance is, however, still a challenging problem when we consider many of the practical difficulties involved i.e. limited number of cameras and positions of these with respect to the scene, variable illumination and weather conditions, intrinsic complexity of analyzed traffic events, need for a real-time frame rate processing, among others. In this paper, we propose a multi-level framework for automatic analysis of complex traffic videos which present different kind of variations. The accurate and efficient extraction of relevant scene information from the video frames is performed in a hierarchical bottom-up form using the system presented. First of all, foreground moving pixels are detected in each frame using a proposed method of adaptive background subtraction. After that, these pixels are grouped into blobs if they share some common properties. Blobs detected in predefined scene entry regions are identified as vehicles and these are tracked along the controlled road area. At the upper level, some traffic monitoring statistics and also related linguistic reports on the evolution of traffic in the scene are generated periodically. Experimental results on the adaptive background method proposed, as well as regarding its integration in the multi-level traffic analysis system, are very satisfactory for the traffic videos analyzed.	background subtraction;multilevel model	Ángel Sánchez;Éldman de Oliveira Nunes;Aura Conci	2012	Integrated Computer-Aided Engineering	10.3233/ICA-2012-0402	computer vision;simulation;background subtraction;floating car data;multimedia	EDA	40.976156323479564	-46.3383407629772	45197
62b098de9fe7807f3346a9abc64706c3df2701d2	special issue: biomechanical and parametric modeling of human anatomy				Sidney S. Fels;Ian Stavness	2014	CMBBE: Imaging & Visualization	10.1080/21681163.2014.948124	simulation;engineering;biological engineering;mechanical engineering	Visualization	39.72575195340699	-38.8986990642728	45269
97b912d99d70fd8b7f684e57da300b595a298970	real-time incipient fault detection for electrical traction systems of crh2		Abstract Electrical traction systems in a high-speed train are the core parts to provide traction force for the whole train. Due to performance degradation of electronic components and the prolonged operation under variously complicated operating environments, incipient faults will inevitably happen and will evolve into faults or failures if they are not successfully detected. Currently, the univariate control charts are used to monitor electrical traction systems of high-speed trains. However, this primitive solution is unable to deal with incipient faults with satisfactory performance. In this paper, a Kullback–Leibler divergence (KLD) and independent component analysis (ICA)-based method is proposed to perform incipient fault detection (FD) in electrical traction systems. Compared with the existing ICA-based methods, the proposed strategy is more sensitive to incipient faults; meanwhile it has low computational load because estimating the probability density functions (PDFs) of the derived independent components and the residuals is avoided. On the experimental platform of the traction system for China Railway High-speed 2-type (CRH2) trains, three typical incipient faults are successfully injected, and the proposed method is successful in detecting these incipient faults.	fault detection and isolation;real-time transcription;traction teampage	Hongtian Chen;Zehui Mao;Ningyun Lu;Wen Chen	2018	Neurocomputing	10.1016/j.neucom.2018.04.058	probability density function;artificial intelligence;train;tractive force;independent component analysis;mathematics;fault detection and isolation;traction (orthopedics);pattern recognition;control engineering;univariate;electronic component	Robotics	36.66822653584004	-30.27517402948604	45314
647446d92229b3c1ca90a6bedc4b5f37894519c1	performance analysis of sparsity-based parameter estimation		Since the advent of the $\ell _1$ regularized least squares method (LASSO), a new line of research has emerged, which has been geared toward the application of the LASSO to parameter estimation problems. Recent years witnessed a considerable progress in this area. The notorious difficulty with discretization has been settled in the recent literature, and an entirely continuous estimation method is now available. However, an adequate analysis of this approach lacks in the current literature. This paper provides a novel analysis of the LASSO as an estimator of continuous parameters. This analysis is different from the previous ones in that our parameters of interest are associated with the support of the LASSO solution. In other words, our analysis characterizes the error in the parameterization of the support. We provide a novel framework for our analysis by studying nearly ideal sparse solutions. In this framework, we quantify the error in the high signal-to-noise ratio regime. As the result depends on the choice of the regularization parameter, our analysis also provides a new insight into the problem of selecting the regularization parameter. Without loss of generality, the results are expressed in the context of direction of arrival estimation problem.	direction of arrival;discretization;estimation theory;lasso;least squares;matrix regularization;profiling (computer programming);signal-to-noise ratio;sparse matrix	Ashkan Panahi;Mats Viberg	2017	IEEE Transactions on Signal Processing	10.1109/TSP.2017.2755602	econometrics;mathematics;without loss of generality;mathematical optimization;lasso (statistics);estimator;discretization;regularization (mathematics);elastic net regularization;direction of arrival;estimation theory	ML	27.162931552713374	-32.79713078310905	45345
7b71fe7eaa3d612fd489b4d6a5c341753ebc9c72	an incremental scheme for dictionary-based compressive slam	databases;data compression;visualization;trajectory;compact representation;pattern matching;visual search;compression distance dictionary based compressive slam large size pointset map compact representation mapper robot slam application incremental map compressor ransac map matching scheme compact projection visual search data compactness;robots;dictionaries;simultaneous localization and mapping;slam robots data compression pattern matching robots;dictionaries trajectory simultaneous localization and mapping visualization databases;slam robots	Obtaining a compact representation of a large-size pointset map built by mapper robots is a critical issue for recent SLAM applications. This “map compression” problem is explored from a novel perspective of dictionary-based map compression in the paper. The primary contribution of the paper is proposal of an incremental scheme for simultaneous mapping and map-compression applications. An incremental map compressor is presented by employing a modified RANSAC map-matching scheme as well as the compact projection visual search. Experiments show promising results in terms of compression speed, compactness of data and structure, as well as an application to the compression distance.	dictionary;simultaneous localization and mapping	Tomomi Nagasaka;Kanji Tanaka	2011		10.1109/IROS.2011.6094729	data compression;robot;computer vision;visualization;visual search;computer science;artificial intelligence;trajectory;theoretical computer science;machine learning;pattern matching;mathematics;simultaneous localization and mapping	Vision	39.928057895299865	-40.39817929267563	45413
cf2c750f8742efaaf719de05d9664602a5324972	nonnegative embeddings and projections for dimensionality reduction and information visualization	eigenvalues and eigenfunctions;principal component analysis data reduction data visualisation eigenvalues and eigenfunctions embedded systems;locality preserving projection;relational data;manifolds;optimization algorithm design and analysis data visualization laplace equations cognition yttrium manifolds;nonnegative laplacian eigenmaps;nonnegative locally linear embedding;information visualization;manifold learning;nonpp;nolpp;data visualisation;embedded systems;laplace equations;data visualization dimensionality reduction information visualization relational data low dimensionality nonnegative embedding nonnegative projections metric multidimensional scaling algorithm mms algorithm nonnegative laplacian eigenmaps nonnegative locally linear embedding nonnegative principal component analysis npca nonnegative orthogonal neighbourhood preserving projections nonnegative orthogonal locality preserving projections nonpp nolpp;dimensionality reduction;yttrium;nonnegative matrix factorization;nonnegative projections;mms algorithm;principal component analysis;cognition;multidimensional scaling;data visualization;low dimensionality nonnegative embedding;optimization;metric multidimensional scaling algorithm;data reduction;npca;manifold learning dimensionality reduction nonnegative matrix factorization;dimensional reduction;nonnegative orthogonal neighbourhood preserving projections;algorithm design and analysis;local linear embedding;nonnegative principal component analysis;nonnegative orthogonal locality preserving projections	In this paper, we propose novel algorithms for low dimensionality nonnegative embedding of vectorial and/or relational data, as well as nonnegative projections for dimensionality reduction. We start by introducing a novel algorithm for Metric Multidimensional Scaling (MMS). We propose algorithms for Nonnegative Locally Linear Embedding (NLLE) and Nonnegative Laplacian Eigenmaps (NLE). By reformulating the problem of MMS, NLLE and NLE for finding projections we propose algorithms for Nonnegative Principal Component Analysis (NPCA), for Nonnegative Orthogonal Neighbourhood Preserving Projections (NONPP) and Nonnegative Orthogonal Locality Preserving Projections (NOLPP). We demonstrate some first preliminary results of the proposed methods in data visualization.	algorithm;data visualization;facial recognition system;format-preserving encryption;information visualization;laplacian matrix;locality of reference;measure-preserving dynamical system;multidimensional scaling;nonlinear dimensionality reduction;principal component analysis	Stefanos P. Zafeiriou;Nikolaos A. Laskaris	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.183	algorithm design;mathematical optimization;combinatorics;data reduction;discrete mathematics;cognition;multidimensional scaling;manifold;relational database;computer science;yttrium;machine learning;mathematics;nonlinear dimensionality reduction;non-negative matrix factorization;data visualization;dimensionality reduction;principal component analysis	Vision	26.36727552319873	-40.935790271495826	45477
9f4573ef05cfd788cc69558fd2e2e759df1c5ef2	learning and interpreting complex distributions in empirical data		To fit empirical data distributions and then interpret them in a generative way is a common research paradigm to understand the structure and dynamics underlying the data in various disciplines. However, previous works mainly attempt to fit or interpret empirical data distributions in a case-by-case way. Faced with complex data distributions in the real world, can we fit and interpret them by a unified but parsimonious parametric model? In this paper, we view the complex empirical data as being generated by a dynamic system which takes uniform randomness as input. By modeling the generative dynamics of data, we showcase a four-parameter dynamic model together with inference and simulation algorithms, which is able to fit and generate a family of distributions, ranging from Gaussian, Exponential, Power Law, Stretched Exponential (Weibull), to their complex variants with multi-scale complexities. Rather than a black box, our model can be interpreted by a unified differential equation, which captures the underlying generative dynamics. More powerful models can be constructed by our framework in a principled way. We validate our model by various synthetic datasets. We then apply our model to $16$ real-world datasets from different disciplines. We show the systematic biases of fitting these datasets by the most widely used methods and show the superiority of our model. In short, our model potentially provides a framework to fit complex distributions in empirical data, and more importantly, to understand their generative mechanisms.	algorithm;black box;cast tool;computer science;cross-sectional data;dynamical system;fits;generative model;han unification;mathematical model;occam's razor;open-source software;parametric model;programming paradigm;random graph;randomness;simulation;synthetic intelligence	Chengxi Zang;Peng Cui;Wenwu Zhu	2018		10.1145/3219819.3220073	artificial intelligence;parametric model;randomness;computer science;machine learning;generative grammar;inference;ranging;power law;heavy-tailed distribution;complex data type	ML	25.094910316360604	-29.963155437794317	45480
413c98ff2d95b5b945825268fd8ffdc65880f715	human pose estimation in videos	performance improvement human pose sequence estimation unconstrained videos unified two stage tree based optimization problem spatial constraints temporal constraints video frames abstraction association intra frame body part constraints inter frame body part constraints computational complexity polynomial time solution abstract body part tree based body part structure optimal tracklet generation spatiotemporal constraints;video signal processing computational complexity image sequences optimisation performance evaluation pose estimation spatiotemporal phenomena trees mathematics;videos optimization computational modeling computer vision computational complexity spatiotemporal phenomena	In this paper, we present a method to estimate a sequence of human poses in unconstrained videos. In contrast to the commonly employed graph optimization framework, which is NP-hard and needs approximate solutions, we formulate this problem into a unified two stage tree-based optimization problem for which an efficient and exact solution exists. Although the proposed method finds an exact solution, it does not sacrifice the ability to model the spatial and temporal constraints between body parts in the video frames, indeed it even models the symmetric parts better than the existing methods. The proposed method is based on two main ideas: 'Abstraction' and 'Association' to enforce the intra-and inter-frame body part constraints respectively without inducing extra computational complexity to the polynomial time solution. Using the idea of 'Abstraction', a new concept of 'abstract body part' is introduced to model not only the tree based body part structure similar to existing methods, but also extra constraints between symmetric parts. Using the idea of 'Association', the optimal tracklets are generated for each abstract body part, in order to enforce the spatiotemporal constraints between body parts in adjacent frames. Finally, a sequence of the best poses is inferred from the abstract body part tracklets through the tree-based optimization. We evaluated the proposed method on three publicly available video based human pose estimation datasets, and obtained dramatically improved performance compared to the state-of-the-art methods.	3d pose estimation;approximation algorithm;computational complexity theory;computer vision;cycle (graph theory);mathematical optimization;np-hardness;optimization problem;time complexity	Dong Zhang;Mubarak Shah	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.233	computer vision;mathematical optimization;theoretical computer science;machine learning;mathematics;geometry	Vision	50.163881408881444	-49.493637785458276	45494
cf93f7ef47f17b28435b59ce29df676b4aa6550c	rubreathing: non-contact real time respiratory rate monitoring system	cyborg insect networks;swram robotics;topological mapping	The respiration rate of a person provides critical information about their well-being. Conventionally, contact sensing is used for breathing monitoring; however, it is expensive, uncomfortable, and immobile. In-home non-contact breathing monitoring is now possible via Doppler radar and motion capture video sensors, yet these technologies are limited in mobility, among other limitations. When monitoring a patient who is free to move around his or her home, it is dificult to scale current non-contact sensors to cover the large area. Our RUBreathing sensor system uses RF received signal strength (RSS) in a network to estimate breathing rate in real-time with high accuracy over a wide area. In this demonstration, we show the sensor continuously estimating a patient's respiration rate from non-contact RSS measurements between wireless devices.	motion capture;rss;radio frequency;real-time locating system;sensor	Anh Luong;Spencer Madsen;Michael Empey;Neal Patwari	2015		10.1145/2737095.2737133	simulation;telecommunications	Mobile	50.58631453728717	-31.995508984578688	45497
a0d81d1af98c37011d73563b9bb18475e2b9948d	learning temporal point processes via reinforcement learning		Social goods, such as healthcare, smart city, and information networks, often produce ordered event data in continuous time. The generative processes of these event data can be very complex, requiring flexible models to capture their dynamics. Temporal point processes offer an elegant framework for modeling event data without discretizing the time. However, the existing maximum-likelihood-estimation (MLE) learning paradigm requires hand-crafting the intensity function beforehand and cannot directly monitor the goodness-of-fit of the estimated model in the process of training. To alleviate the risk of model-misspecification in MLE, we propose to generate samples from the generative model and monitor the quality of the samples in the process of training until the samples and the real data are indistinguishable. We take inspiration from reinforcement learning (RL) and treat the generation of each event as the action taken by a stochastic policy. We parameterize the policy as a flexible recurrent neural network and gradually improve the policy to mimic the observed event distribution. Since the reward function is unknown in this setting, we uncover an analytic and nonparametric form of the reward function using an inverse reinforcement learning formulation. This new RL framework allows us to derive an efficient policy gradient algorithm for learning flexible point process models, and we show that it performs well in both synthetic and real data.	algorithm;artificial neural network;discrepancy function;generative model;gradient;point process;programming paradigm;recurrent neural network;reinforcement learning;smart city;synthetic intelligence	Shuang Li;Shuai Xiao;Shixiang Zhu;Nan Du;Y. Xie;Liyuan Song	2018			the internet;artificial intelligence;generative grammar;machine learning;computer science;reinforcement learning;point process;reproducing kernel hilbert space;recurrent neural network	ML	24.85951136660954	-30.04065491275591	45539
3ade71a16d5cdbaf9b70cdbbeffeb0dd4006fa71	a framework for real-time path planning in changing environments	text;path planning;random sampling;real time;configuration space;motion planning;importance sampling;engineering electronics and electrical	We present a new method for generating collision-free paths for robots operating in changing environments. Our approach is closely related to recent probabilistic roadmap approaches. These planners use preprocessing and query stages, and are aimed at planning many times in the same environment. In contrast, our preprocessing stage creates a representation of the configuration space that can be easily modified in real time to account for changes in the environment, thus facilitating real-time planning. As with previous approaches, we begin by constructing a graph that represents a roadmap in the configuration space, but we do not construct this graph for a specific workspace. Instead, we construct the graph for an obstacle-free workspace, and encode the mapping from workspace cells to nodes and arcs in the graph. When the environment changes, this mapping is used to make the appropriate modifications to the graph, and plans can be generated by searching the modified graph. In this paper, we first discuss the construction of the roadmap, including how random samples of the configuration space are generated using an importance sampling approach and how these samples are connected to form the roadmap. We then discuss the mapping from the workspace to the configuration space roadmap, explaining how the mapping is generated and how it can be encoded efficiently using compression schemes that exploit redundancy in the mapping. We then introduce quantitative robustness measures and show how these can be used to enhance the robustness of the roadmap to changes in the environment. Finally, we evaluate an implementation of our approach for serial-link manipulators with up to 20 joints. KEY WORDS—probabilistic roadmaps, motion planning	algorithm;authorization;connected component (graph theory);cut (graph theory);data compression;design web format;dreamwidth;encode;importance sampling;line level;minimum cut;motion planning;naivety;overhead (computing);plan;preprocessor;probabilistic roadmap;real-time clock;real-time path planning;real-time transcription;region growing;robot;robustness (computer science);sampling (signal processing);serial communication;tree structure;workspace	Peter Leven;Seth Hutchinson	2002	I. J. Robotics Res.	10.1177/0278364902021012001	computer vision;simulation;any-angle path planning;computer science;artificial intelligence;theoretical computer science;motion planning;quantum mechanics	Robotics	51.999131983672484	-24.564412762069313	45540
06810107d2ccfa70a6fed21354f1aa1b2dc76360	bounded-svd: a matrix factorization method with bound constraints for recommender systems	matrix factorization;bound constraints;stochastic gradient descent;collaborative filtering;recommender systems	In this paper, we present a new matrix factorization method for recommender system problems, named bounded-SVD, which utilizes the constraint that all the ratings in the rating matrix are bounded within a pre-determined range. In our proposed method, the bound constraints are included in the objective function so that both the task of minimizing errors and the constraints are taken into account during the optimization process. For evaluation, we compare the performance of bounded-SVD with an existing method, called Bounded Matrix Factorization (BMF), which also uses the bound constraints on the ratings. The results on major real-world recommender system datasets show that our method outperforms BMF in almost cases and it is also faster and more simple to implement than BMF. Moreover, the way the bound constraints are integrated in bounded-SVD can also be applied to other optimization problems with bound constraints as well.	mathematical optimization;optimization problem;recommender system;singular value decomposition	Bang Hai Le;Kazuki Mori;Ruck Thawonmas	2015	2015 International Conference on Emerging Information Technology and Engineering Solutions	10.2197/ipsjjip.24.314	mathematical optimization;computer science;theoretical computer science;collaborative filtering;machine learning;stochastic gradient descent;matrix decomposition;non-negative matrix factorization;recommender system	DB	25.799219525880513	-37.67727687732294	45606
bedd73a287dc0e445a9c5e19636b91a69fbfa4fc	dynamic sensing in a ping-pong playing robot	systeme commande;sistema control;optimisation;recreation facility;sistema experto;representation tridimensionnelle;trajectoire;theorie dynamique;expert systems;caracteristique temporelle;optimizacion;teoria dinamica;robots computer vision dynamics expert systems kinematics;equipement loisir;dynamic system;kinematics;time curve;computer vision;dynamic theory;systeme dynamique;intelligent robot;captador medida;transduction;dynamic environment;temporal characteristics computer vision expert systems kinematics dynamics dynamic sensing ping pong playing robot sensing technologies dynamic environments;control system;measurement sensor;capteur mesure;robot sensing systems sensor systems sensor phenomena and characterization robot control motion planning motion control trajectory control systems manipulators real time systems;trajectory;dynamics;continuous optimization;equipamiento distraccion;robots;caracteristica temporal;three dimensional representation;optimization;trayectoria;systeme expert;robot inteligente;transduccion;sistema dinamico;vision;representacion tridimensional;robot intelligent;expert system	The application of sensing technologies to an environment that changes at rates comparable to the sensing and actuation rates of a robot system is considered. In such dynamic environments, the temporal characteristics of the sensor must be taken into account. Additionally, the processing system must be able to integrate the stream of sensor data into its task plan, compensating for changed sensor data and continuously optimizing the plan. The author examines these issues in the context of a functioning robot system that plays ping-pong. >	robot	R. L. Andersson	1989	IEEE Trans. Robotics and Automation	10.1109/70.88095	robot;control engineering;transduction;vision;computer vision;dynamics;kinematics;simulation;computer science;engineering;artificial intelligence;trajectory;dynamical system;expert system	Robotics	52.20670040230599	-29.553076693167153	45622
e4213f4434d207a3cf92776559a23dcdd322685c	classification and clustering perspective towards spectral unmxing	support vector machines brightness fuzzy set theory image classification image resolution pattern clustering;standards;support vector machines;training;support vector machines training clustering algorithms hyperspectral sensors standards estimation;fcm spectral unmixing svm;hyperspectral sensors;estimation;clustering algorithms;classification pixel decomposition subpixel information spectral unmixing support vector machine based unmixing technique endmember spectral variability endmember extraction optimal performance fcm based method brightness variation fuzziness parameter selection initial seed selection clustering perspective	Spectral unmixing techniques decompose the pixels into constituent fractions in order to extract the subpixel information. This study reviews spectral unmixing techniques from a perspective different from earlier approaches in that the problem is studied from a classification as well as clustering perspective. In this research, we focus on addressing some core issues of spectral unmixing such as endmember variability, requirement of pure endmember values, and initialization sensitivity modelling. We propose a Support Vector Machine (SVM) based unmixing technique that incorporates endmember spectral variability. The method uses endmember extraction techniques to give optimal performance even in the absence of training samples. Further, our study presents an alternation of FCM based method for incorporating spectral variability, and the approach is found to be resilient to the brightness variation. An automatic approach for fuzziness parameter selection is also introduced. The sensitivity of FCM towards endmember initialization has been considerably reduced by optimizing the initial seed selection. The proposed approaches have been analyzed over various standard datasets.	alternation (formal language theory);cluster analysis;fuzzy cognitive map;pixel;spatial variability;spectral method;stellar classification;support vector machine	P. V. Arun;Krishna Mohan Buddhiraju	2016	2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2016.7730605	support vector machine;computer vision;estimation;computer science;hyperspectral imaging;machine learning;pattern recognition;cluster analysis	Vision	30.9546962920989	-43.630642498195556	45642
27fb73cd9790af5c9130c0184e7f700d3bc0f422	minimax optimal procedures for locally private estimation		Working under a model of privacy in which data remains private even from the statistician, we study the tradeoff between privacy guarantees and the risk of the resulting statistical estimators. We develop private versions of classical information-theoretic bounds, in particular those due to Le Cam, Fano, and Assouad. These inequalities allow for a precise characterization of statistical rates under local privacy constraints and the development of provably (minimax) optimal estimation procedures. We provide a treatment of several canonical families of problems: mean estimation and median estimation, multinomial probability estimation, and nonparametric density estimation. For all of these families, we provide lower and upper bounds that match up to constant factors, and exhibit new (optimal) privacy-preserving mechanisms and computationally efficient estimators that achieve the bounds. Additionally, we present a variety of experimental results for estimation problems involving sensitive data, including salaries, censored blog posts and articles, and drug abuse; these experiments demonstrate the importance of deriving optimal procedures.	algorithmic efficiency;blog;experiment;information theory;minimax;multinomial logistic regression;optimal design;physical information;privacy	John C. Duchi;Martin J. Wainwright;Michael I. Jordan	2016	CoRR		econometrics;mathematical optimization;mathematics;statistics	ML	25.834975677998752	-26.147286358726372	45645
076f011e09b9788c022c0578ab8dd0bb3fdf8908	what is the spatial extent of an object?	histograms;mean average precision;object recognition;sampling methods;spatial context;support vector machines;image retrieval;logic;informatics;bag of words;image segmentation;pascal;intelligent systems;kernel;upper bound	This paper discusses the question: Can we improve the recognition of objects by using their spatial context? We start from Bag-of-Words models and use the Pascal 2007 dataset. We use the rough object bounding boxes that come with this dataset to investigate the fundamental gain context can bring. Our main contributions are: (I) The result of Zhang et al. in CVPR07 that context is superfluous derived from the Pascal 2005 data set of 4 classes does not generalize to this dataset. For our larger and more realistic dataset context is important indeed. (II) Using the rough bounding box to limit or extend the scope of an object during both training and testing, we find that the spatial extent of an object is determined by its category: (a) well-defined, rigid objects have the object itself as the preferred spatial extent. (b) Non-rigid objects have an unbounded spatial extent : all spatial extents produce equally good results. (c) Objects primarily categorised based on their function have the whole image as their spatial extent. Finally, (III) using the rough bounding box to treat object and context separately, we find that the upper bound of improvement is 26% (12% absolute) in terms of mean average precision, and this bound is likely to be higher if the localisation is done using segmentation. It is concluded that object localisation, if done sufficiently precise, helps considerably in the recognition of objects for the Pascal 2007 dataset.	bag-of-words model;computer monitor;emoticon;experiment;information retrieval;mean squared error;minimum bounding box;spatial variability;television	Jasper R. R. Uijlings;Arnold W. M. Smeulders;Remko J. H. Scha	2009	2009 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPRW.2009.5206663		Vision	33.25550450926199	-51.52994859396683	45771
828dc431ce758dac61a90bcbfabc0f4b718fb96f	fast and robust object tracking via probability continuous outlier model	observation likelihood function robust object tracking probability continuous outlier model visual tracking method linear representation pcom principle component analysis guassian noise markov random field model outlier free least square standard max flow min cut step;linear programming mathematical model robustness visualization principal component analysis tracking laplace equations;probability model object tracking linear representation outlier handling;probability gaussian noise image representation least squares approximations markov processes minimax techniques object tracking principal component analysis;visualization;laplace equations;principal component analysis;linear programming;mathematical model;robustness;tracking	This paper presents a novel visual tracking method based on linear representation. First, we present a novel probability continuous outlier model (PCOM) to depict the continuous outliers within the linear representation model. In the proposed model, the element of the noisy observation sample can be either represented by a principle component analysis subspace with small Guassian noise or treated as an arbitrary value with a uniform prior, in which a simple Markov random field model is adopted to exploit the spatial consistency information among outliers (or inliners). Then, we derive the objective function of the PCOM method from the perspective of probability theory. The objective function can be solved iteratively by using the outlier-free least squares and standard max-flow/min-cut steps. Finally, for visual tracking, we develop an effective observation likelihood function based on the proposed PCOM method and background information, and design a simple update scheme. Both qualitative and quantitative evaluations demonstrate that our tracker achieves considerable performance in terms of both accuracy and speed.	algorithm;area striata structure;bittorrent tracker;clinical use template;entity name part qualifier - adopted;evaluation;iteration;iterative method;least squares;loss function;markov chain;markov random field;maximum flow problem;minimum cut;motion estimation;noise-induced hearing loss;optimization problem;outlier;outline of object recognition;principal component analysis;signal-to-noise ratio;video tracking	Dong Wang;Huchuan Lu;Chunjuan Bo	2015	IEEE Transactions on Image Processing	10.1109/TIP.2015.2478399	visualization;computer science;linear programming;machine learning;pattern recognition;mathematical model;mathematics;tracking;statistics;robustness;principal component analysis	Vision	46.76614097885309	-49.95063112558818	45823
5305a899c3f45e312f2ae583fd3abb2d1fede458	absolute localization using visual data for autonomous vehicles		In this paper, we propose an algorithm for estimating the absolute pose of a vehicle using visual data. Our method works in two steps: first we construct a visual map of geolocalized landmarks, then we localize the vehicle using this map. The main advantages of our method are that the localization of the vehicle is absolute and that it requires only a monocular camera and a low-cost GPS. We firstly outline our method, then we present our experimental results on real images using a reference database: the KITTI Vision Benchmark	algorithm;ambiguous name resolution;benchmark (computing);bibliographic database;domain generation algorithm;geolocation;global positioning system;key frame;legacy plug and play;national lidar dataset;online and offline;procurement	Safa Ouerghi;Rémi Boutteau;Pierre Merriaux;Nicolas Ragot;Xavier Savatier;Pascal Vasseur	2016		10.5220/0005682105950601	computer science;computer vision;artificial intelligence;pattern recognition;structure from motion;global positioning system;real image;monocular;suite;monocular vision	Robotics	52.43973962578613	-42.81671862404587	45859
5b677e1dff4f8fd2202330bfad688467cf102263	robust sequential learning algorithms for linear observation models	mathematical simulation;metodo cuadrado menor;robust sequential learning algorithms;estimation sequentielle;iterative method;processus gauss;methode moindre carre;optimisation;robust adaptive filters iterative bayesian learning laplace approximation;learning algorithm;methode moindre carre moyen;covariancia;least squares method;least mean squares methods;optimizacion;learning;laplace approximation;learning artificial intelligence approximation theory bayes methods iterative methods;iterative algorithms noise robustness least squares approximation training data bayesian methods additive noise resonance light scattering adaptive filters gaussian noise maximum likelihood estimation;implementation;bayes methods;algoritmo recursivo;modele lineaire;minorization maximization algorithm robust sequential learning algorithms linear observation models linear non gaussian observation model robust penalty functions bayesian learning problem iterative algorithm laplace approximation;covariance;non linear model;metodo secuencial;algorithme apprentissage;modele non lineaire;modelo lineal;sequential method;analisis matematico;minorization maximization algorithm;bayesian learning problem;robust adaptive filters;mathematical analysis;iterative algorithm;linear non gaussian observation model;robust penalty functions;metodo iterativo;aprendizaje;approximation theory;iterative methods;apprentissage;modelo no lineal;algorithme recursif;funcion penalidad;bayesian learning;simulacion matematica;methode iterative;robustesse;linear model;iterative bayesian learning;simulation mathematique;estimacion parametro;filtro adaptable;methode sequentielle;heavy tailed distribution;robustness;optimization;mixture of gaussians;recursive algorithm;gaussian process;sequential estimation;linear observation models;parameter estimation;estimation parametre;filtre adaptatif;learning artificial intelligence;fonction penalite;implementacion;proceso gauss;analyse mathematique;algoritmo aprendizaje	This paper presents a study of sequential parameter estimation based on a linear non-Gaussian observation model. To develop robust algorithms, we consider a family of heavy-tailed distributions that can be expressed as the scale mixture of Gaussian and extend the development to include some robust penalty functions. We treat the problem as a Bayesian learning problem and develop an iterative algorithm by using the Laplace approximation for the posterior and the minorization-maximization (MM) algorithm as an optimization tool. We then study a one-step implementation of the iterative algorithm. This leads to a family of generalized robust RLS-type of algorithms which include several well-known algorithms as special cases. Using a further simplification that the covariance is fixed, leads to a family of generalized robust LMS-type of algorithms. Through mathematical analysis and simulations, we demonstrate the robustness of these algorithms	adaptive filter;approximation;computation;dynamical system;estimation theory;expectation–maximization algorithm;image scaling;iterative method;least mean squares filter;level of detail;mm algorithm;machine learning;mathematical optimization;numerical analysis;recursive least squares filter;simulation;supervised learning	Guang Deng	2007	IEEE Transactions on Signal Processing	10.1109/TSP.2007.893733	econometrics;mathematical optimization;probabilistic analysis of algorithms;mathematics;iterative method;statistics	ML	29.089341352795838	-30.84362386669784	45892
9cb6dfcba2550d5cc5b2ae9dd817ba837da82289	euclidean upgrading from segment lengths	contraste;modelizacion;image tridimensionnelle;vision ordenador;euclidean theory;image processing;valorisation;espace euclidien;quadric;exact solution;procesamiento imagen;quadrico;espacio euclidiano;solucion exacta;traitement image;qualite service;quadrique;computer vision;modelisation;euclidean upgrading;reconstruction image;reconstruccion imagen;video cameras;image reconstruction;upgrading;camera video;theorie euclidienne;tridimensional image;euclidean space;vision ordinateur;etalonnage;valorizacion;camera calibration;solution exacte;camara de video;modeling;calibration;3d reconstruction;service quality;imagen tridimensional;teoria euclidiana;calidad servicio	We address the problem of the recovery of Euclidean structure of a projectively distorted n-dimensional space from the knowledge of the, possibly diverse, lengths of a set of segments. This problem is relevant, in particular, for Euclidean reconstruction with uncalibrated cameras, extending previously known results in the affine setting. The key concept is the Quadric of Segments (QoS), defined in a higher-dimensional space by the set of segments of a fixed length, from which Euclidean structure can be obtained in closed form. We have intended to make a thorough study of the properties of the QoS, including the determination of the minimum number of segments of arbitrary length that determine it and its relationship with the standard geometric objects associated to the Euclidean structure of space. Explicit formulas are given to obtain the dual absolute quadric and the absolute quadratic complex from the QoS. Experiments with real and synthetic images evaluate the performance of the techniques.	quality of service;synthetic intelligence	José Ignacio Ronda;Antonio Valdés	2010	International Journal of Computer Vision	10.1007/s11263-010-0369-z	3d reconstruction;iterative reconstruction;euclidean domain;seven-dimensional space;affine space;computer vision;calibration;camera resectioning;systems modeling;quadric;topology;distance from a point to a plane;image processing;computer science;euclidean space;euclidean shortest path;euclidean distance;mathematics;geometry;magnitude;euclidean distance matrix;service quality;set function	Theory	53.71131148014157	-51.76277416450639	45919
8380090542c0420d4ad53d449cd6bd04cca8d69f	integration of contextual information for tracking refinement	tracking refinement;simulated multisensor tracking scenario;sensor observation;reliability;low level fusion process;multisensor fusion process;likelihood map;context buildings sensor fusion target tracking meteorology equations;contextual information;data fusion;state estimation;pruning strategy;fusion system;contextual knowledge;likelihood masks tracking data fusion contextual information;discounting strategy;target tracking sensor fusion state estimation;sensor likelihood function;simulated multisensor tracking scenario contextual information tracking refinement fusion system low level fusion process contextual knowledge multisensor fusion process state estimation target tracking likelihood map sensor likelihood function discounting strategy pruning strategy reliability sensor observation;sensor fusion;target tracking;meteorology;likelihood masks;context;buildings;tracking	The exploitation of contextual information can bring several advantages to fusion systems at different levels. Although very promising, this topic is still a scarcely explored. In particular, the inclusion of contextual information in low-level fusion processes has not received much attention in the literature. In this paper we propose a framework for integrating contextual knowledge in a multisensor fusion process in order to improve the estimation of a target's state for tracking. Context will be here encoded in the form of likelihood maps to be fused with the sensors' likelihood functions. The framework presented here allows employing either discounting or pruning strategies for assessing the reliability of sensor observations. In our preliminary experiments, the inclusion of context has provided better accuracy in simulated multisensor tracking scenarios.	bayesian network;experiment;high- and low-level;map;oracle fusion architecture;refinement (computing);sensor;synthetic intelligence;variable shadowing	Ingrid Visentini;Lauro Snidaro	2011	14th International Conference on Information Fusion		computer vision;geography;pattern recognition;data mining	Robotics	50.472795465398626	-34.1341200620518	45934
1c642523023d3b712cb728d705998815153fed56	part context learning for visual tracking		Context information is widely used in computer vision for tracking arbitrary objects. Most existing works focus on how to distinguish the tracked object from background or inter-frame object similarity information or key-points supporters as their auxiliary information to assist them in tracking. However, in most cases, how to discover and represent both the intrinsic property inside the object and surrounding information is still an open problem. In this paper, we propose a unified context learning framework that can capture stable structure relations of in-object parts, context parts and the object itself to enhance the tracker’s performance. The proposed Part Context Tracker (PCT) consists of an appearance model, an internal relation model and an context relation model. The appearance model represents the appearances of the object and parts. The internal relation model utilizes the parts inside the object to describe the spatio-temporal structure property directly, while the context relation model takes advantage of the latent intersection between the object and background parts. Then the appearance model, internal relation model and context relation model are embedded in a max-margin structured learning framework. Furthermore, a simple robust update strategy using median filter is utilized, which can deal with appearance change effectively and alleviate the drift problem. Extensive experiments are conducted on various benchmark dataset, and the comparisons with state-of-the-arts demonstrate the effectiveness of our work.	benchmark (computing);bittorrent tracker;computer vision;embedded system;experiment;median filter;structured prediction	Guibo Zhu;Jinqiao Wang;Chaoyang Zhao;Hanqing Lu	2014			computer vision	AI	34.38261120056336	-49.99222620476447	45970
164dfd6945a80666ef6d4bd4df67481265877d75	active facial tracking	image color analysis face skin facial features feature extraction pixel humans;facial feature tracking;real time;skin;feature extraction face tracker skin color model thresholding;facial modeling;facial feature extraction;mouth corners active facial tracking human face tracker real time face tracker facial feature extraction skin color method face region face detection face localization computer vision facial modeling animation facial expression analysis face recognition facial feature points salient points anchor points facial landmarks eye corners nose tip;computer vision;facial expression analysis;skin color;face recognition;skin color model;image color analysis;image colour analysis;feature extraction;pixel;object tracking;facial feature detection;facial features;face;face tracker;humans;thresholding;face detection;object tracking computer vision face recognition feature extraction image colour analysis object detection;object detection	Human face tracker is one of important research areas that is continuously developing. Many methods have been developed for performing an effective and efficient face tracker based system application. One category of the face tracker methods is the real-time face tracker, which is a challenging task in this field. This paper presents a real-time human face tracker development using facial feature extraction. The skin color method is adopted to obtain the face region because of its efficiency in computing which is required in real time face tracker system. The detection and localization of the face and its features is instrumental for the successful performance of subsequent tasks in related computer vision applications. Many high-level vision applications such as facial feature tracking, facial modeling and animation, facial expression analysis and face recognition require reliable feature extraction system. Face detection system is concerned with finding whether there are any faces in a given image. Facial feature points are referred to in the literature as salient points, anchor points or facial landmarks. The most frequently occurring facial features are the four eye corners, the tip of the nose and the two mouth corners. Facial feature detection is a challenging computer vision problem due to high inter-personal changes such as gender, race and the intra-personal variability such as pose, expression and acquisition conditions like lighting, scale, facial accessories.		Mandalapu Sarada Devi;Preeti R. Bajaj	2010	2010 3rd International Conference on Emerging Trends in Engineering and Technology	10.1109/ICETET.2010.93	computer vision;face detection;facial motion capture;speech recognition;kanade–lucas–tomasi feature tracker;pattern recognition;three-dimensional face recognition;face hallucination	Vision	44.89985666919809	-45.489711187240445	46043
badfe4ac830c9cfee76ca7967afcc531c3743933	the accelerated power method for kernel principal component analysis		When faced with the large-scale data set, Kernel principal component analysis (KPCA) is infeasible because of the storage and computational problem. To overcome these disadvantages, an accelerated power method of computing kernel principal components is proposed. First, the accelerated Power iteration is introduced to compute the first eigenvalue and corresponding eigenvector. Then the deflation method is repeatedly applied to achieve other higher order eigenvectors. The space and time complexity of the proposed method is greatly reduced. Experimental results confirm the effectiveness of proposed method.	kernel principal component analysis;power iteration	Weiya Shi;Wenhua Zhang	2011		10.1007/978-3-642-23220-6_71	kernel principal component analysis	NLP	25.140202927704472	-37.49501077993485	46101
3a3bb795a6bfb1a548759daa3ffdc568e4e89888	efficient lookup table based camera pose estimation for augmented reality	camera pose estimation;lookup table;augmented reality;pose estimation	Until now, existing camera pose estimation methods for the widely used square marker-based augmented reality (AR) are either highly sensitive to noise or much time consuming, and developers have to work hard to find the proper trade-off between computational speed and quality in mobile AR applications where computational resources are limited. The major difficulty is that only the 4 corner points of the square AR marker are available, and no redundant point correspondences can be used for a stable estimation. To solve this problem, an efficient lookup table (LUT)-based non-iterative solution is presented in this paper that achieves high stability in the presence of noise better than the most robust and accurate iterative solutions in the field, with the same level of accuracy and a much lower computational complexity. Our central idea consists of extracting a key parameter β from the camera pose and creating a LUT for β by	3d pose estimation;approximation;ar (unix);augmented reality;computational complexity theory;computational resource;fiducial marker;iterative method;lookup table;whole earth 'lectronic link	Shiqi Li;Chi Xu	2011	Journal of Visualization and Computer Animation	10.1002/cav.385	computer vision;augmented reality;simulation;pose;3d pose estimation;lookup table;computer science;computer graphics (images)	Visualization	51.94060691176344	-47.45480405012914	46132
d5aea1ece49c428288cf5ab999824af56c1d1bb1	a decentralized approach to robust subspace recovery	sensors;principal component analysis;distributed databases;robustness;sparse matrices;algorithm design and analysis;data models	This paper considers subspace recovery in the presence of outliers in a decentralized setting. The intrinsic low-dimensional geometry of the data is exploited to substantially reduce the processing and communication overhead given limited sensing and communication resources at the sensing nodes. A small subset of the data is first selected. The data is embedded into a random low-dimensional subspace then forwarded to a central processing unit that runs a low-complexity algorithm to recover the subspace directly from the data sketch. We derive sufficient conditions on the compression and communication rates to successfully recover the subspace with high probability. It is shown that the proposed approach is robust to outliers and its complexity is independent of the dimension of the whole data matrix. The proposed algorithm provably achieves notable speedups in comparison to existing approaches for robust subspace recovery.	algorithm;central processing unit;embedded system;overhead (computing);robustness (computer science);with high probability	Mostafa Rahmani;George Atia	2015	2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)	10.1109/ALLERTON.2015.7447088	random subspace method;computer science;theoretical computer science;machine learning;data mining	Robotics	26.13843170446509	-36.204544191650704	46140
e22e848d3bbe8e75bba09203b70bdb1324dad471	a limb-based graphical model for human pose estimation	convolutional neural network convnet graphical model human pose estimation limbs detection;pose estimation graphical models shape feature extraction elbow joining processes visual systems;limbs detection convolutional neural network convnet graphical model human pose estimation	Modeling the relationship among human joints is one of the most important components in human pose estimation. Most of previous methods define this relationship as a geometric constraint on the relative locations of two neighboring joints. In this constraint, the local appearance of the region connecting two neighboring joints is ignored. However, discarding this image appearance leads to some severe problems, such as double-counting and localization failure when the human pose is rare in the training dataset. Moreover, this image appearance, called human limb, plays an important role in human pose estimation in human visual system. Due to these reasons, we propose to solve a new task: human limb detection, which aims at detecting and representing this local image appearance. We combine this task with human joint localization as a unified framework. After getting the initial detections, we design a two-steps graphical model to capture the spatial relationship among human joints and limbs in a coarse to fine way. We evaluate the proposed method on two widely used datasets for human pose estimation: 1) frame labeled in cinema and 2) leeds sports pose datasets. The experiments results show the effectiveness of our method.	3d pose estimation;cinema 4d;convolution;convolutional neural network;experiment;flic (file format);flow network;graphical model;higher-order function;human computer;human-based computation;relevance;sensor;unified framework	Guoqiang Liang;Xuguang Lan;Jiang Wang;Jianji Wang;Nanning Zheng	2018	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMC.2016.2639788	machine learning;computer science;feature extraction;computer vision;human visual system model;pose;artificial intelligence;graphical model;pattern recognition	Vision	34.75584308432647	-48.85355989786572	46191
1768d4fd83f56c580b67561dcb451e136859458b	generalization error bounds for stationary autoregressive models	ar model;model selection;generalization error;statistical machine learning;structural risk minimization;autoregressive model;interest rate;science learning	We derive generalization error bounds for stationary univariate autoregressive (AR) models. We show that imposing stationarity is enough to control the Gaussian complexity without further regularization. This lets us use structural risk minimization for model selection. We demonstrate our methods by predicting interest rate movements.	autoregressive model;generalization error;model selection;stationary process;structural risk minimization	Daniel J. McDonald;Cosma Rohilla Shalizi;Mark J. Schervish	2011	CoRR		econometrics;machine learning;star model;mathematics;autoregressive model;statistics;generalization error	ML	28.81989511152935	-27.036110065492252	46192
2359dbb0d87b58d85d8fbfbcb5d47688bdc00174	multitarget visual tracking based effective surveillance with cooperation of multiple active cameras	multicamera surveillance system;multiple active cameras;moving object;online position strategy;multicamera system;video surveillance;hierarchical camera selection;surveillance;hidden markov model;surveillance system;real time;multitarget visual tracking;joints;multitarget visual tracking active vision cooperative system multicamera system;multicamera surveillance system multitarget visual tracking multiple active cameras moving object tracking multiple pan tilt cameras distributed camera agent particle filter suboptimal camera action monte carlo method hierarchical camera selection task assignment strategy online position strategy;distributed camera agent;moving object tracking;visualization;hidden markov models;cooperative systems;surveillance cameras target tracking real time systems particle filters monitoring state estimation state space methods markov random fields particle tracking;video cameras;particle filter;monte carlo method;object tracking;field of view;suboptimal camera action;mutual information;task assignment;target tracking;depth estimation;task assignment strategy;visual tracking;computer simulation;monte carlo methods;cameras;video surveillance monte carlo methods object tracking particle filtering numerical methods target tracking video cameras;particle filtering numerical methods;multiple pan tilt cameras;cooperative system;active vision;real time systems	This paper presents a tracking-based surveillance system that is capable of tracking multiple moving objects, with almost real-time response, through the effective cooperation of multiple pan-tilt cameras. To construct this surveillance system, the distributed camera agent, which tracks multiple moving objects independently, is first developed. The particle filter is extended with target depth estimate to track multiple targets that may overlap with one another. A strategy to select the suboptimal camera action is then proposed for a camera mounted on a pan-tilt platform that has been assigned to track multiple targets within its limited field of view simultaneously. This strategy is based on the mutual information and the Monte Carlo method to maintain coverage of the tracked targets. Finally, for a surveillance system with a small number of active cameras to effectively monitor a wide space, this system is aimed to maximize the number of targets to be tracked. We further propose a hierarchical camera selection and task assignment strategy, known as the online position strategy, to integrate all of the distributed camera agents. The overall performance of the multicamera surveillance system has been verified with computer simulations and extensive experiments.	area striata structure;computer simulation;experiment;monte carlo method;mutual information;particle filter;personnameuse - assigned;physical object;real-time locating system;track (course);video tracking	Cheng-Ming Huang;Li-Chen Fu	2011	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2010.2050878	smart camera;computer vision;simulation;computer science;hidden markov model;statistics;monte carlo method;computer graphics (images)	Vision	46.044224483658404	-46.72923678767103	46238
e0f296dd7a8c9e315e4bd4e1108142f2e9e6faec	correlating driver gaze with the road scene for driver assistance systems	driver assistance;sign detection;traffic signs;driver assistance systems das;real time;automobile drivers driver assistance;computer and information science;robotics;journal article;natural sciences;roads and streets;vehicles;automatic sign classification;data och informationsvetenskap;keywords computer vision;gaze direction;traffic features;driver assistance system;real time systems	A driver assistance system (DAS) should support the driver by monitoring road and vehicle events and presenting relevant and timely information to the driver. It is impossible to know what a driver is thinking, but we can monitor the driver’s gaze direction and compare it with the position of information in the driver’s viewfield to make inferences. In this way, not only do we monitor the driver’s actions, we monitor the driver’s observations as well. In this paper we present the automated detection and recognition of road signs, combined with the monitoring of the driver’s response. We present a complete system that reads speed signs in real-time, compares the driver’s gaze, and provides immediate feedback if it appears the sign has been missed by t ©	real-time computing;real-time transcription	Luke Fletcher;Gareth Loy;Nick Barnes;Alexander Zelinsky	2005	Robotics and Autonomous Systems	10.1016/j.robot.2005.03.010	embedded system;computer vision;natural science;simulation;advanced driver assistance systems;computer science;robotics	HCI	48.525620280239686	-36.37353958656924	46263
ab1747d607654be5b5395c7a94b7d19205aeccf0	a real-time object recognition system on cell broadband engine	object recognition;image processing;real time;real time processing;cell broadband engine;face recognition;parallel implementation;high performance;embedded processor	Accurate object recognition based on image processing is required in embedded applications, where real-time processing is expected to incorporate accurate recognition. To achieve accurate real-time object recognition, an accurate recognition algorithm that can be quickened by parallel implementation and a processing system that can execute such algorithms in real-time are necessary. In this paper, we implemented an accurate recognition scheme in parallel that consists of boosting-based detection and histogram-based tracking on a Cell Broadband Engine (Cell), one of the latest high performance embedded processors. We show that the Cell can achieve real-time object recognition on QVGA video at 22 fps with three targets and 18 fps with eight targets. Furthermore, we constructed a real-time object recognition system using SONYR® Playstation 3, one of the most widely used Cell platforms, and demonstrated face recognition with it.	cell (microprocessor);outline of object recognition;real-time cmix	Hiroki Sugano;Ryusuke Miyamoto	2007		10.1007/978-3-540-77129-6_78	embedded system;computer vision;real-time computing;image processing;computer science;cognitive neuroscience of visual object recognition;3d single-object recognition	Robotics	43.94331571868538	-36.021958209187204	46290
767ac8398e845779be111eb4ce8cb60a8b69a511	alignedreid: surpassing human-level performance in person re-identification		In this paper, we propose a novel method called AlignedReID that extracts a global feature which is jointly learned with local features. Global feature learning benefits greatly from local feature learning, which performs an alignment/matching by calculating the shortest path between two sets of local features, without requiring extra supervision. After the joint learning, we only keep the global feature to compute the similarities between images. Our method achieves rank-1 accuracy of 94.4% on Market1501 and 97.8% on CUHK03, outperforming state-of-the-art methods by a large margin. We also evaluate human-level performance and demonstrate that our method is the first to surpass human-level performance on Market1501 and CUHK03, two widely used Person ReID datasets.		Xuan Zhang;Hao Luo;Xing Fan;Weilai Xiang;Yixiao Sun;Qiqi Xiao;Wei Jiang;Chi Zhang;Jian Sun	2017	CoRR		artificial intelligence;machine learning;pattern recognition;computer science;shortest path problem;feature learning	Vision	30.063836469411484	-50.95738508739106	46301
684bab29e9b94b99b1296ae5bf7752ab7cfc5d76	a bayesian estimation of building shape using mcmc	bayesian method;image sequence;bayesian estimator;global regularity;spatial organization;structure from motion;3d reconstruction;bayesian model	This paper investigates the use of an implicit prior in Bayesian Model-based 3D reconstruction of architecture from image sequences. In our previous work architecture is represented as a combination of basic primitives such as windows and doors etc, each with their own prior. The contribution of this work is to Provide a global prior for the spatial organization of the basic primitives. However, it is difficult ot explicitly formulate the prior on spatial organization. Instead we define an implicit representation that favours global regularities prevalent in architeture (e.g. windows lie in rows etc.). Specifying exact parameter values for this prior is problematic at best, however it is demonstrated that for a broad range of values the prior provides reasonable results. The validity of the prior is tested visually by generating synthetic buildings as draws from the prior simulated using MCMC. The result is a fully Bayesian method for structure from motion in the domain of architecture.	markov chain monte carlo	Anthony R. Dick;Philip H. S. Torr;Roberto Cipolla	2002		10.1007/3-540-47967-8_57	3d reconstruction;bayesian average;computer vision;structure from motion;bayesian probability;computer science;machine learning;pattern recognition;mathematics;bayesian statistics;spatial organization;bayesian inference;statistics	Vision	48.570349768482096	-51.94977379013731	46320
9e302f46666fa5ca19233c2573044cc2a0ff6311	generic probability density function reconstruction for randomization in privacy-preserving data mining	quadratic programming;data hiding;perturbation method;quadratic program;large dataset;probability density function;random noise;expectation maximization;parzen windows;convex set;reconstruction algorithm;randomization;em algorithm;privacy preserving data mining	Data perturbation with random noise signals has been shown to be useful for data hiding in privacy-preserving data mining. Perturbation methods based on additive randomization allows accurate estimation of the Probability Density Function (PDF) via the ExpectationMaximization (EM) algorithm but it has been shown that noise-filtering techniques can be used to reconstruct the original data in many cases, leading to security breaches. In this paper, we propose a generic PDF reconstruction algorithm that can be used on non-additive (and additive) randomization techiques for the purpose of privacy-preserving data mining. This two-step reconstruction algorithm is based on Parzen-Window reconstruction and Quadratic Programming over a convex set – the probability simplex. Our algorithm eliminates the usual need for the iterative EM algorithm and it is generic for most randomization models. The simplicity of our two-step reconstruction algorithm, without iteration, also makes it attractive for use when dealing with large datasets.	address space layout randomization;convex set;data mining;expectation–maximization algorithm;iteration;kernel density estimation;noise (electronics);perturbation theory;portable document format;privacy;quadratic programming;utility functions on indivisible goods	Vincent Yan Fu Tan;See-Kiong Ng	2007		10.1007/978-3-540-73499-4_7	mathematical optimization;expectation–maximization algorithm;machine learning;data mining;mathematics;fsa-red algorithm;quadratic programming;statistics;randomization function	ML	30.858512925050444	-29.63731658967862	46361
4ddd0b6644457a2027fe206cee57121e9493e700	human detection and tracking in an assistive living service robot through multimodal data fusion	robot sensing systems;detectors;measurement by laser beam;spatial variables measurement;bayesian methods;service robots;spatial variables measurement laser ranging object detection object tracking sensor fusion service robots;service robotics;laser ranging;humans robot sensing systems detectors cameras measurement by laser beam bayesian methods;human tracking;qa75 electronic computers computer science;sensor data fusion;sensor data fusion human detection human tracking service robotics assistive technology mcmc;human detection;object tracking;assistive technology;mcmc sampling framework human detection human tracking assistive living service robot multimodal data fusion laser range finder depth camera singular modalities bayesian formulation markov chain monte carlo sampling framework;mcmc;humans;sensor fusion;cameras;object detection;ta engineering general civil engineering general	A new method is proposed for using a combination of measurements from a laser range finder and a depth camera in a data fusion process that benefits from each modality's strong side. The combination leads to a significantly improved performance of the human detection and tracking in comparison with what is achievable from the singular modalities. The useful information from both laser and depth camera is automatically extracted and combined in a Bayesian formulation that is estimated using a Markov Chain Monte Carlo (MCMC) sampling framework. The experiments show that this algorithm can track robustly multiple people in real world assistive robotics applications.	algorithm;experiment;markov chain monte carlo;modality (human–computer interaction);monte carlo method;multimodal interaction;robotics;sampling (signal processing);service robot	Alexandre Noyvirt;Renxi Qiu	2012	IEEE 10th International Conference on Industrial Informatics	10.1109/INDIN.2012.6301153	computer vision;simulation;engineering;artificial intelligence	Robotics	53.30351741773707	-36.89980913898744	46440
078649db1e6affaac73930bd6f03fe1c7f519095	sensor planning and control in a dynamic environment	robot sensing systems;configuration space obstacles;vision sensors;sensor systems;motion control;theoretical framework;mobile agent team configuration control;sensors;model system;mobile agents;computational strategy;planning artificial intelligence;mobile robots;computational resources;orbital robotics;sensor planning;optimal control;configuration space;dynamic environment;computational modeling;robot vision;cooperative systems;computational complexity;particle filter;multi robot systems;cooperative systems multi robot systems mobile robots sensors planning artificial intelligence optimal control robot vision computational complexity;particle filtering;planning and control;mobile agent;distributed control;robot vision systems;computational resources sensor planning sensor control dynamic environment mobile agent team configuration control vision sensors computational strategy particle filtering modeled system dynamics configuration space obstacles;robot sensing systems robot vision systems cameras sensor systems laboratories mobile agents computational modeling orbital robotics motion control distributed control;modeled system dynamics;cameras;sensor control	This paper presents an approach to the problem of controlling the configuration of a team of mobile agents equipped with cameras so as to optimize the quality of the estimates derived from their measurements. The issue of optimizing the robots’ configuration is particularly important in the context of teams equipped with vision sensors since most estimation schemes of interest will involve some form of triangulation. We provide a theoretical framework for tackling the sensor planning problem and a practical computational strategy, inspired by work on particle filtering, for implementing the approach. We extend our previous work by showing how modeled system dynamics and configuration space obstacles can be handled. These ideas have been demonstrated both in simulation and on actual robotic platforms. The results indicate that the framework is able to solve fairly difficult sensor planning problems online without requiring excessive amounts of computational resources.	computation;computational resource;image sensor;mobile agent;particle filter;robot;simulation;system dynamics;triangulation (geometry);velocity obstacle	John R. Spletzer;Camillo J. Taylor	2002		10.1109/ROBOT.2002.1013436	control engineering;computer vision;simulation;particle filter;computer science;engineering;control theory	Robotics	51.99418748324086	-26.81141435865598	46466
5bf3a2173a564c97d664c87726827a55f96f23df	localization of mobile sensors and actuators for intervention in low-visibility conditions: the zigbee fingerprinting approach	info eu repo semantics article	Indoor localization in smoke conditions is one of the EU GUARDIANS project goals. When smoke density grows, optical sensors such as laser range finders and cameras cease to be efficient. Zigbee sensor networks provide an interesting approach due to the fact that radiofrequency signals are propagated easily in such conditions. Moreover, they permit having an alternative communication infrastructure to the emergency brigades, allowing also the implementation of localization algorithms for the mobile sensors, actuators, and firefighters. The overall localization method (i.e., ARIEL) aims to acquire the nodes position in real time during an intervention, using different sensor inputs such as laser, sonar, Zigbee, and Wifi signals. Moreover, a fine grained localization algorithm has been implemented to localize special points of interest such as emergency doors and fire extinguishers, using a Zigbee programmable high-intensity LED panel. This paper focuses on the Zigbee fingerprinting localization method used to obtain the position of the mobile sensors and actuators by training a database of radio signals for each scenario. Once this is done the proposed recognition method runs in a quite stable and accurate manner without needing any sophisticated hardware. Results compare the procedure with others such as KNN, and neural networks, demonstrating the feasibility of the method for a real emergency intervention.	as-interface;artificial neural network;fingerprint (computing);internationalization and localization;k-nearest neighbors algorithm;led display;point of interest;radio frequency;sonar (symantec);sensor	José V. Martí;Jorge Sales;Raúl Marín;Ernesto Jiménez-Ruiz	2012	IJDSN	10.1155/2012/951213	embedded system;telecommunications;computer science;computer security	Mobile	50.80574320197471	-32.12934420985852	46525
92938fa9aad692d4703398fdb3a187d225f68aa7	a hybrid classifier for precise and robust eye detection	eye;image processing;learning;support vector machines;maximum likelihood;eye pair template matching image classification eye detection eye location visual cue face image processing image alignment face recognition gaze tracking expression analysis learning low dimensional features eye patterns maximum likelihood svm classifier;image alignment;expression analysis;image matching;eye detection;gaze tracking;image classification;support vector machines eye face recognition feature extraction image classification image matching image registration learning artificial intelligence maximum likelihood estimation object detection;robustness face detection image processing face recognition image analysis detection algorithms filters maximum likelihood detection support vector machines support vector machine classification;maximum likelihood estimation;low dimensional features;eye location;eye pair template matching;face recognition;feature extraction;visual cues;image registration;svm classifier;face image processing;facial expression;learning artificial intelligence;template matching;object detection;eye patterns;visual cue	Eye location is an important visual cue for face image processing such as alignment before face recognition, gaze tracking, expression analysis, etc. In this paper a novel eye detection algorithm is presented, which integrates the characteristics of single eye and eye-pair images to develop a hybrid classifier under the learning paradigm. The low dimensional features representing eye patterns yield by subspace projection are selected via a filter and a wrapper method for a simplified maximum likelihood and a SVM classifier respectively. Eye candidates determined by a cascade of the two classifiers are further verified with eye-pair template matching scores to reject false detections. The performance of this eye detector is assessed on several publicly available face databases and the experimental results demonstrate its robustness to the variations in head pose, facial expressions, partial occlusions and lighting conditions	algorithm;database;eye tracking;facial recognition system;image processing;programming paradigm;sensor;template matching	Lizuo Jin;Xiao-Hui Yuan;Shin'ichi Satoh;Jiuxian Li;Liang-Zheng Xia	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.81	facial recognition system;computer vision;image processing;computer science;machine learning;pattern recognition;maximum likelihood	Vision	40.718116554229006	-49.606583604975526	46540
665049f83eafb48cafb21e4246ccc63e6efb2753	optical image classification: a ground-truth design framework	geophysical image processing;very high resolution vhr;ground truth design;reflective optics system imaging spectrometer ground truth design framework remote sensing field subsequent image processing optical remote sensing image classification segmentation unsupervised method clustering unsupervised method spatial information spectral information high spatial resolution image hyperspectral image ikonos sensors;image segmentation;very high resolution vhr clustering ground truth design hyperspectral image classification level set segmentation support vector machines svms;level set segmentation;subsequent image processing;training;level set;image classification;hyperspectral sensors;segmentation unsupervised method;remote sensing field;optical remote sensing image classification;high spatial resolution image;clustering;remote sensing;spectral information;reflective optics system imaging spectrometer;ground truth design framework;humans;clustering unsupervised method;optical sensors;ikonos sensors;image segmentation training humans level set optical sensors hyperspectral sensors;hyperspectral imaging;remote sensing geophysical image processing geophysical techniques hyperspectral imaging image classification;hyperspectral image;hyperspectral;support vector machines svms;spatial information;geophysical techniques	In the remote sensing field, ground-truth design for collecting training samples represents a tricky and critical problem since it has a direct impact on most of the subsequent image processing and analysis steps. In this paper, we propose a novel framework for assisting a human user in designing ground-truth by photointerpretation for optical remote sensing image classification. The proposed approach is (almost) completely automatic and comprehensive since it aims at assisting the human user from the first to the last step of the process. It is based on unsupervised methods of segmentation and clustering, in order to investigate both the spatial and the spectral information in the process of ground-truth design. The resulting ground-truth is classifier-free and can be further improved by making it classifier-driven through an active learning process. To validate the proposed framework, an experimental study was conducted on very high spatial resolution and hyperspectral images acquired by the IKONOS and the Reflective Optics System Imaging Spectrometer sensors, respectively. The obtained results show the usefulness and effectiveness of the proposed approach.	active learning (machine learning);algorithm;cluster analysis;computer vision;experiment;ground truth;image processing;information theory;interaction;iteration;performance;pixel;sensor;supervised learning;unsupervised learning	Edoardo Pasolli;Farid Melgani;Naif Alajlan;Nicola Conci	2013	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2012.2226041	computer vision;hyperspectral imaging;pattern recognition;remote sensing	Vision	31.542935123032702	-43.75785778701774	46554
9c691d8a83e0909742e1d037ea60f18fbdbbab14	a new asic for real-time linear color space transforms	complementary metal oxide semiconductor;double layer;color space;real time;red green and blue;large scale;application specific integrated circuit;machine vision;high speed;color image compression	Abstract   The design and Very Large Scale of Integration (VLSI) implementation of a new Application Specific Integrated Circuit (ASIC) which converts, in real-time, the Red, Green and Blue (RGB) color coordinates to the   XYZ  , YIQ and YVU color coordinates, is presented in this paper. Its frequency of operation is 13.3 MHz and the rate of operations of this ASIC is 66.5 MIPS. The high-speed operation is achieved by pipelining the data in a vector fashion. The ASIC is implemented using a Double Layer Metal (DLM), 1.0 μm, N-well, Complementary Metal Oxide Semiconductor (CMOS) process provided by the European Silicon Structures (ES2), and it occupies a silicon area of 6.32 mm × 5.11 mm= 32.29 mm 2 . It is intended to be used in colorimetry instrumentation for color measurement and control, in color machine vision in autonomous applications, where the need for short processing times is crucial and in real-time color image compression applications.	application-specific integrated circuit;color space;real-time transcription	Ioannis Andreadis;P. Iliades;Philippos Tsalides	1995	Real-Time Imaging	10.1006/rtim.1995.1037	computer vision;color depth;machine vision;computer hardware;computer science;high color;application-specific integrated circuit;color space;cmos;double layer;computer graphics (images)	Embedded	44.74261065758384	-34.5261066907332	46592
3f34b5e47dcf3498ff05873f1ce31c07f83e3c56	transient signal analysis and classification for condition monitoring of power switching equipment using wavelet transform and artificial neural networks	wavelet analysis;electricity supply systems;vibrations;neural networks;salient feature extraction;signal analysis;switchgear;vibration signals;transient analysis signal analysis condition monitoring vibrations signal processing signal resolution wavelet transforms feature extraction artificial neural networks wavelet analysis;transient analysis;wavelet transforms;transient signal analysis;artificial neural networks;power engineering computing;condition assessment;wavelet transform;condition monitoring;self organising feature maps;multiresolution transforms;feature extraction;signal processing;pattern classification;acoustic signal detection;signal resolution;transient signal processing;learning vector quantization transient signal processing condition monitoring vibration signals mechanical switching devices electricity supply systems wavelet transforms multiresolution transforms salient feature extraction neural networks self organising map;power engineering computing switchgear condition monitoring vibrations acoustic signal detection pattern classification feature extraction self organising feature maps wavelet transforms transient analysis;vector quantisation;multiresolution analysis;artificial neural network;self organising map;learning vector quantization;mechanical switching devices	In the present work, a transient signal processing technique is developed for condition monitoring. This technique is especially applicable to analysing vibration signals which are produced by switching mechanisms. Multiresolution and wavelet transforms are combined to extract salient features with limited dimension from the primary vibration signals. These features are further classified by artificial neural networks for the purpose of condition assessment. The results provide the foundation for the effective application wavelet analysis to condition monitoring of mechanical switchmg devices utilised in electricity supply	artificial neural network;signal processing;switch;wavelet transform	P. Kang;D. Birtwhistle;K. Khouzam	1998		10.1109/KES.1998.725895	electronic engineering;speech recognition;engineering;machine learning	ML	36.915354863142475	-31.607621392722542	46718
4dc26ed7f1c05800011ab91a81087bab65afbd1d	online structure analysis for real-time indoor scene reconstruction	3d scanning;camera tracking;plane detection;object detection;drifting	We propose a real-time approach for indoor scene reconstruction. It is capable of producing a ready-to-use 3D geometric model even while the user is still scanning the environment with a consumer depth camera. Our approach features explicit representations of planar regions and nonplanar objects extracted from the noisy feed of the depth camera, via an online structure analysis on the dynamic, incomplete data. The structural information is incorporated into the volumetric representation of the scene, resulting in a seamless integration with KinectFusion's global data structure and an efficient implementation of the whole reconstruction process. Moreover, heuristics based on rectilinear shapes in typical indoor scenes effectively eliminate camera tracking drift and further improve reconstruction accuracy. The instantaneous feedback enabled by our on-the-fly structure analysis, including repeated object recognition, allows the user to selectively scan the scene and produce high-fidelity large-scale models efficiently. We demonstrate the capability of our system with real-life examples.	data structure;geometric modeling;heuristic (computer science);match moving;outline of object recognition;real life;real-time clock;real-time computing;real-time transcription;regular grid;seamless3d	Yizhong Zhang;Weiwei Xu;Yiying Tong;Kun Zhou	2015	ACM Trans. Graph.	10.1145/2768821	computer vision;camera auto-calibration;simulation;computer graphics (images)	Graphics	52.85918066399348	-46.44218784144162	46749
e32e17a82681a9ae0dc7a896cdd4f99385f6dec4	edge-aware context encoder for image inpainting		We present Edge-aware Context Encoder (E-CE): an image inpainting model which takes scene structure and context into account. Unlike previous CE which predicts the missing regions using context from entire image, E-CE learns to recover the texture according to edge structures, attempting to avoid context blending across boundaries. In our approach, edges are extracted from the masked image, and completed by a full-convolutional network. The completed edge map together with the original masked image are then input into the modified CE network to predict the missing region. The experiments demonstrate that E-CE can generate images with better shapes and structures than CE.	alpha compositing;encoder;experiment;inpainting;mask (computing)	Liang Liao;Ruimin Hu;Jing Xiao;Zhongyuan Wang	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462549	inpainting;encoder;image restoration;artificial intelligence;computer science;pattern recognition;context model	Vision	25.939299525277935	-50.90006865023912	46754
239f8b0262622e90eb9353b101419027e69c6a50	detect globally, refine locally: a novel approach to saliency detection		Effective integration of contextual information is crucial for salient object detection. To achieve this, most existing methods based on 'skip' architecture mainly focus on how to integrate hierarchical features of Convolutional Neural Networks (CNNs). They simply apply concatenation or element-wise operation to incorporate high-level semantic cues and low-level detailed information. However, this can degrade the quality of predictions because cluttered and noisy information can also be passed through. To address this problem, we proposes a global Recurrent Localization Network (RLN) which exploits contextual information by the weighted response map in order to localize salient objects more accurately. Particularly, a recurrent module is employed to progressively refine the inner structure of the CNN over multiple time steps. Moreover, to effectively recover object boundaries, we propose a local Boundary Refinement Network (BRN) to adaptively learn the local contextual information for each spatial position. The learned propagation coefficients can be used to optimally capture relations between each pixel and its neighbors. Experiments on five challenging datasets show that our approach performs favorably against all existing methods in terms of the popular evaluation metrics.		Tiantian Wang;Lihe Zhang;Shuo Wang;Huchuan Lu;Gang Yang;Xiang Ruan;Ali Borji	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00330	architecture;convolutional neural network;feature extraction;object detection;salience (neuroscience);concatenation;artificial neural network;pattern recognition;artificial intelligence;computer science;exploit	Vision	26.949665663708064	-52.075478028457276	46792
2cbbeade0830cd5583428c020d124be3b7210547	multi-target detection and recognition by uavs using online pomdps	perception and mission planning under uncertainty;partially observable markov decision processes;parallel anticipatory planning and execution;offline supervised observation model learning;automatique robotique	This paper tackles high-level decision-making techniques for robotic missions, which involve both active sensing and symbolic goal reaching, under uncertain probabilistic environments and strong time constraints. Our case study is a POMDP model of an online multi-target detection and recognition mission by an autonomous UAV. The POMDP model of the multitarget detection and recognition problem is generated online from a list of areas of interest, which are automatically extracted at the beginning of the flight from a coarse-grained high altitude observation of the scene. The POMDP observation model relies on a statistical abstraction of an image processing algorithm’s output used to detect targets. As the POMDP problem cannot be known and thus optimized before the beginning of the flight, our main contribution is an “optimize-whileexecute” algorithmic framework: it drives a POMDP sub-planner to optimize and execute the POMDP policy in parallel under action duration constraints. We present new results from real outdoor flights and SAIL simulations, which highlight both the benefits of using POMDPs in multi-target detection and recognition missions, and of our “optimize-while-execute” paradigm. Introduction Designing robots that are able to automatically plan for their long-term goals under uncertainty and partial observability of their environment, is challenging due to the various uncertain events to consider and to the “curse of dimensionality” issue of solving methods (Sabbadin, Lang, and Ravoanjanahary 2007; Ross and Chaib-Draa 2007; Smith and Simmons 2005; Bonet and Geffner 2009). High-level strategies, named policies, can be automatically generated and optimized using Partially Observable Markov Decision Processes (POMDPs) (Smallwood and Sondik 1973). Yet, current deployed systems either assume that the problem is available before the mission starts (Taha, Mir, and Dissanayake 2011; Spaan 2008; Bai et al. 2011; Schesvold et al. 2003), so that plenty of time is available to optimize offline the strategy, or the problem is optimized on-line but only over a short-term horizon (Eidenberger, Grundmann, and Zoellner 2009; Miller, Harris, and Chong 2009). In this paper, we address the problem of target detection and recognition by autonomous Unmanned Aerial Vehicles (UAVs), which is an active field of research (Wang et al. 2012). In such missions, the high-level decision strategy of UAVs (e.g. fly to a given zone, land, take image, etc.) depends on many stochastic events (e.g. target detected in a given zone, target recognized, etc.) that may arise when executing the decision rule. Because of the high complexity of solving POMDPs, few deployed UAV systems rely on automatically-constructed and long-term optimized policies. Moreover, in a target detection and recognition mission, if viewed as an autonomous sequential decision problem under uncertainty, the problem is not known before the flight. Indeed, the number of targets and zones making up the environment are usually unknown beforehand: they must be automatically extracted at the beginning of the mission, in order to define the sequential decision problem to optimize. Therefore, contrary to most deployed or published approaches (see below), the POMDP model needed to optimize our UAV’s policy is defined only during the flight, after the number of zones is analyzed online. In order to tackle this challenging decision-theoretic robotic problem, we propose: (1) an “optimize-whileexecute” paradigm to solve the POMDP online during the flight, taking into account time constraints required by the mission’s duration and possible future execution states of the system; (2) an analysis of experimental results conducted on real flights. Our main contribution provides practical algorithmic means to optimize and execute in parallel an on-line POMDP policy under strong time constraints. Note that this contribution is different from the classical on-line approach used by POMDP algorithms like AEMS (Ross and ChaibDraa 2007) or RTDP-bel (Bonet and Geffner 2009), which do not guarantee to provide optimized actions in constrained time when immediately required by the execution engine, which is essential for critical robotic missions. Indeed, previous approaches do not strongly control the time spent to optimize an action in the current or future execution states. The paper is organized as follows. We discuss related work in the next section. We then present backgrounds on POMDP planning, and the POMDP model used to solve our target detection and recognition mission, as well as its automatic generation during the flight depending on the environment’s topography. We also explain how we statistically build a probabilistic abstraction of the image processing algorithm’s classifier output, which feeds our POMDP observation model. Then, we detail our “optimize-while-execute” algorithmic paradigm required to optimize and execute in parallel online POMDP policies. Finally, we discuss simulated and real-flight experiments. Related work POMDPs have been successfully implemented in many real robotic missions, e.g.: intelligent camera network for target tracking (Spaan 2008), ground robotics (Candido and Hutchinson 2011; Eidenberger, Grundmann, and Zoellner 2009), goal detection and active sensing in aerial robotic missions (Miller, Harris, and Chong 2009; Schesvold et al. 2003; Bai et al. 2011), and assistance to disabled persons (Taha, Mir, and Dissanayake 2011). From a practical viewpoint, these approaches can be globally classified in two groups. In the first group, the problem to solve is known in advance, so that the mission can be solved offline. Spaan (2008) uses a POMDP model to control a camera network in order to track targets. Bai et al. (2011) model an aircraft collision avoidance mission as a POMDP to automatically generate the threat resolution logic of the collision avoidance system. Offline POMDP methods have also been recently proposed to model the interaction between robotic devices and human operators (Taha, Mir, and Dissanayake 2011). In the second group, the POMDP problem is optimized online, but over a very short-term horizon – only the next action to perform in most approaches. Eidenberger, Grundmann, and Zoellner (2009) propose a POMDP model of active sensing with an optimization criterion based on information-theory. Miller, Harris, and Chong (2009) use a continuous POMDP framework for UAV’s guidance in order to perform multi-target tracking. Finally, Candido and Hutchinson (2011) also tackle continuous POMDPs to address the problem of motion planning under collision avoidance constraints. In this paper, due to the nature of our multi-target detection and identification mission in an uncertain environment, we need for a radically different approach. Namely, we have to optimize a complex POMDP problem known only at running time, which has to be solved over a long-term horizon in order to maximize information gathering and the chance of achieving a symbolic mission goal: landing near a specific target that has to be detected and distinguished from other targets in the scene. Formal framework: POMDP A POMDP is a tuple ❤❙❀❆❀✡❀ ❚❀ ❖❀❘❀ ❜ ✵ ✐ where ❙ is a set of states, ❆ is a set of actions, ✡ is a set of observations, ❚ ✿ ❙ ✂ ❆ ✂ ❙ ✦ ❬✵❀ ✶❪ is a transition function such that	aerial photography;algorithm;algorithmic paradigm;autonomous robot;compiler;curse of dimensionality;decibel;decision problem;decision theory;experiment;harris affine region detector;high- and low-level;hoare logic;image processing;information theory;markov chain;mathematical optimization;motion planning;online and offline;partially observable markov decision process;programming paradigm;robotics;simulation;television antenna;time complexity;topography;unmanned aerial vehicle;word lists by frequency	Caroline Ponzoni Carvalho Chanel;Florent Teichteil-Königsbuch;Charles Lesire	2013			computer vision;simulation;computer science;artificial intelligence;machine learning	AI	51.292636552229055	-31.196276746346648	46800
52d9477a8293d44b0f8be5c07d56d468d035b0b0	the power of randomization: distributed submodular maximization on massive datasets	qa mathematics;q science general	A wide variety of problems in machine learning, including exemplar clustering, document summarization, and sensor placement, can be cast as constrained submodular maximization problems. Unfortunately, the resulting submodular optimization problems are often too large to be solved on a single machine. We consider a distributed, greedy algorithm that combines previous approaches with randomization. The result is an algorithm that is embarrassingly parallel and achieves provable, constant factor, worstcase approximation guarantees. In our experiments, we demonstrate its efficiency in large problems with different kinds of constraints with objective values always close to what is achievable in the centralized setting.	approximation;automatic summarization;centralized computing;cluster analysis;embarrassingly parallel;entropy maximization;expectation–maximization algorithm;experiment;greedy algorithm;machine learning;mathematical optimization;provable prime;submodular set function	Rafael da Ponte Barbosa;Alina Ene;Huy L. Nguyen;Justin Ward	2015			mathematical optimization;computer science;theoretical computer science;submodular set function;machine learning;mathematics	ML	25.245408307878535	-33.3146713345488	46853
82dd98fefe3ed0c38072de94c73209f9571d439c	robust pca and clustering in noisy mixtures	polynomial algorithm;principle component analysis	This paper presents a polynomial algorithm for learning mixtures of logconcave distributions in R in the presence of malicious noise. That is, each sample is corrupted with some small probability, being replaced by a point about which we can make no assumptions. A key element of the algorithm is Robust Principle Components Analysis (PCA), which is less susceptible to corruption by noisy points. While noise may cause standard PCA to collapse well-separated mixture components so that they are indistinguishable, Robust PCA preserves the distance between some of the components, making a partition possible. It then recurses on each half of the mixture until every component is isolated. The success of this algorithm requires only a O∗(log n) factor increase in the required separation between components of the mixture compared to the noiseless case.	algorithm;cluster analysis;polynomial;principal component analysis	S. Charles Brubaker	2009			computer science;machine learning;pattern recognition;mathematics;statistics;principal component analysis	ML	32.543605256645996	-29.443795728025368	46872
bc83067f5bc6b10a3e3f928c73ca02a355bb3bd7	support vector machine classifier with truncated pinball loss		Feature noise, namely noise on inputs is a long-standing plague to support vector machine(SVM). Conventional SVM with the hinge loss(C-SVM) is sparse but sensitive to feature noise. Instead, the pinball loss SVM(pin-SVM) enjoys noise robustness but loses the sparsity completely. To bridge the gap between C-SVM and pin  -SVM, we propose the truncated pinball loss SVM(pin¯-SVM) in this paper. It provides a flexible framework of trade-off between sparsity and feature noise insensitivity. Theoretical properties including Bayes rule, misclassification error bound, sparsity, and noise insensitivity are discussed in depth. To train pin¯-SVM, the concave-convex procedure(CCCP) is used to handle non-convexity and the decomposition method is used to deal with the subproblem of each CCCP iteration. Accordingly, we modify the popular solver LIBSVM to conduct experiments and numerical results validate the properties of pin¯-SVM on the synthetic and real-world data sets.	support vector machine	Xin Shen;Lingfeng Niu;Zhiquan Qi;Yingjie Tian	2017	Pattern Recognition	10.1016/j.patcog.2017.03.011	computer science;machine learning;pattern recognition;data mining;statistics	Vision	25.481154464138353	-38.21231619193604	46883
3678c2215c49eeb021aa4b4886a145d4abb8cb49	moving object detection using keypoints reference model	signal image and speech processing;biometrics;pattern recognition;image processing and computer vision	This article presents a new method for background subtraction (BGS) and object detection for a real-time video application using a combination of frame differencing and a scale-invariant feature detector. This method takes the benefits of background modelling and the invariant feature detector to improve the accuracy in various environments. The proposed method consists of three main modules, namely, modelling, matching and subtraction modules. The comparison study of the proposed method with a popular Gaussian mixture model proved that the improvement in correct classification can be increased up to 98% with a reduction of false negative and true positive rates. Beside that the proposed method has shown great potential to overcome the drawback of the traditional BGS in handling challenges like shadow effect and lighting fluctuation.	autoregressive integrated moving average;background subtraction;matching (graph theory);mixture model;modeling language;object detection;quantum fluctuation;real-time clock;reference model	Wan Mimi Diyana Wan Zaki;Aini Hussain;Mohamed Hedayati	2011	EURASIP J. Image and Video Processing	10.1186/1687-5281-2011-13	computer vision;speech recognition;background subtraction;computer science;archaeology;pattern recognition;biometrics	Vision	41.69070255380364	-49.41919244454807	46887
319f5dada2305bae2c6901b479cfde5752374081	incorporating expressive graphical models in variational approximations: chain-graphs and hidden variables	variational approximation;bayesian network;latent variable;chain graph;spectrum;posterior distribution;hidden variables;directed graph;markov network;graphical model;approximate inference	Globalvariationalapproximationmethodsin graphical modelsallow efficient approximateinferenceof complex posteriordistributionsby usinga simplermodel. The choiceof the approximatingmodeldeterminesa tradeof betweenthecomplexity of theapproximation procedureandthequalityof theapproximation.In this paper , we considervariationalapproximationsbased on two classesof modelsthatarericher thanstandard Bayesiannetworks,Markov networksor mixturemodels.As such,theseclassesallow to find bettertradeofs in the spectrumof approximations.The first classof modelsarechain graphs, which capturedistributions that arepartially directed. The secondclassof modelsaredirectedgraphs(Bayesiannetworks)with additional latentvariables.Both classesallow representation of multi-variabledependencies thatcannotbeeasily representedwithin a Bayesiannetwork.	approximation;graphical model;hidden variable theory;markov chain;markov model;variational principle	Tal El-Hay;Nir Friedman	2001			latent variable;variable elimination;spectrum;mathematical optimization;combinatorics;directed graph;variable-order bayesian network;computer science;variational message passing;machine learning;bayesian network;mathematics;graphical model;posterior probability;hidden variable theory;dynamic bayesian network;statistics	ML	26.44809736201501	-29.282366108545638	46928
2ad1f6b606656ee50b86f5177972989ff522a6b2	video pause detection using wavelets	content management;discrete wavelet transforms;histograms;wavelet transforms content management entropy indexing statistical analysis video recording video retrieval;video retrieval;video processing;entropy pause detection wavelet;statistics based wavelet transform;wavelet transforms;video indexing;wavelet transform;statistical analysis;gunshot detection systems testing content management indexing content based retrieval wavelet transforms entropy histograms frequency domain analysis statistics;indexing;pixel;video recording;content management system;entropy;watches;digital video;content entropy digital video capture content management system video indexing video retrieval shot boundary detection statistics based wavelet transform;digital video capture;frequency domain;content entropy;wavelet;shot boundary detection;pause detection	As the volume of digital video captured and stored continuesto increase, research efforts have focused on contentmanagement systems for video indexing and retrieval applications.A first step in generic video processing is shot boundary detection. This paper addresses a novel algorithm for abrupt shot (cut/pause) detection - especially on frames with similar statistics - based on the wavelet transformand content entropy. The algorithm has been successfullytested on some video categories including sport and live videos. Its quantitative performance has been comparedto other known methods including pixel, histogram, frequency domain and statistics difference. In each test, the proposed wavelet method outperforms the others.	algorithm;digital video;pixel;shot transition detection;video processing;wavelet	Shiva Zaboli;David A. Clausi	2009	2009 Canadian Conference on Computer and Robot Vision	10.1109/CRV.2009.20	computer vision;speech recognition;computer science;video quality;video tracking;pattern recognition;video denoising;statistics;wavelet transform	Vision	39.22471166969527	-51.828405338352574	46986
2606c886b3c0a4565d0f70a9a32423ec3859daf7	science without (parametric) models: the case of bootstrap resampling	experimental design;parametric model;nonparametric statistics;data;bootstrap resampling;nonparametric statistic;statistical inference;inductive inference;models;model misspecification	Scientific and statistical inferences build heavily on explicit, parametric models, and often with good reasons. However, the limited scope of parametric models and the increasing complexity of the studied systems in modern science raise the risk of model misspecification. Therefore, I examine alternative, data-based inference techniques, such as bootstrap resampling. I argue that their neglect in the philosophical literature is unjustified: they suit some contexts of inquiry much better and use a more direct approach to scientific inference. Moreover, they make more parsimonious assumptions and often replace theoretical understanding and knowledge about mechanisms by careful experimental design. Thus, it is worthwhile to study in detail how nonparametric models serve as inferential engines in science.	design of experiments;inferential programming;mathematical model;occam's razor;resampling (statistics)	Jan Sprenger	2009	Synthese	10.1007/s11229-009-9567-z	nonparametric statistics;bootstrapping;econometrics;statistical inference;parametric model;inductive reasoning;data mining;mathematics;design of experiments;statistics;data	AI	26.952956727068944	-25.533141466213177	47079
ca99697c79fc64b8d9d58d901a88846a1e7b6d05	classification and localization of vehicle occupants using 3d range images	sensors;signalbehandling;air bags;theses;location;three dimensional;video cameras;signal processing;image analysis;seats;passengers	This thesis deals with the problem of classifying automotive vehicle occupants and estimating their position. This information is critical in designing future smart airbag systems providing maximum protection for passengers. According to the American National Highway Traffic Safety Administration (NHTSA), since 1990, in the USA, 227 deaths have been attributed to airbags deployed in low-speed crashes which included 119 children, and 22 infants. In these cases, intelligent deployment of the airbag, based on the type and position of occupant could have avoided these fatalities. Current commercial classification systems based on traditional sensors, such as pressure sensors are not able to detect the position of occupants. Vision-based systems are advantageous over pressure sensor based systems, as they can provide additional functionalities like dynamic occupant position analysis or child seat orientation detection. On the other hand, vision-based systems have to cope with several challenges, such as, illumination conditions, temperature, humidity, large variation of scenes, cost, and computational aspects. This thesis presents new pattern recognition techniques for classifying, localizing and tracking vehicle occupants using a low-resolution 3-D optical time-of-flight range camera. This sensor is capable of providing directly a dense range image, independent of the illumination conditions and object textures. Based on this technology, IEE S.A. is presently developing a camera system for the application of occupant classification. A prototype of this camera has been the basis for this study. The first part of the thesis presents the problem of occupant classification. Herein, we investigate geometric feature extraction methods to discriminate between different occupant types. We develop features that are invariant under rotation and translation. A method for reducing the size of the feature set is analyzed with emphasis on robustness and low computational complexity while maintaining highly discriminative information. In addition, several classification methods are studied including Bayes quadratic classifier, Gaussian mixture model (GMM) classifier and polynomial classifier. We propose the use of a cluster based linear regression classifier using a polynomial kernel which is particularly well suited to coping with large variations within each class. Full scale experiments have been conducted which demonstrate that a classification reliability of almost 100 per cent can be achieved with the reduced feature set in combination with a cluster based classifier. In this safety critical application, it is equally important to address the problem of reliability estimation for the system. State-of-the-art methods to estimate the reliability of the classification are based either on classification output or based on density estimation. The second part of the thesis treats estimation of the reliability of the pattern classification system. Herein, a novel reliability measure is proposed for classification output which takes into account the local density of training data. Experiments verify that this reliability measure outperforms state-of-the-art methods in many cases. Lastly, the problem of dynamically detecting out-of-position occupants is addressed in the third part of the thesis. This task requires detecting and localizing the position of the occupant's head. Traditional head detection methods, such as detecting head-like objects in the image by analyzing the local shapes are not robust with the current sensor. Many regions in a scene such as the shoulder or the elbow of the occupant can be incorrectly detected as the head. In order to cope with these challenges, we exploit topology information in the range image. A modified Reeb graph technique has been developed that extracts a topological skeleton of the 3D contour that is invariant under rotation and translations. Results verify that the Reeb graph detects successfully the head i.e., the head always corresponds to one of the end points of the skeleton. Subsequently, a data association algorithm to select the correct head candidate out of the Reeb graph candidates is presented. Results show that the resulting head detection algorithm based on Reeb graphs is robust under scene changes (A). This document is available at http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-4603.	3d film	Pandu Ranga Rao Devarakota	2008			computer vision;simulation;engineering;forensic engineering	Robotics	41.43515340887585	-46.09802322161433	47103
63f3e84f3157ef782b915b238dccc069b8d24ff0	co-hierarchical analysis of shape structures	structural shape analysis;representative co selection;part correspondence;co hierarchical analysis	We introduce an unsupervised co-hierarchical analysis of a set of shapes, aimed at discovering their hierarchical part structures and revealing relations between geometrically dissimilar yet functionally equivalent shape parts across the set. The core problem is that of representative co-selection. For each shape in the set, one representative hierarchy (tree) is selected from among many possible interpretations of the hierarchical structure of the shape. Collectively, the selected tree representatives maximize the within-cluster structural similarity among them. We develop an iterative algorithm for representative co-selection. At each step, a novel cluster-and-select scheme is applied to a set of candidate trees for all the shapes. The tree-to-tree distance for clustering caters to structural shape analysis by focusing on spatial arrangement of shape parts, rather than their geometric details. The final set of representative trees are unified to form a structural co-hierarchy. We demonstrate co-hierarchical analysis on families of man-made shapes exhibiting high degrees of geometric and finer-scale structural variabilities.	algorithm;cluster analysis;iterative method;shape analysis (digital geometry);structural similarity;unsupervised learning	Oliver van Kaick;Kai Xu;Hao Zhang;Yanzhen Wang;Shuyang Sun;Ariel Shamir;Daniel Cohen-Or	2013	ACM Trans. Graph.	10.1145/2461912.2461924	active shape model;combinatorics;machine learning;shape analysis;mathematics	Graphics	34.57229488131348	-41.55498978099428	47176
3547130f4f1385d8d4f3d500d0730e484bab9d02	hierarchical simultaneous localization and mapping	self organising feature maps collision avoidance mobile robots;mobile robots;obstacle avoidance;self organising feature maps;local features;simultaneous localization and mapping;higher dimensions;collision avoidance;simultaneous localization and mapping orbital robotics topology large scale systems computational complexity silver grid computing feature extraction robot localization organizing;obstacle avoidance simultaneous localization and mapping feature based mapping strategies topological mapping procedure local feature based methods autonomous exploration	This paper presents a novel method of combining topological and feature-based mapping strategies to create a hierarchical approach to simultaneous localization and mapping (SLAM). More than simply running both processes in parallel, we use the topological mapping procedure to organize local feature-based methods. The result is an autonomous exploration and mapping strategy that scales well to large environments and higher dimensions while confronting the issue of obstacle avoidance. We have obtained successful results of our approach in an area spanning 5000 square meters.	autonomous robot;file spanning;obstacle avoidance;simultaneous localization and mapping	Brad Lisien;Deryck Morales;David Silver;George Kantor;Ioannis M. Rekleitis;Howie Choset	2003		10.1109/IROS.2003.1250670	mobile robot;computer vision;computer science;artificial intelligence;machine learning;obstacle avoidance;simultaneous localization and mapping	Robotics	51.66655055351294	-37.58135151077365	47221
138052e3be68bc30eed44dbf0b9ab8a59a51bbf0	face tracking and head pose estimation using convolutional neural networks	unit selection;head pose estimation;face tracking;active appearance models;audiovisual text to speech synthesis;neural network	In applications where face orientation is necessary, but in unpretending environments in terms of lighting, equipment, resolution, etc, employing local tracking techniques would usually fail to give accurate results, regarding the issue of head pose estimation. However, in a similar manner, holistic techniques require the face to be well aligned with the training data. This pre-assumes correct and accurate face tracking, which is also a challenging issue. Here, we propose a face tracker, adjusted to each person's face chrominance values, and learnt online. Based on the face bounding box, Convolutional Neural Networks (CNNs) are employed, in order to calculate face orientation. CNNs are ideal for cases where a lot of distortions exist in the data, and the proposed architecture only utilizes subsets of classifiers, excluding those corresponding to rotation angles far from the current.	3d pose estimation;artificial neural network;convolutional neural network;distortion;facial motion capture;holism;integrated development environment;minimum bounding box	Stylianos Asteriadis;Kostas Karpouzis;Stefanos D. Kollias	2010		10.1145/1924035.1924046	computer vision;speech recognition;computer science;machine learning	Vision	31.024683500584747	-49.7810507681328	47231
c92ec6ad65080217a8f76f96d80e78127cd7037a	bayes networks for sonar sensor fusion	sensor fusion	"""Wide-angle sonar mapping of the environ­ ment by mobile robot is nontrivial due to sev­ eral sources of uncertainty: dropouts due to """"specular"""" reflections, obstacle location un­ certainty due to the wide beam, and distance measurement error. Earlier papers address the latter problems, but dropouts remain a problem in many environments. We present an approach that lifts the overoptimistic in­ dependence assumption used in earlier work, and use Bayes nets to represent the depen­ dencies between objects of the model. Ob­ jects of the model consist of readings, and of regions in which """"quasi location invari­ ance"""" of the (possible) obstacles exists, with respect to the readings. Simulation supports the method's feasibility. The model is readily extensible to allow for prior distributions, as well as other types of sensing operations."""	amiga reflections;bayesian network;co-ment;mobile robot;sonar (symantec);sensor;simulation;statistical model	Ami Berler;Solomon Eyal Shimony	1997			computer vision;simulation;computer science;artificial intelligence;machine learning;sensor fusion;statistics	Robotics	51.674998094852576	-34.42054867017095	47251
07d5c7a69e91c6fad4b8f6d82b8275fd21946fe3	real-time object tracking for augmented reality combining graph cuts and optical flow	i 4 8 image processing and computer vision scene analysis tracking;graph theory;image segmentation;image sequences augmented reality graph theory image segmentation;augmented reality image motion analysis pixel image segmentation image edge detection cameras navigation object detection robustness shape;real time object tracking;h 5 1 information interfaces and presentation multimedia information systems augmented reality;graph cut segmentation real time object tracking augmented reality optical flow;real time;indexing terms;multimedia information system;computer vision;3d model;graph cut;h 5 1 information interfaces and presentation multimedia information systems augmented reality i 4 8 image processing and computer vision scene analysis tracking;graph cut segmentation;object tracking;optical flow;augmented reality;information interfaces and presentation;scene analysis;image sequences	We present an efficient and accurate object tracking algorithm based on the concept of graph cut segmentation. The ability to track visible objects in real-time provides an invaluable tool for the implementation of markerless Augmented Reality. Once an object has been detected, it's location in future frames can be used to position virtual content, and thus annotate the environment. Unlike many object tracking algorithms, our approach does not rely on a preexisting 3D model or any other information about the object or its environment. It takes, as input, a set of pixels representing an object in an initial frame and uses a combination of optical flow and graph cut segmentation to determine the corresponding pixels in each future frame. Experiments show that our algorithm robustly tracks objects of disparate shapes and sizes over hundreds of frames, and can even handle difficult cases where an object contains many of the same colors as its background. We further show how this technology can be applied to practical AR applications.	3d pose estimation;algorithm;ar (unix);augmented reality;color;cut (graph theory);experiment;feature model;graph cuts in computer vision;interactivity;optical flow;outline of object recognition;pixel;pose (computer vision);preprocessor;real-time locating system;real-time transcription;tracking system	Jonathan Mooser;Suya You;Ulrich Neumann	2007	2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality	10.1109/ISMAR.2007.4538839	computer vision;augmented reality;method;index term;cut;object model;computer science;graph theory;video tracking;optical flow;data transfer object;multimedia;image segmentation;computer graphics (images)	Visualization	49.77548446303745	-48.21791875661642	47261
416d8b339e99f9aa33a799c214a84c4758d8a084	truesight a pedestrian navigation system based in automatic landmark detection and extraction on android smartphone	mobile device;open source computer vision library truesight pedestrian navigation system automatic landmark detection automatic landmark extraction android smartphone gps smartphone camera opencv surf algorithm vision based navigation system database improvements;localization;global positioning system androids humanoid robots visualization robustness google image segmentation;smart phones;android;computer vision;navigation;pedestrians;android landmark recognition mobile device navigation localization;feature extraction;visual databases cameras computer vision feature extraction object detection operating systems computers pedestrians smart phones;landmark recognition;operating systems computers;cameras;object detection;visual databases	"""From time to time someone gets lost and askhimself """"How do I get there?"""" With the advent of the GPS this question can be answered. However due to difficulties such as lack of precision, possibility of inaccurate maps,network dependency, and cost lead the pursuit of analternative solution. In order to locate himself the person can use a different method: using a smartphone camera his position is recognized visually, based in environment references, and then an arrow pointing the right direction appears in a map in the display. This method was implemented in the application framework of Android,using OpenCV and its implementation of the SURFalgorithm. The final application is named TrueSight and westudy its viability and limitations. The authors conclude thata vision-based navigation system is viable, but database improvements and exhibition could make it better."""	android;application framework;global positioning system;map;opencv;smartphone	Alessandro Luiz Stamatto Ferreira;Selan Rodrigues dos Santos;Leonardo Cunha de Miranda	2012	2012 14th Symposium on Virtual and Augmented Reality	10.1109/SVR.2012.14	embedded system;computer vision;simulation;engineering	HCI	41.550666328627834	-42.56911179131379	47273
757ac401f6ac4991bb3f171042df3d8481669216	dynamic multi-cue information fusion for robust detection of traffic infrastructure	information integration;information content;pattern recognition;3d visualization	-Visual object detection using single cue information has been successfully applied in various tasks, in particular for near range recognition. While robust classification and probabilistic representation enhance 2D pattern recognition performance, they are 'per se' restricted due to the limited information content of single cues. The contribution of this work is to demonstrate performance improvement using multi-cue information integrated within a probabilistic framework. 2D and 3D visual information naturally complement one another, each information source providing evidence for the occurrence of the object of interest. We demonstrate preliminary work describing Bayesian decision fusion for object detection and illustrate the method by robust detection of traffic infrastructure.	bayesian network;information source;object detection;pattern recognition;self-information;statistical classification	Lucas Paletta;Gerhard Paar	2002			pattern recognition;artificial intelligence;computer vision;information integration;object detection;fusion;probabilistic logic;performance improvement;visualization;information filtering system;computer science	Vision	32.01282785973682	-48.54621340301805	47298
3639706b8321d7dd86b4607a1f563320fcb1c8df	gait recognition with compact lidar sensors	qa75 electronic computers computer science szamitastechnika;szamitogeptudomany	In this paper, we present a comparative study on gait and acti vity analysis using LiDAR scanners with different resolution. Previous studies showed that gait recognition methods based on the point clouds of a Velodyne HDL-64E Rotating Multi-Beam LiDAR can be used for people reid ntification in outdoor surveillance scenarios. However, the high cost and the weight of that sensor m eans a bottleneck for its wide application in surveillance systems. The contribution of this paper is to s h w that the proposed Lidar-based Gait Energy Image descriptor can be efficiently adopted to the measureme nts of the compact and significantly cheaper Velodyne VLP-16 LiDAR scanner, which produces point clouds with a nearly four times lower vertical resolution than HDL-64. On the other hand, due to the sparsity of the data, the VLP-16 sensor proves to be less efficient for the purpose of activity recognition, if th e events are mainly characterized by fine hand movements. The evaluation is performed on five tests scenarios wi th multiple walking pedestrians, which have been recorded by both sensors in parallel.	academy;activity recognition;data descriptor;emoticon;gait analysis;image resolution;point cloud;precise point positioning;sensor;sparse matrix;tag cloud	Bence Gálai;Csaba Benedek	2017		10.5220/0006124404260432	computer vision;simulation;computer science	Vision	41.40762449307621	-41.2720784551952	47308
0c45f23551a88db06af891dcdc633317fa64bc88	predicting saliency using two contextual priors: the dominant depth and the horizon line	detectors;horizon line visual attention contextual guidance saliency dominant depth;saliency;color;computer model;prior knowledge;layout;visualization;computational modeling;dominant depth;feature extraction;visual features;mathematical model;horizon line;visual attention;contextual guidance;spatial frequency;visualization computational modeling feature extraction mathematical model layout color detectors	A computational model of visual attention using visual inferences is proposed. The dominant depth and the horizon line position are inferred from low-level visual features. This prior knowledge helps to find salient areas on still color pictures. Regarding the dominant depth, the idea is to favor the lowest spatial frequencies on close-up scenes whereas the highest spatial frequencies are used to predict salient areas on panoramic view. Some studies showed that the horizon line is a natural attractor of our gaze. Horizon detection is then used to improve the saliency prediction. Results show that the proposed model outperforms existing approaches. However, the dominant depth does not bring any gain in the saliency prediction.	computation;computational model;high- and low-level;image	Olivier Le Meur	2011	2011 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2011.6011848	computer simulation;layout;computer vision;detector;visualization;feature extraction;computer science;machine learning;salience;pattern recognition;mathematical model;spatial frequency;computational model;statistics	Robotics	38.87690073545482	-50.147422323178574	47374
d712a6f2847b9b00879fdb5e0f312d7058302e95	tailigator: cooperative system for safety distance observance	training;visualization;roads;stereo image processing;safety;vehicles;cameras	A driver's inattention to or disregard for the minimum safety distance creates a hazardous situation in which avoiding a rear-end collision is nearly impossible. In a joint effort to implement safety and decision-making processes at an individual level, we present in this paper a cooperative approach to increase the driver's visual awareness of safe distances. Our Tailigator system garners information through the stereoscopic capturing and processing of images by rear cameras to calculate the distance from the leading to a following vehicle. Visual data related to the safety distance is provided to the rear vehicle in real-time, relying on an asynchronous collaborative process. Results from the system qualitative evaluation are discussed.	real-time clock;real-time computing;stereoscopy	Cristina Olaverri-Monreal;Rene Lorenz;Florian Michaeler;Gerd Ch. Krizek;Matthias Pichler	2016	2016 International Conference on Collaboration Technologies and Systems (CTS)	10.1109/CTS.2016.0076	computer vision;simulation;visualization;computer science;computer security	Robotics	45.05793569146502	-41.09869819512261	47381
dc334612aef95b83f0c64c0f43d743165eb0d084	a methodology for extraction building facades from vls	lasers;image segmentation;mobile lidar point clouds building facades extraction vls vehicle borne lidar system building facades automated extraction contour extraction image segmentation building boundaries geo referenced feature image ransac point cloud segmentation feature extraction automatically extracting building footprints;laser radar;point cloud segmentation;radar imaging feature extraction image segmentation optical radar;optical radar;feature extraction;radar imaging;mobile communication;feature extraction buildings image segmentation laser radar mobile communication data models lasers;vls;feature extraction vls point cloud segmentation;buildings;data models	The components and position principle of vehicle-borne LIDAR system are expatiated. Based on the previous experience a new method for automated extraction of building facades from VLS was put forward. It adopt image segmentation and contour extraction tracing to extract building boundaries in the geo-referenced feature image. the existing methods of RANSAC, point cloud segmentation, feature extraction are analyzed, At last the experimental results show that the proposed method provides useful and valid solution for automatically extracting building footprints from mobile LIDAR point clouds.	active contour model;feature extraction;image segmentation;point cloud;random sample consensus;von luschan's chromatic scale	Zuo-Wei Huang;Xiang Chi;Fang Liu	2012	2012 Third International Conference on Digital Manufacturing & Automation	10.1109/ICDMA.2012.18	computer vision;simulation;geography;scale-space segmentation;remote sensing	Robotics	53.510979165848745	-43.74340471130717	47433
e39c4461cb5997f613a3f2fab24b039a3a8f9459	hierarchical clustering of 3-d line segments for building detection	geophysical image processing;hierarchical clustering;pattern clustering;image line segment clustering neural network;image segmentation;centroid neural network;measurement;neural nets;edge detection;aerial image processing 3d line segments hierarchical clustering method building detection centroid neural network satellite image resolution;pattern clustering edge detection geophysical image processing image segmentation neural nets object detection;image segmentation clustering algorithms image edge detection buildings measurement neurons artificial neural networks;image;hierarchical clustering method;aerial image;artificial neural networks;image edge detection;clustering;3d line segments;clustering algorithms;satellite image resolution;line segment;neurons;aerial image processing;high resolution satellite images;buildings;building detection;object detection;neural network	A novel approach for an efficient extraction of rectangular boundaries from aerial image data is proposed in this paper. In this approach, a Centroid Neural Network (CNN) with a metric of line segments is utilized for connecting low-level linear structures or grouping similar objects. The proposed an approach, called hierarchical clustering method, utilizes the fact that rooftops of a building are about the same height and perform clustering process with candidate 3-D line segments with similar heights. Experiments are performed with a set of high resolution satellite image data. The results show that the proposed hierarchical clustering method can remove noisy segments such as shade lines efficiently and find more accurate rectangular boundaries.	aerial photography;algorithm;artificial neural network;cluster analysis;hierarchical clustering;high- and low-level;image resolution;resultant	Dong-Chul Park	2010	The 10th IEEE International Symposium on Signal Processing and Information Technology	10.1109/ISSPIT.2010.5711732	correlation clustering;computer vision;fuzzy clustering;computer science;machine learning;consensus clustering;pattern recognition;cluster analysis;single-linkage clustering;artificial neural network;hierarchical clustering of networks	EDA	33.108036440514475	-44.69645135511507	47451
1f436aa4e68274037fff44e6cfbcd0a1ee3f60df	tell and predict: kernel classifier prediction for unseen visual classes from unstructured text descriptions		In this paper we propose a framework for predicting kernelized classifiers in the visual domain for categories with no training images where the knowledge comes from textual description about these categories. Through our optimization framework, the proposed approach is capable of embedding the class-level knowledge from the text domain as kernel classifiers in the visual domain. We also proposed a distributional semantic kernel between text descriptions which is shown to be effective in our setting. The proposed framework is not restricted to textual descriptions, and can also be applied to other forms knowledge representations. Our approach was applied for the challenging task of zeroshot learning of fine-grained categories from text descriptions of these categories.	experiment;kernel (operating system);kernel method;math kernel library;mathematical optimization;multiple kernel learning;naive bayes classifier;privilege escalation;transfer function	Mohamed Elhoseiny;Ahmed M. Elgammal;Babak Saleh	2015	CoRR		natural language processing;machine learning;pattern recognition;mathematics	AI	25.557582149084997	-47.67473813654022	47453
12110935ec10d53055253b7fe2d7dda381f2c304	learning room occupancy patterns from sparsely recovered light transport models	minimization light emitting diodes sparse matrices color intelligent sensors image color analysis;controllable light;color sensors;light transport model;controllable light room occupancy light transport model color sensors;room occupancy pattern machine learning lighting condition distributed color sensor spatial distribution human readable 2d structure rgb gray level depth sensor depth image cameras high level information vision system sparsely recovered light transport model;lighting image colour analysis image sensors learning artificial intelligence;room occupancy	In traditional vision systems, high level information is usually inferred from images or videos captured by cameras, or depth images captured by depth sensors. These images, whether gray-level, RGB, or depth, have a human-readable 2D structure which describes the spatial distribution of the scene. In this paper, we explore the possibility to use distributed color sensors to infer high level information, such as room occupancy. Unlike a camera, the output of a color sensor has only a few variables. However, if the light in the room is color controllable, we can use the outputs of multiple color sensors under different lighting conditions to recover the light transport model (LTM) in the room. While the room occupancy changes, the LTM also changes accordingly, and we can use machine learning to establish the mapping from LTM to room occupancy.	high-level programming language;human-readable medium;image;machine learning;sensor;sparse matrix	Quan Wang;Xinchi Zhang;Meng Wang;Kim L. Boyer	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.347	computer vision;computer graphics (images)	Robotics	45.882062122898915	-39.49240407944899	47563
394d2eddc83f82d8e1a08d1cec28f502ba1950b4	application of panoramic annular lens for motion analysis tasks: surveillance and smoke detection	motion analysis;lenses motion analysis cameras pixel control systems monitoring optical devices image converters image analysis image motion analysis;qa75 electronic computers computer science szamitastechnika;control systems;image recognition;image motion analysis;szamitogeptudomany;panoramic image acquisition;surveillance;image converters;low resolution;low resolution image panoramic annular lens motion estimation surveillance smoke detection panoramic image acquisition annular rectangular image conversion;motion estimation;smoke detection;computer vision;monitoring system;panoramic annular lens;image recognition surveillance lenses computer vision motion estimation;monitoring;pixel;lenses;image analysis;panoramic image;annular rectangular image conversion;cameras;low resolution image;optical devices	In this paper some applications of motion analysis are investigated for a compact panoramic optical system (Panoramic Annular Lens). Panoramic image acquisition makes multiple or mechanically controlled camera systems needless for many applications. Panoramic Annular Lens’ main advantage to other omnidirectional monitoring systems is that it is a cheap, small, compact device with no external hyperboloidal, spherical, conical or paraboloidal reflecting surface as in other panoramic optical devices. By converting the annular image captured with an NTSC camera to a rectangular one, we get a low-resolution (cc. 2.8 pixels/degree horizontally and 3 pixels/degree vertically) image. We have developed algorithms, which can analyze this low-resolution image to yield motion information for surveillance and smoke detection.	algorithm;ntsc;pixel;radio spectrum scope	Iván Kopilovic;Balázs Vágvölgyi;Tamás Szirányi	2000		10.1109/ICPR.2000.903017	computer vision;image analysis;image resolution;computer science;motion estimation;lens;pixel;computer graphics (images)	Vision	44.808653286926045	-41.0553792987701	47571
73a60905d96e1406feb6482a505fc2464a46b816	generalized transitive distance with minimum spanning random forest	会议论文	Transitive distance is an ultrametric with elegant properties for clustering. Conventional transitive distance can be found by referring to the minimum spanning tree (MST). We show that such distance metric can be generalized onto a minimum spanning random forest (MSRF) with element-wise max pooling over the set of transitive distance matrices from an MSRF. Our proposed approach is both intuitively reasonable and theoretically attractive. Intuitively, max pooling alleviates undesired short links with single MST when noise is present. Theoretically, one can see that the distance metric obtained max pooling is still an ultrametric, rendering many good clustering properties. Comprehensive experiments on data clustering and image segmentation show that MSRF with max pooling improves the clustering performance over single MST and achieves state of the art performance on the Berkeley Segmentation Dataset.	cluster analysis;convolutional neural network;diversification (finance);experiment;file spanning;image segmentation;minimum spanning tree;random forest;uniform theory of diffraction	Zhiding Yu;Weiyang Liu;Wenbo Liu;Xi Peng;Zhuo Hui;B. V. K. Vijaya Kumar	2015			combinatorics;discrete mathematics;computer science;machine learning;mathematics	Vision	25.980386134298804	-40.31452039053024	47582
3ab3b3ff89c316cc586b82c197f7e9723c0a4a8f	ego-motion recovery and robust tilt estimation for planar motion using several homographies	planar motion;slam;datorseende och robotik autonoma system;navigation;estimation;homography;simultaneous localization and mapping;tilt estimation;robustness;matematik;robot vision systems;robotic navigation;cameras	In this paper we suggest an improvement to a recent algorithm for estimating the pose and ego-motion of a camera which is constrained to planar motion at a constant height above the floor, with a constant tilt. Such motion is common in robotics applications where a camera is mounted onto a mobile platform and directed towards the floor. Due to the planar nature of the scene, images taken with such a camera will be related by a planar homography, which may be used to extract the ego-motion and camera pose. Earlier algorithms for this particular kind of motion were not concerned with determining the tilt of the camera, focusing instead on recovering only the motion. Estimating the tilt is a necessary step in order to create a rectified map for a SLAM system. Our contribution extends the aforementioned recent method, and we demonstrate that our enhanced algorithm gives more accurate estimates of the motion parameters.	algorithm;homography (computer vision);mobile operating system;robotics;simultaneous localization and mapping	Mårten Wadenbäck;Anders Heyden	2014	2014 International Conference on Computer Vision Theory and Applications (VISAPP)	10.5220/0004744706350639	computer vision;estimation;navigation;simulation;homography;computer science;motion estimation;geometry;motion field;statistics;robustness;computer graphics (images);simultaneous localization and mapping	Robotics	53.14353560958321	-48.31466867517562	47607
d012e1aebe04857abed5326f33ed12e778501159	optimal fuzzy partitions: a heuristic for estimating the parameters in a mixture of normal distributions	unsupervised learning;fuzzy set;mixed normal distributions;maximum likelihood;normal distribution;multivariate normal;k means;fuzzy sets;fuzzy sets maximum likelihood mixed normal distributions parametric estimation pattern classification unsupervised learning;parametric estimation;pattern classification;maximum likelihood method	An algorithm is described for generating fuzzy partitions which extremize a fuzzy extension of the k-means squared-error criterion function on finite data sets X. It is shown how this algorithm may be applied to the problem of estimating the parameters (a priori probabilities, means, and covariances) of mixture of multivariate normal densities, given a finite sample X drawn from the mixture. The behavior of the algorithm is compared with that of the ordinary ISODATA clustering process and the maximum likelihood method, for a specific bivariate mixture.	algorithm;bivariate data;cluster analysis;heuristic;k-means clustering;loss function	James C. Bezdek;Joseph C. Dunn	1975	IEEE Transactions on Computers	10.1109/T-C.1975.224317	unsupervised learning;econometrics;expectation–maximization algorithm;computer science;pattern recognition;mathematics;generalized normal distribution;maximum likelihood;fuzzy set;maximum likelihood sequence estimation;statistics	Vision	31.254964874152957	-25.349651481852256	47621
27d3b70f0834fcf21ff581a4fcb1eba6fbabcc15	learning and regularizing motion models for enhancing particle filter-based target tracking	probabilistic motion model;common datasets;empirical distribution;data-driven probabilistic motion model;sampling distribution;hybrid discrete-continuous;particle filter-based target tracking;target tracking;regularizing motion model;particle filter framework;local motion	This paper describes an original strategy for using a datadriven probabilistic motion model into particle filter-based target tracking on video streams. Such a model is based on the local motion observed by the camera during a learning phase. Given that the initial, empirical distribution may be incomplete and noisy, we regularize it in a second phase. The hybrid discrete-continuous probabilistic motion model learned this way is then used as a sampling distribution in a particle filter framework for target tracking. We present promising results for this approach in some common datasets used as benchmarks for visual surveillance tracking algorithms.	algorithm;benchmark (computing);matrix regularization;particle filter;sampling (signal processing);streaming media;velocity (software development);video tracking	Francisco Madrigal;Mariano Rivera;Jean-Bernard Hayet	2011		10.1007/978-3-642-25346-1_26	computer vision;simulation;machine learning	Vision	46.156474399679915	-47.77006624702904	47733
aea81017c4fb7b9adef21adaf776257f4a790823	a massively parallel deep rule-based ensemble classifier for remote sensing scenes		In this letter, we propose a new approach for remote sensing scene classification by creating an ensemble of the recently introduced massively parallel deep (fuzzy) rule-based (DRB) classifiers trained with different levels of spatial information separately. Each DRB classifier consists of a massively parallel set of human-interpretable, transparent zero-order fuzzy IF…THEN… rules with a prototype-based nature. The DRB classifier can self-organize “from scratch” and self-evolve its structure. By employing the pretrained deep convolution neural network as the feature descriptor, the proposed DRB ensemble is able to exhibit human-level performance through a transparent and parallelizable training process. Numerical examples using benchmark data set demonstrate the superior accuracy of the proposed approach together with human-interpretable fuzzy rules autonomously generated by the DRB classifier.	artificial neural network;benchmark (computing);certified digital radio broadcast specialist;convolution;ensemble kalman filter;ensemble learning;fuzzy rule;logic programming;prototype;self-organization;visual descriptor	Xiaowei Gu;Plamen P. Angelov;Chao Zhang;Peter M. Atkinson	2018	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2017.2787421	remote sensing;massively parallel;parallelizable manifold;convolutional neural network;fuzzy logic;mathematics;rule-based system;feature extraction;image segmentation;spatial analysis	ML	31.901296157149297	-44.6962886965334	47735
57b6cda422a76f4e95c508358b7b22dfd748a944	feature extraction and classification of ocean oil spill based on sar image	filtering;oceans;neural network feature extraction ocean oil spill classification sar image synthetic aperture radar image 2d otsu algorithm image segmentation;image segmentation;synthetic aperture radar feature extraction geophysical image processing image classification image segmentation neural nets oceanographic techniques remote sensing by radar;neural network ocean oil spill sar 2 d otsu;oils;feature extraction;classification algorithms;oils image segmentation feature extraction classification algorithms oceans synthetic aperture radar filtering;synthetic aperture radar	The detection of ocean oil spill based on synthetic aperture radar (SAR) image has been a hot topic attracting extensive attention. In this paper, a hybrid scheme, in which we extract feature parameters and then achieve classification as follows, is presented. Two-dimensional (2-D) Otsu algorithm is applied in image segmentation process, and neural network is applied in classification course. Before image segmentation, a sort of universal processing is used, and it enables 2-D Otsu algorithm to be more applicable to SAR images of ocean oil spill.	algorithm;aperture (software);artificial neural network;feature extraction;image segmentation;otsu's method;synthetic data	Peng Zhao;Xun Yang;Yan Chen;Ling Tong;Lei He	2016	2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2016.7729380	filter;statistical classification;computer vision;synthetic aperture radar;feature extraction;computer science;machine learning;pattern recognition;image segmentation;scale-space segmentation;remote sensing	Robotics	32.43753482760798	-43.94267139173743	47887
f23573d34797d98bc635790603242ca18a3cbdb3	split-and-match: a bayesian framework for vehicle re-identification in road tunnels	tunnel surveillance;multicamera tracking;vehicle matching;surveillance;algorithm;non overlapping cameras;technology and engineering;identification;nonoverlapping cameras;trace transform;tracking	Vehicle re-identification is key to keep track of vehicles monitored by a multicamera network with non-overlapping views. In this paper, we propose a probabilistic framework based on a two-step strategy that re-identifies vehicles in road tunnels. The first step consists of splitting the re-identification problem by connecting groups of vehicles observed in different cameras using certain motion and appearance criteria. In the second step, we build a Bayesian model that finds the optimal assignment between vehicles of connected groups. Descriptors like trace transform signatures, lane change, and motion discrepancies are used to derive our probabilistic framework. Experimental tests reveal that connected groups derived from the first step are composed of 4 vehicles on average. This allow us to constrain the number of candidate matches and increase the chances of getting the correct match. In the second step, our Bayesian model succeeds in matching vehicles among candidates with very similar appearance and under uneven illumination conditions. In general, our system reports a reidentification accuracy of 92% using a nearest-neighbor matcher, and 98% using a one-to-one matcher. These results outperform previous works and encourage us to further develop our solution for other re-identification applications.	bayesian network;discrepancy function;one-to-one (data model);radio-frequency identification;sensor;type signature	Andrés Frias-Velazquez;Peter Van Hese;Aleksandra Pizurica;Wilfried Philips	2015	Eng. Appl. of AI	10.1016/j.engappai.2015.06.024	identification;computer vision;simulation;computer science;tracking	Vision	42.47498141676605	-47.07270916639022	47931
315600af9dd5759101ea31d6d8fb10ca6c5366fe	accurate dynamic scene model for moving object detection	gaussian processes;indexing terms;adaptive pixel wise gaussian mixture model accurate dynamic scene model moving object detection;computer vision;gaussian mixture model;moving object detection gaussian mixture model background subtraction;background subtraction;moving object detection;layout object detection cameras gaussian distribution lighting laboratories information processing electronic mail face detection surveillance;object detection;object detection computer vision gaussian processes;dynamic scenes	Adaptive pixel-wise Gaussian mixture model (GMM) is a popular method to model dynamic scenes viewed by a fixed camera. However, it is not a trivial problem for GMM to capture the accurate mean and variance of a complex pixel. This paper presents a two-layer Gaussian mixture model (TLGMM) of dynamic scenes for moving object detection. The first layer, namely real model, deals with gradually changing pixels specially; the second layer, called on-ready model, focuses on those pixels changing significantly and irregularly. TLGMM can represent dynamic scenes more accurately and effectively. Additionally, a long term and a short term variance are taken into account to alleviate the transparent problems faced by pixel-based methods.	google map maker;mixture model;object detection;pixel	Hong Yang;Yihua Tan;Jin-Wen Tian;Jian Liu	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379545	computer vision;object-class detection;index term;background subtraction;computer science;viola–jones object detection framework;pattern recognition;mixture model;gaussian process;computer graphics (images)	Vision	44.204777367260306	-50.146773783772524	47986
fbc25f97418e5de932d9a40a280d7fe54caf8ab0	ideal code constrained supervised sparse coding	class labels;ksvd;codebook;sparse coding	In this paper, we proposed a novel sparse coding algorithm by using the class labels to constrain the learning of codebook and sparse code. We not only use the class label to train the classifier, but also use it to construct class conditional codewords to make the sparse code as discriminative as possible. We first construct ideal sparse codes with regarding to the class conditional codewords, and then constrain the learned sparse codes to the ideal sparse codes. We proposed a novel loss function composed of parse reconstruction error, classification error, and the ideal sparse code constrain error. This problem can be optimized by using the transitional KSVD method. In this way, we may learn a discriminative classifier and a discriminative codebook simultaneously. Moreover, using this codebook, the learnt the sparse codes of the same class are similar to each other. Finally, exhaustive experimental results show that the proposed algorithm outperforms other sparse coding methods.	algorithm;code word;codebook;discriminative model;k-svd;loss function;neural coding;parsing;pattern recognition;singular value decomposition;sparse matrix;statistical classification	Wenjing Liao;Robert Williams	2014	JCP	10.4304/jcp.9.4.836-844	k-svd;computer science;theoretical computer science;machine learning;codebook;pattern recognition;sparse approximation;neural coding	AI	25.339084507123705	-45.18496930479148	48032
d84bcc91ae4089b8a34ce64be4b751b3cd10b4a2	image classification via multi canonical correlation analysis	kernel;image classification correlation signal processing conferences feature extraction abstracts kernel;image classification;image classification canonical correlation analysis multi canonical correlation analysis multi linear discriminant analysis feature extraction;abstracts;feature extraction;signal processing;correlation;conferences	This work investigates the role of canonical correlations analysis in image classification problems. Canonical correlation analysis is proposed as an alternative feature selection and reduction method for generic image classification problems. This new method is studied via various image classification problems in comparison with principal components and kernel principal components analysis. Multiple canonical correlation analysis is proposed as a new feature selection and dimension reduction algorithm for image classification problems involving multiple classes. Classification performance and relationship between the extracted image attributes and classification performance are studied by using Caltech 101 dataset.	algorithm;caltech 101;computer vision;dimensionality reduction;feature selection;principal component analysis	Mehmet Cem Catalbas;Yakup S. Ozkazanç	2014	2014 22nd Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2014.6830403	computer vision;contextual image classification;kernel;feature extraction;computer science;machine learning;linear classifier;signal processing;pattern recognition;data mining;mathematics;k-nearest neighbors algorithm;correlation;feature;dimensionality reduction	ML	28.41770516448845	-42.92780152125905	48037
011d9368c1e11f145e569ed92fc1b8a41741a0bb	pose determination of human head using one feature point based on head movement	face recognition;yaw angle;face detection;distance learning;head;videoconference;information technology;videoconferencing	Pose determination of the human head is vital in many applications, such as face recognition, face re-orientation in videoconferencing, distance learning, etc. The paper suggests a simple and effective approach for pose determination based on head movement. Our approach is designed for video-based situations, i.e. it takes the advantage of the availability of the previous frame and the initial pose. It needs one feature point to determine the pitch angle, the yaw angle and the roll angle. The number of feature points to be tracked is significantly less than in many other approaches in this area. It is fast and efficient, and produces a reasonable accuracy	facial recognition system;pitch (music);yaws	Ben Yip;W. Y. Siu;Jesse S. Jin	2004	2004 IEEE International Conference on Multimedia and Expo (ICME) (IEEE Cat. No.04TH8763)		computer vision;simulation;speech recognition;3d pose estimation;telecommunications;computer science;articulated body pose estimation;videoconferencing;information technology	Robotics	47.043475960222935	-45.55103420438665	48136
7627ddf1a676dd28a10e9f633d01611a1ab4a250	past, present, and future approaches using computer vision for animal re-identification from camera trap data		1. The ability of a researcher to re-identify (re-ID) an individual animal upon re-encounter is fundamental for addressing a broad range of questions in the study of ecosystem function, community and population dynamics, and behavioural ecology. Tagging animals during mark and recapture studies is the most common method for reliable animal re-ID however camera traps are a desirable alternative, requiring less labour, much less intrusion, and prolonged and continuous monitoring into an environment. Despite these advantages, the analyses of camera traps and video for re-ID by humans are criticized for their biases related to human judgment and inconsistencies between analyses. 2. In this review, we describe a brief history of camera traps for re-ID, present a collection of computer vision feature engineering methodologies previously used for animal re-ID, provide an introduction to the underlying mechanisms of deep learning relevant to animal re-ID, highlight the success of deep learning methods for human re-ID, describe the few ecological studies currently utilizing deep learning for camera trap analyses, and our predictions for near future methodologies based on the rapid development of deep learning methods. 3. For decades ecologists with expertise in computer vision have successfully utilized feature engineering to extract meaningful features from camera trap images to improve the statistical rigor of individual comparisons and remove human bias from their camera trap analyses. Recent years have witnessed the emergence of deep learning systems which have demonstrated the accurate re-ID of humans based on image and video data with near perfect accuracy. Despite this success, ecologists have yet to utilize these approaches for animal re-ID. 4. By utilizing novel deep learning methods for object detection and similarity comparisons, ecologists can extract animals from an image/video data and train deep learning classifiers to re-ID animal individuals beyond the capabilities of a human observer. This methodology will allow ecologists with camera/video trap data to re-identify individuals that exit and re-enter the camera frame. Our expectation is that this is just the beginning of a major trend that could stand to revolutionize the analysis of camera trap data and, ultimately, our approach to animal ecology.	algorithm;camera phone;computer vision;deep learning;ecology;ecosystem;emergence;feature engineering;feature extraction;mark and recapture;object detection;population dynamics;population informatics	Stefan Schneider;Graham W. Taylor;Stefan Linquist;Stefan C. Kremer	2018	CoRR			AI	39.43573875607852	-42.67954854081786	48137
bfe60f8e488aba6dcccaeaaea58a579b6d838d09	tree-guided mcmc inference for normalized random measure mixture models		Normalized random measures (NRMs) provide a broad class of discrete random measures that are often used as priors for Bayesian nonparametric models. Dirichlet process is a well-known example of NRMs. Most of posterior inference methods for NRM mixture models rely on MCMC methods since they are easy to implement and their convergence is well studied. However, MCMC often suffers from slow convergence when the acceptance rate is low. Tree-based inference is an alternative deterministic posterior inference method, where Bayesian hierarchical clustering (BHC) or incremental Bayesian hierarchical clustering (IBHC) have been developed for DP or NRM mixture (NRMM) models, respectively. Although IBHC is a promising method for posterior inference for NRMM models due to its efficiency and applicability to online inference, its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made. In this paper, we present a hybrid inference algorithm for NRMM models, which combines the merits of both MCMC and IBHC. Trees built by IBHC outlines partitions of data, which guides Metropolis-Hastings procedure to employ appropriate proposals. Inheriting the nature of MCMC, our tree-guided MCMC (tgMCMC) is guaranteed to converge, and enjoys the fast convergence thanks to the effective proposals guided by trees. Experiments on both synthetic and realworld datasets demonstrate the benefit of our method.	bayesian network;binary tree;cluster analysis;converge;heuristic (computer science);hierarchical clustering;markov chain monte carlo;metropolis;metropolis–hastings algorithm;mixture model;monte carlo method;sampling (signal processing);synthetic intelligence	Juho Lee;Seungjin Choi	2015			econometrics;frequentist inference;machine learning;pattern recognition;mathematics;statistics	ML	28.45739759947286	-28.735833010857903	48170
24c66f9fa9a7aafdc736fc5b14729ecd7c1f0b49	ica for position and pose measurement from images with occlusion	robots image recognition;image recognition;independent component analysis;principal component analysis;robots;independent component	A method employing independent component analysis (ICA) to measure the position of a camera and the pose of an object from images in the presence of occlusion is presented. ICA is used to provide a low-dimensional representation of a set of images taken throughout a range of camera positions and object poses. Using the low-dimensional independent component subspace arbitrary camera locations or object poses can then be determined. The ICA technique is compared with principal component analysis (PCA) in the presence of varying degrees of occlusion. Occlusions of translation, pan and pose images were experimentally applied to provide a demonstration of the performance of this method. The independent component subspace is shown to provide more accurate position and pose information than PCA.	experiment;hidden surface determination;independent computing architecture;independent component analysis;principal component analysis	Jeff Fortuna;David W. Capson	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5745435	robot;independent component analysis;computer vision;speech recognition;computer science;machine learning;pattern recognition;principal component analysis	Robotics	42.511923125383966	-51.166211935317186	48202
