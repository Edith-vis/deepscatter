id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
d2f8fbb83c08aa28fc7c195456b63b5499f28b3b	modeling knowledge intensive processes: concepts, methods, and applications - introduction.		As we move further into the information age, knowledge is increasingly becoming a critical component in the competitive success of firms. As markets shift, technologies proliferate, competitors multiply and products acquire rapid obsolescence, successful companies rely on their ability to consistently create new knowledge, disseminate it quickly, and embody it in new products and services. As firms shift from a product centric form to a knowledge centric form, it becomes essential to support various dimensions of knowledge as a critical asset.		Kishore Sengupta;Balasubramaniam Ramesh	2000		10.1109/HICSS.2000.926677	management science	AI	-77.64929713816602	5.152800837749739	33180
03879f204af8aebcf524f3b40d033f7a412cc203	a model for data quality assessment	data quality firewall;data quality assessment;failure rate;data quality filter;data quality;information system;data warehouse;data lineage;business process;type instance	One of the major causes for the failure of information systems to deliver can be attributed to data quality. Gartner’s figures and other similar studies show the failure rate hovering at a plateau of 50% for data warehouses since 2004. While the true cause of poor data quality can be attributed to a lack of supporting business processes, insufficient analysis techniques, along with protecting oneself with the introduction of data quality firewalls for incoming data, the question has to be raised as to whether a data quality assessment of the existing data would be worthwhile or plausible? This paper defines a data quality assessment model that enables a methodology to assess data quality and assign ratings using a score-card approach. A by-product of this model helps establish ‘sluice gate’ parameters to allow data to pass through data quality filters and data quality firewalls.	business process;data quality;failure rate;firewall (computing);information system	Baba Piprani;Denise Ernst	2008		10.1007/978-3-540-88875-8_99	reliability engineering;data quality;engineering;data warehouse;data mining;database;data efficiency;information quality	DB	-71.75168216142411	11.743639368654504	33233
dfa2e09b716ed4cbc139609fc517b12f11482706	us double-stack rail technology and infrastructure: corollaries for china	railroad electrification;car clearances railroads;railroad cars;technological innovations;doublestack service;socioeconomic development;rail technology;railcar structural components;double stack;railroad transportation;clearance;china;containerization;size and weight regulations;rail infrastructure	This article describes the current state of double-stacking (railway transportation) in the United States. The authors consider the corollaries that might be useful for understanding the double-stack network implementation issues currently confronted by China. After a synopsis of the advantages of double-stacking in the US, the authors discuss the types of domestic double-stack infrastructure constraints and several important developing international trends, focusing specifically on the issues associated with the adoption and implementation of double-stack trains in China. The basic conditions of the Chinese double-stack railway infrastructure (i.e., clearance restrictions along bridges, tunnels and overpasses and the limitation of the maximum weight per axle) are then compared to those of the U.S. The authors contend that the U.S. railway transportation system faced many of the same constraints that the Chinese are now addressing, such as clearance constraints, marketing difficulty and terminal efficiency. However, the story of the adoption of double-stack transportation in China will probably turn out to be somewhat different than that of the US. Only non-electrified rail routes are really suitable for double-stack transportation in China, since the removal of clearance constraints due to the electrification is prohibitive. Moreover, the Chinese weight per axle restriction requires a new double-stack car design with less car weight. The authors conclude that, even with these constraints, adoption of double-stack rail transportation systems in China will be easier than that experienced in the US, since the Chinese can learn from the US in terms of technology and management.		Yu Meng;Debbie A. Niemeier	2000	IJSTM	10.1504/IJSTM.2000.001575	economics;clearance;operations management;socioeconomic development;china	HCI	-71.53899821599627	4.3787530752772135	33275
e9cff97c4172327e1efd40896824be5785f6892b	do formal methods really belong in the toolbox of the practicing engineer?	reliability engineering;software design airplanes design engineering reliability engineering power engineering and energy software safety formal verification resists software performance software systems;design engineering;software systems;resists;software performance;power engineering and energy;formal verification;software safety;airplanes;software design	The purpose of this panel is to present different perspectives and opinions regarding the issues associated with the acceptance and use of formal methods by practicing engineers.	formal methods	Lawrence A. King	2000		10.1109/HASE.2000.895451	reliability engineering;personal software process;verification and validation;formal methods;software engineering process group;software sizing;software performance testing;formal verification;software verification;search-based software engineering;computer science;systems engineering;engineering;software design;social software engineering;software reliability testing;component-based software engineering;software development;software engineering;software construction;resist;software walkthrough;programming language;software deployment;software requirements;software system;computer engineering	NLP	-63.38316559807377	27.394259381649313	33301
31ec1a443a5f68a62333ca4c3a87cace18715309	fast and reliable order management design using a qualitative approach	production planning and control	Abrupt and often surprising changes characterize the situation of manufacturing companies. Important for an order management design are those factors which potentially cause turbulence and lead to schedule deviations. The paper describes a method to capture and assess them qualitatively. Based on an analogy to physics, the morphology of turbulence germs is provided. Then, a procedure is described how to successfully transfer approaches for turbulence management to other companies. The last part reflects in detail on the application experience.	mathematical morphology;order management system;turbulence	Hans-Hermann Wiendahl	2007		10.1007/978-0-387-74157-4_28	simulation;engineering;operations management;management science	Robotics	-77.67349503084617	12.037513677624242	33326
5f99287a4f3a19cb22c6a9f86545da70a15fefd0	a lightweight measurement of software security skills, usage and training needs in agile teams		Although most organizations understand the need for application security at an abstract level, achieving adequate software security at the sharp end requires taking bold steps to address security practices within the organization. In the Agile software development world, a security engineering process is unacceptable if it is perceived to run counter to the agile values, and agile teams have thus approached software security activities in their own way. To improve security within agile settings requires that management understands the current practices of software security activities within their agile teams. In this study, the authors have used a survey instrument to investigate software security usage, competence, and training needs in two agile organizations. They find that 1 The two organizations perform differently in terms of core software security activities, but are similar when secondary activities that could be leveraged for security are considered 2 regardless of cost or benefit, skill drives the kind of activities that are performed 3 Secure design is expressed as the most important training need by all groups in both organizations 4 Effective software security adoption in agile setting is not automatic, it requires a driver.	agile software development;application security	Tosin Daniel Oyetoyan;Martin Gilje Jaatun;Daniela Cruzes	2017	IJSSE	10.4018/IJSSE.2017010101	simulation;agile usability engineering;knowledge management;empirical process;lean software development	Mobile	-70.01382501206174	21.286658266596103	33374
4d7235ccaf622f10a3506209623b66088eaddf8c	knowledge management system model in enhancing knowledge facilitation of software process improvement for software house organization	software;communication process;learning process;documentation process knowledge management system model knowledge facilitation enhancement software process improvement software house organization spi sho knowledge promotion community of practice cop software development environment kms communication process learning process sharing process;spi;community of practice;software house organization;software process improvement knowledge management organisational aspects software development management software houses;sharing process;software process improvement;standards organizations;knowledge facilitation;knowledge management system model;software house organization knowledge management knowledge management system knowledge facilitation software process improvement;best practice;knowledge management;kms;documentation process;software houses;sho;software development environment;knowledge promotion;knowledge management system;interviews;knowledge facilitation enhancement;process improvement;organizations;cop;software organizations knowledge management standards organizations programming context interviews;programming;context;software development management;organisational aspects	Software Process Improvement (SPI) is necessity to any Software House Organization (SHO). Process improvement requires embedding the knowledge into SHO's practice and structure especially in promoting and facilitating knowledge of best practice in SPI for the benefits of members of community of practice (CoP). Hence, in this paper, knowledge management (KM) and knowledge facilitation are investigated in order to find practical means to reach process improvement regarding with software development environment. In particular this study, it is also concerned with the process of Knowledge Management System (KMS) in supporting the knowledge facilitation of SPI for SHO. In order to understand the support process, we have studied how KM processes are practiced within SHO which includes communication, learning, sharing and documentation process. The findings explain on how the KMS has been practiced and influence the SPI in SHO. This result indicates that KM processes in SHO are being undertaken in a very informal manner, which has caused SHO to be more informal in their KM processes specifically and SPI generally.	best practice;cloud computing;documentation;integrated development environment;interconnection;knowledge management;management system;mobile computing;network computer;scalability;smoothing;software development;software house	Rusli Abdullah;Amir Mohamed Talib	2012	2012 International Conference on Information Retrieval & Knowledge Management	10.1109/InfRKM.2012.6205036	systems engineering;engineering;knowledge management;operations management	SE	-73.80832489591896	13.278742388944035	33479
4c43eb769480a5730d5bbe566eb11852e1c47a90	cooperation royalty contract design in research and development alliances: help vs. knowledge-sharing		Abstract We study a cooperation royalty contract design problem in a three-party research and development (RD (ii) directly help innovator B by exerting effort during the development stage (the help contract); or (iii) share task-related knowledge with innovator B that would lower the latter’s costs (the knowledge-sharing contract). To capture the inherent incentive alignment problems under moral hazard and evaluate the efficiency of cooperation, we compare the efficiency of the non-cooperation contract, the help contract and the knowledge-sharing contract and identify the conditions under which the marketer benefits most. We find that when the efficiency of knowledge-sharing is high, regardless of how efficient help is, the marketer prefers the knowledge-sharing contract. Interestingly, when the efficiency of help is low and the efficiency of knowledge-sharing is moderate, the marketer prefers the help contract. In addition, when the efficiencies of both types of cooperation are low, the non-cooperation contract would benefit the marketer most. Furthermore, for an Ru0026D task with high technical risk, the best strategy for the marketer is to offer the knowledge-sharing contract.		Xinning Yu;Yanfei Lan;Ruiqing Zhao	2018	European Journal of Operational Research	10.1016/j.ejor.2018.01.053	industrial organization;operations management;mathematics;incentive;innovator;knowledge sharing;moral hazard	Robotics	-81.96857576033364	5.190959892350669	33481
9d83d9cd90f3575e21e5d4ba79f95920cec064c9	an enterprise collaborative management system - a case study of supplier relationship management	cycle time;supplier relationship management;management system;buyer seller relationships;case base reasoning;manufacturing resource planning;product code;enterprise application integration;new product development;management techniques;supply chain management;supplier selection	In this paper, a server-based enterprise collaborative management system using enterprise application integration technology is developed for trial implementation at Honeywell Consumer Products (Hong Kong) Limited, in the area of supplier relationship management. The system facilitates supplier selection using an integrative case-based supplier selection and help desk approach to select the most appropriate suppliers, based on their past performance records from a case-based warehouse. Discusses a case study to integrate Honeywell’s supplier rating system and product coding system by case-based reasoning technique to select preferred suppliers during the new product development process. Finds that the outsource cycle time from the searching of potential suppliers to the allocation of orders is greatly reduced while performance of suppliers can be monitored simultaneously.		King Lun Choy;W. B. Lee;Victor Lo	2004	J. Enterprise Inf. Management	10.1108/17410390410531443	enterprise relationship management;supply chain management;enterprise application integration;economics;cycle time variation;computer science;systems engineering;marketing;operations management;digital firm;management system;universal product code;supplier relationship management;new product development	AI	-68.2948544478626	4.546417197798451	33547
c8fdbc90ac323b0fb1ed8cfa9724c69142283a1c	organization and efficiency in the international insurance industry: a cross-frontier analysis	organization;cross frontier analysis;efficient structure hypothesis;data envelopment analysis;expense preference hypothesis	This paper employs cross-frontier analysis, an innovative tool based on data envelopment analysis, to provide new insight into the relationship between organization and efficiency in international insurance markets. We are the first to empirically test the expense preference hypothesis and the efficient structure hypothesis in a large cross-country study. For this purpose, we consider 23,807 firm-years for 21 countries from northern America and the European Union—a dataset not previously analyzed in this context. We find evidence for the efficient structure hypothesis in selected market segments, but we find no evidence for the expense preference hypothesis. Our results provide insight into the competitiveness of stock and mutual insurers from different countries. At the country level, the results can be used to compare different insurance markets. Our findings are especially interesting for the strategic management of insurance companies as well as for regulators and boards of national insurance associations. 2012 Elsevier B.V. All rights reserved.	capability maturity model;coexist (image);competitive analysis (online algorithm);converge;cost efficiency;data envelopment analysis;strategic management;word lists by frequency	Christian Biener;Martin Eling	2012	European Journal of Operational Research	10.1016/j.ejor.2012.03.037	actuarial science;economics;organization;marketing;international trade;data envelopment analysis;commerce	Metrics	-85.29141157981395	6.114194873520294	33575
c6368cc0c955b59f56abe541c8df346da55a4ba0	a state-of-the-art of empirical literature of crowdsourcing in computing	software;outsourcing;market research;systematics;empirical;software engineering;computing;systematic mapping study;context;crowdsourcing	This paper aims at representing a state-of-the-art of crowdsourcing, along with various trends and opportunities of empirical research of crowdsourcing in computing field of study. A systematic mapping study (SMS) methodology has been employed to synthesize the empirical work done in crowdsourcing. This paper is based on the results generated from 400 primary studies of the SMS. The initial results of this SMS suggest that the crowdsourcing in Computing is not a field in its inception phase, rather, this field has matured considerably, with in less than a decade of empirical research. The paper highlights important trends in empirical crowdsourcing research along with gaps and opportunities for the researchers.	crowdsourcing	Talat Ambreen;Naveed Ikram	2016	2016 IEEE 11th International Conference on Global Software Engineering (ICGSE)	10.1109/ICGSE.2016.37	market research;computing;economics;computer science;engineering;data science;data mining;systematics;world wide web;crowdsourcing;outsourcing	SE	-66.79599293096544	23.566369863659503	33608
44ec90cac2c45b77224044859d0c9f8fce139f0a	perspectives on risk and the unforeseen	uncertainty;risk management;black swan	Black swans have been discussed lately. Some recent contributions to the understanding of black swans have been provided by Aven [1] and Aven and Krohn [3]. It is important to be aware of events which may come as complete surprise, which creates uncertainty in the risk assessment. This is consistent with the proposed revision of the definition of risk by Petroleum Safety Authority (PSA) [Norway]. But we should at the same time also try to look beyond this and see how we can use this concept to avoid serious accidents or at least reduce the consequences should accidents occur. In this paper we proposed to restrict the black swan concept to unknown unknowns. Cases illustrate how the wider definition may lead to misleading and unfortunate effects that will not lead to good risk management practices. The wider definitions proposed by Aven & Krohn are proposed to be counterproductive if seen in a risk	krohn–rhodes theory;risk assessment;risk management	Stein Haugen;Jan Erik Vinnem	2015	Rel. Eng. & Sys. Safety	10.1016/j.ress.2014.12.009	uncertainty;risk management;black swan theory;engineering;mathematics;forensic engineering;operations research;statistics	ML	-64.0288122135163	6.836497384527528	33773
73df62087211a8a3aa8a89c34d36919870fe1f46	message from the ams chairs		Netflix, Amazon, The Guardian and other companies have evolved their applications towards the promising and challenging style of microservice architectures (MSAs). MSA arises from the broader area of Service Oriented Architecture and focuses on specific aspects, such as componentization of small services, application of agile practices for development, deployment and testing of services, usage of infrastructure automation with continuous delivery features, decentralized data management and decentralized service governance. The goal of AMS 2017 is to gather researchers and practitioners to share challenges, solutions, and reflections on the frontiers of architecting with microservices. AMS solicits contributions from both academic and industrial participants, thus fostering active synergy between the two communities.	agile software development;amiga reflections;continuous delivery;microservices;office chair;service-oriented architecture;software deployment;synergy;vhdl-ams;verilog-ams	Patricia Lago;Joost Bosman	2017	2017 IEEE International Conference on Software Architecture Workshops (ICSAW)	10.1109/ICSAW.2017.70	service-oriented modeling;cloud testing;cloud computing;service-oriented architecture;computer science;distributed computing	Visualization	-67.33941407099456	15.402688811010517	33774
49e978130f1b3ffe924c6b10cf4603c710c79847	an information system design theory for an rfid-based healthcare management system	information system design theory;healthcare management system;radio frequency identification rfid	This study is a design science research which describes the design of a radio frequency identification (RFID) based Healthcare Management System (RHMS) for the healthcare industry. In this study, a prototype RHMS has been designed and developed. The evaluations results validate the practical viability of the proposed architecture. We have described an Information System Design Theory for the RHMS which can form a basis for further research. We hope that the lessons learned from this study help support and further the efforts of academicians, researchers and practitioners in RFID based healthcare management system research.	information system;management system;prototype;radio frequency;radio-frequency identification	Eric W. T. Ngai;F. F. C. Suk;C. C. Ng	2008			knowledge management;environmental resource management;management information systems;risk management information systems	HCI	-68.97713073533082	9.318166008187289	33926
dc183973dc2d0393bd416edb4b3c2eba2246cb73	product line engineering	product life cycle management;product line;producao bibliografica artigos completos publicadosem periodicos;software development unified modeling language software reusability adaptation models product life cycle management;software reusability;software development;unified modeling language;software family ple reuse management variability management software development maturity life cycle cost it organization software organization information technology software product line engineering;variability software technology product line tools;tools;variability;adaptation models;software technology;software development management	Product line engineering (PLE) is one of the few industry-ready methods to manage reuse and variability in a defined way and thus bring software development maturity to a more advanced stage. The goal is to deliver specific product variants with fast cycle times at a manageable life-cycle cost with a defined quality level. Many IT and software organizations have started PLE but fail in industrializing the concepts and thus do not achieve sustainable benefits. Authors Klaus Schmid and Eduardo Santana de Almeida look at current technology for modeling and managing variation and thus facilitate PLE. The Web extra at http://youtu.be/R1gybFwAy10 is a video interview with David Weiss discussing the benefits of using software product line engineering to produce software families.	capability maturity model;heart rate variability;software development;software product line;world wide web	Klaus Schmid;Eduardo Santana de Almeida	2013	IEEE Software	10.1109/MS.2013.83	unified modeling language;reusability;personal software process;long-term support;verification and validation;software quality management;software sizing;software project management;computer science;systems engineering;engineering;package development process;social software engineering;operations management;component-based software engineering;software development;software engineering;software construction;software walkthrough;software analytics;software deployment;software development process;product life-cycle management;software quality;software quality analyst;software peer review	SE	-66.40700097449647	22.02631352530617	33933
4376eb77aa4ec06886302c304fc81bc02d4791e8	barriers to innovation in service smes: evidence from mexico	innovation;smes;services;article	Purpose – Specific research related to the study of innovation barriers in service SMEs in the Latin American region is limited. This study thus investigates the effects that external environmental, financial and human barriers have on innovation activities, particularly, within the context of Mexican service SMEs. Design/methodology/approach – Three hypotheses were formulated and tested using structural equation modelling (SEM). Data were collected through an instrument that was developed based on relevant constructs adapted from the literature. The instrument was validated using Confirmatory Factor Analysis, Cronbach’s alpha test and Composite Reliability Index to ensure the reliability of the theoretical model. The instrument was distributed among service SMEs in the Aguascalientes state of Mexico, from were 308 valid responses were obtained. Findings – In general, the results indicate that all of the three barriers investigated (i.e. external environmental, financial and human) hinder innovation in service SMEs, with the external environmental barrier being the most significant of the three. Practical implications – The findings of this research can inform managers of service SMEs and policy makers when formulating and implementing strategies to reduce innovation barriers. Originality/value – Evidence suggests that specific research related to the study of innovation barriers in service SMEs in the Latin American region is limited. This paper fills this research gap by expanding the limited body of knowledge in this field and providing further evidence on this phenomenon. The study also enables the distinctive characteristics of innovation barriers to be understood within a particular context, expanding in this way the body of knowledge on this field.		Gonzalo Maldonado-Guzmán;Jose Arturo Garza-Reyes;Sandra Yesenia Pinzón-Castro;Vikas Kumar	2017	Industrial Management and Data Systems	10.1108/IMDS-08-2016-0339	superconducting magnetic energy storage;innovation;service;economics;marketing;management;commerce	HCI	-84.01291277688941	4.301809620501028	33986
9bcef27bbf5d30ed150a2a02018b859de5b1bf77	a structural analysis of the effectiveness of buying firms' strategies to improve supplier performance	structural equation models;purchasing;and supply chain management;materials management;supply chain management;structure analysis	Many manufacturing firms have increased the amount of component parts and services they outsource, while refocusing on their core capabilities. Outsourcing parts and services to independent, external suppliers means that suppliers’ performance is increasingly critical to the long-term success of these buying firms. Buying firms are increasingly using disparate supplier development strategies to improve supplier performance including supplier assessment, providing incentives for improved performance, instigating competition among suppliers, and direct involvement of the buying firm’s personnel with suppliers through activities such as training of suppliers’ personnel. Using resource-based theory, internalization theory, and structural equation modeling, we examine the impact of these supplier development strategies on performance. We conclude that direct involvement activities, where the buying firm internalizes a significant amount of the supplier development effort, play a critical role in performance improvement. Subject Areas: Materials Management, Purchasing, Structural Equation Models, and Supply Chain Management.	outsourcing;purchasing;structural analysis;structural equation modeling	Daniel R. Krause;Thomas V. Scannell;Roger Calantone	2000	Decision Sciences	10.1111/j.1540-5915.2000.tb00923.x	supply chain risk management;structural equation modeling;supply chain management;economics;marketing;operations management;structural analysis;supply chain;supplier relationship management;materials management;commerce	AI	-81.43161337728057	5.4244200272834275	34000
247e89ac37b224d05f1193c43031f5d5301be7e9	the software aging and rejuvenation repository: http://openscience.us/repo/software-aging/	stress;market research;software measurement;data repository;software systems;aging;research artifacts;software reliability research and development;open access;software aging and rejuvenation;open access support sar research sarry software aging and rejuvenation repository;data repository software aging and rejuvenation research artifacts open access;aging software measurement market research stress software systems	While Software Aging and Rejuvenation (SAR) research has been steadily increasing, the artifacts related to SAR studies (such as software aging measurements and bug datasets) are seldom made available to researchers and practitioners, thus limiting potential improvements of rejuvenation solutions and their practical adoption. We discuss in this paper the role of artifacts in SAR research, and present SARRY (the Software Aging and Rejuvenation RepositorY), an open-access support for the SAR community to share research artifacts (available at http://openscience.us/repo/software-aging/). We invite researchers to contribute to SARRY, in order to aid future SAR research and to improve the visibility and impact of their work.	artifact (error);experiment;participatory culture;population;scheduling (computing);software aging;software bug;software system;stress testing	Domenico Cotroneo;Antonio Ken Iannillo;Roberto Natella;Roberto Pietrantuono;Stefano Russo	2015	2015 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)	10.1109/ISSREW.2015.7392054	market research;computer science;engineering;software engineering;software aging;database;stress;information repository;software measurement;world wide web;software system	SE	-65.3301038298565	31.297914974484538	34070
ee820e6f1bd81c87021401c14f67153a4bd5ef86	software size prediction before coding	software science;software metrics;empirical study;programming language;software systems;program length estimators;size prediction;software development;software metric	There have been many empirical studies conducted for the purpose of objective measurement of software systems. As a result, various 'Software Metrics' have been introduced by various researchers. At present, efforts are being made by software researchers to get qualitatively good software developed within a reasonable cost. The quality of a software product is codetermined be several factors. Most of these factors directly or indirectly depend on the objective measurement of the software characteristics.In this paper, we have applied one of our program length estimator (Ng) on a set of about 200 programs of varying complexities coded in four different programming languages for determining the language level, program volume and programming effort. The results of this program length estimator have also been used to find out the value of constant parameter used in Conte's software size equation for the respective languages for the purpose of estimating size of programs before coding.Further, these results have been used to estimate the size of the programs written in different languages. The estimated program size has then been comparatively analysed with that of the actual program size, which has been computed using our program length estimator (Ng).	programming language;software metric;software sizing;software system	Nasib S. Gill;P. S. Grover	2004	ACM SIGSOFT Software Engineering Notes	10.1145/1022494.1022514	reliability engineering;software sizing;computer science;engineering;theoretical computer science;software engineering;software construction;cosmic software sizing;programming language;software quality;software metric	SE	-65.83472809482494	31.547975055335442	34086
c0bf69a119367b0352a9011e0627bf5c1239f2c9	when do product architectures mirror organisational architectures? the combined role of product complexity and the rate of technological change		In the modularity literature, an architectural decomposition and ‘mirroring’ between task boundaries, knowledge boundaries, and firm boundaries has been suggested as a way to enhance managerial efficiency and as a source of potential strategic advantage. Despite its intuitive appeal, empirical support for ‘mirroring’ is significant but mixed. In this paper, we utilise an industrial economics and knowledge-based perspective to hypothesise how the combined effects of product architecture type, product complexity and the rate of product component change may influence task, knowledge and firm boundaries and hence be associated with either phases of mirroring or non-mirroring (‘misting’). We suggest that whether mirroring or misting is an efficient strategic choice is influenced by the characteristics of both the product architecture and the rate of technological change at the product component level, and changes across time as products evolve. Our framework helps to reconcile existing mixed evidence and provides the foundation for further empirical research.		Nicholas Burton;Peter Galvin	2018	Techn. Analysis & Strat. Manag.	10.1080/09537325.2018.1437259	architecture;empirical research;process management;knowledge management;technological change;economics;modularity;strategic choice;mirroring	NLP	-80.38391988371097	4.565779277140868	34188
ed05f2ed07ca4f11d12e35954df909720d7ac199	neither fish nor fowl: new strategies for selective regulation of information services	information services;voip;next generation networks	INTRODUCTION................................................................................... 374 I. TELECOMMUNICATIONS SERVICES VERSUS INFORMATION SERVICES ....................................................................................... 380 A. The Supreme Court Endorses Cable Modem Internet Access as an Information Service............................................. 382 B. DSL Migrates from Telecommunications Service to Information Service ................................................................. 387 II. VOIP SERVICE REGULATION ...................................................... 391 A. VoIP Service Providers Must Contribute to Universal Service Funding Regardless of Their Regulatory Status ......... 392 B. Outline of the Universal Service Funding Process .................. 393 C. The FCC’s Decision to Include VoIP Service Providers as Compulsory USF Contributors Upheld .................................. 399 D. Mandatory Wiretapping Cooperation for VoIP and Internet Access Providers ........................................................ 403 III. ERODING A NEW COMPETITOR’S COMPARATIVE ADVANTAGES ............................................................................... 412 A. Have Courts Become Too Deferential to the FCC?............... 414 B. Reasoned Analysis or Results-Oriented Decision Making?.... 419 CONCLUSION....................................................................................... 420	cable modem;digital subscriber line;internet access	Rob Frieden	2008	JTHTL		public relations;telecommunications service;operations management;business;computer security	Theory	-86.74772282962142	19.394929086780493	34197
dc381681a08c8838a18cb44d70a145bfdf65dcbd	factors for adopting erp as saas amongst smes: the customers vs. vendor point of view	systemvetenskap;erp;small and medium enterprises;smes;enterprise resource planning;software as a service;adoption;informationssystem och informatik med samhallsvetenskaplig inriktning;saas	This paper identifies the factors for adopting Enterprise Resource Planning (ERP) delivered as Software-as-a-Service (SaaS) among small and medium enterprises (SMEs). The authors conducted a two phases’ qualitative-methodology: interviews with 20 experts from SaaS vendor (Microsoft) and a case-study with executives in three organizations which implemented vendor’s ERP as SaaS (customers). The vendor identified 10 factors for adoption Costs, Security, Availability, Usability, Implementation, Ubiquity, Flexibility, Compatibility, Analytics and Best-practices, where costs, security and availability were considered the most important factors. The three customers identified 3 additional factors: the trust in the Solution partner, Data integrity, and level of Integration of cloud platforms. Considering all 13 factors, Cost, trust in the Solution partner and Availability were identified as the most important for customers, which gave much less importance to the others factors. These results will help professionals and researchers to improve understanding and accelerate ERP adoption as SaaS among SMEs. KEywoRdS Adoption, Enterprise Resource Planning, ERP, SaaS, Small and Medium Enterprises, SMEs, Software as a Service	best practice;data integrity;erp;enterprise resource planning;point of view (computer hardware company);software as a service;usability	Jorge Rodrigues;Pedro Miguel Fernandes Ruivo;Björn Johansson;Tiago Oliveira	2016	IRMJ	10.4018/IRMJ.2016100101	computer science;marketing;software as a service;database;commerce	HCI	-78.929740782296	12.05680197837445	34270
764404267308042bb760eb88ed61e09cc690df48	knowledge partitioning in outsourced software development: a field study	development process;software development process;software development;data collection;field study	The outsourced software development process has traditionally relied on a requirements-driven black-box approach for transferring knowledge of customer needs to vendors. When this approach is feasible, the need for the customer and the vendor to deeply understand each others’ knowledge domain is limited. We describe this as symmetric division of knowledge. However, asymmetric overlaps in knowledge are necessary at the vendor-customer boundary in projects involving conceptual or process newness. In this study, we examine the conditions under which overlaps in knowledge at the vendor-customer boundary are necessary for enhancing the development process in outsourcing relationships. We develop and test a model using data collected in a large-scale field study of 209 software projects in 209 software development organizations belonging to three of the largest global software consortia. The study makes three contributions: (1) we empirically demonstrate that it is more important for a vendor to possess a higher level of business knowledge in conceptually new projects and for the customer to have a higher level of technical knowledge when the project involves process newness, (2) we assess the effectiveness of various integrating mechanisms, and (3) we show that there are potential downsides to blindly increasing vendor-customer overlaps in knowledge beyond those that have traditionally characterized software development.	application domain;black box;business software;customer knowledge;field research;interaction;knowledge integration;outsourcing;predictive modelling;requirement;software development process;weightless (wireless communications)	Amrit Tiwana	2003			knowledge management;goal-driven software development process;domain knowledge;marketing;knowledge extraction;software peer review;software engineering process group;package development process;software mining;team software process;computer science	SE	-67.04203323366208	20.1661464999493	34295
2c6755f15f0be3fc6464a6307d92d4946cda6ae4	linking marketing and supply chain models for improved business strategic decision support	customer behavior;decision support;business decision support systems;market model;marketing models;enterprise wide optimization;decision support system;shareholder value;mathematical model;enterprise modeling;supply chain	A supply chain (SC) model incorporating business strategic decision components is an important tool for gaining a competitive edge in todays global market. Enterprise models of this type must encompass not only the SC, but also the demand chain since understanding the market is crucial for developing good business policies. To operate effectively, marketing activities must be coordinated with other corporate functional areas. Specifically, managers should evaluate the trade-off between marketing and SC decisions to enhance the performance of the overall metric: the shareholders value. Recently, there has been significant progress in developing marketing science models for reaching quantitatively based marketing decisions. In this work, we build on these developments to formulate a mathematical model that accounts for the main relevant business functionalities. The result is a MINLP formulation which optimizes the SC and marketing strategic decisions in an integrated fashion. Moreover, a financial model that evaluates the enterprise value is also incorporated.	decision support system	José Miguel Laínez;Gintaras V. Reklaitis;Luis Puigjaner	2010	Computers & Chemical Engineering	10.1016/j.compchemeng.2010.07.018	return on marketing investment;digital marketing;marketing management;demand chain;decision support system;enterprise modelling;business marketing;strategic business unit;computer science;marketing research;marketing;mathematical model;mathematics;supply chain;marketing strategy;profit impact of marketing strategy;new business development;enterprise value;relationship marketing;business decision mapping	ECom	-77.0079097184514	7.434421436715702	34296
9cd069584f12ea3fa817bd0a8debaa7fe99c1894	multi-dimensional assessment of risks in a distributed software development course	software engineering computer science education cultural aspects educational courses educational institutions organisational aspects risk management;risk management;software engineering;cultural differences education software engineering risks;computer science education;engineering and technology;teknik och teknologier;software educational institutions software engineering collaboration cultural differences context;educational courses;cultural aspects;cultural differences multidimensional risk assessment distributed software development course organizational shift educational institutions global software engineering courses gsd course;organisational aspects	The organizational shift from local to global settings in many software development initiatives has triggered the need for entailing it when educating the future software engineers. Several educational institutions have embraced this need and started collaborating for the provision of global software engineering courses. The rather complex nature of such courses results in a wider range of risks, in comparison to standard software engineering courses, that arise in different dimensions, ranging from course-to result-related, and for different reasons. In this work we provide an assessment of such a variety of risks as well as their causes, and we give a hint on how they may affect each other based on our 10-year-long experience with a tightly integrated GSD course.	categorization;distributed computing;document structure description;graphical system design;software development;software engineer;software engineering	Ivana Bosnic;Federico Ciccozzi;Igor Cavrak;Raffaela Mirandola;Marin Orlic	2013	2013 3rd International Workshop on Collaborative Teaching of Globally Distributed Software Development (CTGDSD)	10.1109/CTGDSD.2013.6635238	engineering management;software engineering process group;systems engineering;engineering;knowledge management;social software engineering	SE	-67.81047715191679	21.248774130282676	34561
835e0dd76daf7179416cf7e46c6ff5c0c3502347	makine ogrenmesi ile mobil uygulama siniflandirilmasi ve otomatik kesif testi(mobile application classification using machine learning and automated exploratory testing)			exploratory testing;machine learning;mobile app	Mehmet Çagri Çalpur;Sevgi Arca;Tansu Cagla Calpur;Cemal Yilmaz	2017			speech recognition;exploratory testing;computer science	ML	-91.06444169289433	22.991325422029654	34591
17913a659a561f94d0e0d0e3b4b89e83adc84c04	developing a cyber-physical autonomous and distributed intersection management - a software engineer's experience report			software engineer	Stefan Gries;Ole Meyer;Julius Ollesch;Florian Wessling;Marc Hesenius;Volker Gruhn	2018		10.3233/978-1-61499-900-3-582	systems engineering;cyber-physical system;software;computer science	SE	-62.85728210276487	24.585855652462826	34711
6e6eaee3d194c4a83e3d5c049e3144b4e7df6dff	adapting an enterprise architecture for business intelligence		Business intelligence (BI) projects have the goal to implement suitable tools for decision support and to integrate them with existing data sources in a company. They have therefore been on CIOs agendas for several years and there are still a lot of BI projects to come. Despite this fact, however, still the majority of BI projects fail to deliver the full benefit for the business that was expected. One factor why such projects are likely to fail is the lack of communication and common understanding of the project by the BI project team and the business departments. In this research, a modelling technique has been implemented that allows to model both the BI project elements as well as the business model in one comprehensive and easily understandable model, which can help to facilitate the communication between the stakeholders of a BI project. The modelling notation has been evaluated against real-world case studies by conducting interviews, which have shown that the implemented modelling technique could indeed improve the project results. An extended version of this paper is available under [1].	agent-based model;decision support system;documentation;enterprise architecture;graphical model;holism;metamodeling	Pascal von Bergen;Knut Hinkelmann;Hans Friedrich Witschel	2015			enterprise architecture;business intelligence;notation;decision support system;management science;project team;systems engineering;business model;business case;engineering	SE	-64.77948942291614	16.029637363093983	34715
08b64030f5a95401473710521f650ad57cd15f71	the state of practice in model-driven engineering	software design methodologies;model driven engineering practice;mde software design methodologies model driven engineering practice software design;mde;software design	Despite lively debate over the past decade on the benefits and drawbacks of model-driven engineering (MDE), there have been few industry-wide studies of MDE in practice. A new study that surveyed 450 MDE practitioners and performed in-depth interviews with 22 more suggests that although MDE might be more widespread than commonly believed, developers rarely use it to generate whole systems. Rather, they apply MDE to develop key parts of a system.	lively kernel;model-driven engineering	Jon Whittle;John Edward Hutchinson;Mark Rouncefield	2014	IEEE Software	10.1109/MS.2013.65	systems engineering;engineering;knowledge management;software design;software engineering	SE	-65.54783246360968	24.095260084262716	34738
90e6d90ec3dcc04a4160904b5c78922e9df07499	information technology and firm boundaries: evidence from panel data	vertical integration;computers;panel data;markets;information technology;firm boundaries;hierarchies;empirical model;transaction costs;diversification	Previous literature has suggested that information technology (IT) can affect firm boundaries by changing the costs of coordinating economic activity within and between firms (internal and external coordination). This paper examines the empirical relationship between IT and firm structure and evaluates whether this structure is consistent with prior arguments about IT and coordination. We formulate an empirical model to relate the use of information technology capital to vertical integration and diversification. This model is tested using an 8year panel data set of information technology capital stock, firm structure, and relevant control variables for 549 large firms. Overall, increased use of IT is found to be associated with substantial decreases in vertical integration and weak increases in diversification. In addition, firms that are less vertically integrated and more diversified have a higher demand for IT capital. While we cannot rule out all alternative explanations for these results, they are consistent with previous theoretical arguments that both internal and external coordination costs are reduced by IT.	panel data	Lorin M. Hitt	1999	Information Systems Research	10.1287/isre.10.2.134	industrial organization;diversification;vertical integration;internal financing;transaction cost;economics;marketing;international trade;panel data;microeconomics;management;information technology;hierarchy;commerce	AI	-82.62171632358414	4.965046878972018	34912
657b59e24b5fac8149ddeecf7ef1bcf162c48ab2	customer churn analysis : a case study on the telecommunication industry of thailand		Customer churn creates a huge anxiety in highly competitive service sectors especially the telecommunications sector. The objective of this research was to develop a predictive churn model to predict the customers that will be to churn; this is the first step to construct a retention management plan. The dataset was extracted from the data warehouse of the mobile telecommunication company in Thailand. The system generated the customer list, to implement a retention campaign to manage the customers with tendency to leave the company. WEKA software was used to implement the followings techniques: C4.5 decision trees algorithm, the logistic regression algorithm and the neural network algorithm. The C4.5 algorithm of decision trees proved optimal among the models. The findings are unequivocally beneficial to industry and other partners.	artificial neural network;c4.5 algorithm;decision tree;logistic regression;weka	Paweena Wanchai	2017	2017 12th International Conference for Internet Technology and Secured Transactions (ICITST)	10.23919/ICITST.2017.8356410	artificial neural network;retention management;computer science;decision tree;software;telecommunications;mobile telephony;data warehouse;information and communications technology;data modeling	ML	-84.21307770231041	10.415290660090351	35037
f9189e107e7fbd2dd3f9eaf2ac23a20470a85c49	sustainable operations management: design, modelling and analysis		In recent years, Sustainable Operations Management (SOM) has started receiving attention from both operations management and management science researchers. SOM includes topics such as green supply chain (eg, Linton et al, 2007; Darnall et al, 2008), green procurement (eg, Seuring and Müller, 2008; Walker et al, 2009) and reverse logistics (RL) (eg, Dowlatshahi, 2005; Srivastava, 2008). SOM has a potentially vital role to play in contributing to solutions for the complex sustainability challenges confronted by many organisations (Kleindorfer et al, 2005; White and Lee, 2009). As a result, a number of operations management researchers and practitioners are dealing with the challenges of integrating the issues of sustainability in protecting the environment and reducing the carbon footprint (eg, Carter and Rogers, 2008; Lee, 2011). Both researchers and practitioners recognise the importance of SOM as a key strategic component in the development of cost-effective and sustainable global supply chains to meet the increasing needs of customers in terms of flexibility, responsiveness and cost while safeguarding natural resources for future generations. Most of the research on SOM has been limited to literature reviews (eg, Linton et al, 2007), conceptual frameworks (eg, Seuring and Müller, 2008), case studies (eg, Pagell and Wu, 2009) and some empirical papers (eg, Zhu et al, 2005). However, SOM requires modelling and analysis for performance measures and metrics of various options available for SOM in both in manufacturing and services. White and Lee (2009) further argue that sustainable development has been largely regarded as a global challenge and the potential of Operational Research (OR) is yet to be properly exploited in this area. An analysis of the OR discipline in developing economies by White et al (2011) supports the latter argument. Through this research, White et al (2011) highlight that OR can make a significant contribution towards enhanced decision making, yet still OR has not been methodically utilised to address the Millennium Development Goals. The contribution by Kleindorfer et al, (2005) highlights the analysis of papers in the area of ‘Sustainable Operations Management’ in the first 50 issues of ‘Production and Operations Management’. Although Kleindorfer et al, (2005) focus on supply chain issues, their analysis offers limited insights to the overall development and status of the area. Similarly, Seuring and Müller (2008), in their literature review, specifically focus on the advent and growth of integrated supply chain management. A much broader effort is made by Srivastava (2007), but primarily focusing on a RL perspective. Ormerod and Ulrich (2013), on the other hand, focus on analysing OR and ethics and argue that despite the energetic engagement of OR in organisations in different contexts, there is still a lot to do— especially providing clarity as to how OR practices can be ethically established. Although sustainability offers an all-encompassing framework for much of the former and continuing environmental research in operations, Linton et al (2007) argue that in essence, sustainability in operations and supply chain management moves beyond current conventional practices. Research into how academia, researchers and practitioners can integrate OR within SOM is critical, as such research not only requires many questions be answered but also the establishment of a wide variety of approaches including conducting case studies, model development, statistical testing of propositions and machine learning (Kleindorfer et al, 2005; Matos and Hall, 2007; Linton et al, 2007). Nevertheless, there are not many articles that deal with modelling and analysis of SOM decision making at strategic, tactical and operational levels that are important for implementation of SOM decisions. The aim of this special issue is to help researchers and decision makers to understand the strategies, tactics and implementation processes involved in SOM decisions and the performance measures and metrics through modelling and analysis of SOM. With the help of these articles appearing in this special issue, one should be able to gain a better understanding of the issues involved in SOM and how to model and evaluate SOM decision-making environments and decisions through appropriate modelling and analysis of both manufacturing and service supply chains. The scope of the special issue is to present researchers and senior managers with conceptual modelling and analysis of various sustainable operations decisions and their performance outcomes in supply chains. This includes optimisation related to closed-loop chains, carbon footprinting of supply chains, life-cycle management, greening supply chains, green and reverse logistics, product and process development towards improving energy savings, efficiency of transport and other related areas. Emphasis is placed on sustainable operations design, modelling and analysis, optimisation, and their performance measurement in a supply chain. The special issue contains articles that cover the following topics:	amiga walker;conceptual schema;eurographics;footprinting;logistics;machine learning;management science;mathematical optimization;procurement;responsiveness;software design;software framework;word lists by frequency	Angappa Gunasekaran;Zahir Irani	2014	JORS	10.1057/jors.2014.26	enterprise modelling;management science	AI	-70.54567526444615	7.390298373694029	35162
d86a8c0171b490ec014f941d6a6b393c8da78667	it and business process outsourcing: the knowledge potential	partenariat;hd industries land use labor;gestion entreprise;entreprise;outsourcing;externalisation;connaissance;information technology;empresa;firm management;technologie information;conocimiento;capital intellectuel;coparticipacion;knowledge;partnership;estudio caso;processus entreprise;firm;etude cas;business process outsourcing;administracion empresa;tecnologia informacion;business process	Abstract Despite the widespread trends in IT and business process outsourcing, there has been too little focus on what happens to knowledge when an organization outsources. We present a framework for evaluating the knowledge potential within five different types of insourcing and outsourcing arrangements. A detailed example of an enterprise partnership relationship is described as a benchmark for how companies can leverage the knowledge potential from IT and BPO outsourcing.	business process;outsourcing	Leslie P. Willcocks;John Hindle;David F. Feeny;Mary Lacity	2004	IS Management	10.1201/1078/44432.21.3.20040601/82471.2	insourcing;knowledge process outsourcing;knowledge;business process;management;law;information technology;outsourcing	NLP	-78.94315092617069	4.900489677842989	35163
7bebf5ddf61f67e259c93841088516871c22f36e	the success of open source software: a review	economics open source software oss source code social sciences information sciences software engineering;open source software communities licenses companies internet;source code software economics public domain software software engineering	Open source software (OSS) is software with its source code available that may be used, copied, and distributed with or without modifications, and that may be offered either with or without a fee. In this paper we present an overview of the state-of-the-art OSS-related research from different fields and disciplines of the social and information sciences. The emerging work on understanding OSS has questioned what has led to its prosperity. We present an historical review of its success, and we discuss some of the key factors that contributed to the rise of the OSS, mainly from the perspective of the software engineering and economics. We conclude with some real-life business examples of companies that achieved their profit with OSS.	information science;open sound system;open-source software;operating system;real life;software engineering;software project management;software testing;usability	Domagoj Margan;Sanja Candrlic	2015	2015 38th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)	10.1109/MIPRO.2015.7160503	personal software process;long-term support;software engineering process group;package development process;backporting;social software engineering;software framework;software development;software construction;software as a service;software technical review;software walkthrough;software analytics;software deployment;world wide web;software requirements;software quality;software peer review	SE	-66.7781153625475	24.27166556562081	35166
7a889587f32ce8753ed1cdc3a718440ec0d3ac85	ifrs, information asymmetry and growth opportunities		This paper aims to examine the impact of IFRS / IAS mandatory adoption on the information asymmetry measured by the properties of financial analysts’ forecasts (error and dispersion). It examines also, whether this impact differs from one company to another with regard to its growth opportunities. This study is based on a longitudinal data set (2002 – 2012) obtained from a sample of companies that belong to the CAC all tradable indexes. The results show a significant effect of these international’s standards on financial analysts’ forecasts, which stress the improvement of earnings’ informational content. By focusing on the effect of growth opportunities, the results show also that they don’t moderate significantly the relationship between IFRS and qualities of financial analysts’ forecasts. This allows to conclude that IFRS are able to reflect the true value of the company regardless its growth opportunities KEywORdS Financial Analysts’ Forecasts, Growth Opportunities, IFRS, Information Asymmetry		Hela Turki;Sonda Wali;Younes Boujelbene	2017	IJSSMET	10.4018/IJSSMET.2017040105	engineering;earnings;accounting;information asymmetry	HCI	-85.43189621412957	7.1408169638261	35277
24c754a70ff6aeac18cc7570eaee7e1af1d471cd	leveraging the crowd: how 48,000 users helped improve lync performance	analytical models;groupware;performance monitoring;performance evaluation;data collection;software analytics;software development performance evaluation customer satisfaction software quality analytical models;software engineering groupware;software engineering;software performance;software data visualization;customer satisfaction;data analysis;software data visualization software performance data collection data analysis performance monitoring software analytics;software development;software quality;data visualization lync performance customer satisfaction collaborative software software development	Performance is a critical component of customer satisfaction with network-based applications. Unfortunately, accurately evaluating the performance of collaborative software that operates in extremely heterogeneous environments is difficult with traditional techniques such as modeling workloads or testing in controlled environments. To evaluate performance of an application in the wild during development, the authors deployed early versions of the software, collecting performance data from application users for key usage scenarios. The analysis package they used produces visualizations to help development teams identify and prioritize performance issues by focusing on performance early in the development cycle, evaluating progress, identifying defects, and estimating timelines.	collaborative software;timeline	Robert Musson;Jacqueline Richards;Danyel Fisher;Christian Bird;Brian Bussone;Sandipan Ganguly	2013	IEEE Software	10.1109/MS.2013.67	software performance testing;computer science;systems engineering;engineering;software development;software engineering;data mining;data analysis;customer satisfaction;software analytics;software quality;data collection	HPC	-64.57975337109988	31.55524946336661	35311
67e98fa6171fb057ee0df37bc5d7aaf485039fa4	from e-processes to e-networks: an e-service-oriented approach	business model;network management;web service;business process;information technology;information system;e commerce	Looking at the recent history of information systems, two complementary trends seem to emerge. On the one side, systems become more modular. The shift is from tight integration to loosely coupled components. On the other side, the distance between business models and information technology (IT) is shortening. Aggressive business models impose new requirements on IT. At the same time, operational capabilities made available by IT drive the definition of new business models. Web services are the most noticeable outcome of the first trend. E-services play a similar role for the second trend. In this paper, we concentrate on e-services and their relationships with the type of business processes used by e-businesses. We refer to this type of processes as e-processes. We first introduce e-services, and outline the interconnections with web services. Focusing on the composition aspects of e-processes and e-services, we then discuss potential and requirements of processlevel cooperation for composite e-service solutions. We refer to these composite solutions as e-networks. The discussion is based on our work on the DySCo (Dynamic e-Service Composer) framework for enetwork management. A brief description of the framework and the associated platform is included, together with an analysis of some of the issues related to service level agreements (SLAs) in e-networks. 1.The e-service vision Until recently, the Internet was about the creation of e-commerce systems, and it was dominated by web sites and storefronts. We have now entered the next Internet evolution: the proliferation of e-services [13]. E-services are modular, nimble, units of service made available from a business to other businesses and to consumers. Almost any business asset can be turned into an e-service and offered efficiently via the Internet. A definition: an e-service is any asset that is made available via the Internet to drive new revenue streams or create new efficiencies. Chapter 1 of the Internet was about businesses being wired to their employees, customers and partners; key business processes being linked to the Internet, and a critical mass of consumers coming online. Businesses were learning how to turn a new technology into a new business resource. Now, the Internet is ready for its next evolution. It is no longer about businesses looking at the web as a technology. The Internet has been absorbed into the core business infrastructure, and businesses can capitalise on this new asset. Chapter 2 of the Internet is about the mass proliferation of e-services. E-services come in the form of modular units, which can be combined and recombined to solve business problems and to help businesses to overcome the limitations of a static business infrastructure. Successful companies will be those capable of turning their business assets into services and deliver them via the Internet. 2.E-services and web services Since the e-service vision was proposed by Hewlett-Packard two years ago, virtually all the major players in the IT industry have aligned behind it. A number of different initiatives have emerged, all paving the way towards the full realisation of the e-service world. First of all, e-services mean openness; and openness means common standards. XML is a clear example of the impact that standards can have on open systems. Layered on top of XML, a number of standardisation efforts are building the foundation for automated business interaction. Platform-level initiatives like SOAP [1], WSDL [3], and WSFL [6] complement initiatives like BPMI [2], ebXML [4], and RosettaNet [10] at the level of business processes. UDDI is an example of the way the dynamic formation of business relationships advocated for e-services can be supported in practice. The changes advocated by e-services go beyond technology into the very structure of businesses. In chapter one of the Internet companies had to reengineer their internal processes in order to sustain webbased channels to customers. Similar if not deeper changes are required to businesses in order to take advantage of the opportunities available in chapter two. Technology is mainly instrumental to most of these changes, but it certainly represents an important catalyst. In particular, flexible IT platforms enable flexible business solutions. Web services represent an important step in this direction. Web services represent an IBM-led initiative aimed at standards for modular and open applications. The definition for web services given by the IBM web services architecture team is “... self-contained, modular applications that can be described, published, located, and invoked over a network, generally, the Web” [5]. WSDL and WSFL are among the outcomes of the web service initiative. The requirements described in the e-service model for business-level services find a clear mapping on the application-level model proposed for web services. 3.E-services and e-processes In chapter one of the Internet, the characterisation of an e-business was given in terms of the capability of a company to interact electronically with its customers. Electronic interaction meant browser-based interaction. The Internet was used as an additional channel to customers. New front-end solutions were built for internal business processes, but the back-end infrastructure of the company was left substantially unchanged. Market forces have already decided the inappropriateness of this type of business model for a number of individual cases. Learning from the (very expensive) experiments made by a number of companies, we propose a model for e-businesses based on the separation between standard processes and eprocesses [11, 14]. The successful operation of a business substantially derives from the synergy between all the processes in the company ecosystem. A distinction between internal and external processes could lead to a definition of e-processes as the external processes involving the Internet [12]. Wide enough to accommodate most of the business architectures emerged in the past, we believe that such a definition fails to recognise the internal impact of electronic interaction in general, and of the Internet in particular. A definition: an e-process is any business process sustaining the operational aspects of an Internetcentred business model. The blur in the distinction between e-processes and standard processes reflects the pervasiveness of Internet-centred business models into the structure of a company. The entire company has to align behind new e-business models, and this is the principle that we try to capture with the definition proposed for eprocesses. Far from suggesting that every process in an e-business is an e-process, our aim is to stimulate a more careful analysis of business processes and their adequacy with respect to the needs and opportunities of the Internet. The focus is on the structure and content of processes rather that the type of technology involved. E-services focus on the way in which the value generated by a company can be offered to other businesses and customers. On the supply side, e-services propose a common approach to aspects like description, discovery, and negotiation of the products generated by an e-business. The e-service offer of an e-business emerges from the customer interaction elements of the e-processes implemented by the company. On the demand side, e-services offer a new way of acquiring business resources and capabilities. E-processes can exploit the potential of a supply chain based on e-services.	align (company);business architecture;business process;composer;e-commerce;e-services;ebxml;ecosystem;electronic business;experiment;information system;internet;loose coupling;openness;requirement;rosettanet;soap;service-level agreement;service-oriented software engineering;synergy;turned a;web services description language;web services discovery;web services flow language;web application;web service;world wide web;xml	Giacomo Piccinelli;Eric Stammers	2002			value-added network;process management;business service provider;business process modeling;electronic business;the internet;artifact-centric business process model;knowledge management;business;new business development;business model	Web+IR	-74.8376107773085	6.337038652023156	35392
8f9e2cc328b0b8b3ce3cbf032f2c18f5c456706a	do investors recognize information technology as a strategic asset? a longitudinal analysis of changes in ownership structure and it capability		IT capability is known to increase financial performance and affects strategic topics like vertical integration and competitive action. It is seen as a strategic although intangible asset and receives a lot of interest in research and practice. Recent studies show that this organizational asset is developed over time and needs continuous investment to be built. Long-term oriented investors value strategic assets as they are essential drivers for firms’ long-term success and survival. Hence, compared to other companies in the same industry, a higher ratio of long-term oriented investors in a firm’s ownership structure should reflect a firm’s ability to create strategic assets. Based on archival data from 2000 to 2009 we investigate the interplay of a firm’s IT capability and its ownership structure. We find that superior IT capability is related to a high ratio of long-term oriented investors. Further, empirical analysis shows that changes in IT capability induce adjustments in the ownership structure. This study contributes to the body of literature on the business value of IT by studying the capital market effects of IT capability. Practical implications and areas of further research are outlined.		André Schäfferling;Heinz-Theo Wagner	2013			capital market;intangible asset;computer science;information technology;vertical integration;finance;archival research;business value	HCI	-82.61269841711777	5.425761343205842	35420
13e82ed5c52fe61a9983683e01724b83db58dc08	service consumer model: understanding and describing consumers for new service development	event driven process chains;disconfirmation model;service innovation;consumer context;consumer process;consumer value;service dominant logic;new service development;event driven process chain;activity theory;design science	Against the background of an increasing discrepancy between consumer access to an evergrowing range of products and being increasingly frustrated with the consumption processes, we propose a model for understanding and describing consumers and their behavior holistically. In this paper, a design science approach is used. The model is build on existing concepts such as the concept of consumer processes, user context, service-dominant logic, disconfirmation, activity theory, and event-driven process chains. The application of the model and its usefulness for improving new service development is demonstrated by an example.		Axel Hochstein;Walter Brenner;Bernhard Schindlholzer	2008			service;marketing;service design;advertising;business;commerce	SE	-75.67116041737837	6.5837058461314015	35474
6f74533421b6a2332a2cc91bf49de07b0c6ca384	colocation as a hybrid ict sourcing strategy to improve operational agility	ict infrastructure strategy;colocation sourcing;operational agility;quantitative field study	Fast access to communication networks and the availability of high-performance information and com-munication technology (ICT) infrastructures is indis-pensable for accelerating business transactions. Yet with increased environmental volatility, companies need to become more agile in identifying and responding to market- and technology-based challenges. Accordingly, a responsive and high-performance ICT infrastructure remains a top priority for firms. Thus, new ICT sourcing strategies may lead to significant competitive advantages, especially in dynamic business environments. This article analyzes a hybrid ICT sourcing strategy called colocation that allows firms to operate their own ICT resources in facilities of special-ized data center providers. Grounded in the theory of dynamic capabilities, we theorize and empirically ex-amine how colocation and top management support enable firms to improve their operational agility in the presence of environmental turbulence.	agile software development;colocation centre;data center;itil;telecommunications network;turbulence;volatility	Roman Beck;Immanuel Pahlke;Jens Vykoukal	2016	DATA BASE	10.1145/2963175.2963177	knowledge management;operations management;commerce	HPC	-77.544990093953	5.758194189799083	35499
4498202c1c418473f92205b2ce4ed618685c02a2	a comparative study among stakeholders on causes of time delay in malaysian multiple design and build projects	multiple projects;time delay factors stakeholders malaysian multiple design projects malaysian build projects construction industry project completion construction project management design and build projects d b projects risk factors malaysian construction;design build projects;time delay;construction industry delay effects delays contracts standards materials electric breakdown;time delay multiple projects design build projects qualitative risk analysis;risk management construction industry design engineering project management;qualitative risk analysis	The construction industry plays an important role in developing a nation to a fully developed status. The construction industry, unlike other industries has unique problems concerning project completion. Projects are now more complicated involving huge contract values, participants from multi-discipline, more specialised works, tighter schedule, more cost consciousness, stringent quality standards, etc. Ultimately, cost and time are the two key parameters that play significant role in construction project management. In Malaysia very few studies have been attempted on the factors causing time delay. Besides, there has been no such study related to multiple Design and Build (D&B) projects. The study focused on D&B projects which have complicated risk governed with fixed time and a fixed contract sum (Lump sum). Thus there is a need for comparative study on the risk factors causing this time delay which enables the Malaysian construction to respond proactively and more effectively to time delay factors.	broadcast delay;consciousness;lumped element model;risk factor (computing)	Chidambaram Ramanathan;Narayanan Sambu Potty	2014	2014 IEEE International Conference on Industrial Engineering and Engineering Management	10.1109/IEEM.2014.7058680	construction engineering;systems engineering;engineering;operations management;construction management	SE	-77.11652252755137	12.768802136888848	35540
f3ce61c4cf058f730fa4cd82d4b19e6f7f7dbc02	using multiple adaptive regression splines to support decision making in code inspections	functional form;exploratory analysis;regression model;multiple adaptive regression spline;software development life cycle;inspection effectiveness;software inspection;regression spline;multivariate statistical analysis	Inspections have been shown to be an effective means of detecting defects early on in the software development life cycle. However, they are not always successful or beneficial as they are affected by a number of technical and managerial factors. To make inspections successful, one important aspect is to understand what are the factors that affect inspection effectiveness (the rate of detected defects) in a given environment, based on project data. In this paper we collected data from over 230 code inspections and performed a multivariate statistical analysis in order to look at how management factors, such as the effort assigned and the inspection rate, affect inspection effectiveness. Because the functional form of effectiveness models is a priori unknown, we use a novel exploratory analysis technique: multiple adaptive regression splines (MARS). We compare the MARS model with more classical regression models and show how it can help understand the complex trends and interactions in the data, without requiring the analyst to rely on strong assumptions. Results are reported and discussed in light of existing studies. 2004 Elsevier Inc. All rights reserved.	artifact (software development);feedback;graham scan;higher-order function;interaction;linear model;log-linear model;nonlinear system;sensor;smoothing spline;software bug;software development process;software inspection;source lines of code	Lionel C. Briand;Bernd G. Freimut;Ferdinand Vollei	2004	Journal of Systems and Software	10.1016/j.jss.2004.01.015	computer science;engineering;software engineering;data mining;software inspection;systems development life cycle;programming language;management;higher-order function;regression analysis;statistics	SE	-70.86304097282978	23.365423837505702	35568
95ec1384b9edb5b95c96bf2cd18dfdacc9bacc40	cooperative supply chain management: the impact of interorganizational information systems	industrie textile;competitive strategy;longitudinal study;competition;system configuration;systeme interorganisationnel;gestion systeme information;market structure;organisation systeme;organizacion sistema;industria textil;supply chain;estructura mercado;information system;structure marche;systeme information;supply chain management;textile industry;business process;competencia;sistema informacion	The impact of interorganixational information systems on the structure and management of a supply chain in the textile industry is analysed from a managerial perspective. Case data from detailed, partial longitudinal studies of manufacturer and retail organizations are presented. The competitive strategies of organizations in the supply chain are described and their associated patterns of communication are analysed. It is shown that companies are moving towards cooperative relationships in an effort to make the supply chain as a whole more competitive. The resulting market structure is an electronic hierarchy in which business processes are integrated across organizational boundaries using interorganizational information systems. The strategies of the individual firms are evolving as new opportunities arise and different problems present themselves. The results are compared with current theories on market structure and competition in an electronic trading environment and future trends are outlined.	autonomous robot;autonomous system (internet);business process;causal filter;computer-aided design;electronic business;electronic markets;electronic trading;emergence;information system;journal of strategic information systems;strategic information system;theory	Christopher Patrick Holland	1995	J. Strategic Inf. Sys.	10.1016/0963-8687(95)80020-Q	supply chain management;competition;textile industry;economics;service management;engineering;marketing;operations management;market structure;supply chain;business process;management;information system;competitive advantage	AI	-69.25891845279895	6.57775363051405	35733
dbcdde3353ca62ba392d4e0ceca35e28ee81692c	designing data governance in platform ecosystems		As platform ecosystems such as Facebook or Twitter are rapidly growing through platform users’ data contribution, the importance of data governance has been highlighted. Platform ecosystems, however, face increasing complexity derived from the business context such as multiple parties’ participation. How to share control and decision rights about data assets with platform users is regarded as a significant governance design issue. However, there is a lack of studies on this issue. Existing design models focus on the characteristics of enterprises. Therefore, there is limited support for platform ecosystems where there are different types of context and complicated relationships. To deal with the issue, this paper proposes a novel design approach for data governance in platform ecosystems including design principles, contingency factors and an architecture model. Case studies are performed to illustrate the practical implications of our suggestion.		Sung Une Lee;Liming Zhu;Ross Jeffery	2018			management science;computer science;ecosystem;design choice;data governance	SE	-77.17297411602064	5.990026655929418	35783
d31046d7c22f49c7f4de5f75f8f76a686c3c3556	insurance versus investigation driven approach for the computation of optimal security investment	financial security;forensics readiness;cyber security insurance;information security;risk minimization;optimal investment	Several research works have proposed economic and financial models to determine the optimal amount of investment in the security of information systems, showing the use of diverse techniques such as Game Theory (Grossklags et al. 2008), Utility Mdodels (Huang and Behara 2013; Miaoui et al. 2014), Return on Information Security Investment (Sonnenreich 2006), and Value at Risk (J. Wang et al. 2008). While many of these works showed the importance of investing in both self-protection and Cyber insurance (to reduce and transfer the residual risk of loss to insurance companies), none of them has considered the importance of security investment in forensic investigation to support insurance claims, ensure a better reimbursement of loss in case of security breach, and increase the return of investment in security. We propose in this paper to distribute the investment in information security into investment in Self-Defense to protect against security attacks, investment in Insurance to transfer the residual risk of loss to insurance companies, and investment in Forensic Readiness to maximize the firm’s potential to collect appropriate digital evidence, and generate provable insurance claims about occurred security breaches. An economic model based on the theory of utility is designed to compute the optimal total investment, taking into consideration the interdependence between the aforementioned three investments. An analysis is conducted to assess the variation of the optimal investments in self-defense and forensic readiness, and the cost of residual risk, with respect to the rate of insurance reimbursement, security vulnerabilities, and potential financial loss.	computation	Yosra Miaoui;Noureddine Boudriga;Ezzeddine Abaoub	2015			information security audit;computer security model;cloud computing security;security management;actuarial science;security information and event management;security convergence;asset;computer science;threat;information security;security analysis;network security policy;computer security;information security management;commerce	Crypto	-65.30380292371163	6.141424610424467	35829
d66eb9b188b9e1c4825523990d60d02a72a66b54	introducing automated gui testing and observing its benefits: an industrial case study in the context of law-practice management software		"""Motivated by a real-world industrial need in the context of a large IT solutions company based in Turkey, the authors and their colleagues developed and introduced automated test suites for GUI testing of two large-scale law-practice management software (comprising of 414 and 105 KLOC). We report in this paper our experience in developing and introducing a set of large automated test suites (more than 50 KLOC in total), using best practices in state-of-the art and –practice, and to report its observed benefits by conducting cost-benefit analysis in the specific industrial context. The project was conducted based on the principles of case-study and """"action research"""" in which the real industrial needs drove the research. Among the best practices that we used are the followings: (1) the page-object test pattern, (2) modularity in test code, (3) creating test-specific libraries, and (4) using systematic guidelines to decide when and what (test cases) to automate. To assess the cost-benefit and Return On Investment (ROI) of test automation, we followed a hybrid measurement approach to assess both the quantitative and qualitative (intangible) benefits of test automation. The empirical findings showed that the automated GUI testing approach has indeed benefitted the test and QA team in the company under study and automation has been highly welcome by the test engineers. By serving as a success story and experience report in development and introduction of automated test suites in an industrial setting, this paper adds to the body of evidence in this area and it aims at sharing both technical (e.g., using automated test patterns) and process aspects (e.g., test process improvement) of our project with other practitioners and researchers with the hope of encouraging more industry-academia collaborations in test automation."""	best practice;experiment;graphical user interface testing;library (computing);manual testing;region of interest;regression testing;software quality assurance;software testing;source lines of code;test automation;test card;test case;test engineer;test suite	Vahid Garousi;Erdem Yildirim	2018	2018 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)	10.1109/ICSTW.2018.00042	systems engineering;automation;return on investment;action research;best practice;graphical user interface testing;software;test case;law practice management;computer science	SE	-67.34756180149297	23.094937862500082	35927
5022341178ef038a64367b4cf2044babbffe7a66	requirements engineering's next top model	unified modeling language games computational modeling adaptation models software engineering;software engineering;modeling requirements;requirements;computational modeling;games;unified modeling language;adaptation models;modeling	A game-show environment let a panel competitively explore the use of various requirements modeling techniques for specifying a complex problem. Although plain old text and rich pictures emerged as the winners, real-world problems are best modeled using a variety of techniques. The Web extra at http://youtu.be/6vfIwSauj5o is an audio podcast of author Jane Cleland-Huang providing an audio recording of her Requirements column, in which she discusses how a game-show environment at the 2013 European Software Engineering Conference let a panel competitively explore the use of various requirements modeling techniques for specifying a complex problem.	jane (software);podcast;requirement;requirements analysis;requirements engineering;software engineering;world wide web	Olly Gotel;Jane Cleland-Huang	2013	IEEE Software	10.1109/MS.2013.129	unified modeling language;games;requirements analysis;software requirements specification;simulation;systems modeling;human–computer interaction;computer science;engineering;software engineering;requirements engineering;modeling language;programming language;computational model	SE	-67.32884330567163	25.374136612293103	35939
8293a315971268d83df522787b6eb6b536a88e6d	towards patterns to enhance the communication in distributed software development environments	pattern;collaborative software development	Distributed Software Development (DSD) is an emerging research area in software engineering. Several conducted research studies identified similar communication problems among DSD teams and tried to solve them. In this paper we present patterns that we have identified while surveying state of the art research studies. The patterns can help to organize DSD teams better in order to enhance their communication. We also highlight some potential future research challenges.	distributed computing;document structure description;integrated development environment;software development;software engineering;tiger team	Ernst Oberortner;Irwin Kwan;Daniela E. Damian	2011		10.1145/2578903.2579159	simulation;behavioral pattern;systems engineering;engineering;knowledge management	SE	-71.97670174897672	20.835261906965528	36055
5a397aa585219f22d832b1c0d2f695dc293416cf	war and peace — how to avoid technology wars	avoid technologywars	Technology wars arise when multiple technologies and products exist and the migration from one to another yields a competitive advantage. This usually happens when there is a discontinuous innovation that changes the way we do business. Technology wars are caused by rough-and-tumble free market forces. They often lead to healthy competition and better products. Sometimes all they lead to are delays in solving problems. Technology wars can be good, but only when we manage their impact. Let’ s consider an issue even bigger than a technology war, namely the life cycle management of software development for the telecommunications industry. Since this industry has been at the leading edge of the adoption of software technology, its experiences are often carried directly into other industries. To manage the software development life cycle one must manage the life cycles of its paradigms and products. A paradigm is a way of solving problems. It is a belief system that the software activities should be conducted in a particular way. The typical examples of paradigms include batch processing, mainframe architecture, relational database, arti® cial intelligence, and more recently and more relevant to this discussion, the distributed object computing. Paradigms typically have a much longer life cycle. They typically have three stages. A paradigm almost always spends it infancy in universities and research institutes. During this period, there are lots of wars but largely irrelevant to the industry. Because it is far too early to change any of industry’ s software development behavior before this stage is complete, many paradigms die before reaching this point. When a paradigm reaches a mature age, people start to build products and software solutions. This is where war starts. Some technology or products will emerge as the winner of the war and some will not. This is where standard processes start which lead to more wars. When we reach the apex of the maturity,	apex (geometry);batch processing;capability maturity model;distributed object;mainframe computer;programming paradigm;relational database;relevance;software development process;system migration	Graham Chen	1998	Journal of Network and Systems Management	10.1023/A:1018771013112	military operations other than war	SE	-71.86447087240934	8.01717612358694	36100
0789ed2879c269dba16d07aac74e79f73e0d8bc6	how have we evaluated software pattern application? a systematic mapping study of research design practices	empirical design;systematic review;software pattern;empirical evaluation;mapping study	Context: Software patterns encapsulate expert knowledge for constructing successful solutions to recurring problems. Although a large collection of software patterns is available in literature, empirical evidence on how well various patterns help in problem solving is limited and inconclusive. The context of these empirical findings is also not well understood, limiting applicability and generalizability of the findings. Objective: To characterize the research design of empirical studies exploring software pattern application involving human participants. Method: We conducted a systematic mapping study to identify and analyze 30 primary empirical studies on software pattern application, including 24 original studies and 6 replications. We characterize the research design in terms of the questions researchers have explored and the context of empirical research efforts. We also classify the studies in terms of measures used for evaluation, and threats to validity considered during study design and execution. Results: Use of software patterns in maintenance is the most commonly investigated theme, explored in 16 studies. Object-oriented design patterns are evaluated in 14 studies while 4 studies evaluate architectural patterns. We identified 10 different constructs with 31 associated measures used to evaluate software patterns. Measures for ‘efficiency’ and ‘usability’ are commonly used to evaluate the problem solving process. While measures for ‘completeness’, ‘correctness’ and ‘quality’ are commonly used to evaluate the final artifact. Overall, ‘time to complete a task’ is the most frequently used measure, employed in 15 studies to measure ‘efficiency’. For qualitative measures, studies do not report approaches for minimizing biases 27% of the time. Nine studies do not discuss any threats to validity. Conclusion: Subtle differences in study design and execution can limit comparison of findings. Establishing baselines for participants’ experience level, providing appropriate training, standardizing problem sets, and employing commonly used measures to evaluate performance can support replication and comparison of results across studies. 2015 Elsevier B.V. All rights reserved.	architectural pattern;baseline (configuration management);problem solving;software design pattern;threat (computer)	Maria Riaz;Travis D. Breaux;Laurie A. Williams	2015	Information & Software Technology	10.1016/j.infsof.2015.04.002	systematic review;computer science;systems engineering;data mining;management science	SE	-71.46224670775419	22.206320112733113	36105
c17e4660cb6b81e9bf6c9adec79ba6fd268d6872	towards a context-dependent numerical data quality evaluation framework		This paper focuses on numeric data, with emphasis on distinct characteristics like varying significance, unstructured format, mass volume and real-time processing. We propose a novel, context-dependent valuation framework specifically devised to assess quality in numeric datasets. Our framework uses eight relevant data quality dimensions, and provide a simple metric to evaluate dataset quality along each dimension. We argue that the proposed set of dimensions and corresponding metrics adequately captures the unique quality antipatterns that are typically associated with numerical data. The introduction of our framework is part of a wider research effort that aims at developing an articulated numerical data quality improvement approach for Oil and Gas exploration and production workflows that is based on artificial intelligence techniques.	artificial intelligence;context-sensitive language;data quality;level of measurement;numerical analysis;real-time locating system;refactoring software, architectures, and projects in crisis;value (ethics)	Hameeda A Naimi;Ernesto Compatangelo;Wamberto Weber Vasconcelos	2018	CoRR		data mining;valuation (finance);computer science;workflow;data quality	AI	-74.7976198032813	12.51008441968541	36145
be1b4fc3f5b1b68ca43efd6bc8685a14ae524a01	which traceability visualization is suitable in this context? a comparative study	visualization;traceability;context;empirical experiment	Traceability supports users in describing and tracking the relationships between software artifacts. Techniques such as traceability matrices and graphs visualize these relationships and help users to access and understand them. Researchers agree that different visualization techniques add valuable information in different contexts. However, there is an ambiguity which visualization is suitable for which context. To clarify this we conducted a comparative study of common visualization techniques, including an experiment and interviews with 24 participants. We found that traceability matrices and graphs are most preferred in management tasks, while hyperlinks are preferred in implementation and testing tasks. Traceability lists seem to be the least attractive technique for most participants. Graphs are preferred to navigate linked artifacts, while matrices are appropriate for overview. Hyperlinks are regarded to fit for fine-grained information. Participants stressed the importance of visualizing semantics of artifacts and links. Our finding also indicates that users are not always able to choose the most suitable visualization.	artifact (software development);graph (discrete mathematics);hyperlink;requirements traceability;scientific visualization	Yang Li;Walid Maalej	2012		10.1007/978-3-642-28714-5_17	traceability;visualization;computer science;engineering;software engineering;data mining;database;world wide web	HCI	-74.99666602416744	23.50113887133928	36221
00607ae6ee198778a9b5719795bba671dcef16ec	towards collaborative agent-based knowledge support for agile projects	knowledge engineering;knowledge management;agile methodologies	Using agile methodologies for enacting projects has been subject to intense research recently. In this paper, we present the Collaborative Agent-based Knowledge Engine approach for supporting mobile workers performing timecritical or business critical tasks. By a combination of sophisticated knowledge management and light-weight workflow model, this approach provides guidance and knowledge as required. Moreover, aspects for maintaining project history is discusses, as well as possibilities for integrating tools regarding computersupported collaborative work already deployed in organizations.	agile software development;documentation;knowledge management;mission critical;requirement;software engineering;window of opportunity	Andrea Freßmann;Rainer Maximini;Thomas Sauer	2005			knowledge engineering;agile unified process;domain knowledge;agile software development;computer science;agile usability engineering;knowledge management	Web+IR	-64.8877619227106	19.293564657852674	36266
2928c27976670171c9af870e2e9e6d15381dbd26	improving success/completion ratio in large surveys: a proposal based on usability and engagement		This paper presents a research focused on improving the success/completion ratio in large surveys. In our case, the large survey is a questionnaire produced by the Spanish Observatory for University Employability and Employment (OEEU in the Spanish acronym). This questionnaire is composed by around 32 and 60 questions and between 86 and 181 variables to be measured. The research is based on the previous experience of a past questionnaire proposed by the OEEU composed also by a large amount of questions and variables to be measured (63-92 questions and 176-279 variables). After analyzing the target population of the questionnaire (with the target population of the previous questionnaire as reference) and reviewing the literature, we have designed 11 proposals for changes in the questionnaire that could improve users’ completion and success ratios (changes that could improve the users’ trust in the questionnaire, the questionnaire usability and user experience or the users’ engagement to the questionnaire). These changes are planned to be applied in the questionnaire in two main different experiments based on A/B test methodologies that will allow researchers to measure the effect of the changes in different populations and in an incremental way. The proposed changes have been assessed by five experts through an evaluation questionnaire. In this questionnaire, researchers gathered the score of each expert regarding to the pertinence, relevance and clarity of each change proposed. Regarding the results of this evaluation questionnaire, the reviewers fully supported 8 out of the 11 changes proposals, so they could be introduced in the questionnaire with no variation. On the other hand, 3 of the proposed changes or improvements are not fully supported by the experts (they have not received a score in the top first quartile of the 1-7 Likert scale). These changes will not be discarded PR E-P RIN T immediately, because despite they have not received a Q1 score, they received a score within the second quartile, so could be reviewed to be enhanced to fit the	a/b testing;experiment;p (complexity);population;relative intensity noise;relevance;usability;user experience	Juan Cruz-Benito;Roberto Therón;Francisco J. García-Peñalvo;José Carlos Sánchez Prieto;Andrea Vázquez-Ingelmo;Martín Martín-González;Jorge M. Martínez	2017		10.1007/978-3-319-58515-4_28	systems engineering;engineering;knowledge management;management science	AI	-72.50885051095374	23.173360982781908	36487
7b7d4ea49c6796d0d508df9ad7552c9af2a9f0e0	multiple social networks analysis of floss projects using sargas	public domain software;open source software developer;open source software;different aspect;open-source project;multiple social networks analysis;integrated study;floss projects;different social network;floss community;floss project;sargas;social networking (online);open source participation;social network analysis;software engineering;multiple social network;social network	"""Due to their characteristics and claimed advantages, several researchers have been investigating free and open-source projects. Different aspects are being studied: for instance, what motivates developers to join FLOSS projects, the tools, processes and practices used in FLOSS projects, the evolution of FLOSS communities among other things. Researchers have studied collaboration and coordination of open source software developers using an approach known as social network analysis and have gained important insights about these projects. Most researchers, however, have not focused on the integrated study of these networks and, accordingly, in their interrelationships. This paper describes an approach and tool to combine multiple social networks to study the evolution of open-source projects. Our tool, named Sargas, allows comparison and visualization of different social networks at the same time. Initial results of our analysis can be used to extend the """"onion-model"""" of open source participation."""	open-source software;social network analysis;software developer	Samuel Felix de Sousa;Marco Antonio Balieiro;Jean Marcel dos Reis Costa;Cleidson R. B. de Souza	2009	2009 42nd Hawaii International Conference on System Sciences	10.1109/HICSS.2009.831	social network analysis;social software engineering;data mining;world wide web;social network	SE	-75.1104647532751	21.892096721874886	36540
254f2de502c12aaab478cad30dc7fbc5f6c40a55	enterprise mobility and support outsourcing: a research model and initial findings	outsourcing;enterprise mobility;support;standardization	The evolution of the wireless industry and the rapid proliferation of a mobile workforce have left businesses at a disadvantage. Business customers must be creative with currently available support resources in order to address their needs. Organizations with high mobile usage maturity levels are moving towards greater device and policy standardization and are seeking a solution to their problems. Given the intrinsic gaps in current mobile network operator business models to meet business customers' wireless device support needs, and that businesses themselves are just starting to develop their capabilities in-house, this paper investigates organizational receptiveness towards outsourcing and the potentially compelling benefits that outsourcing offers.	outsourcing	Christina C. Loh;Andrew D. Stadlen;Rahul C. Basole;John D. Moses;Conor Tuohy	2008	Information, Knowledge, Systems Management		operations management;knowledge process outsourcing;business;commerce;outsourcing	HCI	-74.7774706996492	6.213920116290704	36586
11753d0f6d2ff23451be13fc7c3f637e5be025f0	formal workflow design analytics using data flow modeling	workflow design;data dependency;activity dependency;activity relations;business process automation	a r t i c l e i n f o Keywords: Workflow design Data dependency Activity dependency Activity relations Business process automation Workflow design has become a critical function in enterprise information management. However, only scant research attention has been paid to formal workflow design methodologies. As a result, existing design methods in business process management remain a manual and experiential effort and result in inefficiency in design tasks and potential errors in workflow models. Considering that there are hundreds and thousands of business processes in organizations worldwide, overcoming this deficiency will have an enormous technical and economic impact on enterprise information management. In this paper, we investigate the possibility of incorporating formal analytics into workflow design, thus alleviating the intellectual challenge faced by business analysts when creating workflow models. The workflow design analytics we propose helps construct a workflow model based on information about the relevant activities and their associated data. In addition, our workflow design approach also helps determine whether the given information is sufficient for generating a workflow model and ensures the avoidance of certain workflow anomalies. The significance of our study is to enable the transformation of workflow design from a manual and experiential effort into a more systematic and rigorous approach. Over the last decade during the drive towards e-business, many organizations have realized the importance of Internet-based business process automation as they face the challenge of obtaining competitiveness in the global marketplace. To achieve high levels of business process efficiency , many organizations resort to workflow management systems to integrate business activities that span multiple functional units, such as human resources, marketing, and manufacturing [31,33]. As a prerequisite for effective workflow management, a workflow model is required to specify the execution sequence of activities needed to support certain business functions, such as producing a product or offering a service. Thus, it is critical to design correct workflow models efficiently according to specific business requirements. The existing workflow design approaches, such as the participative approach widely used in practice, mainly focus on collecting business requirements rather than providing a rigorous procedure for generating workflow models. The lack of formal design approaches often causes design problems. Recent empirical studies have shown an error rate of 10%–20% from over 2000 process models used in different industry practice , which include the widely used SAP reference model, the models used in a process reengineering project in the …	angular defect;business process;business requirements;code refactoring;data dependency;dataflow;electronic business;enterprise information management;reference model;requirement	Sherry X. Sun;J. Leon Zhao	2013	Decision Support Systems	10.1016/j.dss.2013.01.028	workflow;xpdl;computer science;knowledge management;business process management;data mining;database;event-driven process chain;world wide web;workflow management system;workflow engine;workflow technology	DB	-72.5444368719932	11.757789038058043	36644
a42384da6f643a9e4b55b4e74b9af600a1179dc5	handling dynamic organizational change with community-based policy management	organisational aspects management of change;disaster management;resource management;knowledge management;data engineering;access control resource management knowledge management engineering management human resource management disaster management context modeling security data engineering educational institutions;organizational modeling dynamic organizational change community based policy management resources management changing business goals;engineering management;policy based management;organizational change;community based policy management;management of change;dynamic organizational change;resources management;organizational modeling;access control;security;context modeling;human resource management;changing business goals;policy management;organisational aspects	Policy-based management (PBM) aims to provide flexibility in the management of resources so as to readily reflect changing business goals. However, as organizations increasingly use electronic means for more of their core business operations, the ability to ensure that policies accurately reflect the operation of an organization becomes more challenging. This paper presents a critique of organizational modeling abstractions used in existing policy and access rule schemes.	netpbm format	Kevin Feeney;David Lewis;Vincent P. Wade	2007	Eighth IEEE International Workshop on Policies for Distributed Systems and Networks (POLICY'07)	10.1109/POLICY.2007.24	knowledge management;environmental resource management;management science;business	DB	-74.47111738892097	14.68427944075776	36663
094e5f0be3c180e07e27e16f933311afcf89da97	track & trace system with serialization prototyping methodology for pharmaceutical industry in eu		This paper provides principles and proposals of the Track u0026 Trace system for purposes of serialization integration in drug identification in pharmaceutical industry. Common principles of industrial product marking are introduced with special part of 2-D Data Matrix code which has been recommended by GS1 consortium for pharmaceutical purposes. The principles of saving production data during whole supply chain between manufacturer and customer to local authority systems will be shown as well. Later chapters uncover some overlooked problems of Tu0026T implementation process and describe suitable methods that minimize risks of failures of final implementation project. Following the theoretical analysis of technology Track u0026 Trace and serialization will be processed as practical methodology.	serialization	Josef Horalek;Vladimir Sobeslav	2017		10.1007/978-3-319-65515-4_15	systems engineering;serialization;data matrix;real-time computing;supply chain;pharmaceutical industry;track and trace;engineering	Metrics	-65.62755176343637	13.070589636244872	36667
1742cfb5619935a1aef23de4d3ea18cc9f5fcba0	fractal management systems for extended, holonic enterprises	holonic enterprises;fractal management systems;management system	The complexity and the extension of the production environment are increasing at a fastening pace. This ever-increasing complexity requires new and more flexible Management Systems and decision supports that go above and beyond ERP’s. Any information system designed and developed to support a manufacturing enterprise has as a (sometimes implicit) first step: the modeling of the enterprise. Four metaphors are briefly explored, in relation to the increasing complexity of the environment to be modeled, and to the parallel increasing flexibility of the resulting models: rnrnrnThe econometric modelrnrnrnThe hydraulic modelrnrnrnThe cybernetic modelrnrnrnThe chain value (Porter)rnrnrnrnIn these last years a new enterprise organization called holonic or cellular has been defined. It is embodied by a multilevel network of holons, a network that has a high degree of flexibility, because it can easily restructure itself depending on the circumstances: in fact each holon has the capability of surviving even if detached by any network, and the ability to search for a new network to integrate into. The holonic structure has a fractal nature, so that each holon is in turn made of holons, and the single persons cooperating in the enterprise represent the lower level. This structure is the best one for enduring the chaotic evolution of the manufacturing environment. The paper concludes with the proposal of a modeling technique to be used when designing the Management System for a manufacturing organization.	fractal;holon (philosophy)	Pierluigi Assogna	2001			systems engineering;knowledge management;operations management;business	DB	-63.447831836269124	11.969251613227831	36749
639ae330fbcac651907ad78b2da0818414dfaa56	stepping up towards greater alignment of repository networks	standards;harmonization;interoperability;repositories	Over the past few years, the Confederation of Open Access Repositories (COAR) has established the initiative “Aligning Repository Networks” in order to foster global repository interoperability and align policies and practices internationally. This paper outlines several activities to align major repository networks across the globe strategically, technically and on the service level.	stepping level	Katharina Müller;Arvid Deppe;Maxie Gottschling;Elóy Rodrigues;Kathleen Shearer	2016			interoperability;computer science;knowledge management;data mining;world wide web	Networks	-69.35645545070886	11.99367557378894	36833
cdb8448038e8705b5ffa7e3025dc70c3ea9e6d4a	interdisciplinary design research for end-user software engineering	004;interdisciplinary design;end user software engineering;interdisciplinary design empirical studies of programmers psychology of programming real world research	"""My research style involves constantly drawing comparisons from one field to another – across academic disciplines, and also across application domains. In these terms, End-User Software Engineering is neither an application domain, nor an academic discipline, but a technological attitude or strategy, applicable in many domains, while also profiting from many research methods and theory bases. In this respect, it is an ideal opportunity for the multidisciplinary enquiry and analogical comparisons on which I habitually base my own research [1]. The phrase """"end-user software engineering"""" itself relies on an analogy, in the sense that software engineering is a professional discipline, whereas the end-users whom we hope to assist are defined precisely by the fact that they are not professionals (at least, not software professionals). Our aim in this research is to identify those techniques within software engineering that might offer most benefit to end-users, potentially including tools for specification, debugging, revision management and so on. As a teacher of professional software engineering, I often draw on the experience of other professional fields, especially design disciplines such as architecture, typography and performance composition [2]. There are certain recurring themes across these design disciplines that I have found to offer substantial insights to professional software engineering. I believe that these same themes can also be productive sources of innovation, by making new analogies to end-user software engineering. In the remainder of this statement, I reflect on some of these analogies. Design takes place in a social context, and is a social process. Our studies of end-user configuration and automation of domestic technologies demonstrate the extent to which family relations and gender roles spill over into practices of end-user programming [3]. Design processes involve modeling – simplifying or abstracting some aspects of the problem domain in order to plan and evaluate design decisions. The use of representations to reason about future consequences is fundamental to end-user software engineering. The constraints that representations place on design activities are described by the cognitive dimensions of notations framework [4], and in turn by a great variety of research into visual representations. Abstract reasoning about the future can be described in terms of the attention investment model [5]. A productive approach to end-user software engineering is to modify users' perception of this investment, whether by Burnett's Surprise-Explain Reward strategy, or by the use of machine learning techniques to infer possible abstractions that might be suggested to the …"""	application domain;cognitive dimensions of notations;debugging;end-user development;machine learning;problem domain;software engineering;version control	Alan F. Blackwell	2007			engineering ethics;computer science;systems engineering;management science	SE	-64.03054406761456	18.18569472580139	36880
cfc5187839e47765257d642987310f77ba0effaa	tkined/scotty: tools for network documentation, monitoring and troubleshooting	network documentation		documentation	Kenneth H. Jacker	1996		10.1145/237466.237675	knowledge management;software engineering;computer science;documentation;troubleshooting	OS	-63.602777098679994	24.508873022009848	37016
6463bbe759bfeaa58b5ec50b9b3bbe5a50e4a924	integrating software by integrating people	software;strategy peopleware teams collaboration;software integration;integrating people;computer graphics;collaboration;integrated technical software solutions;companies;software engineering;teams;team working computer graphics software engineering;team working;strategy;oil and gas;software development teams;substantial revenue growth;awards activities;lead;business;software development;organizations;peopleware;landmark graphics;integrating software;companies collaboration software organizations business awards activities lead;substantial revenue growth integrating software integrating people landmark graphics integrated technical software solutions software development teams	The mission of Landmark graphics was clear - develop integrated technical software solutions for the oil and gas market. In short order it became obvious that in order to develop integrated solutions it would be necessary to integrate the software development teams. Several synergistic initiatives were undertaken to encourage cross team integration in order to support improved software integration. The results from the successful integration catapulted Landmark into a dominant market leading position that provided substantial revenue growth.	graphics;software development;synergy;system integration	Todd Little	2008	Agile 2008 Conference	10.1109/Agile.2008.66	systems engineering;engineering;knowledge management;software engineering	Robotics	-66.33442311853479	20.955832586305085	37146
9a8c26f5c316c70116c832632c679f07d364fa08	context-dependent performance standards in dea	performance measure;performance evaluation;efficiency;building performance;service operation;performance standards;data envelopment analysis dea;relative efficiency;a priori information;context dependent;difference set;decision making unit;data envelope analysis	Data envelopment analysis (DEA) is a mathematical approach to measuring the relative efficiency of peer decision making units (DMUs). It is particularly useful where no a priori information on the tradeoffs or relations among various performance measures is available. However, it is very desirable if “evaluation standards,” when they can be established, be incorporated into DEA performance evaluation. This is especially important when service operations are under investigation, because service standards are generally difficult to establish. The approaches that have been developed to incorporate evaluation standards into DEA, as reported in the literature, have tended to be rather indirect, focusing primarily on the multipliers in DEA models. This paper introduces a new way of building performance standards directly into the DEA structure when context-dependent activity matrixes exist for different classes of DMUs. For example, two sets of branches, whose transaction times are known to be different from each other, usually have two different activity matrixes. We develop a procedure so that a set of standard DMUs can be generated and incorporated directly into the DEA analysis. The proposed approach is applied to a sample of 100 branches of a major Canadian bank where different sets of time standards exist for three distinct groups of branches.	best practice;context-sensitive language;data envelopment analysis;iteration;iterative method;performance evaluation;web standards	Wade D. Cook;Joe Zhu	2010	Annals OR	10.1007/s10479-008-0421-3	operations management;efficiency;context-dependent memory;data mining;data envelopment analysis;mathematics;efficiency;operations research;difference set;statistics	Web+IR	-80.9010194203837	9.630860762330578	37152
a7c272a7763d33c35bb9f55e02aaac1c7fac946f	the safety of systems - proceedings of the fifteenth safety-critical systems symposium, bristol, uk, february 13-15, 2007	software engineering;system performance				2007		10.1007/978-1-84628-806-7	safety engineering;reliability engineering;verification and validation;software engineering process group;performance engineering;system of systems;system of systems engineering;software verification;systems engineering;social software engineering;software development;software construction;systems development life cycle;software deployment;software requirements;software system;computer engineering	Embedded	-63.058250306999305	25.331697549665375	37170
62e24579fd84aaae97edf92f577b091761df5354	effect of task processes on programmer productivity in model-based testing	testing;task processes;programmers;model based development;productivity	"""Research on software process has mostly focused on the overall process of a project or an organization, and on optimizing or improving it. While overall process clearly influences the productivity in a project, it is also true that majority of the effort in a project is spent in executing tasks by programmers or testers. Hence, for a given overall process, productivity is influenced by how efficiently individual programmers execute various tasks. In this work, we focus on processes programmers employ for executing tasks, which we call """"task processes"""", and their impact on a programmer's productivity. For this study, we focus on the task processes for unit testing of modules in a model-based development. We present our approach for studying the task processes used by programmers through video recording of computer monitors of the programmers. We then discuss the results of the field study performed in a CMMi level 5 software company for about four months on a live project by studying execution of tasks by six programmers."""	capability maturity model integration;computer monitor;field research;model-based testing;model-driven engineering;programmer;programming productivity;software development process;software testing;unit testing;video	Damodaram Kamma;Pankaj Jalote	2013		10.1145/2442754.2442758	productivity;real-time computing;simulation;pair programming;computer science;operating system;software engineering;software testing;programming language;model-based design	SE	-65.52678984970125	29.933247937930542	37179
ecd5cd3cac4a675c7a991af2c0aca2cd4357e42d	project management, a bit of communication helps too, on succeeding	project manager	The following paper describes innovative methods and tools for planning and following up software development projects. A research study has been undertaken by a multinational company developing software applications and systems. The research study was based on an extensive literature survey in order to draw up a theoretical frame of reference and knowledge platform in the field of Project Management. The Anatomy concept joined with the On Line Project Monitoring System aim to represent a coherent proposal for project monitoring and control projects.		Enzo Gentili;C. Nidasio;M. Varchetta	1998		10.1007/978-0-387-35321-0_51	level of effort;project management;real-time computing;simulation;change order;earned value management;software project management;computer science;knowledge management;project management triangle;schedule;project portfolio management	Theory	-67.3608151145973	16.44071572741367	37217
177f9ef20803835e22dbfe8323014b245bc5f57f	egovernment standard framework	software;egovframe egovernment;framework platform;national information society agency of korea;egovframe egovernment standard framework;nia national information society agency of korea;nia;standard framework;open source	The eGovernment Standard Framework is an infrastructure environment for implementing application SWs and provides basic functions in the application SW runtime. The eGovernment Standard Framework has an objective to increase the quality of eGovernment services, the efficiency of IT investment and the standardization and the reusability of application SWs through establishing and applying the development framework standard.		Z. MarianoGamboa;Jano U. López Rodríguez;Chang Keun Son	2014		10.1145/2612733.2619953	systems engineering;engineering;knowledge management;computer security	Web+IR	-70.54193072813112	13.6278462938358	37270
d825d59b975792d274c67b86bbf9a39efa796178	grand timely topics in software engineering		This paper provides a survey of recent work on adapting techniques for program analysis to compute probabilistic characterizations of program behavior. We survey how the frameworks of data flow analysis and symbolic execution have incorporated information about input probability distributions to quantify the likelihood of properties of program states. We identify themes that relate and distinguish a variety of techniques that have been developed over the past 15 years in this area. In doing so, we point out opportunities for future research that builds on the strengths of different techniques.	algorithm;data-flow analysis;dataflow;feasible region;program analysis;software engineering;symbolic execution	Jácome Cunha;João Paulo Fernandes;Ralf Lämmel;João Saraiva;Vadim Zaytsev	2015		10.1007/978-3-319-60074-1	software engineering;computer engineering;systems engineering;computer science	SE	-62.962638940791024	29.853462219838097	37302
c8e0848e174397d786b2539f48a36030190a25ac	resilience analysis of service-oriented collaboration process management systems		Collaborative business process management allows for the automated coordination of processes involving human and computer actors. In modern economies, it is increasingly needed for this coordination to be not only within organizations but also to cross organizational boundaries. The dependence on the performance of other organizations should, however, be limited, and the control over the own processes is required from a competitiveness perspective. The main objective of this work is to propose an evaluation model for measuring a resilience of a service-oriented architecture (SOA) collaborative process management system. In this paper, we have proposed resilience analysis perspectives of SOA collaborative process systems, i.e., overall system perspective, individual process model perspective, individual process instance perspective, service perspective, and resource perspective. A collaborative incident and maintenance notification process system is reviewed for illustrating our resilience analysis. This research contributes to extend SOA collaborative business process management systems with resilience support, not only looking at quantification and identification of resilience factors, but also considering ways of improving the resilience of SOA collaborative process systems through measures at design and runtime.	business process;business requirements;competitive analysis (online algorithm);elasticity (cloud computing);elegant degradation;failure cause;modeling language;parallel computing;proactive parallel suite;process architecture;process modeling;run time (program lifecycle phase);service-oriented architecture;service-oriented device architecture;service-oriented infrastructure	Paul de Vrieze;Lai Xu	2018	Service Oriented Computing and Applications	10.1007/s11761-018-0233-5	psychological resilience;computer science;business process management;architecture;process management;resilience factors	SE	-74.80929972716177	11.799649264327972	37395
9dceda2124d942c846e5417a8ac8907f796208e0	special issue on collaboration in software testing between industry and academia		Testing is the most widely practiced method to detect defects and to improve quality in the software industry. Many projects rely on testing as their primary quality assurance measure (Orso and Rothermel 2014). The practical importance of testing is also reflected by the often enormous budgets dedicated to this form of quality assurance (Huang and Boehm 2006). The academic interest is emphasized by a long and outstanding research history on static and dynamic testing of software systems (Orso and Rothermel 2014). Software testing is covered by dedicated academic conferences and events, tracks in major software engineering venues, and a wide array of scientific publications. Yet, how does this research impact the software industry? Despite the numerous ideas and approaches developed in academia, few research results have turned into practical applications that create a lasting benefit for practitioners. There is still a significant gap between the state of academic research and the state and needs of industrial practice. Bridging this gap is of mutual interest as collaboration between industry and academia supports improvement and innovation in industry and ensures industrial relevance in academic research. This special issue focuses on collaboration between industry and academia in the context of software testing. It follows a series of events on industry-academia collaboration, such as the Workshop on Testing: Academia-Industry Collaboration, Practice and Research Techniques (TAIC PART) (Ramler et al. 2016; Alshahwan et al. 2015). The special issue comprises six papers that describe lessons learned from collaboration initiatives, share insights into the challenges and opportunities involved in the joint work of researchers and practitioners, and provide suggestions and approaches for better connecting the two sides. The first paper Industry-academia collaborations in software testing: experiences and success stories from Canada and Turkey by Vahid Garousi, Matt M. Eskandar, and Kadir Software Qual J https://doi.org/10.1007/s11219-017-9395-1	bridging (networking);dynamic testing;relevance;scientific literature;software engineering;software industry;software system;software testing	Michael Felderer;Rudolf Ramler	2017	Software Quality Journal	10.1007/s11219-017-9395-1	software engineering;systems engineering;software;engineering	SE	-66.31009133875574	24.01743505529951	37428
3f1e0cbd214d3c426a56951bf42b3a1bdab15f60	logical — development of cloud computing platforms and tools for logistics hubs and communities	software architecture cloud computing innovation management logistics data processing small to medium enterprises;logistics data processing;small to medium enterprises;logistics cloud computing companies communities europe interviews;companies;software architecture;innovation management;logistics;interviews;europe;communities;cloud computing;sme size lsp logical clouds cloud computing platform development cloud computing tool development logistics hubs logistics communities logistics service providers logistics sector process fragmentation speed demands customization demands service level demands ict infrastructure international logistics innovative cloud computing technologies service management service integration cloud architecture	Logistics service providers (LSP) are facing an increasing complexity of the logistics sector, i.e. growing levels of process fragmentation plus increasing speed, customization and service demands of logistics clients. Adequate powerful, integrated ICT infrastructure and tools are a prerequisite for keeping pace with the ever increasing service level demands within international logistics. The Cloud Computing technology offers significant advantages for data, process and service management and integration. To cope with the related innovation and migration needs, the Central Europe project LOGICAL focuses on the development and implementation of innovative cloud computing technologies. Special attention is devoted to international cooperation of SME-size LSPs. This paper introduces the conceptual basics of LOGICAL, basic use cases requested by the LSPs, and the addressed target groups of LOGICAL clouds. The results of an extensive user survey and demand analysis are presented as well as the related consequences for the cloud architecture.	cloud computing;fragmentation (computing);itil;logistics;tag cloud	Uwe Arnold;Jan Oberländer;Björn Schwarzbach	2012	2012 Federated Conference on Computer Science and Information Systems (FedCSIS)		logistics;software architecture;interview;humanitarian logistics;cloud computing;innovation management;computer science;integrated logistics support	HPC	-72.38314690070371	13.543229487943874	37432
871cde36556b4d698ac4f3db083df83abf6c5151	toolbox: successfully implementing configuration management; top drawer	software maintenance file servers process planning testing software libraries environmental management vehicles software performance production application software;human resource management client server systems software development management configuration management software quality;software development process;client server systems;client server;software development;profitability;client server software development project configuration management product quality software development efficiency enterprise profitability organizations software development processes project components software tools software quality planning;product quality;configuration management;human resource management;software quality;software development management	TO INCREASE PRODUCT QUALITY, development efficiency, and enterprise profitability, many organizations are striving to achieve repeatable, engineered software development processes. Effective configuration management is essential to reaching this goal. CM can organize project components and streamline and control software development processes. A fully deployed and integrated CM solution consists of several tools, as shown in the box on page 100. By carefully designing how CM components fit your processes and how your processes will change as a result of using CM, you can significantly enhance your software quality. The effectiveness of a particular CM solution will depend on how you integrate the CM toolset into a team-oriented development environment. We suggest an approach and provide planning and implementation guidelines for a pilot clientserver software development project. To take advantage of the techniques we describe, your organization should ♦ have a development environment where only one release of an application is in production at a time, ♦ have only one developer working on the same file at a time, and ♦ have some experience in client/server development. Prior experience with CM is not necessary.	adobe streamline;client–server model;configuration management;content-control software;pilot ace;server (computing);software development;software quality;thinking outside the box	Alan Schamp;Heidi Schamp	1997	IEEE Software	10.1109/52.566435	reliability engineering;personal software process;long-term support;verification and validation;software quality management;software sizing;software configuration management;software project management;systems engineering;engineering;package development process;social software engineering;software development;software engineering;release management;human resource management;software construction;software as a service;configuration management;process management;management;software deployment;software quality control;software development process;software quality;client–server model;profitability index;software quality analyst;software system;software peer review	SE	-67.25618972032055	21.813554919724986	37521
29b1d938293808c0e52946a38d7b6c73a58fb401	operations research in postal services - a survey	operations research	Abstract   The collection, processing and delivery of letters and packages comprise one of the largest service industries in the world. This paper provides a broad survey of the application of operations research techniques to analyze and resolve decision problems forced by postal managers in various countries. The problem areas considered include: forecasting, cost and revenue analysis, postal network design, manpower planning and scheduling, maintenance, and service characteristic measurement. The paper concludes with a brief description of the efforts to train postal managers in the use of operations research in their decision-making functions.	operations research;postal	Jatinder N. D. Gupta;LeRoy J. Krajewski	1977	Computers & OR	10.1016/0305-0548(77)90022-3	computer science;mathematics;management science;operations research	ECom	-64.54293283795661	5.067550727923318	37545
47bf50e11dd2fb65442439b7f3d8bf1828c0923e	a consolidated process for software process simulation: state of the art and industry experience	analytical models;software;software process simulation;software process improvement;telecommunication industry;training;empirical;testing;companies;software engineering;guidelines;telecommunication services digital simulation software process improvement telecommunication industry;empirical software process simulation;telecommunication services;telecommunication vendor consolidated process software process simulation modelling spsm;guidelines data models analytical models companies software testing training;programvaruteknik;digital simulation;data models	Software process simulation is a complex task and in order to conduct a simulation project practitioners require support through a process for software process simulation modelling (SPSM), including what steps to take and what guidelines to follow in each step. This paper provides a literature based consolidated process for SPSM where the steps and guidelines for each step are identified through a review of literature and are complemented by experience from using these recommendations in an action research at a large Telecommunication vendor. We found five simulation processes in SPSM literature, resulting in a seven-step process. The consolidated process was successfully applied at the studied company, with the experiences of doing so being reported.	experience;formal system;level of detail;simulation;software development process;venue (sound system)	Nauman Bin Ali;Kai Petersen	2012	2012 38th Euromicro Conference on Software Engineering and Advanced Applications	10.1109/SEAA.2012.69	data modeling;empirical evidence;computer science;systems engineering;engineering;telecommunications service;software engineering;management science;software testing	SE	-65.78484401686232	27.765657958961924	37605
dfb3c60c5abce985f4dcceb56a36a1646229074e	hacker or hero? - extreme programming today (panel session)	extreme programming	Extreme programming is the latest rage, everyone is talking extreme, but who is doing it? XP is in the words of one proponent, is a “lightweight, efficient, low-risk, predictable, scientific, and fun way to develop software”. XP bundles much conventional software engineering wisdom into a practice with a high degree of appeal as a cool technology. Questions for inquiring minds include: Will XP deliver? Will XP scale? How will products based on software developed by XP practices age? What are the elements of XP that can be effectively adopted by organizations outside the XP envelop, e.g. large teams, real-time systems, etc. Is XP the next “silver bullet”?	extreme programming;hacker;mind;no silver bullet;real-time computing;real-time transcription;software engineering	Steven Fraser;Kent L. Beck;Ward Cunningham;Ron Crocker;Martin Fowler;Linda Rising;Laurie A. Williams	2000		10.1145/367845.367892	real-time computing;simulation;extreme programming;computer science;operating system	SE	-69.48538265065208	26.985920873178234	37653
53e7478353223ec6d706ea4d44b33cfbcb5c4c17	a study on the relationship between rural-urban income gap and human capital investment disparity in china: a case study on yunnan province	investment economics time series analysis correlation error correction analytical models educational institutions;error correction model urban rural income gap urban rural human capital investment disparity adf test eg co integration test;urban rural human capital investment disparity;adf test;urban rural income gap;investment;statistical testing autoregressive processes error correction investment regression analysis;autoregressive processes;error correction;eg co integration test;regression analysis;statistical testing;engle granger test rural urban income gap human capital investment disparity china yunnan province adf test eg co integration test error correction model bidirectional granger causality human capital investment gap first order autoregression model;error correction model	This paper, taking Yunnan Province as an example, examines the relationship between rural-urban income gap and human capital investment disparity in China by using ADF test, EG co-integration test and error correction model. The study finds that: firstly, a bi-directional Granger causality existed between the human capital investment gap and income gap in the urban and rural areas of Yunnan Province throughout the period 1991-2010, there existed a positive correlation between the two variables. secondly, the human capital investment gap between urban and rural areas in Yunnan Province was just the one-way Granger causality of the income gap in the short term. Finally, this paper put forward some relevant countermeasures and proposals.	binocular disparity;causality;error correction model;eurographics;integration testing;one-way function	Jianmin Zhang;Xiaoqing Duan;Li Lin;Yuhan Ma	2013	2013 IEEE International Conference on Granular Computing (GrC)	10.1109/GrC.2013.6740449	error correction model;investment;regression analysis;statistics	Robotics	-89.00134675913206	7.044753137683467	37807
edb1fd534a081ae4ed697b138c5a03c8e4168886	from information to operations: service quality and customer retention	empirical study;customer retention;data mining;pattern discovery;indexation;data analytics;service quality index;business intelligence;customer churn;service quality	In business, information is abundant. Yet, effective use of that information to inform and drive business operations is a challenge. Our industry-university collaborative project draws from a rich dataset of commercial demographics, transaction history, product features, and Service Quality Index (SQI) factors on shipping transactions at FedEx. We apply inductive methods to understand and predict customer churn in a noncontractual setting. Results identify several SQI variables as important determinants of churn across a variety of analytic approaches. Building on this we propose the design of a Business Intelligence (BI) dashboard as an innovative approach for increasing customer retention by identifying potential churners based on combinations of predictor variables such as demographics and SQI factors. This empirical study contributes to BI research and practice by demonstrating the application of data analytics to the fundamental business operations problem of customer churn.	kerrison predictor	Balaji Padmanabhan;Alan R. Hevner;Michael Cuenco;Crystal Shi	2011	ACM Trans. Management Inf. Syst.	10.1145/2070710.2070712	customer to customer;marketing;operations management;data mining;customer intelligence;business;customer retention;service quality;customer advocacy	DB	-79.0673494307837	7.844262708067193	37838
9c956f5fb82b436c98b4e2211c6640f5b34303df	sensing social media for corporate reputation management: a business agility perspective	business intelligence and knowledge management	The concept of business agility reflects an organization’s need to develop sensing capabilities for being able to respond to changes in the business environment rapidly. Therefore, intelligent information systems are needed to support decision makers with accurate and timely information. Since corporate reputation is among the most valuable assets, organizations need efficient measuring techniques for being able to manage it. Recently, due to the advent of social media new reputational challenges have emerged for firms, since such technologies significantly increase the risk for being associated with negative issues. Therefore, organizations should utilize there IT-systems for actively sensing social media content as a basis for a quick response to reputational threats. Accordingly, we provide an empirical example on how firms might improve corporate reputation management through sensing social media. Specifically, we analyze a dataset of 271,207 messages about a large American Bank collected from the public microblogging platform Twitter. For our empirical investigation, we applied automated sentiment analysis and manual content analysis. Our results demonstrate how social media might impact corporate reputation and what organizations can do to prepare themselves. Beyond corporate reputation management, analyzing social media content may be valuable for many other purposes to improve an organization’s sensing capabilities.	information system;reputation management;sentiment analysis;social media	Christoph Seebach;Roman Beck;Olga Denisova	2012			public relations;computer science;knowledge management;business process management;media management;process management;information management;design management;business relationship management;philosophy of business;business activity monitoring	AI	-78.85035486866002	7.143704486460476	37869
f1c6669cc3e1dc4dc85c054249b4cc2d874eca27	an ant colony optimization heuristic to optimize prediction of stability of object-oriented components	software quality estimation models ant colony optimization heuristic stability prediction optimization ieee 729 1983 standard software quality assessment software composite characteristics aco approach;ant colony optimization;measurement;software quality ant colony optimisation object oriented programming;accuracy;stability analysis;measurement accuracy object oriented modeling software quality ant colony optimization stability analysis;object oriented modeling;software quality	"""The IEEE 729-1983 Standard defines software quality as """"the composite characteristics of software that determine the degree to which the software in use will meet the expectations of the customer."""" Assessing software quality in the early stages of design and development is crucial in reducing time and effort. Various metrics have been proposed for estimating software quality characteristics from measurable attributes. This paper presents an Ant Colony Optimization (ACO) approach that improves the prediction accuracy of software quality estimation models by intensifying the search around the metric neighborhood. The method has been implemented, and favorable results comparisons are reported."""	algorithm;ant colony optimization algorithms;component-based software engineering;heuristic;software quality	Haidar M. Harmanani;Danielle Azar;Grace Zgheib;David Kozhaya	2015	2015 IEEE International Conference on Information Reuse and Integration	10.1109/IRI.2015.45	von neumann stability analysis;ant colony optimization algorithms;software sizing;search-based software engineering;data mining;accuracy and precision;software quality;measurement;metaheuristic;statistics	SE	-62.99889557902254	30.52242304728336	37901
5343defea8c0429fa634bf5bd1d200af17a00567	change management in e-infrastructures to support service level agreements	change management;maintenance;sla;e infrastructures	Service Level Agreements (SLAs) are a common instrument for outlining the responsibility scope of collaborating organizations. They are indispensable for a wide range of industrial and business applications. However, until now SLAs did not receive much attention of the research organizations that are cooperating to provide a comprehensive and sustainable computing infrastructures or e-Infrastructures (eIS) to support the European scientific community. Since many eIS projects have left their development state and are now offering highly mature services, the IT service management aspect becomes relevant.#R##N##R##N#In this article we are concentrating on the inter-organizational change management process. At present, it is very common for eIS changes to be autonomously managed by the individual resource providers. Yet such changes can affect the overall eIS availability and thus have an impact on the SLA metrics, such as performance characteristics and quality of service. We introduce the problem field with the help of a case study. This case study outlines and compares the change management process defined by PRACE and LRZ, which is one of the PRACE eIS partners and resource providers. Our analysis shows, that each of the organizations adopts and follows distinct and incompatible operational model. Following that, we demonstrate how the UMM, a modeling method based on UML and developed by UN/CEFACT, can be applied for the design of inter-organizational change management process. The advantage of this approach is the ability to design both internal and inter-organizational processes with the help of uniform methods. An evaluation of the proposed technique and conclusion ends our article.	service-level agreement	Silvia Knittl;Thomas Schaaf;Ilya Saverchenko	2011		10.1007/978-3-642-29740-3_15	second-language acquisition;knowledge management;change management;database	DB	-69.01626267306256	14.34665613448774	38114
375eac22e98e7085477c660d65434b5db440a84d	tariff structure of japanese electric power companies: an empirical analysis using dea	policy making;electric power industry;analisis envolvimiento datos;empirical analysis;pricing;prise decision;fijacion tarifa;coste marginal;tariffication;data envelopment analysis;tarification;cout marginal;industria electrica;performance analysis;marginal cost;private sector;electric power;efficiency measurement;toma decision;data envelope analysis;industrie electrique;fixation prix;nombre ramsey;numero ramsey;ramsey number;analyse enveloppement donnee	This research explores a Marginal Cost (MC)-based pricing system, using Data Envelopment Analysis (DEA). This DEA technique is widely applied for performance analysis and eciency measurement in public and private sectors. This article does not follow such a previous research direction, rather directing itself towards the new measurement of MC and Ramsey prices of multiple electric power services. As an important case study, this research applies the proposed DEA approach to examine how much the current taris of Japanese electric power services deviate from these MC and Ramsey prices. This type of research has never been explored in Japan, even though such a research eort is needed by many policy makers and consumers. It is hoped that our empirical ®ndings can serve as a policy-making basis for guiding the Japanese electric power industry. This study believes that this Japanese experience is useful for other nations, such as Asia-Paci®c countries, where many public industries are regulated by their governments. Ó 1999 Elsevier Science B.V. All rights reserved.	compiler;data envelopment analysis;linkage (software);marginal model;open road tolling	Toshiyuki Sueyoshi	1999	European Journal of Operational Research	10.1016/S0377-2217(98)00313-0	economics;public economics;marketing;operations management;data envelopment analysis;mathematics;economy	SE	-83.82980394490943	8.646851412902414	38184
a81882f96a72f0fe605e9c4a987dede58d496b00	improving information quality through it alignment planning: a case study	strategic planning;process design;information quality;process improvement	This article describes how a medium-sized, Midwestern power company implemented an IT alignment planning process. The IT alignment planning process was a successful four-year activity that involved, first, a pilot implementation and then a companywide implementation of the IT alignment planning process. Designed to be flexible and to dovetail with corporate strategic planning processes, IT alignment planning achieved acknowledgment and approval in all divisions of the company. The IT alignment planning process improved and facilitated communication on IT and IT projects throughout the company, from the executive level to the operational level, and brought the IT and client units closer together.	acknowledgment index;information quality	Dan Peak;Carl Stephen Guynes	2003	IS Management	10.1201/1078/43647.20.4.20030901/77289.4	process design;strategic planning;computer science;knowledge management;information quality;world wide web;strategic alignment	AI	-77.68468289372335	10.7615659554753	38196
8ee8293fda55000ae04328ee5999cb74c4820dc4	what is end-user software engineering and why does it matter?	end user development;software engineering;end user software engineering;software quality;end user programming	End-user programming has become ubiquitous, so much so that there are more end-user programmers today than there are professional programmers. End-user programming empowers—but to do what? Make really bad decisions based on really bad programs? Enter software engineering’s focus on quality. Considering software quality is necessary, because there is ample evidence that the programs end users create are filled with expensive errors. In this paper, I consider what happens when we add to end-user programming environments considerations of software quality, going beyond the “create a program” aspect of end-user programming. I describe a philosophy to software engineering for end users, and then survey several projects in this area. A basic premise is that end-user software engineering can only succeed to the extent that it respects the fact that the user probably has little expertise or even interest in software engineering.	end-user development;programmer;software development process;software engineering;software quality;software release life cycle	Margaret M. Burnett	2009		10.1007/978-3-642-00427-8_2	personal software process;computing;software quality management;software engineering process group;computer science;systems engineering;software design;social software engineering;software framework;component-based software engineering;software development;software engineering;software walkthrough;software requirements;software quality;software quality analyst;computer engineering	SE	-69.23346594133764	26.50436879593359	38372
f11311d216e702509f084cd461c5864011e1b3bd	a process to reuse experiences via narratives among software project managers	project management;knowledge reuse;selected works;bepress selected works;knowledge management;software project management;knowledge management project management knowledge reuse;grounded theory;bepress;research framework;tool evaluation;design science	Organizations have lost billions of dollars due to poor software project implementations. Software project management is a complex process requiring extensive planning, effective decision-making, and proper monitoring throughout the course of the project. The knowledge one gains during a project is rarely captured and reused on subsequent projects. In an effort to enable software project managers to repeat prior successes and avoid previous mistakes, this research seeks to improve the reuse of a specific type of knowledge among software project managers, experiences expressed via written narratives. This research proposes that software project managers can improve their management abilities by reusing their own and others’ past experiences using written narratives. This research leverages multiple methodologies – including tool evaluation, grounded theory, design science research, and experimentation – throughout the phases of a design science research framework to create a process to enable software project managers to reuse knowledge gained through experiences on software projects. Guided by the design science research framework, this work leverages both explanation research – to understand the phenomenon of knowledge reuse among software project managers – and design science research – to create a process to facilitate knowledge reuse among software project managers – in an attempt to improve upon the current practices of software project management.	experience;software development;software project management	Stacie Petter	2005			project management;software review;team software process;extreme project management;program management;work breakdown structure;project;software project management;opm3;systems engineering;engineering;knowledge management;management science;project management 2.0;project management triangle;project charter;software development process;project planning;project portfolio management	SE	-70.42760446261211	22.539925669137148	38382
104f98304ca0a6acbc5a68766e26784022c41540	software metrics: progress after 25 years?	software metrics;software;software metrics software measurement books bicycles software quality capability maturity model area measurement pressing process planning solids;software measurement;uncertainty;presses;maintenance engineering;books;pressing;uncertainty software measurement process heuristics;bicycles;capability maturity model;safety;software metric;area measurement;heuristics;process planning;process;software quality;solids	"""This article traces the increasing sophistication and use of software measurement over the past 25 years. It highlights four obstacles to more effective use of measurement: dealing with uncertainty, anticipating change, measuring """"soft"""" characteristics, and developing heuristics."""	heuristic (computer science);software measurement;software metric;tracing (software)	Shari Lawrence Pfleeger	2008	IEEE Software	10.1109/MS.2008.160	maintenance engineering;reliability engineering;computer science;systems engineering;engineering;operations management;software engineering;software metric	SE	-63.4608367813733	30.705377994091336	38391
9f08be76244cc3c13b2d11588c4ca2e4d07084d4	the risk implications of mergers and acquisitions with information technology firms		AbstractWe address the dynamics of post-merger risks for a firm acquiring an information technology (IT) company over a long-term horizon, and examine the impact of mergers and acquisitions (Mu0026A) motives on its post-merger risks after controlling for its self-selection into the specific motives the acquiring firm seeks. We find that a strong run-up in risk occurs before Mu0026A transactions are initiated, but this risk begins to decline over the post-merger period. However, post-merger risks tend to persist for firms seeking Mu0026A transactions with a customer-side motive, whereas this does not occur with a production-side motive. While greater post-merger risks are associated with Mu0026As with a customer-side motive, our results suggest that its association with post-merger risks is moderated by industry dynamism. Overall, our study sheds new light on the post-merger risk by addressing its dynamic nature and uncovering the potential interplay between Mu0026A motives and industry dynamism when IT firms are acquired.		Young Bong Chang;Wooje Cho	2017	J. of Management Information Systems		economics;marketing;mergers and acquisitions;management;commerce	HCI	-84.08393656753888	4.665554719872652	38460
0024e30f2e33e601a2884fe5be0d55c6d722331a	systematic knowledge engineering: building bodies of knowledge from published research	systematic review;empirical software engineering;body of knowledge;systematic knowledge engineering;software inspection;software product lines	Context. Software engineering researchers conduct systematic literature reviews (SLRs) to build bodies of knowledge (BoKs). Unfortunately, relevant knowledge collected in the SLR process is not publicly available, which considerably slows down building BoKs incrementally. Objective. We present and evaluate the Systematic Knowledge Engineering (SKE) process to support efficiently building BoKs from published research. Method. SKE is based on the SLR process and on Knowledge Engineering practices to build a Knowledge Base (KB) by reusing intermediate data extraction results from SLRs. We evaluated the feasibility of applying SKE by building a Software Inspection BoK KB from published experiments and a Software Product Line BoK KB from published experience reports. We compared the effort, benefits, and risks of building BoK KBs regarding the SKE and the traditional SLR processes. Results. The application of SKE for incrementally collecting and organizing knowledge in the context of a BoK was feasible for different domains and different types of evidence. While the efforts for conducting the SKE and traditional SLR processes are comparable, SKE provides significant benefits for building BoKs. Conclusions. SKE enables researchers in a scientific community to reuse and incrementally build knowledge in a BoK. SKE is ready to be evaluated in other software engineering domains.	knowledge engineering	Stefan Biffl;Marcos Kalinowski;Rick Rabiser;Fajar J. Ekaputra;Dietmar Winkler	2014	International Journal of Software Engineering and Knowledge Engineering	10.1142/S021819401440018X	systematic review;computer science;systems engineering;engineering;knowledge management;artificial intelligence;body of knowledge;software engineering;data mining;software inspection;management	SE	-68.80828194317002	20.546479856320662	38470
70e8441843d9a31057a10a101a567826f744aac2	the exploitation and utilization of customer database in crm	management practice;analytical models;database system;business management crm customer relationship management customer database relational model structural chart;relational databases charts customer relationship management;marketing and sales mathematical model biological system modeling analytical models database systems profitability;crm;customer relationship management;biological system modeling;database;database systems;relational model;customer database relational model;mathematical model;profitability;relational databases;structural chart;cusromer;exlioitation crm database cusromer;exlioitation;charts;marketing and sales;business management	This paper, through the construction of customer database of the customer relationship management, has probed into the customer- database relational model and the development of the customer database procedure with emphasis. Then, it also has drawn the structural chart of the customer database system and has described the application mechanism of the customer database. Finally, combining with the business management practices, it has discussed about how the customer database can help business decisions.	competitive analysis (online algorithm);customer relationship management;exploit (computer security);personalization;relational database;relational model	Wang Hualin	2010	2010 International Conference on E-Business and E-Government	10.1109/ICEE.2010.796	enterprise relationship management;customer to customer;voice of the customer;customer relationship management;relational model;loyalty business model;relational database;computer science;marketing;customer reference program;chart;mathematical model;database;customer intelligence;process management;customer service assurance;customer retention;profitability index;conversion marketing;customer advocacy	DB	-69.13158026059163	4.600623295256804	38481
f778716da658eca32beacb19841575791877dcd0	systems thinking and construction productivity	systems thinking	This paper describes research undertaken to investigate the possibility of using systems to model productivity in construction. In particular, it concentrates on the use of systems dynamics and project level productivity. The literature identifies 34 factors affecting productivity but based on a survey of professionals, five of these are recognised as important. They form the basis of a systems model whose development is described. The model includes coefficients produced from analysis of subjective and qualitative views obtained from industry. The results are therefore relative and it would be necessary to validate and calibrate the model for quantitative use in practice.	coefficient;experiment;system dynamics	Michael Mawdesley;Sami Qambar	2000			management science;system dynamics;systems thinking;critical systems thinking;knowledge management;engineering	SE	-79.74830495524242	9.66736932584304	38491
d66f00ccb395bb568c859c4a0f60d4a91e9bf1ee	the challenges facing global erp systems implementations	enterprise resource planning systems;1503 business and management;global operations;erp system;college of business;global companies;business information systems;wide systems	Large global companies are increasing looking towards information systems to standardise business processes and enhance decision making across their operations in different countries. In particular these companies are implementing enterprise resource planning systems to provide this standardisation. This paper is a review of literature which focuses on the use of ERP systems to support global operations. There are many technological and cultural challenges facing these implementations. However a major challenge faced by companies is the balance between centralisation and localisation.	business process;centralisation;erp;enterprise resource planning;information system	Paul Hawking;Andrew Stein;Susan Foster	2007			enterprise application integration;global information system;knowledge management;artifact-centric business process model;management information systems;business process modeling;information system;business activity monitoring;business architecture	SE	-74.86285644535425	4.740095111552568	38495
642bd096c17b3fe4cfc4041bea7a9b2e83aa238d	a model of lean supplier management based on the lean production	quality assurance;supplier relationship management;performance evaluation;lean production;quality assessment;quality system;continuous improvement;indexation;operations management;selection criteria;supplier selection	In this paper, we present a model of lean supplier management between an OEM and its suppliers for the objectives of eliminating wastes, reducing cost and improvement continuously based on the lean production. This model includes supplier selection and categorization, supplier improvement, supplier certification and supplier evaluation. First, the supplier selection process and some basic principals about selection criteria are developed, and all suppliers will be categorized so that different management measures can be used effectively. Then, we design the Supplier Quality Assessment process that focus on a comprehensive, continuous improvement of supplier’s quality system and processes utilizing benchmarked and time-proven techniques. Finally, the index system of performance evaluation on lean suppliers is given in order to understand what performances a supplier has achieved over the past period, to identify chances that a supplier will be improved, and to provide evidences for re-certification of suppliers during next period.	benchmark (computing);categorization;jones calculus;lean integration;performance evaluation;purchasing;requirement;strategic management;supplier relationship management;supply chain attack	Yixun Guo;Zhiduan Xu	2007		10.1007/978-0-387-75902-9_81	lean project management;lean laboratory;systems engineering;operations management;process management;business	DB	-77.94722568083797	13.020902736084937	38706
f85b6cd0135719f86b34096669df10700a23e7d5	economic application of virtual commissioning to mechatronic production systems	design process;production system;development process;software engineering;system design;computer aided engineering;simulation environment	The interaction of heterogenous control hard and software plays a key role in enabling mechatronic production systems to become flexible and agile systems. Nevertheless, control software engineering still tends to be the last step within the development process. To a large extent it is carried out during the commissioning phase of the production ramp-up. On the first hand this leads to a loss of time and quality as well as to a loss of reputation and future orders on the second hand. A method that is referred to as Virtual Commissioning tries to overcome this situation. The aim is to enable control software engineering to, both take over the initiative in system design and to perform important activities earlier in the design process of production equipment. In this paper, the technological and economical scalability of Virtual Commissioning is analyzed. Based on the analysis, a technical concept for a scalable simulation environment is presented. The paper concludes with a new method for the economic application of Virtual Commissioning.	agile software development;assembly language;content-control software;interrupt;list of version control software;mechatronics;production system (computer science);ramp simulation software for modelling reliability, availability and maintainability;scalability;software engineering;software quality;systems design;uptime	Gunther Reinhart;Georg Wünsch	2007	Production Engineering	10.1007/s11740-007-0066-0	project commissioning;simulation;design process;software engineering process group;systems engineering;engineering;production system;software development process;manufacturing engineering;systems design;mechanical engineering	SE	-64.89257315214921	19.703777738464016	38778
b2a9fcafd7e8b8d38c08beb9c313eb7c691a8bdf	enterprise modeling for strategic support	enterprise modeling	SUCCESSFUL STRATEGIC BUSINESS ENGINEERING, whether a reactive effort to gain competitive advantage or a proactive effort to maintain and improve performance, depends on an organization’s ability to accurately and methodically analyze its internal and external environments, people, processes, organizational structure, information uses, and technology. Enterprise modeling (EM) greatly enhances strategic business engineering by providing a structured, diagrammatic framework for depicting the myriad interconnected and changing components addressed in large-scale change. Its representative models of the organization serve as baseline against which all subsequent change is measured and provide a basis for strategic planning. Using EM as a forecasting tool fosters a more effective and efficient planning process that dramatically increases the probabilities of success.	baseline (configuration management);business engineering;centralized computing;diagram;enterprise modelling;expect;ibm system i;qr code;semiconductor consolidation;serial ata	Michael E. Whitman;Michael L. Gibson	1996	IS Management	10.1080/10580539608906989	functional software architecture;enterprise system;enterprise systems engineering;enterprise software;enterprise modelling;computer science;knowledge management;architecture domain;integrated enterprise modeling;service-oriented modeling;process modeling;enterprise architecture management;strategic financial management;management science;enterprise architecture;enterprise integration;enterprise planning system;enterprise information system;business architecture;enterprise life cycle	AI	-73.25336067246205	8.546113881825388	38779
a93718a03942795c09a10c61d5153e371211e340	factors influencing the engagement between enterprise architects and stakeholders in enterprise architecture development		The development of Enterprise Architecture (EA) is facing several challenges. The highly referenced challenges in literature are related to enterprise architects and stakeholders. The enterprise architects and the stakeholders are the main actors in EA development. However, there are limited studies that cover the relationship of the enterprise architects and the stakeholders. The purpose of this paper is to identify the factors characterizing the engagement of enterprise architects and the stakeholders in EA development. The study used a systematic literature review (SLR) as a method to identify the factors and proposing an initial engagement model. The SLR revealed 12 factors that influence the engagement between the enterprise architect and the stakeholders. These factors are organized using the multiple perspective theory under three perspectives namely; technical, organizational and personal that comprise the initial engagement model. The study is contributing by shedding the light on the key aspects of engagement factors between the enterprise architects and the stakeholders in the development of EA. Furthermore, it is an initial step towards developing the engagement framework by comprehending these key aspects.	enterprise architect;enterprise architecture;systematic review	Hamood Al-Kharusi;Suraya Miskon;Mahadi Bahari	2016			business architecture;enterprise information security architecture;knowledge management;integrated enterprise modeling;enterprise architecture management;enterprise planning system;enterprise architecture;computer science;enterprise software;enterprise integration	Web+IR	-78.14407666752119	7.374999106872298	39011
035bed3daafa11d8e361f12c25b19a93bf219615	a diversity analysis of the impact of an interoperability tool to a business ecosystem	sme;electronic mail;small to medium enterprises ecology multi agent systems open systems;multi agent system;ecosystem analysis;collaboration;biological system modeling;diversity analysis;small to medium enterprises;ecology;companies;business collaborative interactions diversity analysis interoperability tool business ecosystem enterprise interoperability multi agent system sme;multi agent systems;business collaborative interactions;business ecosystem;resilience;side effect;ecosystem simulation;ecosystems;ecosystems companies electronic mail resilience biological system modeling collaboration;enterprise interoperability;ecosystem simulation diversity analysis business ecosystem;interoperability;open systems;interoperability tool	In this paper we present how by analysing the diversity of a system we can estimate the impact of an enterprise interoperability tool to a business ecosystem. We report experimental results on using a multi-agent system to simulate a business ecosystem in which we introduce the local effect of Commius, a low-cost interoperability tool for SMEs, into business collaborative interactions. The importance of the results derives from the egocentric attention commonly paid to the collaboration boost brought to the company using this sort of tools without considering the side effects it could bring to the network, where more than one company can also use it. When we apply our results to a cluster of companies, experiments indicate that an interoperability indeed adds dynamism to the network, yet it does not improve the network's capability of resilience.	business ecosystem;enterprise interoperability;experiment;interaction;multi-agent system;simulation	César A. Marín;Gabriel Alejandro Lopardo;Nikolay Mehandjiev	2011	2011 IEEE 20th International Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises	10.1109/WETICE.2011.14	interoperability;ecosystem;computer science;knowledge management;multi-agent system;open system;business ecosystem;cross-domain interoperability;side effect;collaboration	Visualization	-76.14715015853156	16.015680568383395	39167
2c68e7a9c05017503989bc02e23902078bdf240c	does the performance of tdd hold across software companies and premises? a group of industrial experiments on tdd		Test-Driven Development (TDD) has been claimed to increase external software quality. However, the extent to which TDD increases external quality has been seldom studied in industrial experiments. We conduct four industrial experiments in two different companies to evaluate the performance of TDD on external quality. We study whether the performance of TDD holds across premises within the same company and across companies. We identify participant-level characteristics impacting results. Iterative-Test Last (ITL), the reverse approach of TDD, outperforms TDD in three out of four premises. ITL outperforms TDD in both companies. The larger the experience with unit testing and testing tools, the larger the difference in performance between ITL and TDD (in favour of ITL). Technological environment (i.e., programming language and testing tool) seems not to impact results. Evaluating participant-level characteristics impacting results in industrial experiments may ease the understanding of the performance of TDD in realistic settings.	experiment;industrial robot;interval temporal logic;iterative method;programming language;software industry;software quality;test automation;test-driven development;unit testing	Adrian Santos;Janne Järvinen;Jari Partanen;Markku Oivo;Natalia Juristo Juzgado	2018		10.1007/978-3-030-03673-7_17	software quality;software engineering;computer science;systems engineering;unit testing;software;test-driven development	SE	-69.81393242761509	23.727489255563636	39285
8d029754cdf388870856212a1a755875721dde31	connecting technical communicators with technical developers	program understanding;technical communication;development process;software engineering;extreme programming;pair programming;development methodology;documentation	Most program documentation is written by developers for developers. This often results in documentation that is less than satisfactory in terms of quality, relevance, and longevity of information to developers who will be responsible for subsequent enhancements to the product. This paper outlines an approach to producing effective program documentation by connecting technical communicators and technical developers in a pair-wise manner. This approach is modeled on the development methodology advocated by eXtreme Programming (XP). Programmers who adopt XP work in pairs, and thus are in constant communication with one another. By pairing a technical communicator with a technical developer, many of the same benefits can be gained. Rather than eschewing program documentation, the paired communicator and developer instead produce high-quality documentation during the development process, with little extra effort, resulting in a more maintainable software application. The paper also describes some recent experiences in pairing a senior person and a junior person in communicator and developer roles to enhance program documentation of a long-lived and popular research application.	documentation generator;experience;extreme programming;programmer;relevance	Kenny Wong;Scott R. Tilley	2002		10.1145/584955.584992	common source data base;extreme programming;pair programming;human–computer interaction;technical communication;documentation;computer science;software engineering;technical documentation;user analysis;management;world wide web	SE	-72.03567566635482	27.76435243705625	39399
396c2c8de7cd862864ba423ba95e5482703e5dab	establishing a continual service improvement model: a case study		The Continual Service Improvement (CSI) section of the IT Infrastructure Library (ITIL) version 3 provides IT companies with best practices for the improvement of services and service management processes. Unfortunately, many IT companies consider ITIL-based practices (including CSI) too abstract for their purposes. The research problem in this study is: which methods and practices are related to Continual Service Improvement in IT service management? The main contribution of this paper is an improved version of the CSI model that provides a more detailed and practical view of CSI activities: measurement, reporting and processing of the service development ideas. Our model is compatible with ISO/IEC 20000 standard requirements and ITIL v3 practices. The model emphasizes the importance of change management process in the management of development ideas. The CSI model was created in the cooperation with a Finnish IT service provider company and validated with three different service provider companies.		Sanna Heikkinen;Marko Jäntti	2012		10.1007/978-3-642-31199-4_6	process management;it service management;service provider;best practice;information technology infrastructure library;service management;change management;business	NLP	-70.4873473601701	13.653371956176352	39413
18b6a2ca95f6a23818912112c395d0976d1ccf4a	dream - a software design aid system			ray dream studio;software design	William E. Riddle;John H. Sayler;Alan R. Segal;Allan M. Stavely;Jack C. Wileden	1978			software peer review;software engineering;software construction;backporting;software design description;resource-oriented architecture;software system;package development process;computer science;social software engineering	SE	-63.62013328379248	25.52580710565913	39593
f633bb038b10381cc9452bad44b5fb22694551b0	operational it failures, it value destruction, and board-level it governance changes		This paper presents an empirical study of changes that firms implement in their board-level IT governance (ITG) upon experiencing operational IT failures. Consistent with the separation of oversight from management decisions, boardlevel ITG is responsible for monitoring managerial IT decisions and policies for controlling IT resources. We expect that operational IT failures indicating inadequacies in board monitoring of controls over IT resources would result in a negative stock market reaction and, in turn, induce firms to improve their board-level ITG. Our expectation is confirmed based on a sample of 110 operational IT failures from U.S. public financial firms. Specifically, our results demonstrate that subsequent to experiencing operational IT failures firms make improvements to the IT competency level of their boards, and the improvements are proportional to the degree of negative market reaction. However, those improvements are only on the executive side of the board, namely: an increase in the IT experience of internal (executive) directors and an increased turnover rate of CIOs serving on the board. Furthermore, the likelihood of CIO turnover is lower in IT-intensive firms where such turnover could be more disruptive. Our results contribute to understanding the critical connection between operational IT failures and board-level ITG.	chief information officer;failure	Michel Benaroch;Anna Chernobai	2017	MIS Quarterly		computer science;accounting;financial services;corporate governance	OS	-83.16656086519131	6.582647774600904	39616
3514813ff9eb4eaf45eb015e00ba6949736ca495	business experience with computer integrated manufacturing. a survey of current strategy and practice	competitive missions;information technologies;pulp manufacturing;current practice;plant wide integration;bottom up;information technology;manufacturing automation;adoption policy;cim architecture;commerce;development process;value;strategic planning;computer vision;manufacturing processes;competitive missions computer integrated manufacturing current strategy current practice business experience us manufacturing firms adoption policy manufacturing process characteristics cim development process cim architecture value benefits temporal pattern information technologies plant wide integration;computer aided manufacturing;temporal pattern;production facilities;computer integrated manufacturing computer aided manufacturing manufacturing processes information technology production facilities hardware process planning computer vision manufacturing automation pulp manufacturing;manufacturing process characteristics;benefits;current strategy;process planning;commerce computer integrated manufacturing strategic planning;business experience;computer integrated manufacturing;us manufacturing firms;cim development process;field study;business process;hardware;product development	This paper describes the results of a recent field study of CIM adootion strateeies in US manufacturing tkms. The purpose of the study Gas to identify the extent to which CIM technologies are in use in US firms, the impact of a facility’s process characteristics on the CIM development process, and the adoption policy being followed implicitly or explicitly. The survey focused on the following aspects:(u) manufacturing process characteristics, (b) the CIM development process, (c) the CIM architecture, and (d) percetved value and benefits. Our results indicate that CIM implementations follow a definite temporal pattern with respect to the adoption of certain information technologies. In addition. the initiative for CIM programs is usually generated from the bottom-up. This gradual bottomup approuch appears to restrain, rather than enable, plantwide integration for critical business processes such as order fidjillment or product development. While most CIM users find that their CIM projects successfully meet their initial operational goals, the technology seems to be poorly integrated still. More crucially, it appears that CIM is not being adopted as a strategic information system for competitive missions.	bottom-up parsing;business process;computer-integrated manufacturing;emoticon;field research;new product development;strategic information system;top-down and bottom-up design	John Johansen;Uday S. Karmarkar;Dhananjay Nanda;Abraham Seidmann	1995		10.1109/HICSS.1995.375650	computer science;operations management;top-down and bottom-up design;computer-integrated manufacturing;business process;management;information technology;software development process;new product development	HCI	-78.94405034676106	5.127765397363488	39657
8632b59d6b8be5394e72d77685e3cbba98531e53	towards policy-based information management for the joint battlespace infosphere	information brokering;control systems;us defense department;policy based information management;technological innovation;web protocol;loosely coupled system;mission constraints;information industry information services information dissemination military communication access protocols;web and internet services;search engines;information technology;department of defense;mission objective;information services;information management control systems web services simple object access protocol web and internet services service oriented architecture technological innovation information technology access protocols search engines;military communication;information industry;policy based management;information management;information dissemination;web services;access protocols;jbi platform;jbi platform policy based information management joint battlespace infosphere us defense department internet like foundation information service military operation web protocol loosely coupled system mission constraints mission objective information brokering;joint battlespace infosphere;information service;internet like foundation;service oriented architecture;simple object access protocol;military operation	The vision of a Joint Battlespace Infosphere has evolved within the US department of defense as an Internet-like foundation to provide tailored information services and flow among producers and consumers of information needed to conduct military operations. The architecture envisioned for realizing this vision is based on a loosely coupled information environment using commercial standard Web protocols. One of the main obstacles to widespread acceptance of loosely coupled systems is the difficulty of establishing and maintaining control consistent with overall mission constraints and objectives. Policy-based information dissemination management is a mechanism for influencing the flow of information between publishers and subscribers in order to support diverse mission objectives. We describe research being done on the selection and integration of policy-based management services, effectively bridging the gap between those services and the information brokering capabilities of a JBI platform.	information management;joint battlespace infosphere	Robert Cherinka;C. Wild;D. Allen;C. Smith;Yongzhi Zhang;Rafal Panek;S. Semy	2003		10.1109/POLICY.2003.1206961	knowledge management;business;world wide web;computer security	AI	-73.31648989292017	14.905422585817638	39831
f43e383562e931f66d4e966bfe27ddca1fa68065	ibm industry practice: challenges in offshore software development from a global delivery center	application development;software development	Offshore software development has greatly influenced competitiveness among IT companies in the last decade. Despite the fact  that there are matured and developed offshoring methodologies, there is an ongoing tendency to look for new ways of improving  them. Major IT corporations successfully rely on their offshore delivery centers for bridging the gap between communication  and infrastructure boundaries. However, projects tend to fail, so problems have to be considered that arise between on- and  offshore parts within the same corporation. Based on seven case studies from the industry, this paper describes experiences  and challenges faced during the execution of offshore application development between IBM Switzerland and IBM India. Additionally,  approaches on how they can be solved are proposed.  	software development	Ilario Musio	2009		10.1007/978-3-642-02987-5_3	software project management;computer science;systems engineering;engineering;social software engineering;software development;software engineering;devops;software as a service;rapid application development;lean software development;software deployment;manufacturing engineering	SE	-67.22971402727151	21.769245616507575	39888
9070a662c4ca62c497c320b347664ec681ddc60b	making each workhour count: improving the prediction of construction durations and resource allocations		Construction duration is an important performance aspect of building projects because it determines the time-to-market for a building, i.e., it stands between a client’s final decision to construct a building and obtaining the benefits from the designed building. This paper shows how intelligent computing (including semantic modeling, simulation, genetic algorithms, and machine learning) improves the ability of construction professionals to predict the construction schedule duration and direct cost of building projects at the beginning of construction and during construction. Such predictions are important because they inform the schedule commitments made and the allocation of resources that practitioners believe will let them meet the commitments. Hence, the prediction methods must capture the most important phenomena that are likely to impact the duration of activities and of construction. The two applications discussed – Tri-Constraint Method (TCM) and Activity-Flow Model (AFM) – incorporate key phenomena observed in practice, such as handling of workspace constraints and flows required for the execution of activities, that are not part of the currently prevalent concepts and tools. TCM and AFM significantly improve the ability of construction professionals to make more reliable predictions of construction duration.		Martin Fischer;Nelly P. Garcia-Lopez;René Morkos	2018		10.1007/978-3-319-91635-4_15	genetic algorithm;workspace;management science;computer science;atomic force microscopy;resource allocation	NLP	-73.05132301110946	29.389279209011793	39997
6cce7b91db63725352dfed1a400fe4e07ec6b66d	data driven reference architecture for smart city ecosystems		With the convergence of information and telecommunication technologies, the vision of the ‘Smart City’ is fast becoming a reality. City governments in a growing number of countries are capitalizing on these advances to enhance the lives of their citizens and to increase efficiency and sustainability. In this paper, we elaborate on smartCityRA, a reference architecture for Smart City projects, which serves as the design language for creating smart cities blueprints. Such a blueprint caters for diverse stakeholders, devices, platforms, and technologies. We report on our experience in carrying out a proof-of-concept use case with a major telecommunication provider in the UAE. In doing so, we refined our multiple-view model of the initial smartCityRA reference architecture. We show that Data in smart city applications drive the entire development lifecycle and should be considered early in the development cycle. In addition, Data affects all the other views in the smartCityRA and hence the Data View needs to be at the heart of the entire smartCityRA. Realizing the Data view using a component like a Data Hub helped in creating a central integration location for disparate data from different sources, thus reliving developers from dealing with several entities individually. Finally, we show that any smart city reference architecture, like smartCityRA, should be at the right level of abstraction to enable the flexibility of adoption and adaptation by different stakeholders and components.	blueprint;data hub;ecosystem;entity;reference architecture;smart tv;smart city;view (sql);view model	Mohammad Abu-Matar;John Davies	2017	2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)	10.1109/UIC-ATC.2017.8397556	smart city;disparate system;systems engineering;computer science;reference architecture;sustainability;data hub;design language;blueprint;convergence (routing)	Visualization	-69.33613717737036	12.726071744850707	40110
01d54186ec4549d79088135bb32b6fed86f54e42	measuring modularity: engineering and management effects of different approaches	complexity theory;atmospheric measurements;standards;particle measurements;computer architecture;settore secs p 08 economia e gestione delle imprese;product design;supply chain management buyer supplier integration mirroring hypothesis modularity product architecture;standards atmospheric measurements particle measurements product design complexity theory computer architecture	While a variety of “product” modularity measures have been proposed and empirically used, little comparative research has been conducted on the characteristics and efficacy of such measures. This study explores how the use of diverse modularity measures affects the analysis of the “mirroring” hypothesis. Particularly, this study analyzes the relationship between “product” modularity measures and the degree of: 1) buyer–supplier integration in new product development; 2) supply chain configuration. The empirical results suggest which of the analyzed measures of modularity are preferable, question the overall utility of such measures to really understand the organizational implications of complex technological systems, and point to alternative measurement approaches.	disk mirroring;new product development	Anna Cabigiosu;Arnaldo Camuffo	2017	IEEE Transactions on Engineering Management	10.1109/TEM.2016.2614881	simulation;systems engineering;engineering;operations management;product design	SE	-81.03113920686383	5.073253230437738	40149
ae7dfb28199b81d45fc9479977480b6cf1afd314	exploiting devops practices for dependable and secure continuous delivery pipelines		Continuous delivery (CD) pipelines recently gained wide adoption. They provide means for short and high-frequent development cycles in DevOps by automating many steps after a commit has been issued and bringing it into production. CD pipelines have become essential for development and delivery. Hence, they are crucial and business-critical assets that need to be protected from harm in terms of dependability and security. DevOps practices like canary releasing and A/B testing aim to improve the quality of the software that is built by CD pipelines while keeping a high pace of development. Although CD is a part of DevOps, the DevOps practices have primarily been applied to the artifacts that are processed but not on the pipelines themselves. We outline our vision of using these DevOps practices to improve the dependability and security of CD pipelines. The goal is to detect, diagnose, and resolve dependability and security issues in the CD pipeline behavior. In this paper, we outline our envisioned roadmap and preliminary results from an ongoing industrial case study.	a/b testing;cd-rom;continuous delivery;dependability;devops;pipeline (computing);requirement;sensor;software development process	Thomas F. Düllmann;Christina Paule;André van Hoorn	2018	2018 IEEE/ACM 4th International Workshop on Rapid Continuous Software Engineering (RCoSE)	10.1145/3194760.3194763	systems engineering;computer science;devops;software quality;software;continuous delivery;software evolution;commit;dependability;software performance testing	SE	-68.1142090231084	24.186895756259737	40170
b8e4fdcc524895ab434bf8eba8ac3b12d9d4d19c	changing dynamics of software engineering and mysterious human passion	passion;personalities;software engineering;success	World of Software development is facing stormy demands and volatile markets along with change in existing and adoption of new technologies, including open source. This requires extensive changes in the way software professionals work and to survive onslaught of these changing dynamics, Human Passion becomes a determinant. In this paper I have included my observations which were gained over the period of my extensive experience in IT industry. By questioning, interviewing and noticing people behavior what I have gathered is that passion plays an indispensable and critical part in driving masses to adjust and adapt to changing environments; and then successfully achieve the objectives of creation of software systems. Passionate developers are proven to be better placed as compared to their non-passionate counter parts and this understanding is not only restricted to developers but also includes project managers, product owners, architects, quality managers, testers, support personnel and all those who are associated with the software.	onslaught;open-source software;software development;software engineering;software system	Sachin Kohli	2014		10.1145/2593702.2593723	simulation;computer science;engineering;software engineering;personality psychology;management	SE	-69.21633904131026	24.546109519274907	40186
836f61e796fee8acab0524d443a1171e50c00002	introducing case management: opening workflow management's black box	experience feedback;gestion entreprise;groupware;entreprise;analyse fonctionnelle;processus metier;retour experience;empresa;retorno experiencia;firm management;functional analysis;firm;workflow;proceso oficio;workflow management system;coordinacion;administracion empresa;case management;collecticiel;business process;coordination;analisis funcional	Workflow management systems are very adequate for supporting the flow of work through enterprises, but do not deliver coordination support to end-users within the work items they perform. In this paper, the concept of case management is introduced, which specifically targets this type of support. Its associated technology is intended to be used as a harmonious extension of workflow technology, instead of a competing system. A discussion in some depth is presented of the concept, methods, and technology of case management, as well as experiences with its application in industry.	black box	Kees Kaan;Hajo A. Reijers;Peter van der Molen	2006		10.1007/11841760_25	functional analysis;workflow;economics;computer science;technology management;document management system;business process;management;workflow management system;workflow technology	DB	-69.17722002188358	6.838535319792005	40189
1457ed424aa41d9a344749125e21202793f313a9	empirical study of analysts' practices in packaged software implementation at small software enterprises		This study investigates the practices of requirements engineering (RE) for packaged software implementation, as enacted by small packaged software vendors (SPSVs). The research findings lead to introduced new methods of documentation, was not as concerned as general RE practice with looking for domain constraints or with collecting requirements and viewpoints from multiple sources, was more likely to involve live software demonstrations and screenshots to validate user needs, and was more likely to involve the compilation of a user manual. In PSI, prioritising requirements is not a basic practice; instead, analysts collect requirements in a circular process, with managers then directing analysts regarding which requirements to direct most attention toward.	compiler;documentation;requirement;requirements engineering;screenshot	Issam Jebreen;Ahmad Alqerem	2017	Int. Arab J. Inf. Technol.		systems engineering;software deployment;software peer review;machine learning;empirical process (process control model);artificial intelligence;computer science;personal software process;extreme programming practices;software walkthrough;social software engineering;software project management	SE	-68.58576673549985	23.321182700725522	40276
c56fc6961c7720b30c5be6f8c0e190b57546fb88	factory-on-demand and smart supply chains: the next challenge	enterprise integration;supply chain;dispersed network manufacturing	"""Andy Groves, the Chairman of Intel, recently suggested that in five years time all companies will be internet companies, or they won't be companies at all. For many companies, the ability to """"mass customise"""" is no longer a differentiator but an imperative. The notion of Factory-on-Demand (FOD) promotes the ability to mass customise in a """"lean"""" and """"agile"""" production environment through a collection of production centres. FOD is an electronic network production system which links and shares manufacturing resources in geographically dispersed small firms. In such an environment, as the number of product options increases and the number of supply chain options expands, matching product supply with consumer demand becomes a daunting task. This paper develops a positive understanding of some of the issues and challenges of building a responsive supply chain system in the context of our on-going research on designing the FOD network."""		Hamid Noori;W. B. Lee	2002	IJMTM	10.1504/IJMTM.2002.001456	economics;engineering;marketing;operations management;supply chain;enterprise integration;management;commerce;mechanical engineering	Robotics	-73.55543150467234	4.3483071633039225	40295
7f09325a357550c4023444f161e0364944dc3173	value creation in digital application marketplaces: a developers' perspective		Digital application marketplaces are increasingly relevant for digital platform owners seeking to reap the benefits of distributing, brokering, and operating applications by third-party developers. Owners of such marketplaces have two vital goals: address the needs of heterogeneous end-users and attract third-party developers. A key element in simultaneously accommodating these goals is value creation. However, while there is emerging literature on digital application marketplaces, little empirical evidence exits about the value creation process for and by application developers. Drawing on a research study of third-party developers, we synthesize the value creation perspective and digital platforms literature to develop an understanding of value creation in digital application marketplaces from the perspective of third-party developers. Our study identifies and explores six different value sources and their associated value creation and realization. In doing this, our research extends and complements existing digital platform literature and contributes new knowledge about new forms of value creation.	complement (complexity);information systems;software developer	Ahmad Ghazawneh;Osama Mansour	2015				HCI	-76.67653687462072	6.167740351060017	40323
8a5710c38efbc4ba055bba9792a0fee87d1528c4	do agile methods work for large software projects?			agile software development	Magne Jørgensen	2018		10.1007/978-3-319-91602-6_12		SE	-64.99308960926587	23.189738115344536	40324
0ca3d57aa03f636b2e95aed7fe5f272709aa7e31	an expert survey on kinds, influence factors and documentation of design decisions in practice	design decision classification;design decisions;design decision influence factors;design decision documentation;software architecture knowledge management	Support for capturing architectural knowledge has been identified as an important research challenge. As the basis for an approach to recovering design decisions and capturing their rationale, we performed an expert survey in practice to gain insights into the different kinds, influence factors, and sources for design decisions and also into how they are currently captured in practice. The survey was conducted with 25 software architects, software team leads, and senior developers from 22 different companies in 10 different countries with more than 13 years of experience in software development on average. The survey confirms earlier work by other authors on design decision classification and influence factors, and also identifies additional kinds of decisions and influence factors not mentioned in previous work. In addition, we gained insight into the practice of capturing, the relative importance of different decisions and influence factors, and into potential sources for recovering decisions. © 2014 Elsevier B.V. All rights reserved.	design rationale;documentation;software architect;software development	Rainer Weinreich;Iris Groher;Cornelia Miesbauer	2015	Future Generation Comp. Syst.	10.1016/j.future.2014.12.002	knowledge management;management science;business decision mapping	SE	-70.90257228800914	21.599454160436743	40451
3239e8d90e9a39b55b6e935f2e800da6903e3bf6	smart auditing -- innovating compliance checking in customs control	legislation;auditing;risk analysis;auditing smart computing customs controls;biological system modeling;data mining;companies;monitoring process control economics biological system modeling data mining companies;monitoring;process control;accounts data processing;risk analysis accounts data processing auditing intelligent sensors knowledge based systems legislation;economics;knowledge based systems;intelligent sensors;smart computing;customs controls;smart auditing audit services accounting information system intelligent data analysis smart sensors smart computing auditing services governmental controls compliance requirements risk regulations customs control compliance checking	While risk regulations and compliance requirements grow, the capacity of governmental controls only shrinks in many countries. Automation provides a partial solution to this problem, but more is expected from new modes of supervision. The reliability of the provided data becomes essential then. Reliability can be supported by auditing services. At the same time, the advent of Smart Computing creates new requirements on auditing as well as new opportunities. Technology (smart sensors and intelligent data analysis) can support both accounting information system and audit services. To address these developments, this paper discusses the related concepts of Smart Auditing. An initial evaluation has been performed in the domain of customs control as well.	accounting information system;adaptive system;automation;reliability engineering;requirement;risk management;sensor	Faiza Allah Bukhsh;Hans Weigand	2013	2013 IEEE 15th Conference on Business Informatics	10.1109/CBI.2013.27	data mining;business;computer security;commerce	Robotics	-69.85976727066186	12.26826896340231	40452
84049c9ea0de3d31c2c620bed16cd6e134929285	towards rules and laws for software factories and evolvability: a case-driven approach	history;signal design;component based development;software systems;maintenance cost;manufacturing automation;software development process;system theory;computer industry;null;large scale;inductive reasoning;production facilities programming software systems computer industry signal design productivity costs large scale systems manufacturing automation history;software development;production facilities;component model;productivity;programming;large scale systems	The ever-growing size and complexity of software systems creates a strong need for a major improvement in productivity in the software development process. However, automated production does not yet have an equivalent in the software development industry. Maintenance costs in particular, due to changes in requirements, remain extremely high in most software systems. Structured component models, component-based development processes, and initiatives for software factories are trying to tackle these issues. This paper states that there is a need to establish firm rules and laws that govern the process of software changes and evolvability. In order to contribute to the process of discovering these rules and laws, a case-driven approach is used. Based on a highly structured and large-scale codebase, an attempt is made to identify and quantify impacts and propagations of basic changes. Though the study is limited to a single codebase, the purpose is to contribute to an inductive reasoning process that may one day establish laws for software evolvability, similar to the mathematical foundation of systems theory.	component-based software engineering;elementary;inductive reasoning;requirement;software development process;software factory;software system;systems theory	Herwig Mannaert;Jan Verelst;Kris Ven	2006	2006 International Conference on Software Engineering Advances (ICSEA'06)	10.1109/ICSEA.2006.73	reliability engineering;programming;productivity;software engineering process group;computer science;systems engineering;engineering;software development;inductive reasoning;software engineering;empirical process;management;goal-driven software development process;software development process;software metric	SE	-64.98307219763001	26.252068053520354	40531
2ce8e161d2f2050d58b935f6267a961396ee5e38	skills and wills: the keys to identify the right team in collaborative innovation platforms	collaborative innovation;matchmaking;team building;competencies management;crowdsourcing	The access to external expertise and collaboration initiatives became an undeniable strategy for highly innovative sectors. Innovation intermediaries have gained a prominent role in this scenario, and crowdsourcing platforms reached notable results. However, identifying right competencies with demanded innovation is a critical issue. Research is needed in the domain of matchmaking mechanisms for identifying the necessary skills to fulfil different kinds of problems in order to build effective collaborative teams. This paper investigates different approaches adopted in crowdsourcing and main characteristics of teams operating in these contexts. Thus, a set of critical issues is highlighted and a structured team-building methodology for finding suitable solvers in crowdsourcing challenges is presented. Such method is grounded on natural language processing (NLP) and semantic ontologies fulfilling some of the identified criticalities. The description is supported by a case study conducted within a self-devel...		Gabriele Montelisciani;Donata Gabelloni;Giacomo Tazzini;Gualtiero Fantoni	2014	Techn. Analysis & Strat. Manag.	10.1080/09537325.2014.923095	simulation;knowledge management;marketing;management science;management;crowdsourcing	HCI	-68.11821553487871	12.318873272432791	40559
04b2fc99f8a162c6e78542104634c89afac77908	a process-centered it roi analysis with a case study	analysis systems;banking industry;it investment;return of investment;roi;business process	In order to achieve a meaningful IT ROI (Returns of Investment) analysis while taking into account the unique business processes of each industry, we should conduct research into a system to evaluate the effect of investment. In this paper, through careful consideration of the characteristics of the banking industry, a study on IT ROI is conducted from the perspective of business process. The process-centered IT ROI model for analysis discussed here consists of seven phases. The suggested model is explained and verified with a case study of a finance company that we shall call “A”.	business process;information system;list comprehension;performance evaluation;prospective search;region of interest;strategic management;uninterruptible power supply	Choon Seong Leem;Chui Young Yoon;Seung-Kyu Park	2004	Information Systems Frontiers	10.1023/B:ISFI.0000046378.42438.5f	return on investment;marketing;operations management;management;commerce	SE	-78.27665966008716	8.036705368616298	40667
2eb042463c5580a9858fdb064384b11c4841d81a	the knowledge diffusion model associated with innovative knowledge	innovative knowledge;six sigma;knowledge value;knowledge management;enterprise benefits;estimating equation;production services;knowledge diffusion;knowledge diffusion model;adoption of innovation;knowledge base	Introducing innovative knowledge into the existing knowledge base will enhance enterprise benefits successfully. Recent studies presented numerous strategies to manage innovative knowledge; however, these strategies cannot be explored quantitatively and dynamically. This study presents a knowledge diffusion model associated with innovative knowledge to investigate the influence of the adoption of innovative knowledge on knowledge asset advancement and on enterprise benefits enhancement. Various parameters are considered simultaneously in the proposed model to illustrate different sub-systems of knowledge management. The feasibility of the proposed model has been demonstrated by a Taiwanese company that adopts six-sigma knowledge to improve the product/service quality. The results indicate that the model can appropriately describe the relationships among existing knowledge value, innovative knowledge value, and enterprise benefits. Since the adjusted R^2 values of all estimated equations are acceptable, managers can evaluate the performance of knowledge management and further establish more effective knowledge strategies via parameters analysis.		Chih Ming Tsai	2009	Expert Syst. Appl.	10.1016/j.eswa.2009.03.044	knowledge base;organizational learning;computer science;knowledge management;mathematical knowledge management;management science;knowledge value chain;six sigma;estimating equations	Vision	-71.32421729099671	12.65917747748688	40836
f435166fe1c013ee95c5b5f093f83cf3237b7319	an empirical analysis of human performance and error in process model development	model quality;process modeling;human reliability analysis;human error	Process models capture important corporate know-how for an effective Business Process Management. Inconsistencies between process models and corporate reality are a common phenomenon in corporate practice. Human performance in process model development is a major source for these inconsistencies. In this contribution, a human performance analysis of process model development paying special attention to the concept of human error was conducted. It was found that the frequencies of the omissions and erroneous executions of notation elements are significantly higher for novices than for experienced modelers. Moreover, experienced modelers inherently adhere to a verb-object activity labeling style. The overall empirical results indicate that experienced modelers achieve higher process model quality with less expenditure of time than novices.		Alexander Nielen;Denise Költer;Susanne Mütze-Niewöhner;Jürgen Karla;Christopher M. Schlick	2011		10.1007/978-3-642-24606-7_42	simulation;human error;computer science;software engineering;process modeling;data mining;management	Robotics	-70.89402635900541	23.515821700368264	40886
2e82aa4c9342511f9a04b47d10062309efa860e8	grand challenges, benchmarks, and tracelab: developing infrastructure for the software traceability research community	research agenda;metrics;center of excellence;qualitative study;tracelab;benchmarks;cost effectiveness;traceability	The challenges of implementing successful and cost-effective traceability have created a compelling research agenda that has addressed a broad range of traceability related issues, ranging from qualitative studies of traceability users in industry to very technical and quantitative studies. Unfortunately, advances are hampered by the significant time and effort needed to establish a traceability research environment and to perform comparative evaluations of new results against existing baselines. In this panel we discuss ongoing efforts by members of the Center of Excellence for Software Traceability (CoEST) to define the Grand Challenges of Traceability, develop benchmarks, and to construct TraceLab, an extensible and scalable visual environment for designing and executing a broad range of traceability experiments.	benchmark (computing);experiment;grand challenges;scalability;traceability	Jane Cleland-Huang;Adam Czauderna;Alex Dekhtyar;Olly Gotel;Jane Huffman Hayes;Ed Keenan;Greg Leach;Jonathan I. Maletic;Denys Poshyvanyk;Yonghee Shin;Andrea Zisman;Giuliano Antoniol;Brian Berenbach;Alexander Egyed;Patrick Mäder	2011		10.1145/1987856.1987861	systems engineering;engineering;software engineering;reverse semantic traceability;management science;requirements traceability	SE	-67.86447545593762	19.44902820155932	40920
5e9bb2ca2e9f0492c689d2010a942dc6405f85b1	implementing itil-based cmdb in the organizations to minimize or remove service quality gaps	libraries;gap;organisational aspects configuration management database management systems dp management;cmdb;itil;standards organizations;database management systems;information technology;government;configuration management data base;software engineering;service quality gap minimization;it infrastructure library service quality gap minimization configuration management data base;customer satisfaction;computer aided software engineering;business;dp management;quality of service;quality of service libraries asia software engineering computer aided software engineering information technology standards organizations government business customer satisfaction;configuration management;service quality;asia;service quality itil cmdb gap;organisational aspects;it infrastructure library	"""""""Gap"""" is a fundamental problem in every organization and there are some models and methods to """"identify"""" and remove or minimize those gaps in the organizations . This paper tries to use an ITIL based solution to remove or abate those """"key discrepancies"""" in the organizations based on using Model of Service Quality Gaps(S. Q. G) as a most known model to identify gaps in the organizations in perspective of service quality. In this study the following questions were answered: Which gap or gaps are in the organizations based on S.Q.G model? ; Which method is better to make such ITIL- based repository? ; And how CMDB can be used within S. Q. G model in order to remove or abate known gaps?"""	algorithm;artificial intelligence;database;itil;quality of service;structure mining	Mohammad Sharifi;Masarat Ayat;Shamsul Sahibuddin	2008	2008 Second Asia International Conference on Modelling &#x00026; Simulation (AMS)	10.1109/AMS.2008.144	systems engineering;knowledge management;operations management;business	DB	-73.13298659970043	15.37957524576098	40924
2d177fc156d0fedca15ef8a8f066d5f54a515e60	towards a process view on critical success factors in big data analytics projects		The research tries to identify factors that are critical for a Big Data project’s success. In total 27 success factors could be identified throughout the analysis of these published case studies. Subsequently, to the identification the success factors were categorized according to their importance for the project’s success. During the categorization process 6 out of the 27 success factors were declared mission critical. Besides this identification of success factors, this thesis provides a process model, as a suggested way to approach Big Data projects. The process model is divided into separate phases. In addition to a description of the tasks to fulfil, the identified success factors are assigned to the individual phases of the analysis process. Finally, this thesis provides a process model for Big Data projects and also assigns success factors to individual process stages, which are categorized according to their importance for the success of the entire project.	americas conference on information systems;big data;categorization;mission critical;process modeling;rico	Jing Gao;Andy Koronios;Sven Selle	2015			systems engineering;engineering;data mining;management science;critical success factor	ML	-67.42016867692497	12.896369953428193	41064
a637ea2217c154c51e43e955cafcc9dce0a059b7	privacy technologies and policy		The assessment of the maturity of Privacy-Enhancing Technologies (PETs) is a complex and challenging task, which can only be performed by experts in the field. However, at the same time, the need for precise technology readiness and quality definitions for PETs emerges rapidly. In order to overcome this gap, standardised means to assess, discuss, and compare PET maturity levels are necessary. In this paper, we propose an approach for assessing the maturity of PETs. We define both the scales and the methodology for measuring maturity of PETs, in a way that is independent from the domain of application. Based on an in-depth analysis of the criteria to be met by such a PET maturity level scheme, we propose a combined quality-andreadiness level scale to be used for this purpose.	capability maturity model;emergence;polyethylene terephthalate;privacy-enhancing technologies	Bettina Berendt;Thomas Engel;Demosthenes Ikonomou;Daniel Le Métayer;Stefan Schiffner	2015		10.1007/978-3-319-31456-3	privacy by design;business;privacy software;information privacy;internet privacy;computer security	SE	-68.26757653162431	11.56503079440554	41098
ce434364e5112388b74c6696e7a0b81506bbfbe5	designing a flexible approach for higher professional education by means of simulation modelling	modelizacion;forecasting;guide programme etude;reliability;project management;systeme evenement discret;information systems;development;enseignement superieur;gestion production;maintenance;flexibilidad;educational program;soft or;information technology;simulation;packing;higher education;operations research;location;investment;journal;production management;journal of the operational research society;inventory;sistema acontecimiento discreto;purchasing;modelisation;organizational studies;ensenanza superior;discrete event system;history of or;innovation;logistics;estudio caso;marketing;scheduling;gestion produccion;programme enseignement;curriculum guides;etude cas;production;communications technology;flexibilite;computer science;operational research;educacion;innovacion;modeling;programa ensenanza;simulation modelling;applications of operational research;or society;flexibility;jors;management science;infrastructure;professional education	Operational flexibility is a prerequisite for innovations in higher education. Educational programmes have an operational structure which restricts their flexibility. Design principles from the fields of operations management and instructional design were applied for the design of a flexible approach in educational programmes. In a case study, the curriculum and operational data from an institute for higher professional education in the Netherlands were used to develop a discrete-event simulation model, in order to demonstrate effects of operational changes on the flexibility of their educational programmes. Programme coordinators of the institute validated the model and considered it as a feasible solution for organizing their educational programmes, and as a contribution to educational innovation. Journal of the Operational Research Society (2010) 61, 202–210. doi:10.1057/jors.2008.133 Published online 7 January 2009	organizing (structure);simulation	A. Schellekens;Fred Paas;Alexander Verbraeck;Jeroen J. G. van Merriënboer	2010	JORS	10.1057/jors.2008.133	project management;innovation;logistics;simulation;inventory;economics;forecasting;investment;marketing;operations management;reliability;location;higher education;management;operations research;information technology;scheduling	HCI	-68.1497673626259	7.504686779191592	41182
4cbca82acd8bfd62d8db11c342d92e3af2936b8a	analysing virtual organisation risk sources: an analytical network process approach	networks;virtual organisations;analytical network process;weights panel;small and medium sized enterprises;web based organisations;risky decisions;weighting;collaboration;multicriteria decision making;anp;ahp;dynamic environments;internet;risk;smes;virtual organisation;networking;vo;world wide web;competitive environments;multi criteria decision making methods;analytical hierarchy process;risk sources;mcdm;online organisations	SMEs have to collaborate with other enterprises in a virtual organisation (VO) forms to cope with an increasingly dynamic and competitive environment. Despite the increased interest in the area of collaboration information is still lacking about the risk sources of VO. This paper aims to reinforce the proposal for an integrated methodology to classify, manage and assess network level risk sources in VO and discusses the advantages of AHP/ANP over the other multi-criteria decision making (MCDM) methods before discussing the analytical hierarchy process (AHP) and the analytical network process (ANP) methods and the advantages of ANP over the AHP. This will be followed by illustrations of how ANP can be used to assess VO risk sources as part of the framework to support the final decision of VO collaboration. ANP will be used to set up a panel of weights of risk sources to define which risks are more serious. Overall, insights from the research and the process suggested in this research will aid SMEs in making a less risky decision.	analytical hierarchy;collaboration-oriented architecture;virtual organization	Mohammad Alawamleh;Keith Popplewell	2012	IJNVO	10.1504/IJNVO.2012.045209	analytic hierarchy process;economics;computer science;knowledge management;marketing;operations management;management;computer network	ML	-77.36294040187745	7.5358397088920155	41216
babf2160308124f099a45e8b6d13fb8af402c86b	facilitating meaningful collaboration in architectural design through the adoption of bim (building information modelling)	groupware;optimisation;building;bim collaboration design visual;resource allocation;construction industry;communicative benefits bim building information modelling better informed design daylighting construction industry contractors worthwhile collaboration visual aspects;civil engineering computing;buildings collaboration procurement computational modeling data models solid modeling contracts;resource allocation building civil engineering computing construction industry groupware optimisation	Better informed design then allows for a reduction in the use of global resources, to facilitate better siting, geometry, daylighting, function and system implementation in facilities. It allows better optimisation in components and installations placed in those buildings and, in best cases, it makes it possible to generate excess energy to fulfil local needs or even produce a profit. However, and due to the fragmentation, the construction industry is only responding slowly. Indeed, clients and contractors are beginning to have an increasing influence. Trust and worthwhile collaboration can be engendered through the visual aspects of modelling and the communicative benefits accruing.	best practice;bim;building information modeling;fork (software development);information model;mathematical optimization;offset binary	James Harty;Richard Laing	2013	Proceedings of the 2013 IEEE 17th International Conference on Computer Supported Cooperative Work in Design (CSCWD)	10.1109/CSCWD.2013.6581013	resource allocation;knowledge management;building;management;mechanical engineering	Visualization	-66.81398937330475	16.194007784920515	41220
2db101c247b7a14c28b25fde48b8c526b0ba98f0	exploring local cultural perspectives in user interface development in an indian offshoring context: a view from the uk	outsourcing;offshoring;computing;software development;culture	In this paper, we present the results of an exploratory case study on the impact of culture on software development in an offshoring context in India. Our research aims to understand the role of culture in outsourced software development. We interviewed human-computer interface professionals such as frontend developers, user interface designers and usability specialists working for a software development outsourcing vendor in India. The interviews were analysed for occurrence of common themes. Thereafter the cultural models of Hofstede and Hall were used to make sense of these emerging themes.. Our results indicate that cultural influence occurs and has an overarching influence in software development. Three proposals are made in response to the cultural issues highlighted.	dbpedia;documentation;focus group;human–computer interaction;level of measurement;outsourcing;software development;usability;user interface	Malte Reßin;Cecilia Oyugi;José L. Abdelnour-Nocera;David Lee;Dharam Panesar	2012		10.1007/978-3-642-34347-6_20	systems engineering;engineering;knowledge management;commerce	HCI	-70.24350108899199	5.91643096545771	41287
4a06339ddfe6c3b2736a944fad4b194d0bc2768e	does audit quality affect firms' investment efficiency?		AbstractThis study investigates the effect of audit quality on firm investment efficiency for 125 French-listed companies over 2008–2015. It uses parametric and non-parametric measures of firm investment efficiency, based on residuals extracted from the investment efficiency model and the data envelopment analysis (DEA) approach, respectively, to assess whether audit quality improves investment inefficiency. It analyses this relationship after distinguishing between firms that under-invest and those that over-invest. The results show that investment inefficiency decreases with audit quality. Specifically, auditor knowledge leads to less investment in firms prone to over-investment and more investment in firms prone to under-investment. This relationship appears to be independent of a firm’s financial reporting quality, which indicates that auditors provide value-added services that impact the investment decisions of firm managers, separately from the quality of accounting information.		Sabri Boubaker;Asma Houcine;Zied Ftiti;Hatem Masri	2018	JORS	10.1080/01605682.2018.1489357	operations management;accounting;quality audit;audit;data envelopment analysis;inefficiency;investment decisions;computer science;accounting information system	HCI	-83.6941639957225	7.816697539426027	41298
303cc5311feb39f70268206c07a9a6c10b63709a	benefits realized through usability analysis of a tele-nursing call management software system at healthlink bc	biomedical research;bioinformatics	In this paper we describe the analysis of a tele-nursing call management software system at HealthLink BC. Several methods of usability analysis were conducted in a process of continuous quality improvement. In the initial phase usability engineering methods were applied to assess simulated nurse interactions with the call system and decision support software. After modification of the software an evaluation of the resulting benefits of the usability engineering was carried out. Run and control charts were developed and the impact of the changes were assessed in the context of other ongoing changes occurring in the organization. It was found that call handle times were reduced after the software was modified based on the usability analyses, and the software was streamlined to require fewer steps to complete call management tasks. However, ongoing changes in business processes have underlined the need for continual usability analyses and system refinement even within mature systems.	business process;chart;decision support system;interaction;refinement (computing);software system;telephone number;television;usability engineering;benefit	Simon A. S. Hall;Amen S. Lalli;André Kushniruk;Elizabeth M. Borycki	2012	NI 2012 : 11th International Congress on Nursing Informatics, June 23-27, 2012, Montreal, Canada. International Congress in Nursing Informatics		usability lab;usability;usability engineering;component-based usability testing;usability inspection;system usability scale;usability goals;nursing;agile usability engineering;computer science	SE	-73.30428933799698	23.411771410629353	41350
48c03d96bc8393272a4a6de207614fe471c69e64	a comprehensive framework approach using content, context, process views to combine methods from operations research for it assessments	information systems;multi criteria decision making;it evaluation information systems multi criteria decision making case study decision supportsystems;decision support systems;it evaluation	Motivated by IT evaluation problems identified in a large public sector organization, we propose how evaluation requirements can be supported by a framework combining different models and methods from IS evaluation theory. The article extends the content, context, process (CCP) perspectives of organizational change with operations research techniques and demonstrates the approach in practice for an Enterprise Resource Planning evaluation.	enterprise resource planning;operations research;organizational behavior;requirement	Edward Bernroider;Stefan Koch;Volker Stix	2013	IS Management	10.1080/10580530.2013.739896	r-cast;decision support system;decision engineering;computer science;knowledge management;management information systems;management science;evidential reasoning approach;information system	SE	-77.9213401180853	8.266860692656373	41464
6436631b6ed26f37c9f27ec7dfde90d76b4988bc	strategic alignment between academy and industry: a virtuous cycle to promote innovation in technology	dp industry;software engineering;strategic planning;technology transfer;innovation management;innovation;software engineering challenges;strategic planning dp industry innovation management software engineering;software industry academy industry strategic alignment virtuous cycle technology innovation software engineering software development;software engineering challenges innovation technology transfer	Knowledge arises from the observation of problems. In this sense, Software Engineering emerged from the necessity to solve a practical problem: the lack of knowledge on how to develop software properly. However, in the Software Engineering field the interaction between Academy and Industry is still weak. This weakness reduces the capability to promote, in the long term, technology innovation in a comprehensive and sustainable manner. In this paper, we propose a mechanism to allow the strategic alignment between Software Engineering research interests and the software industry challenges. This mechanism will be used to achieve continuous innovation in Software Engineering. Furthermore, we expect to create a virtuous cycle in which both Academy and Industry act together to approximate the state-of-art and the state-of-practice in order to maximize their results.	academy;approximation algorithm;software engineering;software industry	Gleison Santos;Ana Regina Cavalcanti da Rocha;Tayana Conte;Monalessa Perini Barcellos;Rafael Prikladnicki	2012	2012 26th Brazilian Symposium on Software Engineering	10.1109/SBES.2012.31	personal software process;software engineering process group;systems engineering;engineering;knowledge management;social software engineering;operations management;software development;software deployment;software development process	SE	-68.84415043165185	18.634378157431016	41478
ac0ad4e390aa7ea79898e4257fa77ede39a112e8	a model for impact of organizational project benefits management and its impact on end user	sustainable;information technology;organizational project;end user	Today, the interests of an organization cannot be obtained by a single company, because the companies try to focus their business on the activities which are worthwhile to be focused on and they outsource other activities. In this paper, we proposed a conceptual framework and examined it to find the effect of some of the individual, organizational, and technological factors on the usage of organizational project benefits management and its impact on the end user. The outsourcing of information technology has been posed seriously for a long time; therefore, the managers have to increase their knowledge on the subject of outsourcing of information technology and deciding methods. This article has been provided for the purpose of providing a quantitative model for ranking the outsourcing options of information technology which makes use of organizational project benefits management analysis and analytic net process. The strengths, weaknesses, opportunities and threats is an important supporting tool and is usually used as a tool to systematically analyze the inner and outer environment of the organization. The analysis of organizational project benefits management makes it possible to determine the importance of criteria and the ability to evaluate the options to be decided. Analytic net process is suggested to determine the complex structure of outsourcing and determining the weight of the criteria and final stratification; furthermore, by applying this method the interdependence of the main criteria can be considered. The results of the studied case show that the consideration of the dependencies causes a decrease in the strengths and opportunities and increase in the weight of opportunities and threats. KEywORdS End User, Information Technology, Organizational Project, Sustainable	interdependence;outsourcing;threat (computer)	Hodjat Hamidi	2017	JOEUC	10.4018/JOEUC.2017010104	organizational learning;end user;computer science;knowledge management;marketing;management science;management;law;information technology;sustainability	SE	-79.58675284590956	7.5148540385379095	41572
dbb6b1e730676cacb4f4af29374c6e4a7d5051c2	comparing malaysian and scottish firms on practices for strategic capability management	manufacturing capabilities;technology;organisational structures malaysian firms scottish firms strategic capability management malaysian companies scottish companies malaysian manufacturing managers order processing scottish managers manufacturing strategy;strategic planning capacity planning manufacturing organisational aspects;technology change model manufacturing capabilities manufacturing strategy organisation;manufacturing companies production adaptation models planning bibliographies systematics;manufacturing strategy;change model;organisation;ts manufactures	The study of practices for Strategic Capacity Management at five Malaysian companies and four Scottish companies shows that the Malaysian manufacturing managers acted more reactive due to pressures by sales and processing orders, whereas the Scottish managers were implementing a manufacturing strategy more `independently'. Problems with suppliers, albeit sometimes caused by outsourcing, feature high on the list of challenges in both samples. Alignment of organisational structures and investment in technologies are seen by all as key to aligning the manufacturing strategy with the competitive strategy, though actual investments tend to be happening more in Scottish companies.	outsourcing;strategic management	Rob Dekkers;Kanagi Kanapathy	2014	2014 IEEE International Conference on Industrial Engineering and Engineering Management	10.1109/IEEM.2014.7058633	economics;engineering;operations management;management;technology	Robotics	-78.87215604312242	4.997108899000874	41603
7d462f478144fbda7559e41a57cf59c71dc4ba68	are reviews an alternative to pair programming?	software process improvement;controlled experiment;extreme programming;pair programming;reviews	From the first presentation of extreme programming on, pair programming has attracted a wide range of programmers to work together in front of one display. The proposed advantages of pair programming are a faster development cycle and code with higher quality. However, the nearly doubled personnel cost when compared to single developers seems to outweigh these advantages. Instead of showing the superiority of pair programming, we seek an alternative. Can a single developer be assisted by an already known technique with which he produces the quality of pairs with only a fraction of the cost? The answer with some restrictions is: yes, he can. Reviews are a reasonable candidate with respect to code quality and cost.	extreme programming;pair programming;programmer;software bug;software quality assurance;vagueness	M. Mueller	2003	Empirical Software Engineering	10.1023/B:EMSE.0000039883.47173.39	constraint programming;simulation;n-version programming;extreme programming;pair programming;computer science;engineering;operations management;software engineering;engineering drawing;copy and paste programming	SE	-70.6234550740473	28.607503697644166	41673
15d4e188cd93d0e002bc298cdd52a724d5c94f5b	examining potentials of building m&a preparedness	institute for integrated and intelligent systems;faculty of science environment engineering and technology;information systems management;080609	Enterprises are systems of systems that continuously evolve during their lifespan, be it in a directed or emergent way. As enterprises are in fact socio-technical systems, this evolution may occur in one or more specific areas such as the human / organizational, the technology and / or the Information System that integrates the activities performed by humans and machines (technology). This paper addresses a special type of change, brought about by enterprise mergers or acquisitions (M&As). M&As are an important strategic transformation instrument in the hands of management; however, literature reveals that an alarming high percentage of M&As do not achieve their declared objectives. In this paper we attempt to a) demonstrate that the success of such strategic changes depends on several essential and largely overlooked factors, and b) outline a possible approach of building preparedness for M&As), so as to improve the chances of success. This paper also presents a retrospective M&A case analysis to demonstrate the types of potential problems that could have been effectively addressed by anticipatory transformation facilitated by the proposed preparedness building approach.	emergence;emoticon;information system;sociotechnical system;system of systems	Nilesh Vaniya;Peter Bernus;Ovidiu Noran	2013		10.5220/0004418201990210	knowledge management;management information systems	AI	-73.74681366583263	10.903539332761758	41695
9e35bea5a7a9c950207ac29528fe38a6056d7889	"""putting the """"systems"""" in security engineering: an examination of nist special publication 800-160"""	quality assurance;systems oriented approach systems security engineering nist special publication 800 160 national institute of standards and technology special publication 800 160 trustworthy secure systems engineering;nist;systems security engineering;systems engineering and theory;computer security;computer security nist us department of defense military standards systems engineering and theory quality assurance;secure systems nist sp 800 160 800 160 security systems security systems security engineering;us department of defense;secure systems;military standards;800 160;systems security;security;nist sp 800 160;trusted computing publishing security of data systems engineering	Security professionals should be familiar with ongoing developments in the systems security engineering field, specifically the second public release of National Institute of Standards and Technology (NIST) Special Publication 800-160 Systems Security Engineering: Considerations for a Multidisciplinary Approach in the Engineering of Trustworthy Secure Systems. NIST SP 800-160 provides a systems-oriented approach to engineering secure systems in what is perhaps the most significant work in the specialty domain's history.	nist sp 800-90a;security engineering;trustworthy computing	Logan O. Mailloux;Michael McEvilley;Stephen Khou;John M. Pecarina	2016	IEEE Security & Privacy	10.1109/MSP.2016.77	quality assurance;nist;security engineering;computer science;information security;information security standards;nist special publication 800-53;world wide web;computer security	Security	-67.39440849951372	27.7590205246331	41723
1445c755cc5bb315ca431ad526e980b4d658c6ec	rapid development of electronic public services: software infrastructure and software process	software infrastructure;electronic governance;xg2g;public service;electronic public service;software process;messaging infrastructure	This paper presents a software infrastructure to support the execution and rapid development of Electronic Public Services (EPS). The infrastructure provides frameworks, components, services and tools to aid analysis, design, implementation and deployment of EPS in cross-agency environments. In its current version, the infrastructure include: two frameworks - Front-Office and Back-Office, three services - Workflow, Messaging and Infrastructure Management, and two components - Tracking and Notification. The paper presents the requirements, architecture and elements of this infrastructure. It also presents a software process for developing EPS based on the infrastructure. This work was done as part of e-Macao Project, funded by the Government of Macao SAR, to build a foundation for e-Government in Macao.	software development process	Tomasz Janowski;Adegboyega K. Ojo;Elsa Estevez	2007			spatial data infrastructure;systems engineering;operations architecture;software engineering;critical infrastructure;business;world wide web;converged infrastructure;firm-specific infrastructure	SE	-70.30347080514258	13.59507549345015	41747
3e479d437adde1aeb8d91d0f869ca4eb06d0760f	what to improve next? selecting and prioritising in software process improvement	software process improvement		software development process	Darren Dalcher	2005	Software Process: Improvement and Practice	10.1002/spip.247	team software process;software engineering process group;systems engineering;engineering	SE	-64.87401716244683	22.8140094048858	41783
2408be7faae436c134e8e06dfba3c3b33a9c13e6	development of the credit risk assessment mechanism of investment projects in telecommunications		We developed a mechanism of modelling of internal credit ratings (ICRs). It is applied in investment controlling to assess the credit quality of projects of telecommunication companies. Its advantages over the conventional credit risk modelling approaches are higher robustness and incorporation of all modelling operations in one mechanism. The mechanism gives the possibility to compare of modelled ICRs to the public credit ratings assigned by reputable international credit agencies. To achieve higher accuracy, the mechanism converts the input financial data, presented in different accounting standards, to the common basis. The explanatory variables in the mechanism are closely aligned with the credit risk assessment factors listed in the methodologies of international credit rating agencies. The testing of the mechanism shows that ICRs modelled with application of our mechanism had the accuracy ratio of 55% for testing sample and 65% for the design sample. This exceeds the accuracy ratios of ICRs modelled with application of conventional approaches (37%–42%).	risk assessment	Sergei Grishunin;Svetlana Suloeva	2017		10.1007/978-3-319-67380-6_28	project management;robustness (computer science);telecommunications;credit risk;accounting standard;business;finance;credit rating	NLP	-84.062289243381	7.8872456289369515	41790
01b9052616e45cbd917337069289c8bdfff216b4	selection criteria for software development tools for smes - smes and cooperatives in venezuela	software development tools;selection criteria	Software engineering tools have regained interests in recent years due to different changes affecting software developing organizations. These organizations carry out activities that might be undertaken in a plan driven and agile manner with the support of such tools. A proper balance between both approaches and the effective tool adoption will help organizations to meet their objectives and evolve. Small and medium enterprises and Cooperatives (S&C) share common characteristics throughout Latin America. Small and medium enterprises (SMEs) lack of formality in their roles and relationships among interacting individuals, whereas Cooperatives are usually small companies with weaknesses as to management techniques and technological equipment. In fact, both have difficulties when finding the right personnel and tools that best suit their needs. Considering Venezuela as our study subject, we have herein proposed some criteria to assist S&C in the tools selection that support their development processes while fostering the balance required between agility and discipline. Such criteria were formulated based on the characterization of five factors aimed at determining this balance. These contributions will help subsequently identifying methodological and technical aspects to provide guidance to S&C in the improvement of their development processes.	agile software development;chaos theory;interaction;programming tool;software engineering	Lornel Rivas;María A. Pérez;Luis Eduardo Mendoza;Anna Grimán	2008			computer science	SE	-69.50328432380883	20.11459301725653	41866
3dc696ad362f28b042f1956dd4fe4f3af7527fb8	it hiring growth modest, but steady	salary expectations it hiring business expansion analytical skills linux voice over ip voip computer security sap skills cobol it professionals soft skills;employment;it professional;salary expectations;companies information technology project management investments application software management training technology management humans software systems financial management;analytical skills;voice over ip voip;cobol;job candidates it hiring growth information technology salary expectations;job candidates;information technology;voice over ip;soft skills;business expansion;computer security;salaries employment information technology recruitment;it hiring;linux;sap skills;salaries;recruitment;it professionals;it hiring growth	The recent upturn in the economy has been good for business in the US, particularly for information technology (IT). And with continued growth projected for 2006, IT hiring is also slated for continued growth. This paper discusses the growth of hiring and salary expectations for job candidates in the field of IT		Linda Dailey Paulson	2006	IT Professional	10.1109/MITP.2006.22	public relations;analytical skill;simulation;computer science;voice over ip;cobol;management;law;information technology;computer security;linux kernel;soft skills	Web+IR	-74.42172752025836	17.81482600515084	41895
2d092573e0fd67fbfe4e0cad3427f1195e11929c	understanding the economic potential of service-oriented architecture	service-oriented architecture;soa economic potential model;soa literature;business partner;economic rationale;research model;business side;economic potential;soa case;economic aspect;business benefit	Service-oriented architecture (SOA) is one of the most discussed topics in the information systems (IS) discipline. While most computer scientists agree that the service-oriented paradigm has clear benefits in terms of technical quality attributes, it has been difficult to justify SOA economically. The few studies that have investigated the strategic and economic aspects of SOA are mostly exploratory and lack a more comprehensive framework for understanding the sources of its economic potential. Based on IS and SOA literature, our work goes further in suggesting the SOA economic potential model, which describes the causal relationships between the SOA's style characteristics and value it can provide on the business side. Using this model, we investigate 164 SOA cases published between 2003 and 2008 to explore the economic rationale for adopting SOA. Our findings suggest that SOA's business benefits are currently mainly driven by operational and information technology infrastructural improvements. However, enterprises also realize strategic benefits from SOA; for example, by electronically integrating with their business partners by means of SOA. We use the results of our study to derive propositions and suggest a research model for future studies on SOA's economic potential.	service-oriented architecture	Benjamin Müller;Goetz Viering;Christine Legner;Gerold Riempp	2010	J. of Management Information Systems		service;economics;computer science;knowledge management;marketing;service-oriented architecture;management science;law;oasis soa reference model	DB	-78.50548876079606	5.793761000625223	41933
264097d954f7f9380571c0b0873ded9918da5834	participatory development of user experience design guidelines for a b2b company	guideline;mindset;b2b;user experience;organizational change;design;participatory design	As business success is increasingly dependent on an organization's ability to provide a pleasant user experiences (UX) for its products, companies need to find ways to harness every employee to think about UX in their daily work. To support this goal, we present a participatory development process to create user experience design guidelines for a company developing materials-handling equipment for warehouses. The guidelines were developed to steer the work of all R&D designers and developers towards experience-driven design of the products in business-to-business context. The participatory process includes six steps: Spreading awareness of UX within the company, providing information on UX, supporting understanding of UX, co-creation of guidelines, reviewing the outcome, and implementing the guidelines. This paper concentrates on describing the first five phases. The participatory approach is applicable by other organizations to support the change towards experience-driven design. The process and outcome aims to support employees' everyday work aiming for products with pleasant UX.	a/ux;user experience design	Elina Hildén;Heli Väätäjä;Virpi Roto;Kero Uusitalo	2016		10.1145/2994310.2994355	systems engineering;engineering;knowledge management;management science	HCI	-70.43154065908617	6.529895121972352	41978
de8f70960bd6e24f7783d585ab96b0abae54c4f2	feature crumbs: adapting usage monitoring to continuous software engineering		Continuous software engineering relies on explicit user feedback for the development and improvement of features. The frequent release of feature increments fosters the application of usage monitoring, which promises a broad range of insights. However, it remains a challenge to relate monitored usage data to changes that were introduced by an increment and thereby to a particular specific of a feature.	software engineering	Jan Ole Johanssen;Anja Kleebaum;Bernd Brügge;Barbara Paech	2018		10.1007/978-3-030-03673-7_19	software engineering;systems engineering;agile software development;engineering;usage data	SE	-68.27091141476771	23.14685520549729	42125
60a74f20db5b758c68d27355f05cca8ac89efeab	managing the complexity of spi in small companies	quality system;software process	Most known models for SPI (e.g. SEI CMM, ISO standards and other methods derived from those mentioned) are primarily suited for large or medium organizations, but with some tailoring they provide substantial support also for the SPI in small organizations considering their specific characteristics. In the article a case of such tailoring – the PROCESSUS model – is presented. The baseline of the methodology is the integration of the CMM and the ISO 9001 together with the ISO 9000-3. According to the integrated model and the study of different lifecycles, a set of standard procedures (SP) and standard documents (SD) was defined. Each standard procedure provides guidelines on how to perform related activities, who is involved, which documents are supposed to be used/derived within the procedure etc. The set of SP and SD is the essential help for SPI conduction – for the purpose of small companies the optimal use of suggested documents and the disposition of roles was defined. The SP-SD set is also described in the article. Copyright  2000 John Wiley & Sons Ltd	baseline (configuration management);capability maturity model;john d. wiley;software engineering institute;standard operating procedure;technical standard	Romana Vajde Horvat;Ivan Rozman;József Györkös	2000	Software Process: Improvement and Practice	10.1002/(SICI)1099-1670(200003)5:1%3C45::AID-SPIP110%3E3.0.CO;2-2	quality management system;economics;systems engineering;engineering;operations management;software engineering;management;software development process	ML	-70.53474867940993	13.960917801413924	42177
9d889871b30f5f930b630f083f2e34688b5851d0	a study of staff turnover, acquisition, and assimilation and their impact on software development cost and schedule	software development	In this article we investigate how staff turnover, acquisition, and assimi- lation rates affect software development cost and schedule. A system dynamics model of the software development process is employed as our experimentation vehicle. In addition to permitting less costly and less time-consuming experimentation, simula- tion-type models can provide useful insights into the causes behind the different behavior patterns observed. Our results indicate that staff turnover, acquisition, and assimilation rates can increase a projectu0027s cost and duration by as much as 40 to 60 percent. This suggests that the three staffing variables are indeed critical for the successful development of software systems, as well as for the accurate estimation of software development cost and schedule.	cost estimation in software engineering;data assimilation;software development	Brad Tuttle	1989	J. of Management Information Systems		simulation;economics;organization;operations management;human resource management;management;operations research	SE	-70.43877097240144	20.518115493612513	42199
8b35dfdf04c0576bf05aa81991e3d5eae5dcfc7b	improving requirements analysis through cscw	requirement analysis		computer-supported cooperative work;requirement;requirements analysis	Cirano Iochpe	1995			computer-supported cooperative work;computer science;requirements analysis;systems engineering	SE	-62.88080153905622	20.75031840340992	42217
22b898cb717d0b3534bcbc74a30036749f849e8d	certification of algorithm 173: assign			algorithm	Roger S. Scowen	1963	Commun. ACM	10.1145/367651.367683	theoretical computer science;certification;computer science	Graphics	-84.5919495859788	30.688030243554923	42245
25de381f1cafe18c9eb9f9f161878f39ab1e6e8b	adaptive conjoint analysis for the vitalisation of angel investments by entrepreneurs	entrepreneurship;government policy regulation;angel	In order to promote angel investments, attracting those who have entrepreneurial experience is necessary because their capital, unique managing experience, and business insight are highly beneficial to the performance of the investment. This can be accomplished by providing a preferred investment environment for potential angels who have entrepreneurial backgrounds. In this paper, we use an adaptive conjoint analysis to identify preferred investment environments represented as a combination of support policies for business angels who have entrepreneurial experience in Korea. Of particular interest is a preference for angel insurance. This insurance is newly proposed based on prospect theory, which explains that people tend to be more sensitive to a loss than to a gain. The value of the angel insurance is compared to those of tax policies and matching funds for business angels. The empirical results are expected to contribute to promoting angel investments by attracting entrepreneurs to become business angels. ARTICLE HISTORY Received 20 August 2014 Revised 6 November 2015 Accepted 15 December 2015		Jungwoo Suh;So Young Sohn	2016	Techn. Analysis & Strat. Manag.	10.1080/09537325.2015.1134770	actuarial science;economics;entrepreneurship;marketing;management;law;economic growth;commerce	HCI	-82.12877138902286	6.981788966940124	42359
b3b165fab89c3bfbe7ddde30f5488402ab7f3bac	applying manufacturing performance figures to measure software development excellence		The Internet of Things is going to digitize traditional man- ufacturing plants. Apart from being as functional and robust as ever, products required to run these plants will need to be smart and con- nected. They will have software inside. Producing companies monitor their manufacturing excellence related to these products by evaluating manufacturing performance figures such as delivery time and yield. How- ever, for the time being, no figures for the software inside are measured with similar means. Software performance figures have been investigated a lot in software research and in the IT industry. However, as they are software domain- oriented they are difficult to understand for leading managing minds of producing companies. This article demonstrates that it is reasonable to apply manufac- turing performance figures to measure software development excellence. This is a valuable element ensuring future business success of producing companies by enabling their managers to control excellence in software development processes.	software development	Andreas Deuter;Hans-Jürgen Koch	2015		10.1007/978-3-319-24285-9_5	personal software process;software quality management;systems engineering;engineering;operations management;software development;software engineering;software deployment;software metric	Robotics	-67.90793687417447	24.75511153384187	42368
363f68ed5d379b130bcaa19ac9a266707593bdc5	an integrated business model innovation approach: it is not all about product and process innovation	product and process innovation;business model	Not long ago Information and Communication Technology (ICT) was reserved to a few specialists. In the last decades, however, ICTs became ever easier to use and are nowadays open to the majority of western society. Modern ICTs have become part of our daily lives and have even changed our way of life. We are getting used to checking our e-mail inboxes on a daily basis, connecting with our friends via social networking websites such as Facebook (http://www.facebook.com), being reachable 24/7, writing documents on personal computer applications, finding the fastest train connections via an ABsTRAcT	email;fastest;personal computer	Roman Boutellier;Markus Eurich;Patricia Hurschler	2010	IJEEI	10.4018/jeei.2010070101	business model;product innovation;innovation management;computer science;artifact-centric business process model;business process management;process modeling;process management;business process;product management;business process discovery;business rule;new business development;business process modeling	Metrics	-74.47686264424756	4.431573307188718	42373
418ff49d6b59f055f116b8d8142eb3faa26f7212	the carnegie mellon university master of software engineering specialization tracks	databases;domain specific software;specialization tracks;master of software engineering;mse program;human computer interaction;master of software engineering specialization tracks;design engineering;graduate software engineering programs;hci;fundamental software engineering skills;software engineering databases data engineering design engineering knowledge engineering power engineering and energy servomotors human computer interaction production facilities floors;data engineering;commerce;software engineering;software engineering education;domain knowledge;power engineering and energy;domain specific software engineering education;computer science education;real time systems software;human factors;application domain knowledge;educational courses;business;production facilities;servomotors;commerce computer science education teaching educational courses software engineering real time systems human factors;servo system;data handling;business carnegie mellon university master of software engineering specialization tracks domain specific software real time systems software specialization tracks master of software engineering application domain knowledge fundamental software engineering skills mse program real time computing human computer interaction hci;real time computing;carnegie mellon university;floors;space vehicles;teaching;real time systems;knowledge engineering	There is an increasing demand for domain-specific software. For example, the software to control a machine on a factory floor is different in significant ways from the software to manipulate large databases. The software engineer building real-time systems software to control a motor that powers a piece of machinery needs some understanding of the motor’s servo system; whereas a software engineer who designs the software to manage large databases for the NASA Space Station needs specific knowledge about database models as well as the types of data handled on a long-term space vehicle. Speciali zation tracks within the Master of Software Engineering (MSE) Program at Carnegie Mellon University enable students to gain application domain knowledge while developing fundamental software engineering skil ls. The MSE Program currently offers speciali zation tracks in real-time computing, human-computer interaction (HCI), and business. This paper overviews these tracks.	application domain;database model;human–computer interaction;partial template specialization;real-time computing;real-time locating system;real-time transcription;servo;software engineer;software engineering	Carol L. Hoover;Mary Shaw;Nancy R. Mead	1996		10.1109/CSEE.1996.491366	domain analysis;personal software process;architecture tradeoff analysis method;verification and validation;computing;software engineering process group;software sizing;systems engineering;engineering;package development process;social software engineering;component-based software engineering;software development;software design description;software engineering;domain engineering;software construction;software walkthrough;software deployment;software requirements;software system;computer engineering;software peer review	SE	-64.35630093295974	26.554671543659783	42402
b8755254afaeb761aa4dae12773a47ff73f61c7e	preparing for the future: understanding the seven capabilities of cloud computing	cloud computing	To date, conversations about cloud computing have been dominated by vendors who focus more on technology and less on business value. While it is still not fully agreed as to what components constitute cloud computing technology, some examples of its potential uses are emerging. We identify seven cloud capabilities that executives can use to formulate cloud-based strategies. Firms can change the mix of these capabilities to develop cloud strategies for unique competitive benefits. We predict that cloud strategies will lead to more intense ecosystem-based competition; it is therefore imperative that companies prepare for such a future now.	cloud computing;ecosystem;imperative programming	Bala Iyer;John C. Henderson	2010	MIS Quarterly Executive		engineering;systems engineering;cloud testing;cloud computing	HPC	-76.64552068245823	6.497639974416016	42468
f85b7b78681ac5206ef84f7db3814ee420f58df8	the impact of emerging computing models on organizational socio-technical system		Consolidated Enterprise IT solutions have proven to enhance business efficiency when significant fractions of local computing activities are migrating away from desktop PCs and departmental servers and are being integrated and packaged on the Web into “the computing cloud.” Whether referred to Grid, Utility or Cloud Computing, the idea is basically the same: instead of investing in and maintaining expensive applications and systems, users access and utilize dynamic computing structures to meet their fluctuating demands on IT resources and pay a fixed subscription or an actual usage fee. The immense economic demands in the last several years, in conjunction with the immediate reduction of upfront capital and operational costs when cloudbased services are employed, increase the speed and the scale of cloud computing adoption both horizontally -across industries-, and vertically –in organizations’ technology stacks. In actuality, the radical changes for organizations are in rethinking and reengineering their traditional IT resources, advancing them with cloud architectures and implementing services based on dynamic computing delivery models. The changes and business transformations are underway on a large scale, from providers and customers to vendors and developers. The key issues are not only in economics and management, but essentially how emerging IT models impact organizational structure, capabilities, business processes, and consequential opportunities. This paper explores the impact of the dynamic computing models on the organizational socio-technical system and provides the author's vision and experience in strategizing and utilizing emerging cloudbased applications and services.	business process;cloud computing;code refactoring;desktop computer;sociotechnical system;world wide web	Ivan Ivanov	2011		10.1007/978-3-642-36177-7_1	organizational learning	Web+IR	-73.38384744430834	8.150772365770257	42583
3b1c4c2f3ba3661da980549f8e1683983f4957e7	collaboration practices in global inter-organizational software development projects	collaboration practices;software development;global software development;peer to peer;communication;face to face;problem solving;inter organizational software development;interorganizational software development	Global interorganizational software development projects are becoming common, but their management and the creation of practices and processes to support collaboration seem to be harder than what the companies expect. In this article, we present successful collaboration practices collected in an interview study of eight globally distributed interorganizational software development projects. On the basis of 34 semistructured interviews, we were able to identify several practices that the interviewees subjectively deemed successful. The identified collaboration practices include: milestone synchronization, frequent deliveries, and the establishment of peer-to-peer links. The need to plan for problem-solving communication was often neglected in the beginning of the project, despite its paramount importance. We identified several ways to ease related problems, such as having a dedicated person solve problems, using bulletin boards and e-mail lists or dedicated mailboxes. Successful projects had learned the value of two-way communication regarding project progress monitoring. Finally, practices helping in building and maintaining a working relationship included face-to-face meetings, distribution of organization charts, and having people travel to give all sites faces. Copyright  2004 John Wiley & Sons, Ltd.	chart;email;in the beginning... was the command line;interviews;john d. wiley;peer-to-peer;problem solving;software development	Maria Paasivaara;Casper Lassenius	2003	Software Process: Improvement and Practice	10.1002/spip.187	extreme programming practices;software project management;computer science;systems engineering;engineering;knowledge management;software development;software engineering;management science;management;software peer review	SE	-71.73978663460304	15.944967474430648	42645
60624e3f1678a4ab0e473a4e4c1d1b2177410150	evaluation of the hardware for a mobile measurement station	analytic hierarchy process;reliability;quality engineering;quality function deployment qfd analytic hierarchy process ahp hardware selection measurement system mobile observation point quality engineering;analytic hierarchy process ahp;quality function deployment;sensors;quality function deployment mobile communication pollution measurement reliability correlation sensors hardware;engineering tools;quality function deployment qfd;pollution measurement;measurement system;mobile measurement station;mobile communication;engineering tools evaluation of the hardware mobile measurement station harsh environment quality engineering method quality function deployment analytic hierarchy process;quality engineering method;hardware selection;correlation;harsh environment;evaluation of the hardware;mobile observation point;hardware	This paper presents a specific approach to the design of a mobile measurement station (MMS) that will operate in a harsh environment. This paper also presents a selection process for the hardware of the MMS. The work is based on a quality engineering method called the quality function deployment and on an operations research tool called the analytic hierarchy process. These approaches have not found a widespread application in the design of measurement systems. An important part of this paper is the selection of a controller for the MMS, for which the authors describe two engineering tools for the selection procedure. Most engineering designs can benefit from using these two engineering tools for selecting components.	analytical hierarchy;http 404;operations research;quality engineering;quality function deployment;software deployment;system of measurement	Bogdan Dziadak;Andrzej Michalski	2011	IEEE Transactions on Industrial Electronics	10.1109/TIE.2010.2093478	reliability engineering;quality assurance;quality function deployment;analytic hierarchy process;mobile telephony;computer science;systems engineering;engineering;sensor;system of measurement;reliability;mathematics;correlation	SE	-63.25962881262347	28.774611033743614	42653
1f8a14ebd6bc1b1c816b4c9a301113f4393e4467	open source software: placebo or panacea? - panel 3	open source software;interesting issue;crown jewel;basic premise;software source code;project leader;free software;project level;code god;proprietary software company	open source software;interesting issue;crown jewel;basic premise;software source code;project leader;free software;project level;code god;proprietary software company	open-source software	Jesper Holck;Danny Petterson;Kim Östrup;Bob Fitzgerald	2003		10.1007/1-4020-7862-5_14	simulation;engineering;management;social psychology	SE	-77.24489985546421	24.899845103479763	42718
4835067914678d2e3700af0fe30c26779ca61609	shareholder protection and agency costs: an experimental analysis		Two competing principal–agent models explain why firms pay dividends. The substitute model proposes that corporate insiders pay dividends to signal and build trust with outside shareholders who lack legal protection. The outcome model, in contrast, surmises that when shareholders have legal protection, they demand dividends from insiders to prevent them from expropriating corporate funds. Either way, dividends represent an agency cost paid to align the interests of shareholders and insiders. Expropriations by insiders and reduced investment by shareholders are also agency costs, but they are difficult to identify with archival data. Using a laboratory experiment, we identify the impact of strengthened shareholder protection on all three types of agency costs. Dividend payout ratios are five times larger with stronger investor protection, insider expropriation ratios are twice as high, and outsider investment falls by 45%. Thus, we find evidence that strengthening shareholder protection introduces previous...		Jacob LaRiviere;Matthew McMahon;William Neilson	2018	Management Science	10.1287/mnsc.2017.2770	economics;microeconomics;expropriation;agency cost;insider;dividend payout ratio;finance;shareholder;dictator game;dividend	Logic	-84.9522201899907	5.630049364589947	42731
43b9e6cbb87dd04bc1f7a0e126d5d1a585e27f55	targeted strategic alignment via real-time perioperative performance dashboards		This study examines business process management practices of business analytics via balanced scorecards and performance dashboards within hospital processes to target and achieve improvement aligned to strategy. The study maps specific perioperative metrics in the hospital environment to clinical results and demonstrates how business process management is applicable for aligning perioperative clinical outcomes to hospital strategy. Identification of existing limitations, potential capabilities, and the subsequent contextual understanding are contributing factors that yield measured improvement within the hospital’s perioperative process. Based on a 150-month longitudinal study of a large 1,046 registered-bed academic medical center, this case study investigates the impact of integrated systems to qualify and quantify business analytics to improve hospital efficiency and effectiveness across patient quality of care, patient satisfaction, operational efficiency, and financial cost effectiveness. The theoretical and practical implications and/or limitations of this study’s results are also discussed.	beam propagation method;business analytics;business process;customer relationship management;end-to-end encryption;holism;map;real-time transcription	Jim Ryan;Barbara Doster;Sandra Daily;Carmen Lewis	2016			systems engineering;knowledge management;biological engineering	HCI	-78.26185004554173	10.331493735548529	42742
e6f58547adc750e9d1ef1541ac4df0a2d7e0afdb	the effects of human resource capability and internal customer satisfaction on organizational effectiveness		Human resource capability is valuable, rare, irreplaceable, and difficult to imitate; therefore, it is crucial for creating sustainable competitive advantages. Human resource capability can be appropriately used to improve the performance of an organization. This study adopted a process perspective to propose an integrated model that comprehensively considers the key variables of human resource capability and organizational effectiveness. In addition, empirical research was conducted by using a state-owned Company-A as an example. The theory of service-profit chain posits that internal customer satisfaction and loyalty influences customer satisfaction and loyalty before affecting a company’s profitability and growth. Consequently, we explore the effects of human resource capability, internal customer satisfaction and commitment, and organizational effectiveness. Based on research findings, insightful and practical guidance is suggested for leveraging human resource capability to enhance organizational performance.	norm (social)	Huan-Ming Chuang;Mao-Jen Liu;You-Shyang Chen	2015	IJDSN	10.1155/2015/835194	organizational behavior and human resources;knowledge management;customer retention	AI	-80.2865690768931	4.890386506987392	42751
1f2d47ed4b6add85cc0b92b6d1729a019290bc4e	advances in unit testing: theory and practice	software;software testing;test oracles;industries;software engineering;writing;test generation;parameterized unit testing	Parameterized unit testing, recent advances in unit testing, is a new methodology extending the previous industry practice based on traditional unit tests without parameters. A parameterized unit test (PUT) is simply a test method that takes parameters, calls the code under test, and states assertions. Parameterized unit testing allows the separation of two testing concerns or tasks: the specification of external, black-box behavior (i.e., assertions or specifications) by developers and the generation and selection of internal, white-box test inputs (i.e., high-code-covering test inputs) by tools. PUTs have been supported by various testing frameworks. Various open source and industrial testing tools also exist to generate test inputs for PUTs. This technical briefing presents latest research on principles and techniques, as well as practical considerations to apply parameterized unit testing on real-world programs, highlighting success stories, research and education achievements, and future research directions in developer testing.	black box;open-source software;unit testing;white-box testing	Tao Xie;Nikolai Tillmann;Pratap Lakshman	2016	2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)	10.1145/2889160.2891056	non-regression testing;test strategy;keyword-driven testing;black-box testing;regression testing;model-based testing;orthogonal array testing;software performance testing;white-box testing;manual testing;integration testing;computer science;systems engineering;engineering;acceptance testing;software engineering;functional testing;dynamic testing;session-based testing;software testing;unit testing;data-driven testing;writing;system testing;test management approach;algorithm	SE	-63.324460066614	29.20348543916773	42820
0cf3ad34a4d4391df2dc2b44660d561ac7f5b7f7	low degree of separation does not guarantee easy coordination	distributed development;software;outsourcing;electronic mail;coordination delay distributed development software development process onshore outsourcing;delay organizations software resource management electronic mail software engineering outsourcing;software management;distributed processing;resource management;software development process;software management distributed processing;software engineering;coordination delay;organizational problems global competition software companies offshore partners onshore partners distributed onshore development project task allocation complexities;organizations;onshore outsourcing	In the times of increased global competition, software companies are forced to search for more effective development practices and often team up with onshore and offshore partners to develop faster and better products. In this paper we empirically explore a highly distributed onshore development project with a complex coordination structure. Our findings demonstrate that onshore development projects are not protected from coordination and communication challenges and task allocation complexities. Previously reported qualitative findings regarding organizational problems in this paper are supplemented with quantitative measurements of the true coordination delays and additional analysis of coordination patterns and their evolution.	software industry;the times	Zane Galvina;Darja Smite	2012	2012 38th Euromicro Conference on Software Engineering and Advanced Applications	10.1109/SEAA.2012.79	systems engineering;organization;resource management;software engineering;management;software development process;outsourcing	SE	-67.67461104925378	20.939841646777015	42914
53f46fe698f091babf537fcc26e8e057c4f72d2c	"""man-machine-interaction in the field of logistics: example """"internet of things"""""""		The introduction of ICT-systems (ICT: Information- and communication technologies) has led to fundamental changes in numerous fields of the world of work in recent years. Above all the networking with the aid of ICT-applications (e.g. safety systems in cars, airplanes etc.) has created many possibilities to monitor complex systems and processes and to control them more and more without human interfer- ence. Weyer (Autonomie und Kontrolle. Arbeiten in hybriden Systemen am Beispiel der Luftfahrt. Technikfolgenabschatzung—Theorie und Praxis Nr. Dortmund, 2007, p. 35) comments this development as follows: ,,Ein besonderes Merkmal der jung- sten Entwicklung ist zudem das scheinbar unaufhorliche Vordringen autonomer technischer Systeme, die immer mehr zu Mitspielern in derartigen Netzwerken ge- worden sind'' (Translation: A special characteristic of this recent development is the apparently continuous permeation of autonomous technical systems which have increasingly become players in such networks). The emergence of ''hybrid con- stellations, permeated by human actors and (semi)autonomous machines'' (cf. ibid.) is one of the consequences of this development. As it will be shown later, the autonomy of ICT-systems has been stepwise increasing while the role of the human actors is taking a back seat. The triggers are new high-tech-developments in the field of RFID (Radio-Frequency Identification) (Want et al. IEEE Pervasive Computing, 2006, 5:25-33) and the ''Internet of Things'' (The term is attributed to Kevin Ashton who used the expression ''Internet of Things'' for the first time in 1999 (Ashton, RFID Journal. http://www.rfidjournal.com/article/view/4986, 2009). The ''Internet of Things'' describes the combination of the real world with the virtual world of		Lars Windelband;Georg Spöttl	2012		10.1007/978-3-642-35966-8_46	simulation;engineering;operations management	ML	-91.85664630524501	23.97028875555663	42984
b4c8bfa63565e1d91d7a5fed386a3cd52decbbe7	secure concept for online trading of technology data in global manufacturing market		The high-tech strategy of the German government aims to expand the networking and intelligence capabilities of machines, products and services. Thereby it is essential to apply extensive utilization of information and communication technologies (ICT). The goal of the German term “Industrie 4.0” is to merge the physical world with the virtual world [1]. This interconnected digital world enables various opportunities for creating new business models and increasing companies’ revenues at the same time. Online trading of goods increased extensively in the last years, especially online trading of digital goods like music, films and e-books. Various license models and usage control policies are developed for a secure utilization of these goods by customers. Indeed there are still challenges regarding IT security issues that hinder the expansion of digital trading in the industry. This paper demonstrates a new business model for online trading in the automation and manufacturing industry. This model is based on existing resources in companies and hence improves the added-value-chain in companies. Thereby technology data that is required for machine operation in manufacturing processes will be traded. The main concept and workflow of trading processes will be presented. Furthermore various needed license models for usage control of Technology Data (TD) after trading are demonstrated.		Ghaidaa Shaabany;Simon Frisch;Reiner Anderl	2017		10.1007/978-3-319-72905-3_61	automation;digital goods;business;government;marketing;commerce;electronic trading;workflow;manufacturing;business model;information and communications technology	Robotics	-72.88325021009345	5.305842613476876	43052
64af9f0ba7c29f4c7062ba5461e1e7f1dfca927f	it support for business process innovation -- architectural choices and design challenges	organisational aspects business process re engineering innovation management;design challenges;creativity theory;it support;dynamic capability theory;monolithic process innovation system it support business process innovation architectural choice design challenge bpi competitiveness dynamic capability operational process creative activity it tool organization success factor socio technical system task heterogeneity property collaborativeness property technical level organizational level distributed process innovation system;technological innovation sensors collaboration sociotechnical systems computer architecture;innovation management;dynamic capability theory business process innovation design science design challenges it support creativity theory;business process re engineering;business process innovation;design science;organisational aspects	For many organizations, business process innovation (BPI) is a crucial factor to ensure competitiveness. BPI can be characterized as a dynamic capability, since it facilitates change in operational processes. Moreover, it involves creative activity, since new and purposeful business processes are created. In this context, choosing and implementing IT tools that are supportive to these two specifics of process innovation can be considered a key success factor for organizations. However, both characteristics lead to challenges concerning the design of a socio-technical system. Companies have to be aware of those challenges in order to make an informed decision on system design. In this paper, we derive and consolidate characteristics of the innovation process. We identify two major properties - 1) task heterogeneity and 2) collaborativeness - and derive key design challenges on both technical and organizational level that these properties pose for the development of either distributed or monolithic process innovation systems.	business process interoperability;sociotechnical system;systems design	Matthias Voigt;Kevin Ortbach;Ralf Plattfaut;Björn Niehaves	2013	2013 46th Hawaii International Conference on System Sciences	10.1109/HICSS.2013.371	product innovation;innovation management;knowledge management;artifact-centric business process model;business process management;management science;process management;business process;business process discovery;management;business process modeling	EDA	-77.33585335712701	6.344335397550769	43218
bd2d6ed96977659ccb3a55bb0720a78401c52f8d	the research on credit risk of business groups based on related guarantee	empirical;credit risk	The phenomenon of related guarantee is widely exists in the operation of business groups. This related guarantee complicates the credit risk of business groups in one hand and makes the risk control more difficult of commercial banks in the other hand. In this paper it proposed a theoretical model to describe the basic mechanism of credit risk in business groups based on structure model. Then it also analyzed the effect on credit risk of business groups of related guarantee empirically. The research results show that the relationship between amount of guarantee and intensity of credit risk contagion is nonlinear and the different structure of guarantee will bring different influence for credit risk of business groups. 946 Yang Yang et al. / Procedia Computer Science 17 ( 2013 ) 945 – 950 caused by the information diffusion. Other researches which discussed credit risk of business groups quantitatively mainly focused on two perspectives. One is based on the simplified model developed by Jarrow. And the other is based on the structure model developed by Merton. Besides that, a few of scholars also focused on the problem of integrated risk with different related risks quantitatively. But all these researches above did not focus on the effect on credit risk of business groups of related guarantee. Besides the first section, this paper proposed a mathematic model based on structure model to analyze the credit risk contagion between two subsidiaries in section two. And in the section three we discuss the affect for credit risk of business groups with different type of related guarantee based on the empirical data. The last section is the conclusions of this research. 2. The Basic Mechanism of Risk Contagion Based on Related Guarantee The section two can be divided into two parts. The first part of this section is the mathematic model which can describes the mechanism of risk contagion. And the second part of this section is the main result of the theoretical model. 2.1 Mathematic Model It assumes that a business group is constructed by two subsidiaries: subsidiary A and subsidiary B. The subsidiary A has borrowed K from commercial bank at t=0. The mature time of these loan is t=T. For simplicity, it further supposes that the subsidiary B has provided related guarantee for subsidiary A. The guarantee contract promises that the related guarantee has priority right while subsidiary A defaults. Follow the assumption of previous research, it assumes that subsidiary B has debt XB(XB>0) , mature time of its loan is T. For simplicity, subsidiary A has not any other debt during the time [o,T]. To describe the relationship between subsidiary A and subsidiary B in business group, it supposes subsidiary B has stake in subsidiary A. In order to take advantage of structure model, it assumes values of subsidiary A and subsidiary B at time t are VA(t) and VB(t). The value of subsidiary B, VB(t), can be divided into two parts. First part is the value of held equity of subsidiary A. i.e. ) (t VA . The other part of is the rest part of value of subsidiary B which is shown as Y(t). ) ( ) ( ) ( t V t Y t V B A (1) Further, it supposes that the values of these two subsidiaries are subject to the following random process:	c date and time functions;computer science;computer simulation;nonlinear system;stochastic process;theory;yang	Yang Yang;Li Li;Zongfang Zhou	2013		10.1016/j.procs.2013.05.120	actuarial science;credit risk;credit valuation adjustment;financial risk management;credit enhancement	AI	-85.01188220293854	10.198634430756563	43277
e27b9f3c267f27f3963d740c9c9681f7097a5eae	a non-functional requirements recommendation system for scrum-based projects			non-functional requirement;recommender system;scrum (software development)	Felipe Barbosa Araújo Ramos;Antonio Alexandre Moura Costa;Mirko Perkusich;Hyggo Oliveira de Almeida;Angelo Perkusich	2018		10.18293/SEKE2018-107	recommender system;systems engineering;non-functional requirement;computer science;scrum	SE	-63.830308510790694	22.58533070163686	43285
206fdb798cd3c83aace222debb675ed4f7144667	business activity patterns: a new model for collaborative business applications	user study;activity pattern;it value;business process	P. Moody D. Gruen M. J. Muller J. Tang T. P. Moran In this paper, we describe the vision behind the Unified Activity Management project at IBM Research. In particular, we describe and discuss activities, activity-centered computing, and activity patterns and illustrate the potential impact of this approach and its value to individuals, teams, and the enterprise. We discuss business activities and their integration into the development of business processes. We share insights from user studies and feedback from customers on the benefits of the activity model in a variety of business settings.	activity recognition;business logic;business process;function model;ibm research;usability testing	Paul Moody;Daniel Gruen;Michael J. Muller;John C. Tang;Thomas P. Moran	2006	IBM Systems Journal	10.1147/sj.454.0683	business model;business analysis;business domain;business requirements;systems engineering;knowledge management;artifact-centric business process model;business process management;business case;process modeling;electronic business;business process model and notation;process management;business system planning;business process;business relationship management;business process discovery;management;business rule;new business development;business process modeling;business activity monitoring;business architecture	Web+IR	-71.70540834989895	9.165225868181793	43292
627b4ef1d31613dfbd9aa22e45c79efb91676ebf	combining local negotiation and global planning in cooperative software development projects	change management;software project management;software development;planning;development methodology;conflict resolution;version and configuration management;working paper;configuration management;negotiation	In cooperative software development, each programmer has their own plans and conflicts or redundancies inevitably arise among them. We are concerned with two main problems: first, to control changes without sacrificing programmers’ flexibility, and, second, to guide change activities to conform project policies. Traditional methods of change request management focus on the management process s~cture based on project policies while cooperative development methodologies concern mainly with the conflict resolutions among each changes. In this paper, we describe an architecture which deals with proposal of changes. Based on plan integration it seamlessly supports both change coordination through negotiations and the change management process to have changes converge until they meet the project goals.	change management (engineering);change request;converge;programmer;software development	Kazuo Okamura	1993		10.1145/168555.168582	project management;personal software process;change management;extreme project management;program management;software configuration management;software project management;systems engineering;knowledge management;social software engineering;software development;software as a service;management science;business;application lifecycle management;software development process	SE	-65.8066417025243	19.680419950996182	43358
44c31def4bb7067fcbc39818b1dd3650f65a4ec7	empirical study of software component integration process activities	component integration phase;cbs integration process activity;quality property analysis;component based system;software product quality;glue code specification;small to medium sized organisation;architectural model development;component functional specification;software component integration process activity;structural compatibility analysis	The component integration phase is key to component-based system (CBS) success because of its profound impact on the quality of a software product. However, CBS integration is a complex phase because it is rarely the case that components are perfectly matched and ready for ‘plug and play’. The component integration phase involves assembling pre-existing software components usually developed by different parties, and writing glue-code to handle the mismatches between CBS-to-be requirements and available component features. The objective of the study is to gain an in-depth understanding of the impact of integration process activities on the overall success of a CBS. The empirical study also investigates the inter-dependency between the CBS integration process activities. A survey was developed and data from CBS practitioners working in smallto-medium-sized organisations were collected. The results show that ‘component functional specification’, ‘structural compatibility analysis’, ‘architectural model development’ and ‘early glue-code specification’ are integration process activities that have positive correlation with the successful development of a CBS. However, the results indicate that the ‘quality properties analysis’ is not carried out as an integration process activity by the majority of CBS practitioners during development of a CBS. Furthermore, the results of the survey also provide empirical evidence that there is a positive association between various key CBS integration process activities.	component-based software engineering;functional specification;glue code;plug and play;requirement	Sajjad Mahmood	2013	IET Software	10.1049/iet-sen.2012.0120	systems engineering;engineering;engineering drawing;system integration	SE	-69.38420593192917	22.880471039457575	43378
001ef70463623044502f2073d61c6846261c4323	mconcappt - a method for the conception of mobile business applications		Mobile business applications (mobile business apps) bear huge potentials for increased work productivity, work comfort, and even sales if they are of high quality. Usability and user experience, in particular, are among the key quality attributes. The high quality requirements of mobile business apps require them to be thoroughly engineered. Unfortunately, today’s software engineering approaches are often too heavy-weight to allow developing highquality mobile business apps in the context of mobile projects, which often face small budgets, extremely limited effort, and short time-to-market requirements. This paper presents mConcAppt, a user-centered and lightweight conception method for mobile business apps. It provides guidance for requirements engineering and interaction design for mobile business apps and provides interfaces to other activities, such as visual design, architectural design, implementation, and testing. The adoption of mConcAppt in various industrial contexts indicates that it enables organizations to elaborate a concept for a highquality mobile business app in 2-4 weeks.	display resolution;interaction design;list of system quality attributes;requirement;requirements engineering;software engineering;usability;user experience;user-centered design	Steffen Hess;Felix Kiefer;Ralf Carbon;Andreas Maier	2012		10.1007/978-3-642-36632-1_1	knowledge management;business administration	HCI	-75.29369716831961	19.395795550300193	43397
5e606eec4b083d2c60d4aa5a77da4128b56b41d7	using multidimensional concepts for detecting problematic sub-kpis in analysis systems		Business Intelligence, and more recently Big Data, have been steadily gaining traction in the last decade. As globalization triggers the ability for small and medium enterprises to enter worldwide markets, monitoring business objectives and pinpointing problems has become more important than ever. Previous approaches have tackled the detection of particular problematic instances (commonly called Key Performance Indicators-KPIs), trying to search for the events that are driving companies to be far away from organization’s main goals. One of the key problems is that even though KPIs are positive, they are normally calculated from other sub-KPIs and therefore, it is crucial to find out which is the concrete sub-KPI that is negatively influencing the main KPI. Therefore, in this paper, we focus on a semi-automatic approach for finding the key sub-KPIs that have bad results for the company. This approach is checked on real data that are used to create a report showing potential weaknesses in order to help companies to find out which factors may affect concrete sub-KPIs. Our approach allows us to provide insights for decision makers and help them to determine the underlying problems for achieving a goal and thereby, aiding them with taking corrective actions.	sensor	Alberto Esteban;Alejandro Maté;Juan Trujillo	2017		10.1007/978-3-319-70625-2_16	globalization;performance indicator;management science;computer science;data mining;business intelligence;big data;small and medium-sized enterprises;business analysis	Logic	-75.18681330687262	12.07368966047685	43500
b0182c78e8368931850d8b2b35c8eaca38878b18	what programmers really do - an observational study		Although the field of program comprehension as a research discipline has evolved considerably over the past years, only little is known about how software engineers perform their work. In this paper, we report on an observational study that we have carried out to investigate how software developers understand code when they approach a given maintenance task. We particularly focused on the developers’ activities, tools, information needs and their practices. In the study, we observed seven professional programmers at a large supplier in the automotive domain while performing a real maintenance task within their normal workflow. Afterwards we conducted a semi-structured interview to get a deeper insight into the process of program understanding. The focus of our analysis has been on what kind of activities a programmer performs and how those activities depend on each other. We categorize different kinds of activities based on this analysis, highlight challenges faced by the programmers, and discuss the implications of our results on the maintenance process.	categorization;information needs;list comprehension;program comprehension;programmer;semiconductor industry;software developer;software engineer	Rebecca Tiarks	2011	Softwaretechnik-Trends		software engineering;computer science;systems engineering;observational study	SE	-64.72553957611457	30.05624288652226	43641
cbdbc4eca8ecbb9996d0468a607cff7a3dffece8	crowdsourcing processes: a survey of approaches and opportunities	manuals;outsourcing;web and internet services;crowdsourcing processes;internet web technologies;visualization;crowdsourcing visualization web and internet services quality control;web services;writing;tools and platforms internet web technologies crowdsourcing crowdsourcing processes;quality control;research and development directions crowdsourcing approach crowdsourcing process management outsourcing crowd tasks machine tasks;tools and platforms;crowdsourcing	This article makes a case for crowdsourcing approaches that are able to manage crowdsourcing processes -- that is, crowdsourcing scenarios that go beyond the mere outsourcing of multiple instances of a micro-task and instead require the coordination of multiple different crowd and machine tasks. It introduces the necessary background and terminology, identifies a set of analysis dimensions, and surveys state-of-the-art tools, highlighting strong and weak aspects and promising future research and development directions.	crowdsourcing;outsourcing	Pavel Kucherbaev;Florian Daniel;Stefano Tranquillini;Maurizio Marchese	2016	IEEE Internet Computing	10.1109/MIC.2015.96	web service;quality control;visualization;crowdsourcing software development;computer science;data science;internet privacy;writing;law;world wide web;crowdsourcing;outsourcing	Visualization	-75.15523616149774	16.704399679382526	43647
b9b75213db3c4889530d037a3aea72db2a20f881	"""""""there is more than moore in automotive ...."""""""	verification;automotive engineering;automotive engineering permission technological innovation safety consumer electronics electronic design automation and methodology defense industry manufacturing industries automotive applications business communication;reliability;technological innovation;substrate modeling;design engineering;defense industry;automotive;consumer electronics;manufacturing industries;business communication;business environment;electronic design automation and methodology;environmental regulations;automotive applications;permission;value chain;automotive value chain;esd;manufacturability;safety;design;reliability automotive value chain safety environmental regulations eda industry design;substrate modeling design verification automotive design manufacturability esd emc reliability;reliability automobile industry design engineering;environmental regulation;automobile industry;emc;eda industry	"""The complete """"Automotive value chain"""" is facing significant innovation pressure coming from emerging safety and/or environmental regulations & laws as well as the end customer's demand for more functionality & comfort at not increasing prizes. The presentation will explain the challenging implications of this development for Infineon as an IDM. It will also explain that moving towards leading edge technologies (65nm and beyond) is not our preferred answer to meet """"Military quality/reliability requirements at consumer electronics prices"""". Therefore our requirements towards the EDA industry have to be aligned with Automotive applications and the corresponding business environment."""	requirement	Hartmut Hiller	2007	2007 44th ACM/IEEE Design Automation Conference	10.1145/1278480.1278576	design;verification;value chain;engineering;automotive industry;automotive engineering;reliability;business communication;manufacturing engineering	EDA	-64.91653261830486	13.232862406947135	43670
1af8a2f30ffce126aeffb1976d42f0dacfec08ff	an architecture framework for collective intelligence systems	software;architecture viewpoint;software engineering;computer architecture;software architecture;architecture framework;iso iec ieee 42010 architecture framework collective intelligence systems social networks wikis content sharing platforms cis stigmergic mechanisms organization corporate level software architecture;artificial intelligence;terminology;collective intelligence;organizations;programvaruteknik;organizations terminology conferences computer architecture context software artificial intelligence;stigmergy;stigmergy architecture framework architecture viewpoint collective intelligence coordination software architecture;software technology;context;software architecture iec standards ieee standards iso standards social networking online;conferences;coordination	Collective intelligence systems (CIS), such as wikis, social networks and content sharing platforms, have dramatically improved knowledge creation and sharing at society level. There is a trend to exploit the stigmergic mechanisms of CIS also at organization/corporate level. However, despite the wide adoption of CIS, there is a lack of consolidated systematic knowledge of the architectural principles and practices that underlie CIS. Software architects lack guidance to design CIS for the application context of individual organizations. To address these challenges, we contribute with an architecture framework for CIS, aligned with ISO/IEC/IEEE 42010. The CIS-AF framework provides guidance for architects to describe key CIS elements and systematically model a CIS that is well-suited for an organization's context and goals. The framework is grounded in an in-depth analysis of existing CIS, workshops and interviews with key stakeholders, and experiences from developing a prototypical CIS. We evaluated the architecture framework in two cases in industry setting where CIS have been designed and implemented using the framework. Results show that the framework effectively supports stakeholders with providing a shared vocabulary of CIS concepts, guiding them to systematically apply the stigmergic principles of CIS, and supporting them with kick starting CIS in their organizations.	anisotropic filtering;architecture framework;collective intelligence;context (computing);correspondence rule;iso/iec 42010;information system;privacy;social network;software architect;stigmergy;vocabulary;wiki	Jürgen Musil;Angelika Musil;Danny Weyns;Stefan Biffl	2015	2015 12th Working IEEE/IFIP Conference on Software Architecture	10.1109/WICSA.2015.30	software architecture;computer science;systems engineering;organization;engineering;knowledge management;software engineering;architecture framework;collective intelligence;terminology	Web+IR	-66.35183459499517	18.12264230774796	43676
f4cef3d6804b65f41507a73bca1fd9fb694307ed	digital businesses: creation of a research framework for organizational readiness for enterprise 2.0	data management;enterprise information systems;enterprise 2 0;digital business;big data;social media;business management	Customers are no longer at the receiving end in the new digital economies. They have a say in everything and are co-creating products and services. Their connection with other customers is stronger and the influence they exert collectively on businesses is phenomenal. All this has been made possible by the technologies that the collaborative internet has made possible. Businesses have discarded hierarchies and functional pyramid structures in favor of flat empowered structures to improve decision responsiveness in the new age. Competency is fast replacing compatibility amongst successful employees. Geography is dead and interactions take place across boundaries of distance, time, language and culture. This transformation of the business enterprise to Enterprise 2.0 has become possible due to the use of Web 2.0 tools becoming common place and has had far reaching implications. The question that it raises is that are all organizations equally well equipped to take advantage of these changes or is it going to change the relative power equation amongst them to make some small forward looking technology savvy organizations suddenly more powerful than the erstwhile successful large giants who had built themselves on the strength of their products and markets over time. This paper aims at creating a framework that can help evaluate this emerging equation and assess the state of readiness of all organizations to meet this onslaught of business change. The framework addresses these technologies, the way they are impacting business strategy and spells out all that organizations need to do to be able to gear up to face the changing fabric of the new age enterprise.		Ashok Kumar Wahi;Yajulu Medury	2014	IJVCSN	10.4018/ijvcsn.2014010104	public relations;enterprise systems engineering;big data;enterprise software;social media;data management;computer science;knowledge management;operations management;digital firm;enterprise architecture;management;world wide web;enterprise information system;business architecture;enterprise life cycle	DB	-74.84864071026338	5.413043340847822	43693
800a9f3d9322331009c9aeb4a626ed6c00740fdf	virtual operations in common information spaces: boundary objects and practices	virtual community;information space;boundary object;information system;field study	The paper presents a field study aimed at identifying and analyzing the role of boundary artifacts in cross-organization virtual communities of practice (CoP). Our analysis is informed by a recent case study in vacation package assembly (VPA), which is defined as the distributed collective practice carried out by members of a boundary-spanning virtual alliance inhabiting a ‘common’ information space (CIS). The CIS forms the virtuality through which members of the alliance engage in coordinative actions on boundary artifacts. The CIS implements the facilities required for constructing, negotiating and reconstructing these boundary artifacts so as to assemble personalized regional vacation packages for tourists. The results lead to several conclusions on the design of CIS as computational host of virtual communities of practice.	design rationale;document;field research;file spanning;first-class function;list of system quality attributes;personalization;spaces;virtual artifact;virtual community;virtuality (gaming);vocabulary	Demosthenes Akoumianakis;Giannis Milolidakis;Dimitrios Stefanakis;Anargyros Akrivos;George Vellis;Dimitrios Kotsalis;Anargyros Plemenos;Nikolas Vidakis	2009		10.1007/978-3-642-04568-4_22	simulation;engineering;knowledge management;artificial intelligence;marketing;operations management;management;information system;field research	HCI	-67.91422604950743	9.055796018935176	43725
9da344eef06395facda8b551ca180faa7b9162eb	a development framework for customer experience management applications: principles and case study		Customer experience management (CEM) denotes a set of practices, processes, and tools that aim to personalize a customer's interactions with a company around the customer's needs and desires. This personalization depends on the purchase scenario at hand, and on how much a company knows about its customers. In turn, the purchase scenario depends, among other things, on the complexity of the product or service being offered (e.g., a carton of milk versus a house), and the complex set of motivations that can trigger a purchasing process. E-commerce software tool vendors need to provide the building blocks that enable retailers to configure and develop CEM functionalities that take into account these factors. In earlier work, we proposed such building blocks within the context of a CEM development framework that relies on a cognitive modeling of the purchasing process and identifies the touch points between seller and buyer and relevant influence factors. We envision a CEM scenario specification tool that enables business analysts to specify their purchase scenario, from which we generate data structures and algorithms to implement CEM functionalities by instantiating the framework. The framework is embodied in a set of ontologies and algorithm templates that can be instantiated with the specification parameters. In this paper, we present the principles behind our approach, and a prototype CEM scenario specification tool. We illustrate the tool with a moderately complex purchasing scenario, to validate the underlying theory, and to explore implementation strategies	algorithm;cognitive model;complexity;customer relationship management;data structure;e-commerce payment system;embodied cognition;interaction;knowledge management;ontology (information science);personalization;programming tool;prototype;purchasing	Imen Benzarti;Hafedh Mili	2017	2017 IEEE 14th International Conference on e-Business Engineering (ICEBE)	10.1109/ICEBE.2017.27	knowledge management;consumer behaviour;personalization;voice of the customer;purchasing process;computer science;management science;software;ontology (information science);purchasing;data structure	SE	-80.80675897040778	14.082278540000747	43783
791203f745d58767e2cd2eaefeb871ffb63d402d	engineers will tolerate a lot of abuse	employment;dp industry;dp industry personnel human resource management employment;personnel;acoustical engineering engineering management personnel companies security costs contracts programming profession pain system testing;employee turnover programmers jobs organization training costs;human resource management	Every year, one in five programmers changes jobs. The waste of time and money is extraordinary. If you could prevent just one programmer from leaving, you would save your organization $50,000 to $100,000 in replacement and training costs alone. Turnover is expensive now and the costs will likely increase. To address the turnover problem, we must understand it. The author considers the principles of employee turnover.		Watts S. Humphrey	2001	IEEE Software	10.1109/52.951487	engineering;knowledge management;operations management;human resource management;management	Embedded	-68.94451935765542	26.197882480814652	43990
aedfd9d7bb69311496f14c104bf09ee89a0ae4fe	towards supporting business process compliance with policies		This paper discusses early findings of the research in progress to create an approach to support an organization in bridging the gap between existing business processes and policies. Business processes are valuable assets of any organization, and business process modelling has become the key activity for capturing and analysing business processes. However, advances in technology, growing expectation of openness by research funders, competition, regulations in IT security and privacy, and overall economic situation facilitate emergence of new policies, and urge enterprises to change their business processes to be compliant with the new requirements. The goal of the research is to propose the approach for closing the gap between business process models and legal states of business objects described in policies by means of using Bunge-Wand-Weber model. The approach includes means for explicit definition of legal and illegal state spaces of business objects in (1) policies, and (2) as-is business process models, and compliance checking between state spaces of (1) and (2) to indicate the gap. It is an initial input for building to-be business process models that are complaint with newly imposed policies. As a running example to illustrate the approach a publishing business process of a scholar journal is used. New policies from research funders require Open Access (OA) to all outputs from publicly-funded research, and business processes of publishing scholar journals require changes.	business process	Ludmila Penicina	2017		10.1007/978-3-319-64930-6_7	business process;complaint;bridging (networking);process management;business object;business process modeling;business process model and notation;business;publishing	HCI	-73.60021526990884	10.936741823965862	43996
0c25b9c8b1821e0afd0c9277136b4f11ec9054d7	[journal first] what makes a great manager of software engineers?		Having great managers is as critical to success as having a good team or organization. A great manager is seen as fuelling the team they manage, enabling it to use its full potential. Though software engineering research studies factors that may affect the performance and productivity of software engineers and teams (like tools and skill), it has overlooked the software engineering manager. On the one hand, experts are questioning how the abundant work in management applies to software engineering. On the other hand, practitioners are looking to researchers for evidence-based guidance on how to manage software teams. We conducted a mixed methods empirical study to investigate what manager attributes developers and engineering managers perceive important and why. We present a conceptual framework of manager attributes, and find that technical skills are not the sign of greatness for an engineering manager. Through statistical analysis we identify how engineers and managers relate in their views, and how software engineering differs from other knowledge work groups.		Eirini Kalliamvakou;Christian Bird;Thomas Zimmermann;Andrew Begel;Robert DeLine;Daniel M. Germán	2018	2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)	10.1145/3180155.3182525	systems engineering;empirical research;conceptual framework;knowledge engineering;software;computer science;greatness;working group	SE	-70.18161833388362	22.898944923492806	44001
ba3d605a080e906233b38cec6e879ac44fee4ac9	value-based software engineering: reinventing	software management;software engineering;business case;statistical analysis;value based software engineering;software life cycle;survey methods;earned value management;monitoring and control	"""The Value-Based Software Engineering (VBSE) agenda described in the preceding article has the objectives of integrating value considerations into current and emerging software engineering principles and practices, and of developing an overall framework in which they compatibly reinforce each other. In this paper, we provide a case study illustrating some of the key VBSE practices, and focusing on a particular anomaly in the monitoring and control area: the """"Earned Value Management System."""" This is a most useful technique for monitoring and controlling the cost, schedule, and progress of a complex project. But it has absolutely nothing to say about the stakeholder value of the system being developed. The paper introduces an example order-processing software project, and shows how the use of Benefits Realization Analysis, stake-holder value proposition elicitation and reconciliation, and business case analysis provides a framework for stakeholder-earned-value monitoring and control."""	anomaly detection;management system;software engineering;software project management	Barry W. Boehm	2003	ACM SIGSOFT Software Engineering Notes	10.1145/638750.638775	personal software process;verification and validation;software engineering process group;software sizing;earned value management;software configuration management;software project management;systems engineering;engineering;software design;social software engineering;software development;software design description;software engineering;business case;software construction;management science;software walkthrough;software analytics;management;software deployment;survey methodology;software development process;software requirements;software quality;software system;software peer review	SE	-68.31865070074507	21.887678782040584	44004
ba6c45a602d5c6ade7e1735a7fad9f8026ed1963	a dynamic decision approach for long-term vendor selection based on ahp and bsc	gestion integrada;modelizacion;gestion integree;processus innovation;evaluation performance;gestion entreprise;selection problem;analytic hierarchy process;problema seleccion;regime dynamique;performance evaluation;processus hierarchie analytique;fournisseur;systeme aide decision;evaluacion prestacion;processus metier;innovation process;firm management;integrated management;supplier;intelligence artificielle;sistema ayuda decision;long terme;dynamic conditions;proceso innovacion;balanced scorecard;long term;resolucion problema;modelisation;business environment;decision support system;largo plazo;international business;proceso jerarquia analitico;regimen dinamico;artificial intelligence;proceso oficio;administracion empresa;inteligencia artificial;modeling;business process;problem solving;resolution probleme;proveedor;supplier selection;probleme selection	For solving the dynamic condition problem of vendor selection, the analytic hierarchy process method is modified to a dynamic approach in the period of analytic cycle. The balanced scorecard is used to define the 4 major frameworks of supplier selection including customers, financial, internal business processes, and innovation and learning. The 16 attributes are extended from major frameworks. The main character of proposed method is the scores of attributes and alternatives from the estimation of commander’s trade-off can be changed in time axis under the changeable and conjecturable business environments. In case study, the advantage and limit of the model are illustrated.	analytical hierarchy;apache axis;binary symmetric channel;business process	Ziping Chiang	2005		10.1007/11538356_27	innovation;international business;analytic hierarchy process;systems modeling;decision support system;computer science;balanced scorecard;business process;operations research	ML	-68.9436760844761	7.408385423821715	44068
672477f2f0f3f0d577f8a1fafbce5a6144d67983	a consensus of thought in applying change management to information system environments	change management;internal audit;patch management;cobit;configuration management	Failure to keep pace with rapid developments in information technology can subject an organization to inefficiencies in obtaining reliable information that is imperative in making important decisions. The concept of change management attempts to move organizations in an efficient manner toward a desirable future state. In the realm of information processing, there is a continuing development of thought as to an appropriate framework to cover a vast array of issues from patching a specific software application to changing an overall system to better meet the strategic issues of an organization's environment. This article considers related guidance provided in a Global Technology Audit Guide GTAG from The Institute of Internal Auditors, COBIT from the ISACA, and current change and patch management literature in order to demonstrate that there is a common stream of thought in the evolution of a framework for applying change management to information systems.	information system	Jeffrey S. Zanzig;Guillermo A. Francia;Xavier P. Francia	2015	IJISMD	10.4018/IJISMD.2015100102	change control;information technology audit;systems engineering;engineering;knowledge management;marketing;internal audit;change management;database;management science;risk management information systems;configuration management;management	DB	-70.87310775791515	16.602067963649056	44069
d782bec24144eb2d685c65d5194bbc644580bd67	elicitation of information needs in precontract requirements engineering		The precontract phase of a software project is of high importance for the employer as well as the contractor, because this phase yields the basis for a contract between the parties. The main interest of the contractor is to provide an offer which addresses the requirements of the employer in a convincing way and proposes a solution for a reasonable price. In order to decide about the acceptance of an offer, decision makers of the employer require certain information to be contained in the offer. In this paper the results of a survey about the information needs of decision makers are presented. It yields first indications about the relevance of certain information and where in a document decision makers would like to find it. The paper includes a description of the method that has been used to acquire the knowledge.	information needs;relevance;requirement;requirements engineering;software project management	Christian Müller;Matthias Koch;Sebastian Adam	2015			management science;requirements elicitation;software;information needs;requirements engineering;business	Web+IR	-79.76334450868941	9.643126105727484	44083
88051b566970c3f380b43b54da6db7024c5bc55d	a modern approach to integrate database queries for searching e-commerce product		E-commerce refers to the utilization of electronic data transmission for enhancing business processes and implementing business strategies. Explicit components of e-commerce include providing after-sales services, promoting services/products to services, processing payment, engaging in transaction processes, identifying customer’s needs, processing payment and creating services/products. In recent times, the use of e-commerce has become too common among the people. However, the growing demand of e-commerce sites have made essential for the databases to support direct querying of the Web page. This re-search aims to explore and evaluate the integration of database queries and their uses in searching of electronic commerce products. It has been analyzed that e-commerce is one of the most outstanding trends, which have been emerged in the commerce world, for the last decades. Therefore, this study was undertaken to ex-amine the benefits of integrating database queries with e-commerce product searches. The findings of this study suggested that database queries are extremely valuable for e-commerce sites as they make product searches simpler and accurate. In this context, the approach of integrating database queries is found to be the most suitable and satisfactory, as it simplifies the searching of e-commerce products.	business process;database;e-commerce payment system;web page;world wide web	Ahmad Tasnim Siddiqui;Mohd Muntjir	2016	CoRR		business process;e-commerce;graph database;data mining;database;payment;database transaction;computer science	DB	-74.0005248510529	5.685693200866381	44122
ae7614b2fae5426e68ea48563f97698bb855cc16	evolution towards teaching a holistic course in model-driven system development: modeling for enterprise architecture with business and system architecture and platform-based development		This paper presents the evolution of teaching a holistic course on Model-driven Software Development based on analyzing and assessing of a set of related courses during the last ten years, taught both in Norway and the USA. The objectives of these courses are to demonstrate and teach the use of modeling tools through a holistic perspective, from business architecture, requirements, system architecture, software architecture, and to executable models that take advantages of platform-based development. An analysis of the courses from previous years resulted in a recommendation for a new setup for the courses in 2018, in turn, a further analysis of the experiences and results from the 2018 courses leads to a plan for improvements for the courses to be taught in future. The result is a complete enterprise architecture modeling approach education from business architecture to software architecture to functioning software.	business architecture;enterprise architecture;executable;holism;model-driven engineering;model-driven integration;requirement;software architecture;software development;systems architecture	Arne-Jørgen Berre;Shihong Huang;Hani Murad;Guénolé Lallement	2018		10.1145/3270112.3270134	software engineering;business architecture;systems engineering;systems architecture;enterprise architecture;software development;executable;software;computer science;software architecture	SE	-63.3067851988086	23.84692971561957	44170
e00ead1ce21ac9eacca5b8760c1756a6b61a0890	a database-centred approach to the development of new mobile service concepts	text mining;m services;conjoint analysis;morphological analysis;portfolio matrix;mobile services;mobile communications;application stores;service concept development	This paper proposes a database-centred approach to the systematic development of new mobile service concepts. Keyword vectors are first constructed by applying text mining to the collected mobile application documents. Alternative mobile service concepts are generated by employing morphological analysis to the keyword vectors. Subsequently, by applying conjoint analysis, a portfolio matrix is constructed to evaluate and manage the newly derived mobile service concepts. The working of the proposed approach is provided with the help of a case study of mobile game service concept development by analysing the Apple App Store. The proposed approach can supplement the shortcomings of customer-centred approaches, and it is expected to help service managers and designers in the actual development of new mobile service concepts.		Chulhyun Kim;Hakyeon Lee	2012	IJMC	10.1504/IJMC.2012.048111	text mining;differentiated service;morphological analysis;computer science;knowledge management;service delivery framework;marketing;service design;data mining;conjoint analysis;mobile business development;world wide web;computer security	Robotics	-71.9831581885572	10.37492564321654	44370
48e4facc67315dbaf2c4d77257d10e0c630e61a2	a framework for evaluating managerial styles in open source projects	quality assurance;organizational structure	This paper presents the Software Project Governance Framework (SPGF) for characterizing management of software projects, based on mechanisms used for communication and collaboration, the organizational structure of projects, and testing and quality assurance procedures. The framework was developed and validated from interviews and surveys with leaders of more than 70 commercial and communitybased software projects, including both closed and open source projects.	categorization;open-source software;software project management	Eugenio Capra;Anthony I. Wasserman	2008			organizational structure;quality assurance;systems engineering;engineering;knowledge management;environmental resource management;management	SE	-68.82909292605977	19.723809028283597	44372
43d5e32be24cee540d1d8c2ce78ea133dedcc41b	a comparative analysis of mis project selection mechanisms	resource allocation;mis project selection mechanism;organizational change;mis project;application system selection;different type;comparative analysis;mis department;fewest user;top management involvement;different selection biasing;different group;top management;steering committees;user department;different mechanism;mis project selection;group selection;cost benefit analysis;organizational commitment;profitability	MIS projects are selected by any of four different groups within organizations: top management, steering committees, user departments, and MIS departments. Because of their inherent differences, each of these groups is likely to favor different types of MIS projects. That is, they exhibit different selection biasing. An investigation of the nature and extent of this biasing is examined in this research.Data were collected from 176 MIS projects selected from 60 organizations. Projects were categorized as being selected by top management, steering committees, user departments, or MIS departments, and specific characteristics (e.g., size, risk, and organizational commitment) were measured for each project.As hypothesized, the research showed that projects selected by different groups did indeed differ significantly with respect to these characteristics.&bull; Projects selected by top management do not tend to be more strategic, profitable, resource consuming, larger risk, or related to organizational well-being than other project selection groups. These projects, however, did tend to experience the longest start delay and elapsed development time.&bull; Projects selected by steering committees tended to be larger and riskier, and required more organizational change. Formal cost-benefit analysis is more predominant, but surprisingly, projects selected are not more cross-functional in scope. AB&bull; User department-selected projects, comparatively, are smaller, more quickly developed, and involve the fewest users, layers of management, and business functions.&bull; MIS-selected projects have more of an integration focus and follow more logical sequences in development. Their projects experience fewer delays in deliberation and duration, and less concern is given to cost-benefit analysis.The individual biasing attributable to each of the four selection mechanisms is described. The paper concludes by presenting the implications of having each of these groups select MIS projects. Using this information, organizations can establish or assess the effect of using different mechanisms for selecting MIS projects.	biasing;categorization;management information system;organizational behavior	James D. McKeen;Tor Guimaraes;James C. Wetherbe	1994	DATA BASE	10.1145/190675.190677	qualitative comparative analysis;group selection;organizational commitment;resource allocation;engineering;knowledge management;cost–benefit analysis;marketing;operations management;management;world wide web;profitability index	SE	-82.94232630301427	4.867746272027314	44391
c69fc554c23f74561332bdd0a8961c59e1a6588f	test-case prioritization: achievements and challenges	test case prioritization achievements challenges;dan hao lu zhang hong mei 测试用例 优先级 优化算法 test case prioritization achievements and challenges	Test-case prioritization, proposed at the end of last century, aims to schedule the execution order of test cases so as to improve test effectiveness. In the past years, test-case prioritization has gained much attention, and has significant achievements in five aspects: prioritization algorithms, coverage criteria, measurement, practical concerns involved, and application scenarios. In this article, we will first review the achievements of test-case prioritization from these five aspects and then give our perspectives on its challenges.	algorithm;test case;turing test	Dan Hao;Lu Zhang;Hong Mei	2016	Frontiers of Computer Science	10.1007/s11704-016-6112-3	computer science;management science;operations research	SE	-66.221359194706	24.580235518211012	44540
d44c8e915b02cad508200d54079a3a12e21fd09a	a study of student experience metrics for software development pbl	software;information systems;time measurement;software time measurement visualization java educational institutions;metrics;software fault tolerance;visualization;computer science education;online storage student experience metrics software development pbl software failure software defect information system social problem high quality software ict information and communication technology human resource software engineer project based learning educational technique student knowledge student skill product quality development process quality student evaluation metrics loc lines of code;pbl;process improvement;software quality computer science education information systems software development management software fault tolerance;software quality;visualization metrics pbl process improvement;software development management;java	In recent years, the increased failure originated in the software defects, in various information systems causes a serious social problem. In order to build a high-quality software, cultivation of ICT (Information and Communication Technology) human resources like a software engineer is required. A software development PBL (Project-based Learning) is the educational technique which lets students acquire knowledge and skill spontaneously through practical software development. In PBL, on the other hand, it is difficult to evaluate not only the quality of the product but also the quality of the development process in the project. In this paper, we propose the student evaluation metrics to assess the development process in PBL. The student evaluation metrics represent LOC (Lines of Code) and development time for each product developed by a student. By using online storage, these metrics can be measured and visualized automatically. We conducted an experiment to evaluate the accuracy of the metrics about development time. As a result, we confirmed that development time metrics can be measured with approximately 20% of error.	altered level of consciousness;automated planning and scheduling;information system;newton's method;software bug;software development;software engineer;source lines of code	Umekawa Kohichi;Hiroshi Igaki;Yoshiki Higo;Shinji Kusumoto	2012	2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing	10.1109/SNPD.2012.76	personal software process;verification and validation;visualization;computer science;package development process;software development;software engineering;programming language;java;metrics;software development process;information system;software quality;software fault tolerance;time	SE	-65.86776853896102	26.826381580547892	44683
596e3c1896f1c7fd3eaf96a690f9876a40e870e3	tutorial: tools and methodologies for executing successful simulation consulting projects	digital simulation;project management;basic project tips;project endeavor;simulation consulting projects;simulation leaders;simulation project solution	When problems are extremely complex, highly variable and too big for simple calculations, a simulation project solution should be considered. Not surprisingly, the resulting project endeavor will also be complex and should be managed with a clear strategy and attention to detail. This paper will extend beyond basic project tips by providing specific tools and methodologies to help simulation leaders execute successful simulation consulting projects inside or outside their organization.	simulation	Carley Jurishica;Nancy Zupick	2012	Proceedings Title: Proceedings of the 2012 Winter Simulation Conference (WSC)		project management;extreme project management;simulation;software project management;computer science;systems engineering;engineering;estimation;management science;project management triangle;project planning;project portfolio management	HPC	-67.70731395176887	18.182895844249128	44749
4c6c714fdbf6181af04186c2f248c4ce7d211adf	an integrated framework for echain bank accounting systems	information systems;difference operator;accounting;web service;commercial banks;web intelligence;worldwide web;financial institutions;value chain;accounting information system;transaction processing;legacy system;virtual worlds;design methodology;data security	Purpose – The eChain type of bank accounting information system (eCBAS) is proposed to facilitate standard business document electronic exchanges between banks, central factories, and their satellite vendors in the virtual world. This framework integrates various software applications, running on a variety of platforms and/or frameworks facilitating the electronic exchange of standard business documents.Design/methodology/approach – Instead of scrapping legacy systems, this framework takes advantage of web services, XBRL, web intelligent, pre‐warning systems and security technologies to improve the quality and accuracy of accounting information, supporting continuous monitoring and auditing.Findings – Through eCBAS, financial institutes can now closely monitor the cash or production flows of their satellite vendors. Commercial banks can shorten the loan application periods and also offer quick loans with low‐cost capital to domestic small and medium satellite vendors at different operating stages.Original...		Fengyi Lin;Olivia R. Liu Sheng;Soushan Wu	2005	Industrial Management and Data Systems	10.1108/02635570510590129	web service;economics;throughput accounting;design methods;transaction processing;value chain;computer science;engineering;marketing;data security;accounting information system;web intelligence;management;world wide web;legacy system;information system;commerce	EDA	-70.41452699114613	4.987764039360552	44775
07d16ed2448969fb95918d0c0f330079f687bda0	modified agile practices for outsourced software projects	distributed development;agile development;customer satisfaction;self organization;process improvement	"""Frustration with the bureaucratic nature of the disciplined approach has led to the call for agile development. The new approach is defined by the Agile Manifesto (http://agilemanifesto.org/), which values individuals and interactions over processes and tools, working software over comprehensive documentation, customer collaboration over contract negotiation, and agility in responding to change over following a prescribed plan. Agile development does not focus on process improvement; instead it focuses on customer satisfaction and employee empowerment. This is evident from reading the stated values and principles of the Agile Manifesto, which include fairly extreme positions such as """"welcome changing requirements, even late in development"""" and """"the best architectures, requirements, and designs emerge from self-organizing teams.""""  An interesting issue arising from the call for agile development is its role in distributed development, which usually translates to offshore development. A recent study indicates that agile practices can reduce temporal, geographical, and socio-cultural distances in distributed development projects. The study researched agile development between teams located in the U.S. and Ireland, and while it reported that overall communication was improved, it also noted problems related to geographical, temporal, and even language distances. Although there are other reported successes of distributed agile development, the projects are generally small, the team members are likely familiar with each other, and the participants are largely experts or high caliber developers.  This raises a research, as well as a practical, question: can we extend the use of agile practices from small projects to medium and large projects that involve a significant outsourcing component? To address this question, we must drop constraints such as small size projects, and expert developers belonging to the same company, and examine problems arising from geographical, temporal, and cultural distances. Accordingly, agile practices may need to be modified.  In this article, the key issues of software projects with an outsourced component are first identified. These issues are then used as a background to evaluate how standard agile practices stand up when applied to larger projects. This evaluation is followed by recommendations for modified agile practices for outsourced software projects."""	agile software development;documentation;interaction;organizing (structure);outsourcing;requirement;self-organization	Dinesh Batra	2006	Commun. ACM	10.1145/1562164.1562200	self-organization;agile unified process;extreme programming practices;agile usability engineering;computer science;knowledge management;requirement;agile software development;customer satisfaction;empirical process;management;lean software development	SE	-69.37007033104007	22.0346554362527	44786
27a3be7c6bad4d23d6ed2feac71f9454ce824431	an ontology-based approach for selecting performance indicators for partners suggestion	collaborative work;performance indicator;virtual organization	In the current fast-paced world, organizations do not have time to postpone ideas due to the lack of suitable technical and scientific supports that can help them in a rapid connection establishment in order to develop collaborative works. This paper presents a methodology that aids the user to find the appropriated performance indicators that may be used to compare and after to suggest a suitable set of organizations to fulfill a specific Collaboration Opportunity. In this work is made an assumption that the Virtual Organization Breeding Environment has a common set of performance indicators that are known and agreed among the involved organizations.	artificial intelligence;cooperative breeding;virtual organization (grid computing)	Fabiano Baldo;Ricardo J. Rabelo;Rolando Vargas Vallejos	2007		10.1007/978-0-387-73798-0_19	engineering;knowledge management;operations management;management science	HCI	-70.02825466664962	10.691248353845141	44834
30890e9f99a8535c533b0231d00dbec851e7b32f	studying the advancement in debugging practice of professional software developers	dp management;computer operating procedures;computer science;program debugging;software engineering;automatic fault localization methods;back-in-time debuggers;computer science;debugging practice;debugging technologies;large-scale online debugging;professional software developers;debugging;empirical study;program comprehension;survey	In 1997, Henry Lieberman stated that debugging is the dirty little secret of computer science. Since then, several promising debugging technologies have been developed such as back-in-time debuggers and automatic fault localization methods. However, the last study about the state-of-the-art in debugging is still more than 15 years old and so it is not clear whether these new approaches have been applied in practice or not. For that reason, we investigate the current state of debugging in a comprehensive study. First, we review the available literature and learn about current approaches and study results. Second, we observe several professional developers while debugging and interview them about their experiences. Third, we create a questionnaire that serves as the basis for a larger online debugging survey. Based on these results, we present new insights into debugging practice that help to suggest new directions for future research.	computer science;debugger;debugging;experience;software developer	Michael Perscheid;Benjamin Siegmund;Marcel Taeumel;Robert Hirschfeld	2015	Software Quality Journal	10.1007/s11219-015-9294-2	shotgun debugging;computer science;systems engineering;engineering;software engineering;algorithmic program debugging;computer engineering	SE	-66.51131029464008	27.422073590584816	45035
3aaf8bec888e816c10a97fab1613f4962ea9a115	how to measure the performance of a collaborative research center	research performance;fixed effects panel data model;network;collaborative research center;62-07;62-09;62p20;c23;c13;m19	New Public Management helps universities and research institutions to perform in a highly competitive research environment. Evaluating publicly financed research improves transparency, helps in reflection and self-assessment, and provides information for strategic decision making. In this paper we provide empirical evidence using data from a Collaborative Research Center (CRC) on financial inputs and research output from 2005 to 2016. After selecting performance indicators suitable for a CRC, we describe main properties of the data using visualization techniques. To study the relationship between the dimensions of research performance, we use a time fixed effects panel data model and fixed effects Poisson model. With the help of year dummy variables, we show how the pattern of research productivity changes over time after controlling for staff and travel costs. The joint depiction of the time fixed effects and the research project’s life cycle allows a better understanding of the development of the number of discussion papers over time.	abstract summary;academia (organization);coefficient;colorectal carcinoma;cyclic redundancy check;data model;decision making;dietary iron;dimensions;dummy variable (statistics);enterprise life cycle;estimated;financial cost;fixed effects model;graph drawing;health services research;imagery;major depressive disorder;mathematical model;mathematics;money;mosaic - computer software;ncsa mosaic;network theory;object lifetime;occur (action);panel data;paper;polytetrafluoroethylene;scientific publication;self-assessment;semantic analysis (compilers);tag cloud;universities;research center	Alona Zharova;Janine Tellinger-Rice;Wolfgang K. Härdle	2018	Scientometrics	10.1007/s11192-018-2910-8	data mining;performance indicator;new public management;poisson regression;transparency (graphic);dummy variable;empirical evidence;panel data;computer science;research center	HCI	-90.33010995044901	4.301455775392316	45089
bf00a3730989ab7399f885dc75bb965d51cff48f	shareholder litigation and ownership structure: evidence from a natural experiment		We use a natural experiment to identify a causal effect of the threat of shareholder litigation on ownership structure, governance, and firm performance. We find that when it becomes harder for small shareholders to litigate, ownership becomes more concentrated and shifts from individuals to institutions. Director and officer governance protections drop among these firms, and operating performance drops among firms whose ownership structure does not change. These results suggest that the ability of shareholders to coordinate and litigate against management is important for governance.		Alan D. Crane;Andrew Koch	2018	Management Science	10.1287/mnsc.2016.2561	corporate governance;accounting;economics;finance	ML	-84.97199394658321	5.5931479709622565	45095
21e0b780c8adddaa610914115e71dc4b2ac9996a	developing supply chain dynamic capability to realize the value of inter-organizational systems	inter organizational systems;inter organisational systems;personal computers;e business;dynamic capability;supply chain integration;personal computer industry;ios;taiwan;scm;dynamic capabilities;moderating effect;supply chain;electronic business;supply chain management;supply chain cooperation	In face of increasingly complex supply chain, firms are taking steps to develop different kinds of interorganizational systems (IOS) to facilitate information sharing and sustain competitive competency. These systems are expected to provide great business value, however many of them do not fulfill the expected promise as these systems are relatively more complicated and the usage is across supply chains. Built upon process theory and the view of dynamic capability, this study defines two types of supply chain dynamic capability (SDC) -supply chain integration capability and supply chain cooperation capability and proposes that SDC plays a significant role in determining IOS performance. A general survey is conducted in Taiwan PC industry to validate the research model. A linear regression is used to testify the hypotheses. The results show that improving SDC can create greater IOS performance. Furthermore, supply chain integration capability has stronger moderating effect of IOS performance than supply chain cooperation capability. These findings contribute to the literature by confirming the influence of SDC on IOS performance and also by showing which SDC is of primary importance to firms.	organizational behavior;secure digital container;smart data compression	Hsin-Lu Chang;Chien-Hui Chen;Cheng-Hsuan Su	2008	International Journal of Internet and Enterprise Management	10.1504/IJIEM.2011.039913	supply chain management;economics;service management;marketing;operations management;electronic business;supply chain;commerce	Metrics	-82.15943867980427	4.3027300271346425	45101
60667ebdcd9862d06a1afa1606cf9d94a45ee3f2	guest editors' introduction: algorithms and today's practitioner	complexity algorithm software efficiency;software;special issues and sections software algorithms internet software engineering data structures;programming environment;special issues and sections;efficiency;complexity;software engineering;algorithm;internet;data structures;software algorithms;industrial application	Beautiful Code It wasn’t always this way. In the early years of computer science, with few hardware and software resources available, an adequate algorithm for performing a task as elemental as searching or sorting could spell the difference between success and failure. Every programmer had, as part of his or her repertoire of skills, a hip-pocket collection of well-known algorithms for dispatching a variety of tasks. But enormous advances in computing power and programming environments have obscured the importance of algorithms, one of the foundational pillars of our discipline. Today, even university curricula too often pay only lip service to the teaching of algorithmic fundamentals, reinforcing the popular belief that their place at the core of a software engineer’s education is past. Algorithms came back into the headlines recently with the announcement of a possible proof of the so-called P = NP question that has stymied researchers for decades (www.nytimes. com /2010/08/17/sc ience /17proof. Algorithms and Today’s Practitioner	algorithm;computer science;elemental;p versus np problem;programmer;software engineer;sorting	Giuseppe Prencipe;Cesare Zavattari;Alessandro Tommasi;John M. Favaro	2012	IEEE Software	10.1109/MS.2012.9	personal software process;computing;complexity;the internet;software engineering process group;data structure;search-based software engineering;computer science;engineering;software design;social software engineering;theoretical computer science;software framework;component-based software engineering;software development;software design description;software engineering;software construction;efficiency;software walkthrough;resource-oriented architecture;management;software deployment;software requirements;software system;software peer review	Theory	-68.12208138315079	27.406999044605598	45248
78303ab34b35a79f2c2e96006e91b4533ff9f435	the effects of just-in-time systems on financial accounting metrics	regression anaylsis;financial performance;indirect cost;financial ratios;just in time;manufacturing strategy;profitability	The literature indicates that JIT has been successful as an inventory reduction tool. JIT systems do not, however, automatically increase profit, because the benefits from JIT adoption may be offset by the associated direct and indirect costs of implementation such as training, capital expenditures for reengineering, increased shipping costs, and the mechanics of the absorption costing process. Explores the trends of financial performance as indicated by accounting metrics and the magnitude of change in that performance. Seven commonly used financial ratios were studied covering a period from 1990 through 1999 for companies identified as having adopted the JIT philosophy. It is concluded that JIT effects positive trends in the shorter‐term financial measures. It is not clear how deeply entrenched in the US manufacturing strategy, as a whole, JIT practices will need to be deployed before shareholders will feel a significant effect.		David T. Boyd;Larry Kronk;Russell Skinner	2002	Industrial Management and Data Systems	10.1108/02635570210421345	economics;marketing;operations management;indirect costs;management;profitability index;financial ratio	Embedded	-83.80164047152043	5.641367944299429	45287
00b4a8d67444b25745ce2688ab7244b222445606	mps.br: a tale of software process improvement and performance results in the brazilian software industry	software;return on investment;software process improvement;standards organizations;continuous improvement software performance brazilian software industry mps br program software development software process improvement imps;iso standards;dp industry;biological system modeling;return on investment software process improvement performance results;performance results;software process improvement continuous improvement dp industry software houses;software performance;brazilian software industry;software houses;iec standards;continuous improvement;software development;software industry;organizations;productivity;mps br program;organizations software standards organizations productivity biological system modeling iso standards iec standards;imps;software process	In December 2003 a Brazilian nationwide program was created aiming at improving software processes. The MPS.BR Program is responsible for a great evolution in the software development scenario in Brazil by providing the means to support software process improvement initiatives based on the MPS Model. In 2008 a project named iMPS was launched to enable the analysis of the Performance Results of organizations that adopted the MPS Model. This paper describes some of the findings of the studies that have already been conducted. Preliminary evidences show improvement trends regarding cost, quality, schedule, and productivity, especially for those organizations that adopted the MPS Model for continuous improvement purposes.	software development process;software industry	Gleison Santos;Marcos Kalinowski;Ana Regina Cavalcanti da Rocha;Guilherme Horta Travassos;Kival Chaves Weber;José Antonio Antonioni	2010	2010 Seventh International Conference on the Quality of Information and Communications Technology	10.1109/QUATIC.2010.75	return on investment;productivity;software performance testing;systems engineering;organization;engineering;operations management;software development;software engineering;management;software development process	SE	-70.24223419124068	18.303850363736526	45301
871f9635b7798d177d38ec9221f1ff6f92ec984a	value-oriented it project portfolio management		In the present enterprise computing environment, it becomes more and more important to be able to demonstrate financial gains of IT (Information Technology) initiatives compared to their cost. A valuation methodology is a logical, repeatable fi-amework for making IT investment decisions and monitoring projects to ensure that they ultimately contribute to the financial health and growth of the enterprise. Traditionally, organizations have used a bottom-up approach for technology valuation by using metrics such as return on investment and net present value. While it provides a simple, easy-to-understand valuation mechanism, the traditional approach is limited in the type of analyses. To provide more flexible and versatile valuation of technology, we propose a value-oriented project portfolio management with which organizations can view IT staff and initiatives not only as costs but also as assets managed in a similar way as a fund manager would apply to any other investment. The proposed approach is comprised of multiple steps that collectively compute and help maximizing value of IT initiatives for business transformation. It integrates a value model with a project portfolio to factor in financial values in selecting projects and optimizing the portfolio.	enterprise software;top-down and bottom-up design;value (ethics)	Rongzeng Cao;Wei Ding;Chunhua Tian;Juhnyoung Lee	2006		10.1007/0-387-34456-X_22	project management;extreme project management;earned value management;opm3;project risk management;project management triangle;it portfolio management;application portfolio management;schedule;project planning;project portfolio management	AI	-75.74216625169291	10.521691673622264	45377
ddd8230822f0410417a18f10e16eaeac3eb7b851	contextual intelligence for unified data governance		Current data governance techniques are very labor-intensive, as teams of data stewards typically rely on best practices to transform business policies into governance rules. As data plays an increasingly key role in todayu0027s data-driven enterprises, current approaches do not scale to the complexity and variety present in the data ecosystem of an enterprise as an increasing number of data requirements, use cases, applications, tools and systems come into play. We believe techniques from artificial intelligence and machine learning have potential to improve discoverability, quality and compliance in data governance. In this paper, we propose a framework for u0027contextual intelligenceu0027, where we argue for (1) collecting and integrating contextual metadata from variety of sources to establish a trusted unified repository of contextual data use across users and applications, and (2) applying machine learning and artificial intelligence techniques over this rich contextual metadata to improve discoverability, quality and compliance in governance practices. We propose an architecture that unifies governance across several systems, with a graph serving as a core repository of contextual metadata, accurately representing data usage across the enterprise and facilitating machine learning, We demonstrate how our approach can enable ML-based recommendations in support of governance best practices.	artificial intelligence;best practice;data governance;data steward;discoverability;ecosystem;machine learning;requirement	Ed Seabolt;Eser Kandogan;Mary Roth	2018		10.1145/3211954.3211955	architecture;data mining;computer science;metadata;contextual design;knowledge management;use case;discoverability;data governance;analytics;corporate governance	Web+IR	-73.06495627381877	12.243599082241115	45415
1c31dd6b9d3a9debe8b44b34df85d3cc772ab728	informed opportunism as strategy: supporting coordination in distributed collaborative writing		There is little understanding of how distributed writing groups manage their collaboration and what kinds of support are most useful. The paper presents three case studies of distributed collaborative writing groups in academia. The process evolves over time, constantly adapting to changing circumstances. Co-authors offer and make use of a range of information. Their subsequent opportunistic use of this information to make appropriate ad hoc decisions in new circumstances, appears to be essential to achieve flexibility and coordination. We call this informed opportunism. We identify design implications for support tools for distributed collaborative writing.	autonomous robot;awareness;hoc (programming language);pointer (computer programming)	Eevi Beck;Victoria Bellotti	1993			computer science;knowledge management;distributed computing;management;world wide web	HCI	-77.14065248175359	20.896048974071793	45448
6025019ea1b4d4b771fe96250525972adba7cbea	taking a flexible approach to asps	software as a service;small business;application service provider;cloud computing	"""Introduction  In 2001, subscription-based application service providers (ASPs) represented the new paradigm for application deployment. It was anticipated that ASP spending would reach $7.8 billion by 2004, and a 2001 survey conducted by PMP Research revealed that 23% of respondents said they would likely use an ASP in the future. However, this turned out not to be the case. By 2004, spending on ASPs had increased only to $4.2 billion.  Over the past few years, there has been a revival of the ASP model through the notion of cloud computing and """"software as a service."""" The market for cloud-based services was $16 billion in 2008, and projections estimate the market for spending in this area will reach $42 billion by 2012. However, this still only would represent 9% of overall IT spending. These services have traditionally targeted larger companies instead of small or medium-sized businesses (or SMEs). Companies like Oracle, Siebel, and Ariba claim that nearly 60% of their business comes from companies that have annual revenues exceeding $1 billion. However, the ASP model provides the same or greater benefits to SMEs, including lower costs, greater choice, simpler installation (and no related fees), and the ability to access applications from any internet-connected computer. ASP subscribers also often receive """"24 by 7"""" technical support. This access to IT expertise (without maintaining an in-house IT staff) results in further savings for businesses.  Despite the benefits, small businesses have not readily adopted ASPs. The reasons for this lack of adoption include the reluctance of SMEs to replace their existing systems with untried ASPs and the inappropriateness of the """"one-size-fits-all"""" approach that doesn't take in to consideration specific industry or firm requirements. This makes the SME a significant untapped market for companies operating in this space.  The purpose of this article is to more fully understand the SME market for ASPs through an analysis of the factors that are most important to likely adopters. While previous work has broadly investigated adoption by SMEs and the general adoption of ASPs, this study combines both perspectives and proposes the new construct of """"flexibility"""" as influential in the adoption decision. We surveyed 101 SMEs that had not yet adopted ASPs and asked them to rate the importance of several factors that would affect their decision. Correlating likelihood of adoption with the importance of those factors yields several important insights into how ASP vendors should position their product offerings."""	cloud computing;fits;internet;oracle database;programming paradigm;requirement;software as a service;software deployment;technical support	Farheen Altaf;David Schuff	2010	Commun. ACM	10.1145/1646353.1646389	cloud computing;application service provider;computer science;software as a service;management	HCI	-74.18285734897158	6.046093006752072	45458
f7ac15220926c80e263cdaf431e344d7ceff46bf	is alignment improved with co-evolutionary principles: an open source approach	software;theoretical model;information systems;data gathering;co evolutionary principles;is alignment;public domain software;public domain software information systems open systems;adaptation model;open source system;comprehensive theoretical model;open source system is alignment co evolutionary principles information systems comprehensive theoretical model;modular design;organizations;information system;open systems;computer bugs;evolutionary theory;context modeling;computer bugs logic context modeling shape life testing system testing information systems strategic planning;context;open source	Despite extensive research, results on the sources, components and mechanisms to create or sustain information systems (IS) alignment are still lacking. The quest for an IS alignment remains a critical unsolved problem. In an attempt to address this situation, we present and test a comprehensive theoretical model in an OSS context. We explain how IS alignment at the individual level between the Open source system (OSS) project performance and open source developer's activity level is improved. We integrate the co-evolutionary theory (adaptive tension, change rate and modular design) to understand how IS alignment is improved. This study is based on data gathered from over 750 open source projects developing enterprise applications on sourceforge.net over one year. The results support all the hypotheses and we conclude that use of the co-evolutionary theory will better help IS alignment in an OSS context.		Nassim Belbaly;Robert Frank	2010		10.1109/HICSS.2010.243	simulation;computer science;knowledge management;operating system;software engineering;database;world wide web;strategic alignment;information system	SE	-68.82399793433041	20.336494118193293	45470
45c54d41bfb85dd569bc2e199b9b1193228f09bc	metrics for architectural synthesis and evaluation -- requirements and compilation by viewpoint. an industrial experience report		During architectural analysis and synthesis, architectural metrics are established tacitly or explicitly. In architectural evaluation, these metrics are then consulted to assess whether architectures are fit for purpose and in line with recommended practices and published architectural knowledge. This experience report presents a personal retrospective of the author's use of architectural metrics during 20 years in IT architect roles in professional services as well as research and development. This reflection drives the identification of use cases, critical success factors and elements of risk for architectural metrics management. An initial catalog of architectural metrics is compiled next, which is organized by viewpoints and domains. The report concludes with a discussion of practical impact of architectural metrics and potential research topics in this area.	architectural pattern;compiler;requirement;viewpoint	Olaf Zimmermann	2015	2015 IEEE/ACM 2nd International Workshop on Software Architecture and Metrics			SE	-66.22097902049025	18.796213494034557	45700
c47a1dd89f2ec7c8ee8247965dec78df6143a6b3	dynamic communities in evolving customer networks: an analysis using landmark and sliding windows		The widespread availability of Customer Relationship Management applications in modern organizations, allows companies to collect and store vast amounts of high-detailed customer-related data. Making sense of these data using appropriate methods can yield insights into customers’ behaviour and preferences. The extracted knowledge can then be explored for marketing purposes. Social Network Analysis techniques can play a key role in business analytics. By modelling the implicit relationships among customers as a social network, it is possible to understand how patterns in these relationships translate into competitive advantages for the company. Additionally, the incorporation of the temporal dimension in such analysis can help detect market trends and changes in customers’ preferences. In this paper, we introduce a methodology to examine the dynamics of customer communities, which relies on two different time window models: a landmark and a sliding window. Landmark windows keep all the historical data and treat all nodes and links equally, even if they only appear at the early stages of the network life. Such approach is appropriate for the long-term analysis of networks, but may fail to provide a realistic picture of the current evolution. On the other hand, sliding windows focus on the most recent past thus allowing to capture current events. The application of the proposed methodology on a real-world customer network suggests that both window models provide complementary information. Nevertheless, the sliding window model is able to capture better the recent changes of the network.	business analytics;catastrophic interference;customer relationship management;experiment;microsoft windows;purchasing;sensor;social network analysis;wiki;window manager	Márcia D. B. Oliveira;Américo Guerreiro;João Gama	2014	Social Network Analysis and Mining	10.1007/s13278-014-0208-2	simulation;engineering;operations management;data mining	ML	-75.67175952249622	11.800747814872096	45707
3dad50d940f66e23394d88a9126305393527b8d5	measuring knowledge management capabilities	knowledge management	As business professionals know, creating awareness of a problem and its impact is a critical first step toward the resolution of the problem. That which does not get measured, does not get managed (Redman, 1998). In fact, measurement is a precursor to improvement. This is true for knowledge management (KM) capabilities of an organization. “In today’s knowledge-based economy,” Alan Greenspan recently said, “70% of organizational assets are knowledge assets.” Knowledge assets are intangible capabilities, and there is a recognized need to “make a greater effort to quantify the value of such intangible assets” (Teece, 1998b). How does one measure the worth of an organization’s knowledge assets? What does one mean by knowledge assets anyway? In this article, we afford some formal structure to the idea of measuring knowledge management capabilities of an organization, with the ultimate goal of improving business performance through better management of knowledge assets. We describe a large-scale effort at Intel to assess such capabilities with a view to enhance them. This project started in May of 2002. We describe the different types of knowledge assets identified, the potential capabilities associated with managing knowledge assets, the metrics devised for their measurement, and the assessment methodology that is being standardized across the corporation. We also provide results of the initial validation of the instrument and its ability to ascertain KM capabilities correctly. Hundreds of knowledge workers (Davenport, 2003) have so far participated in this study to benchmark KM capabilities of their units. Some units are already planning the next steps for improving their KM capabilities.	benchmark (computing);knowledge management	Uday R. Kulkarni;Ronald D. Freeze	2011			data management;knowledge management;knowledge engineering;risk management information systems;information management;personal knowledge management	AI	-80.9074436812304	11.619349223854673	45776
68f465bc630b2c399f2dff356cd70387ced8e663	framework based on benefits management and enterprise architecture	ciencia;projetos;investigacao;publicacoes;iscte iul	The relationship between Information Technology and Business has been growing, materialized through increasing investment in IT. This is typically considered a burden, as such companies have been seeking alternative supplying approaches like Outsourcing, Managed Services, or Cloud Computing. In this paper, the authors examine how investment is made and stress the balance that has to be reached between the expectations of the business versus the capabilities and cost of IT. A framework for assessing investments in IT that seeks to highlight the advantages of merging IT Benefits Management with an Enterprise Architecture is presented.	enterprise architecture	António Rodrigues;Henrique O'Neill	2012	IRMJ	10.4018/irmj.2012040103	knowledge management;marketing;operations management;enterprise architecture;management;business architecture	OS	-75.91758836361335	9.891555521633306	46036
ba2ba5c8cac33ee7b2b0d828bbd80ff97697f1b0	analysis of the integration between operations management manufacturing tools with discrete event simulation		The purpose of this paper is to analyse the integration of discrete event simulation (DES) in operations management manufacturing tools. Due to the movement of the fourth industrial revolution (Industrie 4.0), the integration of manufacturing is a topic constantly discussed in many areas. Moreover, it presents great research and innovation opportunities. To achieve the objective of this study, a search was conducted using the main keywords found in papers related to manufacturing systems and operations management manufacturing tools. Also, academic databases were literature research to identify the keywords relevant to the study added to DES. We considered only articles from the last 8 years. At the end between the search, the integration between tools such as manufacturing execution system, enterprise resource planning, radio frequency identification, core manufacturing simulation data, e-Kanban with DES were analysed. Furthermore, it was observed that the tools cannot always be used separately, but in some cases, these tools should be used jointly to solve problems related to production systems. Another aspect observed was how the data collected in production systems are fed to the DES models. Through, it was possible to analyse an existing gap regarding how the data is used between DES and manufacturing systems, thereby enabling research development in this area.	simulation	Rodrigo Ferro;Robert Eduardo Cooper Ordóñez;Rosley Anholon	2017	Production Engineering	10.1007/s11740-017-0755-2	manufacturing execution system;manufacturing engineering;discrete event simulation;process development execution system;research development;computer-integrated manufacturing;integrated computer-aided manufacturing;enterprise resource planning;systems engineering;operations management;computer-aided manufacturing;computer science	DB	-66.41915006565709	11.23052848499718	46089
8a2de3563f8db8ec77a6d31f7803ffa872d9863d	software engineering project standards	software;project management;standards;software measurement;project manager;software management;ansi standards;software standards project management software development software engineering software engineering standards software management;maintenance engineering;code standards;software engineering;standards development;guidelines;engineering management;software development;software engineering software standards measurement standards standards development software development management engineering management programming ansi standards code standards software measurement;software standards;measurement standards;programming;software engineering standards;software development management;documentation	Software Engineering Project Standards (SEPS) and their importance are presented in this paper by looking at standards in general, then progressively narrowing the view to software standards, to software engineering standards, and finally to SEPS. After defining SEPS, issues associated with the selection, support, and use of SEPS are examined and trends are discussed. A brief overview of existing software engineering standards is presented as the Appendix.	addendum;software engineering	Martha A. Branstad;Patricia B. Powell	1984	IEEE Transactions on Software Engineering	10.1109/TSE.1984.5010201	maintenance engineering;project management;programming;documentation;computer science;systems engineering;social software engineering;software development;software engineering;software walkthrough;software measurement;management;computer engineering	SE	-64.82148686825742	26.794911868191857	46211
e4016c4dbb5cba4230d8c7115ae553b99f15116d	the innovation factory - an approach to strategic innovation management	business models;knowledge management;innovation management;knowledge economy	In recent years, specifically with respect to the development of the knowledge-based economy, innovation has become a critical issue. In the knowledge economy, innovation increasingly is understood as relating not only to technology, but also to processes and business models. Networking, co-operation, and fluid flow of knowledge impact the way of business and also the way of developing innovations. With this paper, we suggest to address the issue of Strategic Innovation Management by an approach we call the Innovation Factory - a system that fits into both the organisational and technological infrastructure to support the collection, distribution and retrieval of knowledge required to create and support inovation in a cost effective manner. The intention of this contribution is to set out for a roadmap to systematic innovation management, rather than to provide an elaborated concept.		Norbert Jastroch	2004			innovation;product innovation;innovation management;knowledge economy;systems engineering;knowledge management;management science;business;personal knowledge management;open innovation	AI	-76.37506107545666	4.991275674024085	46226
2b352b3ae0c485eb8e9ff2007aa104200ab09a86	collaborative networks: human aspects and corresponding it support	groupware;collaborative work humans virtual enterprises business communication production design engineering process planning electronics industry metals industry companies;collaborative work;virtual enterprises;design engineering;collaboration;ict support;it support;trust issues;business communication;companies;human factors groupware;human aspects;metals industry;human factors;collaborative networks;electronics industry;business;knowledge sharing;production;humans;organizations;process planning;ict support collaborative networks human aspects trust issues communication;communication;business processes collaborative networks human aspects it support knowledge sharing;business process;business processes	Industrial SME face new challenges in the global market as customers are requiring more complete solutions and reducing the number of their suppliers. Small SME can face these new challenges through cooperation within collaborative networks. Human aspects are fundamental in collaborative networks as people, and not organizations or IT systems cooperate. This paper addresses the major human aspects encountered in this form of organization such as trust issues, knowledge sharing, coordination and planning activities as well as communication and mutual understanding and their influence on the business processes and the corresponding supporting IT tools.	business process;collaborative network;ws-trust	Michel Pouly;Charles Huber	2009	2009 13th International Conference on Computer Supported Cooperative Work in Design	10.1109/CSCWD.2009.4968121	knowledge management;management science;business process;business communication;management	Robotics	-65.06095747132608	13.409548145468277	46487
2becabdfeeb3fb021010b7805a450c6a6b959ecb	network externalities in software systems	software systems;network externality	■ Network externalities are the effects on the value of a product that can be ascribed to the presence of a network of users of such a product. They play an essential role in the business success of any product. The authors think that their role is even larger in the software industry, where (a) documentation and effective training are often lacking, and (b) consumers regard interoperability and compatibility as major benefits because of the ever-increasing need to share information and reprocess it over and over again with different tools. However, only few studies exist on this topic. Network externalities are caused by choices operating at different levels of product design data format, GUI metaphors, keyboard sequences, API, and so on. Understanding and planning their presence in a product is difficult. However, the ability to manipulate them properly provides a clear competitive advantage. This article briefly reviews the literature on network externalities, outlines a graphic notation to represent them, and applies such notation to describe an example, that of Microsoft Word 97. The study of the network externalities in Microsoft Word 97 shows several different kinds. The authors focus on the data format, the API, and the human-computer interaction paradigm. irms aim at maximizing the “value” of their products. However, the notion of value is hard to qualify and quantify. The hedonic model represents the value of a product as the sum of the values of the different components of the product [Berndt 1991]. The hedonic model has already been applied to software products in Gandal [1994]. In this paper we assume an hedonic model for the composition of the value of a product, and we cluster the components of the value into “internal” and “external.” Internal components depend on the product per se, that is, its functionality, its usability, its reliability, and so on. External components depend on the environment in which the product is located. The environment is formed by the users of the product and by other complementary or competing products. For instance, the value of e-mail service depends on the internal features of the service—availability, reliability, speed, and maximum size of the mailbox— and also on the environment, i.e., how many people we can reach using such a service. A service can supply the fastest connection or the largest mailbox, but if we cannot reach the people we want, the value we attach to the service is zero. Several researchers have investigated the value of internal components; see for example Boehm [1984] and Putnam and Myers [1992]. External components are usually called “network externalities,” since they are the result of a network of users of products and of complementary products [Farrell and Saloner 1985]. The authors think that network externalities play a strategic role in the software industry because:	application programming interface;barry boehm;documentation;email;emoticon;fastest;graphical user interface;hedonic regression;human–computer interaction;interoperability;microsoft word for mac;programming paradigm;putnam model;software industry;software system;usability	Giancarlo Succi;Paolo Predonzani;Andrea Valerio;Tullio Vernazza	1998	ACM StandardView	10.1145/338183.338194	computer science;public economics;network effect;software engineering;commerce;software system	Web+IR	-73.75894770947549	28.115186163854666	46502
6ee68cbdbb3d4dfbd8befb95c4b06d184e99789b	electronic banking and telecommunications	electronic banking	This paper presents a banking perspective of the maJor trends in electronic banking and telecommunications. Organized into four major trend issues, the paper provides some valuable insights into the telecommunications carrier business, into their impact on financial institutions and banking in particular, but also into free enterprise and regulated national systems, and effects that are caused by lack of competition.	online banking	James C. Grant	1986	Information & Management	10.1016/0378-7206(86)90070-4	computer science;marketing;retail banking;commerce	DB	-75.45772347191368	4.849770288329389	46518
b21fe67f0d067ac38f5a7606e686337deed324c0	the development and current status of medi spice	spi;software process improvement;info eu repo semantics conferenceobject;iso iec 15504 5 2012;medical device software;iso iec12207 2008;spice;iec 62304 2006	There is increasing demand for effective software process assessment and improvement in the medical device industry. This is due to the expanding and complex role that software now plays in the operation and functionality of medical devices. This paper outlines the development and current status of Medi SPICE a software process assessment and improvement model which is being developed to meet the specific requirements of this safety-critical domain. This includes the selection of the most appropriate software process improvement model on which to base Medi SPICE. Its initial development and restructuring to conform to ISO/IEC 15504-5:2012 and ISO/IEC 12207:2008. The structure and content of its process reference model is outlined and an industry based trial assessment of 11 of its processes discussed. Current and future work is considered including the timeframe for the release of a full version of the Medi SPICE model.	iso/iec 15504;iso/iec 42010;reference model;requirement;spice;software development process	Valentine Casey;Fergal McCaffery	2013		10.1007/978-3-642-38833-0_5	embedded system;real-time computing;engineering;operating system	SE	-69.76963958800582	17.02752864059637	46676
c4cafbe26619f2e7075d142c20e812b01a2e3cc6	enhancing preliminary design within concurrent engineering using the matrix of functions and functionalities	preliminary design;concurrent engineering processes;functional matrix;design theory;functionality matrix;product development	In today’s multidimensional world, companies are constantly facing new, increasingly demanding challenges: global business and local operation, the standardization and individualization of products, demanding customers, and fierce competition. These companies want to achieve a shorter product-development time, lower costs, higher quality of the product, and finally, customer satisfaction. In order to achieve these goals, companies have to take into account the first customer’s wants and needs during the process of developing a new product. This article presents the concept of solutions for the functional requirements in the concurrent development of a product by means of a descriptive matrix of the functional requirements and functionalities based on the generative model and criteria for describing the products, the function requirements, and the functionalities. The matrix of the functional requirements and functionalities model intends to improve the initial, preliminary design process, where only the most basic, sporadic information (such as functional requirements and functionalities) is presented. The tasks are parallelized and integrated to reduce the time and costs involved. The model was created as a tool in order to connect the functional requirements with existing technical systems. They partially or fully resolve the functional requirements on the basis of a mathematical model and predetermined conditions, and therefore do not depend solely on the designer’s intuition. The concept improves the main shortcomings of other known methods such as the morphological box. In order to demonstrate the validity, the benefits of the method, the dynamics, to produce explanatory theories and to develop a method that is relevant to practice, the matrix of the functional requirements and functionalities model is tested against a concrete product.	functional requirement;generative model;mathematical model;out of the box (feature);parallel computing;repeatability;subscriber identity module;the matrix;theory	Ziga Zadnik;Marko Starbek;Joze Duhovnik	2012	Concurrent Engineering: R&A	10.1177/1063293X12462044	economics;systems engineering;engineering;marketing;operations management;management science;designtheory;new product development	DB	-66.65921689008427	17.974216024517027	46711
deb305aba24d1866b8168c25c363eaae02a41b76	an evolving pattern library for collaborative project documentation	knowledge management;collaboration;prozess;informatik;design pattern;library;documentation;rollenmodell;entwurfsmusterbibliothek		documentation	René Reiners	2014			systems engineering;engineering;knowledge management;world wide web	HPC	-64.0415021631842	22.650501728797156	46739
9ab4a5be9735c50417c8c9d0ef08d2a157ce2d01	utilizing enterprise systems for managing enterprise risks	enterprise systems ess;risk management;knowledge management;enterprise resource planning erp;business benefits;business intelligence	Enterprise risk management is a critical concept in the current business environment that supports use of tools and processes directed toward monitoring and mitigating organizational risks. Many organizations have embraced enterprise systems (ESs) technology for improving organizational efficiency and effectiveness. ESs provide value by identifying opportunities in operations and assist in managing risks through context sensitive analyses by eliciting relevant information. This research investigates how ES data were transformed into knowledge by a hi-tech manufacturing firm from an ES implementation, and how this knowledge was used to manage risks by utilizing an ES data transformation model from existing literature. Findings indicate that the ES data transformation process resulted from knowledge-leveraging actions at both executive and operational levels. At the executive level, the use of business intelligence module in conjunction with cascades of balanced scorecards helped in assessing progress for achieving goals, and translated decisions into risk-eliminating actions at the operational level. An initial technology-push approach assisted in creating semantically rich representative process models by simulating risk scenarios, leading to a strategy-pull approach for deploying business strategies and decisions. A value assessment strategic model articulates the knowledge-leveraging processes combining human skills with ES tools to optimize enterprise risks. 2013 Elsevier B.V. All rights reserved.	benchmark (computing);enterprise risk management;enterprise system;experience;hitech;linkage (software);simulation;usability	Sanjay Mathrani;Anuradha Mathrani	2013	Computers in Industry	10.1016/j.compind.2013.02.002	enterprise system;enterprise systems engineering;enterprise software;risk management;computer science;systems engineering;knowledge management;marketing;integrated enterprise modeling;digital firm;database;management science;enterprise data management;enterprise architecture;business intelligence;enterprise integration;management;business process modeling;enterprise planning system;enterprise information system;business architecture;enterprise life cycle	SE	-77.31249474943807	8.517884268966153	46743
3d08137b84dfdd543211b677390e1c37dfe3080f	software-as-a service model: elaborating client-side adoption factors	strategic management;outsourcing;application service provider;software as a service;saas	Software-as-a-Service (SaaS) is emerging as a viable outsourcing option for clients interested in paying for the right to access through the network a standardized set of business software functions. SaaS model largely replaced the Application Service Providers (ASPs)-based model, by creating an architecture that that provides no mechanisms for customizing the software on the vendor side; all customization is done on the client side through standardized interfaces. The fact that vendors are not making any client-specific investments makes this outsourcing model quite intriguing. In this paper we investigate client’s side determinants of adopting the SaaS model. We draw on economic, strategic management, and IS theories to develop a theoretical framework. In it, we develop a more elaborate view of uncertainty as some types uncertainty increase the propensity to adopt SaaS, while other types do the opposite. Finally, we integrate the role of the internal enterprise IT architecture into our model.	business software;client-side;outsourcing;software as a service;strategic management;theory	Mingdi Xin;Natalia Levina	2008				Web+IR	-75.417020304182	6.589805878070904	46763
c9d4738460c8c80af78dae3efdca6ca93eea4870	what happens before a project starts? - project start-up from the supplier perspective	project start up;customer;organizations;article	Before an outsourced software project officially begins the contracting or supplier organization has already expended effort. Although project start and start-up effort impact on project success in most cases these are undefined concepts. There are no clear definitions of project start, start-up or the activities that should be completed before project start either in the literature or in practice. Ambiguity around project start sets up risks to the profitability of a project and therefore makes the real success of a project not only uncertain but difficult to measure. A vague project start also makes comparisons between projects and between organizations unreliable. In this paper, we describe a pilot study that reviews project start, project start-up, and project start date, and then investigates what the key activities of the supplier are normally performed by the end of the project start-up phase. We use interviews with software supplier practitioners to define those key activities.	booting;holism;information system;lero (software engineering);software engineering;software project management;undefined behavior;vagueness	Paula Savolainen;June M. Verner;Lesley Pek Wee Land;Graham C. Low	2010		10.1007/978-1-4419-9790-6_52	level of effort;basis of estimate;project management;extreme project management;work breakdown structure;systems engineering;engineering;knowledge management;operations management;milestone;project risk management;pre-construction services;project governance;project management triangle;project charter;schedule;project planning;project portfolio management	SE	-68.93900382418079	22.688652104762546	46886
76f7fb5dbe9f45257cd92e3976ea09a52a67b765	timely detection of coordination requirements to support collaboration among software developers	dp management;project management;software management;team working;collaboration support;coordination requirements;software developers;software development team;software project;timely detection;work dependencies	Work dependencies often exist between the developers of a software project. These dependencies frequently result in a need for coordination between the involved developers. However, developers are not always aware of these Coordination Requirements. Current methods which detect the need to coordinate rely on information which is available only after development work has been completed. This does not enable developers to act on their coordination needs. I have investigated a more timely method to determine Coordination Requirements in a software development team as they emerge.	requirement;software developer;software development;software project management	Kelly Blincoe	2012	2012 34th International Conference on Software Engineering (ICSE)		project management;programming;personal software process;long-term support;verification and validation;team software process;productivity;visualization;software project management;computer science;systems engineering;knowledge management;package development process;social software engineering;software framework;software development;software design description;software engineering;software construction;software as a service;software walkthrough;application lifecycle management;software documentation;software analytics;management;software deployment;software development process;software requirements;collaboration;software peer review	SE	-67.83142520161721	21.846080244976847	46965
12740cac481586705ddf26df774ff8dd175beebd	balancing self-directed and peer-induced efforts in an information technology collaborative software development: a network approach		In a collaboration network environment for software development, it is a highly desirable managerial objective to establish and maintain a stable balance of ongoing engineering activities and workload distributions among the project engineers. Grounded on a theoretical framework and associated simulation models to understand the emergence of this balance, we show that conditioning the development process to remain confined in a stable region of the dynamics could be delicate affair in real applications. Nevertheless, we demonstrate that it is technically feasible to control ongoing development by suitably restricting relevant parameters of the production dynamics, which amounts to achieving a stable balance between self-directed and peer-induced work efforts expended by the project engineers in building the final software product. Strategies for realizing this condition have important implications for managerial decision-making in collaborative software development.	americas conference on information systems;collaborative software;embnet.journal;emergence;simulation;software development;software framework;unfolding (dsp implementation)	Jaideep Ghosh	2016			management science;collaborative software;knowledge management;computer science;information technology	SE	-67.41639176801999	17.367064162311117	47017
139f0e7e89415aced7393ce20ce37366c9692d21	agent aware organizational design (doctoral consortium)	organizational design;dec mdp	I study creation of multiagent organizations via an automated computational process. My organizational design problem frames design decisions in terms of the quantitative impact that an organization is expected to have on the agents’ reasoning and behaviors. I develop techniques for efficiently solving this problem via incremental search of the organizational design space, and extend my organizational design process to provide supplementary information alongside its design that agents can use to inform organizational adaptions.	agent-based model;computation;incremental search;problem frames approach	Jason Sleight	2014			organizational network analysis;organizational learning;organizational behavior and human resources;computer science;knowledge management;organizational effectiveness;management science;organizational architecture	AI	-66.54384308790154	4.621537707469055	47112
2706b5df14849d04b6a245c15b1c0e91b371c425	r.e.m. 2009 - international workshop on reverse engineering models from software artifacts	design model;model recovery;programming language;software systems;migration to mde mdd mdsd;maintenance engineering;data mining;paradigm shift;migration to mde mdd mdsd reverse engineering model driven engineering model recovery;levels of abstraction;software development;model driven engineering;conferences reverse engineering model driven engineering software systems maintenance engineering data mining;source code;conferences;reverse engineering	Model-Driven Engineering (MDE) is a software development paradigm where the focus is on developing models instead of source code. Like higher level programming languages were earlier developed to raise the level of abstraction from the assembly language, MDE aims at further raising the level of abstraction from source code to design models. The paradigm shift from traditional development approaches to MDE adoption is, however, not easy. In most situations, MDE can hardly be started from a clean slate as organizations have significant vested interests in existing systems and the artifacts used for their creation (e.g. versioned source files, build and configuration files, operational and issue tracking data). The aim of this half-day workshop is to bring together researchers and practitioners interested in discussing the challenges, benefits, experiences, and opportunities for reverse engineering models from existing software system artifacts to support the migration to MDE. The workshop will emphasize discussions and position statements over paper presentations.	clean slate program;experience;issue tracking system;model-driven engineering;model-driven integration;programming language;programming paradigm;reverse engineering;software development;software system;version control	Leon Moonen;Tarja Systä	2009	2009 16th Working Conference on Reverse Engineering	10.1109/WCRE.2009.52	maintenance engineering;paradigm shift;model-driven architecture;real-time computing;computer science;systems engineering;engineering;software development;operating system;software engineering;programming language;reverse engineering;software system;source code	SE	-63.88447033741473	28.926393963299514	47130
2415d99f30b29f86b8b943b6afb22b5ead3ab0f4	long-term forecasts of military technologies for a 20-30 year horizon: an empirical assessment of accuracy		During the 1990s, while exploring the impact of the collapse of the Soviet Union on developments in future warfare, a number of authors offered forecasts of military technology appearing by the year 2020. This paper offers a quantitative assessment of the accuracy of this group of forecasts. The overall accuracy — by several measures — was assessed as quite high, thereby pointing to the potential value of such forecasts in managing investments in long-term research and development. Major differences in accuracy, with strong statistical significance, were found between forecasts pertaining primarily to information acquisition and processing technologies, as opposed to technologies that aim primarily at physical effects. This paper also proposes several recommendations regarding methodological aspects of forecast accuracy assessments. Although the assessments were restricted to information available in open literature, the expert assessors did not find this constraint a significant detriment to the assessment process. Introduction and Motivation Technology forecasting is increasingly recognized for its value in both commercial and government endeavors. Worldwide, at least 23 organizations perform technology forecasting (Lerner et al. 2015) as their major product for external customers. In addition, virtually every organization engages in some form of forecasting for internal purposes, implicitly or explicitly, in order to plan its activities. Product developers and manufacturers need technology forecasts in order to know where to invest their product development resources or how to plan new manufacturing facilities. Providers of services, e.g., health care (Doos et al. 2016), use technology forecasts to determine what new equipment should be purchased or whether a purchase should be delayed because the next generation of technology is about to emerge. Governments use technology forecasts to optimize allocation of funds toward supporting educational and scientific research institutions. This paper concerns itself with a particular type of technology forecast: long-term forecasts of military technology. The reasons such forecasts are valuable to a military establishment are not dissimilar to those that make forecasts important to businesses: a military procurement institution uses technology forecasts to determine what system development and procurement efforts should be undertaken (and funded) to optimize the future value of the resulting technology to those who would have to fight in a future war. If the forecast is wrong, the military may end up fighting an enemy who possesses weapons of superior technology (Van Creveld 2010), not unlike a business that relies on a technology forecast to avoid falling behind its competitors. On the other hand, in spite of these fundamental similarities, technology forecasting models in business and in certain types of military system do differ by their time horizons. Here, the time horizon is the number of years between the date a technology is predicted to emerge and the date the forecast was made. While most technology forecasts (performed for business purposes) are short term (the horizon is 1–5 years), the technology forecasts for the military in many cases (although there are many important exceptions) is either mid-term (6–10 years) or long term (11–30 years). The necessity of long-term forecasts in the world of military technology is dictated by long periods of time required for full development of some types of complex military systems. It has become common – and a concern for policy-makers -for a major defense acquisition program to take on the order of 2 decades (Goure 2017) from concept development to initial operating capability. In addition, development of foundational science and technological knowledge underpinning engineering developments of advanced systems often takes another 10 or more years. In this way, 20 years or even longer horizon is often important for military technology forecasts. This paper, therefore, focuses specifically on long-term (20 years or longer) military technology forecasts. However, can such long-term forecasts be sufficiently accurate? It is often observed that the longer the horizon of a forecast, the lower the forecast accuracy, e.g., dropping from 38% accuracy for less than 10 years horizon to merely 14% accuracy for horizons over 11 years (Fye et al. 2013). After all, if accuracy is low, there is no value in developing forecasts. Moreover, an inaccurate — and potentially misleading — forecast is worse than no forecast at all. Unfortunately, relevant empirical studies are rare. As we discuss in the next section of this paper, studies of forecast accuracy in general are few. Of these, only a small fraction deals with technology forecasts. Furthermore, virtually all of them deal with short-term, not long-term, forecasts. The contributions of this study are as follows. First, it adds a valuable empirical data point to a small set of data on long-term forecast accuracy. Second, it constitutes a case study on methodologies suitable for assessing the accuracy of forecasts and yields recommendations for methodological approaches we found to be effective. Finally, it provides support to the argument that long-term technology forecasting is both feasible and sufficiently accurate for the purposes of supporting management decision making on long-term research investments.	data point;emergentism;new product development;next-generation network;procurement;use error	Alexander Kott;Philip Perconti	2018	CoRR			HCI	-80.94195794832078	11.544959999945311	47332
761b69599861eace0b47c883f78aa4f0d087435f	proposal for co-ordination of a rare osi directory pilot				Christian Huitema	1989	Computer Networks	10.1016/0169-7552(89)90055-X	computer network;database;x.500;ordination;directory;computer science	Theory	-85.14808764233116	17.368493112630322	47371
8b15fb6fac50a221c2fd632398739b90c59a9587	the 21st century it workforce: addressing the market imbalance between supply and demand	it careers;it professional;human resources;labor market;human resource;recruitment strategies;human resource management;it workforce;supply and demand	1. ABSTRACT Increasing dependence on organizational technology coupled with the short supply of qualified IT professionals has resulted in a severe market imbalance within the IT workforce. Recruiting the necessary personnel to meet these current and future demands in this tight labor market presents tremendous challenges for IT and human resource managers alike. Organizations are finding it increasingly difficult to attract, motivate, and retain IT workers and traditional human resource practices are proving inadequate. The aim of this study is to identify specific criteria deemed important by entry-level IT applicants in influencing their decisions to enter the applicant pool of a specific fm. Given the tight labor market, the greatest challenge is not selecting the best applicants from among an existing applicant pool, but increasing the size and quality of the potential applicant pool. This project investigates these issues via the development and administration of a survey instrument utilizing a sample of potential entry-level IT applicants. The results will contribute knowledge toward organizational efforts to		Karen D. Schenk;K. Shannon Davis	1998		10.1145/279179.279197	strategic human resource planning;resource management;workforce planning;microeconomics;business;market economy;labour economics	HCI	-86.23530823396386	6.67501129903188	47372
2fafd16c2b33f825cf38e7b13dc92f68ea9d6559	disrupt the disruptor: a theoretical approach of cloud computing on it outsourcing industry disruption	it strategy;outsourcing;technological innovation;computer model;paas cloud computing it outsourcing industry strategic decision making disruptive innovation theory technical research social impact research knowledge gap iaas saas;industries;companies;strategic planning;it strategy cloud computing it outsourcing disruptive innovation;service model;it outsourcing;computational modeling;innovation management;strategic planning cloud computing decision making innovation management outsourcing;cloud computing technological innovation outsourcing industries computational modeling companies;next generation;social impact;disruptive innovation;theoretical foundation;cloud computing	"""Despite the abundant academic discussion of the characteristics of cloud computing and its associated adoption to challenge current IT outsourcing industry, there is still uncertainty for executives to make the sound strategic decision in prioritizing cloud computing as an appropriate model for next generation IT outsourcing solution. In this paper, a new theoretical perspective from disruptive innovation theory has been integrated for addressing the observed emerging industrial paradigmshift -- cloud computing adoption on IT outsourcing industry disruption. By reviewing the state of cloud computing service from technical and social impact research and the status for IT outsourcing industry study, this paper seeks to address the knowledge gap of cloud computing enabled IT outsourcing disruption by synthesizing the academic discussion. It offers a roadmap of past IT outsourcing industry, cloud computing research and future IT operation implication. Inspired by the disruptive innovation analysis of cloud computing from a variety of service models, including IaaS, SaaS and PaaS, the author have identified a number of emerging challenges for IT outsourcing industry and laid the theoretical foundation for future research in this domain. Finally, in addition to theoretical contributions, the scenario-specific practical implication has been presented as managerial """"take-away"""" for both industry incumbent and entrant."""	biconnected component;cloud computing;cloud research;denial-of-service attack;industrial pc;outsourcing;platform as a service;scenario planning;schema (genetic algorithms);software as a service;theory	Zhenyu Yang	2011	2011 IEEE Ninth International Conference on Dependable, Autonomic and Secure Computing	10.1109/DASC.2011.99	strategic planning;innovation management;computer science;knowledge management;knowledge process outsourcing;management science;management;outsourcing	HPC	-74.0561404525512	9.345133708297501	47510
c784ef0294e775849ce0bc4069a87148173aa3ac	exile: a natural consequence of autonomy and belonging in systems-of-systems	degradation;reliability;measurement;system analysis and design;system of systems;resilience;system of systems reliability resilience measurement system analysis and design degradation context;service abandonment governance system of systems operational effectiveness system exile service exile system abandonment;context	Governance is one of the key differentiating elements between traditional systems and systems of systems. While systems are governed by a single authority, systems within a system of systems are often independently governed or governed by fully empowered entities. Such independence is a necessary condition for the autonomy of each constituent system and for enabling the concept of belonging. At the same time, the capability to be autonomous and the voluntary nature of belonging, enables a system of systems to voluntarily expel or exile one or more of its constituent systems. Yet, research has not addressed so far the implications and modeling of potential exile into the operational effectiveness of a system within a system of systems and its impacts on the engineering of systems of systems. This paper presents the concept of system exile as a philosophical necessity in the definition of systems of systems, it discusses some visions to measure the risk of exile, and proposes a way forward to explore mitigation techniques.	autonomous robot;autonomy;entity;exile;system of systems	Alejandro Salado	2016	2016 Annual IEEE Systems Conference (SysCon)	10.1109/SYSCON.2016.7490598	geography;civil engineering;operations management;computer security	SE	-66.29786912818675	8.863613726351794	47547
642bbe8bac92f5c8bc458c83764d8645435c5481	enterprise document system cloud deployment	tool;enterprise;cloud deployment;document systems	"""The software used by enterprise businesses for creating variable-data customer documents must be highly reliable, and vendors are increasingly distributing such software via the cloud as an online service. This means that vendors now assume responsibility for the IT resources hosting and supporting the software as well as the customer documents and data. Vendors also assume responsibility for pushing updates to all customers simultaneously. To support the test and release of new versions, software vendors must deploy and configure the software at an unprecedented rate.  To reduce the time spent deploying and configuring software in the cloud, and to minimize the chance for human error, we present StackLauncher. By making it possible to automatically configure and launch software """"stacks"""" with push-button simplicity, StackLauncher is a valuable addition to the software development lifecycle for cloud deployment of enterprise document software."""	cloud computing;human error;online service provider;push-button;software deployment;software development process	Christopher Alan Wells;Joel Jirak;Steve Pruitt;Anthony J. Wiley	2013		10.1145/2494266.2494292	functional software architecture;long-term support;verification and validation;enterprise system;software quality management;enterprise software;crowdsourcing software development;package development process;backporting;social software engineering;component-based software engineering;software development;middleware;software as a service;database;software walkthrough;software analytics;resource-oriented architecture;software deployment;world wide web;computer security;software system	SE	-68.22844954998143	26.932768972248795	47659
1c0dabe80b5411f1e66182659d27544b433f1d21	dynamic risk management system for the modeling, optimal adaptation and implementation of an erp system	fabrication;modelizacion;europa;adaptacion;project management;fabricacion;grece;manufacturing resource planning;implementation;project manager;gestion risque;risk management;grecia;erp system;computer applications;modelisation;greece;adaptation;manufacturing;software package;gestion projet;computer application;pgi progiciel de gestion integree;gestion riesgo;progiciel;information system;europe;implementacion;modeling;paquete programa;systeme information;gestion proyecto;design methodology;sistema informacion	Purpose – This paper aims to deal with the development of a risk management application for the modelling, optimal adaptation and implementation of an ERP system. Design/methodology/approach – This paper presented a risk management application for the modeling, optimal adaptation and implementation of an ERP system. The application was tested with the operations and capabilities of the ERP commercial package “SINGULAR Enterprise (SEn)” of the Greek Software House DELTA-SINGULAR S.A. Findings – The functional result of this application was proved to support considerably the management of risk within the implementation of the ERP system. Originality/value – To the best knowledge of the authors there is no other current generic research in this technological field concerning small or medium-sized enterprises. With the development of this application, the goals mentioned in the conclusions were achieved.	erp;enterprise resource planning;risk management;software house	Ioannis Zafeiropoulos;Kostas S. Metaxiotis;Dimitris Askounis	2005	Inf. Manag. Comput. Security	10.1108/09685220510602031	project management;simulation;systems modeling;design methods;risk management;operations management;manufacturing;computer applications;fabrication;implementation;management;information system;adaptation	OS	-68.51283922990783	7.307806979399809	47686
bfcc5f59a95693fb954b7602b0a8a65bec6794c3	a risk diagnosing methodology web-based tool for sme's and start-up enterprises		This work presents a risk diagnosing methodology (RDM) web-based tool, that can provide to Small and Medium Enterprises (SME’s) the capability to identify, evaluate and manage the risks associated with a company’s idea development project portfolio. This tool was conceived to support successful innovative product/service development projects, from its idealization to its commercialization, and to encourage SME’s on systematic use of risk management approaches in order to increase their successful rates. This paper also includes a brief literature review of some of the risk management tools and models available to SME’s, as well as comparative analyses of the identified similar tools.	risk management;web application	Luís Pereira;Alexandra Tenera;João Bispo;João Wemans	2013		10.5220/0004548103080317	business;web application;process management;systems engineering	SE	-71.6442351116882	10.397822230687773	47772
17abfa180ecdd8c9c3c5b3cde91ae4e1285c9f98	model-driven development and the future of software engineering education	specification languages computer science education educational courses program compilers software engineering;software engineering;computer science education;programming compilers model driven development software engineering education construction activity software development processes software engineering curriculum problem oriented programming languages;specification languages;computational modeling abstracts;educational courses;program compilers	This paper argues that the research area of model-driven development is likely to lead before long to tools being produced that will automate most of the construction activity within software development processes. It therefore discusses the impact that such developments would have on the software engineering curriculum, in terms of both the body of knowledge and the structures of courses, drawing on analogies with the impact during the 1960s of the development of problem-oriented programming languages and compilers for them.	compiler;model-driven engineering;programming language;software development process;software engineering	Anthony J. Cowling	2013	2013 26th International Conference on Software Engineering Education and Training (CSEE&T)	10.1109/CSEET.2013.6595271	personal software process;verification and validation;computing;software engineering process group;computer science;engineering;package development process;software design;social software engineering;software framework;component-based software engineering;software development;software engineering;software construction;software walkthrough;programming language;resource-oriented architecture;software development process;software requirements;software system;computer engineering;software peer review	SE	-65.1358770218673	26.078847980754247	47781
2691f007bfc64c5405c6ec5794bdc474ef23a253	analysis of software experience data (panel discussion): software reliability measurement during development	representation of solids;computer graphics;geometric data bases;computational geometry;geometric modelling;cad cam	Failure ratio and failure rate are identified as significant measures of software reliability during development. Month-to-month variability is shown to be comparable to that for hardware reliability measures. Module-to-module variability for a given development environment is surprisingly small. Trend lines on failure ratio and failure rate yield a progress index of considerable interest to management. The data collection and analysis procedure will be described. Some surprising results were obtained when causes of failure were examined.	failure rate;software quality;software reliability testing;spatial variability	John Palaimo	1978		10.1145/800178.810127	reliability engineering;computer science;engineering drawing	OS	-63.75412792700817	30.807186621363062	47791
f17801df78e215c71701be93de73f201e1bd9828	application software maintenance: can it be controlled?	quality assurance;software maintenance;data processing;software engineering;operational auditing	Numerous studies have found that subsequent application software maintenance requires a significant amount of a data processing department's resources. Some of the major research in this area [1, 2, 3, 4] revealed the following:• in 1972, 30.1 percent of total programmer effort was devoted to maintenance• in 1978 twenty percent of the systems studied had allocated eighty-five percent of their annual analyst and programming hours to maintenance.• in 1979 as high as seventy-five percent of systems and data processing recourses were spent on application maintenance• a study determined that half of a data processing department's staff time was devoted to maintenance	programmer;software maintenance	Glenn L. Helms;Ira R. Weiss	1984	DATA BASE	10.1145/1040688.1040691	reliability engineering;quality assurance;operational auditing;data processing;engineering;operations management;software engineering;database;software maintenance	Arch	-67.18239801862802	32.014171222004755	47793
7e98db3c4b974b5adde5ae6d49e6487945f61279	the main critical success factors of contractual and relational governances in outsourcing relationships		The relationship between organizations involved in Information and Communication Technology Outsourcing is a key factor for the success of the provision of services. When all parties involved work together, they achieve a high level of cooperation and create a partnership marked by mutual trust and intensive exchange of experiences and knowledge sharing. This work aims to present the results of a survey conducted in one of the greatest information and communication technology poles of Brazil. Several concepts related to contractual and relational governances in outsourcing were identified and allocated within two sets of constructs. Finally, Spearman’s correlation tests were performed to check the strength of the correlations within each set.	outsourcing	Victor Diogho Heuer de Carvalho;Thiago Poleto;Ana Paula Cabral Seixas Costa	2016		10.1007/978-3-319-31232-3_1	business administration;process management;business	DB	-81.9117676663199	4.283204737413597	47850
97157c07687057ef978bb800dff24fc18e97baff	improving small-to-medium sized enterprise maturity in software development through the use of iso 29110			capability maturity model;iso 29110;software development	Jean-Christophe Deprez;Christophe Ponsard;Dimitri Durieux	2014	ERCIM News		data mining;software engineering;software development;computer science	SE	-64.8998680929672	22.53919581578209	47871
236f5a8e6e939ffaa0170ab6b0f895c95454bb50	predicting development effort from user stories	agile methods;software measurement;software prototyping;software measurement software engineering;effort estimation development effort prediction user stories industrial agile software projects;software engineering;effort prediction;effort estimation;user stories;user stories agile methods effort prediction	In this paper, we propose a method for predicting development effort based on user stories. Such approach is well suited for Agile software projects where requirements are developed along with the project and only sketched in a rough manner. We apply the proposed method to two industrial Agile software projects of very different size and structure. We show that such effort estimation works reasonably well if user stories are written in a structured way.	agile software development;cost estimation in software engineering;requirement;user story	Pekka Abrahamsson;Ilenia Fronza;Raimund Moser;Jelena Vlasenko;Witold Pedrycz	2011	2011 International Symposium on Empirical Software Engineering and Measurement	10.1109/ESEM.2011.58	requirements analysis;personal software process;verification and validation;simulation;software sizing;agile unified process;agile usability engineering;software project management;systems engineering;engineering;social software engineering;software development;requirement;software engineering;analysis effort method;software construction;agile software development;software documentation;empirical process;software measurement;lean software development;user story;software development process;software metric	SE	-64.98861919416711	29.257907641753892	48057
61e513cb69347d284634dbc906a5617680db07cd	understanding complex it environments using information analytics and visualization	information mining;business environment;visualization;information management;knowledge sharing;service delivery	"""Today's business environments are going through several major transformations. First, most business environments are increasingly dependent upon vast amount of information. However, in part because of sheer volume, effective use of information is becoming more and more difficult. Second, the IT environments that support businesses are evolving from a simple machine- and automation-centric operational model to a complex people- and interactive service-centric operational model. Where, it becomes critical to analyze and understand the relationships between people, their skills, technologies, and organizations and effectively leverage human and technological resources to drive service delivery excellence and innovation. Unfortunately, very few tools exist to leverage the available information and analyze such relationships. This paper describes a solution, called """"Business Insights Workbench"""" (BIW), which couples a number of information analytics techniques with a unique set of visualizations to help uncover hidden relationships among the key factors of the business environment (e.g., people, their skills, technologies, and organizations). Such understanding can bring many benefits to IT organizations, e.g., effective staffing for projects, collaboration and knowledge sharing, and technology growth and innovation. We use an IT business consulting services (IT BCS) organization as an example to illustrate our approach."""	itil;workbench	Amit Behal;Ying Chen;Cheryl A. Kieliszewski;Ana Lelescu;Bin He;Jie Cui;Jeffrey T. Kreulen;E. Michael Maximilien;James Rhodes;W. Scott Spangler	2007		10.1145/1234772.1234796	visualization;human–computer interaction;computer science;knowledge management;artifact-centric business process model;service delivery framework;data science;management science;business analytics;information management;management	HCI	-73.77828781971748	11.568170425106969	48093
25abfd0e9792a757f0073ced561ac0e46dcaa12f	analysis of propensity to patent and science-dependence of large japanese manufacturers of electrical machinery	analyse bibliometrique;processus innovation;communication scientifique;technology development;comunicacion cientifica;interaction;innovation process;recherche developpement;proceso innovacion;japon;relation science technologie;research and development;analisis regresion;asie;investigacion desarrollo;patents;firm level data;analyse correlation;analyse regression;scientific communication;regression analysis;interaccion;bibliometric analysis;patente;recherche scientifique;brevet;scientific research;scientific knowledge;japan;analisis correlacion;investigacion cientifica;asia;analisis bibliometrico;science technology relation;correlation analysis	The paper aims to clarify the extent to which the results of scientific-oriented research conducted by corporations are reflected in their application-oriented research. Focusing on large Japanese manufacturers of electrical machinery, the paper analyses firm-level data on presentations of scientific papers that represent the results of scientific-oriented research activities, citations of scientific papers in patents, and inventions. The electrical machinery industry, a prototypical science-based industry, has been placing a growing emphasis on scientific-oriented research during the 1990's as is evident from trends in R&D expenses, scientific papers, and inventions. Regression analysis results suggest a complementary relationship between citations of basic scientific knowledge as presented in scientific papers on the one hand and acts of invention on the other hand, in the sense that a rise in citations corresponds to a rise in inventions. Moreover, the results suggest that invention efficiency (number of patent claims per unit of R&D expenditure) has been increasing during the 1990's. Furthermore, the results suggest that, given the exogenous influences on the patent system in Japan, it is necessary to include the number of patent claims when attempting to measure corporate technology development activity through the volume of patent applications. However, there was no finding of a clear relationship between the number of scientific papers and inventions. Implications of these results for corporate R&D strategy are examined.	causality;dummy variable (statistics);enigma machine;linkage (software);norm (social);renaissance;scientific literature;scientometrics;strategic management	Jun Suzuki;Kiminori Gemba;Schumpeter Tamada;Yoshihito Yasaki;Akira Goto	2006	Scientometrics	10.1007/s11192-006-0111-3	innovation;interaction;social science;scientific method;epistemology;computer science;sociology;operations research;sociology of scientific knowledge;regression analysis;statistics	HPC	-86.735945111823	4.736265687478949	48175
033eac0be7300469ce09c7ac0ff28c441f45d106	service process innovation: a case study of bpmn in practice	service management;process innovation;business process modeling notation;process model;process improvement;business process re engineering;simulation tool;simulation model;organisational aspects business process re engineering;technological innovation analytical models unified modeling language business process re engineering information technology conference management innovation management technology management financial management standardization;organisational aspects;process unaware domain service process innovation bpmn business process modeling notation service management process truck dealership n e us to be process as is process process improvement	We describe the application of the Business Process Modeling Notation to the redesign of a service management process in a truck dealership in the N.E. US. After the deployment of a new service management system did not lead to the expected efficiency gains the authors were asked to analyze the as-is service process, make suggestions for improvement, and simulate the financial impact of the proposed changes. The as-is and to-be process were documented in BPMN, and a BPMN-based simulation tool was used to quantify the effects of process improvement. As an outcome of this project we developed an initial subset of BPMN constructs that were a useful starting point for modeling in a process-unaware domain with members of the surveyed organization. We found that the deliberate modification of BPMN constructs allows for the highlighting of common process weaknesses, and we show how an analytical process model changes when it is modified to serve as a simulation models.	business process model and notation;process modeling;simulation;software deployment	Michael zur Muehlen;Danny T. Ho	2008	Proceedings of the 41st Annual Hawaii International Conference on System Sciences (HICSS 2008)	10.1109/HICSS.2008.388	xpdl;service management;computer science;knowledge management;artifact-centric business process model;business process management;software engineering;simulation modeling;process modeling;business process model and notation;process management;business process;process mining;management process;business process discovery;management;business process modeling	HPC	-70.01546400819494	14.148610943329706	48460
75fdabe8e3057848d475dec6fb8e2bd69e73b97f	sesam - simulating software projects	software engineering education animation project management;project management;building block;simulation;attributed graph grammars software project simulation teaching hypothesis evaluation sesam software engineering simulation by animated models project management objects attributes actions relationships;software engineering;simulation software;computer science education;conferenceobject;graph grammar;software tools;dp management;computer animation;software tools computer animation computer science education digital simulation dp management project management software engineering;digital simulation	Teaching software engineering as well as researching in this area is very tedious due to the length and costliness of software projects. SESAM therefore is designed as a simulator for software projects, allowing students to gain reality-like experiences in project management and researchers to evaluate hypotheses on the mechanisms influencing software projects. This paper focuses on the basic assumptions for SESAM, i f s building blocks and the way hypotheses are aj4‘ecting the simulation. After a short description of the requirements for SESAM we introduce objects, attributes, actions, relationships between objects and hypotheses as its ban’c concepts. We present attributed graph grammars as a means for representing hypotheses. Finally we position our project with respect to related work, and we show its present state and future development.	attributed graph grammar;requirement;sesam;simulation;software engineering	Jochen Ludewig;Thomas Bassler;Marcus Deininger;Kurt Schneider;Jürgen Schwille	1992		10.1109/SEKE.1992.227898	project management;personal software process;verification and validation;computing;simulation;software engineering process group;software sizing;simulation software;software project management;computer science;systems engineering;engineering;software design;social software engineering;software framework;component-based software engineering;software development;software design description;software engineering;software construction;computer animation;software walkthrough;programming language;software deployment;software development process;software requirements;software system;computer engineering;software peer review	SE	-65.53004498021505	26.326762914882032	48469
e9a95b7269af600a8fab108a0b7aaa6aeb6278c4	hci requirements for transparency and accountability tools for cloud service chains	information systems;computer science	This paper elaborates HCI (Human-Computer Interaction) requirementsfor making cloud data protection tools comprehensible and trustworthy.The requirements and corresponding user interface design pri ...	cloud computing;human–computer interaction;requirement	Simone Fischer-Hübner;John Sören Pettersson;Julio Alexander Aguilar Angulo	2014		10.1007/978-3-319-17199-9_4	human–computer interaction;computer science;systems engineering;world wide web	Networks	-67.42774405965618	11.085148345977661	48506
d26485b92370f582622a8db942a569304c9d4b66	software development measurement programs		In this chapter we introduce the problems which are addressed by software measurement—e.g., providing quantitative insights, and we describe the possibilities which open up when we have software measurement of products, processes and enterprise in place. We discuss the possibility of quantitative fact-based management, customer data-driven development and using artificial intelligence (or machine learning) once we have a solid measurement program. Towards the end of the chapter we outline the concept of a company-wide measurement program and introduce the content of the book. 1.1 Academic and Industrial View on Software Measurement Measurement is a great activity that engineers love to do; it makes the discipline of engineering so elegant and structured. Software engineering is no exception to that. We can program and create software, but it’s really difficult if we cannot measure properly. Because, if we can’t measure our activities and products, how do we know that we’re on the right track in their development. Together with the ability to process large data sets—the so-called Big Data systems—measurement has gained a strategic value for modern enterprises. Regardless, whether we consider a large software development company with distributed projects or a small agile team developing a mobile game, measurement is strategic. The large companies need the measurement to capture the market’s requirements and to quantify the complexity of their large software projects (or their large number of small projects). The small software development companies need the measurement to assess whether the company’s choices are right for their customers; the small companies need to show to their customers which assets they have and how to cash in on them. Measurement is also strategic in today’s enterprises because of the ability to use data for complex decision models and complex algorithms [ASH+14, ASM+14]. We can see that modern cars are using the data to autonomously drive and to provide a completely new experience for their customers. We can also see that advanced machine learning and artificial intelligence can take advantage of measurement and © Springer International Publishing AG, part of Springer Nature 2018 M. Staron, W. Meding, Software Development Measurement Programs, https://doi.org/10.1007/978-3-319-91836-5_1 1	agile software development;algorithm;artificial intelligence;big data;data system;machine learning;mobile game;requirement;software engineering;software measurement;springer (tank)	Miroslaw Staron;Wilhelm Meding	2018		10.1007/978-3-319-91836-5		SE	-67.71168159038123	18.3192265846355	48542
b5e7afc98e96730779301e9867b036320169e2cc	performance impact of architectural decisions: integrating measurement in silo	future;architectural design;future internet;internet;silo;network architecture	Future Internet architectural design has attracted much research attention recently, and many novel architectural ideas have been suggested. The software and hardware realization of these architectural vision is a complex task in itself, and is worthy of research attention. In this paper, we examine a recently proposed integration of a measurement architecture in our previously described SILO architecture, and examine alternatives in realizing this integration. We demonstrate, by actual implementation and quantitative investigation, the importance of considering realization in architectural research by showing that there are unexpected factors affecting the performance of these realizations, resulting in unintended consequences, and we identify the better alternative.	future internet;silo;unintended consequences	Ahmet Can Babaoglu;Rudra Dutta	2011		10.1145/2002396.2002417	simulation;architectural pattern;systems engineering;engineering;architectural technology	Arch	-75.42588078601321	15.936925891379072	48697
db9870bb8b78464cd181146f00791890486986be	a doctoral program with specialization in information security: a high assurance constructive security approach	information security;specialization;theses;computer architecture;information assurance;teams personnel	A doctoral program in computer science with a specialization in information security is described. The focus of the program is constructive security. Key elements of the program are the strong computer science core upon which it builds, coursework on the theory and principles of information assurance, and a unifying research project. The doctoral candidate is a member of the project team, whose research contributes to the goals of the project and to fundamental advancements in high assurance security.	computer science;computer security;information assurance;information security;network security;partial template specialization;security engineering	Cynthia E. Irvine;Timothy E. Levin	2004		10.1007/1-4020-8145-6_15	certified information security manager;certified information systems security professional;computer science;systems engineering;knowledge management;software engineering	Crypto	-72.5914923985728	17.887862924615504	48708
d158f82b917054d4ac9e36821f6d0735d5e82045	collaboration engineering: designing repeatable processes for high-value collaborative tasks	mission critical systems;heart;collaborative work;design engineering;collaborative engineering;process design;humans;productivity;action research;design engineering process design collaborative work productivity mission critical systems international collaboration heart humans costs	By collaborating people can accomplish more than they could as separate individuals. Yet, achieving effective team collaboration remains a challenge. Organizations struggle to make collaboration work. They often resort to implementing technologies, while experiences show that technology alone seldom is the answer. Collaboration processes need to be explicitly designed structured and managed to maximize the focus of purposeful effort. This is at the heart of a new area of research: Collaboration Engineering - designing recurring collaboration processes that can be transferred to groups that can be self-sustaining in these processes using collaboration techniques and technology. Through a number of action research studies with a host of organizations we developed a Collaboration Engineering design approach. This paper presents and illustrates this approach in terms of its way of thinking, working, modeling and controlling. Finally, an agenda for future research is defined.	engineering design process	Gert-Jan de Vreede;Robert O. Briggs	2005	Proceedings of the 38th Annual Hawaii International Conference on System Sciences	10.1109/HICSS.2005.144	process design;productivity;knowledge management;action research;management science;management;heart	SE	-67.07420138738475	16.370479210500076	48713
2e5ee1c0d99068c90539425d1994c8cc8801154a	automated code review tools for security	common coding problem;static analysis;automated code review tools	Computer security has experienced important fundamental changes over the past decade. The most promising developments in security involve arming software developers and architects with the knowledge and tools they need to build more secure software. Among the many security tools available to software practitioners, static analysis tools for automated code review are the most effective. The paper presents how they work and why all developers should use them.	automated code review;computer security;software developer;static program analysis	Gary McGraw	2008	Computer	10.1109/MC.2008.514	software security assurance;programming;code review;computer science;information security;software development;operating system;software engineering;management;computer security;static analysis;best practice;static program analysis	SE	-63.61102994310641	30.014874979765388	48782
2b7903ffab0c81b9d8f42b2e5c50fce8232d4760	source code based function point analysis for enhancement projects	project management;software maintenance;function point;software systems;software systems software maintenance documentation application software books guidelines phase estimation bridges programming profession information analysis;function point analysis;source code;software maintenance source code function point analysis software systems software projects functional documentation legacy systems enhancement projects;legacy system;software development management software maintenance project management;software development management	Function point analysis is a well known established method to estimate the size of software systems and software projects. However, because it is based on functional documentation it is hardly used for sizing legacy systems, in particular enhancement projects. In this short note we sketch briefly how a Function Point Analysis can be based on the source code.	documentation;function point;legacy system;software system	A. Steven Klusener	2003		10.1109/ICSM.2003.1235445	project management;reliability engineering;personal software process;long-term support;verification and validation;team software process;software sizing;software project management;computer science;systems engineering;engineering;package development process;backporting;social software engineering;software framework;software development;function point;software engineering;software construction;software walkthrough;software documentation;software analytics;software maintenance;software deployment;software quality;software system;software peer review	SE	-64.43821037231159	27.02552437439451	48792
5cbea1a7fa22cde9fe7c6a05a0e53a2c7e92c53b	woped - an educational tool for workflow nets		WoPeD stands for Workflow Petrinet Designer and is an open-source software distributed over the Sourceforge platform. WoPeD’s focus lies on verification, visualization and explanation. Over the years, WoPeD has become a widely-used tool in the academic sector. This demo will provide information about recently added and improved features and functions. The demo will show how to use WoPeD as an e-learning tool to get a better understanding of fundamental BPM concepts.	game demo;open-source software;petri net;sourceforge	Thomas Freytag;Martin Sänger	2014			software;systems engineering;xpdl;workflow management system;visualization;petri net;workflow technology;workflow;knowledge management;computer science	Logic	-63.736114959569235	23.019815477211587	48803
49d5ee78a3ca325ad22d920ee6db87c94e63592e	business process design and implementation for customer segmentation e-services	outsourcing;electronic commerce;outsourcing customer services internet electronic commerce;process design application specific processors costs companies portfolios profitability automation helium informatics customer relationship management;customer services;business process design;internet;application service provider;business process;big bang;internet application service providers asp model e crm services business process design customer segmentation e service	"""Sourcing CRM services on the Web via application service providers (ASP) is a key business tactics for reducing the total ownership costs and implementation risks linked to """"big bang"""" CRM implementations. To deliver CRM through the ASP model it is imperative to have a detailed map of the business process and implementation framework underpinning the delivered service. This paper contributes to the e-CRM implementation landscape by providing a detailed account of the business process design and implementation support for a customer segmentation e-service aimed at supporting a best of breed and piecemeal approach to CRM sourcing."""	bang file;business process;customer relationship management;e-services;imperative programming;world wide web	Pedro R. Falcone Sampaio;Yong Jun He	2005	2005 IEEE International Conference on e-Technology, e-Commerce and e-Service	10.1109/EEE.2005.43	service level requirement;customer to customer;customer relationship management;business service provider;business process reengineering;artifact-centric business process model;marketing;electronic business;process management;business;business process;customer retention;business process modeling;service quality;commerce;customer advocacy	Robotics	-73.73313635067579	7.96693924174302	48947
9487baccfbb45919bdff643232412fcba0e888ed	sources of strategic fit in high-tech firms	business and management studies;quantitative;europe;business and corporate strategy	This study analyses the concept of strategic fit as a factor explaining organisational performance. We propose that strategic fit should include aspects of environment perception as well as the gap between planning processes and strategic implementation. Studying the firm's capacity to reduce this gap, we identify various organisational capabilities and analyse their relationships. Through managers’ responses from a sample of firms in European high-technology sectors, we find sufficient empirical evidence to affirm that strategic fit facilitates improvement in organisational performance. We also observe that strategic flexibility, real options and organisational learning are sources of strategic fit. So-called learning organisations facilitate generation of options that give the organisation the strategic flexibility needed to achieve fit with the environment. Strategic flexibility is defined in both the adaptive and anticipative sense. Our article thus offers a new perspective to contribute to better und...		Ignacio Tamayo-Torres;Antonio J. Verdú-Jover;Víctor Jesús García-Morales	2012	Techn. Analysis & Strat. Manag.	10.1080/09537325.2012.723125	strategic planning;quantity;knowledge management;marketing;operations management;strategic financial management;pest analysis;strategic sourcing;profit impact of marketing strategy;management;strategic thinking;strategic alignment;strategic control;strategic fit	NLP	-79.61173617278885	5.047358597493684	49035
9fbd63e2b648d3f165a0164501aac9020f620fcd	technical debt-related information asymmetry between finance and it		This position paper proposes a new stream of research targeted at technical debt as a source of information asymmetry between finance and IT professionals involved in information technology investment decisions. Finance teams interact with technology teams in several ways, predominantly when business cases require review and during the annual budgeting process. During these discrete interactions, finance teams are required to digest large amounts of technical strategy and architectural information chock-full of technical terminology and diagrams. Typically, the estimates for effort are soft and risk is difficult to measure. It is within this context that finance approves budgets and projects that inevitably result in the accumulation of technical debt. This paper discusses some of the dynamics at work between finance and IT teams within large complex organizations when they meet to make technology investment decisions. In addition, future research is proposed aimed at reducing information asymmetry, thereby leading to improved IT investment decisions and better management of technical debt.1		Thomas Stablein;Donald J. Berndt;Matthew Mullarkey	2018	2018 IEEE/ACM International Conference on Technical Debt (TechDebt)	10.1145/3194164.3194180	position paper;business;information technology;finance;terminology;technical debt;investment decisions;information asymmetry;business case	SE	-74.773714051605	10.871437422048558	49096
da8ff68fbc79d889f6f9504453d24832d050673d	perceptions of the benefits from the introduction of case: an empirical study	case methodology;system delivery time;human resource;considerable productivity;empirical study;managerial infrastructural factor;computer-aided software;uk company;quality gain;successful introduction;software engineering;information engineering;software development	"""Computer-aided software engineering (CASE) tools have generated much interest as potential means for easing the software development and maintenance bottleneck. To date, the picture regarding their contribution is incomplete and confused, particularly concerning the realization of productivity and quality gains. An in-depth study of one company’s experiences with the introduction of CASE is described. Quantitative data is available to allow objective comparison of changes in productivity and IS quality consequent upon the CASE introduction. Questionnaires were used to determine the perceptions of both developers and their customers to the new methodology and tools. The importance to the successful introduction of CASE of the human resource, technical, and managerial infrastructural factors are also investigated. Introduction Over the last five years a number of firms have turned to Computer-Aided Software Engineering (CASE) methodologies as a means to enhance the effectiveness and efficiency of information systems (IS) development (for example, Banker and Kauffman, 1991). CASE tools have generated much interest among researchers and practitioners as potential means for easing the software development and maintenance bottleneck (Orlikowski, 1993). To date, the picture regarding the contribution of CASE is incomplete and confused, particularly concerning the realization of productivity gains in systems development. 1 This paper seeks a greater understanding of this contribution. It describes the results of an indepth study of the introduction of CASE into one large British manufacturing company. It records the productivity and quality gains achieved in the company’s IS development and the perceptions of participants to the initiative. Additionally, the organizational factors that impact on success are identified. The paper is structured as follows. Rrst the company in which the empirical study took place is described. Then the aims of the research are stated, previous literature is reviewed, and the research questions are posed. Next, the data sources including the characteristics of the questionnaire respondents are described. Then the detailed findings including tangible systems outcomes, the perceptions of these tangible outcomes, and perceptions of the associated, less tangible outcomes, are offered, and the """"infrastructure"""" within which the systems development process takes place is discussed. Finally, the additional findings that emerged from the study and the summary and conclusions are presented. MIS Quarterly~December 1994 353"""	computer-aided software engineering;information system;management information systems quarterly;software development process	Paul N. Finlay;Andrew C. Mitchell	1994	MIS Quarterly		information engineering;systems engineering;engineering;marketing;operations management;software development;management science;empirical research;management	SE	-79.75918188104134	9.379764711216945	49311
769107f3a552be0a6f5ddc59e8d982b9eca476fa	knowledge map and enterprise ontology for enhancing business process reengineering in healthcare: a case of radiology department		The spending in healthcare is constantly increasing according to available data, and the resource consumption does not actually reflect expected improvement in the healthcare services. As a result there is urgent need for healthcare to reengineer their business process in order to achieve improvements in critical area such as cost and quality. To address this issue, this study proposed a method based on design science approach by integrating knowledge map, enterprise ontology and lean in order to find the non-value added transactions and subsequently reengineer them to enhance healthcare efficiency. Enterprise ontology provides a details and better understanding of dynamic nature of an organization as such was chosen for as basis for this study. The method was demonstrated in radiology department making it possible to find and addresses non-value added transactions and subsequently reengineer the department. Evaluation was carried out by means of descriptive method using Moody and Shank framework and interview with healthcare practitioners. The finding shows that the method has enhanced healthcare efficiency by eliminating non-value added transactions.	business process;code refactoring;knowledge management;radiology	Mahdi Alhaji Musa;Mohd Shahizan Othman	2016	IJEIS	10.4018/IJEIS.2016040103	engineering;knowledge management;marketing;operations management;data mining;database;management;world wide web	ML	-71.46473070530823	11.145970876241233	49337
6ef43e9c1ea5685f18b021970316ced35e5408b4	when good enough software is best	software engineering;parameter optimization good enough software fast development cheapness correct software software development organization customer desires critical project parameters cost schedule staffing functionality quality;software development;costs scheduling programming gratings application software software systems constraint optimization maintenance electric shock calculators	nently on his wall: “You can get it fast; you can have and technology to it cheap; you can get it right. Pick mo!” That same sign could be displayed on the wall of e v q sojinarework together. d evelopent organization. And yet most of our customers want all three. Ed Yourdon tackles that dilemma in this issue’s column. He contend that we don’t rationally establish proper balance among the critical project parameters: cost, schedule, stafing, finctiimality, and quality. Our customers want 11s to optimize all these parameters, even when this is clearly impossible. The purists among you may find Ed’s comments grating. But I w p e c t that those of you who’ve been bloodied in the pro-ject wars will find wisdom in his work. Roger P r e m n	pick operating system;principle of good enough	Edward Yourdon	1995	IEEE Software	10.1109/52.382191	personal software process;long-term support;verification and validation;software quality management;software sizing;computer science;systems engineering;engineering;package development process;backporting;operations management;software development;software design description;software engineering;software construction;software walkthrough;software deployment;software quality control;software development process;software quality;software metric;software quality analyst	Arch	-69.1024232477592	26.57875059780879	49407
33c9700a1725a9eff5085248cc3419fa8a0b6a6c	the 24-hour knowledge factory: can it replace the graveyard shift?	computers;software;night work;collaboration;companies;collaborating centers knowledge factory night work evolving knowledge;computing profession;collaborating centers;production facilities;evolving knowledge;workflow management software;call centres;organizations;workflow management software call centres;production facilities testing collaborative work manufacturing costs programming australia packaging data engineering design engineering;computing profession knowledge factory collaborative computing;collaborative computing;programming;knowledge factory	Collaborating centers in time zones six to eight hours apart can transfer work so that every center is working during the daytime. Although this concept avoids the hazards of night work, it requires careful planning and a way to automatically capture evolving knowledge.		Amar Gupta	2009	Computer	10.1109/MC.2009.25	programming;organization;knowledge management;management;collaboration	HCI	-67.22859339248258	19.06864689553285	49440
da1f2faf68ce008eae5b96a038eed80da7e22e2d	information architectural design in business process reengineering	information management system;information architecture;architectural design;information systems security;gestion entreprise;science gestion;mis systems;information systems research;restructuration;proceso;journal of it;project management;jit;teaching cases;organization management;etude theorique;information security;case studies;information science;information security systems;information technology;production system;business information technology;security information systems;systeme production;it journals;restructuracion;firm management;technologie information;information systems management;organizacion proyecto;it teaching cases;integration;sistema produccion;operational research society;business model;modelo;journal of information technology teaching cases;architecture information;computer information systems;reingenierie;jit journal;geographic information systems;integracion;information technology journal;information management;information systems journals;information systems technology;estudio teorico;managing information systems;processus;accounting information systems;information and management;management information systems;gestion projet;define information systems;administracion empresa;strategic information systems;modele;business information management;concepcion arquitectural;soft system methodology;process;information system;health information systems;computer information technology;journal of information technology;theoretical study;gestion organizacion;business information systems;conception architecturale;tecnologia informacion;business systems analyst;reengineering;business process reengineering;models;systeme information;gestion organisation;journal information technology;it journal;management science;journal of information systems;business process reengineering bpr;sistema informacion;information technology journals	Business process reengineering and information architecture share a common strategic and business process focus. Both can be mutually supportive of each other's objectives. Information architecture design can produce a stable IA capable of supporting existing as well as improved business processes. Reciprocally, business process redesign (BPR) provides a high profile business justification for the IA endeavour. Given proper collaboration between corporate and IT strategic planners, both BPR and IA efforts should produce a number of valuable common outputs. These include the identification of business processes within an organization, the prioritization of these processes based on their strategic relevance, the establishment of process performance measures, and the modelling of these processes and their supporting information resources. A synergistic model of IA and BPR is presented and selected IA techniques and modelling methods are recommended. Future research is suggested concerning the need to test the relationship between BPR and IA.	business process;code refactoring	William J. Kettinger;James T. C. Teng;Subashish Guha	1996	JIT	10.1080/026839696345405	business process reengineering;computer science;systems engineering;engineering;knowledge management;artifact-centric business process model;business process management;electrical engineering;process modeling;management information systems;business process model and notation;process management;business system planning;business process;business process discovery;management;business rule;information technology;business process modeling;information system;business architecture	EDA	-68.77129121070556	6.8874706697325125	49443
a29368f692c6371188573da235ae639a7f875b1d	the end of software engineering and the start of economic-cooperative gaming	resource limitation;software systems;software engineering;cooperative game;software development;on the fly;failure prediction	"""""""Software engineering"""" was introduced as a model for the field of software development in 1968. This paper, reconsidering that model in the light of four decades of experience, finds it lacking in its ability to explain project success and failures, predict important issues in running projects, and help practitioners formulate effective strategies on the fly. An alternative underlying model for software development is presented: Software development as a series of resource-limited, goal-directed cooperative games of invention and communication. The primary goal of each game is the production and deployment of a software system; the residue of the game is a set of markers to assist the players of the next game. People use markers and props to remind, inspire and inform each other in getting to the next move in the game. The next game is an alteration of the system or the creation of a neighboring system. Each game therefore has as a secondary goal to create an advantageous position for the next game. Since each game is resource-limited, the primary and secondary goals compete for resources. The cooperativegame model provides the benefits that the software engineering model misses: It raises to the proper priority level issues crucial to successful software projects; it explains how teams with messy-looking processes sometimes outperform others with tidier processes; and it helps busy practitioners decide how to respond to unexpected situations. Finally, it is seen that much of engineering in the general belongs in the category of resource-limited, cooperative games."""	on the fly;software deployment;software development;software engineering;software system	Alistair Cockburn	2004	Comput. Sci. Inf. Syst.	10.2298/CSIS0401001C	non-cooperative game;game design;game development tool;simulation;software engineering process group;computer science;knowledge management;artificial intelligence;software development;game mechanics;software engineering;machine learning;game art design;data mining;game developer;screening game;simulations and games in economics education;software walkthrough;game design document;management;software deployment;computer security;game testing;software system	SE	-69.76535954930465	24.818256270107064	49534
e9cd4a83decde62df4a38c40054ed17711e50f96	non-functional property driven service governance: performance implications	general and miscellaneous mathematics computing and information science;performance software architecture;communications;implementation;functional properties;software architecture;business;performance analysis;service sector;enterprise service bus;management;communication pattern;business process;communication service	Service governance is a set of businesses processes, policies and technical solutions that support enterprises in their implementation and management of their SOA. The decisions of service governance, especially concerning service boundaries at the enterprise level, influence the deployment topology of business services across or within business organizations. Deployment topologies are realized by integration technologies such as Enterprise Service Bus (ESB). Service governance and technical solutions interact in a subtle way including through communication patterns and protocols between services and ESBs, as well as the deployment and configuration of ESB. These factors have a strong influence on the Non-Functional Properties (NFP) of a SOA solution. A systematic approach is essential to understand alternative technical solutions for a specific service governance decision. This paper proposes a modeling approach to evaluate the performance-related NFP impacts when mapping service governance to technical solutions using an ESB. This approach is illustrated by the quantitative performance analysis of a real world example, service governance from an Australian lending organization.	dhrystone;enterprise service bus;interaction;non-functional requirement;performance evaluation;performance prediction;proxy server;routing;soa governance;service-oriented architecture;software deployment;software performance testing;stress testing;web mapping	Yan Liu;Liming Zhu;Leonard J. Bass;Ian Gorton;Mark Staples	2007		10.1007/978-3-540-93851-4_6	tertiary sector of the economy;service provider;service level requirement;service level objective;software architecture;service catalog;service product management;differentiated service;computer science;knowledge management;service delivery framework;service design;database;service;business process;data as a service;implementation;management	Networks	-77.53983769826702	13.030158138539573	49572
4672c46d4944f430b35ae51057a6ab4555a885f2	enhancing process compliance in complex product developments: insights from an international manufacturer	sbd;complexity;s bpm;process compliance;sid;product development	Poor process compliance oftentimes leads to insufficient fulfillment of strategic goals. To prevent this, adequate initiatives to achieve process compliance and consequently to achieve corporate objectives need to be taken. In this context, the product development represents a significant core process to achieve strategic goals for manufacturing companies.  Using the example of an international manufacturer, this contribution describes the derivation of effective actions to increase compliance with the product development process by using S-BPM models. The relevant stakeholders as well as the role specific tasks and responsibilities are identified assessing interactions and behaviors applying Subject Oriented modeling notations. Based on this basic information, we conduct semi-structured interviews with relevant stakeholders to identify the problem areas where process users need support. Moreover, we conclude implications to increase process compliance and show how to translate these implications into actual support content such as templates and checklists	beam propagation method;interaction;new product development;semiconductor industry	Peter Schott;Matthias Lederer;Sebastian Huber;Alexander Keppler	2015		10.1145/2723839.2723860	complexity;computer science;systems engineering;engineering;operations management;management science;management;new product development	Mobile	-76.44626522094791	9.60685040383014	49577
c533bdf2171b70f932a6435e12ce7c3cb033cb4e	chapter four - characterizing software test case behavior with regression models			software testing;test case	Bryan Robbins	2017	Advances in Computers	10.1016/bs.adcom.2016.12.002		SE	-63.214800802122454	27.704207188181197	49591
f34d614dbd299b22c2f73d0b0e4e1842ff0edf56	quantitative analysis of requirements evolution across multiple versions of an industrial software product	software metrics;software;formal specification;industrial software product;standards organizations;biological system modeling;software organization quantitative analysis requirements evolution industrial software product software engineering defect estimation;software engineering;software houses;formal verification;systems analysis formal specification formal verification software houses software metrics;estimation;quantitative analysis requirements evolution measure analysis;systems analysis;software correlation organizations standards organizations biological system modeling estimation;software organization;requirements evolution;measure;quantitative analysis;analysis;organizations;correlation;defect estimation;business rules	Requirements evolution is one of critical problems influencing software engineering activities. Despite there is much research on requirements evolution, there still lacks quantitative understanding of requirements evolution. In this paper, we quantitatively analyze requirements evolution across multiple versions of an industrial software product. Based on data of requirements evolution and defects, we analyze the relationship between requirements evolution and requirements as well as between defects and requirements evolution. We also analyze the evolution characteristics about requirements modification. Our findings include estimation of the number of defects using evolved requirements may increase accuracy of defect estimation and business rule is the most volatile part in requirements. These findings deepen our understanding of requirements evolution and can help software organizations manage requirements evolution.	evolution;requirement;self-replication;software bug;software engineering	Hailong Wang;Juan Li;Qing Wang;Ye Yang	2010	2010 Asia Pacific Software Engineering Conference	10.1109/APSEC.2010.15	reliability engineering;systems analysis;requirements analysis;estimation;software requirements specification;requirements management;requirement prioritization;measure;formal verification;computer science;systems engineering;quantitative analysis;organization;requirement;software engineering;system requirements specification;analysis;formal specification;non-functional testing;programming language;business rule;correlation;non-functional requirement;requirements traceability;software metric	SE	-63.989537419331384	28.836302124509537	49600
ad0a3e13fa06955ffecfe4116eca7ec70b70333f	context-based project management		Context-based computing has become an integral part of the software infrastructure of modern society. Better software are made adaptive to suit the surrounding environment. Context-based applications best fit into environments that undergo constant and frequent changes. Temperature management, Time management, GPS are just few examples where context-awareness becomes inevitable. Project Management is another domain that requires constant monitoring. The current tools of project management handle data gathering, plotting, and organizing, but requires high-level of human intervention to analyze data and integrate it. To the extent of our knowledge there is no efforts to introduce context awareness to project management domain. In this work, we introduce context and formally model project context using FCA. Additionally, we provide the results of the full implementation of our approach on a real-world software project. We show that our approach can formally answer queries that traditional tools could not answer. Also, we introduce a brief comparison between our approach and traditional project management software. Finally, we show that our approach can improve project management tools and minimize the effort spent by project managers.	comparison of project management software;context awareness;curve fitting;floor and ceiling functions;formal concept analysis;global positioning system;high- and low-level;organizing (structure);package manager;software project management	Ammar Alsaig;Alaa Alsaig;Mubarak Mohammad	2016		10.1007/978-3-319-56357-2_2	extreme project management;management science;project management;opm3;project risk management;computer science;project management triangle;project portfolio management;project planning;software project management	SE	-64.7823985806223	11.535419435804148	49652
1c5fee5bad3411cf2fcb861f25b2fd29d78a5d3c	collaborative software engineering	collaborative software engineering;swinburne	Read more and get great! That's what the book enPDFd collaborative software engineering will give for every reader to read this book. This is an on-line book provided in this website. Even this book becomes a choice of someone to read, many in the world also loves it so much. As what we talk, when you read more every page of this collaborative software engineering, what you will obtain is something great.	collaborative software;online and offline;online book;software engineering	Alan C. Brown	2001		10.1007/978-3-642-10294-3	personal software process;verification and validation;software engineering process group;systems engineering;engineering;knowledge management;package development process;social software engineering;component-based software engineering;software development;software engineering;software construction;software walkthrough;software analytics;resource-oriented architecture;software deployment;software development process;software requirements;collaborative software;software system;software peer review	SE	-64.30804272170855	24.922763372918244	49682
bff04b41e5fea577f1de820388ea3331da1ee37b	balancing strategic interests and technological requirements for mobile services	mobile;business models;organizational networks;mobile technology	Designing business models for mobile services is a complex undertaking because it requires multiple actors to balance different design requirements. A business model can be seen as a blueprint of four interrelated components or domains: service, technology, organization and finance domain. Little attention has been paid to how these different domains are related to one another. This knowledge is needed to enhance our understanding of what constitutes a viable business model. In this paper the interdependencies between two of these domains, namely organization and technology domain, are explored by analyzing critical design issues in business models for mobile services, i.e. partner selection, network openness, network governance, and network complexity in the organization domain, and security, quality of service, management of service profiles, system integration and accessibility in the technology domain. A causal framework is developed, which links these critical design issues to expected network value and business model viability.	accessibility;blueprint;causal filter;interdependence;network governance;openness;quality of service;requirement;system integration	Timber Haaker;Edward Faber;Harry Bouwman	2004		10.1145/1052220.1052298	business model;business domain;business requirements;systems engineering;knowledge management;artifact-centric business process model;management science;industrialization of services business model;business;mobile business development;business rule;new business development;business process modeling	Mobile	-74.24057132566628	8.706693430691233	49776
a380a73786612500309a5fc36d6f56ce571e4ac4	practicing what we preach: understanding inhibitors to the faithful use of project management practices		Effective management of software development efforts is one of the most challenging aspects within the IT discipline today. Literature has greatly extended our understanding of project management (PM) practices that enhance the likelihood of software development project success, such as the use of scope change control to manage project changes or structured walkthroughs to enhance product congruence with customer expectations. Nevertheless, a disconnect seems to exist between a software developer’s knowledge of valuable PM practices and their faithful use of those practices. The research model presented in this dissertation seeks to address key antecedents to a software developer’s faithful use of PM practices. The model builds on previous literature by clarifying the role of perceived usefulness in an individual’s usage decision, addressing temporal aspects of the intention to use – usage relationship, examining institutional factors that impact the actual use of a PM practice, and considering drivers of faithful versus ceremonial usage. The ultimate goal of this research is to provide guidance regarding the following question: what factors encourage a software developer to faithfully utilize PM practices?	best practice;change control;software developer;software development;software project management;zeller's congruence	Jeff Crawford	2005			knowledge management;political science;management science;pedagogy	SE	-74.34066814632783	21.437207808113474	49817
4059dc29a1c670d1d07d15cb278c3555716fa278	java file security system (jfss) evaluation using software engineering approaches		A Java File Security System (JFSS) [1] has been developed by us. That is an ecrypted file system. It is developed by us because there are so many file data breaches in the past and current history and they are going to increase day by day as the reports by DataLossDB (Open Security Foundation) organization, a non-profit organization in US so it is. The JFSS is evaluated regarding the two software engineering approaches. One of them is size metric that is Lines of Code (LOC) in the software product development. Another approach is the customer oriented namely User Satisfaction Testing methodology. Satisfying our customers is an essential element to stay in business in modern world of global competition. We must satisfy and even delight our customers with the value of our software products and services to gain their loyalty and repeat business. Customer satisfaction is therefore a primary goal of process improvement programs as well as quality predictions of our software. With the help of User Satisfaction Index that is calculated for many parameters regarding the customer satisfaction. Customer Satisfaction Surveys are the best way to find the satisfaction level of our product quality.	customer relationship management;data breach;java;new product development;open security;software engineering;source lines of code	Brijender Kahanwal;Tejinder Pal Singh	2012	CoRR		reliability engineering;computer user satisfaction;systems engineering;engineering;software engineering;customer satisfaction;customer retention;management	SE	-66.32362697639196	28.198380424066997	49843
0a2ef7f2f2dacba95f8a34701194eb845fcdd0fb	experiences in using practitioner's checklists to evaluate the industrial relevance of requirements engineering experiments		Background: A grand challenge for Requirement Engineering (RE) research is to help practitioners understand which RE methods work in what contexts and why. RE researchers recognize that for an RE method to be adopted in industry, RE practitioners should be able to evaluate the relevance of empirical studies to their practice. One possible approach to relevance evaluation is the set of perspective-based checklists proposed by Kitchenham et al. Specifically, the checklist from the practitioner's perspective seems to be a good candidate for evaluating the relevance of RE studies to RE practice. However, little is known about the applicability of this checklist to the RE field. Moreover, this checklist also requires a deeper analysis of its reliability. Aim: We propose a perspective-based checklist to the RE community that allows evaluating the relevance of experimental studies in RE from the practitioner's/consultant's viewpoint. Method: We followed an iterative design-science based approach in which we first analyzed the problems with a previously published checklist and then developed an operationalized proposal for a new checklist to counter these problems. We performed a reliability evaluation of this new checklist by having two practitioners apply the checklist on 24 papers that report experimental results on software requirements specifications' comprehensibility. Results: We report first-hand experiences of practitioners in evaluating the relevance of primary studies in RE, by using a perspective-based checklist. With respect to the reliability of the adjusted checklist, 9 of out 19 questions show an acceptable proportion of agreement (between two practitioners). Conclusions: Based on our experience, the contextualization and operationalization of a perspective-based checklist helps to make it more useful for the practitioners. However, to increase the reliability of the checklist, more reviewers and more discussion cycles are necessary.	experience;grand challenges;iterative design;iterative method;relevance;reliability engineering;requirement;requirements analysis;requirements engineering;software requirements specification	Maya Daneva;Klaas Sikkel;Nelly Condori-Fernández;Andrea Herrmann	2018	2018 IEEE/ACM 6th International Workshop on Conducting Empirical Studies in Industry (CESI)	10.1145/3193965.3193966	systems engineering;engineering;contextualization;empirical research;management science;operationalization;checklist;requirements engineering;software requirements	SE	-70.89655104761768	22.662640162975602	49989
e3b350d0417a95e5f2678d419cadede39e26be81	it indicators and organizational performance: a study of the retail sector in brazil		Technology as a means to meet the needs of an increasingly competitive and demanding market is a reality. This is also true in the retail context. Due to this increases the IT importance that needs to be managed in an increasingly efficient and transparent way to the business areas. As IT becomes more strategic Information Technology management indicators need to be increasingly aligned with business strategy and become once they are an important communication tool in the operating, projects and innovation dimensions. The aim of this study is to identify the importance of IT indicators in a retail company and its relationship with organizational performance. To meet this goal, this study aimed to identify the use of IT performance indicators related to large companies performance in the retail sector. The results showed that: (1) the management of IT indicators in retail is still related to operational management of IT; (2) the performance and value of IT as a whole is still measured based on perception; and (3) the use of IT performance indicators related to business can help IT value of communication in organizations.	amiga reflections;business process;business requirements;capability maturity model;information security;knowledge management;relevance;strategic management;theory	Heverton Roberto Oliveira Cesar de Moraes;Maria Cunha;Marco Alexandre Terlizzi	2017			organizational performance;environmental resource management;information technology;marketing;business	HCI	-78.4003472251752	7.029276101474855	50108
b69ded3b71e033afec34db0b0c790df982413b8b	controlling schedule duration during software project execution	g000 computing and mathematical sciences	This thesis describes a method of identifying the influences on schedule delays in projects that develop large software systems. Controlling schedule duration is a fundamental aspect of managing projects because of the financial losses associated with late projects. While challenges with controlling software projects have been investigated, there still seemed to be more to be learned about the interplay of a range of factors during project execution and that affect project duration when developing and integrating software systems within enterprise architecture environment.	software project management	Zana Ahmedshareef	2015			team software process;real-time computing;software project management;computer science;systems engineering;software engineering;analysis effort method;schedule	SE	-68.26226701872982	22.456587599984857	50223
6e2b7da5b35966fa0269a61fdf162086671543f2	e-business standards setting		Many industry sectors are facing a number of challenges to the established relations between players (the automotive sector is a particularly prominent case in point; see also Gerst & Jakobs, 2006). To meet the production requirements, standardization of processes, systems, and data are inevitable. A current trend in manufacturing is that OEMs1 attempt to cooperate with fewer suppliers, but on a worldwide scale. The use of ICT2 related technologies, particularly ebusiness systems, facilitates the creation of a network of relationships within a supply chain. Yet, such inter-organizational integration requires interoperability that cannot be achieved without widely agreed standards. But how should standards be set, and who has—or should have—a say in the standardization process? In many cases, an SME3 supplier does business with more than one OEM. In this situation, bi-lateral standardization to improve the cooperation between OEMs and suppliers, and between different suppliers, respectively, is inefficient. Still, this has been the approach of choice in many cases.4 However, possible alternatives are available. In the automotive industry, for example, portals were developed as a form of sector-specific harmonization. Yet, these attempts to develop standardised technology largely failed. This holds particularly for the most prominent example, Covisint. Its failure may be attributed to various technical, organizational, and economic reasons. The main contributing factors, however, included the unequal power distribution during the development process (only the large OEMs had a say; the suppliers were largely left in the cold), and the equally imbalanced distribution of benefits (which mirrored the power distribution). The fact that Covisint was sector-specific probably represented another problem as many suppliers did not only do business within the automotive sector, but with other industries as well (see Gerst et al. (2006) for a far more detailed discussion of this subject). This rather negative example suggests that perhaps yet another alternative approach should be deployed. One straightforward such alternative would be to take these activities to a dedicated standards organization. After all, portal technology relies heavily on underlying e-business standards such the extended markup language (XML), the UDDI registry (universal description, discovery, and integration), the Web services description language (WSDL), SOAP, and many others. Moreover, many of these organizations offer a more level playing field for smaller companies, certainly in theory (see Jakobs (2004) for a perhaps more realistic view).		Kai Jakobs	2007			interoperability;business;standards organization;supply chain;electronic business;standardization;soap;commerce;automotive industry;level playing field	AI	-74.20732921759566	7.300450019451512	50335
71ca088a684cf4cb7de14275bf608dd8116018aa	corporate recordkeeping: new challenges for digital preservation		In this paper, we describe an innovative approach to the challenges associated with managing corporate records in the digital environment. Issues and problems with the use of EDRMS are well documented but alternatives are not yet mature enough for workplace implementation. The recordkeeping functionality of Microsoft SharePoint is disputed by practitioners, but this enterprise content management system appears to be emerging as a default solution to manage records. Applying genre theory in the configuration of SharePoint will assist records managers in negotiating shared understanding with their information technology colleagues which is essential in order to achieve digital preservation objectives.	content management system;digital environment;enterprise content management;sharepoint	Gillian Oliver;Fiorella Foscarini	2011			world wide web;digital preservation;internet privacy;business	OS	-73.86640746421773	4.747914779640358	50386
f42972cefd4156752f37e1df710c9918dc1ad114	engineering knowledge-intense, personoriented services - a state of the art analysis		This paper provides a state-of-the-art analysis of service engineering (SE) approaches for knowledgeintense person-oriented (KIPO) services, focussing on IT-enabled provision of such services. Key attributes are derived that distinguish KIPOs from other services. These attributes are integrated in a framework with regard to their applicability on KIPOs development and used for a systematic literature review. KIPOs are of high economic relevance, yet they are laggards in terms of realization of IT potentials. As the most value-creating activities in service provision are bound to persons or personal knowledge, KIPOs design is complicated. The analysis reveals several gaps in SE research. In particular, identified shortcomings of existent approaches are an insufficient level of detail, i.e. no concrete actions or methods for deployment are described, a lack of practical corroboration as well as insufficient IT support. Further, current approaches are not sufficiently equipped to handle the interplay between people-bound activities and technical components. This paper contributes to IS research by clearly identifying these gaps in SE methods. It further provides researchers with ideas for future research activities and guides practitioners in selecting methods that serve as candidates to be integrated into KIPOs development in order to leverage IT potentials more systematically and efficiently.	level of detail;relevance;software deployment;systematic review	Philipp Menschner;Christoph Peters;Jan Marco Leimeister	2011			software deployment;knowledge management;leverage (finance);systematic review;personal knowledge base;computer science	Web+IR	-68.24337545773929	11.79112115617481	50420
e8413fb4937da02b0c3450a103b4dcc9ab389a27	critical infrastructure security curriculum modules	scada;course modules;control systems;information security;critical infrastructures;risk management;emergency preparedness;vulnerability;control system;programmable logic controllers;cybersecurity;critical infrastructure;security;programmable logic controller	Critical infrastructures have succumbed to the demands of greater connectivity. Although the scheme of connecting these critical equipment and devices to cyberspace has brought us tremendous convenience, it also enabled certain unimaginable risks and vulnerabilities. The importance of critical infrastructure (CI) protection has never been pronounced and we are in a juncture in history where CI security is paramount. Although research in this area of national need has grown steadily, pedagogical materials in this area is slow to keep up. This paper presents the development of course modules for critical infrastructure security curriculum. Although these course modules can be used to augment an existing course in CI, they can also be utilized as bases with which to build a complete CI course. Existing laboratory setups which can be used to supplement the course are also described. The course modules and the supplemental laboratories are envisioned to be great instruments for training future information security professionals. These pedagogical materials can also be used as supplements to other courses that pertain to information security, risk management, or emergency preparedness.	critical infrastructure protection;cyberspace;information security;risk management	Guillermo A. Francia	2011		10.1145/2047456.2047464	control system security;computer science;control system;information security;programmable logic controller;computer security;information security management	Security	-63.89420536950971	7.8466002441477345	50449
f84311c5f2b3c6ba1b850ee3c70b6283bf971a87	comparing school ownership performance using a pseudo-panel database: a malmquist-type index approach	malmquist index;data envelopment analysis;productivity and competitiveness	The Malmquist Index methodology is widely used in production economics for measuring the productivity changes of a set of decision making units (DMUs) within different time periods. Moreover, it is quite usual in the management literature to compare public and private DMU performance operating under the same economic sector. However, the standard Malmquist index needs a panel database to be implemented, i.e. it focuses on analyzing the evolution of the same DMUs over time. In this paper, we propose a different way of using the Malmquist index that allows us to further analyze the performance divergences between public and private DMUs when only a pseudo-panel database is available. The paper extends Camanho and Dyson’s oneperiod Malmquist-type index (Camanho and Dyson, 2006) when a pseudo-panel database is available. We apply this methodology to examine the differences in public high school and private government-dependent high school productivity for three Spanish Regions -Catalonia, Castile-Leon and Basque Countrywhich entered representative samples in PISA 2003, 2006 and 2009. Finally, in order to obtain a measure of the significance of results, we use and adapt the bootstrap methodology proposed by Simar and Wilson (1999) to obtain the confidence intervals for the Malmquist-type indices and their components. The results suggest a persistent and significant higher performance of private government-dependent schools over public institutions in all three Spanish regions. Moreover, this difference persists over time.	best practice;best, worst and average case;black box;bootstrapping (statistics);cloud data management interface;database;digital mockup;dyson (operating system);inverted index;leon;liburuklik;panel data;persistence (computer science)	Juan Aparicio;Eva Crespo-Cebada;Francisco Pedraja-Chaparro;Daniel Santín	2017	European Journal of Operational Research	10.1016/j.ejor.2016.06.030	economics;computer science;operations management;data envelopment analysis;mathematics;economy;management	DB	-85.6364083459039	8.109260576041383	50462
954fce9e3b195552097e8d18756ee2fd2eb40160	small slovene firms and strategic information technology usage	selected works;information technology;bepress;small business	The extent to which information technology (IT) is used strategically is measured in a sample of 147 small Slovene firms. Slovenia is interesting from a small business perspective, because from 1990, when the transformation of its economy started, the number of small business has increased almost 6 times (from almost 6.500 to nearly 35.000 in 1998). The results have shown IT industry leadership and also IT’s role in a firm to be the strongest predictors in the strategic usage of IT. It was also found that IT is particularly well utilized in firms emphasizing innovation and, to a lesser degree, an efficiency strategy. Firms pursuing a low-cost strategy were the least likely to utilize IT strategically.	branch predictor;mike lesser	Dusan Lesjak;Monty L. Lynn	2000			marketing;law;information technology;commerce	ECom	-83.0510093028216	5.6922263114323295	50538
4eff13561339f512e8a4a56f304b2cff0805b011	risk analysis of energy performance contracting projects in russia: an analytic hierarchy process approach	risk analysis;ahp;analytic hierarchy process	Systematic and effective risk management in energy performance contracting (EPC) projects requires a sound understanding of the main risks faced by energy service companies (ESCOs) and other energy service providing companies (ESPCs), which accomplish such projects under vulnerable market conditions in Russia. This study explores the EPC project risks (risk factors and their causes) and develops a risk analysis framework that is applied to three Russian sectors: (1) industrial; (2) housing and communal services; and (3) public. The identified general risks were validated by Russian EPC practitioners in expert interviews. An analytic hierarchy process (AHP) approach was then used to rank the identified risks in terms of their contribution to the riskiness of EPC projects. The data were obtained from a web-based questionnaire survey conducted among Russian ESCOs and ESPCs. For improving consistency of the obtained AHP results, the maximum deviation approach (MDA) for 8×8 matrices and the induced bias matrix model (IBBM) for 3×3 and 4×4 matrices were applied. This study indicates that there is a need for a widely usable formal approach for risk analysis and management in EPC projects in Russia. Causes of risk related to the financial and regulatory aspects were found to contribute most to the riskiness of EPC projects performed in all three focus sectors in that country.		Maria Garbuzova-Schlifter;Reinhard Madlener	2015		10.1007/978-3-319-42902-1_92	environmental resource management;operations management;business;commerce	SE	-78.80659403611021	9.993300693170035	50545
1d006be9bda65dee580143fb32da1bbb383b9651	request based virtual organisations (rbvo): an implementation scenario	virtual organisations;e commerce	Evolving e-commerce technologies increasingly enable organisations to participate in different types of network forms or in electronic markets with previously unidentified trading partners. Virtual organisations (VO) take different forms, have varying lifecycles and involve different scope and depth of relationships. This paper examines the literature in terms of the terminology of virtual organisations, the business drivers, the common theoretical concepts and models as well as the enabling technologies. A specific form of VO, Request Based Virtual Organisation (RB VO), is then considered in relation to these VO variants, particularly as realised through the practical work done within the framework of the EU sponsored LA URA project that facilitates interregional zones of adaptive electronic commerce.	e-commerce;electronic markets;virtual organization	Bob Roberts;Adomas Svirskas;Brian Matthews	2005		10.1007/0-387-29360-4_2	economics;computer science;systems engineering;knowledge management;commerce	Web+IR	-76.84327566356993	4.785085782598559	50631
4ed9c94358cb98a773cee080f24dc2e310f7788b	common structures in system dynamics models of software acquisition projects	system dynamics modeling;software process simulation;system dynamics;software acquisition	In consequence of Abdel-Hamid and Madnick’s pioneer work on system dynamics modeling of software development processes, there is a great number of recent publications dealing with the simulation of the software development process (or sub-processes of it) with system dynamics. Projects that contract third parties to develop the software are even more unpredictable and underestimated by management than pure development projects, and, thus, there is also a need to model and simulate these projects. This article deals with the design of system dynamics models of the unexplored domain – at least with respect to system dynamics – of software acquisition; a framework is described, which captures causal structures common to all models of acquisition projects. Outputs of one concrete instance of the framework are presented finally. Copyright  2004 John Wiley & Sons, Ltd.	causality;john d. wiley;simulation;software development process;system dynamics	Tobias Häberlein	2004	Software Process: Improvement and Practice	10.1002/spip.197	personal software process;simulation;computer science;systems engineering;engineering;software design;social software engineering;software framework;software development;software design description;software engineering;software construction;system dynamics;management;software deployment;goal-driven software development process;software development process;software metric;software system	SE	-64.83271860274552	20.672981011427318	50790
dd7f7fc1f8933936fcb747cad502db636e095ffb	enterprise application management in cloud computing context	service management;service evolution;cloud service brokerage;cloud computing	Rapid growth of various types of cloud services and web APIs is creating new opportunities for innovative enterprise application. As a result, organizations are beginning to rely on external cloud providers to deliver a significant part of their IT infrastructure and software services. An important challenge, in particular in situations where a large number of cloud providers are involved relates to maintaining continuity of operation in the face of changes in external services. Most current research on this topic deals with this problem from service provider perspective by focusing on version management and related issues. Alternatively, the management of cloud services is delegated to a cloud service broker. There is a need to consider this problem from the perspective of service consumers and to develop effective methods that protect service consumer applications from changes in external services. In this paper, we draw on existing literature on service management and evolution, and cloud service brokerage and present a service-based framework designed to manage enterprise applications in cloud computing environments.	application lifecycle management;cloud computing;enterprise software;scott continuity;service-oriented architecture;version control	George Feuerlicht;Hong Thai Tran	2014		10.1145/2684200.2684359	service provider;panorama9;cloud computing security;service level requirement;service bureau;service level objective;service catalog;cloud computing;service product management;differentiated service;knowledge management;service delivery framework;marketing;service design;software as a service;utility computing;business;services computing;data as a service;world wide web;service system	Web+IR	-72.43505597260193	12.44269524653105	50871
b6937595247776753db52b04f4d9b1aae0c026de	antecedents and consequences of firms’ process innovation capability: a literature review and a conceptual framework	high quality realization mechanisms firm process innovation capability conceptual framework literature review long term competitive advantage manufacturing firm efficiency manufacturing firm effectiveness process innovation management capability based perspective knowledge management principal distinction;knowledge management;manufacturing industries;manufacturing industries innovation management knowledge management;innovation management;technological innovation production product development manufacturing process planning innovation management;production technologies capabilities literature review manufacturing process development process innovation process technologies	Process innovation can allow both efficiency and effectiveness gains and is a key source of long-term competitive advantage in manufacturing firms. However, the literature on managing process innovation is broad and fragmented, and it has not yet been systematically reviewed in the scholarly literature. Drawing on a capability-based perspective, the aim of this paper is to provide a systematic review of the process innovation literature. We synthesize our findings into a conceptual framework displaying the antecedents and consequences of firms' process innovation capability. First, a parsimonious review of the process innovation literature is conducted. Second, a conceptual framework of firms' process innovation capability is developed to synthesize the literature and to advance knowledge about managing process innovation. A principal distinction between a firm's potential and realized process innovation capability is drawn, and it is argued that high-quality realization mechanisms are critical for achieving desired process innovation outcomes. Finally, implications for theory, management practice, and recommendations for future research are provided.	capability-based security;occam's razor;systematic review	Johan Frishammar;Monika Kurkkio;Lena Abrahamsson;Ulrich Lichtenthaler	2012	IEEE Transactions on Engineering Management	10.1109/TEM.2012.2187660	product innovation;economics;innovation management;systems engineering;knowledge management;process management;manufacturing;management	HCI	-78.76637594815409	5.909355667141131	50887
739f70f4d3aeeab50f92eb517714ce786810abea	formal specification of a mental health delivery system	mental health;formal specification		formal specification	Richard A. Kemmerer	1989			systems engineering;formal specification;mental health;reliability engineering;computer science	HCI	-63.24658899933782	25.57365364280309	50946
963c94357e8d1de93ecc5b72731a923961487e07	knowledge reuse in open source software: an exploratory study of 15 open source projects	knowledge reuse;knowledge reuse flossdevelopment;open source software technological innovation costs programming software performance humans seminars context internet licenses;flossdevelopment;exploratory study;open source software;open source	To date, there is no investigation of knowledge reuse in open source software projects. This paper focuses on the forms of knowledge reuse and the factors impacting on them. It develops a theory drawn from data of 15 open source software projects and finds that the effort to search, integrate and maintain external knowledge influences the form of knowledge to be reused. Implications for firms and innovation research are discussed.	exploratory testing;open-source hardware;open-source software	Georg von Krogh;Sebastian Spaeth;Stefan Haefliger	2005	Proceedings of the 38th Annual Hawaii International Conference on System Sciences	10.1109/HICSS.2005.378	knowledge management;software development;software engineering;domain engineering;software construction;exploratory research;domain knowledge	SE	-68.4478237245646	22.70945048118144	51056
d45e7775ed5b7db47ec618196fe94025813f9c27	a dea-based incentives system for centrally managed multi-unit organisations	banking;data envelopment analysis dea;centralised management;incentive regulation	In multi-unit organisations such as a bank and its branches or a national body delivering publicly funded health or education services through local operating units, the need arises to incentivize the units to operate efficiently. In such instances, it is generally accepted that units found to be inefficient can be encouraged to make efficiency savings. However, units which are found to be efficient need to be incentivized in a different manner. It has been suggested that efficient units could be incentivized by some reward compatible with the level to which their attainment exceeds that of the best of the rest, normally referred to as “super-efficiency”. A recent approach to this issue (Varmaz et. al. 2013) has used Data Envelopment Analysis (DEA) models to measure the super-efficiency of the whole system of operating units with and without the involvement of each unit in turn in order to provide incentives. We identify shortcomings in this approach and use it as a starting point to develop a new DEA-based system for incentivizing operating units to operate efficiently for the benefit of the aggregate system of units. Data from a small German retail bank is used to illustrate our method.		Mohsen Afsharian;Heinz Ahn;Emmanuel Thanassoulis	2017	European Journal of Operational Research	10.1016/j.ejor.2016.10.040	actuarial science;economics;operations management;data envelopment analysis;commerce	ML	-82.87348317265969	6.83575444555736	51079
f7710eb426958ea027b3b9490037d3eea2011ea3	socialemis: improving emergency preparedness through collaboration	emergency response;contingency planning;emis;emergency preparedness;emergency management;knowledge base	The definition of the contingency plan during the preparedness phase holds a crucial role in emergency management. A proper emergency response, indeed, requires the implementation of a contingency plan that can be accurate only if different people with different skills are involved. The goal of this paper is to introduce SocialEMIS, a first prototype of a tool that supports the collaborative definition of contingency plans. Although the current implementation is now focused on the role of the emergency operators, the accuracy of the plan will also take advantage of information coming from the citizens in future releases. Moreover, the contingency plans defined with SocialEMIS represent a knowledge base for defining other contingency plans.	contingency plan;knowledge base;prototype	Ouejdane Mejri;Pierluigi Plebani	2012		10.1145/2187980.2188182	knowledge base;computer science;knowledge management;management science;emergency management	AI	-65.32041384620295	4.472108026402813	51255
764182a99dca7e88fe64e938cc72d737f9d11d8f	an evaluation framework for data quality tools	address normalisation;on line interface;integration;communication conference;accuracy;relevancy;deduplication;dqm;data quality;completeness;connectivity;consistency;criteria;framework;evaluation framework;information product	Data Quality is a major stake for large organizations and software companies are proposing increasing numbers of tools focusing on these issues. The scope of these tools is moving from specific applications (deduplication, address normalization etc ...) to a more global perspective integrating all areas of data quality (profiling, rule-detection...). A framework is needed to help managers to choose this type of tool. In this article, we focus on tool-functionalities which aim to measure the quality of data(bases). We explain what one can expect of such functionalities in a CRM context, and we propose a general matrix which can be used for the evaluation and comparison of these tools.	customer relationship management;data deduplication;data quality;profiling (computer programming);software industry	Virginie Goasdoué;Sylvaine Nugier;Dominique Duquennoy;Brigitte Laboisse	2007			computer science;systems engineering;data mining;database	SE	-70.45502571498746	12.636879181698353	51339
829b5f5688051c58360ba7f167cd43f1c316a8a0	practical open systems - a guide for managers (2. ed.)	open system			Ian Hugo	1993			systems engineering;knowledge management;software engineering	Theory	-64.21454959759204	21.74308909305325	51430
1a3b611400e8863f38729521ae85c5c68b6ed8af	can we trust our results? a mapping study on data quality	manuals;software engineering data sets;systematics;data collection;systematics software engineering noise data collection cleaning context manuals;software engineering;trusted computing;systematic mapping;trustworthy data software engineering research software engineering datasets trustworthiness systematic mapping study data quality topic data set data quality problem;empirical studies;data quality;data handling;context;software engineering data sets data quality empirical studies systematic mapping;trusted computing data handling software engineering;cleaning;noise	Background: The quality of data sets used in software engineering research is of the utmost importance. To ensure credibility of results obtained from use of data sets, the quality of the data must be examined. Objective: This study provides an overview of recent research(2008-2012) involving data quality in software engineering datasets, with the goal of generally understanding what research there is that addresses data quality, and in particular to determine to what degree researchers have addressed any data quality issues in order to evaluate the trustworthiness of their results. Method: We performed a systematic mapping study to investigate treatment of data quality issues in software engineering research. A total of 64 papers published from 2008 to 2012explicitly address issues with the quality of data and use software engineering data sets. These studies were classified according to the data quality topic, data set and data quality problem. Results: We found only 31 studies gave serious consideration for how the quality of the data affected their results. We observed that there is a lack of clear and consistent terminology regarding data quality, especially with respect to the kinds of quality problems a data set might have. As a first step to address this problem, we propose a model that describes the lifecycle that research data goes through when used in research. Conclusions: The results suggest that researchers should give more attention to the quality of data sets in order to produce trustworthy data for reliable empirical research, and that the research community needs to better understand and communicate issues with data quality.	data quality;experimental software engineering;trust (emotion);trustworthy computing	Marshima Mohd Rosli;Ewan D. Tempero;Andrew Luxton-Reilly	2013	2013 20th Asia-Pacific Software Engineering Conference (APSEC)	10.1109/APSEC.2013.26	data quality;computer science;noise;data science;software engineering;group method of data handling;data mining;database;systematics;information quality;trustworthy computing;empirical research;data collection	SE	-68.29957957456237	30.559888688355755	51503
343fa761c20fdaee9d7f65d3d5097e428f712493	metrics and laws of software evolution - the nineties view	software metrics;dynamic programming;financial data processing;project management;software process improvement;chaos;software maintenance;software process dynamics;genetic programming;multi input multi output;data analysis;feedback;software evolution;process metrics;negative feedback;multi input multi output system;e type software development;process modelling;software development;os 360;dynamics and improvement;world wide web;lehman s laws software metrics software evolution e type software development multi input multi output system feedback os 360 software technology feast 1 project fw financial transaction system logica fastwire software process modelling software process improvement software process dynamics;informatics;first record;feedback software metrics financial data processing transaction processing software maintenance;logica fastwire;feast 1 project;transaction processing;uniform resource locators;software technology;lehman s laws;software process modelling;dynamic programming educational institutions uniform resource locators world wide web genetic programming data analysis informatics negative feedback chaos project management;fw financial transaction system;software process;evolution	The process of E-type software development andevolution has proven most difficult to improve, possibly due to the fact that the process is a multi-input, multi-output system involving feedback at many levels. This observation, first recorded in the early 70s during an extended study of OS/360 evolution, was recently captured in a FEAST hypothesis; a hypothesis being studied in on-going twoyear project, FEAST/1. Preliminary conclusions based on a study of a financial transaction system, FW, are outlined and compared with those reached during the earlier OS/360 study. The new analysis supports, or better does not contradict, the laws of software volution, suggesting that the 1970s approach to metric analysis of software evolution is still relevant today. It is hoped that FEAST/1 will provide a foundation for mastering the feedback aspects of the software evolution process, opening up new paths for process modelling and improvement.	bus mastering;lehman's laws of software evolution;process modeling;software development	Meir M. Lehman;Juan Fernández-Ramil;Paul Wernick;Dewayne E. Perry;Wladyslaw M. Turski	1997		10.1109/METRIC.1997.637156	project management;genetic programming;real-time computing;simulation;transaction processing;computer science;systems engineering;engineering;software evolution;software development;software engineering;dynamic programming;evolution;feedback;data analysis;programming language;software maintenance;informatics;management;negative feedback	SE	-65.1547541172583	26.379408696216085	51515
6afd7c62c7423b0deb9e65db1ee97113a3f58884	erp/erp ii issues and answers: session overview	information systems;application software;paper technology;packaging;internet;enterprise resource planning predictive models marketing and sales programming application software internet packaging environmental management paper technology information systems;enterprise resource planning;predictive models;environmental management;programming;marketing and sales	This is the fourth year at HICSS where there is a session devoted to ERP (Enterprise Resource Planning) systems research. Last year, participants at the session discussed the idea of expanding ERP to include the add-on products sometimes called ERP II or EAI (Enterprise Application Integration or expanded ERP systems). Based on this suggestion, the mini-track was modified to include ERP II systems as well. There were 10 full papers submitted and 1 other paper that was partially complete. The papers selected for this year’s conference cover a full range of topics from conceptual modeling to specific case studies. Even though software sales are depressed, ERP systems remain in the forefront of software development activities. More important, however, ERP is just the beginning of a future trend towards EAI. Some think that EAI, a term even more vague than ERP, is the future of computing systems and a necessary foundation for full integration of enterprise-wide applications [Manchester, 1999, Stein, 1999, and Teresko, 1999]. The literature is just beginning to define what EAI means, but all agree that it is a pre-requisite to successful deployment of e-commerce and e-business applications. Most simply, EAI is the opening of internal systems to customers and suppliers so that e-business can be enabled through the Internet. How EAI can take place is still a large question. The first paper addresses this broader area of EAI. “Towards a Novel Framework for the Assessment of Enterprise Application Integration Packages” provides a conceptual framework that can be used both to select an appropriate EAI solution for an enterprise and to conduct further research in EAI implementation. Similarly, the second paper, “Benefits of an ERP Maintenance Model to Management”, introduces a model to use to	erp;high- and low-level	Gail Corbitt	2003		10.1109/HICSS.2003.1174607	programming;marketing management;packaging and labeling;application software;the internet;enterprise software;computer science;knowledge management;marketing;operations management;sales management;predictive modelling;management;law;human resource management system;enterprise planning system;information system	AI	-73.50363802299184	5.636423979922109	51606
d2366863557a914a99471354c142de666e96084a	starting product lines (i) — systematic product line planning and adoption	economic analysis;product line;software product line	 To successfully and effectively adopt a software product line approach, the transition must be well aligned to the specific product line situation. As more and more organizations aim at a product line transition, this becomes increasingly an issue. To successfully and effectively adopt a software product line approach, a thorough analysis of the economic implications of the adoption must be performed, and the product line introduction needs to be adequately planned. This requires a precise picture of the product line through product analysis and modeling. A thorough analysis of the economic implications of the adoption must be performed, and the introduction of the product line needs to be correspondingly planned. Of course, such a transition has serious ramifications for the component structure of the software. We will discuss these consequences and show how the economic analysis itself can be used as a basis for deriving an adequate structure for the software. Thus, this tutorial provides a concise overview of the current state of the art of product line planning and adoption that is aimed at both researchers and practitioners of product line development.		Isabel John;Klaus Schmid	2004		10.1007/978-3-540-28630-1_31	systems engineering;product management;software;software product line;computer science	Theory	-67.97141664461037	22.348177914554388	51642
9b5e45b06aefeb7375060333f157e1fdbb901ba1	component-based software engineering and the issue of trust	licensure;software testing;third party testing;software component framework;component based software engineering;application software;software engineering software quality software reusability software testing computer science production application software coordinate measuring machines programming information technology;component based development;information technology;software engineering;software reusability;software component;production;trusted component;computer science;coordinate measuring machines;programming;software quality;component based development cbd	Software component consumers are entitled to trusted components. This panel addresses the criteria for trusted components and presents generally accepted definitions for all terms used to describe both software components and the methods and processes required to verify trusted software components.	component-based software engineering;trusted operating system	George T. Heineman;William T. Councill;Janet S. Flynt;Alok Mehta;John R. Speed;Mary Shaw	2000		10.1145/337180.337501	medical software;verification and validation;computer science;systems engineering;backporting;social software engineering;software framework;component-based software engineering;software development;software engineering;software construction;software walkthrough;programming language;resource-oriented architecture;information technology;software deployment;software system;computer engineering;software peer review	SE	-63.393162574329	27.34773569174515	51705
1fbb67778e5e5313b3e2c009eb06667085fe0b23	a green flag over mobile industry start-ups: human capital and past investors as investment signals		Crowdfunding and online start-up platforms are becoming important communication tools for startups and investors. Existing literatures on online start-up platforms usually focus on reward-based crowdfunding platform, which do not offer any equity to backers. In addition, there have not been many empirical researches about equity-based crowdfunding due to the novelty of the regulation. This study analyzes the association between funding amount and early stage start-ups’ underlying characteristics, the type of past investors, and influence of investors in the context of equity-based crowdfunding. The distinction of our research is the aspect of approach that we use population data from online start-up platform for the mobile industry. We find that start-up’s funding outcome is positively related to start-up’s human capital and pure investors. Moreover, our study extends theoretical understanding of the importance of human capital and past investors in start-up, and also contributes to the entrepreneurship literature by examining creditable signals for early stage start-up investment.	crowdfunding	Jungkook An;Woojin Jung;Hee-Woong Kim	2015			industrial organization;computer science;marketing;entrepreneurship;novelty;human capital;equity (finance);population;commerce;venture capital	Web+IR	-83.6039138707483	6.304547161311194	51741
438995ab1218bd4455aad3cba3f695c8e008519b	translating standards into practice: experience and lessons learned at the department of veterans affairs	likelihood of adoption scale;health information technology;reference implementation;standards adoption;interoperability;standards life cycle	"""The increased need for interoperable electronic health records in health care organizations underscores the importance of standards. The US Department of Veterans Affairs (VA) has a long history of developing and adopting various types of health care data standards. The authors present in detail their experience in this domain. A formal organization within VA is responsible for helping to develop and implement standards. This group has produced a Standards Life Cycle (SLC) process endorsed by VA key business and information technology (IT) stakeholders. It coordinates the identification, description, and implementation of standards aligned with VA business requirements. In this paper, we review the adoption of four standards in the categories of security and privacy, terminology, health information exchange, and modeling tools; emphasizing the implementation approach used in each. In our experience, adoption is facilitated by internal staff with expertise in standards development and adoption. Use of processes such as an SLC and tools such as an enterprise requirement repository help formally track and ensure that IT development and acquisition incorporate these standards. An organization should adopt standards that are aligned with its business priorities and favor those that are more readily implementable. To assist with this final point, we offer a standard """"Likelihood of Adoption Scale,"""" which changes as standards specifications evolve from PDF documents only, to PDF documents with construction and testing tools, to fully functional reference implementations."""	alignment;business requirements;categories;electronic health records;health care;health information exchange;information sciences;interoperability;multi-level cell;nomenclature;portable document format;privacy;requirement;specification;veterans affairs;standards characteristics	Omar Bouhaddou;Tim Cromwell;Mike Davis;Sarah A. Maulden;Nelson Hsing;David Carlson;Jennifer Cockle;Catherine Hoang;Linda Fischetti	2012	Journal of biomedical informatics	10.1016/j.jbi.2012.01.003	reference implementation;interoperability;learning standards;computer science;knowledge management;data mining;database	HCI	-71.18047646311558	13.815864491073024	51762
24517a528bfa42e749941c3123c8fb39e44b8b77	business intelligence in magazine distribution	small enterprise supply chain;business intelligence;article;information interpretation	This case discusses the use of business intelligence systems in the running and optimisation of magazine distribution by a UK company. The company collects a wide range of data to help it monitor and optimise a supply chain involving subcontractors. The case study raises a number of issues which are discussed. It illustrates the variety of forces which are driving companies to adopt business intelligence systems. It demonstrates how business intelligence systems can help run business processes. It explores the problems and issues with sourcing, collecting and cleaning data. Issues around anonymisation and the concept of a 'single version of the truth' are discussed and ethical issues highlighted. It concludes that an understanding of the role of interpretation in data collection, collation and subsequent decision making is critical to business intelligence and calls for more research in this area.		Neil McBride	2014	Int J. Information Management	10.1016/j.ijinfomgt.2013.09.006	intelligence cycle;computer science;engineering;knowledge management;artifact-centric business process model;electrical engineering;marketing;operations management;business case;business analytics;business intelligence;management;business rule;new business development;world wide web;business process modeling;business activity monitoring	AI	-74.74041088226737	10.57264658175511	51896
e709f27b8a63c7be28267e286f57eca11694aba7	financial performance evaluation of china's listed steel companies from creditors' perspective	creditors;capital structure;financial performance;iron;stock exchange;listed steel companies;principal component analysis;performance model;financial performance evaluation	Due to the impact of the global financial turmoil and the rising cost of iron ore, the rise in share prices of the iron and steel sector in 2006 and 2007 and drop in 2008 has raised a great deal of concern among creditors about the performance of China's listed steel companies. This paper attempts to identify the concerns of the creditors through questionnaires, and elicit the financial status and developing trend of the listed steel companies by performing model verification (e.g. Principal Component Analysis) to process and interpret the data from the Shanghai and Shenzhen stock exchanges for the last three years. With the 38 listed steel companies divided into six groups, creditors are then able to better make credit decisions, control credit scale and credit cycle so as to ensure the safety and efficiency of the credit capital. The results also provide the evidence that the equity structure and capital structure of the listed steel companies needs improvement and optimization.	mathematical optimization;performance evaluation;principal component analysis;steel	Li Wenyi	2009		10.1145/1655925.1655987	stock exchange;capital structure;finance;iron;commerce;principal component analysis	Metrics	-84.90944345983975	7.398486770791705	51966
7ae035dea7cf2bfb820051dca854d69ff4893ab3	introduction to the minitrack on data warehousing	data warehouses;manufacturing data processing;e-commerce;performance management;data warehousing;business data processing;customer relationship management;manufacturing resources planning;supply chain	Data warehouse repositories and their applications offer great potential for understanding and managing customer relationships, uncovering important patterns and trends n corporate data, and supporting comprehensive performance measurement systems, such as Balanced Scorecards. The papers in this year’s data warehousing minitrack investigate how data warehousing can be used to support these kinds of initiatives and how data warehousing can be made more effective technically. In “Method for Demand-Driven Information Requirements Analysis in Data Warehousing Projects,” Robert Winter and Bernard Strauch present a 13-step methodology for analyzing the requirements of data warehousing projects. Expert interviews were conducted and used to develop the methodology, and parts of the methodology were applied successfully in data warehouse projects. Steven Hanes investigates approaches and tools used to collect, manage, and deliver institutional metrics to the senior command of the United States Marine Corps in his paper, “Institutional Metrics for the United States Marine Corps.” The paper explores current performance management strategies and tool architectures, with emphasis on the balanced scorecard approach and the commercial, off-the-shelf software that supports it. The initial evaluation framework is presented. The third study in this minitrack focuses on customer relationship management and describes a customeroriented architecture for data warehousing. Hans-Georg Kemper and Phil-Lip Lee describe two kinds of customer relationship management initiatives – analytical and operational – in “The Customer-Centric Data Warehouse – An Architectural Approach to Meet the Challenges of Customer Orientation.” They suggest how the incorporation of an operational data store into a data warehouse environment helps to support combined analytical and operational needs. As applications for data warehousing continue to grow, the amount of data that organizations must manipulate and store increases. The next two papers address some of the challenges with large volumes of warehoused data. The first paper, “Ad-Hoc Association Rule Mining Within the Data Warehouse” by Svetlozar Nestorov and Nenad Jukić, offers a new data mining framework that is tightly integrated with data warehousing technology. The framework analyzes star schema organized data; leverages the processing power of the data warehouse; queries data across the data warehouse; and incorporates varying levels of data aggregation. A second way to manage large volumes of data is through virtual data marts, an approach presented by Leslie Hodge and Charles Milligan in “Managing Virtual Data Mart(s) With Metapointer Tables.” Hodge and Milligan describe a data management method that enables users to access relevant subsets of large data populations that reside in different data warehouses. It is challenging for academics to stay abreast of changes to teach and research effectively in the data warehousing field. Teradata, a division of NCR, working closely with academics, recently introduced its Teradata University Network (TUN) initiative. TUN is a website that academics can use to find materials for data warehousing, DSS/BI, or database classes; to access knowledge and training materials from subject matter experts; and to connect with colleagues in the academic and business communities. In the final presentation in this minitrack, Hugh Watson, Director of the TUN Advisory Board, will describe the resources available on TUN and how academics can benefit from its offerings.	association rule learning;customer relationship management;data aggregation;data mart;data mining;database;operational data store;population;requirement;requirements analysis;star schema;steven anson coons;subject matter expert turing test;system of measurement	Barbara Wixom;Paul Gray;Hugh J. Watson	2003		10.1109/HICSS.2003.1174601	computer science;knowledge management;marketing;data warehouse;process management;data analysis;management	DB	-68.28017734648431	9.815094813871298	51985
3e5ea62ffb01f60f92592ae5fcc42362b3fe12ed	visualizing centrality of process area networks in cmmi-dev	network analysis;software process model	In order to nd a clue to effectively introducing and utilizing new technology in improving software development process, we analyze and visualize the in-degree centrality of the process area networks in terms of the related process areas in CMMI-DEV. By visualizing the results of in-degree centrality analysis, we can have a perspective of process improvement in using advanced technology.	capability maturity model integration;centrality;directed graph;software development process	Shigeru Kusakabe;Hsin-Hung Lin;Yoichi Omori;Keijiro Araki	2015		10.1145/2785592.2794405	computer science;systems engineering;data science;data mining	HPC	-75.64391268785411	22.351239354983328	52043
7f4cc44e5611325a6d6810313bd9447a44965d4b	α capability-oriented modelling and simulation approach for autonomous vehicle management		Abstract Cities strive to invent and deploy smart solutions in the domain of urban mobility, so as to offer innovative services to citizens and visitors and improve the overall quality of life. The focus of this paper is on Autonomous Vehicular Safety (AVS) management systems. Such systems exploit vehicular and road side infrastructures to increase the effectiveness of vehicular communications and the relevant services and applications offered in urban environments. AVS management systems combine physical with digital components, each with different capabilities, and involve many different stakeholders, with often competing goals. Sophisticated development of algorithms for simulating the behaviour of the AVS system is required to avoid wasted resources if, after it is implemented, shortcomings with respect to the originally stated goals are found. To achieve this, a capability-oriented modelling approach is introduced and applied to AVS using the CORE framework. This paper argues that the development of an AVS system entails a coherent approach that gradually progresses from vague statements of requirements to the algorithm implementation. To this end, the paper demonstrates how one may proceed from requirements to implementation in a systematic and model-driven manner, while simulation results prove the applicability of the approach.		George Dimitrakopoulos;Evangelia Kavakli;Peri Loucopoulos;Dimosthenis Anagnostopoulos;Theodoros Zographos	2019	Simulation Modelling Practice and Theory	10.1016/j.simpat.2018.11.005	computer science;real-time computing;systems engineering;management system;exploit	AI	-63.24884925096879	11.014468115749752	52058
008588d24894076ea26576a62db2c080a0839aad	house of strategy: a model for designing strategies using stakeholders' opinion	delphi survey;strategic planning;house of strategy;indian shrimp industry;gos tree	We model consensus of fuzzy-opinions in a non-repetitive Delphi survey.We propose House of Strategy, a model for Goal, Objective, and Strategy planning.House of Strategy can be used to prioritize strategies for implementation.House of Strategy can be used as a strategy communication tool. The paper presents building of a model, i.e. House of Strategy, out of a two-round Delphi survey, which can be used in strategic planning to develop sets of Goals, Objectives, and Strategies. The survey was made among the stakeholders of the Indian Shrimp Industry in order to set the future goals, and concurrent objectives and to design strategies to achieve them. The first round of Delphi survey resulted into generation of sets of Goal, Objectives, and Strategies. The second round of the survey gathered opinions of survey panelists regarding the desirability and feasibility of the Goals and Objectives, and feasibility and effectiveness of the Strategies. Consensuses of stakeholders opinions on the each of the items with respect to above said criteria were estimated using standard fuzzy consensus building algorithm. Hierarchical representation of a Goal, their corresponding Objectives and Strategies form a tree like structure, called as GOS tree. Seven such GOS trees were developed, each representing a Goal at its top echelon. Validity of each GOS tree was tested using the ratings of the panelists on the goal, objectives, and strategies with respect to the feasibility criterion. The proposed validation process of a GOS tree resulted two set of weights separately for the objectives and the strategies that explained the importance of each of the strategies in achieving the corresponding objective and importance of each of the objectives in aligning towards the goal. These importance weights of the objectives and the strategies were local to the GOS tree. Seven such GOS trees were developed each representing for a future goal. As a strategy might have presence in multiple GOS trees and contributing towards multiple goals, an overall contribution of the strategies was to be found out. We proposed a framework, House of Strategy, which can be used to combine all the GOS trees at one place and could help, in a two-step process, in finding out the overall contribution of each of the strategies towards overall achievement of Industrys goals. The House of Strategy consisted of two sub-Houses, i.e. Goal-Objective (GO) and Objective-Strategy (OS) sub-Houses. In the first step, the GO sub-House calculated the overall priority of each of the Objectives depending on its local importance weights (with respect to each of the GOS tree it represented) and corresponding desirability of the local goal and its own desirability score. Similarly, in the second step, the OS sub-House helped in calculating the overall priority of each of the strategies depending upon the priority of its corresponding objectives (in a GOS tree) as derived by the GO sub-House, its local importance weights (with respect to each of the GOS tree it represented), and its own effectiveness score. Using the House of Strategy, we have prioritized the strategies raised during the Delphi survey. The proposed methodology of building the House of Strategy can be used for designing strategies for an organization/industry using stakeholders' opinion and prioritizing them in order of their overall importance in achieving intended objectives and goals. Furthermore, the House of Strategy can be used as an effective communication tool to portray the goals, objectives, and strategies along with their priorities at one place.		Santosh Kumar Prusty;Pratap K. J. Mohapatra;C. K. Mukherjee	2017	Computers & Industrial Engineering	10.1016/j.cie.2017.04.001	strategic planning;delphi method;engineering;environmental resource management;operations management;mathematics;management;statistics	SE	-79.74186460692954	10.947622724253481	52099
1cd88db92024a21d6ba9d088e0fbaac83efee2d6	capacity planning for mission-critical saas style system			software as a service	Atsushi Haruna;Yoshiyuka Sekimoto;Takashi Muira	2009			capacity planning;systems engineering;software as a service;mission critical;business	Robotics	-64.19755583263023	21.720856906556158	52135
0d71f95d16acd27f8e437a28caa66d8dc5cbbc46	diversification and performance of japanese it subsidiaries: a resource-based view	japan knowledge based industries;resource based view;knowledge based industries;corporate diversification;global it;japan;firm performance	This paper examines the relationship between corporate diversification and performance for Japanese subsidiaries in the information technology (IT) industry. In particular, it reports on our study which drew on the resource-based view of the firm to make the distinction between related and unrelated diversification. Data for more than 5 years were used to test our hypotheses. The results suggest that related subsidiaries out-perform unrelated subsidiaries on a number of dimensions including: performance, survival, and employee productivity growth. The study also found support for resource-based arguments suggesting that related diversification in knowledge-based (tertiary) industries, such as IT, plays a more important factor in firm success than in primary or secondary industries.	diversification (finance)	Michael R. Wade;Jane I. Gravill	2003	Information & Management	10.1016/S0378-7206(02)00012-5	operations management;management;commerce	DB	-83.19366359140078	5.308115507949635	52159
914b8a88108b975513ef39cea8159ddbba125898	concepts of product software	information management system;eur j inform syst;information systems security;mis systems;information systems research;information security;case studies;information science;ejis special issue;information security systems;information technology;business information technology;security information systems;european journal of information systems;information systems management;operational research society;business model;computer information systems;geographic information systems;information technology journal;information management;european journal of is;information systems journals;european journal information systems;information systems technology;managing information systems;accounting information systems;information and management;management information systems;define information systems;strategic information systems;business information management;soft system methodology;ejis journal;information system;health information systems;computer information technology;business information systems;business systems analyst;ejis;european journal;management science;journal of information systems;european journal of information system;information technology journals	Received: 31 March 2006 Revised: 10 May 2007 Accepted: 9 August 2007 Abstract Both the impact of software on life and our dependence on software is rapidly increasing. Using product software is an everyday phenomenon and product software is a major worldwide industry. Yet, there are very few scientific studies reported on the engineering of product software specifically. In this paper, we discuss specifics of the software business, the various terms used for product software and provide our definition of product software. Moreover, we explain difference between product software and tailor-made software from a software development perspective and provide a new framework for the categorization of product software. This paper points out the urgent need for more research on product software and the directions. European Journal of Information Systems (2007) 16, 531–541. doi:10.1057/palgrave.ejis.3000703	algorithm;artificial intelligence;book;categorization;computer science;coupling (computer programming);cycle (graph theory);european journal of information systems;information systems journal;information system;method engineering;new product development;release management;requirement;sjaak brinkkemper;soft systems methodology;software business;software development;software development process;software industry;software quality;software release life cycle;user requirements document;web application;web service	Lai Xu;Sjaak Brinkkemper	2007	EJIS	10.1057/palgrave.ejis.3000703	personal software process;medical software;long-term support;verification and validation;software quality management;software sizing;computer science;systems engineering;engineering;knowledge management;electrical engineering;package development process;backporting;social software engineering;software framework;software development;software design description;software construction;management information systems;management science;software walkthrough;software analytics;information technology;software deployment;information system;software metric;software quality analyst;software peer review	SE	-64.04584994270151	21.822780789976918	52174
1b78e623b2d470b6b8c8c6d94fdd4a0b1f223701	"""a """"genomic"""" classification scheme for supply chain management information systems"""	evaluation function;decision support;relationship management;information systems;supply chain operations reference;data management;qualitative analysis;fuzzy logic;performance improvement;pattern matching;enterprise information system;software package;supply chain;computer software;enterprise system;process model;information system;supply chain management;design methodology	Supply Chain Management Information Systems (SCM IS) are increasingly critical for synchronizing information among the customers and suppliers of a supply chain. Wide variation and overlap in the functionality of different SCM IS makes analysis and comparison difficult. Traditional flat taxonomies using one or two dimensions of functionality have limited utility for software selection and analysis. Instead, this paper proposes a “genomic” classification approach that enables characterization of an SCM IS by the relative presence or absence of a larger set of functional attributes (or “genes”). A qualitative analysis of over 1800 pages of SCM IS documentation and independent analyst reports is used to identify relevant SCM IS functional attributes. The resulting model enables a more structured and useful approach to SCM IS software selection and evaluation. This paper contributes a novel approach for conceptualizing and analyzing complex information systems using faceted rather than traditional flat taxonomies.	comparison and contrast of classification schemes in linguistics and metadata;documentation;faceted classification;management information system;taxonomy (general)	Tim S. McLaren;David C. H. Vuong	2008	J. Enterprise Inf. Management	10.1108/17410390810888688	supply chain management;data management;computer science;systems engineering;knowledge management;marketing;operations management;database;information system;enterprise information system	AI	-69.39243380519456	4.979121970042235	52196
73e141b0795f9d2f5a67e64c862f8c4de1a196e2	global logistic system asia co., ltd		This case study examines the air cargo industry in Hong Kong, where an electronic trading network was launched by four international airlines with considerable success in the mid-1990s. Two key factors explain the success. First of all, the electronic network limited its service to preserve carefully the distribution of power among the stakeholders. Secondly, the system roll out took advantage of the four founding airlinesu0027 local strongholds as points of departure. The case study also addresses the possibilities of extending the network into a full-scale electronic market for Hong Kongu0027s air cargo community.		Jan Damsgaard	1998		10.1145/353053.353107	knowledge management;information management;management science;air cargo;information system;management information systems;strategic information system;electronic trading;engineering;business model;accounting information system	Robotics	-73.97969471271813	4.5596479114528385	52220
669b1f48d40a31017765a6bc4da93ccdaa13a130	software development waste		Context: Since software development is a complex socio-technical activity that involves coordinating different disciplines and skill sets, it provides ample opportunities for waste to emerge. Waste is any activity that produces no value for the customer or user. Objective: The purpose of this paper is to identify and describe different types of waste in software development. Method: Following Constructivist Grounded Theory, we conducted a two-year five-month participant-observation study of eight software development projects at Pivotal, a software development consultancy. We also interviewed 33 software engineers, interaction designers, and product managers, and analyzed one year of retrospection topics. We iterated between analysis and theoretical sampling until achieving theoretical saturation. Results: This paper introduces the first empirical waste taxonomy. It identifies nine wastes and explores their causes, underlying tensions, and overall relationship to the waste taxonomy found in Lean Software Development. Limitations: Grounded Theory does not support statistical generalization. While the proposed taxonomy appears widely applicable, organizations with different software development cultures may experience different waste types. Conclusion: Software development projects manifest nine types of waste: building the wrong feature or product, mismanaging the backlog, rework, unnecessarily complex solutions, extraneous cognitive load, psychological distress, waiting/multitasking, knowledge loss, and ineffective communication.	bottom-up proteomics;computer multitasking;distress (novel);feedback;interaction design;inventory;iteration;lean software development;rework (electronics);sampling (signal processing);sociotechnical system;software engineer;sorting;taxonomy (general);top-down and bottom-up design;waste	Todd Sedano;Paul Ralph;Cécile Péraire	2017	2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)		personal software process;long-term support;team software process;software quality management;extreme programming;computer science;systems engineering;engineering;package development process;social software engineering;software development;software design description;software engineering;management science;software walkthrough;empirical process;software analytics;management;lean software development;software deployment;goal-driven software development process;software development process;software metric;software peer review	SE	-70.84174314450476	22.90575955608202	52314
7e726f361066702b84a91e92db40ecd3760f8724	economics of software reuse and market positioning for customized software solutions		Most of the software companies can neither be typical software product based company like Microsoft nor afford to develop each customized application for individual customer from clean slate without taking into consideration of reuse. Software companies are under increasing competitive pressure for improving delivery parameters such as cost, quality, and time. Systematic reuse is an opportunity of continued cost reduction, quality improvement and lead time reduction in software delivery. Systematic reuse largely depends on the scope of delivering customized software applications in the same market segment repeatedly to multiple customers. Thorough market analysis provides basic inputs for defining generic product concept for delivering mass customized solutions. The problem of establishing a successful new business around a generic software product concept is not challenging because of shortage of ideas, but rather problems exist in proper analysis of the market and adoption of reuse capability for continued price reduction and quality improvement to deal with evolving market forces for delivering mass customized solutions. This paper, therefore, suggests the application of market positioning strategy to benefit from software reuse for delivering customized software solutions. In this paper, economics of software reuse has been integrated with market positioning for delivering customized software solution. It is believed that such integration will improve the decision making ability of software professionals for strengthening the capability of software companies in delivering customized solutions to target market segment by taking the advantage of software reuse economics.	clean slate program;code reuse;engineering design process;global positioning system;programming tool;software deployment;software industry	M. Rokonuzzaman;Kiriti Prasad Choudhury	2011	JSW		personal software process;verification and validation;software quality management;software engineering process group;software sizing;package development process;backporting;social software engineering;software framework;component-based software engineering;software development;domain engineering;software construction;software walkthrough;software deployment;software quality control;software quality	SE	-66.53613469594659	20.790894145906815	52318
a03e874885bb6d731a1f2ac59d455593f0250897	smart decisions: an architectural design game	software systems;design methods;computer architecture;software architecture;games for learning;big data;games;attribute driven design;design methodology	"""Architecture design is notoriously difficult to teach and to learn. Most competent architects in industry have deep knowledge won from long years of experience. But if we want architecture design to be methodical and repeatable, we need better methods for teaching it. Simply waiting for an aspiring architect to accumulate 10 or 20 years of experience is not acceptable if we believe that software engineering is a true engineering discipline. In this paper we describe our experiences with the development of a game that aids in teaching architecture design, specifically design employing the Attribute-Driven Design method. We discuss our approach to creating the game, and the """"design concepts catalog"""" that provides the knowledge base for the game. Finally, we report on our experiences with deploying the game, and the (enthusiastic) assessments and feedback that we have received from industrial and academic participants."""	attribute-driven design;knowledge base;software engineering	Humberto Cervantes;Serge Haziyev;Olha Hrytsay;Rick Kazman	2016	2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)	10.1145/2889160.2889184	game design;reference architecture;software architecture;simulation;big data;database-centric architecture;level design;design methods;experience design;computer science;systems engineering;engineering;knowledge management;software design;game mechanics;software engineering;game art design;environmental graphic design;game developer;simulations and games in economics education;design education;game design document;management;high-level design;game testing	SE	-67.48507883934349	25.18014569481847	52319
d0b3fc5cd7f887542089582cdbe0c4f558b7619c	suppliers' behavior on the post-adoption stage of business-to-business e-reverse auctions: an empirical study	e reverse auctions;e business;e procurement;level of use;suppliers behavior;post adoption stage	Reverse e-auctions are increasingly being used as an alternative business-to-business e-procurement model to exchange products and services among enterprises. Despite their high interest from the academic community, there has been limited empirical study focusing on their post-adoption stage. Based on extant literature dealing with their e-business model and building on emerging concepts in B2B e-commerce, this paper intends to investigate the impact of various factors on suppliers' level of use of e-reverse auctions by examining their internal and external environment. A relevant conceptual framework is developed and examined on data collected from 60 supplying firms that currently utilize e-reverse auctions. These are analyzed through factor analysis and multiple discriminant analysis. Findings show that firms get influenced to a greater extent by their top management strategic practices and competitors' business actions. The results also provide interesting insights and useful hints to both researchers and practitioners.		Vaggelis Saprikis	2013	Telematics and Informatics	10.1016/j.tele.2012.04.002	economics;marketing;operations management;e-procurement;electronic business;management;commerce	ECom	-80.49727248699506	5.120150569832929	52422
97b152b3f507c2746d1e7798609deac7d28ed4b6	comprehend and analyze knowledge networks to improve software evolution	social network;emergent behavior;software evolution;social network analysis;social structure;knowledge management;software architecture;software development	When a set of people are connected by a set of meaningful social relationships we talk of a social network. A social network represents a social structure and the underlying structural patterns can be used to analyze and comprehend how people relate to each other and their emergent behavior as a group. Developing software is fundamentally a human activity. Developers cooperate and exchange knowledge and information, creating in fact, a particular type of social network that we call knowledge network. In this paper we investigate knowledge networks in software development teams by applying social network analysis and we use the Apache web server as a case study. By analyzing the structural communication and coordination patterns in Apache we have been able to identify the Apache knowledge network, highlight potential communication bottlenecks, and find brokers and important coordination points in the software development team. Furthermore, our work enables a software architect to analyze and maintain the organization and the software architecture aligned during software evolution. An important lesson that we have is that the analysis of knowledge networks constitutes an additional tool to be added to the traditional software architecture assessment methods. Copyright © 2009 John Wiley & Sons, Ltd.	betweenness;centrality;chart;code refactoring;cohesion (computer science);component-based software engineering;conway's game of life;conway's law;distributed computing;emergence;interdependence;john d. wiley;maemo;mobile phone;nokia n800 internet tablet;open-source software;partial template specialization;principle of abstraction;programming language;server (computing);social network analysis;social structure;software architect;software architecture;software developer;software development;software development process;software evolution;software maintenance;software quality;software system;structural pattern;subversion;tablet computer;traceability;web server	Christian Del Rosso	2009	Journal of Software Maintenance	10.1002/smr.408	software architecture;personal software process;team software process;social network analysis;software engineering process group;software mining;computer science;systems engineering;engineering;knowledge management;data science;software evolution;social software engineering;software framework;component-based software engineering;software development;software design description;software engineering;social structure;software walkthrough;software analytics;resource-oriented architecture;software deployment;emergence;social network	SE	-75.32744984363084	22.02404804665543	52469
222acb65b3483b82539007cba9c37c57b9276cad	dominant innovation design for smart products-service systems (pss): strategies and case studies	technological innovation batteries companies industries manufacturing sensors;market globalization business value customer centric marketable reality systematic innovative thinking product service innovation dominant innovation design innovation toolset innovation matrix application space map qfd tools hidden customer needs identification product service requirements it enabled service solution service thinking blueprint planning technical solution evaluation service platform implementation service operation service business model smart products service systems pss;product service system;product service system design;design;value engineering customer services globalisation innovation management product design quality function deployment strategic planning	Market globalization has brought us not only new opportunities, but also new challenges. The theme of innovation has become a mandatory topic for all industries - it has become a focal point for the enterprise, society and the world. The goal of innovation is to create business value by developing worthwhile ideas into a customer-centric marketable reality. This, for most companies, is difficult to achieve due to the lack of a methodology and tools for systematic innovative thinking. This paper introduces the concept and strategies for product-service innovation based on a Dominant Innovation Design approach. An innovation toolset that effectively combines an Innovation Matrix, Application Space Map, and QFD tools is explained. This methodology enables the identification of hidden customer needs and gaps as they relate to product-service requirements. This paper also shares two case studies of how this tool-set can be utilized for real product-service innovation. The case study includes an overall IT-enabled service solution for a traditional product, which encompasses service thinking, blueprint planning, technical solution evaluation, service platform implementation, and service operation. Combining these elements can lead manufacturing industries into new service business model based on their core product while creating more value to their customers.	blueprint;focal (programming language);physical symbol system;quality function deployment;requirement;service innovation;smart products	Jay Lee;Hung-An Kao	2014	2014 Annual SRII Global Conference	10.1109/SRII.2014.25	service level requirement;product innovation;service product management;innovation management;systems engineering;knowledge management;service delivery framework;operations management;service design;business	Metrics	-72.89810253170121	8.359705707185126	52479
9987401c3c98becb87376632edd0b9cf008170a0	developing information systems to support flexible strategy	information system	"""The literature on strategic applications of information technology(IT) and strategic information systems (SIS) contains a number of """"classic"""" success cases. These systems are successful in terms of one particular strategy framework, but there is evidence that the demands placed upon SIS are changing. Changes in the business environment and in the rate of development of technology have created a requirement for more flexibility in the strategic process. This must be supported by a """"new generation"""" of SIS that integrate with flexible infrastructures to provide long-term effective support for a flexible business strategy. This article looks at the changing nature of the strategic: process and examines a framework that has potential to assist in these aims. A number of techniques are analyzed to assess their capability for identifying strategic applications of IT. Finally, a way of providing a flexible IS/IT infrastructure relevant to the development of a flexible SIS is discussed."""		Alan Eardley;David E. Avison;Philip Powell	1997	J. Org. Computing and E. Commerce	10.1207/s15327744joce0701_4	strategic planning;computer science;operations management;management science;information system	HPC	-78.17502804014691	5.400531385572602	52491
6ecaebc0e62213391cef1e7b5b134399151afd97	renewal model of mobile data services through user experience analysis	user analysis;user interface;mobile data service;satisfiability;user experience	In this paper, we introduced real renewal examples of KTF's mobile data service. At the time of development, we focused on user analysis and tried to improve the existing mobile data service based on user's opinion. After the new mobile data service was launched based on user experience analysis, the customers were satisfied with the newly developed mobile data service. We expect that new mobile data service would make continuous revenue streams and contribute to image up the company.	usage data;user analysis;user experience	Sun-Joo Jun;Min-Jeong Kim	2007		10.1145/1377999.1378056	user interface design;user experience design;mobile search;mobile qos;mobile web;human–computer interaction;mobile database;computer science;service delivery framework;database;mobile business development;internet privacy;user analysis;user interface;mobile computing;management;world wide web;satisfiability;mobile payment	Mobile	-73.31599525395295	6.180421708989868	52513
50b27af67fc9f44e8ca20b40b47790adda25f51a	a comparative study of order-driven and quote-driven markets using artificial markets		In Japanese financial market, there exists large gap between traded volumes among listed items or securities exchanges. For example, turnover of (TSE) in August, 2005 is 44,548 billion JPY and that of (OSE), the second largest stock market in Japan, in August, 2005 is 2,184 billion JPY. That is, the turnover of OSE is only 5 % of that of TSE. There are many items which are listed on both TSE and OSE, however, most of them are traded at TSE[1][2]. For example, the turnover of TOYOTA on 16/9/2005 is 448, 360 units at TSE on one hand, and that at OSE is only 1 unit on the other hand. While there are items which are traded actively at OSE, traded volumes of most items are very small.		Tsuyoshi Matsunaga;Hajime Kita	2006		10.1007/978-4-431-73167-2_10	business;stock market;financial system;financial market	ECom	-85.49517008064986	6.809668782876909	52514
95d29a44fce724552e002fa983759087760e0de8	social capital and entrepreneurial success in female entrepreneurship	entrepreneurial success;social capital;female entrepreneur;entrepreneurial motivation	At present China is in the period of a market economy and modern social transition. Why has the number of women entrepreneurs increased? What leads women to leave their jobs and start their own businesses? Social capital is fundamental resources for women to be self-employed, with key resources of mobilization in female entrepreneur being directly determined by her amount of social capital which concerns corporate existence and business development. However, for a longtime, due to structural and other factors, opportunities are extremely limited for women access to social capital and gender differences in social capital set women entrepreneurs in a marginal status. Therefore,The purpose of the study was to identify the perceived personal characteristics and social capital within each woman who and how pursued entrepreneurship. Constructing social capital and enhancing the organizations network resources for the expansion of women entrepreneurship space has become essential in the context of constructing harmonious society.	job stream;marginal model;social capital	Jue-ping Xie	2014			business;management;economic system;labour economics	HCI	-83.72919840121077	5.9832670258961755	52531
905f3552f5bac88c1cf1ada03f8e70f0960aaffe	on the testing maturity of software producing organizations	software metrics;software testing;software development metrics software producing organization testing maturity software test management;software testing informatics costs embedded software software quality application software personnel project management resource management system testing;software metrics program testing software development management software houses;software houses;structural testing;engineering and technology;teknik och teknologier;program testing;software test management;software development metrics;software development management;software producing organization testing maturity	"""This paper presents data from a study of the current state of practice of software testing. Test managers from twelve different software organizations were interviewed. The interviews focused on the amount of resources spent on testing, how the testing is conducted, and the knowledge of the personnel in the test organizations. The data indicate that the overall test maturity is low. Test managers are aware of this but have trouble improving. One problem is that the organizations are commercially successful, suggesting that products must already be """"good enough"""". Also, the current lack of structured testing in practice makes it difficult to quantify the current level of maturity and thereby articulate the potential gain from increasing testing maturity to upper management and developers"""	capability maturity model;principle of good enough;software testing;test case	Mats Grindal;A. Jefferson Offutt;Jonas Mellin	2006	Testing: Academic & Industrial Conference - Practice And Research Techniques (TAIC PART'06)	10.1109/TAIC-PART.2006.20	test strategy;reliability engineering;personal software process;verification and validation;regression testing;software sizing;software performance testing;system integration testing;software project management;systems engineering;engineering;acceptance testing;package development process;social software engineering;software reliability testing;software development;software engineering;software construction;software testing;software deployment;software quality;software quality analyst;software peer review	SE	-65.44384163324703	28.468414021747073	52578
ce2b9221d185bc5205ad77e404df07ad6f2e069e	international entrepreneurship and information technology strategies of the multinational enterprises from emerging markets	multinational enterprises;emerging markets;technological catch up;mnes;emerging market mnes;it strategies;information technology;international entrepreneurship;springboard;dic;information technology strategies;its;innovation;dynamic internalisation;dynamic influence cycle;internationalisation strategies	Traditional foreign direct investment FDI theories fail to provide a theoretical framework for international entrepreneurship IE and internationalisation strategies IS of multinational enterprises from emerging markets EM MNEs that strive to acquire competitive advantages through dynamic international expansion. EM MNEs share certain features of IS in common: springboard, dynamic internalisation, technological catch-up, and knowledge exploration. Furthermore, innovation for creating new products and production methods led by contemporary EM MNEs was and is the outcome of their pursuit of externally oriented and knowledge-seeking, dynamic opportunity-seeking, and agile mimic IE.		OhSuk Yang;Ryosuke Sugie	2015	IJBIS	10.1504/IJBIS.2015.068482	innovation;economics;marketing;international trade;management;information technology;emerging markets;commerce	ECom	-78.64624353449192	4.812563228692934	52588
c484c893c209c108de7f74dbc23ddef69e3a5234	empirical research in software engineering: a workshop	software reusablity;empirical study;progress;software engineering;reuse definition;open problems;attitudes;survey;assessment;empirical research	• The NSF Software Engineering Research Centers Program, which could have Centers oriented more towards software~ with appropriate industry participation as a success criterion. • The State of Califorrda's Micro program, which provides matching funds for industry-supported university research. • Establishing and coordinating counterpart initiatives for experimentation with advanced research concepts and capabilities in Federal missicm-oriented agencies (DoD, DoT, NASA, DoE, NIH, DoC, etc.) via HPCC-like mechanisms.	hpcc;ibm notes;software engineering	Susan S. Brilliant;John C. Knight	1999	ACM SIGSOFT Software Engineering Notes	10.1145/311963.311998	personal software process;computer science;systems engineering;social software engineering;software development;software engineering;management science;software walkthrough;empirical process;empirical research;software peer review	SE	-69.24447199256821	18.88310059858482	52628
2eead2831710203eeb595504ab9b7f840716d053	industry shakeouts and product strategies: lessons from the us laser printer industry		ABSTRACTMany industries experience a shakeout, which occurs when, after an initial increase, the number of firms drops significantly in a short period of time. A shakeout drives changes in the market structure and, thus, accompanying changes in firms’ strategies. In this paper, we explore differences in firms’ product strategies (i.e. product quality improvement, product line management, and product market strategy) before and after the initiation of an industry shakeout, focusing on the role of product exit. In analyzing a sample of US laser printer manufacturers and their products for the period between 1983 and 2002, we find clear differences in firms’ product exit decisions before and after the initiation of a shakeout. With these findings, our study contributes to the understanding of the link between industry shakeouts and firms’ product strategies.	laser printing;printer (computing)	Kwangwook Gang;Min Jeong;Minseok Park	2018	Techn. Analysis & Strat. Manag.	10.1080/09537325.2018.1458978	marketing;market structure;quality management;economics;line management;product market	SE	-82.69612261444446	7.042007498886054	52691
58c0dbd1a2383e40996fc5ac213aa93686421c0f	procurement maturity and it-alignment models: overview and a case study		More and more firms urge their procurement departments to optimize their processes and leverage IT in order to reduce costs, increase quality and sustainability of received products and services, and shorten delivery times. Already in the nineties of the previous century Henderson and Venkatraman (IBM Systems Journal 32(1):4–16, 1993) identified business/IT-alignment as a key to organizational performance. Many maturity and IT-alignment models have since been developed, yet, specific procurement maturity models, including business/IT-alignment principles, are scarce. The aim of this paper is to provide an overview of procurement maturity models that include IT-alignment. We start with a presentation of business/IT-alignment principles, detailing dimensions (areas of concern) for alignment, specifically for the procurement domain. Subsequently, maturity principles are discussed, resulting in maturity levels for the procurement domain. Finally, an in depth study of a specialized procurement model for the construction industry is presented, in which simulation techniques for testing are successfully applied. The discussed models and application justify a business/IT-alignment approach for procurement departments.	capability maturity model;data validation;procurement;simulation	Johan Versendaal;J. M. van den Akker;Xiaochun Xing;Bastiaan de Bevere	2013	Electronic Markets	10.1007/s12525-013-0130-x	procurement;systems engineering;engineering;marketing;operations management;management;service integration maturity model	SE	-70.87199144987989	9.970349707096505	52947
67999c47c52236404dd521bd608573beceb8f176	early communication: key to software project success	project management software project success customer seller communication software development problems software development management;project management;development process;software development;software development management;uncertainty probability project management computer architecture phase estimation costs statistics software development management computer science nails;project management software development management	The paper considers how customer/seller faulty communication causes many software development problems. At least part of this communications breakdown stems from customers' lack of comprehension regarding their role in the development process. Developers must help customers realize that software development is an inherently complex and uncertain activity that can only succeed through close cooperation.	software project management	Ware Myers	1999	IEEE Computer	10.1109/2.762809	project management;personal software process;long-term support;verification and validation;software quality management;software project management;knowledge management;package development process;social software engineering;software development;software engineering;software construction;software as a service;software walkthrough;application lifecycle management;project management triangle;management;lean software development;software deployment;goal-driven software development process;software development process;software peer review	Visualization	-68.48831765195536	22.633241526286838	53052
ab39ec5660352da95a64e8018ce590147b76e729	networking companies: from basic web services to business solutions	web service	ATLANTA, Jul 10, 2008 (BUSINESS WIRE) -Internap Network Services Corporation (NASDAQ:INAP), a global provider of endto-end Internet business solutions, announced today its Performance IP(TM) network and data center colocation services are being used by OpSource(TM), the leader in Web operations, to power their Software-as-a-Service (SaaS) delivery solutions and European expansion. The Internap services support OpSource's comprehensive Web operations solution including OpSource On-Demand(TM), OpSource Billing(TM), and OpSource Connect(TM) as well as the company's growing business in Europe.	colocation centre;data center;dot-com company;software as a service;web service	Joachim Niemeier	1999			web service;web application security;web development;business process execution language;web analytics;web standards;computer science;ws-policy;service-oriented architecture;services computing;web intelligence;ws-i basic profile;web 2.0;world wide web	Web+IR	-75.6906659438987	17.795701110118348	53055
99bf343556cb253fc9b2905127bc81f28f9d3f09	framework for optimizing collaboration using stimulation		Though collaboration is an important factor in any organization’s business cycle, but achieving collaboration is not straight forward and poses many challenges. In order to enhance collaboration various tools and techniques have been proposed. Proposed research work aimed at finding out elements that can stimulate collaboration. How these stimuli can be utilized along with other factors to positively affect collaboration is also part of project. A measurement model is also intended to be developed under proposed work to study and optimize stimuli in collaborative processes. Finally a framework which provides basis for an adaptive environment for collaboration using stimuli and collaboration measurement model has been proposed along with tool support. Validation of proposed research work will be carried out through empirical analysis, process mining, case studies, experimentation and other available methods. Empirical analysis approach for extracting data from real world case-studies will be used.	experiment;optimizing compiler	Muhammad Muneeb Kiani	2015			management science;process mining;stimulation;computer science	SE	-78.774772087942	9.252362139942862	53070
40fe900fb62f61f19401491ad4486e50735d93d1	issue-based variability management	rationale management;empirical software engineering;requirements engineering;product line engineering	0950-5849/$ see front matter 2012 Elsevier B.V. A http://dx.doi.org/10.1016/j.infsof.2012.02.005 ⇑ Corresponding author at: Harman International, G E-mail address: Anil.Thurimella@gmail.com (A.K. T Context: Variability management is a key activity in software product line engineering. This paper focuses on managing rationale information during the decision-making activities that arise during variability management. By decision-making we refer to systematic problem solving by considering and evaluating various alternatives. Rationale management is a branch of science that enables decision-making based on the argumentation of stakeholders while capturing the reasons and justifications behind these decisions. Objective: Decision-making should be supported to identify variability in domain engineering and to resolve variation points in application engineering. We capture the rationale behind variability management decisions. The captured rationale information is useful to evaluate future changes of variability models as well as to handle future instantiations of variation points. We claim that maintaining rationale will enhance the longevity of variability models. Furthermore, decisions should be performed using a formal communication between domain engineering and application engineering. Method: We initiate the novel area of issue-based variability management (IVM) by extending variability management with rationale management. The key contributions of this paper are: (i) an issue-based variability management methodology (IVMM), which combines questions, options and criteria (QOC) and a specific variability approach; (ii) a meta-model for IVMM and a process for variability management and (iii) a tool for the methodology, which was developed by extending an open source rationale management tool. Results: Rationale approaches (e.g. questions, options and criteria) guide distributed stakeholders when selecting choices for instantiating variation points. Similarly, rationale approaches also aid the elicitation of variability and the evaluation of changes. The rationale captured within the decision-making process can be reused to perform future decisions on variability. Conclusion: IVMM was evaluated comparatively based on an experimental survey, which provided evidence that IVMM is more effective than a variability modeling approach that does not use issues. 2012 Elsevier B.V. All rights reserved.	common criteria;design rationale;domain engineering;heart rate variability;metamodeling;open-source software;problem solving;software product line;spatial variability	Anil Kumar Thurimella;Bernd Brügge	2012	Information & Software Technology	10.1016/j.infsof.2012.02.005	idef6;systems engineering;engineering;domain engineering;management science;requirements engineering;empirical process;management	SE	-69.6937736318411	19.708216402134614	53136
bb83b3cf164fafd63906ccc23aee44cbaaa21268	business information systems	concepts incrementally;practical example;comprehensive introduction;integrated understanding;management student;prior knowledge;business information systems;strong theoretical base	The recent establishment of cloud computing and its increasing importance for consumers have attracted new ways of service creation and provisioning over the internet. Research has dealt with the establishment of new cloud computing markets and new opportunities for collaboration in these emerging markets. We found that the necessary decisions are very much similar to decisions that have to be made in “classic” offline supply chains. Based on methods for supplier selection, we develop and illustrate a method for the evaluation and selection of cloud services. The fuzzy AHP approach we propose allows decision makers to account for uncertainty in a proven, well-structured way.	cloud computing;information systems;management information system;online and offline;provisioning	Witold Abramowicz	2013		10.1007/978-3-642-38366-3	business activity monitoring;process management;information system;business process modeling;artifact-centric business process model;information engineering;strategic information system;business;new business development;business rule	HCI	-72.57083001541378	7.479737905484221	53162
9effab0d1616311d917568d8f27a4c1f2e0381d9	improving service quality and productivity: exploring the digital connections scaling model	production function;cyber infrastructure;service cycle times;enterprise engineering;service science;scaling;transaction costs;productivity;digital connections;extended enterprises;service quality	The basic model argues that Digital Connections Scaling (DCS) of customers, providers and/or resources is a fundamental way to reduce service cycle time and transaction cost, and thereby to improve service quality and productivity. Digitisation makes entities connectable, and scaling decreases the marginal cost for the customer and the provider to cocreate new values. Three types of economies of DCS are postulated: the accumulation effect, the networking effect and the ecosystem effect on facilitating value propositions and cocreation. The paper also presents enterprise engineering principles, new micro-economic production functions, and an extended cyber-infrastructure model to substantriate DCS.	ecosystem;enterprise engineering;entity;image scaling;marginal model;scalability;tree accumulation	Cheng Hsu;James C. Spohrer	2009	IJSTM	10.1504/IJSTM.2009.024093	service level requirement;transaction cost;productivity;economics;scaling;knowledge management;marketing;operations management;service design;production function;management;service quality	Networks	-75.96991873538313	7.044795999849285	53226
7bae25d5e3df7a60aff8bc06e1c770e20d403fc9	the value system designer - an infrastructure for building the virtual enterprise	value system designer;virtual enterprise;value system	Much research has been conducted on what a virtual organisation is and how it should work With this paper I will address the question, how a virtual enterprise can be designed to be agile and to so best support its short-term business opportunities. I shall present a framework for the organisational design and the changing business roles of the ‘business architect’ who constructs the various phases of the virtual enterprise’s lifecycle. I refer to this infrastructure for creating virtual enterprises as the ‘Value System Designer’; a set of methods and tools to select partners, reengineer business- and logistic processes and to set up an information and communication platform for the virtual enterprise. These methods and tools have been developed in the EU project TELEfiow and the Eureka ‘Virtuelle Fabik’ project. I shall focus on the experiences gained from the numerous projects and summarise crucial success factors for designing virtual enterprises. Thus, this paper gives insights and applicable know-how for companies and managing engineers acting as virtual enterprise architects, for example leaders of project consortia or joint ventures or as first-tier suppliers co-ordinating supplier (sub-) nets.	virtual enterprise	Bernhard R. Katzy	1999			enterprise systems engineering;enterprise software;systems engineering;engineering;knowledge management;operations management;enterprise integration;enterprise planning system;enterprise information system;enterprise life cycle	OS	-66.50997351731627	17.32306090375486	53292
aab6f5e051fdf7de14c14f48c50c0169a940446c	are supplier selection criteria going green? case studies of companies in brazil	management industrial management;selection;journal article;marketing distribution of products;environmental sciences;supplier evaluation;brazil;artigo;environmental management;face to face;supply chain management;supplier selection;design methodology	Purpose – The purpose of this paper is to verify if Brazilian companies are adopting environmental requirements in the supplier selection process. Further, this paper intends to analyze whether there is a relation between the level of environmental management maturity and the inclusion of environmental criteria in the companies’ selection of suppliers. Design/methodology/approach – A review of mainstream literature on environmental management, traditional criteria in the supplier selection process and the incorporation of environmental requirements in this context. The empirical study’s strategy is based on five Brazilian case studies with industrial companies. Face-to-face interviews and informal conversations are to be held, explanations made by e-mail with representatives from the purchasing, environmental management, logistics and other areas, and observation and the collection of company documents are also employed. Findings – Based on the cases, it is concluded that companies still use traditional criteria to select suppliers, such as quality and cost, and do not adopt environmental requirements in the supplier selection process in a uniform manner. Evidence found shows that the level of environmental management maturity influences the depth with which companies adopt environmental criteria when selecting suppliers. Thus, a company with more advanced environmental management adopts more formal procedures for selecting environmentally appropriate suppliers than others. Originality/value – This is the first known study to verify if Brazilian companies are adopting environmental requirements in the supplier selection process.	capability maturity model;email;environmental resource management;futures studies;h2 database engine;logistics;new product development;purchasing;requirement	Ana Beatriz Lopes de Sousa Jabbour;Charbel J. C. Jabbour	2009	Industrial Management and Data Systems	10.1108/02635570910948623	selection;supply chain management;economics;design methods;marketing;operations management;supplier relationship management;management	SE	-82.93049274102721	11.62053852676526	53294
915306fd488f937d8531c4d9695923d8235be7cc	integrating open innovation and business process innovation: insights from a large-scale study on a transition economy	transition economy;open innovation;survey;business process innovation	Open innovation and Business Process Innovation (BPI) have been investigated by their respective research communities for decades. However, the important relationship between externally focused open innovation and internally implemented BPI is currently unexplored, especially in transition economies. The main purpose of our research is to contribute towards closing this important research gap. This paper describes the main findings of a research study of innovation practices of 224 companies operating in a transition economy. We propose and validate a comprehensive model of integrative-innovation and offer some important insights into the relationship between externally-focused R&D collaboration and a firm’s internal process innovation.	business process interoperability;closing (morphology);knowledge management;open innovation;word lists by frequency	Amila Pilav-Velic;Olivera Marjanovic	2016	Information & Management	10.1016/j.im.2015.12.004	innovation;economics;innovation management;knowledge management;marketing;open innovation;management	HCI	-78.00231080162857	5.514156229344117	53480
866c69cea5ffc843b1eea4705ad23661bbbfecd2	collaborative infrastructure for test-driven scientific model validation	unit testing;model validation;cyberinfrastructure	One of the pillars of the modern scientific method is model validation: comparing a scientific model's predictions against empirical observations. Today, a scientist demonstrates the validity of a model by making an argument in a paper and submitting it for peer review, a process comparable to code review in software engineering. While human review helps to ensure that contributions meet high-level goals, software engineers typically supplement it with unit testing to get a more complete picture of the status of a project.   We argue that a similar test-driven methodology would be valuable to scientific communities as they seek to validate increasingly complex models against growing repositories of empirical data. Scientific communities differ from software communities in several key ways, however. In this paper, we introduce SciUnit, a framework for test-driven scientific model validation, and outline how, supported by new and existing collaborative infrastructure, it could integrate into the modern scientific process.	high- and low-level;mathematical model;software engineer;software engineering;unit testing	Cyrus Omar;Jonathan Aldrich	2014		10.1145/2591062.2591129	computer science;engineering;data science;software engineering;data mining;management science;regression model validation;unit testing;programming language	SE	-67.0262636083075	26.699484489062893	53558
66baa816ca75cd154075fe96898fbee610459c0d	enterprise internationalisation by foreign investments and technical cooperation	slovenia;foreign direct investment;technical cooperation;national economy;defence sector;international investments;input output;input output analysis;foreign investment;regional development;indirect effect;international investment;design methodology	Purpose – The purpose of this paper is to investigate the internationalisation of enterprises by specific forms of foreign direct investments (FDI) and international technical cooperation in the defence sector.Design/methodology/approach – The effects of defence FDI and international technical cooperation activities on the Slovenian economy are investigated using the input‐output modelling approach.Findings – The author finds important direct and indirect effects on the Slovenian economy from the FDI and international technical cooperation inflows into enterprises. These positive effects are directly and indirectly linked to enterprises in different statistically classified industrial activities by predominance of the civil sector, which is important for the development of defence and civilian high‐technological base products in different regional parts of Slovenia.Originality/value – The presented findings can be used to arrange industrial and regional development strategy and policy measures aimed at ac...	internationalization and localization	Stefan Bojnec	2011	Industrial Management and Data Systems	10.1108/02635571111137269	economics;international trade;foreign direct investment;economy;management;economic system	Robotics	-84.62962938822182	6.1302320471769995	53586
99dab7f57a7fca31647efcb605938bbf7242ad39	cloud service platform: hospital information exchange(hix)	cloud service;business model;china;health information exchange hix;health manager system;cloud computing	Health Information eXchange HIX is a part of Happiness Cloud Service Platform of Happiness Guangdong in Guangdong Province of China based on innovation of cloud-based business model. This article illustrates the hospital health care business services system based on cloud computing. major business functions of HIX includes integrated mobile medical information services, and mobile health information services. Key cloud service platform capabilities include appointment of HIX registration, doctor-patient interaction and Health Manager System, medical statistical analysis, and the other integrated support module including service platform and platform management provided by two major cloud computing technologies of SaaS and PaaS. Medical cloud services of HIX is an innovative business model for cloud computing, that is, the medical and health services provided to the public going though by cloud computing all over Guangdong Province in China.		Zhiyuan Fang;Li Wei	2013	IJISSS	10.4018/jisss.2013070104	cloud computing security;economics;cloud computing;computer science;marketing;operating system;services computing;world wide web;commerce	Mobile	-70.37742611670201	9.510104882889555	53609
881a75816ff0f9e4bc1a930e47c8694d013c5bd6	vendor strategies for business process and applications outsourcing: recent findings from field research	outsourcing;electronic commerce;information technology;information technology outsourcing electronic commerce internet;outsourcing communication industry computer industry application specific processors convergence telecommunication computing web and internet services taxonomy web services service oriented architecture;ebusiness business process outsourcing internet products internet services applications services provisioning customer caution web services;internet	The convergence between telecommunications and computing industries enabling the provision of Internet/net centric products and services has seen the emergence and collapse of the first phase applications services provisioning (ASP) industry. This paper discusses the findings of a longitudinal research program comparing traditional and applications outsourcing methods and practices. Preliminary research has uncovered a mismatch between vendor hype about the benefits of the ASP business model, and customer caution in adopting the solution. Developing a taxonomy of net sourcing approaches, the paper argues that scale, scope and integration are three key areas which must be addressed by vendors in their business plans. With the advent of web services as a new IT architecture, it is suggested applications outsourcing will extend to business processes outsourcing as important challenges relating to scale, scope and integration will be overcome. Introduction Towards the late 1990s, the literature on ebusiness largely comprised of speculation about the benefits for large and small firms from e-business in the knowledge driven economy (DTI, 1999). The central theme of much of this literature was based around the traditional rhetoric of the automate or liquidate debate, which suggests that firms either embrace the latest information and communications technologies (ICTs) or become uncompetitive. In the knowledge-based economy of the 21 century, ICT industry vendor-hype surrounding the potential benefits of e-business continued unabated. The convergence of the telecommunications and computing industries saw the mobilisation of resources by firms to develop new ebusiness opportunities (Chatterjee et al, 2002). This suggested a revision in information systems sourcing strategies, as many pundits argued that applications sourcing would present new challenges to traditional sourcing methods and practices (Howcroft, 2001). Throughout the year 2000, numerous firms entered the marketplace as self-styled application services providers (ASPs). ASPs came in several guises from telecommunications firms offering web hosting and software application enablement services, to start-up ASPs with no IT infrastructure of their own, but the desire to offer industry-focused (vertical)) or businessfocused (horizontal) software applications to their customers. Independent software vendors (ISVs) also 0-7695-1874-5 saw opportunities for web-enabling their software applications with the option of either becoming an ASP themselves or through a licensing arrangement with an ASP (Currie and Seltsikas, 2001). Unlike the service bureaus of the 1980s, the ASP model would leverage the power and flexibility of Internet-based computing to enable secure, high performance, highly available delivery of enterprise, vertical and business applications over the network. Some commentators argued that ASP was simply a modern-day service bureau for the networked age. Application outsourcing, using the ASP model, would be different from traditional outsourcing since software applications were priced on a rental (per-seat, per-month) contract. Software-as-a-service was to become the new silver bullet, for both vendors and customers alike. In particular, small medium businesses (SMBs) would be able to benefit from this utility model, as pay-as-you-go software applications procurement would reduce their total cost of ownership (TCO) of IT. By the end of 1999, SMBs were being offered a variety of enterprise, vertical or businesses software applications from ASPs at an affordable price. This paper draws from a five-year research study (ongoing) on the market, business, managerial and technical challenges facing global firms, focusing specifically on traditional information systems outsourcing models and frameworks and application outsourcing, involving ASPs and web services. The paper is divided into three sections. First, it gives an overview of the research study. Second, it compares and contrasts literature on traditional outsourcing with applications outsourcing. Third, by drawing from the findings of two ongoing research projects, it develops a conceptual model on information systems sourcing scenarios. The model evaluates vertical application sourcing provisioning (V-ASP), business application sourcing provisioning (B-ASP), joint venture sourcing (JVS) and business processing outsourcing (BPO) in relation to scale (the extent to which a firm enters into outsourcing contracts in relation to vendor capabilities), scope (the extent to which it is possible to source specific activities, tasks, processes or applications from a third party vendor) and integration (the extent to which software applications can be integrated across business processes). Finally, the paper concludes by suggesting that a knowledge-gap exists between vendors and 1 The two research projects are funded by the EPSRC and ESRC and run from 2001-2003 and 2002-2004 respectively. /03 $17.00 (C) 2003 IEEE 1 Proceedings of the 36th Hawaii International Conference on System Sciences 2003 customers, which has, to some extent, inhibited the adoption of net sourcing. Research Study As the 21 century began, practitioner and academic interest in e-business as the new panacea to enhance competitive performance was widespread. Numerous books, articles and conferences emerged with the aim of educating management about the many advantages of e-business. During this period, the term application services provision or ASP became a hot topic. The ‘hype’ generated by the IT industry claimed the ASP revolution would grow to $25 bn by 2005 (Davis, 1999). According to the ASP Industry Consortium, ‘An ASP manages and delivers application capabilities to multiple entities from data centres across a wide area network’. ASP was a sub-set of e-business. The ASP phenomenon was perceived as a revolution in the IT industry as the business model of offering software-as-a-service would penetrate the SMB market. Application outsourcing using an ASP model was highly relevant to traditional outsourcing models, such as total and selective sourcing. Some commentators (Cherry Tree, 1999) suggested that new methods and practices would need to be developed for application outsourcing since traditional outsourcing was largely a business model based upon a one-to-one relationship between the supplier and customer. ASPs using a one-tomany model would therefore need to develop customer relationships serving remote rather than local delivery of software applications. Taking this work as a starting point, a pilot study was developed to elicit data and information on the much-hyped ASP business model. The research methodology comprised of qualitative and quantitative approaches. Semi-structured interviews were carried out with managers and technologists from vendor firms. Field research was conducted in Silicon Valley in the USA and in the UK, France and Germany. Twenty-eight firms were visited, ranging from independent software vendors (ISVs), Internet services providers (ISPs), co-location and data centre firms, telecommunications firms (Telco’s), networking and hardware firms. All of these firms had entered, or were planning to enter, the ASP market, either as technology enablers (providing the hardware, data hosting or networking) or software products and services providers (ASPs, ISVs, systems integrators or consultants). A questionnaire survey was targeted at small and medium businesses (SMBs) in the UK. SMBs with between 50 and 150 employees were selected to determine the extent to which they outsourced all or part of their IT facility. They were also questioned about their knowledge of the ASP market and if they were or planning to use the services of an ASP. The questionnaire was designed to elicit data and information on specific types of software applications used by firms rather than treat technology as a black box (Benbasat, 2001). 0-7695-1874-5 The results of the pilot research raised many more questions than answers. The data and information from the field research in the US and Europe confirmed industry trends towards application outsourcing as a means of generating new business. In Silicon Valley, many firms had won first-round venture capital to enter the ASP market, although the interviews with CEOs, CIOs and other professional personnel (marketing and finance directors) suggested that several business and technical challenges needed to be overcome if the ASP model was to succeed. For example, many of the software applications being offered by ASPs had not originally been written as web-enabled software solutions. Many business applications (i.e. HR, accounting, fixed asset management, etc) and vertical applications (i.e. manufacturing, health and retail) were written for client server technologies. As a result, webenabling these software applications as either a replacement of existing ones or offered as new ones was not sufficient in itself to entice customers. The large enterprise resource planning (ERP) vendors had all began ASP business to offer SMBs ‘vanilla ERP’ applications, only to find that such a strategy was like using a sledgehammer to crack a nut. ERP vendors did not have the channel partnerships with SMBs to gain traction in the market, nor did the SMBs wish to replace their legacy software applications with new offerings, particularly if they did not offer any cost advantages. This led many ERP vendors to revise their ASP business models, with one firm abandoning its direct channel in favour of using value added resellers (VARs). In view of the above, it was apparent that many ASPs, without their own IT infrastructure, needed to partner with other firms (Telco’s, ISVs, ISPs, systems integrators, data centre providers, etc). However, many later found that these partnerships proved more difficult to negotiate than was originally anticipated. The questionnaire survey into customer perceptions of the ASP busines	black box;book;business process;business software;client–server model;consortium;data center;erp;electronic business;emergence;enterprise resource planning;entity;field research;go software;independent software vendor;information system;interviews;legacy system;no silver bullet;one-to-one (data model);outsourcing;procurement;provisioning;semiconductor industry;server (computing);server message block;software as a service;systems integrator;total cost of ownership;traction teampage;vertical market software;web hosting service;web service	Wendy L. Currie;Bhavini Desai;Naureen Khan;Xinkun Wang;Vishanth Weerakkody	2003		10.1109/HICSS.2003.1174791	the internet;computer science;marketing;knowledge process outsourcing;services computing;law;information technology;commerce;outsourcing	HCI	-74.20701339460332	6.067324214297755	53654
315e72e1924fe564edc9d3b2ffdb882ad73acc0c	a retrofitting auction service business model proposal within a smart city context		Smart City initiatives are currently a common place, with several projects initiating. The SusCity Project is one of such projects, on-going within the city of Lisbon, focusing on the energy efficiency of buildings. One of the objectives of the project is to create value to the city, therefore proposing new business models to be adopted. Therefore, we have identified auction services as one possible solution for energy efficiency in residential buildings. We see an opportunity to extend our previously defined PGR approach to generate a new business model supporting the use of retrofitting auction services and demonstrate it within the SusCity Project. Our main contributions reside in the extension of the PGR approach with the added heuristics definition and use of the Lean Canvas to support the generation of the intended business model.	binary symmetric channel;business model canvas;canvas element;diagram;e-services;heuristic (computer science);intelligent platform management interface;smart city	Pedro Torrinha;Carlos E. Salgado;Ricardo J. Machado	2018	2018 Third International Conference on Fog and Mobile Edge Computing (FMEC)	10.1109/FMEC.2018.8364077	management science;smart city;energy consumption;efficient energy use;unified modeling language;heuristics;retrofitting;business;business model	SE	-66.64706009008414	10.401940966562034	53673
a3b66928515b08ba64ba13e8cd0d773539f6ecd8	an integrated design framework for mass customisation in the consumer electronics industry	consumidor;dynamic programming;concepcion modular;conception conjointe;mass customisation;proceso concepcion;multidisciplinaire;entreprise;concepcion ingenieria;programacion dinamica;engineering design;mc;design process;diseno conjunto;conception ingenierie;consommateur;product planning;empresa;collaboration;customization;personnalisation;developpement produit;industrie electronique;consumer electronics;lenguaje algebraico;mass production;by product;product styling;integrated design;concepcion integrada;agile design;scenario;codesign;sous produit;consumer;electronique grand public;electronics industry;subproducto;comportement utilisateur;firm;industria electronica;programmation dynamique;personalizacion;modular design;scenario planning;multidisciplinary;estructura producto;langage algebrique;co design;planning produit;multidisciplinar;user behavior;collaborative design;produccion en masa;production masse;web3d;product families;article;structure produit;algebraic language;conception integree;desarrollo producto;comportamiento usuario;product structure;product architecture;programacion producto;electronica de consumo;processus conception;conception modulaire;product development	This paper investigates a framework for Mass Customisation (MC) in the consumer electronics industry. While personalisation is motivating the adoption of MC, providing adequate options for matching consumer needs remains a major challenge. This challenge can be conquered by applying scenario planning, product family architecture and a product styling platform. Simultaneously, introducing Web3D-based consumer co-design, enterprises can understand user preferences and then improve design concepts. An integrated framework and Agile Design Process (ADP) are proposed for achieving multi-disciplinary collaboration by these methods. Pilot surveys participated by design personnel indicate that the framework and ADP have high potential for harnessing MC.	personalization	Hassan Abdalla;Tun-Hsueh Chan	2011	IJCAT	10.1504/IJCAT.2011.038549	co-design;simulation;computer science;engineering;management;engineering design process	EDA	-82.94968856936345	9.9979087519971	53680
a1dd5654d5a6b0b97495fa162f05150155a85071	workshop on directions in software engineering environments (wodisee)	software engineering tools;automotive software engineering;software engineering;research and development;software engineering environment;icse workshop	The goal of this workshop was is to bring togetherresearchers and practitioners with an interest indeveloping, extending, deploying and using softwareengineering tools. Theis workshop will provides aninteractive forum for the exchange of ideas anddiscussion about future trends in software engineeringenvironment research and development.The outcomes of this workshop will beare a summary ofthe state of the art in software engineering environmentresearch and development, and the identification of keydirections for future research in this area.	icse;knowledge spillover;snapshot (computer storage);software engineering	John C. Grundy;Ray Welland;Hermann Stoeckle	2004		10.1145/1022494.1022509	engineering management;personal software process;systems engineering;engineering;social software engineering;software engineering;requirements engineering;software walkthrough;software development process;software peer review	SE	-65.97489005313376	23.884040613560863	53683
98f86707e115b0467ba7bd3b599a31729b6851fe	barriers to the adoption of management accounting (ma) processes in enterprise resource planning (erp) environments	software;organizations standards organizations management accounting technological innovation software costing process control;technological innovation;standards organizations;costing;organisational aspects enterprise resource planning management accounting;erp skill management accounting process ma process enterprise resource planning environment erp environment information technology enterprise resource planning system erp system case study organization technology related barrier organizational related barrier;barriers;erp;management accounting;adoption erp management accounting barriers case study;process control;organizations;adoption	Coupling of information technologies with management accounting (MA) processes is a proven method of exploiting their full potential. In spite of that, adoption of these processes embedded in enterprise resource planning (ERP) systems is limited. This study, using a cross-sectional field study conducted across four case study organizations, investigates the barriers to the adoption of MA processes in ERP environment and identifies challenges. Poor reporting capabilities of ES and ineffective upgrades and customization are some of the perceived technology related barriers, the study found. In addition, lack of resources and skills, work-a rounds', uncontrolled excel use and accountants' generic preference to use excel are other organizational related barriers limiting the adoption. Either, because of the constrained resources for adoption or because of individual managerial preferences to use 'work-a rounds' and/or inadequate ERP skills, managements do not attempt to exploit the full potential, the study observed.	cross-sectional data;erp;embedded system;enterprise resource planning;field research;uncontrolled format string	Ravi Seethamraju	2015	2015 48th Hawaii International Conference on System Sciences	10.1109/HICSS.2015.503	organization;knowledge management;environmental resource management;process control;human resource management system;management accounting;activity-based costing	HCI	-77.15607818795893	9.096246426957274	53727
a363ca2da34bdc1333d85601c61d510bbcbc2761	towards defining dimensions of knowledge systems quality	knowledge managements systems;intellectual capital;knowledge assets;knowledge systems;quality dimensions;knowledge management system;ontology;knowledge work;competitive advantage	Knowledge management systems (KMS) are extremely important for organisations, primarily because they help to manage a key organisational resource - intellectual capital with the potential to produce a competitive advantage. The usefulness of this resource, however, is only as good as the quality of the knowledge that it contains. In order to improve the quality of KMS, a set of test measures is required. The purpose of this paper is to define some dimensions that can be used to measure the quality of the knowledge management system and to compare KMS quality across systems.	knowledge-based systems	Lila Rao-Graham;Kweku-Muata Osei-Bryson	2007	Expert Syst. Appl.	10.1016/j.eswa.2006.05.003	knowledge economy;computer science;knowledge management;body of knowledge;ontology;personal knowledge management;knowledge value chain;domain knowledge;competitive advantage	DB	-78.88721520084471	4.317801360668926	53844
e0b256d5e3aedc269fd93599422af8147dbce925	austria from cscw over cwe to ce: the evolution of needs and tools mates revisited		Meetings, owning a central role in engineering projects, are the major way of performing the information exchange necessary for other engineers to make progress in their mutually depending tasks. They also play a central role in getting consensus and in achieving quality. However, when project members are not colocated this works less well. Travelling, standard remedy for this, is not an acceptable solution in the long run. Besides direct travel expenses, even greater cost are caused by wasting human time and energy due to commuting-like work situations. A solution must be “well integrated” and easily accessible in daily working situations. Current alternatives, like TV conferences in separate studios, still require co-location, scheduling, and disconnect people from usual working contexts. It must instead be light-weight and conveniently available, providing a media for the fine network of human interaction, the driving force in complex problem solving. Improving the efficiency of collaboration processes is the most promising way of increasing productivity in knowledge intensive projects. Primarily, easy-to-use means for interaction between people and between people and information, independent of place and time, are to be provided. The area of distributed information multimedia, groupware and CSCW is addressing this need. Quote from “The Euro-presence White Paper”, D. Schefström, CDT/Luleå, Sweden (1995)	collaborative software;colocation centre;common weakness enumeration;computer-supported cooperative work;eclipse;information exchange;mind;pervasive informatics;problem solving;scheduling (computing);vision document;workspace	Ger v. d. Broek;Franz-Josef Stewing	2007			systems engineering;white paper;control engineering;collaborative software;information exchange;mathematics;scheduling (computing);computer-supported cooperative work;studio	HCI	-66.82477371833029	16.157464079621427	53899
38a92c1530b8bb6418ecaa5c77c98755cc5279f1	evolving communities of practice: ibm global services experience	community of practice;growth and development;business model	In 1995, IBM Global Services began implementing a business model that included support for the growth and development of communities of practice focused on the competencies of the organization. This paper describes our experience working with these communities over a five-year period, concentrating specifically on how the communities evolved. We present an evolution model based on observing over 60 communities, and we discuss the evolution in terms of people and organization behavior, supporting processes, and enabling technology factors. Also described are specific scenarios of communities within IBM Global Services at various stages of evolution.	organizational behavior	Patricia Gongla;Christine R. Rizzuto	2001	IBM Systems Journal	10.1147/sj.404.0842	business model;systems engineering;engineering;knowledge management;management science;management	HCI	-74.20247183995812	9.322304779252223	53958
9d95710853ffe468c9263a68cc646dfc868705bf	a quality assurance approach and case study in bss	quality assurance;software;probability;standards;correlation quality assurance capability maturity model standards probability mathematical model software;capability maturity model;mathematical model;testing case reusability approaches bss quality assurance approach business support systems telecom market test case design test data correlation based approach test case selection;correlation;quality assurance bss test case selection;software reusability business data processing program testing quality assurance	The BSS (Business Support Systems) has played a critical role in strictly competition telecom market. They constantly face stringent challenges such as quality assurance as well as time to market and budget. The test case design is an important step and a higher cost in system quality assurance phase. However by knowledge few test data correlation based approach has some drawbacks such as the coverage limitations and effectives of test case selection. We believe if there is a suitable testing case reusability approaches would have good and enough coverage and lower cost. Therefore, in this paper, we present a quality assurance framework and reusability based approach to design and select test case effectively. On the other hand the study introduces a real world BSS quality assurance works as case study to practice and examine the proposed approach. This article is a continuous work of the BSS transformation project.	analytical hierarchy;business requirements;numerical analysis;regression testing;source-to-source compiler;test case;test data	Teh-Sheng Huang;Chia-Yen Chan;Jeu-Yih Jeng	2016	2016 30th International Conference on Advanced Information Networking and Applications Workshops (WAINA)	10.1109/WAINA.2016.89	program assurance;quality assurance;qa/qc;functional testing;probability;mathematical model;correlation;capability maturity model;statistics;software quality analyst	SE	-63.81402023299848	28.165005793604507	54034
2b6050d4aab88f208355e39251aa51ee8deec599	security testing of internal tools	quality assurance;quality assurance security testing internal tools software industry;testing;software engineering;software engineering program testing quality assurance security of data;software tools software testing computer industry information security software quality computer hacking shipbuilding industry employee rights personnel books;software security;program testing;building security in;software industry;security testing;building security in software security testing;security of data	As the software industry continues to mature, software companies are realizing that they must dedicate more resources to quality assurance (QA) processes. But even though security testing as part of an overall QA process for products shipped to customers is starting to gain acceptance in the software industry as a necessity, the majority of software vendors pay little to no attention to the security of the tools they use internally. In this article, the author explore why testing (security testing in particular) on internal tools should be incorporated into the QA process. In short, a responsible software company shouldn't produce insecure software, regardless of whether this software is meant for internal use only.	security testing;software industry;software quality assurance	Edward Bonver	2008	IEEE Security & Privacy	10.1109/MSP.2008.21	software security assurance;quality assurance;verification and validation;system integration testing;software project management;computer science;acceptance testing;package development process;backporting;social software engineering;software reliability testing;software development;software construction;software testing;security testing;software deployment;software quality control;computer security;software quality;software quality analyst;software peer review	SE	-67.94195393765729	26.958636304584996	54064
16011a37c88433cbd80d03ee45c35fa026c8d43c	workshop for e-government via software services (wegovs2 2009)	software;semiconductor optical amplifiers;dynamic reconfiguration;e government;software services;service oriented architecture e government software services;best practice;government;heterogeneous environment;satisfiability;data mining;information presentation;software architecture;software architecture government data processing;electronic government;conferences service oriented architecture government software semiconductor optical amplifiers context data mining;service oriented architecture;government data processing;context;conferences	e-government is a strategic direction to reduce administrative burden and make government more efficient and servicedirected. Despite recent achievements in research and actual implementation of e-Government in practice, numerous questions still remain on how to achieve full interoperability and dynamic reconfiguration of e-government services, with better accessibility, more transparency and manageability. The Service-Oriented Architecture (SOA) paradigm provides the necessary theoretical and technological support for the desired electronic government systems to become a reality. It satisfies the demand for a highly distributed heterogeneous environment with a large number of autonomous services and their providers together with users with their own goals and preferences. Therefore, the combination of the e-Government domain and SOA appears to be natural, as SOA provides an ideal platform to achieve the goal of a citizen-oriented and citizen-centric government organization. This workshop will provide a forum to discuss how the SOA paradigm may facilitate further advances in e-Government research as well as best practice examples of already implemented and applied solutions in the context of e-Government. We plan to have both formal and informal presentations on theoretical, technical and implementation aspects of the application of SOA for e-Government challenges and issues. The workshop’s primary topic is an adaptation of SOA research results in the context of e-Government. As the WETICE primary interest this year deals with services and service applications, the workshop is perfectly suited within the WETICE main theme. The demand of the e-government domain for a solid collaborative infrastructure provided an additional interest for workshop participants to attend colocated WETICE workshops, and vice versa. The workshop will be held at the University of Groningen on June 29–July 1, 2009, it is in its first edition and forms part of the 18th IEEE International Workshop on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE 2009). An international programme committee of academic and industrial experts in the fields of service-oriented computing, software architecture, distributed computing, Quality of Service in e-government platforms, variability management, e-service delivery, and of various aspects of social, technical, and economical aspects of eGovernment. The chairs received several papers, one of which was accepted for presentation. Each paper was reviewed by at least three members of the programme committee. The accepted paper A Classification of Software Engineering Approaches to Service Oriented Architectures in Various Fields by A. Kontogogos and P. Avgeriou covers the main aspects of the workshop by providing a survey of the existing work in the area of service-oriented computing, and its applicability to e-government. The paper presents a highlevel description and comparison of the SOA approaches by illustrating six different frameworks and summarising their key features. Authors present and analyze some of the most influential approaches from a software engineer’s point of view that belong either to the academic or to the industrial field. Despite their differences though, all of these approaches share a service oriented mentality, with the purpose of lessening the issues of clients and companies, students and teachers, citizens and government employees alike. The workshop will host an invited keynote speaker, Dr. Carsten Hentrich, an experienced architect in the domain of SOA in e-Government. Dr. Hentrich will elaborate on the major drivers for this domain: long term influences like EU law and demographic changes can already be observed as important drivers for change in European governments. Those influences shape how governmental institutions provide their services, how they operate internally and with each other; even across national boundaries and according to international European regulations. The IT of governmental institutions needs to deal with these long term influences. SOA as a strategy is a possible way to deal with these changing circumstances. Dr. Hentrich will illustrate how those different influences drive changes towards SOA in governmental institutions in Germany. In this context, the specific challenges of SOA in e-Government will be highlighted according to examples from 2009 18th IEEE International Workshops on Enabling Technologies: Infrastructures for Collaborative Enterprises	accessibility;autonomous robot;best practice;colocation centre;distributed computing;e-government;e-services;heart rate variability;itil;interoperability;programming paradigm;quality of service;service-oriented architecture;service-oriented device architecture;software architecture;software engineer;software engineering	Alexander Lazovik;Paris Avgeriou;Hans Wortmann	2009	2009 18th IEEE International Workshops on Enabling Technologies: Infrastructures for Collaborative Enterprises	10.1109/WETICE.2009.69	software architecture;computer science;knowledge management;operating system;software engineering;database;distributed computing;management;law;world wide web;computer security;government;best practice;oasis soa reference model;satisfiability	Visualization	-63.712214140514654	20.468637220401934	54239
3244812d75296723332feb507bceeabaa46cd61b	rts - an integrated analytic solution for managing regulation changes and their impact on business compliance	text analytics;semantic;document processing;question answering	Governance, Risk Management and Compliance are key success factors for corporations. Every company worldwide must ensure a proper compliance level with current and future laws and regulations, but managing the dynamic nature of the regulatory environment is a challenge, for both small and medium business as well as large corporations. Specifically the challenge is knowing and interpreting which regulations impact a particular business. Governments and standard bodies keep producing new, revised legislation, and businesses today rely on employees and consultants for tracking and understanding impact on their operations.  This paper introduces a novel prototype solution that addresses these concerns through the use of advanced text analytics. In particular the system is able to discover sources of regulatory content on the world wide web, track the changes to these regulations, extract metadata and semantic information and use these to provide a semantically guided comparison of regulation versions. Moreover, by leveraging the IBM DeepQA architecture, the solution is able to cross reference business objectives with the regulatory database and provide insights about the impact of new and revised laws on a company's business.	cross-reference;governance, risk management, and compliance;prototype;risk management;text mining;watson (computer);world wide web	Davide Pasetto;Hubertus Franke;Weihong Qian;Zhili Guo;Honglei Guo;Dongxu Duan;Yuan Ni;Yingxin Pan;Shenghua Bao;Feng Cao;Zhong Su	2013		10.1145/2482767.2482798	public relations;engineering;knowledge management;data mining	DB	-72.76295788841547	10.779279281102113	54336
938d9abf994ecf41548e59ef4493053af306d413	dynamic game analysis in program emergency management of engineering construction company	engineering construction company;project management;game theory;resource allocation scheme program management emergency management dynamic game network model;dynamic game model framework;dynamic game;resource allocation;program management;resource allocation scheme;resource management;construction industry;program manager;program emergency process model dynamic game model framework program emergency management resource allocation program manager engineering construction company;games construction industry floods resource management project management dynamic scheduling process control;emergency management;dynamic game network model;resource allocation construction industry emergency services game theory project management;games;network model;process control;program emergency management;process model;floods;dynamic scheduling;emergency services;program emergency process model	Along with the trend that emergency events frequently occur in the program management, it becomes significantly important to strengthen the program emergency management, and the scientific and reasonable resource allocation shall exert dual effects on the disposal of unexpected event. The paper proposes the process model of the program emergency management, analyzes the game between the program emergency events and program managers of the engineering construction company under the model framework of the dynamic game, and discusses how to generate the allocation scheme for relief materials with the game model.	network model;process modeling	Guo Guang-xiang;Wang Zhuo-fu	2010	2010 International Conference on E-Business and E-Government	10.1109/ICEE.2010.679	project management;games;game theory;simulation;program management;economics;dynamic priority scheduling;resource allocation;knowledge management;resource management;operations management;network model;process control;process modeling;sequential game;management;emergency management	SE	-68.16807441075822	17.019214339413203	54350
0c13c681e1600806b659dfa356c13a59143e2afd	safe operation of mini uavs: a review of regulation and best practices	normative;unmanned aerial units;articulo;risk management;safety;risk assessment;safety unmanned aerial units risk assessment normative risk management	This paper is focused on the safety of mini UAV (mUAV) systems. It presents the great efforts that are being done in air legislation, including the present and future normative. Nevertheless, considering that the work is not finished yet, a low-level risk analysis concept is introduced. Based on the international regulations, a specific three-step structure for mUAV hazard analysis is presented: identification, assessment, and reduction in a recursive loop that provides a solid architecture for facing the wide range of possible risks.	best practice;unmanned aerial vehicle	David Sanz;João Valente;Jaime del Cerro;Julián Colorado;Antonio Barrientos	2015	Advanced Robotics	10.1080/01691864.2015.1051111	risk assessment;simulation;risk management;normative;engineering;computer security	Robotics	-63.944960638909514	10.460043316222087	54529
4b423d3d321cc6685717ee4dcf4473381d654755	ipv6 transitioning management - laying the foundation for managed ipv4/ipv6 interoperation		This paper highlights the need to supplement the work within the IPv6 community on IPv6 management with mechanisms specifically to support IPv6 transitioning. As a major feature of any IPv6 network for the considerable future, management support for the deployment and operation of a transitioning infrastructure is vital. We will propose a format for transitioning management information and examine how transitioning aspects of managed IPv6 networks can be handled within a transitioning management framework	content management system;interoperation;management information system;requirement;software deployment	Michael Mackay;Christopher Edwards	2004				DB	-71.92988679099464	13.59530058125572	54545
357795872c69cca7f7a42e47048acdcdb76536b6	an upstream business data science in a big data perspective		The rugged geographies, geomorphologies and complex geological environments make the explorers more challenging exploration and production (E & P). Despite challenges, many sedimentary basins, associated oil & gas fields and E & P Ventures are productive and commercially viable. The difficulty in understanding the connectivity among multiple reservoirs is due to lack of knowledge of multidisciplinary data of petroleum systems, complicating the data integration and interpretation process. The geological and geophysical data of an upstream business are vital assets of any oil & gas industry, in particular in E & P perspective. The data are often unstructured with a variety of anomalous attributes, mingling with volumes of spatial-temporal dimension attributes and instances. In recent years, the concepts of Big Data have taken different hype in petroleum industries, because of involvement of big sized data in the data integration process. Because of the unstructured data sources, a new direction in the database organization is needed. Investigating the science behind the Big Data and their integrated interpretation of the upstream project is a principal objective of the research. In this context, various constructs and models are articulated with different artefacts. Opportunities of Big Data are explored with exploration data and business analytics, supporting sustainable E & P systems. Petroleum management information systems (PMIS) and digital petroleum ecosystems (PDE) are developed to establish a connectivity among various data sources in multiple domains and systems. The implementation of robust methodologies ascertains the significance of the integrated upstream business in the oil and gas industries that comply with the characteristics of the Big Data. © 2017 The Authors. Published by Elsevier B.V. Peer-review under responsibility of KES International.		Shastri L. Nimmagadda;Torsten Reiners;Amit Rudra	2017		10.1016/j.procs.2017.08.236	data science;business analytics;data mining;multidisciplinary approach;big data;digital ecosystem;data integration;computer science;management information systems;petroleum;unstructured data	DB	-71.19761869956822	5.624064979713435	54575
c2fb18ba33795527e2520d65780ea419f34884bc	startups and requirements (keynote)		"""Tech startups typically approach requirements gathering differently from the process that is used to develop requirements in existing businesses and for existing products. Many of these startups operate in """"stealth mode"""", taking care to minimize the number of people with whom they share their innovative ideas. It’s common for these startups to create a succession of early releases, and apply user feedback from those releases to evolve the features and functions of their products. This talk describes various approaches used by tech startups to define product requirements, and contrasts those approaches with those used in more traditional requirements engineering activities."""	requirement;requirements elicitation;requirements engineering;stealth;succession	Anthony I. Wasserman	2014		10.1109/RE.2014.6912240	systems engineering;requirements engineering;computer science;requirements analysis	SE	-68.20648360296218	24.734733130814433	54581
e1d6643f24e877bc6898d1217dbe557320b2cf88	"""community created open source hardware: a case study of """"ecars - now!"""""""	motivations;community;open source hardware	Based on a survey, we describe the demographic and motivational structure of one open source hardware (OSH) community and compare it to open source software (OSS) communities. Taken separately, both the demographics and the motivational structure of the OSH community fall clearly within the typology of OSS communities, but when the two are taken together the OSH community forms a type of its own. We also discuss bottlenecks in OSH development revealed by the survey and subsequent interviews.	open-source hardware;open-source software	Tiina Malinen;Teemu Mikkonen;Vesa Tienvieri;Tere Vadén	2011	First Monday		community;motivation;knowledge management;open source hardware;world wide web;computer security	NLP	-74.70957319783604	21.348423827054397	54729
8bfb21331f381217268971f13249469d3fefbce8	the network maturity model for internet development	system engineering;ip networks design engineering human resource management technology management computer networks process design project management engineering management computer science educational institutions;project management;quality of service internet performance evaluation;network layout;performance evaluation;design engineering;network maturity model;resource allocation;complex network;enterprise networks;systems engineering;network performance;project management network maturity model internet enterprise networks quality of service resource allocation network layout network performance systems engineering quality management;technology management;computer networks;process design;maturity model;internet;engineering management;ip networks;computer science;quality of service;human resource management;quality management	The Internet has attracted more than 50 million users during the past four years, and there is every indication that its growth will continue at a similar rapid pace. As use of both enterprise networks and the Internet continues to grow, addressing the need for quality is an increasingly important issue. The paper considers the Network Maturity Model, a process for defining and implementing business decisions about allocating development resources, engineering decisions about network layout and performance, and management decisions about network operations. NMM introduces sound business and engineering practices from the disciplines of systems engineering and project and quality management to the design and operation of complex networks.	capability maturity model	Sourav Bhattacharya;Jeffrey M. Capone;Kevin J. Dooley;Srihari Palangala;H.-S. Yang;W. Baumann;J. Fritsch	1999	IEEE Computer	10.1109/2.796144	project management;process design;quality management;the internet;quality of service;resource allocation;computer science;knowledge management;technology management;human resource management;network performance;management;capability maturity model;complex network	Visualization	-71.62934173531846	17.58325841750604	54735
138cc73376648f8140591b1bf43f0d46c9d6358e	integrating agent based simulation in the design of multi-sided platform business model: a methodological approach		The sharing economy emerged in recent years as a model disrupting the approach to traditional B2B and B2C value chains by giving access to underutilized resources at a fraction of the cost to whom cannot or do not want to buy new products. In this context, multi-sided platforms (MSPs) play the pivotal role of providing the environments and the technological infrastructures able to match make the needs of manifold user insisting on them. The manufacturing sector didn't remain untouched by this trend, but still struggle to set up the value drivers supporting companies in the change. How can companies move towards new business models based on MSPs? How can they be supported in the value creation by the platforms? Aim of the proposed study is to present a MSP platform and related ecosystem for the automation sector, designing its business model and analyzing the tackled limitations and potential improvements. The selected case study brought to the definition of a methodological approach to MSP business model design based on both qualitative and quantitative analysis of the dynamics ruling the platform ecosystem, combing a static conceptualization of the MSP business model canvas with an agent-based simulation model, for capturing and emulating the behavior of the MSP stakeholders with the purpose to validate the sustainability of the platform ecosystem.	agent-based model;business model canvas;conceptualization (information science);ecosystem;emulator;max;sharing economy;simulation	Andrea Barni;Elias Montini;Silvia Menato;Marzio Sorlini;Víctor Anaya;Raul Poler Escoto	2018	2018 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)	10.1109/ICE.2018.8436360	conceptualization;automation;management science;business model canvas;data modeling;sustainability;business;business model;sharing economy	DB	-75.28738428562201	6.391757058930702	54745
d4156061816ae0b73b502c7c94fbd511f646c8ef	a combined use of dea (data envelopment analysis) with strong complementary slackness condition and dea-da (discriminant analysis)	conditional discrimination;strong complementary slackness condition;dea;public sector;discriminant analysis;data envelope analysis	This study discusses a combined use of DEA (Data Environment Analysis) with SCSC (Strong Complementary Slackness Condition) and DEA–DA (Discriminant Analysis). Many studies use DEA to evaluate the performance of various organizations in private and public sectors. A conventional use of DEA is not perfect because it still contains zero in many multipliers. This implies that DEA does not fully utilize information on all inputs and outputs. As a result, DEA produces many efficient organizations. To overcome the methodological difficulty, this study proposes a new use of DEA/SCSC and DEA–DA to reduce the number of efficient organizations.	data envelopment analysis;linear discriminant analysis;linear programming	Toshiyuki Sueyoshi;Mika Goto	2011	Appl. Math. Lett.	10.1016/j.aml.2011.01.021	econometrics;data envelopment analysis;public sector;linear discriminant analysis	Theory	-85.3748436323917	7.808366459579774	54795
01dd6512c3967a023450b4b0831392f74a3bed92	information systems maintenance: an integrated perspective		This article argues that information systems maintenance is a more complex and integrated task than portrayed in the literature. It involves not only the maintenance of the applications software, but also all the other elements in an operational system. The literature relating to the maintenance of each element is reviewed to reveal substantial underdevelopment i some areas and fragmentation between elements. The present practice of focusing upon the maintenance of particular individual elements i criticized and a new focus upon changes in the information inputs to the systems development process is proposed. Alternative methods of managing the maintenance operation are examined and the implications of these methods in terms of designing the procedures, staffing the maintenance function, and the need for communication are discussed.	fragmentation (computing);information system;operational system;software development process	Chris Edwards	1984	MIS Quarterly		economics;systems engineering;engineering;operations management;management;operations research;system lifecycle	SE	-65.94047036046584	8.196363388224578	54832
e0ea6ae300388c816e5a1fd5241bd0e23666e542	decision analytics - key to digitalisation		Abstract The context we address is digitalisation and we want to make the case for decision analytics as one of the key drivers to both meet the challenges from big data/fast data and to work out the new possibilities we are getting to mobilise knowledge, i.e. to make tacit knowledge explicit and to make it accessible and usable for automated, intelligent systems. The use of powerful, intelligent systems is one of the relevant solutions in the digitalisation that is now spreading in industry and business. Digitalisation brings increasing competition, slimmer margins for productivity and profitability and more pronounced requirements for effective planning, problem solving and decision making. This requires a transfer of knowledge from experts and experienced people to novice system operators—and to automated, intelligent systems—a transfer we call knowledge mobilisation. We will work out reasons for why digital coaching will be a key part of knowledge mobilisation and a key step in the development of instruments we need for the progress of digitalisation.		Christer Carlsson	2018	Inf. Sci.	10.1016/j.ins.2017.08.087	machine learning;artificial intelligence;decision analysis;management science;usable;big data;profitability index;tacit knowledge;coaching;knowledge management;intelligent decision support system;computer science	DB	-65.54014500736113	7.589536949974888	54848
e19e7e00ca2abe04c213896600988a58aad34f9c	impact of non-technical components on success and failure of software development		There is a lack of quantitative research into the non-technical components of software development projects. Further research on success and failure of software projects developed in-house are sparse. Therefore a study was conducted in India among the industries that are into in-house software development and focused at the following: 1)To study the level of non-technical components/factors, 2) To measure the percentage of software development success and failure from the perspective of software practitioners and from the organizations’ perspective (practitioners’ view), 3) To test whether there is any significant association on software development success and failure between the software practitioners’ perspective and the organizations’ perspective, 4) To study if any significant difference exists across the type of projects on the selected (dimensions of the) non-technical components, 5) To study if any significant difference exists across the duration of the projects on the selected (dimensions of the) non-technical components..	in-house software;software development;sparse matrix	S. N. Geethalakshmi	2009	JCIT		long-term support;verification and validation;computer science;social software engineering;software reliability testing;software development;software deployment;software quality analyst	SE	-70.08130125597779	23.258171705758645	54849
9e37b83e280e0ad9fc7cbd720b9015d91b0e028d	cost-benefit analysis of digital rights management products using stochastic models	human behavior;digital rights management drm;stochastic petri nets;benefit cost analysis;security metrics	Although Digital Rights Management (DRM) has been proven effective and successful in protecting the confidentiality of sensitive documents by providing access control, DRM products have not been widely adopted and used to their potential. One reason for this could be that cost and benefit of these products have not been analyzed in a systematic and quantitative manner to date. As a result, companies do not have an established procedure to evaluate the cost and benefit of implementing these products. In this document, the benefits of implementing DRM products in enterprises are quantified using stochastic Petri-net models and are compared with the security needs of a corporation and potential costs incurred by the implementation process. An evaluating procedure for implementing DRM products is established. This procedure has the potential to be used to improve the ability of a corporation to make sensible security investment decisions.#R##N##R##N#The implementation of MS IRM (Microsoft Information Rights Management), one of the DRM products, was studied as a type case. In this case study, the MS IRM system was analyzed; a group of security metrics were developed for measuring and evaluating the effectiveness of the MS IRM system, in terms of increased security provided. Stochastic models are a core part of the process. It was found that the business process is a critical factor in determining document security. Although DRM products improve security, they typically increase the cost to the company and potentially reduce the productivity of staff. Therefore, for a successful deployment of the DRM system, it is recommended that a company evaluate the benefit and cost of DRM systems quantitatively using the procedures described in this document.	digital rights management;stochastic process	Wen Zeng;Kaiyu Liu;Maciej Koutny	2013			operations management;data mining;business;computer security	AI	-65.3323065168896	6.130376361878364	54875
204aaae2a1c6c3f101212e127fe11e58928eaecc	the use of simulation to evaluate automated equipment for a clinical processing laboratory	technical report;simulation model	A series of simulation models were created to evaluate several vendor proposals submitted for automation of the Central Processing Laboratory at Mayo Clinic in Rochester, Minnesota, The goals of automating Central Processing are to deliver a high quality product (medical specimen) to the appropriate testing laboratory at a lower cost, decrease turnaround time for test results, and reduce potential employee exposure to biohazards. In this paper we discuss the simulation models we developed to perform the evaluations.	biological specimen;display resolution;simulation	Gene C. Dankbar;Jane L. Shellum;Kevin E. Bennet	1992		10.1145/167293.167835	simulation;computer science;systems engineering;technical report;software engineering;simulation modeling	HPC	-69.75630426979059	14.54816465727144	54920
5da19bfb2a4e269397e2cb75f7e43784730402c3	simulation interoperability using micro saint simulation software	discrete event simulation simulation interoperability micro saint simulation software business profit productivity com services;simulation software;discrete event simulation open systems business data processing;business data processing;profitability;discrete event simulation military standards standards development medical services defense industry power system modeling companies communication standards analytical models manufacturing processes;open systems;discrete event simulation	For the past fifteen years, Micro Saint simulation softw has been helping people answer questions on how to m their businesses more profitable and productive. Rece customers have requested that Micro Saint have capability to communicate and exchange information w other programs. In response to this request, COM Serv was added to the most recent release of Micro Saint. This paper will focus on the new features of Mic Saint 3.1 and specifically COM Services.	interoperability;simulation software	Wendy K. Bloechle;K. Ronald Laughery	1999		10.1145/324138.324229	simulation;simulation software;computer science;engineering;discrete event simulation;management science;open system;world wide web;profitability index	Arch	-73.100175989778	17.74144233852343	54936
11cb64b1d9702896cd58612b4d0ed927824c6a4a	the creation of standards: the work of an international standardization organization	standardisation industrial property mobile radio;standards;ipr licensing problem;iso;open mobile alliance;ipr licensing problem international standardization organization open mobile alliance oma;companies;oma;standardisation;organizational form;mobile radio;international standards organization;mobile communication;communications technology;industrial property;organizations;majority voting;international standardization organization;organizations standards standardization mobile communication communications technology companies iso;standardization	This paper gives an account of the organization and work procedures of the open mobile alliance (OMA), a standardization consortium. Consortia are often established as a reaction to perceived inefficiency in the work procedure and organizations of formal standardization organizations in a field. The paper shows that in spite of this, OMA has adopted the work procedures and organizational setup of their counterparts. OMA works on a consensus basis even if the formal rule of the organization is majority voting. The paper argues that this is due to the legacy of the organization and its members, and that the overall structure and spirit of the telecommunication field have a decisive impact on how new organizations work. The paper also argues that the whish for legitimacy steers the organisation towards its current structure, and alternative organizational forms that are less open and inclusive could create IPR licensing problems, and reduce the perceived legitimacy of the OMA standards.	interpro;oma;organizing (structure);peripheral;service data objects	Endre Grøtnes	2007	2007 5th International Conference on Standardization and Innovation in Information Technology	10.1109/SIIT.2007.4629322	public relations;telecommunications;computer science;marketing;operations management;sociology;management;law;standardization	SE	-73.41067426496325	14.159005090986177	54988
c6f30387ddd06e8b967e2f1d0d38d03fe024cbfe	"""are """"best practices"""" requirements documents a myth?"""	best practice;software engineering;requirements;best practices	You might think that it would be fairly straightforward to find a requirements document that has been constructed according to the best practices. Youu0027d be wrong.	best practice;document;requirement	Johann Rost	2006	IEEE Software	10.1109/MS.2006.64	systems engineering;engineering;knowledge management;software engineering;management;best practice	Visualization	-68.01857391296176	25.04051176376709	54999
6943ff9b25fddcf19d4f7733700359660632ee33	what factors influence the design of a linked data generation algorithm?		Generating Linked Data remains a complicated and intensive engineering process. While different factors determine how a Linked Data generation algorithm is designed, potential alternatives for each factor are currently not considered when designing the tools’ underlying algorithms. Certain design patterns are frequently applied across different tools, covering certain alternatives of a few of these factors, whereas other alternatives are never explored. Consequently, there are no adequate tools for Linked Data generation for certain occasions, or tools with inadequate and inefficient algorithms are chosen. In this position paper, we determine such factors, based on our experiences, and present a preliminary list. These factors could be considered when a Linked Data generation algorithm is designed or a tool is chosen. We investigated which factors are covered by widely known Linked Data generation tools and concluded that only certain design patterns are frequently encountered. By these means, we aim to point out that Linked Data generation is above and beyond bare implementations, and algorithms need to be thoroughly and systematically studied and exploited.	algorithm;design pattern;linked data;space: above and beyond	Anastasia Dimou;Pieter Heyvaert;Ben De Meester;Ruben Verborgh	2018				HCI	-74.45453457989676	25.432926190464418	55052
3210ef7ee24b10be74a71176663727a9644f9ab8	business model dynamics: a longitudinal, cross-sectional case survey	business models;business model dynamics;case survey	To maintain alignment with technology, regulation and market developments in the outside world, companies need to adapt their business models over time. As most literature has studied business models in a static approach, understanding is lacking on how external forces drive internal business model design choices. This paper studies which type of external drivers are most influential throughout the life cycle of business models. To do so, we surveyed 45 longitudinal case descriptions on business model dynamics of (networks of) organizations in various domains. Our results partly support our hypotheses. Market and technology drivers are most relevant in early stages of new business models, while regulation is far less important than we expected. These results mainly apply to small start-ups rather than large, established companies.	business requirements;conceptualization (information science);device driver	Mark de Reuver;Timber Haaker;Harry Bouwman	2007			management science;process management;business	ECom	-79.21230923293115	5.524702419054193	55110
32f5f84f371a23f1cb5636bfb306a95dc2dd70cb	system maintenance in academia: using agile model driven design		This paper presents the experiences of student project teams as they embark on the process of enhancing an existing software system. Prior to this, a system has already been delivered to the client, a local ambulance organization. Two teams are assigned to work independently of each other on this project. The teams follow an agile driven model approach to design and development. This reactive approach is not perfectly suited to use in the academic environment and the problems encountered by the teams are discussed. The collaboration needed within each team during the lifecycle phases is described. Problems seen during the project are examined.	agile software development;model-driven engineering;software system	Joo Tan	2010			agile software development;agile unified process;manufacturing engineering;agile usability engineering;engineering;systems engineering	SE	-65.29834938843874	20.06834192163744	55119
af710450f5ea2cb5c683b169018664fee03581e9	colt: concept lineage tool for data flow metadata capture and analysis		Most organizations are becoming increasingly data-driven, often processing data from many different sources to enable critical business operations. Beyond the well-addressed challenge of storing and processing large volumes of data, financial institutions in particular are increasingly subject to federal regulations requiring high levels of accountability for the accuracy and lineage of this data. For companies like GE Capital, which maintain data across a globally interconnected network of thousands of systems, it is becoming increasingly challenging to capture an accurate understanding of the data flowing between those systems. To address this problem, we designed and developed a concept lineage tool allowing organizational data flows to be modeled, visualized and interactively explored. This tool has novel features that allow a data flow network to be contextualized in terms of business-specific metadata such as the concept, business, and product for which it applies. Key analysis features have been implemented, including the ability to trace the origination of particular datasets, and to discover all systems where data is found that meets some user-defined criteria. This tool has been readily adopted by users at GE Capital and in a short time has already become a business-critical application, with over 2,200 data systems and over 1,000 data flows captured.	data flow diagram;data system;dataflow;flow network;interactivity;lineage (evolution);whole earth 'lectronic link	Kareem S. Aggour;Jenny Weisenberg Williams;Justin McHugh;Vijay Kumar	2017	PVLDB	10.14778/3137765.3137783	accountability;data mining;database;metadata;business operations;data system;data science;origination;computer science;data flow diagram	DB	-73.50027109631986	12.233474855566795	55163
48c206c7bde074bafdec57d68699f75b50d7f341	evolving defect folklore: a cross-study analysis of software defect behavior	developpement logiciel;empirical study;methode empirique;defecto;metodo empirico;heuristic method;empirical method;metodo heuristico;environmental effect;development process;efecto medio ambiente;body of knowledge;desarrollo logicial;defect;software development;defaut;estructura producto;methode heuristique;effet environnement;structure produit;product structure	Answering “macro-process” research issues – which require understanding how development processes fit or do not fit in different organizational systems and environments – requires families of related studies. While there are many sources of variation between development contexts, it is not clear a priori what specific variables influence the effectiveness of a process in a given context. These variables can only be discovered opportunistically, by comparing process effects from different environments and analyzing points of difference. In this paper, we illustrate this approach and the conclusions that can be drawn by presenting a family of studies on the subject of software defects and their behaviors – a key phenomenon for understanding macro-process issues. Specifically, we identify common “folklore,” i.e. widely accepted heuristics concerning how defects behave, and then build up a body of knowledge from empirical studies to refine the heuristics with information concerning the conditions under which they do and do not hold.	heuristic (computer science);organizational behavior;software bug	Victor R. Basili;Forrest Shull	2005		10.1007/11608035_1	artificial intelligence;empirical research;algorithm	AI	-69.23145180665853	22.698826515857398	55194
0b8118b285fcfbe538f8061c37467ead93c9438d	virtual communities as a resource for the development of oss projects: the case of linux ports to embedded processors	communaute virtuelle;virtual community;network analysis;analisis regresion;logiciel libre;software industry;analyse regression;social network analysis;software libre;regression analysis;analyse de reseau;embedded processor;comunidad virtual;virtual communities;open source software	Open source software (OSS) projects represent a new paradigm of software creation and development based on hundreds or even thousands of developers and users organised in the form of a virtual community. The success of an OSS project is closely linked to the successful organisation and development of the virtual community of support. The main objective of this article is to analyse the activity of virtual communities. Social network analysis is employed to analyse Linux ports to embedded processors as a case study to achieve this aim. The obtained results confirm the necessity of structuring the virtual community with a selection of active developers and core members to promote community activity and attract peripheral users, expanding the impact of the underlying software. The obtained result will be useful for the software industry migrating to the open source software paradigm.	central processing unit;embedded system;linux;virtual community	Sergio L. Toral Marín;M. Rocío Martínez-Torres;Federico Barrero	2009	Behaviour & IT	10.1080/01449290903121394	social network analysis;social science;network analysis;computer science;systems engineering;engineering;artificial intelligence;social software engineering;software development;operating system;world wide web;regression analysis	Embedded	-75.91383033225321	21.721057303651932	55333
e18e45c00a6da2629e017e16caeaf985e0fd3f23	ontology of organizational it security awareness-from theoretical foundations to practical framework	end user acceptance;application software;it security awareness internalization;motivational factors organizational it security awareness ontology overall security descriptive awareness prescriptive awareness intrinsic motivation requirements end user acceptance it security awareness internalization;intrinsic motivation requirements;intrinsic motivation;motivational factors;it security;personnel security of data human factors dp management;human factors;target recognition;personnel;organizational it security awareness ontology;overall security;area measurement;ontologies;humans;dp management;descriptive awareness;measurement standards;theoretical foundation;security;management training;ontologies computer errors security humans timing area measurement measurement standards target recognition management training application software;security of data;computer errors;user acceptance;prescriptive awareness;timing	Although organizational IT security awareness is an important element and the overall security of any organization, some of its foundations, such as understanding the difference between descriptive and prescriptive awareness and the requirements of intrinsic motivation, have not been properly investigated. As a result, end-users often fail to internalize target goals and follow guidelines. To maximize end-user acceptance and the internalization of IT security awareness, the authors formulate a set of motivational factors aimed at enhancing the understanding and use of prescriptive awareness.		Mikko T. Siponen;Jorma Kajava	1998		10.1109/ENABL.1998.725713	application software;motivation;computer science;knowledge management;ontology;information security;human factors and ergonomics;computer security	Crypto	-73.20981036058309	15.928650742116375	55340
b4cb6706bc992a828d30072ed04cab9667a187d4	web accessibility issues for the distributed and interworked enterprise portals	quality criteria web accessibility interworked enterprise portals accessible web sites decision making process;portals;study design;mathematics;portals path planning robot kinematics orbital robotics mobile robots robotics and automation artificial intelligence contracts application software mathematics;senior citizens;application software;search engines;benchmark;path planning;web accessibility;business impacts;contracts;mobile robots;orbital robotics;business impacts web accessibility benchmark quality criteria;accessible web sites;business case;internet;decision making process;web sites;interworked enterprise portals;artificial intelligence;quality criteria;web sites decision making internet portals;organizations;economics;usability;robotics and automation;robot kinematics	This contribution presents a framework and study design for the measurement of efficiency and quality of accessible Web sites. In particular, the framework developed constitutes an instrument for benchmarking of Web accessibility implementation effects by means of aggregation of quantitative data and may-in a later stage-be applied to real business cases. Hence, this framework represents a valuable tool for the measurement of efficiency of Web accessibility and may support organizations in their decision making process concerning Web accessibility implementation.	portals;web accessibility	Marie-Luise Leitner;Rudolf Hartjes;Christine Strauss	2009	2009 International Conference on Parallel Processing Workshops	10.1109/ICPPW.2009.75	mobile robot;decision-making;application software;web modeling;the internet;benchmark;usability;web design;web accessibility initiative;web standards;computer science;organization;knowledge management;web accessibility;business case;motion planning;clinical study design;world wide web;robot kinematics	DB	-80.28284979397517	12.399446672433708	55405
e42c70b48db3bef963340f7819cd914720110ed8	kdd as an enterprise it tool: reality and agenda (abstract)	business intelligence;decision support;data warehouse;commercial endeavor;database server;enterprise it tool;olap tool;it infrastructure;business reality;commercial kdd product	"""KDD is a key technology for harnessing the """"business intelligence"""" IT infrastructure. Although KDD has been a longstanding field of research, it is in its infancy in commercial endeavors. Commercial KDD products have many major weaknesses that have slowed KDD's becoming an enterprise IT tool. In this presentation, I will review the technological and business reality of KDD, and discuss what needs to happen before KDD can attain the status that other enterprise IT tools have reached, including database servers, data warehouses, decision support and OLAP tools, etc."""		Won Young Kim	1999		10.1007/3-540-48912-6_1	simulation;knowledge management;data mining	Logic	-71.73569851506626	5.783438504598323	55520
9157bd71423fc5f053afc7064726bd0cc2c99277	software quality engineering: testing, quality assurance, and quantifiable improvement. by jeff tian. wiley/ieee computer society press, 2005. isbn: 0-471-71345-7, pp 412	quality assurance;software quality		international standard book number;john d. wiley;quality engineering;software quality	Ian Gilchrist	2006	Softw. Test., Verif. Reliab.	10.1002/stvr.346		SE	-63.54434840528269	27.519028834372698	55549
b9599f3473b40c306fe2a4835c33615a20a7e83f	mobile banking and brokerage systems ) managing is risks in the beginning 21st century	new technology;empirical study;risk management;operational risk;mobile service;system management;face to face;high risk	The beginning 21st century makes high demands on industries dealing with new technologies. Starting from an outline of new challenges of the 21st century, the situation of banks being particularly exposed to these conditions is described. The mobile banking channel is focused as one example of banks ́ business processes involving new technology. 24 banks are identified from the list of the 100 largest German banks that offer mobile services and an empirical study is conducted in order to explore their first experiences with mobile banking services. These telephone interviews are the first of a three-step research process. For a more in-depth analysis of banks ́ IS risks, face-to-face interviews will be conducted in a second step. The research process will be finished with selected case studies about requirements for new methods and tools of banks ́ IS risk management. This paper presents selected results of the first research step. One of the most important aspects for all their decisions has been the high risk dealing with new technology and a lack of established IS risk management guidelines. Examples of such operational risks are considered and a variety of different existing approaches is discussed and analyzed with respect to their potential to minimize risks, though being not explicitly dedicated to risk management. However, new risk management approaches are treated by the industry with a certain reserve. On the basis of that analysis, suggestions for further research are made to develop a practicable operational IS risk management approach. 1. Challenges and Risks in the Beginning of the 21st Century With many companies that were engaged in electronic commerce activities coming down from an almost infinite seeming hype leading to irrational investments, the long-term welfare of a company and its stakeholders have been brought into focus again. Concepts for integrative management (Bleicher 1999) and corporate governance guidelines (Berghe 2002, OECD 1999, Post, Lee and Sachs 2002) provide a basis for monitoring and controlling corporate performance. However, that perspective is often limited to mere Mobile Banking and Brokerage Systems – Managing IS Risks in the Beginning 21 Century 919 financial performance, disregarding losses that can occur due to the way how a company operates its business rather than how it finances its business. Traditional companies have increasingly come to realize a certain sustaining value of electronic and mobile business and now put in place the technology and processes to offer mobile services to their customers. Their starting position has been characterized by the advantage of having learned from failures of other companies during the hype while having already a wide experience with system development as well as with a variety of other contact and distribution channels in the context of a multi channel strategy. Much of the learning is simply not transferable to the 21 century experiences; though in fact, traditional companies face many challenges of quite a different nature. What are these challenges in particular? Coinciding changes of market and business conditions at an accelerating pace Changing market and business conditions have often been described as a new phenomenon, especially in recent times of globalization; yet they have been shown to be true and perceived as being paradigmatically challenging at every stage of economic and technical progress. However, the particular challenges of the 21 century accrue from the coincidence of a variety of different changes; for instance: • intensified competition on a global as well as on a national level; • increasing volatility of customer needs; and • augmenting complexity of technology. Thus, companies find themselves torn in an “acceleration of improvement paradox” with the demands of accelerating and improving processes simultaneously, i.e. minimizing input while maximizing output at the same time, and all this in an increasingly complex and dynamic environment. Since both are competing objectives, coming along with the pressure of reducing costs and time, there are particular risks inherent in strategic and operative management decisions that can lead to performance reductions and losses. Businesses increasingly dependent on information systems In utilizing more and more new technologies, businesses become increasingly dependent on information systems. This makes systems delivery on time and within budget even more important and increases the demands on the practicability and flexibility of project management concepts, methods of software process improvement, and other means of controlling and reducing risks during the development and operation of information systems. Moreover, higher technological complexity leads to increasing interdependencies and interactions in system development and operation on the technical as well as on the organizational-personnel level which hold special sources of error and risks. Development of information systems becomes more difficult and complex Technological progress in most cases implies more difficult and complex development procedures. Mobile application systems, for instance, represent a typical and graphic example as they are, within their species, heterogeneous over three dimensions: • Degree of innovation, • Speed of development, and • Risk. Dependent on their values in these three dimensions mobile systems require different, i.e. project and situation specific, process models and development procedures (Kemper and	business process;corporate governance;e-commerce;experience;information system;interaction;interdependence;mobile banking;nist hash function competition;requirement;risk management;software development process;the 100;volatility	Elke Wolf	2003			actuarial science;it risk management;risk management;risk management information systems;business;commerce	SE	-75.41709813017368	7.773372098809996	55652
3f23acbdc06f3e98a33e86546735cd7a07571caa	decision making in disaster management: from crisis modeling to effective support		Crisis management is a challenging domain to model because of its multiple stakeholders, its complex resource management, its rich communication and control mechanisms, etc. It consists in sets of interdependent activities occurring in an evolving environment under stressful conditions and high pressures. It becomes difficult to support given that any decision includes multiple actors, multiple emergency services, etc. [1]. Therefore, we believe that a systemic modeling of such complex socio-technical system (including the decisions makers, stakeholders, coordination processes, etc.) would improve the potential to support effectively the crisis management. We focus in this paper on the effective use of modeling to manage crisis. In this paper, we present firstly most relevant existing metamodels of crisis management. Secondly, we describe the criteria we defined to classify these metamodels. Thirdly, we present the results of this classification and finally, we conclude by suggesting first requirements needed for an effective metamodel useful to support decision processes in crisis management.		Wissem Eljaoued;Narjès Bellamine Ben Saoud	2017		10.1007/978-3-319-67633-3_10	crisis management;knowledge management;metamodeling;emergency management;management science;business decision mapping;resource management;computer science	ML	-66.65941702330288	14.019770555719889	55735
fec81400d85a9e1176d5b8b769c13897b2e9e678	a cost-benefit model for software quality assurance activities	software quality assurance;cost benefit model	Software project managers must schedule quality assurance activities. This is difficult because not enough information is available. Therefore, we developed and validated the quantitative model CoBe. It is based on detailed relationships and is quantified with historical data. It allows to decide which reviews and tests have to be conducted, how they are conducted, and how corrected defects are retested. The results are costs and benefits for quality assurance activities during development and after delivery. Results are given in terms of effort, time, and staff. They are summed up and weighted financially so that an optimal trade-off between costs and benefits can be found. The model is validated with real-world data: Detailed relationships and the complete model are validated with data from 21 student projects. A sensitivity analysis was conducted. CoBe was also validated with data of two industry projects. Overall, the model results are sufficiently accurate. But a calibration is necessary for applying the model in a specific environment. For this, only a few parameters must be set. Their values can be obtained from data that is available frequently from past projects.	software bug;software project management;software quality assurance	Tilmann Hampp	2012		10.1145/2365324.2365337	reliability engineering;computer science;systems engineering;engineering;data mining;software quality analyst	SE	-64.80376438261507	31.147795247348537	55759
e31baa925363859d9dc8adcd3ee40d3da5753193	cloud computing adoption factors and processes for enterprises - a systematic literature review	factor;standards;cloud provider;confirmation;internal;enterprise;regulations;implementation;advantage;risk management;it governance;complexity;integration;cloud service model;compatibility;external;decision;evaluation;auditability;process;adoption;security;cost;cloud computing	Cloud computing (CC) has received an increasing interest from enterprises since its inception. With its innovative Information Technology (IT) services delivery model, CC could add technical and strategic business values to enterprises. However, it poses highly concerning, internal and external, issues. This paper presents a systematic literature review to explore cloud computing adoption processes in the context of enterprise users and the factors that affect these processes. This is achieved by reviewing 37 articles published about CC adoption. Using the grounded theory approach, articles are classified into eight main categories: internal, external, evaluation, proof of concept, adoption decision, implementation and integration, IT governance, and confirmation. This is concluded in two abstract categories: CC adoption factors and CC adoption processes whereas the former affect the latter. The results of this review indicate that there are serious issues need to be tackled before enterprises decide to adopt CC. Based on the findings of this review, the paper provides future Information Systems (IS) research directions toward the previously under-investigated areas regarding the phenomenon. This involved the call for further theoretical and indepth empirical contributions to the area of CC adoption by enterprises.	backward induction;cloud computing;database;information system;systematic review;utility computing;web search engine	Rania Fahim El-Gazzar	2014		10.5220/0004841900780087	cloud computing security;regulation;complexity;advantage;cloud computing;risk management;computer science;knowledge management;information security;evaluation;operating system;compatibility;implementation;process	HCI	-79.42169488569375	12.396829347320498	55799
da7c544f885aae7523c8b9a98e0095de586918df	requirements management in a product line scenario	incremental development;configuration management systems analysis formal specification software reusability;cycle time;switched system;formal specification;requirements management;product line;development environment;systems analysis;software reusability;marketing management concrete cost function switching systems business portfolios;success factor;portfolio management;configuration management;configuration management requirements management product line scenario software reuse cost telecommunication systems alcatel s12 switching system business unit marketing portfolio management requirements reuse planning	The product line approach is a major reuse concept that has shown concrete commercial results over the past years. Product lines achieve reuse in the functionality and solution space, and thus improve quality, cycle time and cost. They are specifically promising in cases where many variants are produced almost simultaneously, such as in telecommunication systems. Though appealing, the concept is difficult to introduce specifically into an existing development environment. All too often the impacts on requirements management are not considered globally enough. The opposite of a product line concept is to allow every variant of a product to implement its own requirements only loosely coupled to the base product. This article describes the introduction of a product line approach in Alcatel's S12 Switching System Business Unit. Success factors are described related to the interface with marketing, requirements definition, roadmapping and portfolio management, requirements reuse, planning and prioritization, incremental development, and configuration management are explained. Practical impacts are described as well as tricks and traps. Proceedings of the IEEE Joint International Conference on Requirements Engineering (RE’02) 1090-705X/02 $17.00 © 2002 IEEE	configuration management;exception handling;feasible region;iterative and incremental development;loose coupling;proceedings of the ieee;requirement;requirements engineering;requirements management	Christof Ebert	2002		10.1109/ICRE.2002.1048514	product cost management;reliability engineering;systems analysis;requirements management;market requirements document;requirement prioritization;economics;service product management;innovation management;business requirements;cycle time variation;computer science;systems engineering;engineering;operations management;product lifecycle;product design specification;requirement;software engineering;iterative and incremental development;formal specification;development environment;configuration management;configuration item;product management;application lifecycle management;management;new product development;vision document;project portfolio management	SE	-66.33133031204369	21.332821499482115	55835
eb7f540e543bee3083a8fe63ba7486bd8ad59ee8	modeling just-in-time production systems: a critical review	critical review;just-in-time production system;linear program;linear programming;shape;control systems;production system;production systems;system performance	Kanban-controlled serial manufacturing systems have recently received considerable attention. A large proportion of the literature on the topic is devoted to success stories. There is also an important model- based effort in gaining insight into the behavior of such systems, in identifying important success factors, and ultimately in optimizing various aspects of systemsu0027 performance. This paper focuses exclusively on model-based approaches in studying pull systems. Even though analytic models such as linear programming formulations or queueing approximations exist, the inherent complexity of pull systems makes simulation an essential tool in studying them. The objective of this paper is therefore to critically review selected papers that have recently appeared in refereed journals, highlight their approach, point out deficiencies, where appropriate, re-emphasize their message, and suggest new directions for research.		Charles Corbett;Enver Yücesan	1993		10.1109/WSC.1993.718325	simulation;computer science;systems engineering;engineering;linear programming;production system	OS	-65.63241831365092	8.331278320986204	55883
6c019a3c25f8cdccbc5e5a772216f2283e5515ed	analysis, test and verification in the presence of variability (dagstuhl seminar 13091)	004;verification program analysis testing semantics of programming languages software engineering	This report documents the program and the outcomes of Dagstuhl Seminar 13091 “Analysis, Test and Verification in The Presence of Variability”. The seminar had the goal of consolidating and stimulating research on analysis of software models with variability, enabling the design of variability-aware tool chains. We brought together 46 key researchers from three continents, working on quality assurance challenges that arise from introducing variability, and some who do not work with variability, but that are experts in their respective areas in the broader domain of software analysis or testing research. As a result of interactions triggered by sessions of different formats, the participants were able to classify their approaches with respect to a number of dimensions that helped to identify similarities and differences that have already been useful to improve understanding and foster new collaborations among the participants. Seminar 24. February to 1. March, 2013 – www.dagstuhl.de/13091 1998 ACM Subject Classification D.2.4 Software/Program Verification, D.2.5 Testing and Debugging, D.2.13 Reusable Software, D.3.1 Formal Definitions and Theory, F.3.1 Specifying and Verifying and Reasoning about Programs, F.3.2 Semantics of Programming Languages	debugging;formal specification;formal verification;heart rate variability;interaction;semantics (computer science);software testing;spatial variability	Paulo Borba;Myra B. Cohen;Axel Legay;Andrzej Wasowski	2013	Dagstuhl Reports	10.4230/DagRep.3.2.144	computer science;systems engineering;data mining	SE	-62.947371373033974	29.66357825450204	55980
8a5fd8b1bed4676479696238ff029749306724e3	concept to support a cost effective implementation of information technology service management according to iso 20000	information technology service management itsm;it infrastructure library itil;iso 20000;cost effective implementation	Information Technology Service Management (ITSM) delivers IT-based services to organizations. It supports the attainment of business goals by aligning IT activities with business requirements. ITSM is usually implemented by following a process oriented approach that is further specified by dedicated frameworks like the ISO 20000 standard and the IT Infrastructure Library (ITIL) of the Office of Government Commerce. Due to cost effectiveness considerations a complete implementation of such frameworks is not necessarily required, since opportunities for improvements can be located in a limited set of process areas, depending on each organization’s individual case. In order to follow this approach, a concept is presented that supports the cost effective implementation of ITSM. It is based on sharing specific knowledge that eases the identification of relevant objectives within ITSM frameworks and furthermore supports the identification of dependencies for implementation planning. Results of the conducted validation case indicate the concept’s fitness for purpose.	business requirements;itil;requirement	Jan-Helge Deutscher;Carsten Felden	2010			information technology infrastructure library;systems engineering;knowledge management;management science;business;incident management	SE	-70.96400146608742	12.968124772409805	56006
e136e1411c5f5dc185ed21f0bbeaa65f876c7896	exploring the transaction dimensions of supply chain management	gestion integrada;especificidad;hybrid governance;gestion integree;cout transaction;empresa virtual;tratamiento transaccion;logistique;entreprise virtuelle;uncertainty;gestion red;transaction cost;supply chain management networks;integrated management;asset specificity;systeme incertain;firm cooperation;scm networks;logistics;governance structure;binary choice;gestion reseau;coste transaccion;virtual enterprise;transaction dimensions;key informant survey;transaction costs;cooperation entreprise;network management;specificity;specificite;transaction processing;sistema incierto;survey;uncertain system;cooperacion empresa;supply chain management;traitement transaction;logistica	It is argued that firm cooperation in supply chain management can be classified as a hybrid governance structure in a new institutional sense. Using a key informant survey, exploratory data on transaction dimensions and supply chain management was gathered and analyzed. Findings suggest that the implementation of such cooperation is not a binary choice but rather a matter of varying degrees. All factors commonly associated with influencing transaction costs (asset specificity, uncertainty and frequency) are significantly higher for firms with more fully implemented supply chain management, making a more integrated solution advantageous. Additionally, the data suggests a strong influence of transaction frequency on the degree of asset specificity and uncertainty.	sensitivity and specificity	Britta Lietke;Madlen Boslau	2007	IJNVO	10.1504/IJNVO.2007.013541	transaction cost;supply chain management;economics;service management;computer science;marketing;operations management;management	HCI	-82.41588383338609	8.904011769015272	56111
d77b62b781c4f319120a0b8e12328bd1940abfb3	antecedents and outcomes of boundary objects in knowledge interaction in the context of software systems analysis	pragmatics;systems analysis information systems knowledge management;information systems;boundary objects;data collection;system requirements;knowledge management;software systems;semantics;information system field knowledge interaction software systems analysis knowledge management research knowledge generation knowledge workers;boundary object;knowledge interaction;semantic boundary;knowledge worker;end users;systems analysis;syntactics;system design;project performance;project success;higher frequencies;organizations;information system;atmosphere meteorology organizations knowledge management semantics syntactics pragmatics;atmosphere;meteorology;continuous process;knowledge workers	Much of knowledge management research has focused on knowledge generation, translation and storage during interactions among knowledge workers, which has led to project success; however, less effort has been made to examine the effects of artifacts or boundary objects that such interactions yield. Thus, the study aimed to help fill this research gap by investigating, not only the categories and characteristics of boundary objects, but also the possible antecedents, taking into consideration the link between outcome and type of boundary object. In the information system field, system design and implementation is a continuous process of interactions between system analysts and end-users, with most interactions occurring in the stage of defining system requirements. Thus, this stage offers an appropriate context to study knowledge interaction. Based on data collected and codified from 82 records of knowledge interaction, the results showed that project performance could be highly enhanced by making use of syntactic, semantic, pragmatic and metaphoric boundary objects. In addition, higher atmosphere leads to a higher frequency of the occurrence of semantic boundary objects. Although the results may be inevitably linked to the context being investigated, the importance of the expected findings will trigger like studies in other contexts.	information system;interaction;knowledge management;requirement;software system;system requirements;systems design	Eugenia Y. Huang;Travis K. Huang	2011	2011 44th Hawaii International Conference on System Sciences	10.1109/HICSS.2011.67	computer science;knowledge management;database;semantics;management science;information system;pragmatics	HCI	-65.63340286728403	17.072607227577365	56219
84f66273915ef30084b269368941f7f99dfa1cf3	managing prototype knowledge/expert system projects	sistema experto;project management;intelligence artificielle;organizacion proyecto;technology transfer;artificial intelligence;gestion projet;inteligencia artificial;systeme expert;expert system	Computer science technology in the 1990s will involve knowledge/expert systems. Fundamental issues of technology transfer, training, problem selection, staffing, corporate politics, and more, are explored.	computer science;expert system;prototype	James M. Cupello;David J. Mishelevich	1988	Commun. ACM	10.1145/42411.42416	project management;computer science;knowledge management;artificial intelligence;operations research;expert system	Logic	-67.06902426954682	6.627742631117289	56330
0b1db67e40d887a0e1c005c38fedc33b73f97fcd	creating and evolving developer documentation: understanding the decisions of open source contributors	software documentation;open source projects;developer documentation;qualitative study;grounded theory;qualitative studies;framework;open source	Developer documentation helps developers learn frameworks and libraries. To better understand how documentation in open source projects is created and maintained, we performed a qualitative study in which we interviewed core contributors who wrote developer documentation and developers who read documentation. In addition, we studied the evolution of 19 documents by analyzing more than 1500 document revisions. We identified the decisions that contributors make, the factors influencing these decisions and the consequences for the project. Among many findings, we observed how working on the documentation could improve the code quality and how constant interaction with the projects' community positively impacted the documentation.	documentation;library (computing);open-source software;software framework;software quality	Barthélémy Dagenais;Martin P. Robillard	2010		10.1145/1882291.1882312	common source data base;computer science;systems engineering;qualitative research;technical documentation;database;internal documentation;world wide web	SE	-73.42093318784138	22.254821017487494	56353
68c44c64abcf49cf3fed40794f013917066fcf62	discussion on requirements for agile/virtual enterprises reconfigurability dynamics: the example of the automotive industry	automotive industry	Product life cycles tend to shorten, time to market also, and even the global networked structures, corresponding to the recent approaches of the Extended, Virtual and Agile Enterprise (Agile/Virtual Enterprise model — A/VE) tend to last shorter and shorter time. The ability of dynamic reconfigurability is a requirement that the enterprises corresponding to this A/VE model must satisfy to assure a permanent alignment with the market. A/V E can have as many instantiations as required either by product changes or as a requirement of quality and competitiveness improvement. In the paper we discuss the need and the dynamics of the A/V E reconfiguration, using as support example the automotive industry supply chain.	agile software development;reconfigurability;requirement	Maria Manuela Cunha;Goran D. Putnik	2002			engineering;automotive engineering;advanced product quality planning;process management;manufacturing engineering	Robotics	-64.13270559768968	13.493805657255036	56392
0190a9d06eb1cb5685631c37ef8fdcc036b62f13	practical similarities and differences between systematic literature reviews and systematic mappings: a tertiary study		Background: Several researchers have reported their experiences in applying secondary studies in Software Engineering (SE), however, there is a lack of studies discussing the distinction between Systematic Mappings (SMs) and Systematic Literature Reviews (SLRs). Aims: The objective of this paper is to present the results of a tertiary study conducted to collect and evaluate evidence to better understand similarities and differences between SLRs and SMs related to four aspects: research question, search string, search strategy and quality assessment. Method: We identified 170 secondary studies that were reviewed to answer a set of Research Questions (RQ) related to the practical conduction of secondary studies in SE. Results: Results show that both SLRs and SMs have generic RQs, broad search strings, and adopt automatic search as search strategy. However, quality assessment has been more widely adopted in SLRs. Conclusions: In practice, only the quality assessment is conducted differently in SLRs and SMs. Keywords—Systematic Literature Review, Systematic Mapping, Secondary Studies	decision tree;software engineering;string searching algorithm;systematic review	Bianca Napoleão;Katia Romero Felizardo;Érica Ferreira de Souza;Nandamudi Lankalapalli Vijaykumar	2017		10.18293/SEKE2017-069	systems engineering;computer science	SE	-71.73332966152134	22.149828116832612	56446
2f934b262bb14149b8718cfb36ec1d71ed110473	critical success factors in enterprise resource planning implementation a case-study approach	esfahan steel company esco;change management;critical success factors csfs;enterprise resource planning erp;enterprise resource planning;critical success factor;business process reengineering bpr	Although introducing Enterprise Resource Planning (ERP) to an organization has enormous benefits, it may entail new hazardous challenges if it cannot be well managed. This research focuses on the critical ERP success factors from a case study involving the Esfahan Steel Company, which started ERP implementation in September 2002. An in-depth research of ERP implementation processes and the level of adhering to five chosen ERP critical success factors—project management, top management supports, business process reengineering, and change management and Training—are conducted. Research results revealed that the five critical success factors (CSFs) are highly interdependent and the strengths and weaknesses of each have influenced the quality of ERP implementation to a large extent. requirement for more integrated and flexible business processes. “The key underlying idea of ERP is the use of information technology to achieve the capability to plan and integrate enterprise-wide resources” (Kumar et al., 2003). ERP integrates the processes and applications of various functions of an organization such as design, production, purchasing, marketing, and finance (Kumar et al., 2003). ERP implementation can both gain enormous advantages for successful companies and at the same time be a disaster for those compaDOI: 10.4018/jeis.2010070104 International Journal of Enterprise Information Systems, 6(3), 48-58, July-September 2010 49 Copyright © 2010, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. nies which cannot manage its implementation successfully. ERP implementation projects are the most difficult development projects for organizations due to its complexity, enterprise wide scope and fundamental organizational changes needed to align with the new system and business processes (Wilder & Davis, 1998). Therefore, as emphasized by Holland and Light (1999) all managers who want to implement ERP have to ask two important questions of “How can ERP systems be implemented successfully?” and “What are the critical success factors for ERP implementation?” This paper has three main objectives. Recognizing the critical success factors for ERP implementation by reviewing the most critical and relevant literature, examining the level of adhering to the identified success factors in the selected case study; and investigating the impacts of each critical success factor on ERP implementation and on the other factors in the selected case study.	align (company);business process;code refactoring;erp;enterprise information system;enterprise resource planning;information systems;interdependence;purchasing	Behrouz Zarei;Mina Naeli	2010	IJEIS	10.4018/jeis.2010070104	economics;systems engineering;marketing;operations management;change management;process management;critical success factor;management;enterprise planning system	SE	-75.88153653464678	9.12078658733601	56502
05c60d23dd61cf65e34e2bec8702659bcea9d8c8	software configuration management: past uses and future challenges	software configuration management	1. Meaning and Goals of Software Configuration Management Software Configuration Management is one of the established sub-fields of Software Engineering, and one that provides recognized benefits to practitioners and managers, SCM tools have been in widespread use for decades, there is a historical hardware model on which to build, and a variety of commercial tools are on the market. Our work is not yet done, and most of the needs remain unmet. This essay presents some views on where we are and the areas of future growth.	software configuration management;software engineering	Stuart I. Feldman	1991		10.1007/3540547428_39	long-term support;change management;verification and validation;software configuration management;software project management;systems engineering;environmental resource management;package development process;social software engineering;software development;software construction;software as a service;software walkthrough;application lifecycle management;software deployment;software system;software peer review	SE	-66.42394667076428	21.542669927486845	56546
e8f42c779e3e329813fe82e111804da036f7a308	understanding the strategic actor diagram: an exercise of meta modeling	meta model;universe of discourse	i-star (i*) modeling uses the actor concept to ground the intentions of a given Universe of Discourse. Our work contributes to the understanding of the actor concept as used in i*. We have used a collaborative approach to better understand the actor concept. The authors met 9 times to discuss the topic. The goal was to discuss i* meta-models, which was later specialized to discuss actor modeling. After the meetings and after one week of collaborative work using a collaboration based editor, “Writely”, we have agreed on presenting our model from two different perspectives, but both using UML as the meta language. We understand that these models, designed by consensus, represent what we have labeled the SA Diagram or the Strategic Actor Diagram. The article presents the models we have arrived as well as the process we have used. We believe that making this process transparent will help to shed light not only on the concept of actor, but on the process of meta-modeling as well.	actor model;diagram;domain of discourse;google forms;metamodeling;unified modeling language	Julio Cesar Sampaio do Prado Leite;Vera Werneck;Antonio de Pádua Albuquerque Oliveira;Claudia Cappelli;Ana Luiza A. Cerqueira;Herbet de Souza Cunha;Bruno González-Baixauli	2007			diagram;actor modeling;metamodeling;management science;political science;metalanguage;unified modeling language;knowledge management;domain of discourse	HCI	-64.83495336360666	15.05299421552646	56566
656d741590f9a3b3670529d2d0b5345d17d36027	"""proposal and validity of global intelligence partnering model for corporate strategy """"gipm-cs"""""""	corporate strategy;is success	One of the requisites for winning corporate competitions today is success in the “global marketing” for quickly offering high-quality, latest model products in response to customer needs. For manufacturers to advance “manufacturing” that precisely meets the customers’ preferences, it is vital that their affairs and management sections also share the global view and become a core of corporate management and strategy. More specifically, the key to success in “global production” lies in full functionalization of “partnering,” in which forefront divisions of technology, production, and sales as well as the affairs and management sections collaborate in a cooperative strategic scheme to realize “global quality and optimal production.” This study proposes Global Intelligence Partnering Model for Corporate Strategy, “GIPM-CS” mainly in connection with the administration. Further, the effectiveness of this model is verified at the successful companies.	microsoft forefront;strategic management	Manabu Yamaji;Kakuro Amasaka	2007		10.1007/978-0-387-74157-4_8	knowledge management;management science;business;technology strategy;management	AI	-74.15188628474098	6.003153633702091	56656
7b1016a96fa4d65e8b46c8b673c030150b362cc0	design and degradation modelling through artificial neural networks	degradation data;folding force degradation;degradation path;anns;reliability testing;component design;design evaluation;artificial neural networks;accelerated degradation testing;automotive components;vehicle design;degradation paths;automobile industry;australia;artificial neural network;product development	Automotive is one of the major manufacturing industries in Australia that requires extensive reliability test for the components used in vehicles. To achieve a shorter time-to-market and a highly reliable product while reducing the amount of physical prototyping, there is a growing need for better understanding on the effect that the design parameters have on the degradation of the product. This paper presents comprehensive descriptions of applying Artificial Neural Network (ANN) to capture the relationships between design and degradation. Consequently, two models of different practical significance are created as the result of the work. The vision of the models is to be used by the testers and designers as a guideline in design evaluation, so that time-consuming and expensive iterations of the product developmental cycle can be reduced substantially. The degradation of the folding force of a mechanical system is used to illustrate our approach.	artificial neural network;elegant degradation;iteration	Hungyen Lin;L. X. Kong;Hung-Yao Hsu	2007	IJMR	10.1504/IJMR.2007.013429	simulation;computer science;engineering;operations management;machine learning;forensic engineering;artificial neural network;new product development;mechanical engineering	HCI	-73.44888178164977	28.684480652658397	56696
0365c982909fb9ff8c4582ae280892eec8489a4f	gase: visualizing software evolution-in-the-large	gases visualization software systems computer industry gas industry software tools software architecture electrical equipment industry computer architecture large scale systems;software structural change visualization;software systems;large software systems;gase visualization tool;data visualisation;graphical analyzer for software evolution;long lived software systems;software evolution;long lived software systems legacy systems software structural change visualization gase visualization tool graphical analyzer for software evolution architectural changes industrial software system software evolution in the large visualization large software systems;legacy systems;software evolution in the large visualization;industrial software system;legacy system;architectural changes	Large and long lived software systems, sometimes called legacy systems, must evolve if they are to remain useful. Too often, it is di cult to control or to understand this evolution. This paper presents an approach to visualizing software structural change. A visualization tool, called GASE (Graphical Analyzer for Software Evolution), has been used to elucidate the architectural changes in a sequence of eleven revisions of an eighty thousand line industrial software system.	legacy system;software evolution;software system	Richard C. Holt;J. Y. Pak	1996		10.1109/WCRE.1996.558900	software visualization;verification and validation;software sizing;computer science;systems engineering;package development process;backporting;social software engineering;software framework;component-based software engineering;software development;software design description;software engineering;software construction;software analytics;resource-oriented architecture;software deployment;legacy system;data visualization;software system;computer engineering	SE	-68.9319678899178	29.02838736310857	56784
90d17705c980ca2d34dcb70a382b19877bc148b1	exploring software supply chains from a technical debt perspective	graph theory;software metrics;statistical analysis;supply chain management;supply chains;eclipse platform code;sonar;betweenness centrality;decision making;dependency graphs;graph metrics;heuristic approach;software development organizations;software evaluation;software piece technical debt;software products;software supply chain information;statistical analysis;supply chain quality evaluation;technical debt plug-in	"""Software development has evolved from software development organizations building custom solutions for every need and creating a backlog of applications needed by users to specialized organizations producing components that are supplied to other software development organizations to speed the development of their software products. Our objective is to illustrate how a manager might use supply chain information to evaluate software being considered for inclusion in a product. We investigated the Eclipse platform code to illustrate analysis methods that produce information of use to decision makers. The technical debt of the software pieces was measured using the Technical Debt plug-in to Sonar as one input into the evaluation of supply chain quality. The dependency graphs of """"uses"""" relationships among files were analyzed using graph metrics such as betweenness centrality. There was a statistically significant moderate correlation between the technical debt for a file and the betweenness centrality for that file. This relationship is used as the basis for a heuristic approach to forming advice to a development manager regarding which assets to acquire."""	betweenness centrality;eclipse;heuristic;plug-in (computing);software development;sonar;technical debt	J. Yates Monteith;John D. McGregor	2012	2013 4th International Workshop on Managing Technical Debt (MTD)		reliability engineering;long-term support;software quality management;software sizing;computer science;systems engineering;package development process;operations management;software development;software design description;technical debt;software technical review;software walkthrough;software deployment;software quality control;software quality;software metric;software system	SE	-65.53570135393662	29.844554227900932	56800
7ab11aa9ca01c9485cd93461dd18bf38860f2448	towards moral autonomous systems		Both the ethics of autonomous systems and the problems of their technical implementation have by now been studied in some detail. Less attention has been given to the areas in which these two separate concerns meet. This paper, written by both philosophers and engineers of autonomous systems, addresses a number of issues in machine ethics that are located at precisely the intersection between ethics and engineering. We first discuss different approaches towards the conceptual design of autonomous systems and their implications on the ethics implementation in such systems. Then we examine problematic areas regarding the specification and verification of ethical behavior in autonomous systems, particularly with a view towards the requirements of future legislation. We discuss transparency and accountability issues that will be crucial for any future wide deployment of autonomous systems in society. Finally we consider the, often overlooked, possibility of intentional misuse of AI systems and the possible dangers arising out of deliberately unethical design, implementation, and use of autonomous robots.	autonomous robot;autonomous system (internet);bottom-up proteomics;coherence (physics);complex system;complex systems;error-tolerant design;machine ethics;mechatronics;problem solving;requirement;software deployment;top-down and bottom-up design;verification and validation	Vicky Charisi;Louise A. Dennis;Michael Fisher;Robert Lieck;Andreas Matthias;Marija Slavkovik;Janina Sombetzki;Alan F. T. Winfield;Roman Yampolskiy	2017	CoRR		robot;data mining;software deployment;accountability;computer science;management science;conceptual design;machine ethics;transparency (graphic);autonomous system (internet);legislation	AI	-64.54565224451322	7.431071079694842	56811
e48c7d2d2ad1a745c99728d63f980015d4fa1ca0	an improved model of administrative decision-making support system based on mas	administrative decision making support system;monitoring agent administrative decision making mas improved model;multi agent system;information technology;government;mas;law;monitoring agent;monitoring subsystems;support system;dynamic environment;legal factors;multi agent systems;monitoring;decision support systems;multi agent systems decision making decision support systems government data processing;artificial intelligence;developing country;administrative decision making;decision making government proposals monitoring problem solving law legal factors artificial intelligence educational institutions information technology;china;proposals;china administrative decision making support system mas consulting subsystem monitoring subsystems government multi agent system;government data processing;consulting subsystem;problem solving;improved model	Referring to administrative decision-making systems and procedures in developed countries, this paper proposes an improved model of administrative decision-making support system based on MAS, which emphasizes the effect of consulting and monitoring subsystems on administrative decision-making, to overcome the deficiency of present decision-making support system. Specifically, the framework of this improved model and its operational mechanism are analyzed elaborately. In China’ complex and dynamic environment, this new model would help the government to solute effectively the difficulties of administrative decision-making and to improve its adaptability to changing administrative system.	angular defect	Yi Zhang;Siwen Yuan;Xiaolin Xu	2009	2009 International Joint Conference on Artificial Intelligence	10.1109/JCAI.2009.143	developing country;computer science;knowledge management;artificial intelligence;multi-agent system;data mining;management science;china;government	Robotics	-64.31076684749907	5.64006684858032	56959
5370abb7257c135c3da966bb2b4bab1ea7a4cceb	management tool for software factory contracts for a brazilian public agency		Contracting Information Technology (IT) services has become common practice in the Public FederalrnAdministration (APF). It allows the APF to focus its resources on the primary activities, allowing a betterrnexecution of the planning, coordination, supervision and control tasks. One of the challenges is therninterdisciplinarity involved in managing contracts, from the characteristics of the object – financial andrnlegal aspects – to human relationship aspects. The objective of this work is to use the case study andrnresearch-action techniques and specify a tool for software development and maintenance contractsrnmanagement, according to agile principles, to support the management of software factory contractsrnbetween a Brazilian Public Federal Agency and its suppliers, in line with the specifications of therncontracting notices. The tool specification is in the development stage. Bibliographical and documentalrnresearch activities, as well as object diagnostics, have been accomplished. From the experience, it isrnpossible to glimpse the specification of a contract management tool aligned with the legislation, thernagency contracts, and the software development and maintenance processes defined in the agilernmethodology.	software factory	Thabata Helen Macedo Granja;Rejane Maria da Costa Figueiredo;Edna Dias Canedo	2017			operations management;knowledge management;engineering management;computer science;software development;agile software development;software factory;information technology;contract management;legislation	Logic	-69.49115428515671	13.93690958752235	56990
79198b0fcf435653ea29908a7ecaaa90095d57df	crowdsourcing to elicit requirements for myerp application	software;erp application crowdsourcing requirements elicitation;systems analysis enterprise resource planning software houses;stakeholders;companies;crowdsourcing companies software stakeholders privacy;german medium size software company crowdsourcing myerp application software systems requirements elicitation;crowdsourcing;privacy	Crowdsourcing is an emerging method to collect requirements for software systems. Applications seeking global acceptance need to meet the expectations of a wide range of users. Collecting requirements and arriving at consensus with a wide range of users is difficult using traditional method of requirements elicitation. This paper presents crowdsourcing based approach for German medium-size software company MyERP that might help the company to get access to requirements from non-German customers. We present the tasks involved in the proposed solution that would help the company meet the goal of eliciting requirements at a fast pace with non-German customers.	crowdsourcing;requirement;requirements elicitation;software system	Pratyoush K. Srivastava;Richa Sharma	2015	2015 IEEE 1st International Workshop on Crowd-Based Requirements Engineering (CrowdRE)	10.1109/CrowdRE.2015.7367586	requirements analysis;crowdsourcing software development;systems engineering;knowledge management;requirements elicitation;data mining;business;software requirements	SE	-72.26489340413502	18.609097723218106	57003
8a55fb217fbe8318e00cade34f41fbf2ff0335a7	structuring manufacturing strategy	performance measure;manufacturing systems;corporate strategy;decision analysis framework;business strategy;decision analysis framework manufacturing strategy decisionmaking problem corporate strategy business strategy marketing strategy;spectrum;decision analysis;strategic planning decision making manufacturing systems marketing;strategic planning;marketing strategy;marketing;costs virtual manufacturing decision making manufacturing automation investments performance evaluation companies usa councils outsourcing preventive maintenance;manufacturing strategy;decisionmaking problem	Manufacturing strategy comprises decisionmaking problems in terms of manufacturing practices to achieve manufacturing objectives through linkages of performance measurement. It is pervasively influential, long-term, and dynamic owing to the conformance with corporate strategy, business strategy, and marketing strategy. Therefore, there are many underlying sub-problems which can be expressed in a wide spectrum of forms. In this study, we will firstly define the spectrum of problem-structuring. After that, a decision analysis framework as a guide for modeling problems in different forms is introduced. Finally, a realistic problem is illustrated for discussions.	causality;conformance testing;data structure;decision analysis;observable;requirement;strategic management	Chen-Fu Chien;Jei-Zheng Wu	2007	2007 IEEE International Conference on Automation Science and Engineering	10.1109/COASE.2007.4341839	integrated computer-aided manufacturing;marketing;operations management;competitor analysis;management science;computer-integrated manufacturing;marketing strategy;business;technology strategy;functional strategy;profit impact of marketing strategy	Robotics	-77.24821800033877	7.924730690241671	57026
20cc39a8f30981ca7205f60e3bdb642a3dd572d8	problems in agile global software engineering projects especially within traditionally organised corporations: [an exploratory semi-structured interview study]	distributed agile development;geographically distributed agile development;exploratory semi structured interview study;traditionally organised corporations;agile global software engineering;distributed agile software engineering	Agile methods are used for different kinds of software development, even for complex and distributed projects. This leads to a mixture of traditional company organisation as well as project organisation and agile concepts, especially when agile methods are applied within traditionally organised corporations. Therefore, the main objective of this exploratory semi-structured interview study is to phrase out the occurring problems of this mixture, to describe them, to name their possible effects and to suggest potential solutions. Practitioners as well as researchers can use those information to improve their projects and to investigate possible solutions to lessen the described problems. During our study we have interviewed seven persons in six interviews who were working for five different companies in Germany. Our analysis revealed eight problems of agile global software engineering projects. Three of them are more likely and more harmful when traditionally organised corporations are involved. But one of those three problems seems solely occurring in global software engineering projects where one or more traditionally organised corporations take part.	agile software development;semiconductor industry;software engineering	Ingo Richter;Florian Raith;Michael Weber	2016		10.1145/2948992.2949019	agile usability engineering;management science;empirical process;lean software development	SE	-71.20754839777553	20.534669896963177	57078
4f0cc26ac056e12002fd3fefa7b5dc9b209d3daa	rfid in supply chain management	challenges;companies;supply chains;supply chain management radiofrequency identification;bullwhip effect;rfid tags;scm;ripple effect;rfid;supply chain;challenges rfid supply chain management;bullwhip effect rfid supply chain management scm ripple effect;supply chain management supply chains companies rfid tags;radiofrequency identification;supply chain management;demand uncertainty;competitive advantage	Supply chain management (SCM) is essential for a company. Effective SCM can always enable the company maintain a stable and lasting competitive advantage, thereby can increase its overall competitiveness. But there are two main problems in the traditional SCM, which are known as “Ripple Effect” mainly due to the delay of a particular activity which results in the delay of the whole SCM and “Bullwhip Effect” caused by the demand uncertainty in the supply chain, respectively. As markets become more global and competition intensifies, the old mode of the traditional SCM is not feasible any more. Companies must find a new way out. RFID technology is a good choice, which can greatly overcome the shortcomings of the traditional SCM and improve its efficiency. However, on the other hand, there are also some challenges. This paper mainly studies the application of RFID technology in supply chain management, the strengths and weaknesses of it are all analyzed.	competitive analysis (online algorithm);radio-frequency identification;ripple effect	Leian Liu;Zhiqiang Chen;Dashun Yan;Yi Lu;Hongjiang Wang	2010	2010 International Conference on E-Business and E-Government	10.1109/ICEE.2010.825	radio-frequency identification;supply chain management;economics;marketing;operations management;supply chain;commerce	DB	-80.87433395282517	7.0906118219822005	57116
fb84ae7886589c6eba9ba8aba6dd0a13efa1636d	smartpm: an adaptive process management system for executing processes in cyber-physical domains		Demo Abstract. Nowadays, the automation of business processes not only spans classical business domains (e.g., banks and governmental agencies), but also new settings such as healthcare, smart manufacturing, domotics and emergency management [2]. Such domains are characterized by the presence of a Cyber-Physical System (CPS) coordinating heterogeneous ICT components with a large variety of architectures, sensors, actuators, computing and communication capabilities, and involving real world entities that perform complex tasks in the “physical” real world to achieve a common goal. In this context, Process Management Systems (PMSs) are used to manage the life cycle of the processes that coordinate the services offered by the CPS to the real world entities, on the basis of the contextual information collected from the specific cyber-physical domain of interest. The physical world, however, is not entirely predictable. CPSs do not necessarily and always operate in a controlled environment, and their processes must be robust to unexpected conditions and adaptable to exceptions and external exogenous events. In this paper, we tackle the above issue by introducing the SmartPM System (http://www.dis.uniroma1.it/ ̃smartpm) an adaptive PMS which combines process execution monitoring, unanticipated exception detection (without requiring an explicit definition of exception handlers), and automated resolution strategies on the basis of well-established Artificial Intelligence techniques, including the Situation Calculus and IndiGolog [1], and classical planning [3]. Significance to the BPM field. Exception handling is one of the most important tasks that process designers undertake during process modelling and execution [5]. In cyberphysical domains, the fact is that the number of possible anticipated exceptions is often too large, and traditional manual implementation of exception handlers at design-time is not feasible for the process designer, who has to anticipate all potential problems and ways to overcome them in advance. Furthermore, in such domains many unanticipated exceptional circumstances may arise during the process execution, requiring to adapt running process instances in a situationand context-dependent way. While most PMSs of today shy away from dealing with the inherent dynamic nature of cyber-physical domains by providing manual or semi-automated techniques to deal with unanticipated exceptions, the management of processes enacted in such domains requires a PMS providing real-time monitoring and automated adaptation features during process execution. In this direction, SmartPM allows (i) the continuous screening of real-world objects performed by the physical sensors disseminated in the cyber-physical domain of interest, by transforming the “continuous” knowledge extracted from the domain in its digital counterpart; and (ii) the formalization of explicit mechanisms to model world changes and responding to anomalous situations, exceptions, exogenous events in an	anomaly detection;artificial intelligence;business process;context-sensitive language;cyber-physical system;entity;exception handling;home automation;management system;physical symbol system;process modeling;real-time clock;semiconductor industry;sensor;situation calculus	Andrea Marrella;Patris Halapuu;Massimo Mecella;Sebastian Sardiña	2015				AI	-73.55310222104613	11.5946326722624	57231
cd2b21ef97165cbea1be262fa0f4e1b5ce89bc6e	the linkage between tmt knowledge diversity and firm-level innovation: the role of organisational search scope and managerial discretion		In this study, we develop a new perspective on the linkage between the knowledge base of the top management team (TMT) and innovation performance. Using longitudinal data on the patent activities of 120 firms in US manufacturing industries, we find that the knowledge diversity based on a TMTu0027s prior experiences affects organisational innovation. Specifically, firms can achieve greater innovativeness if their top teams have a higher degree of knowledge diversity, i.e., a more generalised knowledge base. In addition, the degree of organisational search scope positively moderates this linkage between TMT knowledge diversity and firm innovation. In addition, we also found that the degree of managerial discretion at the industry level enhances the linkage between TMT knowledge diversity and firm innovation.	linkage (software)	Shapoor Kakoolaki;Theresa S. Cho;Jina Kang	2018	IJTM	10.1504/IJTM.2018.10016467	knowledge management;knowledge base;economics;discretion;manufacturing	NLP	-82.3352057091362	4.237728092615403	57237
14fa20c989c27f15abc2091fb7454798cf023944	value implications of investments in information technology	gestion integrada;gestion integree;commerce electronique;gestion entreprise;comercio electronico;inversion;processus metier;information technology;resource management;enterprise resource planning systems;firm management;integrated management;technologie information;investment;gestion recursos;enterprise resource planning system;information technology and firm performance;information value;investissement;gestion ressources;proceso oficio;administracion empresa;tecnologia informacion;business value;valor informacion;firm value;valeur information;relative investment in information technology;strategic role of information technology;electronic trade;business process;firm performance;business value of information technology	The year 2000 countdown provided a uniquely visible instance of spending on information technology by U.S. companies. With public attention riveted on potential Y2K malfunctions, managers were forced to evaluate their IT and make decisions whether to modify or replace existing systems. In the aftermath of Y2K, critics charged that the problem was overblown and that companies overspent on IT. In contrast, we posit in this paper that efforts companies made to renew and upgrade their IT may have positioned them to take advantage of new e-business applications. As Y2K approached, managers could invest opportunistically in IT that would enable them to connect with customers and suppliers in new ways. Contrary to the alleged overspending, we find that firm value increased on average with Y2K spending by Fortune 1000 companies. In particular, higher firm value and subsequent earnings were associated with Y2K spending for firms in industries where IT was considered to have a transforming influence – altering traditional ways of doing business by redefining business processes and relationships. We also test whether the positive association between firm value and Y2K spending diminished with Y2K spending by industry peer firms but we do not find support for this relative investment hypothesis.	business process;definition;electronic business;half-life 2: episode one;year 2000 problem	Mark C. Anderson;Rajiv D. Banker;Sury Ravindran	2006	Management Science	10.1287/mnsc.1060.0542	inversion;economics;investment;marketing;resource management;operations management;business value;finance;business process;management;information technology	HCI	-81.67750279902299	8.395565814504213	57238
ec56503fc897d8c48ab09d4a7b454321d5d9689a	professional engineers ontario's approach to licensing software engineering practitioners	software engineering professional engineers ontario software engineering practitioner licensing software practitioner qualifications assessment academic preparation work experience criteria;training;software engineering;body of knowledge;training software engineering professional aspects accreditation societies;professional aspects;licenses software engineering qualifications design engineering knowledge engineering software quality mathematics object oriented modeling data engineering power engineering and energy;accreditation;societies	Professional Engineers Ontario (PEO) has developed a methodology to assess software practitioners ’ qualifications for licensing purposes. It entails a comprehensive assessment of the applicants’ academic preparation and work experience vis-ri-vis P E 0 ’s sofnvare engineering body of knowledge and criteria f o r acceptable experience. Using this approach, P E 0 has licensed close to two hundred sofrware engineering practitioners to date. 1NT.RODUCTION A four-year Canadian Engineering Accreditation Board (CEAB)-accredited undergraduate engineering degree, or its equivalent, and the completion of forty-eight months of acceptable work experience are the main requirements for professional engineering licensure. At present, however, there are no CEAB-accredited software engineering programs. Nevertheless, there are a number of software practitioners who are interested in pursuing licensing. During the Fall of 2000, software engineering programs in the engineering faculties at McMaster University, University of Ottawa, and the University of Western Ontario were evaluated by the CEAB and a decision, in each case, is pending. In the meantime, P E 0 has devised a methodology to assess software practitioners’ qualifications. The model recognizes that the academic preparation for software engineering licensing requires in-depth training in the basic sciences, mathematics, engineering sciences, engineering design and complementary studies, consistent with the CEAB’s well-established criteria. PEO’s assessment serves to ascertain if the applicants’ overall training has enabled them “to bridge the knowledge gap” between their respective academic preparation and that required for professional software engineering practice. It emphasizes the need for them to gain a good understanding of engineering fundamentals to ensure an awareness of, and an appreciation for the boundaries of other engineering disciplines. Using this model, P E 0 has licensed approximately two hundred professional software engineers. P E 0 is cognizant of the rapid developments in the software engineering field and their potential impact on its licensing requirements. Therefore, PE0 will make sure that any significant changes are incorporated into its established body of knowledge while maintaining an emphasis on fundamental principles. PEO’s QUALIFICATIONS ASSESSMENT APPROACH The Table below shows PEO’s software engineering body of knowledge which provides the basis for assessing applicants’ qualifications. It permits an assessment of their academic preparation, including any professional development training, as well as their experience. 0-7695-1059-0/01 $ 0.00	engineering design process;requirement;software engineering body of knowledge;software engineer;test template framework	Norman S. W. Willams	2001		10.1109/CSEE.2001.913823	engineering management;personal software process;software engineering process group;systems engineering;engineering;social software engineering;civil engineering software;software engineering;software engineering professionalism;software walkthrough;software requirements;software peer review	SE	-66.73350353428519	26.297919596839858	57350
8d0f0cc5f2f2ebc4de773a893ade1c384cb216c7	myopenfactory	erp system;order processing;erp software;processing technology;connected erp system;integrated order management;industry wide inter-company order;delivery process;inter-company order processing;shorter delivery time	Shorter delivery times and higher delivery reliability force engineering companies to integrate their order processing with all other business partners. Today's ERP systems do not have the ability to exchange data from one cooperation partner to another in a flexibly automated way. That is why today's inter-company order processing is still done manually by telephone and fax. With a totally integrated order management, there are possible cost savings of up to 50% in order processing. Furthermore, it helps companies to optimize their logistic and delivery processes. myOpenFactory provides an industry wide inter-company order processing technology. The developed IT-platform is founded on service oriented architecture, allowing all platform functions to be executed by the connected ERP systems via web services. The described technical compatibility has been realized in cooperation with a group of leading suppliers of ERP software. Furthermore a standardized data model secures an interface free flow of information. Commercializing the myOpenFactory platform, a new internet business model has been developed - the myOpenFactory cooperative society.		Günther Schuh;Achim Kampker;Charlotte F Narr;Till Potente;P. Attig	2008	Int. J. Computer Integrated Manufacturing	10.1080/09511920701607766		Robotics	-65.9382527724882	11.115858287805807	57393
b163901d20b6af639a40294117abb52a10651230	simmi 4.0 - a maturity model for classifying the enterprise-wide it and software landscape focusing on industry 4.0	electronic mail;information systems;industries;companies;internet;production	The increasing digitalization of business and society leads to drastic changes within companies. Nearly all enterprises have to face enormous challenges when dealing with topics such as Industry 4.0/Industrial Internet. One of these challenges represents the realistic classification of the company's own IT infrastructure. In this paper we present a maturity model (SIMMI 4.0 - System Integration Maturity Model Industry 4.0) that enables a company to classify its IT system landscape with focus on Industry 4.0 requirements. SIMMI 4.0 consists of 5 stages. Each describes several characteristics of digitization, which allows a company to assess itself. Additionally, recommended activities are presented for each stage of digitization, which can enable a company to reach the next stage of maturity. We also present several possible topics for future research to improve and refine the developed maturity model.	capability maturity model;industry 4.0;internet;requirement;system integration	Christian Leyh;Thomas Schäffer;Katja Bley;Sven Forstenhäusler	2016	2016 Federated Conference on Computer Science and Information Systems (FedCSIS)	10.15439/2016F478	the internet;artificial intelligence;database;management;service integration maturity model;law;information system	Vision	-73.36857722407659	6.074746762857256	57511
43779e37d74b70ab91a7bb75eb4803a1afeaa1d3	a study of the metrics applied to the software evaluation framework 'sef'	software metrics;software quality software measurement australia testing information technology software engineering human factors programming psychology;software performance evaluation;empirical software engineering;metrics;qualitative study;human factors;software reliability software development management software metrics software performance evaluation software quality;software reliability;empirical software engineering metrics study software evaluation framework software characteristics model testing stakeholder software quality;software quality;software development management;conference proceeding;software evaluation	The primary objective of this paper was to present a study on the metrics which can be applied to the software evaluation framework. The paper presents the results of a preliminary study, which focuses on the measurements applied to the framework. It provides a description of the metrics used and an analysis of how they compare to each other in the measurement of the software characteristics. This objective was accomplished by empirically testing the model with quantitative techniques. An earlier qualitative study provided a list of metrics, which were then tested with the quantitative study described in this paper. The results of this study are important as it identifies the metrics, perceived by stakeholders as essential for applying the software evaluation framework to software evaluation.	spectral edge frequency	Bernard Wong	2003	Third International Conference on Quality Software, 2003. Proceedings.	10.1109/QSIC.2003.1319085	reliability engineering;personal software process;verification and validation;software sizing;computer science;systems engineering;engineering;human factors and ergonomics;package development process;social software engineering;software reliability testing;software development;software design description;software engineering;software construction;software testing;software walkthrough;software analytics;software measurement;software deployment;software quality;software metric;software quality analyst;software system;software peer review	SE	-65.90195806332241	29.302636145252904	57653
73fde0e49a903a25a319b35ef01d8a95558eca16	examining the strategy development process through the lens of complex adaptive systems theory	modelizacion;forecasting;reliability;project management;complexity theory;information systems;complexite calcul;ingenieria de sistemas;maintenance;teoria sistema;soft or;information technology;systems engineering;packing;conceptual model;strategy development process;prise de decision;complex adaptive system;sistema complejo;operations research;location;development process;systeme adaptatif;conceptual framework;investment;journal;journal of the operational research society;strategic planning;inventory;purchasing;modelisation;complejidad computacion;estrategia empresa;history of or;strategic decision making;logistics;complex adaptive systems cass;systeme complexe;systems theory;complex system;marketing;computational complexity;scheduling;theorie systeme;systems thinking;adaptive system;ingenierie systeme;planification strategique;production;sistema adaptativo;communications technology;enseignement;computer science;operational research;educacion;toma decision;firm strategy;modeling;strategie entreprise;planificacion estrategica;applications of operational research;or society;jors;management science;infrastructure;teaching;ensenanza	The development of strategy remains a debate for academics and a concern for practitioners. Published research has focused on producing models for strategy development and on studying how strategy is developed in organisations. The Operational Research literature has highlighted the importance of considering complexity within strategic decision making; but little has been done to link strategy development with complexity theories, despite organisations and organisational environments becoming increasingly more complex. We review the dominant streams of strategy development and complexity theories. Our theoretical investigation results in the first conceptual framework which links an established Strategic Operational Research model, the Strategy Development Process model, with complexity via Complex Adaptive Systems theory. We present preliminary findings from the use of this conceptual framework applied to a longitudinal, in-depth case study, to demonstrate the advantages of using this integrated conceptual model. Our research shows that the conceptual model proposed provides rich data and allows for a more holistic examination of the strategy development process.	complex adaptive system;systems theory	Roger Julius Hammer;John S. Edwards;Efstathios Tapinos	2012	JORS	10.1057/jors.2011.97	project management;complex adaptive system;logistics;strategic planning;inventory;economics;forecasting;investment;computer science;conceptual model;marketing;operations management;adaptive system;reliability;conceptual framework;complexity management;location;management;operations research;information technology;scheduling;systems theory;systems thinking	HCI	-68.33281561598947	7.468882694285069	57694
df0df5d55c62e4aab2b025aba03c9db88c11e91c	impact of it infrastructure on customer service performance: the role of micro-it capabilities and online customer engagement		This research argues that information technology (I T) capabilities can be classified in macroand micro-IT capabilities. We propose that I T infrastructure capability (a macro-IT capability) enables the development of social media and e-business technology capabilities (two micro-IT capabilities) to online engage custom ers and improve the firm’s customer service performance. We test the proposed model by using the variance-based structural equation modelling technique employing an innovativ e secondary dataset on a sample of 100 small U.S. firms. Results suggest that IT infra structure capability positively affects customer service performance through two micro-IT c apabilities (social media and ebusiness technology) and social and conventional on line customer engagement.	electronic business;social media;structural equation modeling	Jessica Braojos-Gomez;Jose Benitez-Amado;Francisco Javier Lloréns Montes	2015			customer intelligence;customer engagement;attitudinal analytics;marketing;computer science;customer reference program;process management;customer advocacy;customer to customer;customer service assurance;customer retention	AI	-80.23177989390825	4.557479155995437	57704
9f10c984bedbabcea5fafb1644900891f0f5d8f4	it readiness in small and medium-sized enterprises	time budget;information technology;small and medium size enterprise;literature review;small to medium sized enterprises;design methodology	Several studies have documented that IT projects often do not successfully meet defined objectives regarding time, budget, and functionality. There can be multiple causes for this, and an important factor in this context is the extent to which a company is ready for an IT project. To help understand this dynamic, this paper presents a framework for analyzing 'IT readiness' in SMEs (small and medium-sized enterprises) based on a literature review. The framework is illustrated and investigated by three case studies and shows that the framework of IT readiness in SMEs is useful for assessing company readiness and supporting the management of a project. The framework and case studies provide an improved understanding of how to evaluate readiness of an SME for an IT project and provides a solid basis for SMEs who plan to engage in an IT project and help to increase the chances of success.		Anders Haug;Søren Graungaard Pedersen;Jan Stentoft Arlbjørn	2011	Industrial Management and Data Systems	10.1108/02635571111133515	design methods;knowledge management;operations management;management science;law;information technology	SE	-76.838369664549	9.262438483503898	57716
61db134b511ccd7e87b9da9bf53cc8f44b0e3e7c	co-opetition between differentiated platforms in two-sided markets	differentiated platforms;platform technology;value proposition;inferior platform;technology improvement;network value;dominant platform;two-sided markets;direct network interconnection;two-sided market;intermediary platform;platform market	Technology is an important factor underlying the value propositions of intermediary platforms in two-sided markets. Here, we address two key questions related to the effect of technology in platform markets. First, how does technology asymmetry affect competition between platforms? Second, how does it affect the incentives for platforms to collaborate? Using a game-theoretic model of a two-sided market where technology strongly influences network value, we show that small asymmetries in platform technologies can translate into large differences in their profitability. We find that technology improvements by the inferior platform do not significantly increase its profits, but can reduce opportunities for fruitful cooperation, since collaboration is less likely in markets with closely matched competitors. We also show that collaboration is most profitable when it takes the form of direct network interconnection. Interestingly, collaboration may provide incentives for a dominant platform to accommodate entry, where it would not otherwise do so.		Ravi Mantena;Rajib L. Saha	2012	J. of Management Information Systems		game theory;economics;marketing;operations management;management;competitive advantage;commerce	ECom	-87.28559575715107	5.501817235453175	57755
0d4c0210137152a4cf42f94c9a286c6819eb43ee	development of scientific software and practices for software development: a systematic literature review		The development of adequate scientific software within the framework of a research project plays a key role in the success of the research itself. However, not all research teams complete the development of a specific software within the deadlines and with the necessary quality standards. These difficulties have been studied for a lot of years and we can conclude that these applications are difficult to create because defining their requirements is a complex task and usually developers are not fully skilled in construction of these. The following systematic literature review characterizes this kind of development and exposes its difficulties. A number of methodological solutions as well as a series of widespread practices are presented, aimed at improving the development of such kinds of software. A combination of elements is offered which would allow software development teams to choose the most adequate solution according to their specific requirements. The research results show the difference between engineering and scientific disciplines from the type of problems that it solves. Because of this, it is difficult to use a software development technique to work well in both cases and for this reason, the proposals are very useful for the future of scientific software development.	requirement;software development process;systematic review	Gerardo Cerda Neumann;Hector Antillanca Espina;Víctor Parada Daza	2017	JSW		artificial intelligence;software peer review;machine learning;systems engineering;software;computer science;software development;software technical review;systematic review	SE	-67.8414381493096	23.418742793855802	57758
066c27e4bbc5794b4d1b37ed463448e87007df5a	managing infrastructure vulnerability: an empirical study on the use of performance management systems that seek to reduce vulnerability of network industries	performance measurement systems infrastructure vulnerability performance management systems network industries;empirical study;rail transportation;measurement;netherlands;performance management;computer networks security measures;supply interruptions;government;energy industry;performance measurement systems pms;contracts;electricity supply industry rail transportation government energy management technology management industrial training load management measurement contracts production;network industries;transportation services;technology management;infrastructure economics defense measures;service disruptions;infrastructure vulnerability;performance management systems;industrial training;power system management;load management;production;punctuality;network vulnerabilities;electricity supply industry;electricity industry;performance measurement systems;energy industries;infrastructure economics security measures;performance measurement system;operational services vulnerabilities;energy management;power system management electricity supply industry;infrastructure vulnerabilities	Many governments use Performance Measurement Systems (PMS) to measure vulnerability of network industries and to manage the process of reducing this vulnerability. We focus on two important aspects of vulnerability: interruption of supply for the Dutch electricity industry and punctuality in the Dutch railway industry. First, we describe the design of the PMS in both industries. Next, we confront the characteristics of these PMS's with the underlying complexity of the operational service in both sectors. Further, we describe the positive and perverse effects of PMS in action. Finally, we interpret our findings with a number of theoretical notions.	interrupt;unintended consequences;vulnerability (computing)	Hans de Bruijn;Mark de Bruijne;Bauke Steenhuisen	2007	2007 IEEE International Symposium on Technology and Society	10.1109/ISTAS.2007.4362235	operations management;vulnerability assessment;business;commerce	Arch	-67.1310641194124	8.721210224134495	57760
b0855f2715d00f500ac72f1aa4b7bbb13f841401	devops metrics		Your biggest mistake might be collecting the wrong data.	devops	Nicole Forsgren;Mik Kersten	2017	ACM Queue	10.1145/3178368.3182626		Theory	-68.9168574142776	25.306627130105333	57943
eb80408414f5f0fa5783f275f3a0e44d77017afa	a model for the suggestion of logistics partners for virtual organizations		In a Virtual Organization (VO) scenario the difficult to select the most appropriate logistic providers is even higher. Part of this is due to the intrinsic nature of a VO, which is a temporary and dynamic alliance of autonomous, heterogeneous and geographically dispersed companies (often small and medium enterprises) created to attend to very particular business opportunities, sharing costs, benefits and risks, and acting as it was one single enterprise. In the literature review the specification of the methodology for selecting logistics partners to compose virtual organizations using key performance indicators has not yet been adequately explored. This work proposes a methodological support, to be called as a framework for suggesting of logistics partners for virtual organizations (VOs) based on performance indicators analysis.	autonomous robot;decision support system;formal ontology;logistics;prototype;requirement;software prototyping;value (ethics);virtual organization (grid computing)	O. Correia-Alves;J. R. Rabelo	2011			knowledge management;operations management;business;management	HCI	-70.05784339738906	10.509194668823126	57981
8e4a65c14b91720d2dd0056818c38a9cc448a0fa	cooperation in highly distributed design processes: observation of design sessions dynamics	distributed system;proceso concepcion;groupware;systeme reparti;design process;critical study;visualizacion;traitement flux donnee;simultaneidad informatica;etude critique;collaborative tools;preparacion serie fabricacion;estudio critico;visualization;information flow;concurrency;sistema repartido;visualisation;data flow processing;coordinacion;process planning;ingenierie simultanee;ingenieria simultanea;communication channels;preparation gamme fabrication;collecticiel;simultaneite informatique;concurrent process;concurrent engineering;industrial design;distributed design;coordination;processus conception	In the recent years, industrial design programs have shifted from a co-located, sequential process to a globally distributed, concurrent process. Therefore, communication and coordination within and among teams have become major challenges. Surprisingly, several studies demonstrate that collaborative tools exist but are very seldom deployed nor used. Indeed, co-located working sessions remain a critical cooperation milestone for otherwise remote teams. In order to better understand the dynamics of those meetings, and identify what makes them so critical for the design process, we analysed working sessions of aeronautical design teams. This article reports the results of our observations based on three criteria: information types, information flow and interactions performed.#R##N##R##N#Our study shows the critical importance of private communication channels, and investigates how they relate to the working session’s process. It highlights the need of parallel visualization channels for cross-references, and analyses the use of interactions depending on ownership and context.		François Laborie;David Jacquemond;Matthieu Echalier	2005		10.1007/11555223_5	simulation;industrial design;visualization;computer science;artificial intelligence;operating system;database;distributed computing;management;operations research;computer security;algorithm	EDA	-66.76635113154036	7.54046486455779	57992
0aeb3f1a5d7d7c9ded1f3132e26b914eda2f2221	specification by example for educational purposes		The Specification By Example (SBE) is a guideline for building the right software, a software that meets customer requirements. It is based on seven process patterns and enhances communication and collaboration and it usually is used in agile software development. The connection between education and agile software development sounds actually as an emergent topic. In this paper, we propose to structure a teaching approach in analogy to an agile software developement by transposing each process pattern of SBE to a corresponding one in the teaching domain. Moreover, we show that thanks to the emergence of a collective intelligence process, the students are more confident and more responsible. Such a course offers the opportunity to learn not only technical skills, but also some values in a new mindset.	agile software development;collective intelligence;emergence;requirement;specification by example	Isabelle Blasquez;Hervé Leblanc	2017		10.1145/3059009.3059039	computer science;goal-driven software development process;agile unified process;knowledge management;empirical process (process control model);personal software process;package development process;agile usability engineering;specification by example;lean software development	SE	-66.9640481194019	25.172875753708457	58036
d23b9b4cca7198e30caa2c410515cb1d6be80041	digital innovation and the division of innovative labor: digital controls in the automotive industry	digital control hierarchy;digital controls;automotive industry;digital innovation;mirroring hypothesis;division of innovative labor;dual product hierarchy;inclusionary hierarchy;systems integration	In this study of the U.S. automobile industry, we highlight the way the division of innovative labor across firms in the supply chain can be influenced by a particular form of digital innovation known as “digital control systems.” Digital control systems are becoming ubiquitous in complex products, and these digital innovations integrate other components across a product structure and introduce a level of indeterminacy and unpredictability in the organization of the interfirm division of innovative labor. Much of organizational scholarship holds that accompanying a shift toward increasingly modular product structures, component suppliers are engaging in relatively more design and invention around the components that they supply. We find that the evolution of digital controls may reverse this pattern, because in the wake of a major shift in the digital controls technology, suppliers actually engage in relatively less component innovation in comparison with their large manufacturing customers. To explain this shift, we characterize complex product structures in terms of two distinct product hierarchies: the inclusionary and the digital control hierarchy. In using this distinction to analyze the evolution of automotive emission control systems from 1970 to 1998, we reconcile two competing views about the interfirm division of innovative labor.	control system;digital revolution;indeterminacy in concurrent computation;word lists by frequency	Jaegul Lee;Nicholas Berente	2012	Organization Science	10.1287/orsc.1110.0707	simulation;automotive industry;operations management;management;law;system integration	HCI	-78.42538946811118	4.495113542183569	58064
56df6313642124147e3dfd74c5ccea9c09a0e500	design chain collaboration - a strategic view		Design chain collaboration involves the process to integrate suppliers’ design expertise in the product design stage. With the increasing of product information contents and the extending of design authority distribution, the collaboration process has become more and more complex. Lots of studies have been focused on developing applications such as application integration software or project management applications in order to manage the complexity. However, as the complexity of design chain collaboration keeps growing, these applications might meet their limitation soon. Instead of trying to manage the complexity, this paper focuses on discussing strategic approaches that can be used to reduce design chain collaboration complexity. The sources that keep the complexity grow are first examined, and strategic approaches to reduce design chain collaboration complexity is discussed in this paper.	web design	Wen-Chieh Chuang;Hun-Hsiang Yang	2004	IJEBM			EDA	-64.3156347714685	12.569320683878546	58109
2c7b419ad8bb8dac258705003eefa62b7dafd9a9	on qkd industrialization	quantum key distribution;quantum cryptography	Abs t r ac t . During the 25 years of existence of the first protocol for Quantum Key Distribution, much has been said and expected of what came to be termed as Quantum Cryptography. After all this time, much progress has been done but also the reality check and analysis that naturally comes with maturity is underway. A new panorama is emerging, and the way in which the challenges imposed by market requirements are tackled will determine the fate of Quantum Cryptography. The present paper attempts to frame a reasonable view on the issues of the security and market requirements that QKD should achieve to become a marketable technology.	capability maturity model;quantum cryptography;quantum key distribution;reality check (tv series);requirement	J. Dávila;Daniel Lancho;J. Martinez;Vicente Martín	2009		10.1007/978-3-642-11731-2_36	telecommunications;engineering;nanotechnology;computer security;quantum cryptography	Theory	-70.61923662615821	25.95551868357916	58239
82e8a451d306bad0600f43268cfb93710a17413d	an exploratory study on the implementation and adoption of erp solutions for businesses		Enterprise Resource Planning (ERP) systems have been covered in both mainstream Information Technology (IT) periodicals, and in academic literature, as a result of extensive adoption by organisations in the last two decades. Some of the past studies have reported operational efficiency and other gains, while other studies have pointed out the challenges. ERP systems continue to evolve, moving into the cloud hosted sphere, and being implemented by relatively smaller and regional companies. This project has carried out an exploratory study into the use of ERP systems, within Hawke's Bay New Zealand. ERP systems make up a major investment and undertaking by those companies. Therefore, research and lessons learned in this area are very important. In addition to a significant initial literature review, this project has conducted a survey on the local users' experience with Microsoft Dynamics NAV (a popular ERP brand). As a result, this study will contribute new and relevant information to the literature on business information systems and to ERP systems, in particular.	erp;enterprise resource planning;exploratory testing;management information system;microsoft dynamics nav 2016;pareto efficiency	Emre Erturk;Jitesh Kumar Arora	2017	CoRR		public relations;operations management;management	SE	-81.74654683466206	6.355044428357661	58297
b7553d23173bdd917bd49c4658b41ba1d28ae820	a maturity model for cbse	cmu;component based software engineering;maturity model;software development tooling;capability maturity model;global software development;offshore development;infrastructure	The Capability Maturity Model (proposed by SEI-CMU) does not consider Component Based Software Engineering (CBSE) principles in its considerations of levels and KPAs. It is therefore necessary to consider a model that is based on peculiarities and importance of CBSE and hence a new model under the name ICMM (Integrated Component Maturity Model) for this purpose is being proposed herein. The model, ICMM, is applicable for many types of organizations like organizations that develop components only, organizations that develop CBS or organizations that develop components along with CBS. This work starts a discussion and calls for more extensive research oriented studies by professionals and academicians for perfection of the model.	capability maturity model;component-based software engineering;software engineering institute	Ratneshwer Gupta	2009		10.1145/1506216.1506241	leancmmi;computer science;systems engineering;engineering;software engineering;programming language;management;service integration maturity model;capability maturity model;people capability maturity model	SE	-64.15441471470983	20.071118945420643	58342
0b27cc609ff38d48636df2289d8b144bb926919c	holonic institutions for multi-scale polycentric self-governance	self organising systems;polycentric governance;holonic architectures;multi agent systems;smartgrids;electronic institutions	Effective institutions are key to the success of self-governing systems, yet specifying and maintaining them can be challenging, especially in large-scale, highly dynamic and competitive contexts. Political economist Elinor Ostrom has studied the conventional arrangements for sustainable natural resource management and derived from these eight design principles for self-governing institutions. One principle, nested enterprises, is straightforwardly expressed, but is arguably structural rather than functional, and so is more resistant to declarative specification; yet it also appears to be critical to the effectiveness of complex compositional systems. In this paper, we converge the ideas of holonic systems with electronic institutions, to propose a formalisation of this principle based on holonic institutions. We show how holonic institutions provide a structural framework for nested enterprises, which can be designed as composite systems of systems. This, we believe, is compatible with Ostrom’s ideas for polycentric governance of complex systems. We use a case study in energy distribution to illustrate these ideas.	autonomic computing;complex systems;computer architecture;converge;declarative programming;ecosystem;expectation propagation;game demo;grand challenges;holon (philosophy);situated;sociotechnical system;system of systems	Ada Diaconescu;Jeremy V. Pitt	2014		10.1007/978-3-319-25420-3_2	engineering;knowledge management;operations management;management	OS	-67.14255071945803	9.380443442803141	58357
52e616ec965621b55d3b6f1858ca157ba53a0f1b	cloud computing: adoption considerations for business and education	cloud deployment models;cloud computing electronic learning business software as a service security hardware;educational institutions business data processing cloud computing computer aided instruction distance learning;e learning;cloud infrastructure;cloud computing e learning system iaas infrastructure as a service paas platform as a service saas software as a service it information technology academic institution business institution;cloud deployment models cloud computing e learning cloud infrastructure;cloud computing	Cloud computing is gathering significant momentum in business and academia through the rich benefits it offers. It is apparent from the literature that both businesses and academic institutions would benefit greatly from the adoption of cloud technology, providing the challenges presented are overcome. This paper aims to review prevalent literature on cloud computing, presenting an initial comprehensive insight into how cloud technology is transforming businesses and the wider Information Technology (IT) industry in general, the service deployment models of Software as a Service (SaaS), Platform as a Service (PaaS), and Infrastructure as a Service (IaaS), and a discussion of perceived benefits and challenges of cloud adoption. The paper's core focus addresses the domain of Education as an area of cloud application with a cloud based e-learning system developed to demonstrate the capabilities and effectiveness of cloud technology. The last section of the paper offers a conclusion, discussing how cloud computing will evolve hereafter along with recommendations for furthering our research work.	cloud computing;platform as a service;software as a service;software deployment;usability testing	Adam Smith;Jagdev Bhogal;Mak Sharma	2014	2014 International Conference on Future Internet of Things and Cloud	10.1109/FiCloud.2014.54	cloud computing security;service catalog;simulation;cloud computing;computer science;operating system;cloud testing;utility computing;data as a service;world wide web;converged infrastructure	HPC	-67.93458912307628	10.755925266086978	58577
46cafc741de8d8653b5aee1c296a1ddf5a3f7efe	on the importance attributed to different knowledge in software development environments	software development environment		integrated development environment;software development	Karina Villela;Ana Regina Cavalcanti da Rocha;Guilherme Horta Travassos	2005			systems engineering;environmental science;development environment;knowledge management	SE	-63.95380820327499	22.41453990357784	58682
952345123ce8595f860b404194352a5575a389f9	industrial-scale environments with bounded uncertainty: a productivity maximisation challenge		We present an outline of the operating domain for control software in Ocado warehouses, and provide results which suggest that in this well-understood and highly controlled environment, there are limits to the uncertainty which planning and control systems need to consider. More specifically, that planning approaches can be biased towards rapid recovery when something goes wrong, rather than trying to deal with all possible eventualities up-front. Since academic interest has generally focused on complex and highly fault-tolerant up-front planning, we believe this domain and planning approach is fertile ground for further investigation.	content-control software;control system;fault tolerance	Daniel Sykes;Gavin Keighren	2018	2018 IEEE/ACM 1st International Workshop on Robotics Software Engineering (RoSE)	10.1145/3196558.3196563	psychological resilience;redundancy (engineering);management science;task analysis;software;warehouse;bounded function;computer science;control system	SE	-66.2842858567252	15.830114880989507	58697
d867c1c06778a1b11941ba9fc5e6ec00d813b1ea	software effort, quality, and cycle time: a study of cmm level 5 projects	software quality capability maturity model regression analysis software cost estimation software development management;cycle time;software cost estimation;data collection;linear regression model;software development process;software quality coordinate measuring machines programming costs capability maturity model object oriented modeling productivity best practices iso standards six sigma;capability maturity model;software development;software cost estimation software development effort software quality software cycle time estimation cmm level 5 project capability maturity model software development process improvement organization productivity linear regression model;productivity cost estimation time estimation software quality;regression analysis;productivity;cost estimation;software quality;software development management;time estimation	The Capability Maturity Model (CMM) has become a popular methodology for improving software development processes with the goal of developing high-quality software within budget and planned cycle time. Prior research literature, while not exclusively focusing on CMM level 5 projects, has identified a host of factors as determinants of software development effort, quality, and cycle time. In this study, we focus exclusively on CMM level 5 projects from multiple organizations to study the impacts of highly mature processes on effort, quality, and cycle time. Using a linear regression model based on data collected from 37 CMM level 5 projects of four organizations, we find that high levels of process maturity, as indicated by CMM level 5 rating, reduce the effects of most factors that were previously believed to impact software development effort, quality, and cycle time. The only factor found to be significant in determining effort, cycle time, and quality was software size. On the average, the developed models predicted effort and cycle time around 12 percent and defects to about 49 percent of the actuals, across organizations. Overall, the results in this paper indicate that some of the biggest rewards from high levels of process maturity come from the reduction in variance of software development outcomes that were caused by factors other than software size	capability maturity model;scientific literature;software development;software sizing	Manish Agrawal;Kaushal Chari	2007	IEEE Transactions on Software Engineering	10.1109/TSE.2007.29	reliability engineering;productivity;cycle time variation;computer science;systems engineering;engineering;linear regression;software development;software engineering;capability maturity model;software development process;software quality;regression analysis;cost estimate;data collection	SE	-65.90308639980739	29.602337438341124	58730
5b4e082428bde9853c02c45a15ca2e25a71d82be	large-scale data envelopment analysis (dea) implementation: a strategic performance management approach	benchmarking;modelizacion;forecasting;logro;unfolding;evaluation performance;analisis envolvimiento datos;disaster;reliability;base donnee;project management;information systems;permanente matriz;performance evaluation;performance management;ahorro;achievement;deploiement;maintenance;evaluacion prestacion;soft or;nonprofit social services;information technology;estimation non parametrique;packing;evaluacion comparativa;database;despliegue;base dato;operations research;location;investment;journal;intelligence economique;journal of the operational research society;strategic planning;qualite service;data envelopment analysis implementation;inventory;emergency;purchasing;modelisation;non parametric estimation;large scale;history of or;outcome measurement;logistics;data envelopment analysis;marketing;permanent;scheduling;sinistre;saving;urgencia;controle qualite;planification strategique;production;urgence;competitive intelligence;communications technology;reussite;computer science;estimacion no parametrica;operational research;performance evaluation system;escala grande;inteligencia economica;data envelope analysis;quality control;epargne;permanent matrice;modeling;service quality;planificacion estrategica;applications of operational research;or society;jors;management science;control calidad;infrastructure;siniestro;analyse enveloppement donnee;calidad servicio;echelle grande	We present one of the first large-scale implementations of data envelopment analysis (DEA) at the heart of a permanent performance management system in its third year of operation. The system evaluates more than 1000 field unit operations devoted to disaster relief, emergency communications, and life-saving skills training. The following research objectives were accomplished: (a) advanced a conceptual model for measuring performance in the nonprofit sector; (b) adapted a DEA formulation to account for differences in the operational environment of the field units, and included service quality, and effectiveness measures alongside traditional efficiency measures; and (c) created from scratch data collection (service quality and outcome achievement survey instruments) and report generation tools necessary for the deployment of evaluation results to the field in an user-friendly format for managers. While the suitability of DEA for real-life performance measurement is demonstrated, challenges of a DEA implementation are also discussed.	data envelopment analysis;strategic management	Alexandra Medina-Borja;Kalyan S. Pasupathy;Konstantinos P. Triantis	2007	JORS	10.1057/palgrave.jors.2602200	project management;performance management;simulation;strategic planning;competitive intelligence;economics;marketing;operations management;data envelopment analysis;management;operations research;information technology	HPC	-68.041958632051	7.655683336519516	58733
eb8a784231d8a97e06aefa901f09b979092047e5	inner source--adopting open source development practices in organizations: a tutorial	software;inner source;standards organizations;open source development;software houses public domain software;companies;software engineering;info eu repo semantics article;key factors;medical services;software industry inner source adoption open source development practices software organization;open source software software development software engineering standards organizations;software development;communities;adoption;open source development practices;open source open source development inner source software engineering software development;open source software;open source;tutorial	Inner source, the adoption and tailoring of open source development practices in organizations, is receiving increased interest. However, although it offers numerous benefits, many practitioners are unclear about what it is and how to adopt it. When adopting inner source, organizations should consider nine factors pertaining to product, process, and organization. A description of three inner-source initiatives illustrates these nine factors.	open-source software	Klaas-Jan Stol;Bob Fitzgerald	2015	IEEE Software	10.1109/MS.2014.77	computer science;systems engineering;engineering;knowledge management;software development;software engineering	SE	-72.76267307461445	16.98354883971666	58762
22b9f651182637862fe97ae2ac87a8901a89f188	method to reduce the gap between construction and it companies to improve suitability before selecting an enterprise system	selection factor;perception gap;suitability;realizability;enterprise system;importance	This study proposes a method to reduce the perception gap between the system required by construction companies and the system proposed by information technology (IT) companies during the negotiation process before selecting an enterprise system. The goal of the proposed method is to increase the developed system’s suitability to a construction company. First, a set of selection factors was determined and defined. The gap between construction companies’ requirements and the IT companies’ offerings was then analyzed. Then a method to reduce the gap to improve suitability before selecting the system was described. The applicability of the method was tested by comparing selected and non-selected cases from two previous projects. The analysis result showed that the method is highly applicable to both construction companies and IT companies. Construction companies can apply the method to select a preferred system on the basis of suitability; and during the negotiation process before system selection, IT companies can apply the method to analyze whether a system satisfies the specific requirements of a construction company. The set of defined selection factors is also applicable to the evaluation criteria of the enterprise system. © 2016 Elsevier B.V. All rights reserved.	enterprise system;requirement	Chijoo Lee;Chiheon Lee	2017	Computers in Industry	10.1016/j.compind.2016.12.005	enterprise system;computer science;systems engineering;engineering;knowledge management;marketing;operations management;suitability analysis;database	SE	-77.9611269077597	9.652395543153517	58819
71ab308fecab619aea0a80c1021d9f8d042dc340	new opportunities in marketing data mining	data mining	INTRODUCTION Data mining has been widely applied in many areas over the past two decades. In marketing, many firms collect large amount of customer data to understand their needs and predict their future behavior. This chapter discusses some of the key data mining problems in marketing and provides solutions and research opportunities.	data mining	Victor Lo	2009			engineering;marketing research;data science;marketing;data mining	ML	-71.48031403195778	5.760383050518046	58857
0b9b8a8d7a622c41403240184cb99f1b4200acdc	software licensing: a classification and case study	conference item;software control;licenses computer science software measurement computer crime costs computer industry terminology legged locomotion software testing law;industrial property;software control software licensing software distribution;software licensing;software distribution	Software licensing schemes are controls put in software to grant or deny the use of the software. It plays an important part in the distribution and the control of software. This paper reviews some of the technologies behind software licensing schemes and presents a classification and a case study.	password cracking;software house;software license	Sathiamoorthy Manoharan;Jesse Wu	2007	First International Conference on the Digital Society (ICDS'07)	10.1109/ICDS.2007.38	software distribution;personal software process;medical software;long-term support;verification and validation;software sizing;computer science;package development process;backporting;social software engineering;component-based software engineering;software development;software design description;software construction;software walkthrough;software analytics;software deployment;computer security;software requirements;software quality;software system;software peer review	SE	-67.39220721982143	27.670411941531928	58977
cd8806bf318e25e565f3e36350237debbe7f6701	case implementation: the importance of multiple perspectives	multiple perspectives	Implementation histories of four companies were analyzed to identify organizational actions increasing the likelihood of successfully implementing CASE tools. It is concluded that management must actively implement the tool and cannot assume key individuals trained to use it will promote its use. Additionally, it is found tha~ although a CASE tool may be successfully implemented when organizational measures are use& it may be a failure when individual outcomes are measured.	computer-aided software engineering	Judy L. Wynekoop;James A. Senn	1992		10.1145/144001.144033	management science;knowledge management;engineering	Security	-70.38965003864358	19.119503623797637	59020
b16ab7f6251d558ec58d816160169abfe3e3bd73	predicting software development errors using software complexity metrics	domain model;developpement logiciel;prediccion;complexite;complexity metrics;erreur;software complexity;complejidad;metric;regression model;ingenieria logiciel;complexity;software engineering;lines of code;desarrollo logicial;software development;orthogonal complexity domains errors prediction program size program volume software development errors software complexity metrics program error measures factor analysis empirical data regression models regression analysis complexity factors orthogonal measures program structure predictive models;genie logiciel;metrico;regression analysis;programming computer errors software metrics predictive models software measurement testing computer science software reliability parameter estimation data analysis;prediction model;error;quality control;software reliability;prediction;metrique;software reliability quality control	Predictive models that incorporate a functional relationship of program error measures with software complexity metrics and metrics based on factor analysis of empirical data are developed. Specific techniques for assessing regression models are presented for analyzing these models. Within the framework of regression analysis, the authors examine two separate means of exploring the connection between complexity and errors. First, the regression models are formed from the raw complexity metrics. Essentially, these models confirm a known relationship between program lines of code and program errors. The second methodology involves the regression of complexity factor measures and measures of errors. These complexity factors are orthogonal measures of complexity from an underlying complexity domain model. From this more global perspective, it is believed that there is a relationship between program errors and complexity domains of program structure and size (volume). Further, the strength of this relationship suggests that predictive models are indeed possible for the determination of program errors from these orthogonal complexity domains. >	programming complexity;software development	Taghi M. Khoshgoftaar;John C. Munson	1990	IEEE Journal on Selected Areas in Communications	10.1109/49.46879	halstead complexity measures;computer science;theoretical computer science;regression analysis;statistics	Mobile	-65.97124994675568	31.213476092207568	59034
75cf4d4f585fcb03c71291292aa79d0f7bf421f0	evolving an industrial analytics product line architecture	extensibility;reusability;industrial analytics;performance;knowledge;asset health;interoperability;software product line	This paper focuses on an industrial experience with software product lines of analytics-enabled solutions, specifically the evolution of the software product line architecture for a Subject Matter Expert Workbench toolset which supports analytic plugins for multiple software product lines. As context, the toolset product line was intended for integration of expert knowledge into a family of industrial asset health applications at runtime. The toolset architecture is now being evolved to build and manage plugins for multiple Industrial Analytics solutions (software systems and services) beyond asset health. This evolution is driving changes in the desired architecture qualities of the toolset; widening the stakeholder pool and influencing priorities; affecting the architecture tradeoffs and decisions; and triggering updates to the product line architecture, the guidance for applying it, and the current prototype of the toolset. We describe our experiences in handling this evolution, assess lessons learned, and discuss potential relevance to other product line scenarios.	industrial robot;plug-in (computing);programming tool;prototype;relevance;run time (program lifecycle phase);software product line;software system;workbench	Karen Smiley;Werner Schmidt;Aldo Dagnino	2015		10.1145/2791060.2791106	reference architecture;interoperability;reusability;simulation;extensibility;performance;computer science;systems engineering;engineering;operations management;operating system;software engineering;knowledge	SE	-67.33122902614743	18.808799397934937	59073
0817830434cb2fc2c6548837754b20318699a4f3	growth of newcomer competence: challenges of globalization	cultural difference;theoretical framework;best practice;universal repository;communication complexity;software engineering;recommending system;project competence;recommender system;culture difference;learning trajectory;communication;open source	The transfer of entire projects to offshore locations, the aging and renewal of core developers in legacy products, the recruiting in fast growing Internet companies, and the participation in open source projects, present similar challenges of rapidly increasing newcomer competence in software projects. In particular, culture differences, communication complexity, and the rapid influx of developers with little or no project knowledge common in these phenomena pose practical and research questions for software engineering. For example, how do different cultures impact project learning? Are there best practices for competence-enhancing communication? How to learn from the experiences of top developers to improve the training of newcomers? What resources and tools can be provided to help newcomers learn faster and become more productive? These questions sketch a project-learning-focused agenda needed to address outlined challenges. We propose how emerging measurement methods utilizing rich data in software repositories and the theoretical frameworks based on cognitive and organizational science may be applied to address these challenges and to improve understanding of how humans learn.	best practice;communication complexity;experience;open-source software;software engineering;software repository	Minghui Zhou;Audris Mockus	2010		10.1145/1882362.1882452	simulation;computer science;systems engineering;engineering;knowledge management;software engineering;communication complexity;management;best practice;recommender system	SE	-73.51757499825348	19.342961127093456	59102
99cf479fa87dcf94aacc8a1034b3bb66928748b6	normasearch: a big data application for financial services		In the recent years banking and financial markets are trying to learn how Big Data can help to transform their processes and organizations, improving customer intelligence, reducing risks, and meeting regulatory objectives. The collection and the analysis of new legislations, understanding if they are introducing new aspects with potential impacts on different fields, could be the basis of a system able to give support in the strategic decision making process and to evaluate the potential impacts on both management and strategic activities. Here we want to present NormaSearch, a Big Data application developed by Exprivia, an international leading company in Italy in the process consulting, technology services and information technology solutions. NormaSearch is able to analyse specifical information taken from the web, both in a structured and unstructured form, and its application in the financial fields.	big data;customer intelligence	Ylenia Maruccia;Giovanni Pansini;Gloria Polimeno;Felice Vitulano	2016			market data;big data;services computing;finance;business;marketing;financial services	HCI	-71.68069227762957	5.656878840871283	59250
14b6308599c4de02167adfac05cf53fae80aa8e9	system analysis and modeling : language profiles : 5th international workshop, sam 2006, kaiserslautern, germany, may 31 - june 2, 2006 : revised selected papers	modeling language	Language Profiles.- A UML Profile for Communicating Systems.- Implementing the eODL Graphical Representation.- Distributed Real-Time Behavioral Requirements Modeling Using Extended UML/SPT.- Formal Operations for SDL Language Profiles.- Evolution of Development Languages.- Automating Scenario Merging.- Timed High-Level Message Sequence Charts for Real-Time System Design.- Timed Use Case Maps.- Model-Driven Development.- Application of Stuck-Free Conformance to Service-Role Composition.- A Simulator Interconnection Framework for the Accurate Performance Simulation of SDL Models.- Refactoring and Metrics for TTCN-3 Test Suites.- SDL Design of a Radio Resource Control Protocol for 3G Evolution Systems with Two Different Approaches.- Language Implementation.- Evaluation of Development Tools for Domain-Specific Modeling Languages.- Towards Integrated Tool Support for the User Requirements Notation.- ConTraST - A Configurable SDL Transpiler and Runtime Environment.		Sam;Reinhard Gotzhein;Rick Reed	2006		10.1007/11951148	computer science;mathematics;modeling language;programming language	Arch	-88.89119249903317	27.962317161819573	59326
aa177a3862ab5392d1e50982fbfdb31283df25b8	developing a decision-making framework for web service security profiles: a design-science paradigm	analytic hierarchy process;analytical hierarchy processing;bepress selected works;research paradigm;web service security profile design science decision making ahp analytical hierarchy processing framework;web service;ahp;security profile;laboratory experiment;decision making framework;web services security;design science;framework	In this paper, we provide a novel analytical hierarchy process decision-making framework for Web service security profiles. This framework aids in solving the dilemma of which Web service security profile is most fitting in a particular situation. This is because the developed framework allows architects and developers to take informed decisions following a systematic and manageable approach. In developing the framework, we followed the design-science research paradigm within which we incorporated a number of laboratory experiments.	analytical hierarchy;experiment;programming paradigm;web service	Bachar Alrouh;Mutaz M. Al-Debei;George Ghinea	2010		10.1145/1874590.1874618	web modeling;sherwood applied business security architecture;computer science;systems engineering;knowledge management;data mining	Web+IR	-65.2863986673695	14.406266211892193	59357
3aa9afaf3d3e5ac0bc477ae71c2f15b1332545e1	assessing and evaluating value and cost effectiveness of e-government initiatives: focusing the step of the financial evaluation	net present value;real options;value of e government;financial evaluation	We propose a framework to assess the value and cost effectiveness of E-Government; the instrument supports lead and management of E-Government initiatives and programs. Furthermore it takes different stakeholder perspectives into account. The process for analysis consists of four steps: with an initial measurement framework we raise context and goals of the specific E-Government decision making. In a second step a financial evaluation takes place and in a third one a qualitative analysis completes the data collection process. In the last step of the methodology we consolidate the findings of the previous measurements and develop a line of argument respectively—when comparing different projects—we make a ranking. In this paper we focus on the elements of the financial evaluation for measuring the economic value for the society: we propose a traditional Net Present Value (NPV) methodology enhanced by real option analysis, a module on risk management and sensitivity analysis. In the closing remarks we assess chances and challenges of the financial evaluation in the E-Government field and draw a connection to the other elements of our framework.	closing (morphology);e-government;risk management	Alessia C. Neuroni;Alberto Rascon;Andreas Spichiger;Reinhard Riedl	2010			actuarial science;economics;operations management;management science	Web+IR	-77.78694848707045	10.010425226398127	59425
65674579ea3f89e0b628fa08b94f32afd5307332	personnel planning databases and modeling: a software approach	software systems;discrete time;technical report;flow simulation	Two obstacles frequently encountered by model builders in personnel planning are the lack of data on the personal dynamics of organizations and of flexible software systems for execution of models tailored to particular organizations and issues. In this paper we present the main ideas behind the INTERACTIVE FLOW SIMULATOR, a software system constructed to avoid these difficulties. A procedure for building longitudinal personnel databases is outlined. An algorithmic view of discrete time models which encompasses a wide class of personnel planning and other models is presented.	algorithm;automated planning and scheduling;data retrieval;database;software system	Luis B. Boza	1977			discrete time and continuous time;simulation;computer science;systems engineering;engineering;technical report;software construction;management science;world wide web;software system	SE	-64.7438269622443	17.44879094145137	59551
512b18354da32d0a8b31865aa50c39c57e6a6693	faldo's folly or monty's carlo	consultants;or;case studies;operations research;journal;operational research practice;application of operational research;operational research;practitioners;management;or society;management science;or insight	The Ryder Cup is arguably the most prestigious and most exciting golf tournament in the World. It is a team event contested once every 2 years between 12 golfers from Europe and 12 golfers from the United States of America. For the 12 singles matches on the final Sunday, each captain selects the order in which his players tee off. In 2008, after an eventual US victory, the sporting press was hugely critical of Nick Faldo’s (the European captain) slate selection. This article looks to explore the justification of such criticism. First, existing academic results are reviewed and, where necessary, updated for 2008. Second, using Monte Carlo simulation, we consider the scheduling of players who react differently under pressure. This simple sporting example illustrates how Monte Carlo simulation can be used to analyse a range of potential scenarios enabling better, more informed decisions. Within a business context, where a winning outcome is essential, non-OR practitioners must understand how OR techniques can be used to make better, more informed decisions. This article concludes by discussing how the Ryder Cup model, together with a related example analysing interdependent project risks, was successfully used within a consultancy environment to introduce non-OR practitioners to the theory behind and the potential of Monte Carlo simulation. OR Insight (2009) 22, 185–200. doi:10.1057/ori.2009.8	computer go uec cup;interdependence;monte carlo method;monty newborn;scheduling (computing);simulation	Stefan Sadnicki;Shilpa Shah	2009	OR Insight	10.1057/ori.2009.8	simulation;computer science;operations management;management;operations research	HCI	-65.56809562938824	5.022259902289661	59590
da15692f465e38ba2e52341cb978c3735d46e542	object-oriented design for manufacture	legislation;engineering design;information model;design process;object oriented design;design for manufacture;object oriented programming;design method;product design	Economically effective and timely product manufacture requires that appropriate decisions are taken during the design of the product. Recent years have seen very considerable growth in the study and communication of knowledge in the area of design for manufacture (DFM). At the same time there have appeared many additional pressures on the design process: rapid changes in market requirements, evolution and revolution in product and process technologies, legislated strictures and liabilities, and quality and cost issues which must increasingly be addressed earlier in the course of product design. As a consequence there is a strongly perceived need to enhance design methods in order to address the many disparate aspects of design in an efficient manner. In this paper the DFM problem is addressed within this larger context. The approach described in the paper begins with the hypothesis that the concept of manufacturability can be established as one of a number of required attributes of a part which is to be designed and manufactured. An information model is then proposed which provides the potential to allow determination of manufacturability to be made in an ongoing fashion in parallel with other design activities. The nature of the information model is shown to be in harmony with object-oriented programming environments. Finally, the information model is used to illustrate the potential to embody the large body of DFM knowledge which is already in existence but which has yet to be systematically encoded.	design for manufacturability;information model;requirement	Owen R. Fauvel	1994	J. Intelligent Manufacturing	10.1007/BF00124682	iterative design;design process;design methods;idef4;information model;computer science;systems engineering;engineering;computer-automated design;object-oriented design;design education;product design;design technology;object-oriented programming;design for manufacturability;engineering drawing;generative design;manufacturing engineering	EDA	-65.03321786071261	11.545616157501051	59591
95772cce933f41a51f8517dc4f18a477038ce5f0	the effect of culture on it diffusion: e-mail and fax in japan and the u.s	it cross cultural studies;it diffusion;japanese business;productivity;uncertainty avoidance;information richness;social presence;system use;perceived usefulness	Few cross-cultural studies have investigated how firms diffuse new information technologies IT. Still fewer have advanced a theoretical perspective on possible cultural effects. In a world moving rapidly toward corporate multinationalism, this oversight seems notable. As foreign managers locate plants and offices in the U.S. and as American managers establish foreign subsidiaries and offices abroad, it is important for these managers to know in advance as much as possible about the impact of culture on technological innovation. Japan and the U.S. are cases in point. Both have subsidiaries and actively market goods and services in the other country, far flung enterprises for which IT seems to be a natural coordinating mechanism. Yet while U.S. companies exploit the advantages of IT such as E-mail, Japanese firms do not. The Japanese, however, do utilize FAX extensively. Culture is one fruitful explanation for these differences. To examine these two markedly different cultures and the effect of these differences on technological innovation, a large Japanese airline and financial institution were chosen as representative Asian sites. The IT experiences of 209 Japanese knowledge workers are contrasted with those of 711 knowledge workers in comparable firms in the United States on certain dimensions. Using Hofstede's work on culture and social presence/information richness theory as grounding, it was hypothesized that high uncertainty avoidance in Japan and structural features of the Japanese written language could explain Japanese perceptions about new work technologies such as E-Mail and FAX. Furthermore, the theoretical conceptualization in the paper attempts to account for Japanese departures from the U.S. experience. Results from empirical tests verified many, but not all of the predicted differences between Japanese and American knowledge workers. In general, cultural effects seem to play an important role in the predisposition toward and selection of electronic communications media. Surprisingly, responses to traditional media such as face-to-face and telephone were remarkably similar between cultures.	fax	Detmar W. Straub	1994	Information Systems Research	10.1287/isre.5.1.23	productivity;social science;economics;uncertainty avoidance;artificial intelligence;marketing;operations management;management;social psychology;law;commerce	Crypto	-83.72443415724904	5.091086465778765	59694
0a040cdc89875b24b92c81e2097d5b58195a9ee2	analysis of document-mining techniques and tools for technology intelligence: discovering knowledge from technical documents	document analysis;research outputs;knowledge management;software systems;technology intelligence;data mining;intelligence operatives;technology management;document mining;external sources;km;document data;internal sources;technological threats;technological opportunities;technical documents;ti;software selection	This research proposes a method for extracting technology intelligence (TI) systematically from a large set of document data. To do this, the internal and external sources in the form of documents, which might be valuable for TI, are first identified. Then the existing techniques and software systems applicable to document analysis are examined. Finally, based on the reviews, a document-mining framework designed for TI is suggested and guidelines for software selection are proposed. The research output is expected to support intelligence operatives in finding suitable techniques and software systems for getting value from document-mining and thus facilitate effective knowledge management.		Sung Joo Lee;Letizia Mortara;Clive I. V. Kerr;Robert Phaal;David Probert	2012	IJTM	10.1504/IJTM.2012.049102	kilometer;economics;knowledge management;data science;marketing;technology management;technical documentation;data mining;management;software system	HCI	-70.91963724426951	6.035111867319996	59730
08e27e13db66388d9a8cd1286729df598955c5ed	technology forecasting using matrix map and patent clustering	statistical forecasting;vacant technology forecasting;k medoids clustering;research and development;support vector clustering;matrix map;patent clustering;europe;china;united states of america	Purpose – The purpose of this paper is to propose an objective method for technology forecasting (TF). For the construction of the proposed model, the paper aims to consider new approaches to patent mapping and clustering. In addition, the paper aims to introduce a matrix map and K-medoids clustering based on support vector clustering (KM-SVC) for vacant TF. Design/methodology/approach – TF is an important research and development (R&D) policy issue for both companies and government. Vacant TF is one of the key technological planning methods for improving the competitive power of firms and governments. In general, a forecasting process is facilitated subjectively based on the researcher’s knowledge, resulting in unstable TF performance. In this paper, the authors forecast the vacant technology areas in a given technology field by analyzing patent documents and employing the proposed matrix map and KM-SVC to forecast vacant technology areas in the management of technology (MOT). Findings – The paper examines the vacant technology areas for MOT patent documents from the USA, Europe, and China by comparing these countries in terms of technology trends in MOT and identifying the vacant technology areas by country. The matrix map provides broad vacant technology areas, whereas KM-SVC provides more specific vacant technology areas. Thus, the paper identifies the vacant technology areas of a given technology field by using the results for both the matrix map and KM-SVC. Practical implications – The authors use patent documents as objective data to develop a model for vacant TF. The paper attempts to objectively forecast the vacant technology areas in a given technology field. To verify the performance of the matrix map and KM-SVC, the authors conduct an experiment using patent documents related to MOT (the given technology field in this paper). The results suggest that the proposed forecasting model can be applied to diverse technology fields, including R&D management, technology marketing, and intellectual property management. Originality/value – Most TF models are based on qualitative and subjective methods such as Delphi. That is, there are few objective models. In this regard, this paper proposes a quantitative and objective TF model that employs patent documents as objective data and a matrix map and KM-SVC as quantitative methods.	baudot code;cluster analysis;control theory;experiment;high-level programming language;in-memory database;integrated circuit;k-medoids;mathematical model;medoid;nl (complexity);scalable video coding;semiconductor;sparse matrix;test data;test set;the matrix;touchscreen;xiii	Sunghae Jun;Sang-Sung Park;Dong-Sik Jang	2012	Industrial Management and Data Systems	10.1108/02635571211232352	economics;engineering;marketing;operations management;data mining;operations research;china	AI	-86.33788836306276	9.056375533749998	59733
8f31165b8a719924976ca82137e3b8ffd3bd9236	evidence-based timelines for agile project retrospectives - a method proposal	agile;computer and information science;natural sciences;datavetenskap datalogi;retrospective;computer science;software process;software visualization	Retrospective analysis of agile projects can support identification of issues through team reflection and may enable learning and process improvements. Basing retrospectives primarily on experiences poses a risk of memory bias as people may remember events differently, which can lead to incorrect conclusions. This bias is enhanced in project retrospectives which cover a longer period compared to iteration retrospectives. To support teams in recalling accurate and joint views of projects, we propose using an evidencebased timeline with historical data as input to project retrospectives. The proposed method was developed together with a large software development company in the telecommunications domain. This paper outlines a method for visualizing an evidence-based project timeline by illustrating aspects such as business priority, iterations and test activities. Our method complements an experience-based approach by providing objective data as a starting point for reflection and aims to support objective analysis of issues and root causes.	agile software development;experience;iteration;timeline	Elizabeth Bjarnason;Björn Regnell	2012		10.1007/978-3-642-30350-0_13	simulation;systems engineering;engineering;data mining	SE	-69.7215738193554	22.368612457648084	59772
e47d7c27f20cbd82f998bf9de78c2354275a0b6c	research on multi-layer risk management method in complex product development		The development of complex products is a complicated multidisciplinary concurrent engineering with complicated task structure and strong technical innovation. The distributed development environment and networked collaborative design model not only bring great challenges to the modeling of complex product development, but also increase the need for risk assessment of the development process. In consideration of the shortcomings that existing risk modeling methods rarely take into account that the multi-layer quality of complex product development structure, the coupling characteristic of concurrent engineering, the simulation involving too many subjective factors and not making joint evaluation for cost risk and schedule risk this paper proposes a multi-layer cost schedule joint risk assessment model based on DSM. Combining with artificial neural networks and the Cholesky factor which can join two independently distributed variables, a DSM-based cost schedule joint simulation method is designed. Finally, this paper gives an introduction to the complex product risk management platform which has been developed and deployed to a relevant enterprise.	artificial neural network;cholesky decomposition;financial risk modeling;layer (electronics);new product development;risk assessment;risk management;simulation;subject (philosophy)	Yongchao Xie;Gongzhuang Peng;Hemmg Zhang	2017	2017 IEEE 21st International Conference on Computer Supported Cooperative Work in Design (CSCWD)	10.1109/CSCWD.2017.8066698	risk management;simulation;systems engineering;knowledge management;distributed development;new product development;computer science;multidisciplinary approach;concurrent engineering;risk assessment;schedule;process modeling	DB	-63.861214573319565	13.616066147070264	59839
0a2c7e762a74ef17128821ddb38703a57d778719	customer integration in new product development - a literature review concerning the appropriateness of different customer integration methods to attain customer knowledge		In many instances, customers are seen as one of the key resources for new product development (NPD), as they often have deep product knowledge as well as experience and creativity potential gained by regular product usage. From knowledge management perspective, customers’ input to NPD is manifested in different forms of knowledge. Customers’ input to NPD typically reflects their needs and desires (need information) but may also represent suggestions describing how ideas can be transferred into marketable products (solution information), in some cases it even leads to radical innovations (leading edge information). In order to internalize customer knowledge, in theory different methods are discussed. However, little is known about these methods’ effectiveness and efficiency to transmit customers’ knowledge to firms. This research identifies a total of 15 methods with the help of a systematic literature review. By systematically analyzing these methods, we found that there are methods within which customers are involved only “passively” in NPD, as well as methods that enable a more “active” customer integration. This study exhibits that the methods which enable an active customer integration, compared to methods where customers are integrated only passively in NPD, are more suitable for attaining customer knowledge within innovation development.	assistive technology;compiler;customer knowledge;engineering design process;internet;list of toolkits;microsoft outlook for mac;new product development;norm (social);simulation;software engineering;sticky information;systematic review;word lists by frequency	Shkodran Zogaj;Ulrich Bretschneider	2012				AI	-75.05042177168544	4.807349961558412	59883
e338b463cb7cd078512793e75794ac8692c6cc25	the marketing chain in the mobile internet era	value added services;electronic commerce;marketing data processing;industry ecosystem development marketing chain mobile internet era 3g technologies mobile internet business model mobile internet economy business survival business migration cross industry alliance enterprise resources integration information flow mobile e commerce application oriented mobile internet platform mobile internet market growth industry ecosystem extension;e commerce;growth and development;biological system modeling;business model;information flow;3g mobile communication;internet;mobile communication;application oriented value added service platform mobile internet economy marketing chain cross industry alliance mobile e commerce;mobile computing;mobile internet;mobile computing 3g mobile communication electronic commerce internet marketing data processing;mobile communication internet business telecommunications biological system modeling industries educational institutions	"""With the maturity and universal application of 3G technologies, Internet business models continue to penetrate the mobile Internet business model. Mobile Internet is gradually becoming the main trend. Mobile Internet Economy will profoundly change the mode of business survival, growth and development. How to quickly find the direction of marketing efforts at the mobile Internet era, and start business """"migration"""" are very important. This article discusses the four key aspects of the marketing chain in the mobile Internet economy: cross-industry alliance in mobile Internet economy, enterprise resources integration based on information flow, different patterns of mobile e-commerce, the development of application-oriented mobile Interne platform. The combination of these four aspects will promote industry cooperation in upstream and downstream enterprises, drive the mobile Internet market growth and pull the entire industry ecosystem extension and development."""	capability maturity model;dot-com company;downstream (software development);e-commerce;ecosystem;internet;programming paradigm	Shao-Zhen Yang	2011	2011 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2011.6016917	e-commerce;business model;the internet;information flow;mobile telephony;computer science;internet presence management;mobile business development;mobile payment	ML	-74.69524683251059	5.891601488904662	60067
22750612fc69f2912317429db2edd86870826b59	using idef0/petri net for ontology-based task knowledge analysis: the case of emergency response for debris-flow	emergency response;idef0 petri net;input output;debris flow task ontology idef0 petri net;ontologies computer aided software engineering earthquakes knowledge management sustainable development hazards problem solving information analysis information management asia;task analysis;debris flow;task ontology;it adoption;petri net;sustainable development	"""In terms of the progress of the industry, the velocity and dynamic nature of the global environment, has caused serious damage and diminished our earth’s resources. A lot of effort has been spent on global sustainable development around the world, however, from a practical standpoint, the Incident or Hazard Command System (IHCS), especially emergency response for Debris-flow, is scare in Taiwan. The purpose of this study was twofold. First, it adopted the theory of task ontology to build up a three-level mediating representation for a task analysis, and the task of """"emergency response for Debris-flow"""" served as an example. Second, from the modeling standpoint, a methodology, called TTIPP, was provided to systemically analyze the process of the task and subtask in terms of the inputs, outputs, mechanisms, and controls, using IDEF0 and Petri net."""	computation;computer;freedom of information laws by country;global variable;idef0;national supercomputer centre in sweden;petri net;problem solving;requirement;task analysis;velocity (software development)	Wen-Yu Liu;Kwoting Fang	2006	Proceedings of the 39th Annual Hawaii International Conference on System Sciences (HICSS'06)	10.1109/HICSS.2006.523	input/output;simulation;computer science;knowledge management;artificial intelligence;operations management;software engineering;task analysis;database;petri net;sustainable development	Robotics	-64.30203731657143	5.646984268232851	60118
cdeeae9e91fe30bd6fc5b75b8fe9d1b9d66c7c2c	the need for an industrial engineering software library	software libraries;industrial engineering	Abstract   There is a  real  need for an Industrial Engineering Software Library. This need has been specified by previous studies and quantified by a widely distributed questionnaire. This article briefly outlines the work that has been accomplished so far towards the establishment of a clearinghouse to which industrial engineers can turn for available software. The question remaining is: How tto support and implement this project which we believe would be of great value to industrial engineers and others interested in quantitative techniques for decision making?	industrial engineering;library (computing)	Hamed Kamal Eldin	1977	Computers & Industrial Engineering	10.1016/0360-8352(77)90001-8	personal software process;software engineering process group;computer science;systems engineering;engineering;social software engineering;industrial engineering;software engineering;software construction	SE	-66.43720295621074	25.795924212654942	60120
b2125d26a7ea778734382df0f8223b67644576d9	business process innovation as an enabler of proactive value chains		Proactive value chains are an emerging business practice rooted in advanced process management and underlying technologies and organizations. The paper presents a recently inaugurated research and innovation program in manufacturing, and proposes business process innovation as an enabler of proactive value chains. Finally the paper discusses the role of business process innovation in the transformation of the manufacturing value chains.	business process;value (ethics)	Charles Møller	2015		10.1007/978-3-319-14430-6_2	knowledge management;business administration;process management	HCI	-73.07814431425453	8.326802344670488	60263
b656d8f5de09c601f67f277111f342bf8c09cc22	empirically refining a model of programmers' information-seeking behavior during software maintenance	information;software maintenance;empirical evidence	Several authors have proposed information seeking as an appropriate perspective for studying software maintenance activities. However, there is little research in the literature describing holistic information-seeking models in this context. Additionally, in the one instance where an information-seeking model has been proposed, the empirical evidence presented in support of that model is extremely limited. This paper presents a small quasi-experiment that serves to further evaluate and refine this preliminary information-seeking model. Talkaloud data, generated by two professional programmers, engaged in real software maintenance activities, was captured and then coded. This evaluation largely validated the model but also suggested several important refinements. The study, its results and its impact on the information-seeking model are discussed in this paper.	emergence;holism;implicit shape model;information seeking behavior;iteration;iterative method;programmer;psychology of programming;sfiaplus;software maintenance;xojo	Jim Buckley;Michael P. O'Brien;Norah Power	2006				SE	-73.60293564973544	22.213754968704777	60549
1ebb2e0c0bc936011a1be810116244ad4f8346c3	identifying the importance of software reuse in cocomo81, cocomoii	software cost estimation;software engineering;software project management;relative error;effort estimation;software reuse;cost model;historical data;project planning	Software project management is an interpolation of project planning, project monitoring and project termination. The substratal goals of planning are to scout for the future, to diagnose the attributes that are essentially done for the consummation of the project successfully, animate the scheduling and allocate resources for the attributes. Software cost estimation is a vital role in preeminent software project decisions such as resource allocation and bidding. This paper articulates the conventional overview of software cost estimation modus operandi available. The cost, effort estimates of software projects done by the various companies are congregated, the results are segregated with the present cost models and the MRE (Mean Relative Error) is enumerated. We have administered the historical data to COCOMO 81, COCOMOII model and identified that the stellar predicament is that no cost model gives the exact estimate of a software project. This is due to the fact that a lot of productivity factors are not contemplated in estimation process. The vital dilemma we identified is that “software reuse” is being eclipsed although most of the contemporary software projects are based on object oriented development where no component is made from scratch (Inheritance). By using the principal of software reuse the ROI (Return of Investment) is also bolstered for the companies. So further research exposure is in “software Reuse” and Reuse software cost estimation model. KeywordsReuse, Size, Effort, Cost estimation, COCOMO, MRE.	analysis of algorithms;approximation error;automated planning and scheduling;cocomo;code reuse;cost estimation in software engineering;interpolation;region of interest;scheduling (computing);software development effort estimation;software project management;stellar (payment network)	Ch. V. M. K. Hari;P. V. G. D. Prasad Reddy;J. N. V. R. Swarup Kumar;G. SriRamGanesh	2009	CoRR		project management;reliability engineering;personal software process;long-term support;approximation error;verification and validation;team software process;simulation;software sizing;software project management;systems engineering;engineering;package development process;social software engineering;software development;software design description;estimation;software engineering;software construction;project management triangle;software deployment;software development process;project planning;software metric;software peer review	SE	-68.8414203823911	23.093594683701774	60615
173d45c494aef78f82ea1b1c1994fa8e6ddfacdf	managing the impact of employee turnover on performance: the role of process conformance	standards;employee turnover;retention;governance compliance;process management;knowledge exploitation;customer focus and relationships;retail operations;service operations;business processesretail industryunited states	We examine the impact of employee turnover on operating performance in settings that require high levels of knowledge exploitation. Using 48 months of turnover data from U.S. stores of a major retail chain, we find that, on average, employee turnover is associated with decreased performance, as measured by profit margin and customer service. The effect of turnover on performance, however, is mitigated by the nature of management at the store level. The particular aspect of management on which we focus is process conformance—the extent to which managers aim to reduce variation in store operations in accordance with a set of prescribed standards for task performance. At high-process-conformance stores, managers use discipline in implementing standardized policies and procedures, whereas at low-process-conformance stores, managers tolerate deviations from these standards. We find that increasing turnover does not have a negative effect on store performance at high-process-conformance stores; at low-process-conformance stores, the negative effect of turnover is pronounced. Our results suggest that, in settings where performance depends on the repetition of known tasks, managers can reduce turnover’s effect by imposing process discipline through standard operating procedures.	computer performance;conformance testing;operating system;standard operating procedure	Zeynep Ton;Robert S. Huckman	2008	Organization Science	10.1287/orsc.1070.0294	economics;knowledge management;marketing;operations management;turnover;management;service system	Metrics	-83.04142360650306	6.779154965482315	60682
5c9ad10737a1bf3f5316530e166cc86029413915	supporting sales representatives on the move: a study of the information needs of pharmaceutical sales representatives		The purpose of the current study is to understand the nature of the challenges that sales representatives face as a result of operating within a highly mobile and heterogeneous work environment. The paper also focuses on how the sales representatives manage their information needs and discusses the properties of mobile support systems that would enable them to work effectively despite their being extensively mobile. This is achieved through a case study involving the sales representatives of a medium-sized pharmaceutical company	information needs	Chihab BenMoussa	2005			public relations;marketing;advertising;business;manufacturers' representative	Web+IR	-78.70725805533696	6.208061399069651	60821
ebd946975ee11b7396c336d9d3d97b7755ef1a70	communication issues in requirements elicitation: a content analysis of stakeholder experiences	pilot study;computacion informatica;grupo de excelencia;requirements elicitation;content analysis;research paper;ciencias basicas y experimentales;system development;communication	The gathering of stakeholder requirements comprises an early, but continuous and highly critical stage in system development. This phase in development is subject to a large degree of error, influenced by key factors rooted in communication problems. This pilot study builds upon an existing theory-based categorisation of these problems through presentation of a four-dimensional framework on communication. Its structure is validated through a content analysis of interview data, from which themes emerge, that can be assigned to the dimensional categories, highlighting any problematic areas. The paper concludes with a discussion on the utilisation of the framework for requirements elicitation exercises.	categorization;dimension 3;holism;interdependence;requirement;requirements elicitation;theme (computing)	Jane Coughlan;Mark Lycett;Robert D. Macredie	2003	Information & Software Technology	10.1016/S0950-5849(03)00032-6	content analysis;systems engineering;engineering;requirements elicitation;management science	SE	-66.39091945376875	15.184753357969857	60835
16c92fcbc3b3461796bb0eef7d534879ee74ba4d	essential elements of an sme-specific search of trusted cloud services	trust;small and medium sized enterprises smes;semantic search;cloud services;cloud computing	Cloud computing holds tremendous potential for small and medium-sized enterprises (SMEs) since it offers technologies which can improve their strategic, technical and economic situation. However, along with the benefits there are also risks which concern legal, technical and economic issues associated with cloud computing, which cause concerns to the SMEs. These concerns can be reduced by improving the transparency of cloud services including the understanding of the technology and knowledge about the service providers, already when searching for the appropriate services. To overcome these concerns and increase the transparency cloud service search systems must assist the user during the search process and provide more details about the cloud services, in particular about the attributes which influence the trust of an SME into a service and its provider. The paper introduces a solution for searching appropriate cloud services by focusing on the essential elements. A unique element in this approach is the automated monitoring and evaluation of the provided attributes of the cloud services.	cloud computing	Andrea Horch;Constantin Christmann;Holger Kett;Jürgen Falkner;Anette Weisbecker	2014		10.5220/0004844000880094	cloud computing security;cloud computing;semantic search;computer science;knowledge management;operating system;services computing	Metrics	-79.39831568914074	12.702208197647327	60907
e65a38804e3c55a6fdd3a7f28fed3c322d625204	compass - the verification and validation of an operational analysis model for use in the prediction of nimrod mra4 operational effectiveness	mathematical simulation;forecasting;optimisation;reliability;project management;information systems;simulation systeme;optimizacion;maintenance;soft or;information technology;packing;defense nationale;programmation stochastique;aeronef;weapon;operations research;location;investment;journal;journal of the operational research society;aeronave;inventory;purchasing;history of or;logistics;marketing;simulacion matematica;scheduling;national defence;simulation mathematique;arma;production;communications technology;defensa nacional;optimization;computer science;operational research;arme;stochastic programming;verification and validation;system simulation;simulacion sistema;simulation systeme aeronef patrouilleur maritime;programacion estocastica;applications of operational research;or society;jors;management science;infrastructure;aircraft	Comprehensive Maritime Patrol Aircraft Systems Simulation (COMPASS) is a mathematical model of the Nimrod MRA4 weapon system that is being developed to enable the prediction of operational effectiveness. COMPASS will be used throughout the Nimrod MRA4’s development life cycle to conduct tactics development and weapon system performance evaluation. Following Nimrod MRA4’s entry into service, the COMPASS model may be used by the customer, the Ministry of Defence (MoD) Defence Procurement Agency (DPA), to assess operational effectiveness of potential system upgrades and to evaluate the effect of changes to the threat or operating environment. BAE SYSTEMS is developing tactical scenarios to test operational performance. The operational requirement is decomposed, at the highest practicable system level, into metrics that can be measured easily under test or trials conditions. Having quantified a system’s capabilities in these lower level metrics, COMPASS can be modified to reflect them accurately. COMPASS uses these tactical scenarios to simulate the Nimrod MRA4 undertaking a number of Anti Submarine Warfare (ASW) and Anti Surface Unit Warfare (ASuW) activities. This paper describes the philosophy and process being applied to the development of a fully verified and validated COMPASS mathematical model. It also details the approach to using COMPASS to determine Nimrod MRA4 operational effectiveness through the construction of tactical scenarios. Journal of the Operational Research Society (2004) 55, 413–421. doi:10.1057/palgrave.jors.2601663	compass;mathematical model;operating environment;operations research;performance evaluation;procurement;simulation;software development process;verification and validation	J. F. M. Brennan;A. L. Denton	2004	JORS	10.1057/palgrave.jors.2601663	stochastic programming;project management;logistics;verification and validation;simulation;inventory;economics;forecasting;investment;computer science;marketing;operations management;reliability;mathematics;location;management;operations research;information technology;scheduling	SE	-64.6965569619296	5.883056836374715	61129
3cf83ece57d06eef9cd3e82b5efcadaf83a7c7b2	adding value every sprint: a case study on large-scale continuous requirements engineering		Agile development practices, such as continuous integration and continuous delivery, promise value through shorter time to market and increased flexibility. While these practices have been widely adopted in small-scale, they have shown to be challenging to adopt in large-scale, system development. This is often due to a distance between customer and developer in large scale systems, and the need to break down value from the whole system into manageable parts. The notion of value is fundamental for agile methods, especially for practices such as continuous delivery to the customer. However, how value should be handled in development practices is not clearly understood. In this paper, we investigate how the notion of adding value in every sprint has been perceived in a large-scale system development. Based on an exploratory qualitative case study, the outcome shows that it is perceived beneficial by practitioners although it comes at a price and challenges exist.	agile software development;continuous delivery;continuous integration;requirements engineering;scrum (software development);sprint (software development)	Rashidah Kasauli;Eric Knauss;Agneta Nilsson;Sara Klug	2017			sprint;requirements engineering;agile software development;continuous delivery;time to market;systems engineering;engineering	SE	-68.46526842296602	22.82213987910083	61416
4190ae43f4f1d750dd7db1867775c1102b8a4be3	realization of the neural network model of prediction of the software project characteristics for evaluating the success of its implementation	software;training;matlab neural network model software project characteristics software requirements specification srs;artificial neural networks;software management formal specification mathematics computing neural nets project management;mathematical model;neuronet model of software project characteristics prediction based on analysis of srs software requirements specification srs software project success of software project implementation software project characteristics;predictive models;neurons;biological neural networks;software artificial neural networks neurons training biological neural networks mathematical model predictive models	This paper proves the dependence of successful of software project implementation on the software requirements specification (SRS), the actuality and importance of the skill to evaluate the possible success of software project based on the specifications. The neural network model of prediction of the software project characteristics for evaluating the success of its implementation based on analysis of specifications is first time proposed and implemented in Matlab.	artificial neural network;matlab;network model;requirement;software project management;software requirements specification	Tetyana Hovorushchenko;Andriy Krasiy	2015	2015 IEEE 8th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)	10.1109/IDAACS.2015.7340756	personal software process;software requirements specification;verification and validation;software sizing;software verification;software project management;computer science;software design;social software engineering;software development;software design description;machine learning;software construction;mathematical model;predictive modelling;software deployment;software development process;software requirements;artificial neural network;software system	SE	-64.87269050118208	29.3696345089526	61439
7913e4640b8ba0fc495d36c16504b6d6f831b859	a reference model for agile quality assurance: combining agile methodologies and maturity models	organizations quality assurance capability maturity model planning training standards organizations software;software quality capability maturity model quality assurance software prototyping;quality improvement software development organizations maturity models level rating capability maturity model integration cmmi agile methodologies agileqa rm agile quality assurance reference model qa implementation model evaluation agile activities model assessments agile qa practices;cmmi quality assurance agile methodologies maturity models	The area of Quality Assurance (QA) is a challenge for many software development organizations that want to implement maturity models level rating, like Capability Maturity Model Integration (CMMI), together with agile methodologies. This study aims to present AgileQA-RM, an Agile Quality Assurance Reference Model to help organizations in QA implementation. The model proposition considers results of a systematic literature review about CMMI and agile, and a case study about QA in an organization with CMMI and agile. The methodology for model evaluation started with a check of its process areas adherence with other maturity models and with agile practices and activities. A survey based on expert opinion was also performed. Five maturity levels and eighteen process areas compose AgileQA-RM. Each process area has mandatory purpose, expected results and informative work products. Initial model assessments indicates that it can contribute to help organizations assessing its current situation about agile QA practices and supporting quality improvement through adoption of agile QA practices.	agile software development;capability maturity model integration;information;process area (cmmi);reference model;software quality assurance;systematic review	Fernando Selleri Silva;Felipe Santana Furtado Soares;Angela Lima Peres;Ivanildo Monteiro de Azevedo;Pietro Pereira Pinto;Silvio Romero de Lemos Meira	2014	2014 9th International Conference on the Quality of Information and Communications Technology	10.1109/QUATIC.2014.25	standard cmmi appraisal method for process improvement;reliability engineering;quality assurance;agile usability engineering;leancmmi;systems engineering;engineering;capability maturity model integration;process management;empirical process;service integration maturity model;lean software development;software quality control;software quality analyst	SE	-69.21573132607541	21.45518957248214	61575
4eee4e24d7c079aea1f1f07d85a01b5dce5e4ebd	plm reference model: a preliminary proposal for reference model evolution		The adoption of product lifecycle management (PLM) business approach requires the implementation of PLM systems. Currently there is a wide variety of PLM systems in the market, but the scope differences between existing systems hamper the selection process for manufacturing companies. This paper presents a reference model for PLM systems and discusses a preliminary proposal for the evolution of this model. The main expected applications for the model are: serve as the basis for the comparison of different commercial systems on selection processes at the industry; contribute to the definition of the theoretical scope of PLM systems; serve as reference for software companies to prioritize the inclusion of new features in their systems.	benchmark (computing);evolution;reference model	Eduardo de Senzi Zancul	2012		10.1007/978-3-642-35758-9_47	systems engineering;engineering;knowledge management;operations management	DB	-70.32595355342869	12.53208171889178	61620
8e6770bf9bcb3084e725b8c75cd5202183a06a7d	business model aproach for qoe optimized service delivery	law;business;end user potential market qoe b2b qoe model;europe;business europe telecommunications education broadband communication law sociology;broadband communication;sociology;telecommunications	Current B2B (Business to Business) models are not capable to cover neither customer expectations in terms of quality nor personalization. Network, service and equipment providers are tied to traditional business models, missing the opportunity to increase their revenues derived from the integration of Quality of Experience (QoE) models in their frameworks. In this work, we propose a B2B QoE model which comprises the main guidelines to successfully integrate the QoE within the value chain and provide with added-value services to potential subscribers. We also evaluate the potential QoE end-user market for six European countries. Results indicate that there is a niche for QoE based models which rely on the joint action of value chain actors and its agreement with the regulatory environment.	affinity analysis;encapsulated postscript;itil;niche blogging;personalization;processor affinity;regulation a	José I. Aznar;Eduardo Viruete;Julián Fernández-Navajas;José Ruíz;Jose Saldana;Luis Casadesus	2011	Proceedings of the International Conference on e-Business		marketing;advertising;law;commerce	HPC	-76.69885415876972	6.803909668368307	61740
abe8b4bbb1b7c39e22420339bdb06c83b16b98f7	a measure of cross-training benefit versus job skill specialization	workforce assessment;efficiency;cost benefit;cross training;similarity function	Given ever-higher labor costs, organizations should periodically assess the match of personnel skills and quantities with required duties. Consolidating similar functional specialties can improve efficiency by increasing staffing for high-demand jobs, or by identifying areas where staff may be reduced. However, such consolidation activities are often done anecdotally, and can potentially overlook successful skill pairings. We propose a model that enables an objective, repeatable skills consolidation assessment process. Our model—a cost/benefit ratio—identifies skill pairings which are likely to merge successfully, by comparing the costs of training to the benefits of increased staffing level efficiencies for these jobs. Published by Elsevier Ltd.	coefficient;hoc (programming language);job stream;partial template specialization;semiconductor consolidation	Kenneth A. Marentette;Alan W. Johnson;Lisa Mills	2009	Computers & Industrial Engineering	10.1016/j.cie.2009.03.010	simulation;engineering;knowledge management;cost–benefit analysis;operations management;efficiency;management	Web+IR	-87.35850547503462	7.475795122073924	61745
71a7e13c2c0acdfeea05bc2cebd4e49b276485ce	using big data strategy for the development of the communication industry		This study is being discussed in order to strengthen and secure communications services revenues as a strategy for survival among fierce competition in the telecom services market. Telecommunications companies are experienced in connecting a variety of information, as well as providing various types of data storage. Due to the advent of Big Data era, analysis and utilization of data has emerged as an important issue. Therefore, telecommunications companies have to develop new service and achieve excellence for consumers by utilizing internal data they collect so they can increase profit. In the past, companies underwent internal utilization, but utilizing data sets in the external phase has recently begun. Ultimately, revenue should be obtained through the data and a variety of analyses by providing services directly to consumers. The carriers’ revenue growth factor leads to a reduction in call charges for consumers. At the same time, Big Data can lead to market growth.	big data	Soonduck Yoo;Jungihl Kim;Kwangsun Ryu	2014		10.1007/978-3-319-11538-2_32	data science	Robotics	-72.77098352663246	5.389597328132086	61799
0c296c791770b341614c718e72d7c298b9dcdac8	patterns for technology companies		1 Abstract This paper builds on the author's earlier work (Kelly, 2005a, Kelly, 2005b) by adding two more business strategy patterns that describe common strategies used by technology companies; specifically software companies but the strategies should be extensible to other technology sectors. These patterns are:	kelly criterion;software industry;strategic management	Allan Kelly	2006			knowledge management;systems engineering;computer science	SE	-73.07287868078184	9.420085034963563	61812
5c725c21789ab629837cc7ecee83aa25a752e97e	a review of scheduling problems and research opportunities in motion picture exhibition	forecasting;decision support;relationships of or with other disciplines;motion pictures;resources;allocation;scheduling;or practice;optimization;management;entertainment;applications	New opportunities for operational efficiency in movie exhibition exist as a result of recent developments in the industry, such as the mass-scale conversion to digital cinema, the explosion of customer data sources, and the availability of new channels for watching movies. This paper provides an industry overview and a review of existing research on forecasting and scheduling problems in movie exhibition. The authors identify opportunities for academic research in a digital era of movie exhibition and provide context for practical applications in the industry.	schedule (project management)	Katherine Goff Inglis;Saeed Zolfaghari	2017	Interfaces	10.1287/inte.2016.0864	entertainment;simulation;decision support system;economics;forecasting;computer science;engineering;artificial intelligence;marketing;multimedia;advertising;management;operations research;information technology;scheduling;resource	Robotics	-71.20049442443822	5.7364437264771455	61896
b8af5281f43a805817a032a8963c7b2c5a5ef4d4	requirements change: what's the alternative?	software;gallium software ieee computer society press artificial neural networks complexity theory australia software engineering;user needs;complexity theory;software risks;requirements management;software defect density software project cost software scheduling;requirements change;defect density;software defect density;software engineering;requirements engineering;artificial neural networks;requirements volatility;system recovery;requirement engineering;ieee computer society press;software scheduling;software risks requirements engineering requirements management user needs requirements change requirements volatility;conference proceeding;australia;gallium;software project cost	Numerous studies have shown that a software projectpsilas cost, schedule and defect density escalate as the rate of requirements change increases. Yet none of these studies have explored the effects of not making requirements changes in response to changes in user needs. This paper explains why a project incurs just as much, if not more, risk when requirements changes are suppressed.	requirement;software bug	Alan M. Davis;Nur Nurmuliani;Sooyong Park;Didar Zowghi	2008	2008 32nd Annual IEEE International Computer Software and Applications Conference	10.1109/COMPSAC.2008.216	reliability engineering;requirements management;computer science;systems engineering;engineering;software engineering;requirements engineering;management;gallium	SE	-66.16379543742838	28.366535790785633	61986
f962d5320820da8152cfe56aede4f6f0fbf0c62b	when is crowdsourcing advantageous? the case of crowdsourced software testing	business studies;software testing;other research area;economics;crowdsourcing	Crowdsourcing describes a novel mode of value creation in which organizations broadcast tasks that have been previously performed in-house to a large magnitude of Internet users that perform these tasks. Although the concept has gained maturity and has proven to be an alternative way of problem-solving, an organizational cost-benefit perspective has largely been neglected by existing research. More specifically, it remains unclear when crowdsourcing is advantageous in comparison to alternative governance structures such as in-house production. Drawing on crowdsourcing literature and transaction action cost theory, we present two case studies from the domain of crowdsourced software testing. We systematically analyze two organizations that applied crowdtesting to test a mobile application. As both organizations tested the application via crowdtesting and their traditional in-house testing, we are able to relate the effectiveness of crowdtesting and the associated costs to the effectiveness and costs of in-house testing. We find that crowdtesting is comparable in terms of testing quality and costs, but provides large advantages in terms of speed, heterogeneity of testers and user feedback as added value. We contribute to the crowdsourcing literature by providing first empirical evidence about the instances in which crowdsourcing is an advantageous way of problem solving.	crowdsourcing;software testing	Niklas Leicht;Nicolas Knop;Christoph Müller-Bloch;Jan Marco Leimeister	2016			economics;crowdsourcing software development;computer science;knowledge management;data science;marketing;software engineering;data mining;database;software testing;world wide web;crowdsourcing;business studies	SE	-79.21516261657843	6.3792776149779735	62058
2dfd26dc19db5eb67c18355e358e54bbf8cf4e53	exchanging preliminary information in concurrent engineering: alternative coordination strategies	bepress selected works;information processing;preliminary information concurrent engineering communication coordination problem solving strategies product development information processing;communication;concurrent engineering;preliminary information;coordination;problem solving strategies;product development	Successful application of concurrent development processes (concurrent engineering) requires tight coordination. To speed development, tasks often proceed in parallel by relying on preliminary information from other tasks, information that has not yet been finalized. This frequently causes substantial rework using as much as 50% of total engineering capacity. Previous studies have either described coordination as a complex social process, or have focused on the frequency, but not the content, of information exchanges. Through extensive fieldwork in a high-end German automotive manufacturer, we develop a framework of preliminary information that distinguishes information precision and information stability. Information precision refers to the accuracy of the information exchanged. Information stability defines the likelihood of changing a piece of information later in the process. This definition of preliminary information allows us to develop a time-dependent model for managing interdependent tasks, producing two alternative strategies: iterative and setbased coordination. We discuss the trade-offs in choosing a coordination strategy and how they change over time. This allows an organization to match its problem-solving strategy with the interdependence it faces. Set-based coordination requires an absence of ambiguity, and should be emphasized if either starvation costs or the cost of pursuing multiple design alternatives in parallel are low. Iterative coordination should be emphasized if the downstream task faces ambiguity, or if starvation costs are high and iteration (rework) costs are low. (Preliminary Information; Concurrent Engineering; Communication; Coordination; Problem-Solving Strategies; Product Development; Information Processing) Introduction Concurrent engineering, the practice of executing coupled development activities in parallel, has become the common mode of product development as time-to-market has gained in importance over the last 15 years (Takeuchi and Nonaka 1986, Wheelwright and Clark 1992, Krishnan and Ulrich 2001). Given tight project schedules, many engineers cannot afford to wait until all required information input is available, and have to start “in the dark,” requiring close coordination with other interdependent activities. Coordination among tightly coupled (interdependent) and parallel tasks forces parallel teams to share preliminary information about work in progress. Production tool orders have to be based on rough sketches of product designs, product concepts must be developed while uncertainty remains about the customer’s needs, and components must be specified while interacting systems are still under development. This kind of coordination often proceeds in an informal, ad hoc manner. It is hard to tell if the right information is being shared at the right time, as in the place of factual data (“the total vehicle mass is 3,126 pounds”); there is only vague preliminary data (“at present, we expect the vehicle mass to be around 3,000 pounds”). As one automotive executive put it, “Designing a car is much like building a house: you cannot afford to suspend the kitchen planning until you have put up the walls. But, if you start the kitchen planning too early, using preliminary floor plans from the architect, you are likely to do it twice. [. . .] We need a new way of exchanging information between the architect and the kitchen planner. Currently, our kitchen planner’s idea of concurrent engineering is that they should receive the floor plans as they did in the past, just six months earlier. They don’t understand that the nature of the information has changed!” C. TERWIESCH, C. H. LOCH, AND A. DE MEYER Exchanging Preliminary Information in Concurrent Engineering ORGANIZATION SCIENCE/Vol. 13, No. 4, July–August 2002 403 This highlights the two fundamental coordination problems addressed in this article. First, the uncertainty facing the kitchen planner with the floor plan may not necessarily arise from technologies or markets, but may be a consequence of the project manager’s decision to overlap (execute in parallel) two sequentially dependent activities. But how can the architect (upstream) inform the kitchen planner (downstream) that the information is only preliminary in nature? Second, we need to understand how the downstream party should use the preliminary information. If treated as final information, it is likely to lead to costly rework (if you plan the kitchen twice, you order a large appliance but end up not having the space to put it in). At the other extreme, not releasing any information until it has “converged” basically holds up the kitchen planning until the walls are up—an approach which avoids rework but sacrifices any time gains from parallel task execution. Coordination strategies outlined in the existing organizational literature have primarily focused on finding appropriate organizational structures to respond to uncertainty and interdependencies (Brown and Eisenhardt 1995). However, as most of these models have been static in nature (Adler 1995), they cannot fully capture the concept of concurrency, which is by definition time dependent. Prior studies have also left the concept of preliminary information itself undefined, despite numerous recommendations to define it (e.g., Clark and Fujimoto 1991). In the case of the kitchen planner, the existing literature would recommend forming a cross-functional team and engaging in frequent information exchanges. Although an important first step in dealing with uncertainty and interdependence, this fails to answer the fundamental question of what to communicate. The key questions when coordinating concurrent tasks are not how often to exchange information, but rather what information to exchange at what moment in time, and how to react to it. Moreover, preliminary information exchange, which results from the combination of interdependence and concurrency, is a time-dependent construct which is gradually finalized as upstream advances in its problem solving (or, if the uncertainty is caused by external events, as these events unfold). A model addressing this issue must be dynamic in nature. Theoretically our work extends a classical line of research on information exchange, uncertainty, and interdependence (e.g. Thompson 1967, Galbraith 1973) which has frequently been used as a theoretical foundation for the literature in the emerging field of new product development, such as Clark and Fujimoto (1991), Sobek et al. (1999), Krishnan et al. (1997), and Loch and Terwiesch (1998). More recently, detailed empirical studies of new product development projects have not only applied existing organizational theories, but successfully extended them (e.g. Adler 1995, Staudenmeyer 1999, Eisenhardt and Tabrizi 1995). In this article we present a qualitative study of an engineering project facing several situations of interdependence and concurrency, thus heavily dependent on preliminary information exchange. Using data collected from 10 engineering decisions traced on-site in a vehicle development project, we develop a dynamic model of coordination that hinges on the concept of preliminary information exchange. We study this exchange from three perspectives: that of the information-providing party, the information-receiving party, and of the system designer, as is reflected in our three research questions: •How does the information provider transmit preliminary information, and how is it revised over time? •How do downstream activities adjust to changes in the preliminary information received? •What trade-offs are relevant for downstream when using preliminary information, and specifically, can preliminary information be traded off against budget, time, or system performance? Based on these perspectives, we present a time-dependent model for coordinating interdependent tasks which in turn helps to define two alternative coordination strategies that we label iterative and set based. Second, we address the trade-offs faced by a development team in choosing a coordination strategy and how they change over time. This allows an organization to match its problem-solving strategy with the interdependence it faces. Relying on preliminary information too early can lead to rework in the form of costly iterations. Conversely, waiting for information to reach a desired level of certainty foregoes the time gains that come with parallel problem solving. We start by clarifying the theoretical problem of coordinating concurrent interdependent tasks and how it relates to existing literature in organizational theory and new product development. Secondly, we present our research methodology and a detailed description of the engineering decisions requiring coordination. Building on on-site observations, the three perspectives of preliminary information are presented. From this we derive the two coordination strategies and identify the managerial tradeoffs involved in choosing between them. Theoretical Problem and Literature Background Consider a very simple example of a development process where two activities are overlapped to reduce deC. TERWIESCH, C. H. LOCH, AND A. DE MEYER Exchanging Preliminary Information in Concurrent Engineering 404 ORGANIZATION SCIENCE/Vol. 13, No. 4, July–August 2002 Figure 1 Coordinating Parallel Activities Requires the Use of Preliminary Information velopment leadtime. The overlap allows the informationabsorbing downstream operation (e.g., stamping die development) to start before the information-supplying upstream activity (e.g., product design) is completed, thereby potentially reducing the overall cycle time due to concurrency benefits. In a fully sequential process (Figure 1, left), no information is released to downstream until upstream has gained full knowledge of its task. When downstream finally starts, it can rely on finalized information from upstream. This is symbolized by a formal release milestone in the p	concurrency (computer science);die (integrated circuit);downstream (software development);field research;hoc (programming language);information exchange;information processing;interaction;interdependence;iteration;mathematical model;new product development;problem solving;rework (electronics);schedule (computer science);software appliance;systems design;theory;undefined behavior;upstream (software development);vagueness	Christian Terwiesch;Christoph H. Loch;Arnoud De Meyer	2002	Organization Science	10.1287/orsc.13.4.402.2948	simulation;information processing;computer science;operations management;management science;management;new product development;concurrent engineering	AI	-64.2480223365712	14.035209526965692	62139
77359daa6f297d7920e44367396c7fd752b2f15b	the design, implementation and analysis of test experiments	design for testability;sample size;data collection;statistical testing design for testability;circuit testing logic testing circuit simulation circuit faults delay effects image analysis costs profitability customer satisfaction virtual manufacturing;test experiments analysis test design test implementation;statistical testing	This paper discusses aspects which have to be considered in order to conduct a successful test experiment. Emphasis is on using data collected from testing large numbers of parts. Statistical arguments are presented relating to making meaningful conclusions, which includes the effects of sample size. Several pitfalls that can corrupt or skew data are discussed, and the importance of unambiguous presentation of data and results is stressed	experiment	Peter C. Maxwell	2006	2006 IEEE International Test Conference	10.1109/TEST.2006.297735	non-regression testing;test strategy;sample size determination;keyword-driven testing;reliability engineering;statistical hypothesis testing;electronic engineering;simulation;orthogonal array testing;white-box testing;manual testing;integration testing;computer science;systems engineering;automatic test pattern generation;design for testing;session-based testing;test script;test management approach;statistics;data collection	Robotics	-64.30011373123436	30.810831835159817	62196
19ba9707212be294f37e9bedfa53086542c02863	a survey of scientific software development	verification;verification activities;scientific software development;scientists;software engineering;scientific researches;conference paper;scientific softwares;software development;keywords development practices;tool use;software design scientific software;version control;scientific software;survey;scientific research;surveys	Software for scientific research purposes has received increased attention in recent years. Case studies have noted development practices, limitations, and problems in the development of scientific software. However, applicability of the results of these studies to improving the wider scientific software development practices is not known. This paper presents a survey of 60 scientific software developers. The survey was conducted online from August--September 2009, and aims to identify where improvements to scientific software practices can be made. While our results generally confirm previous work, we have found some notable differences. The use of IDEs and version control tools among the surveyed scientific software developers has increased, and trace-ability of scientific software is not as important to scientific software developers as it is to scientific software users. Documentation also appears to be more widely produced than previous studies indicate. However, there remains room for improvement in tool use, documentation, testing, and verification activities for scientific software development.	documentation;integrated development environment;software developer;software development;user (computing);version control	Luke Nguyen-Hoan;Shayne Flint;Ramesh Sankaranarayana	2010		10.1145/1852786.1852802	verification;scientific method;software engineering process group;crowdsourcing software development;computer science;revision control;systems engineering;engineering;software development;software engineering;software walkthrough;software documentation;software peer review	SE	-65.89984128612528	25.48803249237868	62246
13d58b1a68ed54e17b70c2b1cae1d5244c5fbfba	software process	software development;social fac tors in software development;empirical studies;software development environments;software process;agile software de velopment	This paper is a travelogue of Software Process research and practice in the past 15 years. It is based on the paper written by one of the authors for the FOSE Track at ICSE 2000. Since then, the landscape of Software Process research has significantly evolved: technological breakthroughs and market disruptions have defined new and complex challenges for Software Engineering researchers and practitioners.   In this paper we provide an overview of the current status of research and practice, highlight new challenges, and provide a non-exhaustive list of research issues that, in our view, need to be tackled by future research work.	computer science;holism;icse;mechatronics;openness;relevance;requirement;software development process;software engineering;software industry;software system	Alfonso Fuggetta;Elisabetta Di Nitto	2014		10.1145/2593882.2593883	personal software process;verification and validation;team software process;software engineering process group;computer science;systems engineering;engineering;package development process;software evolution;social software engineering;software development;software design description;software engineering;software construction;development environment;management science;software walkthrough;empirical process;software analytics;empirical research;lean software development;software deployment;goal-driven software development process;software development process;software peer review	SE	-66.31599949771575	23.62879733227573	62382
800b053ec459446de924c676271c8391d9c1a7a3	a method to realize traceability in development processes	reuse;software lifecycle;automotive spice;functional safety;traceability	"""The SoQrates Working Group \""""Traces\"""" develops methods for documenting a continuous flow of information and takes into account the transparency of the activities of the different disciplines. Particularly, the level of detail, accuracy, and completeness of the interfaces are considered. These methods consider also the required traceability demanded by different standards. By using the developed methods, a platform development with reusable components is possible. In combination with an efficient variant management, existing components can be combined to new customer products in a very short time design to market. The closed traceability approach increases quality and safety veritably without additional resources. This paper describes the current results of the SoQrates Working Group \""""Traces\"""" as a method description with best practice examples of the participating companies and research institutions."""		Rainer Dreves;Frank Hällmayer;Lutz Haunert;Bernhard Sechser;Armin Rieß	2016	Journal of Software: Evolution and Process	10.1002/smr.1828	reliability engineering;embedded system;traceability;systems engineering;engineering;software engineering;reverse semantic traceability;reuse;iso/iec 15504;functional safety;software development process;requirements traceability	SE	-62.85544262592928	21.21406644260027	62437
123ab4734e854637ca64225ef4c33632e6687c0d	visualization of software assurance information	data visualization standards market research software quality space vehicles image color analysis;information visualization software assurance;information visualization;software engineering;software assurance data software assurance information software development project information visualization techniques;software assurance;software engineering program visualisation;program visualisation	During the conduct of Software Assurance on a software development project, data is gathered on both the software being developed, and the development processes being followed. It is from this information that Software Assurance derives insights into the quality of the software itself and the efficacy of the development process. For large software developments such data can be voluminous, making deriving and conveying insights challenging. This motivates our ongoing efforts to apply information visualization techniques to software assurance data. While visualization techniques have long been applied to software itself, the application to software development processes and the data they yield is relatively novel. We report on several such applications and the insights they revealed. We offer some suggestions for the further investigation of information visualization techniques applied to assurance data.	information visualization;microsoft software assurance;software development process	Martin S. Feather;Joel M. Wilf	2013	2013 46th Hawaii International Conference on System Sciences	10.1109/HICSS.2013.601	software security assurance;software visualization;personal software process;verification and validation;information visualization;software sizing;software project management;computer science;package development process;social software engineering;software development;software engineering;software construction;database;software walkthrough;software analytics;software deployment;software quality control;software quality;software metric;software quality analyst;software system;software peer review	SE	-65.02334752960834	31.628209479060455	62490
ca8fdcdf1a324a0d8a6ce001daecf4390d63f429	automated support for human resource allocation in software process by cluster analysis	mining software repositories;data mining;human resource management;software process	It is widely recognized the potential of using organizational data analysis to enable automated tools supporting process management task. The organizational repositories should be used in an active way to accordingly support dynamic decision-making process in software project management. In this paper, we briefly describe a research aiming to support the human resource allocation process in the software process context based on the analysis of organizational repositories. It intends to provide an organizational data analysis as a mean to take empirical evidence to perform fact-based decisions upon historical and ongoing organizational experiences. As the work is in its beginning, we also present some differences from other already existing approaches and the main challenges to be overcome through completion of this work.	cluster analysis;software development process;software project management	Thiago Jorge A. Santos;Adailton Magalhães Lima;Carla Alessandra Lima Reis;Rodrigo Quites Reis	2014		10.1145/2593822.2593830	personal software process;verification and validation;team software process;organizational learning;software engineering process group;software project management;computer science;systems engineering;knowledge management;package development process;resource management;data mining;empirical process;goal-driven software development process;organizational behavior management	SE	-67.97846494781992	19.975370006883754	62578
a36f8a86fad73a06ae1a2f9cb01234879af64278	designing and developing automated refactoring transformations: an experience report	software reliability software houses software maintenance;automatic refactoring phase automated refactoring transformations software development companies software product maintainability manual refactoring phase tool building phase;companies encoding manuals automation software histograms conferences;software maintenance automated refactoring code smells coding issues	There are several challenges which should be kept in mind during the design and development phases of a refactoring tool, and one is that developers have several expectations that are quite hard to satisfy. In this report, we present our experiences of a two-year project where we attempted to create an automatic refactoring tool. In this project, we worked with five software development companies that wanted to improve the maintainability of their products. The project was designed to take into account the expectations of the developers of these companies and consisted of three main stages: a manual refactoring phase, a tool building phase, and an automatic refactoring phase. Throughout these stages we collected the opinions of the developers and faced several challenges on how to automate refactoring transformations, which we present and summarize.	code refactoring;mind;software development	Gábor Szoke;Csaba Levente Nagy;Rudolf Ferenc;Tibor Gyimóthy	2016	2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)	10.1109/SANER.2016.17	reliability engineering;personal software process;long-term support;verification and validation;software sizing;computer science;systems engineering;package development process;backporting;social software engineering;software framework;software development;software engineering;software construction;software walkthrough;software analytics;software maintenance;software deployment;code refactoring;software peer review	SE	-64.47245774337044	27.628161016107196	62612
69b59a4733ad88ae9f85d363034f9f22583655d1	logistics traceability for supply chain improvement - case study of smmart project	supply chain	Tracking and tracing of shipment is nowadays a key element of customer service. RFID technology, which provides real-time tracking, helps to indicate and to monitor the transition of the events along the supply chain. Traceability permits not only to reduce the total logistics cost and to shorten the order cycle time, but also to increase efficiency, to improve quality performance and to offer new added value services for clients. The Authors present the theoretical background of the problem as well as the experiences and the ideas of new solutions which are currently developed within the SMMART 6 Framework Program Project.	logistics;radio-frequency identification;real-time clock;real-time computing;traceability;track and trace	Paulina Blaszkowska;Jana Pieriegud;Michal Wolanski	2007			systems engineering;process management	HCI	-65.68857408950072	13.607790344134555	62649
abd849d6958d031bbc591e2dc2faf08592f99376	assessment of the scale-scope dilemma in production systems: an integrative approach	integrated approach;integrated assessment;integrable model;production system;economies of scale;product complexity;economies of scope	Companies in high wage countries are increasingly confronted with the challenge of optimizing economies of scope and economies of scale simultaneously to succeed on a global market place. An integrated assessment of production systems facing this challenge is essential to evaluate the actual state of a company and to provide a basis for drawing the right conclusions to reconfigure production systems successfully. In this paper an integrated model for measuring economies of scope as well as economies of scale is introduced, defining the fundamental domains of a production system. The major objectives resulting from the overall scale-scope dilemma are broken down for each domain and the main dimensions for an assessment of each domain are defined. A new measure named Degree of Efficiency is defined, quantifying the fulfillment of the opposing objectives in each domain and hence, the contribution to an overall resolution of the scale-scope dilemma.	production system (computer science)	Günther Schuh;Jens Arnoscht;Arne Bohl;Daniel Kupke;Christopher Nußbaum;Jerome Quick;Michael Vorspel-Rüter	2011	Production Engineering	10.1007/s11740-011-0315-0	economics;economies of scale;marketing;operations management;management science;production system;welfare economics	AI	-76.91203119560714	7.790172778077399	62650
140357b3e6c8b42c99a94b9975858ec88a108104	the benefits of networking	mejoramiento procedimiento;developpement logiciel;technologie communication;reseau communication;software process improvement;computer software maintenance;ingenieria logiciel;software engineering;maintenance logiciel;amelioration procede;desarrollo logicial;software development;genie logiciel;process improvement;communication technology;red de comunicacion;communication network;tecnologia comunicacion	Networking: K-12. ERIC Digest.................................................. 1 THE BENEFITS OF NETWORKING........................................ 2 HOW ARE NETWORKS USED?............................................. 2 GETTING STARTED........................................................... 4 AROGERS@BONITA.CERF.FRED.ORG.................................. 5 REFERENCES AND READINGS............................................ 6	commitment scheme;experience;experiment;population;semantic web service;software development process;spontaneous order	Jørgen Bøegh;Mads Christiansen;Ebba Þóra Hvannberg;Tor Stålhane	2000		10.1007/978-3-540-45051-1_19	information and communications technology;simulation;systems engineering;engineering;software development;software engineering;operations research;telecommunications network	HPC	-79.9192331731397	21.47260060238635	62680
08bd89414611029ab008e51a8848cb3111bce202	introducing software engineering by means of extreme programming	good basic understanding;own practical experience;older student;basic software engineering concept;important concept;paper report;introducing software engineering;important aspect;software engineering;extreme programming;year undergraduate course;teaching;automotive engineering;feedback;testing;portfolio management;computer science	"""This paper reports on experience from teaching basic software engineering concepts by using Extreme Programming in a second year undergraduate course taken by 107 students. We describe how this course fits into a wider programme on software engineering and technology and report our experience from running and improving the course. Particularly important aspects of our setup includes team coaching (by older students) and """"team-in-one-room"""". Our experience so far is very positive and we see that students get a good basic understanding of the important concepts in software engineering, rooted in their own practical experience."""	extreme programming;fits;software engineering	Görel Hedin;Lars Bendix;Boris Magnusson	2003			simulation;extreme programming;software engineering process group;extreme programming practices;systems engineering;engineering;software design;social software engineering;software development;software engineering;feedback;software testing;computer engineering;project portfolio management	SE	-66.34206793098895	26.154862822768905	62693
0e225c5cecad11356a336f9aaac90a3ccaca48a8	staffing software maintenance and support projects	service level agreement sla;service provider;software maintenance;rule of thumb;queueing theory;software project management;service level agreement sla software maintenance systems support queueing theory;software maintenance contracts project management queueing analysis thumb personnel software systems costs productivity management information systems;queueing model;service level agreement;lower bound;systems support	In the past decade outsourcing a software system's support and maintenance has become relatively common across most organizations. In this paper we consider a few issues governing the staffing of support and maintenance projects from the perspective of the software service provider. Though queueing models have been used for the estimation of staffing requirements and control in the past, little attention has been paid to the service level agreements (SLAs) that most service providers have to adhere to, as part of the service contract. In this paper we consider the problem of finding the minimum staffing requirement by modeling the support environment as an M/M/s queue while including an SLA type requirement. For the same system, we show that a lower bound on the service rate exists and how it is to be calculated. We also validate certain rules of thumb that software project managers can employ in light of changing conditions and discuss the caveats to these rules of thumb.	outsourcing;queueing theory;requirement;service-level agreement;software as a service;software maintenance;software project management;software system	Jai Asundi;Sumit Sarkar	2005	Proceedings of the 38th Annual Hawaii International Conference on System Sciences	10.1109/HICSS.2005.553	service provider;service level requirement;service level objective;software project management;computer science;operations management;software engineering;software as a service;database;rule of thumb;upper and lower bounds;queueing theory;software maintenance	SE	-68.84089237598403	25.94442347946865	62765
e779935964dfd14a795ddb2a7266bef0e04f5feb	research on the factors of transfer pricing of chinese enterprises	pricing;companies;taxation asset management globalisation organisational aspects pricing profitability;asset liability ratio transfer pricing roa corporate governance structure internal control reorganization the scale of intangible assets the total scale of the enterprise;profitability;correlation;pricing companies correlation profitability educational institutions;asset liability ratio transfer pricing chinese enterprises economic globalization cross regional affiliated parties profit tax benefits intangible assets enterprise value creation chinese a share listed companies roa corporate governance structure internal control reorganization	Economic globalization has intensified the transactions among companies' cross-regional affiliated parties. More and more enterprises manipulate profit to obtain the tax benefits through transfer pricing. At the same time. intangible assets are becoming the main driving factors for enterprises' value creation. It has become a trend that lots of companies are transferring profit or implementing other strategic arrangements through the transfer of intangible assets. This article selects the Chinese A-share listed companies as samples to study the factors affecting transfer pricing behavior in a enterprise and finds that there is significant correlation between transfer pricing and many factors like ROA, corporate governance structure, internal control, reorganization, the scale of intangible assets, the total scale of the enterprise, asset-liability ratio.	corporate governance;resource-oriented architecture	Cheng Hui;Wu Zhan Xia	2014	2014 International Conference on Behavioral, Economic, and Socio-Cultural Computing (BESC2014)	10.1109/BESC.2014.7059522	finance;consumption-based capital asset pricing model;business;commerce	DB	-84.53346424795647	6.276533864240157	62785
e51b04c5156cccddd33a0da0c5f1c319104dedfa	sustainable packaging technology to improve food safety		In this paper, we introduce an innovative packaging technology to develop sustainable packages to significantly improve food safety in the supply chain. The technology is referred to as sustainable because it also addresses the socioeconomic and environmental requirements of sustainability. The technology has four components: the first three components involve using modified atmosphere packaging, controlled release packaging, and ecofriendly packaging to develop novel materials to construct packages, while the fourth component involves using a decision support system to facilitate package design. A conceptual framework of this technology is also provided to illustrate how the knowledge of food science, packaging technology, materials science, and information technology can be integrated in a systematic manner to improve food safety in the supply chain. The development of this technology requires the concerted efforts of researchers from several disciplines, and there are great opportunities for researchers of information systems to contribute to developing the hardware and software necessary for this technology.		K. L. Yam;Paul Takhistov	2016	IBM Journal of Research and Development		computer science;systems engineering;engineering;food packaging	Robotics	-63.35085545910103	9.04374640136509	62956
f6347c4cc926138716cf4cd4c6ec7d48e2a8f4e9	development of activity models of integrated safety and disaster management for industrial complex areas	decision support;management system;disaster management;design and development;industrial organization;agent system;activity model;use case;safety and disaster management	There are increasing challenges that face industrial organizations to manage safety in normal and to minimize risks when disaster occurs. Such challenges increase when many production plants are placed in industrial complex area. This paper describes activity models to manage plant safety and disasters of production plants in industrial complex areas. The proposed activity models are used to design and develop integrated decision support and management system for safety and disaster management of industrial complex areas, which is validated using case study scenarios.		Yukiyasu Shimada;Hossam A. Gabbar	2008		10.1007/978-3-540-85567-5_1	construction engineering;systems engineering;engineering;environmental resource management	Robotics	-63.527983588102394	9.208012869064083	63037
7ea638471532eee90cead1084506f995c7c808c0	scope/complexity: a framework for the classification and analysis of information-decision systems		Abstract:A new framework for the clasification and analysis of information-decision systems is presented. It is based upon two key dimensions, Scope and Complexity, each of which can be adequately described in terms of two principal components. For Scope, these are the organizational descriptors of breadth and depth, while for Complexity they are the problem descriptors of uncertainty and variety.This framework is shown to be a logical extension and synthesis of previously presented frameworks. The Scope/Complexity framework is sufficiently flexible to accommodate a variety of information system perspectives and to enable the analyst and researcher to graphically and algebraically portray the existing system and to evaluate various strategies for process improvement.An example of the application of the Scope/Complexity construct in the context of a vehicle scheduling decision support system is presented. In the final section, the Scope/Complexity framework is employed to identify a series of research hypo...		Salvatore Belardo;Harold L. Pazer	1985	J. of Management Information Systems		computer science;knowledge management;data mining;management science	Metrics	-63.52738151895151	13.542754366604582	63039
5edf2e5879399a9f321f3bcfa64e6e41657c76e0	multiview hierarchical agglomerative clustering for identification of development gap and regional potential sector		Corresponding Author Tb Ai Munandar Department of Informatics Engineering, Faculty of Information Technology, Universitas Serang Raya, Banten, Indonesia Email: tbaimunandar@gmail.com Abstract: The identification of regional development gaps is an effort to see how far the development conducted in every District in a Province. By seeing the gaps occurred, it is expected that the Policymakers are able to determine which region that will be prioritized for future development. Along with the regional gaps, the identification in Gross Regional Domestic Product (GRDP) sector is also an effort to identify the achievement in the development in certain fields seen from the potential GRDP owned by a District. There are two approaches that are often used to identify the regional development gaps and potential sector, Klassen Typology and Location Quotient (LQ), respectively. In fact, the results of the identification using these methods have not been able to show the proximity of the development gaps between a District to another yet in a same cluster. These methods only cluster the regions and GRDP sectors in a firm cluster based on their own parameter values. This research develops a new approach that combines the Klassen, LQ and hierarchical agglomerative clustering (HAC) into a new method named multi view hierarchical agglomerative clustering (MVHAC). The data of GRDP sectors of 23 Districts in West Java province were tested by using Klassen, LQ, HAC and MVHAC and were then compared. The results show that MVHAC is able to accommodate the ability of the three previous methods into a unity, even to clearly visualize the proximity of the development gaps between the regions and GRDP sectors owned. MVHAC clusters 23 districts into 3 main clusters, they are; Cluster 1 (Quadrant 1) consists of 5 Districts as the members, Cluster 2 (Quadrant 2) consists of 12 Districts and Cluster 3 (Quadrant 4) consists of 6 Districts.	biological anthropology;cluster analysis;email;hierarchical clustering;high-availability cluster;informatics engineering;java;letter-quality printer	Tb. Ai Munandar;Azhari;Aina Musdholifah;Lincolin Arsyad	2018	JCS	10.3844/jcssp.2018.81.91	machine learning;data mining;computer science;artificial intelligence;economic base analysis;hierarchical clustering	SE	-84.39580100555185	10.555874726224783	63157
546655eadd482a6a95aee802e534003903edd65b	restrictions in open source: a study of team composition and ownership in open source software development projects				Poonacha K. Medappa;Shirish C. Srivastava	2018				SE	-91.35632491349425	24.782729314285813	63224
a087bda515634e5c26bb55ec790ffe772f120487	on architectural design of trustman system applying hici analysis results - the case of technological perspective in vbes	trustman system;index terms: hici;vbe;trust elements;indexing terms;relational data	Organizations need to trust each other for smoothing their collaboration to both acquire and respond to more and better business opportunities that none of them could do otherwise. However, in Virtual organization Breeding Environments (VBEs) there are several difficulties related to the processes that must be performed to facilitate organizations create trust to each other. The main cause of these difficulties is the way these processes are currently performed which in most cases are ad hoc and manual. Among others the processes include: assessing trust level of organizations, creating trust among organizations, establishing trust relationships, and managing trust related data. As a result, configuring virtual organizations (VOs) within the VBE environment is becoming challenging and difficult. To match the pace in the market these processes must be quickly performed and must produce very accurate results. Thus they must now be supported with semiautomatic systems. In order to properly assess trust level of organizations in VBEs, trust elements and trust relationships must be thoroughly characterized. This paper addresses the characterization of trust elements and automation of processes related to the management of trust among organizations. It presents a three-stage approach for identifying and analyzing trust elements. The identified trust elements and their related analysis results are then applied in the design of mechanisms and architectures for Trust Management (TrustMan) system. The design of two architectures (operational and componential) for TrustMan system is also presented in this paper.	cooperative breeding;hoc (programming language);smoothing;trust management (managerial science);vesa bios extensions;virtual organization (grid computing)	Simon Samwel Msanjila;Hamideh Afsarmanesh	2008	JSW		index term;relational database;computer science;knowledge management;database;management science;computer security;computational trust	SE	-74.65334013617677	15.148564073025497	63226
7b7f18c8db5094608692ebad1e6cc43a625efb7f	a fuzzy superiority and inferiority ranking based approach for it service management software selection		Purpose – Information technology service management (ITSM) has become a major IT department management system in organizations. Successful implementation of ITSM depends on select adequate ITSM software. Evaluation and selection of the ITSM solution or software packages is complicated and time-consuming decision-making problem. This paper aims to present an approach for dealing with such a problem. Design/methodology/approach – This approach introduces functional, non-functional requirements and novel fuzzy out-ranking evaluation method for ITSM software selection. The presented approach breaks down ITSM software selection criteria into two broad categories, namely, functional (service strategy, service design, service transition, service operation, continual service improvement according to Information Technology Infrastructure Library V3) and non-functional requirements (quality, technical, vendor, implementation) including totally 46 selection criteria. A novel fuzzy superiority and inferiority ranking (FSIR) was developed and made applicable for ITSM software selection based on identified criteria. Findings – The proposed approach is applied to IT services company to select and acquire ITSM software, and the provided numerical example illustrates the applicability of the approach for this choice. The approach can facilitate firms to achieve suitable ITSM software and have a precise acquisition decision; however, the limitation of dependency on experts’ competence and proficiency in the both ITSM field and IT technical issues exists. Research limitations/implications – The approach can facilitate firms to achieve suitable ITSM software and have a precise acquisition decision; however, the limitation of dependency on experts’ competence and proficiency in the both ITSM field and IT technical issues exists. Practical implications – Facilitating of ITSM implementation through its handy software selection is the major impact of current research. Originality/value – A facile FSIR-based approach for software selection has been customized to contribute to the current literature in the ITSM field. Facilitating of ITSM implementation through its handy software selection is the major impact of current research.	functional requirement;fuzzy set;handy board;itil;non-functional requirement;numerical analysis	Saeed Rouhani	2017	Kybernetes	10.1108/K-05-2016-0116	artificial intelligence;management science	SE	-71.07126931444996	12.797016513247648	63239
2d921812a5959699df2ccf0a318a6332f7744b25	an empirical study of architecting for continuous delivery and deployment		Recently, many software organizations have been adopting Continuous Delivery and Continuous Deployment (CD) practices to develop and deliver quality software more frequently and reliably. Whilst an increasing amount of the literature covers different aspects of CD, little is known about the role of software architecture in CD and how an application should be (re-) architected to enable and support CD. We have conducted a mixed-methods empirical study that collected data through in-depth, semi-structured interviews with 21 industrial practitioners from 19 organizations, and a survey of 91 professional software practitioners. Based on a systematic and rigorous analysis of the gathered qualitative and quantitative data, we present a conceptual framework to support the process of (re-) architecting for CD. We provide evidence-based insights about practicing CD within monolithic systems and characterize the principle of “small and independent deployment units” as an alternative to the monoliths. Our framework supplements the architecting process in a CD context through introducing the quality attributes (e.g., resilience) that require more attention and demonstrating the strategies (e.g., prioritizing operations concerns) to design operations-friendly architectures. We discuss the key insights (e.g., monoliths and CD are not intrinsically oxymoronic) gained from our study and draw implications for research and practice.	continuous delivery;list of system quality attributes;semiconductor industry;software architecture;software deployment;while	Mojtaba Shahin;Mansooreh Zahedi;Muhammad Ali Babar;Liming Zhu	2018	Empirical Software Engineering	10.1007/s10664-018-9651-4	psychological resilience;systems engineering;empirical research;software deployment;devops;conceptual framework;continuous delivery;computer science;software;software architecture	SE	-68.78177880773508	21.492142432029993	63296
9c5ae9a35ff2b31baa16a4ea8414159d0ea175d3	process-based performance measurement in healthcare networks	performance measure;key performance indicator;integrated care;real time processing;network performance;web service;balanced scorecard;patient satisfaction;system architecture	Coordination and controlling in healthcare networks becomes increasingly important to enable integrated care scenarios, to enhance patient satisfaction and to reduce costs of the treatment processes. Based on the balanced scorecard a process-oriented approach for performance measurement in healthcare networks is introduced. The underlying systems architecture is presented. Integrating data from different sources and providers enables the calculation and visualization of key performance indicators in a network performance cockpit. Compliance scorecards are used to implement the network strategy and to ensure the achievement of goals. Real-time process data is obtained from a component that controls the flow of interorganizational treatment processes by web service technology. This component also supports treatment processes by process oriented e-services.	data mining;e-services;gatekeeper;information privacy;network performance;real-time transcription;simulation;systems architecture;web service	Günter Schicker;Jörg Purucker;Freimut Bodendorf	2007			performance measurement;web service;computer science;knowledge management;performance indicator;balanced scorecard;network performance;law;systems architecture	Metrics	-69.57154812062613	9.425039383350747	63345
f618c8af2eac201c0ac01fdebd646e7a493a7300	improving defect removal effectiveness for software development	defect removal model;electrical capacitance tomography;software testing;organization;software process improvement;program verification program debugging program testing;software development process;testing;computer industry;program verification;development process;inspection;defect removal;programming system testing software testing inspection software quality computer industry statistics electrical capacitance tomography quality management coordinate measuring machines;program testing;software development;industry data;statistics;system testing;industry statistics;process improvement;program debugging;program defect removal effectiveness;coordinate measuring machines;program testing program defect removal effectiveness software development industry statistics organization industry data defect removal model inspection software process improvement program verification;programming;test process;software quality;quality management	The paper first reviews the industry statistics on defects, presents a model of defect removal, then develops the relationships between the defect removal effectiveness of different defect removal phases. The defect removal effectiveness of an average organization is determined based on published data. Combining the industry data with the defect removal model, insights into improvement of the defect removal process can be gained. It is determined that the defect removal effectiveness of individual phases cannot be higher than that of the overall development process. Also, introducing inspection to the software development process will certainly allow an average organization to achieve higher defect removal effectiveness.	disaster recovery plan;software bug;software development process	Hareton K. N. Leung	1998		10.1109/CSMR.1998.665789	reliability engineering;quality management;systems engineering;engineering;software engineering;software testing;software development process;computer engineering	SE	-65.16406980511111	28.37257275390646	63370
7f6967d172f18393e8c811c14d966f055be3545d	challenges and recommendations when increasing the realism of controlled software engineering experiments	developpement logiciel;methode empirique;metodo empirico;empirical method;empirical software engineering;controlled experiment;software engineering;technology transfer;lessons learned;desarrollo logicial;software development;transferencia tecnologica;industrial application;transfert technologie	An important goal of most empirical software engineering experiments is the transfer of the research results to industrial applications. To convince industry about the validity and applicability of the results of controlled software engineering experiments, the tasks, subjects and the environments should be as realistic as practically possible. Such experiments are, however, more demanding and expensive than experiments involving students, small tasks and pen-and-paper environments. This chapter describes challenges of increasing the realism of controlled experiments and lessons learned from the experiments that have been conducted at Simula Research Laboratory.	apply;experiment;experimental software engineering;it risk management;population;programming tool;recommender system;simula	Dag I. K. Sjøberg;Bente Anda;Erik Arisholm;Tore Dybå;Magne Jørgensen;Amela Karahasanovic;Marek Vokác	2003		10.1007/978-3-540-45143-3_3	simulation;software engineering process group;software development;empirical research	SE	-66.09210630532473	25.69809577846651	63385
258541a5955a9652ed99a72a13ddb3f614768998	empirical software engineering models: can they become the equivalent of physical laws in traditional engineering?		Traditional engineering disciplines such as mechanical and electrical engineering are guided by physical laws. They provide the constraints for acceptable engineering solutions by enforcing regularity and thereby limiting complexity. Violations of physical laws can be experienced instantly in the lab. Software engineering is not constrained by physical laws. Consequently, we often create software artifacts which are too complex to be understood, tested or maintained. As too complex software solutions may even work initially, we are tempted to believe that no laws apply. We only learn about the violation of some form of “cognitive laws” late during development or during maintenance, when too high complexity inflicts follow-up defects or increases maintenance costs. Initial work by Barry Boehm (e.g., CoCoMo) aimed at predicting and controlling software project costs based on estimated software size. Through innovative life cycle process models (e.g., Spiral model) Barry Boehm also provided the basis for incremental risk evaluation and adjustment of such predictions. The proposal in this paper is to work towards a scientific basis for software engineering by capturing more such time-lagging dependencies among software artifacts in the form of empirical models and thereby making developers aware of so-called “cognitive laws” that must be adhered to. This paper attempts to answer the questions why we need software engineering laws and how they could look like, how we have to organize our discipline in order to build up software engineering laws, what such laws already exist and how we could develop further laws, how such laws could contribute to the maturing of science and engineering of software in the future, and what the remaining challenges are for teaching, research, and practice in the future.	barry boehm;cocomo;electrical engineering;experimental software engineering;software project management;software sizing;spiral model	H. Dieter Rombach	2011	Int. J. Software and Informatics		simulation;computer science;artificial intelligence;software engineering;data mining;management;world wide web;algorithm;programming complexity	SE	-65.13686856834113	18.625523173671386	63552
362170c31cec593fce77f8799f6deee52bc0b31f	5th international workshop on software engineering for computational science and engineering (se-cse 2013)	computational engineering;computational science;software engineering	Computational Science and Engineering (CSE) software supports a wide variety of domains including nuclear physics, crash simulation, satellite data processing, fluid dynamics, climate modeling, bioinformatics, and vehicle development. The increases importance of CSE software motivates the need to identify and understand appropriate software engineering (SE) practices for CSE. Because of the uniqueness of the CSE domain, existing SE tools and techniques developed for the business/IT community are often not efficient or effective. Appropriate SE solutions must account for the salient characteristics of the CSE development environment. SE community members must interact with CSE community members to understand this domain and to identify effective SE practices tailored to CSEs needs. This workshop facilitates that collaboration by bringing together members of the CSE and SE communities to share perspectives and present findings from research and practice relevant to CSE software and CSE SE education. A significant portion of the workshop is devoted to focused interaction among the participants with the goal of generating a research agenda to improve tools, techniques, and experimental methods for CSE software engineering.		Jeffrey C. Carver;Tom Epperly;Lorin Hochstein;Valerie Maxville;Dietmar Pfahl;Jonathan Sillito	2013	2013 35th International Conference on Software Engineering (ICSE)			SE	-63.0065634053181	20.096222252231836	63595
98b4deeb1ab1e359cc11f681ca52448f95d44ff5	end-user debugging strategies: a sensemaking perspective	debugging;empirical study;gender difference;spreadsheets;information foraging;gender hci;gender differences;end user software engineering;sensemaking;debugging strategies;end user programming	Despite decades of research into how professional programmers debug, only recently has work emerged about how end-user programmers attempt to debug programs. Without this knowledge, we cannot build tools to adequately support their needs. This article reports the results of a detailed qualitative empirical study of end-user programmers' sensemaking about a spreadsheet's correctness. Using our study's data, we derived a sensemaking model for end-user debugging and categorized participants' activities and verbalizations according to this model, allowing us to investigate how participants went about debugging. Among the results are identification of the prevalence of information foraging during end-user debugging, two successful strategies for traversing the sensemaking model, potential ties to gender differences in the literature, sensemaking sequences leading to debugging progress, and sequences tied with troublesome points in the debugging process. The results also reveal new implications for the design of spreadsheet tools to support end-user programmers' sensemaking during debugging.	categorization;correctness (computer science);debugging;information foraging;programmer;sensemaking;spreadsheet	Valentina Grigoreanu;Margaret M. Burnett;Susan Wiedenbeck;Jill Cao;Kyle Rector;Irwin Kwan	2012	ACM Trans. Comput.-Hum. Interact.	10.1145/2147783.2147788	human–computer interaction;computer science;knowledge management;algorithmic program debugging;empirical research;debugging	HCI	-73.54823205164934	22.2751349182646	63629
5a98323f17fcb5e54619aa934b0f3a0cf9a2fed1	electronic services as a tool of resource sharing	software;organizational architectures;information technologies;new technology;resource allocation;information technology;resource management;maintenance engineering;electronic services;web services resource allocation;social processes;social institutions;social processes resource sharing information technologies organizational architectures social institutions evolutionary business;business;resource sharing;web services;subscriptions;production;resource management information technology costs automatic control information processing consumer electronics government technology management image resolution autonomous agents;evolutionary process;optimum design;evolutionary business	This article studies the types of resource sharing made possible by various information technologies, and extrapolates into new types of resource sharing that can be implemented with new technologies, systems, and organizational architectures. Resource sharing can bean overarching design criterion for systems, organizations, and social institutions where the cost of information determines the extent of feasible sharing and the optimum design.We argue that optimum structures may need to be computed, designed and implemented explicitly, because they may not be achievable through evolutionary business and social processes. There is considerable evidence that evolutionary processes relying on incremental changes do not always lead to globally optimum structures, and explicit design may be necessary. This article details one approach to such explicit design based on resource sharing, related technologies, organizational architectures, and supporting transactions.	extrapolation;optimal design;web service	Levent V. Orman	2008	IEEE Technology and Society Magazine	10.1109/MTS.2008.925534	web service;shared resource;resource allocation;computer science;knowledge management;environmental resource management;management science;law;information technology	EDA	-77.60061854422787	4.576765748106997	63662
beee8eef62b32e8b3a6bbc874807d03adf254c36	operations support systems: caught in the winds of change	telecommunication management network;education and training;operations support systems;standards;marketplace;telecommunication computing;operating system;guidelines;marketplace operations support systems operations systems telecommunications management network standards;telecommunication standards;operations systems;telecommunication standards guidelines;telecommunications management network;telecommunication network management	Summary form only given, as follows. The purpose of this presentation is to describe the tremendous changes that are impacting Operations Support Systems (OSSs), aka Operations Systems (OSs), today, and why OSS developers and providers are being faced with challenges and opportunities previously unheard of. After a brief historical perspective of how OSSs evolved and the current dilemma that they face, the presenter covers some environmental and operational factors and transitions that are currently shaping and driving OSS development. The presenter concludes with a bottom line perspective for the OSS marketplace that includes some recommended requirements, the role of standards and the Telecommunications Management Network (TMN) guidelines, and the impacts on education and training.		Dan Walsh	1998		10.1109/NOMS.1998.654897	operations support system;element management system;computer science;operating system;management science;telecommunications management network;management;computer security;computer network	Robotics	-74.09895150413784	17.04805981740519	63690
2b5b5d1daf9f6c7fa6ecfd67052b569ffa1e9851	new product development process: proposal for an innovative design modelling framework including actors evaluation of innovation costs and value	modelling framework;new product development	Firms are facing very short and important innovation cycles, particularly in IT and Telecommunication sectors. Then a question appears: why do some innovations succeed whereas other fail. From offer's point of view, a way could be to evaluate impacts of a decision to innovate for each of the actors involved in this product trajectory. Therefore the goal of such an approach is reducing high Innovation development risks by integrating the diverse stakes of life cycle actors and by helping design teams to integrate the evolution of some key environmental processes. We introduce in this paper the characteristics of the Innovation Process and Engineering Design Phase for high level innovations. In this framework, we propose an Innovation Valuation Model integrating strategic	new product development	N. Sechi;M. Lawson;René Soenen	2001			product innovation;systems engineering;engineering;knowledge management;management science	HCI	-75.22110486924218	7.607839291707554	63739
51d957fad14d1e72cf17c21e30560a45b1b022ef	message from the general chair		QEST concerns the techniques and tools necessary for, roughly speaking, quantitatively evaluating the performance, dependability, or correctness of many types of systems, mostly in computer science and in communications. The spirit of QEST is to provide a forum where both methodological and practical problems related to the different aspects of those quantitative evaluation processes are discussed. QEST is characterized by successfully merging researchers and practitioners in different areas sharing their interest in these problems, and QEST papers go from theoretical or methodological work to case studies or to the design of software tools. Our 2008 issue is a perfect example of this success, and the different components of the conference, that is, the four tutorials offered, the three invited presentations given by prestigious keynote speakers and the high quality papers and tool presentations selected by our Technical Program Committee, illustrate how those different aspects of the main underlying problem, put the behavior of the system in numbers, fit in a complementary way.	computer science;correctness (computer science);dependability;display resolution	Sunil D. Sherlekar	2002		10.1109/VLHCC.2005.50		Logic	-63.83281280364135	19.205514117998725	63955
a5184f7c3c5758179c2fd3e1df5143b2119dd190	reframing outsourcing through social networks: evidence from infocert's case study	relational view;governance;it outsourcing;settore secs p 08 economia e gestione delle imprese;social collaboration;outsourcing lifecycle	In an over-connected world where ICTs dominate firms' development and evolution, outsourcing is an increasingly adopted practice by IT firms facing a third-generation of inter-firm interactions: after the IT and business processes' outsourcing, and then the offshore outsourcing, now we face a sourcing ecosystem tagged as human cloud, where the online work and virtual workers are the center of the new system. Notwithstanding some relevant contributions to the literature about IT outsourcing, still few is known about how coordination between client and supplier can achieve superior outcomes through the development of collaborative practices. In particular, the use of IT tools devoted to sociality as a coordination mechanism has been under-investigated. This research provides insights about how a company can change attitudes and behaviors of client and supplier thanks to an IT tool deputed to collaboration: the social collaboration system. Through an explorative case study, our paper provides two main contributions to the literature about IT outsourcing: i) we show how the adoption of a social collaboration system improves ITO governance and performance, providing further empirical evidence on the role of social mechanisms in ITO relationships; ii) we show how the introduction of a social collaboration system in outsourcing management can influence and change the building blocks of its life-cycle.	business process;ecosystem;indium tin oxide;interaction;outsourcing;social collaboration;social network	Giovanni Vaia;Anna Moretti	2014		10.1007/978-3-319-11367-8_8	knowledge management;knowledge process outsourcing;business;management;commerce;outsourcing	AI	-79.24229230758905	4.37248866694359	63957
bc1ba6fc42823c69005452e25eb4ecd234715308	riverbank financial: changing the role of information technology	information management system;sistema operativo;information systems security;gestion entreprise;mis systems;information systems research;journal of it;jit;teaching cases;inovacion;information security;case studies;information science;competitividad;information security systems;information technology;business information technology;security information systems;it journals;transformacion;firm management;technologie information;information systems management;riverbank financial;it teaching cases;operational research society;business model;journal of information technology teaching cases;innovation;operating system;competitivity;estudio caso;computer information systems;user centered;jit journal;geographic information systems;information technology journal;information management;information systems journals;information systems technology;managing information systems;etude cas;oriente utilisateur;accounting information systems;information and management;management information systems;institution financiere;define information systems;systeme exploitation;administracion empresa;strategic information systems;financial institution;business information management;soft system methodology;transformation;information system;health information systems;computer information technology;journal of information technology;business information systems;tecnologia informacion;competitivite;business systems analyst;systeme information;journal information technology;it journal;management science;journal of information systems;sistema informacion;information technology journals	The information technology (IT) group at a large Canadian financial institution has recently implemented a new operating framework that significantly changed the role of IT within the organization. The traditional, technology-centric, independent entity has been retailored into an integrated, customer-focused business unit. Consistent with their new vision, the IT group has shifted its focus towards IT-business integration, increased IT employee satisfaction and decreased turnaround in system development. Amongst the successes of the implementation to date is one perspective that has attracted the attention of IT executives. Customers who observed the transition of the IT group have raised concern about Riverbank Financial's ability to maintain leadership in technological expertise. IT executives must now consider the possibility that the new operating framework has taken too much focus away from technology and identify possible changes.		M. Kathryn Brohman;Duncan Copeland	1999	JIT	10.1080/026839699344584	public relations;economics;computer science;engineering;knowledge management;electrical engineering;management information systems;management;information technology;information system	HCI	-81.79490080695845	8.308069745945204	63963
4d5ba8cf5ec9f09abb4398792ab833edfd7e3768	towards validating prediction systems for process understandability: measuring process understandability	software metrics;reliability;concurrent computing;structural process attribute;software measurement;time measurement;software management;prediction system validation;metric;internal structure;data mining;software metrics software management;adaptation model;software measurement software metrics humans software algorithms scientific computing costs time measurement process design design for experiments concrete;process understandability metrics;process understandability;software metric;reliability and validity;correlation;measurement approach;structural process attribute prediction system validation process understandability metrics software metrics;measurement approach process understandability metric	Motivated by software metrics, several process metrics for measuring internal (structural) process attributes have been proposed. Integrated in validated prediction systems, these metrics can be used to predict values of external process attributes like, for example, process understandability. There are only a few papers dealing with finding a process understandability metric and validating prediction systems for process understandability. Looking at these publications, we identified possible problems with metric reliability and validity. In this paper, we define new metrics for process understandability inspired by existing work. Furthermore, we present some hypotheses about effects of measuring process understandability. Conducting an experiment, we got some encouraging findings supporting these hypotheses: different aspects of process understandability can be complicated in varying degrees for a process and asking only some few questions about a process can cause values for process understandability differing very much from the real value. These findings should be considered in future work about measuring process understandability.	experiment;randomness;software metric	Joachim Melcher;Detlef Seese	2008	2008 10th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing	10.1109/SYNASC.2008.24	concurrent computing;computer science;data mining;software metric	Visualization	-66.50071921941509	30.91630618605893	64003
cbd8719de6efd534ac1dba3f5f17fa1efc8da824	sensitivity analysis of relative worth in quality function deployment matrices		This article investigates how sensitive relative worth of product requirements in quality function deployment (QFD) is to different rating scales and/or worth-calculation methods used. The two most popular rating scales and worth-calculation scales are identified from the data of 295 QFD matrices published in conference and journal articles (called empirical QFD), and the sensitivity of relative worth in these empirical QFD (measured by probabilities of large changes of relative worth) is studied by switching rating scales and/or worth-calculation methods. The results suggest that relative worth does not significantly change when a worth-calculation method is fixed. In contrast, if a worth-calculation method is changed, large changes may occur. These large changes are more likely in QFD relationship matrices with small numbers of rows and columns less than or equal to six. These results may imply that designers need to commit to a worth-calculation method when they use information from QFD to make their d...	quality function deployment;software deployment	Shun Takai;Robins Mathai Kalapurackal	2012	Concurrent Engineering: R&A	10.1177/1063293X12442411	reliability engineering	DB	-66.74079592882349	31.12744086137397	64014
4c18c3975b0c6e149f886a2730705a2085c8dbe9	pair programming: issues and challenges	agile software development;empirical software engineering;pair programming;experience design	Pair programming, two programmers collaborating on design, coding and testing, has been a controversial focus of interest as Agile Software Development continues to grow in popularity both among academics and practitioners. As a result of the many investigations into the effectiveness of pair programming in the last decade, many have come to realize that there are many hard-to-control factors in pair programming in particular and in empirical software engineering in general. Because of these factors, the results of many pair programming experiments are not easy to replicate and the relative productivity of pair and solo programming are still not fully understood. So far, it has been concluded by previous studies that pair programming productivity can vary, but few have shown how and why this is the case. In this chapter, we discuss a number of challenging factors in the adoption of pair programming and present an approach to deal with them. We discuss how different factors may affect our experimental outcomes and improve experiment design to reveal how and why pair programming can be made productive, at least, in controlled situations.	agile software development;baseline (configuration management);design of experiments;experiment;experimental software engineering;pair programming;programmer;programming productivity;self-replication;structure of observed learning outcome	Kim Man Lui;Kyle Atikus Barnes;Keith C. C. Chan	2010		10.1007/978-3-642-12575-1_7	simulation;extreme programming practices;computer science;systems engineering;software development;software engineering	PL	-71.57318369493325	24.440789574325475	64019
c92cedadbbb4bedd8b6dc42f9796160180eea936	a data quality dashboard for cmms data		Reliability or survival data analysis is an important tool to estimate the life expectancy and failure behaviour of industrial assets such as motors or pumps. One common data source is the Computerized Maintenance Management System (CMMS) where all equipment failures are reported. However, the CMMS typically suffers from a series of data quality problems which can distort the calculation results if not properly addressed. In this paper, we describe the possible data quality problems in reliability data with a focus on CMMS data. This list of problems is based on the results of six case studies conducted at our company. The paper lists a set of metrics which can be used to judge the severity. We also show how the impact of data quality issues can be estimated. Based on this estimate, we can calibrate a series of metrics for detecting the	dashboard;data quality;distortion;global variable;management system;sensor	Ralf Gitzel;Subanatarajan Subbiah;Christopher Ganz	2018		10.5220/0006552501700177	systems engineering;dashboard (business);operations management;computer science;data quality	DB	-66.00662773057705	12.544313875983407	64056
a70e83725ae91d2a945f49169cd64398d8aed610	enterprise integration with advanced information technologies: erp and data warehousing	gestion integrada;gestion integree;gestion entreprise;data integrity;information technology;resource manager;resource management;firm management;integrated management;technologie information;integration;it value;enterprise integration;erp enterprise resource planning;estudio caso;processus entreprise;integracion;enterprise resource planning;data warehousing;software package;etude cas;deposito datos;administracion empresa;business administration;progiciel;information system;data handling;information need;data warehouse;tecnologia informacion;warehousing providers;paquete programa;systeme information;business process;value added;business process integration;competitive advantage;sistema informacion	In today’s dynamic and changing environment, companies have a strong need to create or sustain their competitive advantages. In order to be competitive, companies need to be responsive and closer to the customers, and deliver value‐added products and services as quickly as possible. Companies also need to be able to support organizational information needs faster and better than their competitors. These goals can be realized by applying two emerging information technologies: enterprise resource planning (ERP) supporting business process integration; and data warehousing supporting data integration. Companies with the further integration of ERP and data warehousing will have great advantages in the competitive environment. Two cases have been studied and presented to illustrate its values.	erp;enterprise integration	Yun E. Zeng;Roger H. L. Chiang;David C. Yen	2003	Inf. Manag. Comput. Security	10.1108/09685220310480390	information needs;computer science;knowledge management;value added;marketing;resource management;operations management;data warehouse;group method of data handling;data integrity;business process;enterprise integration;management;information system;competitive advantage	DB	-69.43576730070566	6.743611471205724	64130
0e2df41c99766c39e626eca502bcc668aca94d44	usability evaluation of cooperation visualisation in enterprises: framework development and validation based on empirical results		Cooperation visualisation is used for planning, coordinating, and controlling enterprise cooperation within cross-organisational knowledge management. Visualisation is created using different modelling languages like BPMN 2.0. In this research paper we focus on the usability evaluation of cooperation visualisation in enterprises. We developed a usability evaluation framework and derived a model of hypotheses from it. This model is also linked to usability theory and verified with empirical results. Quantitative data from 432 managers with experience in using visualisation for enterprise cooperation has been collected and the causal model has been tested using the structure equation modelling method partial least squares. The empirical study reveals a critical path for usability evaluation of cooperation visualisation in enterprises. For theory, the impact of organisation properties and usage attributes on the usability of cooperation visualisation in enterprises is demonstrated. For practice, especially the user properties, further usage conditions, usage requirements, and usage variants are supporting the critical usability evaluation path and aid to improve management decisions, model interpretation and usability of cooperation visualisation in enterprises.	causal model;critical path method;knowledge management;modeling language;partial least squares regression;requirement;usability	Erik Kolek;Eva Bittner	2017			visualization;knowledge management;heuristic evaluation;critical path method;pluralistic walkthrough;usability;usability engineering;systems engineering;computer science	SE	-66.55480553039291	14.592113225974806	64141
f923dc6f7950878e045f57792c348f70041e30e5	unit testing considered useful	application development;bepress selected works;unit testing;software development life cycle sdlc software engineering testing;software development process;testing;testing software development life cycle sdlc software engineering;software engineering;sdlc;program testing;scientific programming;software development life cycle;software development testing;software development testing unit testing scientific programming;system testing books internet software testing calendars organizing switches hardware history production	Testing is an important part of application development hardware engineers, in particular, have a long established history of testing for the obvious reason that it's awfully hard to rebuild a microprocessor every time a bug pops up in the design stage-not to mention the enormous headaches such bugs generate on the software side. In this installment of Scientific Programming, we'll discuss the role of testing in the software development process and examine ways to leverage automated unit testing in your projects	microprocessor;software bug;software development process;unit testing	George K. Thiruvathukal;Konstantin Läufer;Benjamin Gonzalez	2006	Computing in Science & Engineering	10.1109/MCSE.2006.124	test strategy;development testing;verification and validation;software performance testing;system integration testing;computer science;acceptance testing;package development process;software reliability testing;software development;software construction;software release life cycle;software testing;systems development life cycle;management;software development process	SE	-68.02047871626564	27.597911601594898	64218
0a542c2cc42624004dee0c36212dc5f1ffcf392a	software engineering within the context of a computer science degree programme	project management;software engineering;computer science education;educational courses;tools software engineering computer science degree programme software development lifecycle project management small group project;small group project;tools;computer science education software engineering;software engineering computer science education educational courses;software development lifecycle;computer science degree programme	Software engineering is not taught as a separate subject, but the theme runs throughout the three years of the computer science degree programme. The students start with an introduction to the software development lifecycle and project management through a small-group project in the second term of the first year, and by the end of their three years' study have gained understanding and insight into a variety of tools and methods.	computer science;software engineering	Brenda M. Mace	1989	Software Engineering Journal	10.1049/sej.1989.0025	project management;personal software process;computing;software engineering process group;software project management;computer science;systems engineering;engineering;social software engineering;software development;software engineering;software walkthrough;application lifecycle management;software development process;software requirements;computer engineering;software peer review	SE	-65.80662717906512	26.052965961467475	64244
f248f5915407fa44baf39804ffeed63c898c9896	aacsb 2003 accreditation standards: impact on continuous quality improvement	quality improvement;services and standards;management education;continuous quality improvement;continuous improvement;business;accreditation;process improvement;aacsb;service operations;quality management	"""The strategic goal of accreditation by AACSB International remains continuous quality improvement in the content, in all aspects of the delivery and in the administration of management education. This paper explores the impact of the new AACSB accreditation standards on this goal. These standards, approved in 2003, were intended to reflect a philosophical shift from """"mission-centric"""" to """"improvement-centric"""". Based on our analysis, we conclude that the new standards fall short of their strategic objective of supporting continuous improvement in management education because many are not process-based. In this paper, we propose an """"integrated framework of business performance measurement"""" (IFBPM) approach to better align the 2003 standards with AACSB's goal of continuous quality improvement of management education. We provide a set of IFBPM guidelines for revising the standards so that they are truly process-based, thus supporting ongoing continuous quality improvement of management education."""		Scott R. Hedin;Clarence H. Barnes;Jason C. H. Chen	2005	IJSS	10.1504/IJSS.2005.005805	engineering management;quality management;economics;engineering;operations management;accreditation;management;service system	DB	-70.50543719863191	14.269468148172164	64248
e0187d59da38188343aa8b329fd49630b8eb43c2	measuring e-commerce technology enabled business value: an exploratory research	e commerce models;business to business;e business;e commerce;information technology;empirical evidence;diffusion theory;internet commerce;business value;b2c e commerce;b2b e commerce	abstraCt While a plethora of anecdotal evidence exists, there is little empirical evidence on the value-creating potential of e-commerce technologies. The present research investigates whether firms using e-commerce technologies are successful in generating business value and, if so, what e-commerce drivers determine success and how to best use these drivers. This work shows how diffusion theory can be used to analyze the widespread utilization of e-commerce technologies and how they create business value. It presents an exploratory model of e-commerce business value grounded in information technology (IT) business value and productivity literature. We use a sample from more than 550 company executives, identified as innovative and successful users of IT.	e-commerce;exploratory testing	Mo Adam Mahmood;Leopoldo A. Gemoets;Laura L. Hall;Francisco J. López;Ritesh Mariadas	2008	IJEBR	10.4018/jebr.2008040104	e-commerce;business model;business analysis;business transformation;empirical evidence;economics;business requirements;computer science;artifact-centric business process model;marketing;business value;business case;electronic business;business analytics;mobile business development;business networking;business relationship management;management;business rule;new business development;world wide web;line of business;business activity monitoring;commerce;consumer-to-business	HCI	-80.63353817871135	5.50227343365917	64265
d7ca7268aa8cfdd45c086d1d982812cdba6190da	the relationship between information technology governance and information technology performance in taiwanese financial enterprises		More and more enterprises regard Information Technology (IT) as their most valuable property and make full use of IT to maximize the performance of their business operations. As a result, enterprises are attaching more importance to coordinating their IT strategy and enterprise strategy in order to get the most from their IT investment. For the sake of better IT performance and long-term development, firms must adopt a complete strategy for IT governance. In Taiwan, most financial enterprises have not considered IT governance to be a necessity, and those which are implementing IT governance have difficulty explaining, systemically, how it affects IT performance. Based on the five dimensions of IT governance, this study uses the balanced scorecard to measure IT performance and discusses the influence of effective governance on IT performance. The results show that the five dimensions of IT governance (strategic alignment, value delivery, risk management, resource management, and performance measurement) are all positively correlated with IT performance. The results of this study can help Taiwan’s financial enterprises set the proper course for IT governance and more clearly understand how it serves to improve IT performance. The Relationship between Information Technology Governance and Information Technology Performance in Taiwanese Financial Enterprises		Ruey-Shiang Shaw;Che-Pin Cheng;Ta-Yu Fu;Chia-Wen Tsai;Dong-Cheng Yen	2014	IJIDE	10.4018/ijide.2014070102	marketing;operations management;management	OS	-81.77975275620989	6.833462122323577	64503
56807d3e9bed14c778961a5ce4dd213572e490e0	software engineering tools and environments: a roadmap	computer program;software engineering tools;separation of concern;process centered software engineering environments;process centered software engineering environment;software engineering;integration;software engineering environment;programming support environments;time to market;tools;separation of concerns;software engineering environments	Tools and environments to aid developers in producing software have existed, in one form or another, since the early days of computer programming. They are becoming increasingly crucial as the demand for software increases, time-to-market decreases, and diversity and complexity grow beyond anything imagined a few decades ago. In this paper, we briefly review some of the history of tools and environments in software engineering, and then discuss some key challenges that we believe the field faces over the next decade.	complexity;computer programming;software engineering	Harold Ossher;William H. Harrison;Peri L. Tarr	2000		10.1145/336512.336569	personal software process;verification and validation;computing;software engineering process group;separation of concerns;software verification;search-based software engineering;computer science;systems engineering;engineering;social software engineering;component-based software engineering;software development;software engineering;software construction;software walkthrough;programming language;resource-oriented architecture;software deployment;computer-aided software engineering;software requirements;software system;computer engineering	SE	-64.77719073748415	25.95154956912928	64547
98d0abc328d9db7453da828a38aeebabb3216fc4	anatomy of a process mapping workshop	development process;process management;process design;process planning;insurance companies;insurance;business process	Many tools are in use for representing and analyzing business processes, but little information is available on how these tools are used in practice by process design teams. This paper analyzes one process mapping workshop in detail. Over three days, two facilitators and five representatives of the organization and business functions redesigned the core auto insurance business at a mid-size Swiss insurance company. The mapping tool used during the session was IDEF0. The purpose of this paper is to share our experiences in using IDEF0 in the workshop setting. In addition to a narrative description of the workshop, we offer our observations on how such workshops can be conducted effectively and on the strengths and weaknesses of IDEF0 in this context. The final business process map did not emerge from a logical, linear development process. Rather, the workshop was characterized by constant refinement and development of an existing structure, punctuated by an occasional radical idea that forced the group to throw out the current process and start over. The hierarchical approach of IDEF0 proved critical in keeping the group focused on its task of abstracting the essence of the process itself from the details of current practice. The mapping tool proved to be less convenient for representing a sequence of events in time, multiple cases, and conditional flows of work. The setting Secura Insurance is a mid-size, all-lines Swiss insurance company, with annual premium income of $350 million, or approximately 1.6 per cent of the Swiss insurance market. Secura was started in 1959 by the MIGROS business empire. MIGROS, which was founded in 1925 by Gottlieb Duttweiler, is a unique Swiss institution. Originally begun as an effort to provide low-cost food staples to the rural population, it has grown over the years to dominate Swiss retail food sales with a market share around 25 per cent. It has also branched out into banking, insurance, general retailing, travel, communications, education, and other businesses. In 1993, Secura undertook an ambitious effort to redesign its business processes and to introduce workflow management technology (Blaser and Meiler, 1996). Pilot projects were carried out with two software integrators, IBM and UNISYS. The winning firm was UNISYS, which planned to use the Staffware workflow engine, also called Staffware, on this project. However, the software had to be converted to the OS/2 operating system for compatibility with Secura's existing systems infrastructure. The resulting system failed to perform adequately and was eventually abandoned. This failure was owing to a combination of technical problems (involving Smalltalk and OS/2) and conflicts between the two separate teams involved in the implementation, one The current issue and full text archive of this journal is available at http://www.emerald-library.com	archive;business process;idef0;industrial and organizational psychology;os/2;operating system;refinement (computing);smalltalk;switzerland;workflow engine	Jan Fülscher;Stephen G. Powell	1999	Business Proc. Manag. Journal	10.1108/14637159910283029	process design;economics;insurance;computer science;systems engineering;engineering;knowledge management;artifact-centric business process model;business process management;marketing;operations management;process modeling;business process model and notation;process management;business process;process mining;business process discovery;management;business process modeling	HCI	-67.97815477462302	9.120082509546402	64552
2735c9ffec123c36979a0eaf23e3402930572de2	an overview of moonlight applications test automation	test automation;user interface;mono project automation;moonlight	New generations of web applications are developed by new technologies like Moonlight, Silverlight, JAVAFX, FLEX, etc. Silverlight is Microsoft's cross platform runtime and development technology for running Web-based multimedia applications in windows platform. Moonlight is an opensource implementation of the Silverlight development platform for Linux and other Unix/X11-based operating systems. It is a new technology in .Net 4.0 to develop rich interactive and attractive platform independent web applications. User Interface Test Automation is very essential for Software industries to reduce test time, cost and man power. Moonlight is new .NET technology to develop rich interactive Internet applications with the collaboration of Novel Corporation. Testing these kinds of applications are not so easy to test, especially the User interface test automation is very difficult. Software test automation has the capability to decrease the overall cost of testing and improve software quality, but most testing organizations have not been able to achieve the full potential of test automation. Many groups that implement test automation programs run into a number of common pitfalls. These problems can lead to test automation plans being completely scrapped, with the tools purchased for test automation becoming expensive. Often teams continue their automation effort, burdened with huge costs in maintaining large suites of automated test scripts. This paper will first discuss some of the key benefits of software test automation, and then examine the most common techniques used to implement software test automation of Moonlight Applications Test Automation. Finally, we discussed test automation and their potential of online test automation.	internet;javafx;linux;microsoft silverlight;microsoft windows;open-source software;operating system;software quality;software testing;test automation;test script;unix;user interface;web application	Appasami Govindasamy;K. Joseph SureshJoseph	2010	IJCOPI		embedded system;home automation;simulation;engineering;process automation system;operating system;totally integrated automation	SE	-69.15942158609305	28.985948354444734	64577
7e2145ea5b7dd4d01ef95e6015b8b050c5f88e76	effect of delays on complexity of organizational learning	organizational learning;cognitive map;complejidad espacio;gestion entreprise;modele entreprise;competitividad;delay effect;carte cognitive;simulation;espace etat;efecto retardo;firm management;search strategy;complexity;long terme;modelo empresa;long term;business model;largo plazo;state space method;mapa cognitiva;computer experiment;methode espace etat;heterogeneidad;effet retard;state space;retard;strategie recherche;path dependence;competitiveness;space complexity;administracion empresa;complexite espace;espacio estado;retraso;competitivite;antecedent;organizational performance;heterogeneity;heterogeneite;metodo espacio estado;estrategia investigacion;antecedente	We examine how delays between actions and their consequent payoffs affect the process of organizational adaptation. Formal conceptions of organizational learning typically include the assumption that payoffs immediately follow their antecedent actions, making the search for better strategies relatively straightforward. However, previous actions influence current organizational performance through their effects on organizational resources and capabilities. These resources and capabilities cannot be modified instantly, so delays -from actions, to changes in resources and capabilities, to altered organizational performance -are inevitable. Our computational experiments show that delays increase learning complexity and performance heterogeneity through two mechanisms. First, complexity of state-space and, therefore, of learning grows exponentially with delay length. Second, the time required to experience the benefits of long-term strategies means the intermediate steps of those strategies are initially undervalued, prompting premature abandonment of potentially fruitful regions of the strategy space. We find that these mechanisms often cause organizations to converge to suboptimal, routine-like cycles of actions, based on organizations’ continually updated cognitive maps of how actions influence payoffs. Furthermore, the evolution of these cognitive maps exhibits path-dependence, leading to heterogeneity across organizations. Implications for overcoming temporal complexity and the impact of initial cognitive maps are discussed.	cognitive map;converge;experiment;path dependence;state space	Hazhir Rahmandad	2008	Management Science	10.1287/mnsc.1080.0870	business model;organizational performance;complexity;organizational learning;simulation;computer experiment;economics;cognitive map;state space;artificial intelligence;operations management;heterogeneity;dspace	AI	-81.68009429243105	9.466786929703929	64589
a713991e0c1609338195595c4707ea8726d9008d	specifying information security needs for the delivery of high quality security services	instruments;information security;organization business vision;intersymbol interference;resource management;tellurium;it governance;security requirements;joining processes;security of data organisational aspects;information security requirements;solution concept;it governance information security requirements organization business vision service level management;security of data;service level management;information security intersymbol interference computational intelligence society instruments tellurium resource management context aware services joining processes;computational intelligence society;context aware services;organisational aspects	In this paper we present an approach for specifying and prioritizing information security requirements in organizations. We propose to explicitly link security requirements with the organizations' business vision, i.e. to provide business rationale for security requirements. The rationale is then used as a basis for comparing the importance of different security requirements. Furthermore we discuss how to integrate the aforementioned solution concepts into a service level management process for security services, which is an important step in IT Governance.	design rationale;itil;information security;requirement	Xiaomeng Su;Damiano Bolzoni;Pascal van Eck	2007	2007 2nd IEEE/IFIP International Workshop on Business-Driven IT Management	10.1109/BDIM.2007.375022	information security audit;computer security model;standard of good practice;cloud computing security;itil security management;security through obscurity;security information and event management;security engineering;security convergence;asset;knowledge management;environmental resource management;information security;information security standards;security service;business;security testing;network security policy;computer security;information security management	Security	-73.31703503143027	15.813179864274256	64626
6e4ab1b7352252133f331dafe00364becfa56970	smart knowledge capture for developing adaptive management systems	smart knowledge capture;spatial decision support;adaptive management	In this paper we describe how a well­designed Spatial Decision Support System (SDSS) may  facilitate  initial  (immediate) decision making, while establishing a robust foundation and framework for improving effectiveness over time as new data and knowledge becomes available. 'Smart Knowledge Capture' is a set of methods for rapidly developing a strong SDSS for adaptive management. We review some of the MIS tools used in Smart Knowledge Capture: multi­criteria decision analysis (MCDA) tools, online surveys, online knowledge portals, ontology systems, and describe the architecture of an SDSS  that stores  and  utilizes  this  knowledge.     We  illustrate  these  concepts  using  our  recent work  supporting  the  development of  a revised desert tortoise recovery plan.	decision analysis;disaster recovery plan;knowledge management;portals;spatial decision support system	Philip J. Murphy;Naicong Li;Roy C. Averill-Murray;Paul Burgess;Nathan Strout	2008			knowledge management;environmental resource management;data mining	AI	-65.41595425904458	4.238583456909452	64659
0b949c14787e6a7b44e8384610cd4c25ddeeb6c6	enterprise resource planning systems: an assessment of applicability to make-to-order companies	make to order mto;hf commerce;enterprise resource planning erp;applicability;literature review	Many vendors of Enterprise Resource Planning (ERP) systems claim their products are widely applicable–configurable to meet the needs of any business, whatever the product or service offering. But Make-To-Order (MTO) companies, which produce high-variety and bespoke products, have particularly challenging decision support requirements and it remains unclear whether ERP systems can meet their needs. This paper takes a contingency-based perspective of ERP adoption, assessing the fit or alignment between ERP functionality and a MTO production strategy. MTO features considered include: decision support requirements at critical Production Planning and Control (PPC) stages, idiosyncratic marketrelated features, typical company size and supply chain positioning, and shop floor configuration. It finds a substantial gap or misalignment between ERP functionality and MTO requirements; for example, between decision support provided by ERP systems and the decision support required by MTO companies at the customer enquiry and design & engineering stages. A research agenda for improving alignment is outlined, with implications for academics, MTO managers and ERP software developers. This includes: developing decision support tools that reflect the customer enquiry management activities of MTO companies; embedding MTO-relevant PPC concepts within ERP systems; and, conducting an in-depth empirical study into applications of ERP systems in MTO companies, assessing their performance impact. 2012 Published by Elsevier B.V.	bespoke;decision support system;erp;enterprise resource planning;requirement;software developer	Bulut Aslan;Mark Stevenson;Linda C. Hendry	2012	Computers in Industry	10.1016/j.compind.2012.05.003	systems engineering;knowledge management;operations management;management	SE	-77.44337312543165	9.616245407805708	64720
225511835bd7c138542b594ddb44218edec17897	investigating the relationship between activities of project management offices and project stakeholder satisfaction		Organizations need to execute a number of projects simultaneously in order to react market changes. Besides that, technological advancements enable large scale, complex projects feasible. Keeping all these projects under control and executing projects effectively is a serious problem for every organization. PMOs are appeared to be a solution for managing multi-project environments, but their contribution to organizations is tortuous. In this research, activity areas of PMOs are derived from existing works, and the relationships between PMO activity areas and stakeholder’s satisfaction are investigated by applying PLS-SEM methodology. There are six activity areas defined, and named as “project procurement”, “knowledge management”, “project team management”, “communication”, “project planning & follow up”, and “development and maintenance of IT”. With the guidance of existing literature research model is developed. Results show that “project planning & follow up”, and “project procurement” activities have direct effect on “stakeholder satisfaction” at medium level. Besides that, “knowledge management” and “IT infrastructure” have indirect effect through “project planning & follow up”. KeywORdS PLS-SEM, Project Management Office (PMO), Project Stakeholder Satisfaction, Supportive Activities for Projects		Dilek Özdemir Güngör;Sitki Gözlü	2017	IJITPM	10.4018/IJITPM.2017040103	systems engineering;engineering;operations management;management	HCI	-75.06924090872258	9.41099743570188	64738
81e9ee8d5f487b61b9d05468f38c894ed21bff69	avoiding failure by using a formal problem diagnosis process	system engineering;project management;formal model;generic model;project manager;systems engineering;team management;process design;formal method;team working;team work;system development;problem solving	Purpose – Seeks to provide an overview of a formal method for diagnosing and resolving problems that arise during the system development process.Design/methodology/approach – Using a communicative problem‐solving model proposed by Lumsden and Lumsden, the author applies the model to the problem diagnosis process in systems development by way of a case study.Findings – Applying a formal model to problem diagnosis and resolution provides a way for the development team to truly understand the problem it is trying to solve and the implications of the solution. When the model is used, the problem can be analyzed within its larger organizational context, thereby creating more effective solutions that will be welcomed and embraced.Originality/value – This paper will be of interest to systems developers and managers, especially those who need to diagnose and resolve issues within their larger organizational context. By situating the general model proposed by Lumsden and Lumsden within the systems development cont...		H. Frank Cervone	2006	OCLC Systems & Services	10.1108/10650750610640784	project management;process design;teamwork;knowledge management;management science	OS	-64.95018261666962	18.199982164729047	64790
fb77093f5a6fcc9256755c03aa9d357b530cd4a2	a computer-aided process simulation model to navigate value chain reconfiguration using a three-layered methodology	performance measure;integrated information system;key performance indicator;analytic hierarchy process;event driven process chains;process capability;decision maker;balanced scorecard;research methodology;value chain;architecture of integrated information system;systems thinking;key performance indicators;knowledge integration;event driven process chain;process simulation;critical success factor;simulation model	This paper develops a computer-aided process simulation model (CPSM) to navigate value chain reconfiguration. The model uses a three-layered methodology incorporating the principle of knowledge-integrated traceability (KIT) to create more efficient performance measures and provides a rigorous and stepwise clustered module structure that will act as a guideline for all entities decision makers are involved. The model is based on three kinds of mature toolsets variable. First, an in-depth interview of a group of experts makes the BSC (Balanced Scorecard) KIT model along with AHP (Analytic Hierarchy Process) technique into an integrated AHP-BSC structure. Second, internal validation of core propositions during CPSM development is then used to construct an account both qualitative and quantitative decision factors in the best delivery causality diagram of theory-building mode. Defining the KIT of toolset variables into two specific themes allows a practical hierarchy for AHP-BSC structure to be evaluated without excessive computation barriers, and the systems thinking archetype of strategic activities in the ARIS-EPCs (Architecture of Integrated Information System/extending Event-driven Process Chains) architecture delivers the validity and reliability of research methodology as validation model of CPSM development. Third, the hybrid ST-ARIS (System Thinking) simulation model is constructed based on the feasibility test of relevant decision factors whether or not KPI (Key Performance Indicator) could be measured. The reconfiguration provides new insights for contributing to the foundations to link the process capability of the value chain with the three-layered methodology, and emphasises the CPSM development by highlighting a specific approach associated with KIT.	simulation	Chung-Chou Tsai;Sununta Siengthai;Donyaprueth Krairit;Lalit M. Johri	2011	JIKM	10.1142/S0219649211003036	process simulation;knowledge integration;economics;knowledge management;marketing;operations management;performance indicator;management	EDA	-77.19273393641238	8.83591765783866	64839
65d229de557cfab5f0deb1e87ed03f52116c2196	engineering security vulnerability prevention, detection, and response		Around the turn of the 21st century, practices began to emerge to guide teams toward engineering software to stop attackers and users from utilizing unintended functionality by violating the system designer’s assumptions to cause a security breach. Yet, breaches are reported daily in the news in all domains—from the casual to the critical. The goal of this article is to help software engineers, software engineering educators, and security researchers understand opportunities for education and research through an analysis of current software security practices. The analysis is conducted on data on the use of a subset of 113 software security practices by 109 firms over 42 months, as reported in the Building Security In Maturity Model (BSIMM) Version 8 report. This article is part of a theme issue on software engineering’s 50th anniversary.	application security;capability maturity model;software engineer;software engineering;systems design;vulnerability (computing)	Laurie A. Williams;Gary McGraw;Sammy Migues	2018	IEEE Software	10.1109/MS.2018.290110854	systems engineering;computer science;software development;software security assurance;software bug;casual;vulnerability (computing);software;capability maturity model	SE	-67.3227682206545	27.989962342711646	64880
d0efbeab1127d11c81048783bb5eb447b69d6023	achieve agile enterprise system through collaboration with bpms	groupware;corporate modelling;groupware business data processing corporate modelling;bpms collaboration standardization agility enterprise system;business data processing;collaboration monitoring standards business process re engineering industries best practices;sap enterprise systems agile enterprise system packaged enterprise systems backbone it systems business process management technologies business process management suites enterprise system implementation es bpms collaboration three level solution model ibm bpm;enterprise system;agility;bpms collaboration;standardization	Packaged enterprise systems (ES) have achieved strategic significance in enterprises as their backbone IT systems since late 80s of last century. They provide rich and standard functionality to support business running. However, the concern that packaged enterprise systems are not agile enough has been discussed for decades. This drawback becomes more and more critical because of the more and more fierce competition. Enterprises are embracing business process management technologies and tools as the solutions of pursuing agility. In recent years, collaboration between packaged enterprise systems with business process management suites (BPMS) becomes more and more important to achieve a balance between standardization and agility. Firstly this paper reviews the history of applying business process management principles, technologies and tools into enterprise system implementation. To achieve a lifecycle ES-BPMS collaboration, this paper proposes a three level solution model with some key considerations for applying the solution into practices. At last this paper lists some existing offerings of enterprise system and BPMS collaboration from major packaged enterprise system and BPMS vendors, also introduces our first attempt with IBM BPM and SAP Enterprise systems.	agile software development;beam propagation method;business process;enterprise system;internet backbone;software deployment	Qinhua Wang;Changrui Ren;Feng Chen	2012	2012 Annual SRII Global Conference	10.1109/SRII.2012.61	functional software architecture;enterprise system;enterprise application integration;enterprise systems engineering;enterprise software;enterprise modelling;systems engineering;knowledge management;integrated enterprise modeling;process modeling;enterprise architecture management;process management;enterprise data management;enterprise architecture;business;enterprise integration;business process modeling;enterprise planning system;enterprise information system;business architecture;enterprise life cycle	OS	-72.49944078231279	8.319411056988056	64911
e03b5c58cfc022e29bf085e1852d2ac22636ef5a	openness of information resources - a framework-based comparison of mobile platforms	open innovation;information management;openness	In the smartphone sector we are discerning a competition between Symbian, Apple, and Google for the dominating mobile platform. In the design of their mobile platforms and operating system these organizations use varying degrees of “openness” on different platform elements. We follow a design science approach to construct and apply a framework to better understand how these organizations vary the degrees of openness of different information resources in order to create successful mobile platforms.	mobile device;mobile operating system;openness;smartphone;symbian	Daniel Schlagwein;Detlef Schoder;Kai Fischbach	2010				Mobile	-81.61405841594723	10.871688961177892	64915
baa2f317562937b1a1e961ccddf347365bdddc7c	in praise of use cases – a paean with a software accompaniment		ABSTRACTThis article reminds readers of the benefits that use cases bring to the software development process. Use cases, as featured in the UML (Unified Modeling Language), are contrasted with the much terser “user stories” favoured by agile methods. With their normal and alternative flows, and extending and included behaviours, use cases encourage developers to consider actor–system interaction in detail, preparing the way for coherent mechanisms of interacting and inheriting objects that realise the required functionality. The textual and visual representation of use cases has a simplicity that encourages discussion of requirements among developers, and between developers and clients, but only if “use case basics” are understood and applied consistently. An innovative use of educational software is proposed, to alleviate some fundamental but recurring difficulties, and to give students in large cohorts the benefit of focussed tuition and feedback. The approach will appeal to educators in software engin...		Ian O'Neill	2018	Computer Science Education	10.1080/08993408.2018.1472949	knowledge management;software development process;praise;unified modeling language;software;educational software;agile software development;user story;computer science;use case	Logic	-62.979403494752034	19.02222640144041	65011
e56fb715ffcf802b873b19abfb63e8c8f546e8ad	gaining competitive advantage, or how to succeed as vice-president of information systems	bargaining power;competitive strategy;business strategy;success factor;information system;competitive advantage	Most vice-presidents of informations systems do not succeed or fail because of technical factors. Rather, their careers rise or fall according to how they meet the organization's overall business objectives.  This paper discusses in detail a methodology that can be applied to any business and/or industry. The approach has helped organizations, and their information executives in particular, to focus on the factors that must go right for information systems to be a critical component of the overall business strategies.  The paper discusses competitive strategies and the five forces that give them impetus: the relative bargaining power of buyers/consumers, the relative bargaining power of suppliers, the rivalry among existing firms, the threat of new entrants, and the threat of substitute products and/or services. The challenge of the information systems executive is to apply technology to help the organization gain a competitive advantage in these areas.  The methodology goes through five steps, including assessment of the industry's information systems technology component, how to measure and plot the risk/change relationship for the organization, how to develop a probability-of-success factor for the organization, how to measure the organization's specific risks, and last, how to develop organizational action steps.	information system	M. Victor Janulaitis	1984		10.1145/1499310.1499377	marketing;operations management;business;strategic information system;management;competitive advantage	ECom	-80.39717850383634	6.574657023993585	65029
273e312c755dcd08cbad5025753e532fde011b25	defense acquisition system simulation studies	modeling and simulation;department of defense;weapon system;complex system;acquisition;process model;system simulation;afit;discrete event simulation	"""A systems engineering process model for the acquisition of large, complex systems for the U.S. Department of Defense (DoD) is being adapted for ongoing experiments in acquisition process policies. The discrete-event simulation model of the larger """"enterprise of acquisition"""" for weapon systems has a broad scope from program beginning through development. It reveals some of the challenges and risks in weapon system acquisition.  Initially the model was used to evaluate potential policies as interventions and/or system changes in an Air Force context. The simulation results showed varying degrees of influence on program outcomes and suggested no single antidote exists for solving acquisition problems. Many of the negative outcomes reflected through cost and schedule overruns are due to the behavior of the acquisition system itself.  A collaboration between the Air Force Institute of Technology (AFIT), the Naval Postgraduate School (NPS), and The Aerospace Corporation is underway to translate the model and adapt it for new considerations. Shortly we will propagate new model versions and results to the public, and use it for additional Air Force and Navy programs of concern."""	complex systems;experiment;process modeling;simulation;systems engineering	Joseph R. Wirthlin;Dan X. Houston;Raymond J. Madachy	2011		10.1145/1987875.1987906	simulation;engineering;operations management;operations research	Robotics	-68.15388867763683	17.186888901993896	65102
ca4ba07029d38fbd367ea07d9ae2cc73c2bddd28	guest editors' introduction implementing quantitative sqa: a practical model	application software;software maintenance;software maintenance productivity programming software quality costs application software guidelines software design humans air transportation;air transportation;development process;guidelines;humans;productivity;software design;programming;software quality	Considering the product is not enough in SQA you must also consider the development process. This model addresses both needs. B ecause software affects more and more aspects of our life, the costB effective development and maintenance of high-quality software is increasingly important. Software quality assurance (SQA) has become an indispensable dimension of software development, designed to guarantee that quality and productivity requirements are fulfilled. We put this special issue together to help people understand the increasing importance of SQA as an essential part of software projects, outline some new ideas and approaches to SQA, and report on some practical experiences.	requirement;software development;software quality assurance	Victor R. Basili;H. Dieter Rombach	1987	IEEE Software	10.1109/MS.1987.231408	programming;personal software process;long-term support;verification and validation;productivity;application software;software sizing;computer science;systems engineering;engineering;package development process;backporting;software design;social software engineering;software framework;component-based software engineering;software development;software engineering;software construction;software walkthrough;resource-oriented architecture;software maintenance;software deployment;software development process;software quality;aviation;software quality analyst;computer engineering;software peer review	SE	-63.313774070068234	27.310366233427292	65219
6eec7549b631cec84e9470736cbbcf2119936f3a	estimating software projects based on negotiation	collaborative work;web information system;estimation method;software engineering;software development;historical data	The Software Engineering community has been trying to get fast and accurate software estimations for many years. Most of the proposed methods require historical information and/or experts’ judgment. Because of that, the current methods are not suitable for novice developers or persons who do not know the company development capability. In order to help overcome such need, this paper proposes a software estimation method named CEBON (Collaborative Estimation Based On Negotiation). The method is applicable to small/mediumsize projects (1-6 months). It focuses on supporting estimation of Web information systems in scenarios where historical data is not available. The CEBON method has been used to estimate eight real projects. The obtained results were compared with the real projects execution, which were carried out by novice developers in Chile. The comparison indicates the method is able to deliver quite accurate results. In addition, a survey applied to the involved developers shows they feel comfortable using the estimation method. The article also describes a collaborative software application supporting the CEBON process and a preliminary evaluation of both the estimation method and the supporting tool.	collaborative software;communications protocol;cost estimation in software engineering;information system;newton's method;requirement;software project management	Sergio F. Ochoa;José A. Pino;Fabian Poblete	2009	J. UCS	10.3217/jucs-015-09-1812	computer science;software development;software engineering;data mining;world wide web	SE	-67.31335619691374	26.244845915606806	65263
5036c4d20bdcaf2115df50d132d163ec421dd202	on the evolution of lehman's laws	software architecture;software evolution;lehman s laws	In this brief paper, we honour the contributions of the late Prof. Manny Lehman to the study of software evolution. We do so by means of a kind of evolutionary case study: First, we discuss his background in engineering and explore how this helped to shape his views on software systems and their development; next, we discuss the laws of software evolution that he postulated based on his industrial experiences; and finally, we examine how the nature of software systems and their development are undergoing radical change, and we consider what this means for future evolutionary studies of software. I. LEHMAN’S INTELLECTUAL JOURNEY Meir “Manny” Lehman did not follow a traditional career path for an academic. His father died when he was young, and Lehman had to enter the workforce to help support his family instead of attending university.1 He got his first job in 1941 “performing maintenance on” (i.e., repairing) civilian radios; in England at the height of the second world war, this was a job of real importance. For the most part, his work involved replacing the components that the “tester” (or “debugger”, in software engineering parlance) had determined to be problematic. He found the work repetitive and dull; he decided that he really wanted to be a tester. One day, his foreman called in sick and Lehman was allowed to do testing, but when the foreman returned, Lehman was told to go back to maintenance. When Lehman protested that he thought he had been promoted, the foreman replied “Well, you’re not paid to think” [9]. Lehman would later dedicate a large portion of his life to demonstrate that those who do “maintenance” of software should be paid to think too. Like many other pioneers of computer science, Lehman lived the computer revolution from its conception. His early work was dedicated to building some of the first computers, and by the end of his life he was witnessing the ubiquity of mobile computing. It is likely that he would have spend his life working on hardware, had it not been for the few years he spent at IBM between 1964 and 1972. IBM had originally hired him to help build physical computers; but in 1968 in a radical change of direction, he was asked to investigate programming practices within the company.2 This project took him to study the development of the operating system IBM S/360 and its successor, IBM S/370. 1He would later attend Imperial College London, where he received a PhD in 1957. 2According to Laszlo Belady, “Lehman at the time ’was on the shelf’ [...] IBM never fired anyone. Instead, they put them ’on the shelf.” [1] To put it into context today: IBM S/360 was an operating system for the IBM 360, a computer with up to 8 MBytes of memory; its fastest models could not reach speeds of 0.2 MIPS.3 Lehman discovered that programmers were becoming increasingly interested in assessing their productivity, which they measured in terms of daily SLOCs and passing unittests. He noticed that productivity was indeed increasing, but at the same time the developers appeared to be losing sight of the overall product. In his words, “the gross productivity for the project as a whole, and particularly the gross productivity as measured over the lifetime of the software product had probably gone down” [9]. It was at this time that he developed a close friendship with Laszlo Belady. Together they would challenge the prevailing models and assumptions of software maintenance processes, and champion the study of software evolution as a field in its own right. In 1972, Lehman left IBM to join Imperial College London, where he would continue his work in software engineering research.4 While he did not consider himself a programmer, his work at IBM had allowed him to study and understand programmers and their products better than most. He had witnessed first-hand the challenges of producing industrial programs for a real-world environment. He observed that the processes involved in developing and maintaining software formed a kind of feedback system, where the environment provided a signal that had profound impact upon the continued evolution of the system. Lehman’s engineering-influenced views on software systems and their development were in stark contrast to other well known computer scientists, such as Edsger Dijkstra, who had a more formal view on what a programs is. For Dijkstra, a program was a mathematical entity that should be derived, iteratively, from a formal statement what it was supposed to do. In this model, you start with a precise specification, and then implement a series of formal “step-wise” refinements, gradually making it more concrete, and ultimately executable. He preferred “formal derivation of program from spec through a series of correctness-preserving transformations” over more informal and traditional views of software development, and he championed the view that teaching programming should 3The project manager was Fred Brooks, later to become a professor and win the Turing Award. 4He was also instrumental in creating, at Imperial College, one the first programs in software engineering. emphasize creating a specification and then progressively transforming it into a program satisfies it [5].5 Lehman recognized the value of Dijkstra’s position, but at the same time felt that it was not a practical model for the problem space of industrial software or for the style of development he had observed at IBM. So he postulated that programs could be divided into two main categories: S-type programs, which are derived from a rigorous specification that is stated up-front, and can be proven correct if required; and E-type (evolutionary) programs, which are strongly affected by their environment and must be adaptable to changing needs [8]. In his view, E-type systems are those that are embedded in the real world, implicitly suggesting that S-type programs were less common — and thus less important — outside of the research world.6 II. LEHMAN’S LAWS OF EVOLUTION In Lehman’s view, “The moment you install that program, the environment changes.” Hence, a program that is expected to operate in the real world cannot be fully specified for two reasons: it is impossible to anticipate all of the complexities of the real world environment in which it will run; and, equally importantly, the program will affect the environment the moment it starts being used. As time passes the environment in which the software system is embedded will inevitably evolve, often in unexpected directions; the environment of a program — including its users — thus becomes input to a feedback loop that drives further evolution. A program might, at some point, perfectly satisfy the requirements of its users, but as its environment changes, it will have to be adapted to continue doing so. In the words of Lehman, “Evolution is an essential property of real-world software” and “As your needs change, your criteria for satisfaction changes”. Over time requirements will change, and software must evolve to continue to satisfy these new requirements. If the environment is the one that drives the evolution, programmers are the ones who evolve the program. Lehman noted that evolving a software system was not an easy task. He summarized his observations in what we today call Lehman’s Laws of Software Evolution (adapted from [10], [4]): 1) Continuing change — An E-type software system7 that is used must be continually adapted, else it becomes progressively less satisfactory. 2) Increasing complexity — As an E-type software system evolves, its complexity tends to increase unless work is done to maintain or reduce it. 5There are symmetries between their positions and those of the disciplines of engineering and mathematics; this is reinforced by the fact that Lehman viewed software as a feedback system — a typical engineering model — while Dijkstra viewed it as a mathematical concept. 6Originally Lehman created a third category: P-type. P-type programs cannot be specified and their development is iterative. Later, he decided that Ptype programs were really a subset of E-type, and he reduced his classification to E-type and S-type programs only. 7Lehman used the term program, but we decided to update the descriptions to the more current term “software system”. 3) Self-regulation — The E-type software system’s evolution process is self regulating with close to normal distribution of measures of product and process attributes. 4) Conservation of organizational stability — The average effective global activity rate on an evolving E-type software system is invariant over the product lifetime. 5) Conservation of familiarity — During the active life of an evolving E-type software system, the content of successive releases is statistically invariant. 6) Continuing growth — Functional content of an E-type software system must be continually increased to maintain user satisfaction over its lifetime. 7) Declining quality — An E-type software system will be perceived as of declining quality unless rigorously maintained and adapted to a changing operational environment. 8) Feedback System — E-type software systems constitute multi-loop, multi-level feedback systems and must be treated as such to be successfully modified or improved. While Law 8 Feedback System was the last to be formulated, arguably it should have been the first to be stated, as its themes pervade the others. As discussed above, Lehman based his observations on the notion that real world software, once deployed, forms a feedback loop. The feedback comes from many different sources, including the stakeholders (e.g., they might want new features), the environment in which the system runs (technical — e.g., new versions of the operating system might render the software system unusable unless it is adapted — and non-technical — e.g., a system for tax management might need to be updated to changes in the taxation laws), and the system itself (e.g., its own defects might need to be fixed). Thi	acm turing award;computer science;computer scientist;correctness (computer science);debugger;embedded system;emoticon;essence;executable;experience;fastest;feedback;formal proof;fred (chatterbot);hoc (programming language);homeostasis;ibm system/360;ibm system/370;iterative method;lehman's laws of software evolution;lászló bélády;mobile computing;operating system;problem domain;programmer;requirement;software development;software engineering;software maintenance;software system;spec#;terminal emulator;third-party software component;type system;usability	Michael W. Godfrey;Daniel M. Germán	2014	Journal of Software: Evolution and Process	10.1002/smr.1636	software architecture;computer science;engineering;artificial intelligence;software evolution;software engineering;lehman's laws of software evolution;management	SE	-68.98600876702076	28.025670798319133	65393
ca15d0a316a7c766a01b912fab4ff0d771d75125	introduction of electronic book ordering with edifact in a special library: a case study	libraries;commerce electronique;europa;biblioteca empresa;allemagne;comercio electronico;standards;commande;case studies;data;boehringer mannheim;firm library;livre;change;purchasing;germany;foreign countries;electronic book;internet;estudio caso;guidelines;ordering systems;online systems;libro;electronic libraries;etude cas;control;special libraries;echange donnee informatise;computer software;productivity;europe;book;pharmaceutical industry;alemania;library acquisition;edifact;publishing industry;bibliotheque entreprise;electronic data interchange;electronic trade;library automation;cambio dato electronico	For some time, the central library at Boehringer Mannheim, a company in the research‐based pharmaceutical industry, has placed the majority of its book orders using EDIFACT. This article describes the situation before the introduction of the system, the technical and organisational preconditions, and the changes that were necessary to DP systems. The results are presented and the new situation of EDI versus the Internet is discussed.	e-book;edifact	Peter Stadler;Ernst Mernke;Martin Thomas	1999	The Electronic Library	10.1108/02640479910330110	productivity;the internet;telecommunications;computer science;edifact;electronic data interchange;sociology;management;operations research;law;world wide web;scientific control;data	Logic	-68.86397200518351	5.642469827478285	65406
cb60745b6c6b4ebd7bdfe9549e6eb9ccb2d7af71	computer safety, reliability and security		Software reliability engineering has recently been playing a rapidly increasing role in industry [1]. This has occurred because it carefully plans and guides development and test so that you develop a more reliable product faster and cheaper. In this paper we will first describe what software reliability engineering is. Then we will discuss the current state of the practice; that is, how industry is using it. The current “best” way of practicing software reliability engineering will be discussed. Finally, we will outline some of the important open research questions; solutions to these problems hold great promise for further advances.	kelly criterion;open research;reliability engineering;software quality;software reliability testing;winston w. royce	Alberto Pasquini	1999		10.1007/3-540-48249-0	software security assurance;reliability engineering;security analysis;computer security	SE	-63.25470415441538	29.902215455882043	65465
fb778248f7e832ae5f48260a6992d056b36a3934	process management practices and quality systems standards: risks and opportunities of the new iso 9001 certification	iso 9000;conceptual framework;process management;total quality management;quality system;quality systems;human activity;quality management	The attention focused on the complex set of human activities through which a company attempts to create value for its stakeholders is one of the most characteristic features of the total quality management (TQM) concept. This focalisation on the concept of process in the field of competitive quality is today mirrored in the prerequisites of normative quality, as the new ISO 9001: 2000 standard encourages a process approach to quality management. This article aims to investigate the relationship between the crucial aspects of process management and the normative indications of ISO 9001: 2000. To this aim, a conceptual framework is proposed which integrates the core components of process management methodologies described in the literature. The framework is used to analyse the requirements of the new quality system standard and to evaluate the conceptual advancement of ISO 9001: 2000 toward TQM principles and practices. The paper discusses how the harmonisation of normative requirements with TQM could entai...		Stefano Biazzo;Giovanni Bernardi	2003	Business Proc. Manag. Journal	10.1108/14637150310468371	iso/iec 9126;quality assurance;quality management;quality policy;quality management system;economics;iso 9000;systems engineering;engineering;knowledge management;environmental resource management;quality audit;management;quality of analytical results	DB	-76.24457004123124	8.360656713985039	65471
d258c546349a473cdb9df8c1d9c00fe9fa1d54e4	a review of incentive based demand response methods in smart electricity grids	demand response;incentives;smart grids;electrical systems	Smart electricity grid is a complex system being the outcome of the marriage of power systems with computing technologies and information networks. The information transmitted in the network is utilized for controlling the power flow in the electricity distribution grid. Thus smart grid facilitates a demand response approach, where grid participants monitor and respond to information signals with their electricity demand. This review paper focuses on a subclass of demand response methods and more particularly in incentive based demand response. It aims at providing a review of the existing and proposed methods while briefly explaining their main points and outcomes. In the current approach, the plethora of methods on incentive based demand response is grouped according to the tools adopted to implement the incentives. The overall goal is to provide a comprehensive list of incentive design tools and be a point of inspiration for researchers in the field of incentive based demand response in smart grids.	smart tv	Vasiliki Chrysikou;Miltiadis Alamaniotis;Lefteri H. Tsoukalas	2015	IJMSTR	10.4018/IJMSTR.2015100104	economics;marketing;operations management;demand management;smart grid;commerce	Robotics	-65.11824288170827	9.0818777562144	65490
536f41b4f59c55beb2a35f2775546879167d2869	adventures in adaptation: a software engineering playground!		A long research career has inevitably meant that the focus of our work may appear almost random, meandering through distributed computing, software architectures, requirements engineering and model checking! However, in retrospect, a rational reconstruction suggests that there might have been a thread which binds these research adventures together: that of dealing with change. The need to handle change, particularly at run-time, provides a wonderful set of challenges, making research into adaptive and self-managing systems a playground for software engineering researchers. We need to provide a set of comprehensive, consistent and pragmatic approaches to deal with challenges in aspects such as requirements goals and goal revision, domain modelling and model revision, planning and plan revision, and software configuration and reconfiguration. Based on our experiences, this talk will provide some insight into our approaches and suggest some recommendations for those that enjoy adventure playgrounds.	distributed computing;model checking;requirement;requirements engineering;retrospect (software);run time (program lifecycle phase);software architecture;software engineering	Jeff Kramer	2015	2015 IEEE/ACM 10th International Symposium on Software Engineering for Adaptive and Self-Managing Systems		domain analysis;requirements analysis;software requirements specification;verification and validation;simulation;software engineering process group;software configuration management;software verification;computer science;systems engineering;software design;social software engineering;component-based software engineering;software development;software engineering;domain engineering;software construction;software architecture description;requirements engineering;software walkthrough;resource-oriented architecture;software requirements;software system	SE	-64.94146739766565	24.211668761686425	65496
b7f05f28cc3e66b526bf15d6437e0b8c6be65cbd	how opuscapita used internal rpa capabilities to offer services to clients		Business process outsourcing (BPO) providers’ business models are based on their ability to provide services to customers more efficiently than client organizations can perform the tasks in-house. BPO providers have developed expertise in technology, processes and change management, and they often benefit from low-cost workforces. The large scale of their operations is the cornerstone for both the efficiency and the profitability of their businesses. However, their business models may now be threatened by a new technology called robotic process automation (RPA). The term RPA means “the automation of service tasks that were previously performed by humans.”3 RPA technology has evolved from the automation of repetitive tasks of a single user to a platform-based application that is capable of automating complex business rules and orchestrating hundreds of software “robots”4 to address large volumes of work. These robots mimic the way humans perform tasks. They can log in to systems using their own user names,	business process;change management (engineering);humans;login;outsourcing;process (computing);robot;user (computing)	Petri Hallikainen;Riitta Bekkhus;Shan Pan	2018	MIS Quarterly Executive			Robotics	-67.63266378216365	13.532031006274705	65497
c2e7da50035b70db53affcf0725206f17b996d85	key requirements for predictive analytical it service management - architectural key characteristics for a cloud based realization		While trying to maintain sustainable competitive advantage, IT service providers are challenged with tremendous service complexity and a low level of flexibility caused by the lack of transparency, constrained scalability and the missing ability to identify needed service measures proactively. For overcoming these challenges, this paper presents a well-evaluated set of identified key requirements for a feasible realization of a highly scalable cloud based architecture that supports predictive analytics in several domains of IT Service Management. This presented concept goes far beyond traditional approaches and pertinent state-of-the-art software solutions by focusing on business analyses based on knowledge creation and domain-independent knowledge sharing. The proposed approach is based on profound analyses of related work as well as modern service oriented design and business analyses paradigms. It provides semantic complexity handling, structured and multi-layered service interaction, cloud-enabled scalability management as well as predictive business analyses based on semantic reasoning, decision-making support and pattern recognition. The derived results eventually provide solution architects with a feasible and technical independent fundament for architectural implementation decisions. It ultimately enables IT service providers to cope with modern flexibility needs and complexity challenges and therefore to continuously satisfy customers to gain competitive advantage.	complexity;compute node linux;graphic art software;holism;itil;pattern recognition;relevance;requirement;scalability;service-oriented architecture	Christopher Schwarz;Hans-Peter Bauer;Lukas Blödorn;Erwin Zinser	2015		10.5220/0005490502970303	computer security	Web+IR	-72.46292078012569	12.033115928514158	65575
cfd098cf708e00c6b8c6ed1dd0cb7ac786a46def	design of the mass customization strategy for special purpose vehicles		As a personalized production model, mass customization can have both characteristics of low costs and differentiation, so it is more suitable for the car market where demand differentiation is significant. In the field of special-purpose vehicles, mass customization achieves economies of scale and market effects through modularization, differentiation and other ways. It combines advantages of the two modes of mass production and customized production, and also reflects a way of thinking that is to think from the view of customers.		Jianzhong Li	2011		10.1007/978-3-642-23235-0_71	simulation;mass customization;engineering;marketing;operations management	Vision	-73.5536309786554	4.554837582107899	65591
c8e6201cf07b7c1dff25189b278943a2a634f3f5	the extended enterprise?# [1]: advancing supplier-customer competitiveness	target costing;power density;economic value added;score;value;value chain;extended enterprise;best value;value added	"""A 1991 article [2] by one of the authors defined the impact that electronic process control has had on the ability to manufacture mechanical components to higher levels of performance more consistently and more reliably. These new manufacturing capabilities have enabled astute product designers to design machines that require less weight and space to transmit more power with greater reliability. These Power Density advances reduce the total cost of ownership by creating a more favourable balance between performance and cost. This paper suggests a model that effectively integrates the core competencies of all participants across the supply hierarchy to take advantage of Power Density concepts by building more effective partnering relationships between customers and suppliers. It points out the opportunity which senior management has to enhance such relationships and to create more value through synergies rather than destroying it through a persistently one-sided demand, """"Supplier, lower your price to me."""""""	competitive analysis (online algorithm);extended enterprise	Robert L. Leibensperger;James W. Griffith	2001	IJMTM	10.1504/IJMTM.2001.001400	economics;economic value added;value chain;engineering;value added;marketing;operations management;business value;power density;score;target costing;management;statistics;mechanical engineering	Crypto	-80.28612995608306	8.172399775245953	65605
0fef8c7895e4187dded28d85fdc8809327ea1dd2	exploring network modelling and strategy in the dutch software business ecosystem		In today’s product software market, the practices of re-use, partnering and 3rd party contracting give rise to complex software ecosystems. Over the duration of a product life-cycle, product software vendors build up relationships with their suppliers and other partners, which range from informal acknowledgements of each other’s presence to strategic alliances. There is still a lack of understanding surrounding the roles, connections, relationships, and resulting networks within software ecosystems. Using modelling techniques and statistical analysis, these networks can be used as tools to further that understanding. In this paper a collection of 67 software supply networks will be modelled as a network graph. Using clustering and two extensions of basic software supply network data, we identify several major players and domains in the Dutch software industry. Three business strategy perspectives are then related to the data to provide an example of their potential practical use.	business ecosystem;software business	Wesley Crooymans;Priyanka Pradhan;Slinger Jansen	2015		10.1007/978-3-319-19593-3_4	knowledge management;environmental resource management;management science	Crypto	-76.01472640255993	5.586456511436815	65749
01acdc5682e0711caaafc526b0c6686be28c3aa7	the engineering of speed and delivery		It has been my good fortune and great privilege to lead major projects in the semiconductor industry to automate production planning, delivery quotation and factory execution. Looking back, successful development and implementation of systems for managing on-time delivery and efficient factory operation always entailed empathy for the professionals in terms of understanding and appreciating the challenges they face, garnering thorough domain knowledge, designing an excellent manufacturing systems architecture, reaching consensus on more structured business rules for factory operation and operations planning, careful and complete data maintenance - automated as much as possible - and practical algorithms and logic fully addressing the challenges. It also required equipping professionals with new skills, information and perspective plus changes in job descriptions and performance evaluations to better align professional efforts with company value and with new manufacturing systems. Analytical approaches will be described for the entire production cycle: capacity planning, production planning and delivery quotation, and factory floor execution.	algorithm;align (company);database;semiconductor industry;systems architecture	Robert C. Leachman	2016	2016 Winter Simulation Conference (WSC)		electronics;simulation;systems engineering;engineering;industrial engineering	Robotics	-67.3488740360249	16.470867375600932	65774
419d3ab12e1e25b528e83491f45f41dfe0d6d879	vetting automatically generated trace links: what information is useful to human analysts?		Automated traceability has been investigated for over a decade with promising results. However, a human analyst is needed to vet the generated trace links to ensure their quality. The process of vetting trace links is not trivial and while previous studies have analyzed the performance of the human analyst, they have not focused on the analyst's information needs. The aim of this study is to investigate what context information the human analyst needs. We used design science research, in which we conducted interviews with ten practitioners in the traceability area to understand the information needed by human analysts. We then compared the information collected from the interviews with existing literature. We created a prototype tool that presents this information to the human analyst. To further understand the role of context information, we conducted a controlled experiment with 33 participants. Our interviews reveal that human analysts need information from three different sources: 1) from the artifacts connected by the link, 2) from the traceability information model, and 3) from the tracing algorithm. The experiment results show that the content of the connected artifacts is more useful to the analyst than the contextual information of the artifacts.	algorithm;information model;information needs;interviews;prototype;software development;traceability;tracing (software)	Salome Maro;Jan-Philipp Steghöfer;Jane Huffman Hayes;Jane Cleland-Huang;Miroslaw Staron	2018	2018 IEEE 26th International Requirements Engineering Conference (RE)	10.1109/RE.2018.00-52	data science;information model;systems engineering;traceability;requirements engineering;information needs;design science research;requirements traceability;tracing;computer science;vetting	SE	-75.21984950545047	22.930830259732424	65864
526c46f1889767a94f3da7661110f10ceebc89d1	development of an instrument to measure stress among software professionals: factor analytic study	occupational stress;software development process;exploratory factor analysis;software professionals;questionnaire survey;software industry	This study investigates sources of negative pressure among software professionals, from the perspective of the software development process. A multiple response questionnaire (survey instrument) was developed to measure sources of pressure among software professionals, based on a series of interactions with academicians, software professionals and senior software professionals employed in software industry. Ten key factors that cause stress in software professionals are identified using exploratory factor analysis from 156 usable responses. These ten factors are found to explain nearly two-thirds of the variance. The results indicate that stress resulting from fear of obsolescence and individual team interactions account for maximum variance. The results reveal that the stress levels are not high, among the respondents of the current study. The implications of the study are discussed.	exploratory factor analysis;interaction;software development process;software industry	K. S. Rajeswari;R. N. Anantharaman	2003		10.1145/761849.761855	reliability engineering;systems engineering;engineering;knowledge management	HCI	-71.5636421475728	22.62963808481546	65994
677612dbb086ebb77674e5da19e6d7a098d170d0	an empirical illustration to validate a floss development model using s-shaped curves	measurement;industries;software engineering;public domain software;software engineering public domain software;software evolution model floss development model s shaped curves open source software free libre oss software engineering homogeneous phenomenon software development model adapted staged model empirical method apache ivy;computer bugs adaptation models measurement data models open source software industries;adaptation models;computer bugs;open source software;data models	Open source software (OSS) or Free/Libre OSS (FLOSS) has become an interesting source of research in software engineering. However, it has been criticized that FLOSS development is often considered as a homogeneous phenomenon grounded by assumptions rather than empirical evidence. Proper empirical methods that can shed light into FLOSS development are desirable. In this paper, we propose an empirical method to validate a software development model for FLOSS, the Adapted Staged Model for FLOSS. We mined some selected metrics from Apache Ivy and study their evolution using S-shaped curves. Our results indicate that S-shaped curves can model software evolution well for Ivy. Moreover, we demonstrated that our method can be used to identify successfully different stages of its development, validating part of the Adapted Staged Model for FLOSS.	apache ivy;mined;open-source software;software development process;software engineering;software evolution	Ana Erika Camargo Cruz;Hajimu Iida;Norbert Preining	2013	2013 IEEE International Conference on Software Maintenance	10.1109/ICSM.2013.74	domain analysis;data modeling;personal software process;verification and validation;simulation;software bug;software sizing;computer science;systems engineering;engineering;package development process;backporting;social software engineering;component-based software engineering;software development;software engineering;software construction;empirical process;software analytics;software deployment;public domain software;goal-driven software development process;software development process;measurement;software system	SE	-66.48091625356001	30.065556862527018	66015
e55687da3176475275f8e106e02ebe4b44dc046e	python: characteristics identification of a free open source software ecosystem	public domain software object oriented languages;ecosystems open source software data mining computer languages biological system modeling bridges;python ecosystem free open source software ecosystem;public domain software;characteristics free open source software software ecosystem python exploratory case study roles;object oriented languages	Analysing a free open source software ecosystem can be beneficial and can help stakeholders in numerous ways. The analysis can help developers, investors, and contributors, to decide which software ecosystem to invest in and where to invest. Another reason for making an analysis is to assist ecosystem coordinators in governing their ecosystem. The paper provides an insight on the free open source software ecosystem of Python. It presents an analysis of the software ecosystem itself and the different characteristics it has. Based upon the conducted analysis with the available dataset, the research concludes that the free open source software ecosystem of Python contains three ecosystem roles that define its ecosystem. Next to that, it has grown exponentially from 31 active developers in 2005 to 5,212 December 2012. These results can help set up a strategy for the future of the Python ecosystem. At this point in time, it is necessary to make arrangements for the ongoing growth of the Python ecosystem. Failing to do so can lead to a growing number of unusable features, and eventually advance to an unhealthy ecosystem.	failure;open-source software;python;software ecosystem;usability	Rick Hoving;Gabriel Slot;Slinger Jansen	2013	2013 7th IEEE International Conference on Digital Ecosystems and Technologies (DEST)	10.1109/DEST.2013.6611322	computer science;systems engineering;package development process;database;world wide web	SE	-69.3607049300004	30.40395998066601	66051
b3651efd4fa7190179dbbe3c4d1ea811cf0a3152	mis planning: a methodology for systems prioritization	mis planning;systems prioritization	Abstract   The literature proposes a number of approaches that allow for the consideration of corporate goals and objectives in prioritizing information systems using both financial and non-financial criteria. Research in cognitive psychology suggests that an individual confronted with a simultaneous consideration of both qualitative and quantitative factors tends to assign greater salience to concrete factors than to more abstract criteria. This paper proposes a multi-dimensional methodology that allows for the prioritization of systems proposals based on attributes that are mostly qualitative as the first step in the resource allocation phase of MIS planning. This initial prioritization is used in conjunction with other quantitative factors to arrive at a final system portfolio. The methodology is illustrated with the aid of a case study conducted at a non-profit organization.	planning	Ritu Agarwal;Linda Roberge;Mohan Tanniru	1994	Information & Management	10.1016/0378-7206(94)90021-3	systems engineering;engineering;management science;simple prioritization;operations research	Robotics	-79.11697941999266	10.73084790738489	66153
25264720ffc0d02d341527cfb55fa4d636e8f46f	on knowledge transfer skill in pair programming	agile software development;collaboration;pair programming	Context: General knowledge transfer is often considered a valuable effect or side-effect of pair programming, but even more important is its role for the success of the pair programming session itself: The partners often need to explain an idea to carry the process forward. Goal: Understand the mechanisms at work when knowledge is transferred during a pair programming session; provide practical advice for constructive behavior. Method: Qualitative data analysis of recordings of actual industrial pair programming sessions. Results: Some pairs are much more efficient in their knowledge transfer than others. These pairs manage to (1) not attempt to explain multiple things at once, (2) not lose sight of a topic, (3) clarify difficult points in stages. Conclusions: Pair programming requires skill beyond software development skill. To be able to identify knowledge needs and then push such knowledge to or pull it from the partner successfully is one aspect of such skill. We characterize a number of its elements.	pair programming;problem solving;software development;software engineering;vagueness;dialog	Franz Zieris;Lutz Prechelt	2014		10.1145/2652524.2652529	simulation;pair programming;computer science;engineering;knowledge management;artificial intelligence;software engineering;agile software development;management;collaboration	AI	-72.29729528162709	24.91786298875881	66183
7f17660edaec7937b280a80c2fbb88fc39947c1e	applying structuration theory to investigate business process change	customer orientation;standards organizations;information technology;structuration theory;humans organizations interviews buildings information technology standards organizations;business process change;management of change business data processing;business data processing;international competitiveness;it service organization structuration theory international competition customer orientation business process change management;business process management;interviews;management of change;humans;organizations;buildings;business process	As international competition is still growing and customer orientation comes more into focus, organizations increasingly seek to understand and manage business processes. In order to achieve operational excellence many organizations try to transform or standardize business processes, prominently termed business process change. This paper provides new insights into the management of business process change by taking a structurational perspective. I conducted a case study in an IT service organization and tried to elicit what social processes exactly contribute to the success of business process change initiatives. Therefore, I made a finer distinction between a work and a build level of business process management to account for a more accurate investigation of underlying factors.	business process;customer relationship management	Daniel Grgecic	2011	2011 44th Hawaii International Conference on System Sciences	10.1109/HICSS.2011.73	business model;business analysis;business transformation;interview;business process reengineering;organization;knowledge management;artifact-centric business process model;business process management;business administration;marketing;business case;process modeling;management science;process management;business system planning;business process;business relationship management;business process discovery;management;business rule;new business development;information technology;philosophy of business;business process modeling;business architecture	DB	-74.91659774706623	8.803639842284086	66229
d4a196540808243e0304418cbc98ef1b3f278472	using bayesian network to estimate the value of decisions within the context of value-based software engineering		The software industryu0027s current decision-making relating to product/project management and development is largely done in a value neutral setting, in which cost is the primary driver for every decision taken. However, numerous studies have shown that the primary critical success factor that differentiates successful products/projects from failed ones lie in the value domain. Therefore, to remain competitive, innovative and to grow, companies must change from cost-based to value-based decisionmaking where the decisions taken are the best for that companyu0027s overall value creation. This paper details a case study where value-based decisions made by key stakeholders to select features for the next sprint of an Internet of Things (IoT) project, stored in a decisions database, were used to build and validate a value estimation model. This modelu0027s goal was to estimate the overall value contribution that each feature being discussed during a decision-making meeting would bring to the company, if selected for implementation. The estimation technique employed was Bayesian Network, and validation results were quite positive.	bayesian network;internet of things;mathematical model;software engineering;software industry;sprint (software development)	Emilia Mendes;Mirko Perkusich;Vitor Freitas;João Nunes	2018		10.1145/3210459.3210468	sprint;systems engineering;management science;project management;software;critical success factor;computer science;bayesian network;internet of things	SE	-69.98945214411341	21.974342309986373	66234
abf1d7f023c913795207fcbddf3c6ff4dd3f0b31	adam case. using upper case tools in software engineering laboratory	case tool	In general, CASE tools are a fundamental support to improve and automate the software development process in every stage of the project life cycle. From this point of view, it seems necessary that the future software engineers should be trained in the appropriate techniques to build software applications, but they must also take the opportunity to apply these techniques with the help of automated tools. However, these tools are not always available for teaching purposes, because of two reasons: the high cost of the tools licences and the enormous diversity in this area. In this article, ADAM CASE is presented. It is an upper CASE tool developed at the Computer Science and Automatics Department of the University of Salamanca. The main aim of the work is to solve the existing lack of CASE tools in Software Engineering laboratory course within technical studies of Computer Science in this University.	computer-aided software engineering	Francisco J. García-Peñalvo;M. N. Moreno;Ana María Moreno;G. González;Belén Curto;Francisco J. Blanco	2001			software engineering;software verification and validation;software development process;computer-aided software engineering;software construction;software;systems engineering;engineering	SE	-65.85456570828457	25.83335777101434	66324
fb8e9a00b9801cb5177dc9c7637272b3c1045a8b	vertical integration and information technology adoption: a study of the insurance industry	vertical integration;investments;technological innovation;web and internet services;information technology;information technology insurance investments web and internet services web server shape costs technological innovation network servers environmental economics;network servers;client server;shape;it investment;environmental economics;web server;technology adoption;organizational structure;information technology investment;insurance	This paper examines how differences in vertical integration and prior IT investments shape IT investment patterns within an industry. We focus on the insurance industry where information technology investment has had an important and sustainable impact and where longstanding, systematic differences in the extent of vertical integration in distribution across firms provides a natural way to examine the effect of vertical integration on technology adoption. We show that although less vertically integrated insurers had slower rates of Internet adoption ceteris paribus, less integrated insurers had greater prior investment in compatible client/server technologies which increased the rate of adoption. Without accounting for prior investments, one might conclude that less vertically integrated firms had higher rates of Internet adoption, ceteris paribus. These results demonstrate the importance of controlling for technology differences when examining the effects of organizational structure on IT investment.	client–server model;server (computing)	Chris Forman;Anne Gron	2005	Proceedings of the 38th Annual Hawaii International Conference on System Sciences	10.1109/HICSS.2005.670	organizational structure;vertical integration;insurance;shape;marketing;management;law;information technology;web server;commerce	Visualization	-81.20403763684506	6.143339644307374	66328
88776ac405a4f7a5cb6c19a448621d487cad20f4	monitoring platform emergence: guidelines from software networks		In this paper we explore how platforms emerge and evolve due to independent actions by companies providing them or launching products on them. We use the software industry as the setting for our study. We analyze the pattern of evolution for Windows, Unix, and Linux over 14 years. Based on this, we derive some lessons for companies aspiring to compete in settings where platforms and complementors play a major role. We support our analysis using visualizations.	emergence;linux;microsoft windows;music visualization;patterns of evolution;software industry;unix	Bala Iyer;Chi-Hyon Lee;N. Venkatramen;Dan Vesset	2007	CAIS		systems engineering;software engineering;computer engineering	Security	-69.47586791331891	28.958610464146087	66405
63e5ca0026a0d99d3ec2ddf103bddfc0fbf9c172	feature article - research portfolio for inventory management and production planning systems	inventory management;production planning	This paper provides a nontechnical survey of the status of inventory management systems. The central theme is an enumeration of practical problems that need research and analytic attention. These problems arise in the diagnostic, design, system performance forecasting, and implementation phases of real-life inventory management improvement projects. The paper describes many of the detailed aspects of such projects, and discusses where operations research has not yet provided sufficient scientific foundation to judge the merit of pragmatic alternatives for dealing with these problems.	inventory	Harvey M. Wagner	1980	Operations Research	10.1287/opre.28.3.445	inventory theory;economics;operations management;mathematics;management science;management	Robotics	-65.7494650982973	8.01849737073354	66418
76985df4afbf100a60cfc948841419e85c7d66fa	game-based learning in it service transition - the case of a mobile sales service by a small team in brazil	deployment time;itil;learning curve;game based learning;software deployment;motivation;bpm	IT Service Transition (ITST) is naturally challenging because it usually involves changes that run counter customer and provider staffs’ preconceptions, habits and established practices. Changes that affect implicit or explicit business processes (BP) adopted by the provider’s Service Transition team are particularly daunting since they may impact the team’s morale negatively and contaminate customer’s personnel who might be anxious to start with. Inability to properly implement and manage changes due to Service Transition process adjustments and retooling may lengthen deployment time, lower quality and even cause the provider to fail. In order to efficiently handle such changes, the provider’s ITST team must be motivated towards, trained in and quickly made proficient with new work tools, routines and practices. This paper provides preliminary evidence that blending Business Process Management (BPM) to gamification concepts and tools can accelerate learning in an IT Service Transition context. For that, we consider the case of a small IT Service provider in Brazil when transitioning a sales support IT service from Palm OS to Android	alpha compositing;android;beam propagation method;business process;gamification;itil;microsoft outlook for mac;mobile operating system;operating system;palm os;process modeling;sales force management system;software deployment	Thiago Paiva Brito;Josias Paes;J. Antão B. Moura	2014		10.5220/0004828401100116	service level requirement;simulation;motivation;information technology infrastructure library;business service provider;computer science;knowledge management;business process management;artificial intelligence;service delivery framework;operating system;software engineering;learning curve;management;software deployment;world wide web	HCI	-71.41488596271469	15.746797765242247	66419
e70554de9f7a9abe79711c8ac373430828605470	physical and logical security management organization model based on iso 31000 and iso 27001	document structure;18;iso 27001;iso 31000 topics 4;20;standards organizations;security iso standards risk management standards organizations organizations;risk analysis;security of data iso standards organisational aspects;physical assets physical security management organization model logical security management organization model iso 31000 standard iso 27001 standard security departments administration organisms document structure information assets;iso standards;risk management;security management;21;21 physical and logical security convergence risk analysis risk management iso 27001 iso 31000 topics 4 18 20;organizations;security;security of data;physical and logical security convergence;organisational aspects	This paper describes both the necessity of Physical and Logical Security management convergence and its implementation difficulty due to different organization models in most of the correspondent Security departments on enterprises and Administration organisms. This paper presents a methodology that makes it possible to comply with the ISO 31000 standard (for physical security) and ISO 27001 standard (for logical security) methodologies, analyzing simultaneously both information and physical assets. This paper presents an organization model proposal based on ISO 31000 standard (for physical security) and ISO 27001 standard (for logical security), and it integrates both models in the same company, being able to comply with both standards. The paper includes the proposed document structure for the model and a practical example of application.	iso/iec 27001:2013;logical security;physical security;security management	Koldo Pecina;Ricardo Estremera;Alfonso Bilbao;Enrique Bilbao	2011	2011 Carnahan Conference on Security Technology	10.1109/CCST.2011.6095894	computer security model;standard of good practice;iso/iec 9126;public relations;itil security management;certified information systems security professional;information security management system;iso 9000;knowledge management;information security;information security standards;security service;business;computer security	DB	-71.15028500639264	13.766744937963496	66435
2eb1461e3dea949ceca944303508ed36e31e5329	bilateral collaboration and the emergence of innovation networks	modelizacion;bilateral;collaborative r d;reseau information;processus innovation;entreprise;networks;recombinaison;empresa;innovation process;probabilistic approach;proceso innovacion;small world;espace parametre;information network;modelisation;knowledge;firm cooperation;innovation;enfoque probabilista;approche probabiliste;cognition;firm;espacio par metro;parameter space;cognicion;recombination;network formation;recombinacion;cooperation entreprise;innovacion;modeling;information gain;embeddedness;cooperacion empresa;red informacion;innovation network	In this paper, we model the formation of innovation networks as they emerge from bilateral decisions. In contrast to much of the literature, here firms only consider knowledge production, and not network issues, when deciding on partners. Thus, we focus attention on the effects of the knowledge and information regime on network formation. The effectiveness of a bilateral collaboration is determined by cognitive, relational, and structural embeddedness. Innovation results from the recombination of knowledge held by the partners to the collaboration, and its success is determined in part by the extent to which firms' knowledge complement each other. Previous collaborations (relational embeddedness) increase the probability of a successful collaboration, as does information gained from common third parties (structural embeddedness). Repeated alliance formation creates a network. Two features are central to the innovation process: how firms pool their knowledge resources, and how firms derive information about potential partners. When innovation is decomposable into separate subtasks, networks tend to be dense; when structural embeddedness is important, networks become cliquish. For some regions in this parameter space, small worlds emerge.	emergence	Robin Cowan;Nicolas Jonard;Jean-Benoit Zimmermann	2007	Management Science	10.1287/mnsc.1060.0618	innovation;economics;knowledge management;operations management;mathematics;management;statistics	ML	-86.67147554448363	4.650511324491445	66453
e3dba0d11b17194467688f6ff173ca166a5ab54a	simulation modelling and analysis of a border security system	performance measure;border security;modelizacion;evaluation performance;analisis estadistico;performance evaluation;securite;evaluacion prestacion;military simulation;statistical method;major element;system performance;modelisation;simulation experiment;militar;militaire;statistical analysis;system design;safety;analyse statistique;secure system;military;seguridad;modeling;simulation modelling	Border control is vital to the security of a nation and its citizens. All countries look at measures to improve the security of their borders. But increasing security can bring a substantial financial burden. In this study, we analyze the border security problem of Turkey using a simulation approach. Our main objective is to find more efficient ways of improving border control and security along Turkey’s land borders. To achieve this, we examine the structure of the border security system and its major elements, examine the relationships between performance measures, and assess the effectiveness of security elements on each system performance measure. We also look into the issues of planned changes and additional resources, and we evaluate new alternative system designs. The results of simulation experiments are analyzed by statistical methods. 2006 Elsevier B.V. All rights reserved.	experiment;simulation	Gökhan Çelik;Ihsan Sabuncuoglu	2007	European Journal of Operational Research	10.1016/j.ejor.2006.04.040	computer security model;military branch;simulation;systems modeling;security convergence;computer science;computer performance;operations research;network security policy;computer security;statistics;systems design	Security	-68.19053043879174	6.102578465545528	66540
d561fc128923f5bcca666bfed106e8f17ee636a3	a study of design flow of management experiments based on software engineering	experimental economics;analytical models;software;software engineering economics organisational aspects;experimental method;experimental management;instruments;design flow;biological system modeling;management experiments design flow;software engineering;organizational management;receivers;computational modeling;management experiments;software engineering experimental management management experiments experimental economics;engineering management software engineering innovation management research and development management technology management conference management financial management water conservation hydroelectric power generation educational institutions;economics;experience base;experience design;organizational management software engineering management experiments design flow;organisational aspects	Experiment is not the privilege of science, and it could also be adopted in social. An existed classifying of management experiment with some misunderstandings is reinterpreted and description in detail, according to the roles of IT in experiments. A design flow of management experiment is proposed based on successful software engineering, so as to make the process of management experiment design more reasonable and normative, which is crucial to apply the management experimental method to improve the organizational management application, to test and discover management theories.	design flow (eda);design of experiments;experiment;repeatability;software engineering;theory;verification and validation	Lan-juan Liu	2009	2009 Fifth International Joint Conference on INC, IMS and IDC	10.1109/NCM.2009.18	experience design;knowledge management;design flow;management science;experimental economics;computational model	SE	-76.50758518727856	16.518805589374985	66595
26e88e2163e228b4f273eca45e29e5a3483d65ac	the use of object-oriented models in requirements engineering: a field study	information model;object oriented model;object oriented methods;professional practice;formal model;selected works;conceptual model;conceptual framework;object oriented;requirement engineering;object oriented approach;bepress;information system;requirement specification;use case;field study	In many organizations, there has been a move toward the use of object-oriented methods for the development of information systems. Little is understood, or reported on the basis of research, of the use of object-oriented methods by practicing professionals in the production of requirements specifications for commercial or industrial sized projects. In this paper, we outline a conceptual framework of “what might be happening” in professional object-oriented requirements engineering based on the common characteristics of published, well known object-oriented methods. We then describe a research project and the findings from a set of six case studies that have been undertaken that examine professional practice from the standpoint of the epistemology contained in the conceptual model. In these studies, it was found that the more formal models of objectorientation were rarely used to validate, or even clarify, the specification with clients or users. Rather, analysts tended to use informal models, such as use cases or ad hoc diagrams, to communicate the specification to users. Formal models are more often used internally within the analysis team and for communicating the specification to the design team.	diagram;field research;hoc (programming language);information system;requirement;requirements engineering	Linda Dawson;Paul A. Swatman	1999		10.1145/352925.352949	meta-process modeling;use case;requirements analysis;information model;computer science;systems engineering;knowledge management;conceptual model;requirement;conceptual framework;requirements engineering;object-oriented programming;functional requirement;non-functional requirement;software requirements;information system;field research	SE	-63.90172758366132	18.55407643505369	66608
51a8bda94f99f3701eb2a3f7520d13f76e4ad23c	development, application, and institutionalization of a system simulation model for korean agricultural policy analysis	transport capacity;analytical models;food processing;policy analysis;resource management;testing;independent component analysis;multidisciplinary teams;aquaculture;feedback;agricultural sector;agricultural development;agricultural policy;analytical models independent component analysis agriculture testing aquaculture crops resource management production systems feedback couplings;production systems;component model;crops;agriculture;couplings;foreign trade;system simulation;simulation model	The Korean agricultural sector model (KASM) was developed in Korea by a multidisciplinary team of Korean and American researchers The eclectic approach used by the joint Korean-American team in developing the KASM set of models is explained. A brief description of the models, examples of their application, and progress toward their institutionalization into the Korean agricultural decision structure are given. The description of this work should be useful to other such teams engaged in similar activities. KASM is a system of five simulation models which can be used, independently or in conibination with one another, for agricultural planning and policy analysis by the Ministry of Agriculture and Fisheries. KASM's overall problem definition and structure are closely related to broad national values held by Korean policymakers with respect to agricultural development. Likewise, the five component models of the KASM system reflect key elements of any agricultural sector analysis: population; crop technology change; farm resource allocation and production; demand, prices, and foreign trade; and feedback linkages between agriculture and the rest of the economy. Each of the five models was tested independently and in combination as part of the credibility-testing process, in which model credibility is viewed as growing out of performance in five types of tests: coherence (validation), correspondence (verification), clarity, utility, and workability. KASM has been used on a number of occasions in analyses for preparation of Korea's Fourth Five-Year Plan, in analyzing grains price policy options, and in projecting food processing, marketing, and transportation capacity requirements.	requirement;system simulation	Michael H. Abkin;Tom W. Carroll;Fred A. Mangum;George E. Rossmiller	1979	IEEE Transactions on Systems, Man, and Cybernetics	10.1109/TSMC.1979.4310279	independent component analysis;aquaculture;agriculture;computer science;artificial intelligence;food processing;policy analysis;resource management;feedback	SE	-87.4243935512731	9.046411311240028	66620
040b6166373da5293a56c42743892bec34fc54f1	certification of ipavement applications for smart cities a case study		The installation of Intelligent Pavement (IPavement) in cities highlights the obvious need for the development of software services that can be offered by this technology. These services should be developed in conformance with international quality standards such as ISO/IEC 25000, which make it possible to give assurance that the services must established quality requirements. This paper therefore presents the environment for the quality certification of the services developed for the IPavement, created by the authors. This environment is formed by an assessment process, a quality model, and set of assessment tools. The results of a case study carried out to evaluate the quality of a service developed for IPavement are also set out; this study has tested the practical application of the environment created and has proven the need to develop tools to assist in the evaluation of the quality IPavement services.	conformance testing;requirement;smart city	Jesús Ramon Oviedo;Moisés Rodríguez;Mario Piattini	2015	2015 International Conference on Evaluation of Novel Approaches to Software Engineering (ENASE)		iso 26262;iso/iec 9126;reliability engineering;quality assurance;quality control;quality policy;information security management system;iso/iec 12207;iso 9000;systems engineering;engineering;software engineering;software quality control;quality of analytical results	SE	-62.901290364833	27.93872449597237	66651
1f569e1858594b008d539821d1dc0c27d5c25b64	long-term sustainability of open source software communities beyond a fork: a case study of libreoffice	technology;computer and information science;teknik;data och informationsvetenskap	Many organisations have requirements for long-term sustainable software systems and associated communities. In this paper we consider longterm sustainability of Open Source software communities in Open Source projects involving a fork. There is currently a lack of studies in the literature that address how specific Open Source software communities are affected by a fork. We report from a case study aiming to investigate the developer community around the LibreOffice project, which is a fork from the OpenOffice.org project. The results strongly suggest a long-term sustainable community and that there are no signs of stagnation in the project 15 months after the fork. Our analysis provides details on the LibreOffice developer community and how it has evolved from the OpenOffice.org community with respect to project activity, long-term involvement of committers, and organisational influence over time. The findings from our analysis of the LibreOffice project make an important contribution towards a deeper understanding of challenges regarding long-term sustainability of Open Source software communities.	committer;digital asset;existential quantification;fork (software development);libreoffice;open sound system;open-source software;requirement;software maintenance;software system;word lists by frequency	Jonas Gamalielsson;Björn Lundell	2012		10.1007/978-3-642-33442-9_3	systems engineering;engineering;software engineering;world wide web;technology	SE	-75.01280564851582	20.885054419305327	66733
0b607608ba875ee762ef3a2eedbed9c6b2750342	a case study in pervasive retail	trust;mobile;pervasive computing;field test;retail systems;fast moving consumer goods;business model;supply chain;gsm	In this paper we discuss the rationale for the development of MyGrocer, a second-generation pervasive retail system, as well as its implications for the fast moving consumer goods (FMCG) sector. We will only touch upon the technology infrastructure and the required technical developments since these have been discussed extensively elsewhere. The focus here is on the one hand, on the analysis of the business forces that dictate the development of pervasive retail and on the other, the implications and the opportunities for innovative business models offered by the dis-intermediation effect of pervasive retail on the supply chain of FMCG. The MyGrocer system has undergone two phases of field-testing and is expected to be fully operational by the end of this year. The development of the MyGrocer architecture is a collaborative effort between industry and academia within Europe.	design rationale;distributed interactive simulation;pervasive informatics	George Roussos;Juha Tuominen;Leda Koukara;Olli Seppala;Panos E. Kourouthanassis;George M. Giaglis;Jeroen Frissaer	2002		10.1145/570705.570722	marketing;advertising;business;commerce	HCI	-77.60876163985525	15.438934855151889	66743
4fe98275978aabe91626cb1915b0c2ad3ba7d7a0	a standardization framework for electronic government service portals	electronic services;electronic government	Although most eGovernment interoperability frameworks (eGIFs) cover adequately the technical aspects of developing and supporting the provision of electronic services to citizens and businesses, they do not exclusively address several important areas regarding the organization, presentation, accessibility and security of the content and the electronic services offered through government portals. This chapter extends the scope of existing eGIFs presenting the overall architecture and the basic concepts of the Greek standardization framework for electronic government service portals which, for the first time in Europe, is part of a country’s eGovernment framework. The proposed standardization framework includes standards, guidelines and recommendations regarding the design, development and operation of government portals that support the provision of administrative information and services to citizens and businesses. By applying the guidelines of the framework, the design, development and operation of portals in central, regional and municipal government can be systematically addressed resulting in an applicable, sustainable and ever-expanding framework.	portals	Demetrios Sarantis;Christos Tsiakaliaris;Fenareti Lampathaki;Yannis Charalabidis	2008		10.1007/b137171_81	knowledge management;database;world wide web	DB	-71.91297025729982	13.5106343097669	66792
c8a0840a9eb616d415e8102a4d763e92eb2747dc	mind the gap: can and should software engineering data sharing become a path of less resistance?		The facility to process data is, arguably, the defining capability underpinning the transformative power of software: the relationships of each to the other are deep and extensive. This is reflected in the degree to which both software engineering practitioners and researchers rely upon data to direct their endeavours. Ironically however, while both the industrial and research communities are dependent upon data these dependencies present a dichotomy. Practitioners can suffer an abundance of data, much of it dark, which they struggle to interpret and apply beneficially. Isolated by gaps between industry and academia researchers often find themselves lacking data, watching as their industrial counterparts pursue a different and distinct course of action.  Integrating evidence with experience gained in practice and through engagement with research this talk offers an industrial perspective on whether this situation can be improved upon; and what the benefits of achieving this outcome, particularly for practitioners, might be.	interpreter (computing);software engineering	Ken R. Wallace	2017	2018 IEEE/ACM 40th International Conference on Software Engineering: Software Engineering in Practice Track (ICSE-SEIP)	10.1145/3183519.3183541	software engineering;underpinning;computer science;software;transformative learning;digital commons;data sharing;software measurement	SE	-74.73586126130435	20.66298118997565	66823
5494cdb4589fdf4c609c4ad4dfa5bde0ea5cfe45	on the development of software tools for testing web service	web service		programming tool;web service	Tsung-Teng Cheng;Chih-Hsiung Fu	2004			systems engineering;software peer review;web modeling;software construction;cloud testing;system integration testing;package development process;web testing;software as a service;engineering	SE	-63.007502498073684	25.509183428161826	66897
393cddc2d3d7132722ef088cbdcc23eb799b8f6e	measuring slacks-based efficiency for commercial banks in china by using a two-stage dea model with undesirable output	benchmark;commercial bank;two stage dea;slacks based efficiency measure	Bank industry plays a critical role in the economic development of China. In this paper, we develop a new two-stage data envelopment analysis approach for measuring the slacks-based efficiency of Chinese commercial banks during years 2008–2012, where the banking operation process of each bank is divided into a deposit-generation stage (division) and a deposit-utilization stage (division). In the approach, the increase of desirable outputs and the decrease of undesirable outputs are simultaneously considered in order to identify the inefficiency of a bank. Three efficiency statuses are first defined for such a system to investigate its input-output performance and divisional performances, and a full efficiency status is then defined based on these statuses. The empirical results show that the improvement of the banks’ performances during this period was mainly contributed by the improvement of deposit-utilization stage. Besides, the results also show that our approach can provide a benchmark for the intermediate measures of the two stages of an inefficient bank.	australian bibliographic network;benchmark (computing);data envelopment analysis;performance	Qingxian An;Haoxun Chen;Jie Wu;Liang Liang	2015	Annals OR	10.1007/s10479-015-1987-1	actuarial science;benchmark;economics;operations management;mathematics;economy	ML	-84.52677639903126	7.379716163746924	66997
9d963fe1ac150f8f44b44dabb94e6b4e703ad241	symbolic execution and constraint solving (dagstuhl seminar 14442)	004;symbolic execution software testing automated program analysis constraint solvers	"""This report documents the program and the outcomes of Dagstuhl Seminar 14442 """"Symbolic Execution and Constraint Solving"""", whose main goals were to bring together leading researchers in the fields of symbolic execution and constraint solving, foster greater communication between these two communities and exchange ideas about new research directions in these fields.#R##N##R##N#There has been a veritable revolution over the last decade in the symbiotic fields of constraint solving and symbolic execution. Even though key ideas behind symbolic execution were introduced more than three decades ago, it was only recently that these techniques became practical as a result of significant advances in constraint satisfiability and scalable combinations of concrete and symbolic execution. Thanks to these advances, testing and analysis techniques#R##N#based on symbolic execution are having a major impact on many sub-fields of software engineering, computer systems, security, and others. New applications such as program and document repair are being enabled, while older applications such as model checking are being super-charged. Additionally, significant and fast-paced advances are being made in research at the intersection of traditional program analysis, symbolic execution and constraint solving. Therefore, this#R##N#seminar brought together researchers in these varied fields in order to further facilitate collaborations that take advantage of this unique and fruitful confluence of ideas from the fields of symbolic execution and constraint solving."""	constraint satisfaction problem;symbolic execution	Cristian Cadar;Vijay Ganesh;Raimondas Sasnauskas;Koushik Sen	2014	Dagstuhl Reports	10.4230/DagRep.4.10.98	computer science;theoretical computer science;concolic testing;algorithm	SE	-71.39654729367028	31.159504409734723	67059
e76c45fd55a773387fbe2155cf8fe7b1cedc48cc	complex entity management through eats: the case of the gascon rolls project	settore l lin 12 lingua e traduzione lingua inglese		gascon rolls	Eleonora Litta;Jamie Norrish;Miguel Vieira	2012			engineering;artificial intelligence;operations management;algorithm	DB	-91.52872148770265	24.56548348810077	67129
79a32d1a0e698c6dfb2b4fc376bc6eaf01f8580c	assessing and managing the benefits of enterprise systems: the business manager's perspective	operational benefits;strategic benefits;enterprise systems;it infrastructure benefits;perceived net benefit flow;organizational benefits;is effectiveness;business benefits;enterprise system;erp systems;managerial benefits	This paper focuses on the benefits that organizations may achieve from their investment in enterprise systems (ES). It proposes an ES benefit framework for summarizing benefits in the years after ES implementation. Based on an analysis of the features of enterprise systems, on the literature on information technology (IT) value, on data from 233 enterprise systems vendor-reported stories published on the Web and on interviews with managers of 34 organizations using ES, the framework provides a detailed list of benefits that have reportedly been acquired through ES implementation. This list of benefits is consolidated into five benefits dimensions: operational, managerial, strategic, IT infrastructure and organizational, and illustrated using perceived net benefit flow (PNBF) graphs. In a detailed example, the paper shows how the framework has been applied to the identification of benefits in a longitudinal case study of four organizations.	enterprise system;world wide web	Shari Shang;Peter B. Seddon	2002	Inf. Syst. J.	10.1046/j.1365-2575.2002.00132.x	enterprise system;computer science;systems engineering;knowledge management;management science;enterprise information system;enterprise life cycle	OS	-78.29239989753164	8.148144433058496	67136
9049c193b19fc7a12855a0e91f021c4b5695871d	software engineering for multi-agent systems iii, research issues and practical applications [the book is a result of selmas 2004].			multi-agent system;software engineering	Alessandro F. Garcia;Ricardo Choren;Carlos J. P. Lucena;Paolo Giorgini;Tom Holvoet;Alexander Romanovsky	2004		10.1007/b106347	search-based software engineering;computer science;systems engineering;social software engineering;component-based software engineering;software development;software engineering;software construction;software walkthrough;software requirements;software system	AI	-63.352900197089504	24.876889210525515	67137
d1ba5f4181e91247d7147e87728491a50bbda494	"""open access versus urheberrecht: wird das urhebergesetz dem medium """"internet"""" gerecht?"""	open access	Open access publications gain more and more importance. German copyright regulations do not oppose publications according to the principles of open access. However, they do not improve the conditions for the development and establishment of open access publications. The newest amendment of the German Copyright Act (Urhebergesetz) did not change this situation.		Heike Stintzing	2004			computer science;physics	ECom	-86.99635554648252	19.48014483140341	67189
b36e8d275b1e2d9bccc8011b37608c508cb1d254	a european study of e-business maturity and ict-benefits: is there a conditional relationship?	e business;ict benefits;firm size;ict adoption;information and communication technology;maturity	The present study explores the relationship between the e-business maturity and the perceived benefits from (Information and Communication Technologies) ICT at the firm level. We aim to debunk this relationship in terms of its strength and stability, and to explore the conditions which may influence it. Taking an economic approach, we hypothesise that the relation between e-business maturity and perceived benefits from ICT adoption will be influenced the costs of intra-organisational adaptations due to ICT. Using data from an European survey on ICT adoption and e-business maturity of the firms (N=7,072) we show that the correlation between the e-business maturity and the perceived benefits from ICT adoption is indeed positive, significant and stable over countries, industries, firm size and age. Further, the findings confirm the hypothesis that intra-organisational adaptations due to ICT moderate the positive correlation between a firms’ e-business maturity and perceived benefits from ICT.	capability maturity model;electronic business	Ronald Batenburg;Ioanna D. Constantiou	2009			information and communications technology;economics;operations management;electronic business;management;maturity;economic growth;commerce	HCI	-82.93115898963647	4.4631820214618845	67237
c36e4b5a082c83a071059da6e86befb0ac61ca63	the joint sales impact of frequency reward and customer tier components of loyalty programs	loyalty program;customer tier programs;segmentation;database marketing;frequency reward	We estimate the joint impact of the frequency reward and customer tier components of a loyalty program on customer behavior and resultant sales. We provide an integrated analysis of a loyalty program incorporating customers’ purchase and cash-in decisions, points pressure and rewarded behavior effects, heterogeneity, and forward-looking behavior. We focus on four key research questions: (1) How important is it to combine both components in one model, (2) Does points pressure exist in the context of a two-component loyalty program, (3) How is the market segmented in its response to the combined program, and (4) Do the programs complement each other in terms of the incremental sales they produce? Our most basic message is that the frequency reward and customer tier components of loyalty programs should be modeled jointly rather than in separate models. We find strong evidence for points pressure, for both the customer tier and frequency reward components, using both modelbased and model-free evidence. We find a two-segment solution revealing a “service-oriented” segment that highly values cash-ins for room upgrades and staying in “luxury” hotels, and a “price-oriented” segment that is more price sensitive and highly values the frequency reward aspects of the loyalty program. Further, we find that both components generate incremental sales. Also, there was slight synergy between the programs but not a huge amount. Overall, each component contributes to increased revenues and doesn’t interfere with the other.	multitier architecture;resultant;service-oriented device architecture;synergy	Praveen K. Kopalle;Yacheng Sun;Scott A. Neslin;Baohong Sun;Vanitha Swaminathan	2012	Marketing Science	10.1287/mksc.1110.0687	loyalty business model;marketing;operations management;advertising;customer retention;segmentation	SE	-82.76492002596332	6.740186343045113	67289
1131acd8bf83f5e9ea5197245e929c1043203065	a consolidated understanding of technical debt		Technical debt utilises financial debt as a metaphor to describe the phenomenon of increasing software development costs over time. Whilst this phenomenon is evidently detrimental to the long-term success of software development, it appears to be poorly understood in the academic literature. The absence of a clear definition and model for technical debt means that the notion of technical debt remains metaphorical, thus preventing the realisation of technical debt’s utility as a conceptual and technical communication device. This exploratory study reconciles the high-level, abstracted view of technical debt presented in academic literature. It establishes the boundaries of the technical debt phenomenon and develops a comprehensive theoretical framework to facilitate future research. The resulting theoretical framework portrays a holistic view of technical debt that incorporates a set of precedents and outcomes, as well as the phenomenon itself.	high- and low-level;holism;software development;technical debt;while	Edith Tom;Aybüke Aurum;Richard T. Vidgen	2012			software development;exploratory research;technical communication;realisation;computer science;management science;debt;metaphor;technical debt;phenomenon	Graphics	-75.88714570905537	12.446795407396849	67371
1a75f9fc8366eb89d2d56acda17a4abcf6075371	do feelings matter? on the correlation of affects and the self-assessed productivity in software engineering	emotions;empirical software engineering;feelings;affects;moods;psychological measurements	Background: software engineering research (SE) lacks theory and methodologies for addressing human aspects in software development. Development tasks are undertaken through cognitive processing activities. Affects (emotions, moods, feelings) have a linkage to cognitive processing activities and the productivity of individuals. SE research needs to incorporate affect measurements to valorize human factors and to enhance management styles. Objective: analyze the affects dimensions of valence, arousal, and dominance of software developers and their real-time correlation with their self-assessed productivity (sPR). Method: repeated measurements design with 8 participants (4 students, 4 professionals), conveniently sampled and studied individually over 90 minutes of programming. The analysis was performed by fitting a linear mixedeffects (LME) model. Results: valence and dominance are positively correlated with the sPR. The model was able to express about 38% of deviance from the sPR. Many lessons were learned when employing psychological measurements in SE and for fitting LME. Conclusion: this article demonstrates the value of applying psychological tests in SE and echoes a call to valorize the human, individualized aspects of software developers. It reports a body of knowledge about affects, their classification, their measurement, and the best practices to perform psychological measurements in SE with LME models.	best practice;cognition;human factors and ergonomics;large marine ecosystem;linkage (software);real-time transcription;software developer;software development;software engineering	Daniel Graziotin;Xiaofeng Wang;Pekka Abrahamsson	2015	Journal of Software: Evolution and Process	10.1002/smr.1673	simulation;emotion;feeling;engineering;management	SE	-71.87670184676566	22.611118747196322	67473
9921f11563a155c876a7074c7c949950ddb9086c	datawarehouse design for educational data mining	software;loading;data mining;organizations	Business intelligence (BI) builds upon a set of tools and applications that enable the analysis of vast amounts of information (Big Data). Educational institutions handle large volumes of Big Data every year. There is a strong need for the use of BI in these institutions to improve their processes and support decision making. The core technology in a BI project is a datawarehouse (DW). This paper describes the design considerations for the implementation of the DW in an educational scenario. The DW will be used in a knowledge discovery process to handle the information for the analysis of key performance indicators using educational data mining (EDM) techniques. The DW along with an enterprise architecture (EA) repository are the key technological assets of a knowledge management framework (KMF). This framework was designed to put order in the creation, capture, transfer and digitalization of knowledge. This guide and the framework are two of the outcomes of a research project in a private university. Furthermore, a case study suggests how to choose the best methodology in higher institutions. In the case study the steps for the DW design are presented. This study can be useful for academics and practitioners that plan to design a DW to analyze information using EDM techniques.	big data;dreamwidth;educational data mining;enterprise architecture;kevoree;knowledge management	Oswaldo Moscoso-Zea;Andres Sampedro;Sergio Luján-Mora	2016	2016 15th International Conference on Information Technology Based Higher Education and Training (ITHET)	10.1109/ITHET.2016.7760754	computer science;systems engineering;organization;engineering;knowledge management;artificial intelligence;operating system;data mining;management;world wide web	DB	-71.57872264966221	10.574777935893017	67558
bc5698d8d2f03f5e016c1c339da5005496e49de4	verdict machinery: on the need to automatically make sense of test results	performance regression test analysis;non functional testing oracle;verdict system;testing oracle;automation	Along with technological developments and increasing competition there is a major incentive for companies to produce and market high quality products before their competitors. In order to conquer a bigger portion of the market share, companies have to ensure the quality of the product in a shorter time frame. To accomplish this task companies try to automate their test processes as much as possible. It is critical to investigate and understand the problems that occur during different stages of test automation processes. In this paper we report on a case study on automatic analysis of non-functional test results. We discuss challenges in the face of continuous integration and deployment and provide improvement suggestions based on interviews at a large company in Sweden. The key contributions of this work are filling the knowledge gap in research about performance regression test analysis automation and providing warning signs and a road map for the industry.	complex systems;continuous delivery;continuous integration;display resolution;embedded system;foreign function interface;functional testing;next-generation network;non-functional testing;regression testing;software deployment;test automation	Mikael Fagerström;Emre Emir Ismail;Grischa Liebel;Rohit Guliani;Fredrik Larsson;Karin Nordling;Eric Knauss;Patrizio Pelliccione	2016		10.1145/2931037.2931064	computer science;engineering;automation;software engineering;data mining;operations research	SE	-67.97195155417914	23.99980384963074	67602
892f8a5b6e3fde37894e1d7711e15096f214e124	software business		Modern low-end cars have embedded more than 30 to 50 so-called Electronic Control Units (ECUs), featuring around 50 million lines of code (LOC). At commercial rate, it represents $1,500 Mio (1.5 billion/milliard). However, a modern high-end car features around 100 million LOC, and this number is planned to grow to 200–300 millions in the near future. As a comparison, a F-22 fighter jet features less than 2 million LOC and a Boeing 787 around 14 million LOC. This presentation focuses on the Automotive Software Development market, the value chain in this market, and how to be part of it. Upcoming new trends as autonomous driving and the car as part of the Internet of Things lead the future automotive software development. Software engineers play an important role in the automotive industry to build up more sophisticated and added-value technology. I will talk about the balance act between being predictable by using processes conform to ASPICE and the need to be cost efficient and agile in the fast changing environment pushed by the influence of Consumer Electronic and Internet Services. I will be discussing the AUTOSAR approach as the upcoming industry standard in this business area, mentioning safety requirements and the ISO 26262. This talk will be interesting to professionals and students who intend to understand and know more about Automotive Software, and to clarify concepts of the car industries. Christoph Gaertner is responsible for building up a Software Development Department for Bosch in Braga. He is working for Bosch since 2008 and before coming to Portugal he was a section head at Bosch Car Multimedia in Leonberg developing augmented reality solutions for the car driver. He was leading Software Projects for developing display based Instrument Cluster for a German premium car brand. He was an Software Developer and Architect for Head-Unit System at Harman Becker. He started his career in a consultancy company during the new economy hype end of the 90ies where he already researched and developed smart appliances for the connected home. He has a Diploma in software engineering from the University of applied sciences Esslingen, Germany. One Size Does Not Fit All: Software Product Management For Speedboats vs. Cruiseships	autosar;agile software development;augmented reality;automotive software;autonomous car;code refactoring;component-based software engineering;cost efficiency;customer relationship management;diploma;documentation;donald becker;embedded system;emoticon;image scaling;internet of things;new economy;requirement;smart tv;software business;software developer;software engineer;software product management;source lines of code;technical standard;web service;workaround	João M. Fernandes;Ricardo J. Machado;Krzysztof Wnuk	2015		10.1007/978-3-319-19593-3		SE	-75.86007885147951	18.478097339317607	67612
28e05b9f85ce39d1a4c662c1afcdc28241ba4d46	synergy and technology gaps in export logistics chains between a chilean and a spanish medium-sized port	ports;technological synergy;communication systems;business practices;technology gaps;logistics;export logistic chains;supply chain management;trends;chile;spain	In this study we analyze the integral logistics links in a Chilean and Spanish medium-sized port to determine the synergy and technological gaps in the export logistics chain, according to the links they hold with both the business core and processes at operational level. To assess the degree of synergy in a technological context, flow-charts of each service process in the Chilean port system are provided. A comparative analysis was accomplished for a Chilean and a Spanish mediumsized port, and then we determine the key factors related to Process and Communication Technologies PCT, technological integration and collaboration among the actors in the integral logistics; at strategic and business levels, some apparent causes that may account for the technology gaps in these two ports are also analyzed at operational level. New trends to implement a port platform for the export logistics chain based on a collaborative model to improve efficiency, effectiveness and efficacy in the port are proposed. Different technologies present in export logistics chains are used to improve the management of the cargo transportation by trucks and ensure traceability and tracking of cargo from the exporter to the port. Finally, discusses the main trends in technological development in ports at the country level. © 2015 The Authors. Published by Elsevier B.V. Selection and/or peer-review under responsibility of the organizers of ITQM 2015	chart;logistics;qualitative comparative analysis;synergy;traceability	Claudia A. Durán;Felisa M. Córdova	2015		10.1016/j.procs.2015.07.055	logistics;supply chain management;port;computer science;communications system	SE	-78.72397058731417	9.823128319209019	67700
fcda327400e32223e72a89330e01792023640775	multi-criteria business intelligence approach		Multi Criteria Business Intelligence approach (MCBI) aims to enhancement Business Intelligence Applications (BIA) by applying Multi-Criteria Decision Making (MCDM). MCBI approach contributes to improve Business Intelligence Decision Support System (BIDSS) for BIA. Also MCBI approach presents a standard method to evaluate and select business decisions. The recommended business decision is the suitable and optimal choice to implement. The proposed model for MCBI approach that consists of five major components. The first component is business objectives, problem definition and main goals. The second component is a business heterogeneous data treatment which gathering from different resources and related with different areas. The third component is a unified business intelligence databases. The fourth component is a business intelligence processing. The fifth component is a evaluating the business decisions to select the suitable and optimal solution.	business continuity planning;common criteria;data mining;database;decision support system	Torky Sultan;Ayman E. Khedr;Mohamed M. R. Ali	2011		10.1007/978-3-642-32573-1_4	business analysis;intelligence cycle;intelligence cycle;reactive search optimization;competitive intelligence;sociological intelligence;business analytics;process management;business intelligence;web intelligence;business activity monitoring	AI	-70.82824653918522	9.32938367698016	67822
4c1bdb3d257c697b5aff3dc298d7f11bbe930bc4	evaluating the value-added benefits of using requirements reuse metrics in erp projects	requirements reuse metrics;requirements engineering activity;requirements measurement activity;benefits assessment;erp project;value-added benefit;frequent benefits assessment;requirements reuse measurement practice;measurement exercise;reuse- driven requirements engineering;erp requirements elicitation;reuse metrics usage pattern;enterprise resource planning;measurement program;requirements reuse measurement;requirement engineering;value added;requirements elicitation	Measurement programs often go astray and fail to reach full success because of misconceptions and differences in expectations about the benefits to be realized as a result of a measurement exercise. This paper suggests how to plan and apply requirements reuse measurement in a business focussed way, by doing frequent benefits assessments. We describe an approach to analyzing, evaluating and tracking the reuse metrics usage patterns in Enterprise Resource Planning (ERP) projects and the benefits gained from integrating requirements reuse measurement practices in the ERP requirements elicitation- modelling- negotiation cycle. Relationships between requirements measurement activities and requirements engineering activities and deliverables are studied in the context of SAP R/ 3 implementation projects.	erp;enterprise resource planning;requirement;requirements elicitation;requirements engineering;reuse metrics	Maya Daneva	2001		10.1145/375212.375283	reliability engineering;requirements analysis;requirements management;business requirements;systems engineering;engineering;knowledge management;value added;requirement;requirements elicitation;requirements engineering;non-functional requirement	SE	-69.29150557645292	21.347009200564052	67959
95cd602aca0a746e34db7c760eb58e4bc53e452f	at work with the digital detectives - when number crunchers and bean counters meet	digital detective;number crunchers;bean counter			Danny Bradbury	2006	Digital Investigation	10.1016/j.diin.2006.01.004	algorithm	HCI	-91.9067139285848	31.838163374525855	67997
f47e7792f9663a829bbef04fa77afdb9ddcd95f0	a green software development life cycle for cloud computing	green design software development product life cycle management cloud computing;information technology green it cloud computing sdlc software development life cycle;information technology;energy efficient software development green software development life cycle cloud computing green crusaders carbon footprint datacenters it infrastructures;software engineering;sdlc;computer centres;power aware computing;software development life cycle;software engineering cloud computing computer centres power aware computing;green it;cloud computing	Cloud computing's recent proliferation has received attention from green crusaders hoping to mitigate the carbon footprint of large datacenters and IT infrastructures, but what about the software? A new framework for a greener cloud focuses on energy-efficient software development.	cloud computing;software development process	Nitin Singh Chauhan;Ashutosh Saxena	2013	IT Professional	10.1109/MITP.2013.6	computing;real-time computing;cloud computing;computer science;software development;operating system;cloud testing;software as a service;utility computing;systems development life cycle;law;information technology;software deployment	SE	-69.78787199870499	27.367058534353202	68004
475b1f036b33076c710c5882f7b0e3decab54784	enabling interoperability in the area of multi-brand vehicle configuration	satisfiability;point of view;business process	Summary. With the change of EU regulations in the automotive market in 2002, multi-brand car dealers became possible. Despite the high economical expectations connected with them, the existing IT infrastructure does not provide satisfying sup- port for these changes as it had been developed independently by each brand for many years. Thus in this paper, we sketch solutions for two of their most important problems: (1) the systems of the manufacturers have to be accessed separately and in a proprietary manner which aggravates a product finding process across brands and (2) the status of the customer is not explicit because of a technological gap be- tween car configuration and CRM system. Our solution is based on research results and tools from the EU project ATHENA3, especially the SAP tool suite for cross- organizational business processes which is presented in more detail. We conclude the paper by explaining the lessons we learned from a technical and research transfer point of view.		Michael Klein;Ulrike Greiner;Thomas Genssler;Jürgen Kuhn;Matthias Born	2007		10.1007/978-1-84628-858-6_83	computer science;theoretical computer science;database;distributed computing	EDA	-75.17170136391772	8.009218566700069	68034
e469636ad5a1eacf5fe5df07c946ce9e2dc88c3a	assessing the cost-effectiveness of inspections by combining project data and expert opinion	industrial case study;investments;software cost estimation;project data;particle measurements;software inspections;inspection;software engineering;inspection costs software engineering software quality monitoring investments programming technology management particle measurements large scale systems;technology management;inspection software quality cost benefit analysis software cost estimation;large scale;expert knowledge elicitation;monitoring;industrial case study cost effectiveness software inspections project data expert opinion software engineering software quality investments;cost effectiveness;expert opinion;expert knowledge;monte carlo simulation;programming;software inspection;cost benefit analysis;software quality;large scale systems	There is a general agreement among software engineering practitioners that software inspections are an important technique to achieve high software quality at a reasonable cost. However, there are many ways to perform such inspections and many factors that affect their costeffectiveness. It is therefore important to be able to estimate this cost-effectiveness in order to monitor it, improve it, and convince developers and management that the technology and related investments are worthwhile. This work proposes a rigorous but practical way to do so. In particular, a meaningful model to measure costeffectiveness is selected and a method to determine the cost-effectiveness by combining project data and expert opinion is proposed. To demonstrate the feasibility of the proposed approach, the results of a large-scale industrial case study are presented.	qr code;software development;software engineering;software inspection;software quality	Lionel C. Briand;Bernd G. Freimut;Ferdinand Vollei	2000		10.1109/ISSRE.2000.885866	reliability engineering;programming;cost-effectiveness analysis;inspection;systems engineering;engineering;cost–benefit analysis;technology management;software engineering;software inspection;management science;management;software quality;monte carlo method	SE	-64.4844388181025	31.2164477411289	68035
c96f784d859d1d3616caac50ec798b37bfb337db	uncertainties in software projects management	uncertainty in projects project management uncertainty;software management project management;uncertainty project management software complexity theory companies;commercial project success software project management organizational environment project success rate project activity project potential competitive advantage uncertainty source project managers project staff	The organizational environment becomes increasingly focused on projects, it is time to unleash the power and energy embedded in projects. The disturbingly poor project success rate makes it imperative that the organization pays more attention to their project activity, their potential, and the competitive advantage that they can bring. The goal of this article is to understand what gives rise to uncertainty; to recognize its symptoms and explore strategies for controlling it. This article offers a classification to source of uncertainty in software projects, a guide to manage software projects in phases of uncertainty and our aim is to assist project managers and staff to reduce uncertainty and thereby contribute to commercial project success.	embedded system;imperative programming;software project management;uncertainty quantification	Marcelo L. M. Marinho;Suzana Cândido de Barros Sampaio;Hermano Perrelli de Moura	2014	2014 XL Latin American Computing Conference (CLEI)	10.1109/CLEI.2014.6965153	basis of estimate;project 112;project management;extreme project management;program management;work breakdown structure;earned value management;software project management;opm3;systems engineering;knowledge management;environmental resource management;project sponsorship;estimation;project risk management;business;project management 2.0;project management triangle;project charter;schedule;project planning;project portfolio management	SE	-69.65366737273627	23.004380658968284	68096
4fdece2a12672b36a2b1dfdfc8522ae79e8cc9b2	process aggregation using web services	gestion integrada;gestion integree;gestion entreprise;red www;processus metier;reseau web;service web;firm management;integrated management;process integration;service process;systeme integre;sistema integrado;web service;flujo informacion;proceso servicio;flux information;information flow;internet;processus service;data aggregation;gestion electronique de processus;workflow;world wide web;system development;administracion empresa;integrated system;business process	This paper examines the opportunities and challenges related to data and process integration architectures in the context of Web Services. A primary goal of most enterprises in today’s economic environment is to improve productivity by streamlining and aggregating business processes. This paper illustrates how integration architectures based on Web Services offer new opportunities to improve productivity that are expedient and economical. First, the paper introduces the technical standards associated with Web Services and provides business example for illustration. Abstracting from this example, we introduce a concept we call Process Aggregation that incorporates data aggregation and workflow to improve productivity. We show that Web Services will have a major impact on Process Aggregation, making it both faster and less expensive to implement. Finally, we suggest some research directions relating to the Process Aggregation challenges facing Web Services that are not currently being addressed by standards bodies or software vendors. These include context mediation, trusted intermediaries, quality and source selection, licensing and payment mechanisms, and systems development tools.	business process;data aggregation;programming tool;software development process;technical standard;web service	Mark Hansen;Stuart E. Madnick;Michael Siegel	2002		10.1007/3-540-36189-8_2	data aggregator;web service;workflow;web modeling;the internet;business process execution language;information flow;web standards;computer science;ws-policy;services computing;business process;operations research;world wide web;process integration	Web+IR	-68.46415259988	5.065261533573509	68129
66fcd819260de29cbcaf871dff904dabddfe8160	e-commerce metrics for net-enhanced organizations: assessing the value of e-commerce to firm performance in the manufacturing sector	electronic commerce;empirical analysis;measurement;perforation;e commerce;firm size;resource based theory;construct validity;dynamic capabilities;it intensity;e commerce metrics;validation;net enhanced organizations;business value;manufacturing sector;firm performance	In this study, we developed a set of constructs to measure e-commerce capability in Internetenhanced organizations. The e-commerce capability metrics consist of four dimensions: information, transaction, customization, and supplier connection. These measures were empirically validated for reliability, content, and construct validity. Then we examined the nomological validity of these e-commerce metrics in terms of their relationships to firm performance, with data from 260 manufacturing companies divided into high IT-intensity and low IT-intensity sectors. Grounded in the dynamic capabilities perspective and the resource-based theory of the firm, a series of hypotheses were developed. After controlling for variations of industry effects and firm size, our empirical analysis found a significant relationship between e-commerce capability and some measures of firm performance (e.g., inventory turnover), indicating that the proposed metrics have demonstrated value for capturing e-commerce effects. However, our analysis showed that e-commerce tends to be associated with the increased cost of goods sold for traditional manufacturing companies, but there is an opposite relationship for technology companies. This result seems to highlight the role of resource complementarity for the business value of e-commerce—traditional companies need enhanced alignment between e-commerce capability and their existing IT infrastructure to reap the benefits of e-commerce. (Electronic Commerce; IT Intensity; e-CommerceMetrics;Measurement;Validation;FirmPerformance; Net-Enhanced Organizations)	capability maturity model;complementarity theory;e-commerce	Kevin Zhu;Kenneth L. Kraemer	2002	Information Systems Research	10.1287/isre.13.3.275.82	e-commerce;computer science;marketing;operations management;business value;construct validity;management;world wide web;measurement;statistics;commerce	AI	-81.18789513157839	5.554020064286697	68138
7a7d9e6426f1d1234847597e080547dbd713fe6b	collaborative knowledge management: case studies from ship design	computer aided design;ship design;market requirements;cad;cooperation;information technology;knowledge management;collaborative tools;information flows;suppliers;classification societies;shipyards;shipbuilding industry;ships;ship designers;norway;scm;knowledge flows;km;design processes;shipowners;supply chain partners;communications technology;ict;interfirm collaboration;collaborative design;business information systems;ship classification;supply chain management;cooperative work	The recent increase in information and knowledge flows within the shipbuilding industry, due to the implementation of computer-aided design and tougher market requirements for ship design, generates a need for effective collaboration between ship designers, shipowners, shipyards, suppliers, classification societies, and other supply chain partners. This paper uses a comparative case study approach to explore how two different ship design firms have organised cooperative work during the design process, and how they employed computer-aided collaborative tools for knowledge management. This study will be interesting for practitioners from shipbuilding and ship design firms, and scholars of knowledge management and applications of collaborative knowledge management tools in organisations.	collaborative software;computer-aided design;knowledge management;requirement	Marina Z. Solesvik	2011	IJBIS	10.1504/IJBIS.2011.041788	information and communications technology;supply chain management;systems engineering;engineering;knowledge management;marketing;operations management;management;law	EDA	-70.19849243799445	5.405929911343185	68186
0022ff9a1f01881ae70f70be4ca5bfb9ab44a9fd	applying sampling to improve software inspections	resource scheduling;empirical study;inspection time;efficiency;software engineering;sampling;empirical evidence;datavetenskap datalogi;programvaruteknik;sampling methods;monte carlo simulation;software inspection;simulation model	The main objective of software inspections is to find faults in software documents. The benefits of inspections are reported from researchers as well as software organizations. However, inspections are time consuming and the resources may not be sufficient to inspect all documents. Sampling of documents in inspections provides a systematic solution to select what to be inspected in the case resources are not sufficient to inspect everything. The method presented in this paper uses sampling, inspection and resource scheduling to increase the efficiency of an inspection session. A pre-inspection phase is used in order to determine which documents need most inspection time, i.e. which documents contain most faults. Then, the main inspection is focused on these documents. We describe the sampling method and provide empirical evidence, which indicates that the method is appropriate to use. A Monte Carlo simulation is used to evaluate the proposed method and a case study using industrial data is used to validate the simulation model. Furthermore, we discuss the results and important future research in the area of sampling of software inspections.	sampling (signal processing);software inspection	Thomas Thelin;Håkan Petersson;Per Runeson;Claes Wohlin	2004	Journal of Systems and Software	10.1016/S0164-1212(03)00249-8	reliability engineering;sampling;real-time computing;simulation;computer science;software engineering;management;statistics	SE	-64.66842062023088	31.43105234174138	68192
efe013dc9ad47bfb1047577f166c0f593ae45570	km-mail: an approach for sharing, evaluating and organizing knowledge-rich e-mail	knowledge evaluation;knowledge sharing;knowledge management;e-mail;knowledge crystallization	In today's Internet-connected world, knowledge-rich e-mail is a popular means of communication between domain experts in 'mission-critical' situations. However, traditional e-mail discussion groups and protocols have certain shortcomings that limit their effectiveness in expert knowledge sharing and reuse. To address these weaknesses, alternative strategies can be employed to enhance the knowledge management capabilities of the Internet in general and e-mail in particular. We present a novel knowledge management-based e- mail (KM-Mail) framework that capitalizes on the utilization of e-mail for knowledge sharing and reuse. We will also outline a mechanism to allow the evaluation and organization of knowledge shared via e-mail.		Yu-N Cheah;Kee Guan Lim	2003			knowledge extraction;business;knowledge engineering;domain knowledge;management science;knowledge value chain;personal knowledge management;knowledge sharing;knowledge integration;open knowledge base connectivity;knowledge management	ML	-67.45346730189274	8.625619371255183	68212
6a4c6b36ee688a6bef07413a854a0eba1cc9d549	short-range wireless technologies with mobile payments systems	challenges;future development;wireless technologies;mobile payments	Mobile devices and wireless technologies seem destined to make a large and continuing impact on our lives. Mobile devices especially have been widely used in the last few years, and recent developments in wireless technologies have provided some new solutions to problems of connectivity. Wireless technologies provide a new channel for implementation of mobile payments systems. The potential of short-range wireless technologies in commercial markets is enormous. Mobile payments is one such exciting application presently in development. Mobile payments systems offer great potential for both mobile and non-mobile products and services. However, in this early stage, mobile payments systems are facing some challenges. There are several available technologies for the applications and the mobile payments world is still waiting for standardization.This paper examines several available short-range wireless technologies for mobile payments systems. The ways in which short-range wireless technologies support mobile payments systems are explored. This paper also evaluates the advantages and issues involved with using these technologies. Finally, this paper will discuss the future developments of mobile payments with wireless technologies and the challenges faced by the mobile payments industry.	mobile payment;mobile phone	Jiajun Jim Chen;Carl Adams	2004		10.1145/1052220.1052302	simulation;engineering;operations management;mobile computing;computer security;mobile payment	Mobile	-77.75941096849382	15.59146636055546	68292
4c6a83bd621db175c7a0c145a12864b335d6a5c1	end user software engineering: chi'2008 special interest group meeting	empirical studies of programmers esp;end users shaping effective software euses;end-user development eud;end-user software engineering euse;natural programming;psychology of programming;web authoring	End users create software whenever they write, for instance, educational simulations, spreadsheets, or dynamic e-business web applications. Researchers are working to bring the benefits of rigorous software engineering methodologies to these end users to try to make their software more reliable. Unfortunately, errors are pervasive in end-user software, and the resulting impact is sometimes enormous. This special interest group meeting has two purposes: to incorporate attendees' and feedback into an emerging survey of the state of this interesting new sub-area, and generally to bring together the community of researchers who are addressing this topic, with the companies that are creating end-user programming tools.	electronic business;end-user development;pervasive informatics;programming tool;simulation;software development process;software engineering;spreadsheet;web application	Brad A. Myers;Margaret M. Burnett;Mary Beth Rosson;Andrew Jensen Ko;Alan F. Blackwell	2008		10.1145/1358628.1358687		SE	-65.27639637637007	24.18283968418215	68343
76cd088bb11feaa9e589e3658477e9aca493b592	an approach for value adding process-related performance analysis of enterprises within networked production structures	evaluation function;performance indicator;performance analysis;production network;value added	This conceptual paper focuses a methodology for the analysis of performances of enterprises operating in production networks. In order to derive adequate results exclusively the operative perspective of performance analysis is investigated. Operative performance analysis implies the analysis of performances of network members considering a special value-adding-process. The introduced approach is divided into two segments: the value adding process-neutral phases and the value adding process-specific phases. In that context special methodologies for the determination of performance indicators, corresponding parameters, evaluation functions and weightings are focussed. Additionally possible consequences are discussed.	evaluation function;performance;profiling (computer programming)	Hendrik Jähn	2007		10.1007/978-0-387-74157-4_2	reliability engineering;systems engineering;engineering;operations management	HPC	-68.43809866295418	14.424312386335316	68370
ea4a4a35cc335f60b43c39a6a4602938c3f9435c	early user-testing before programming improves software quality	user testing;software quality	This position statement does not focus on usability al hough it presents data from a software up-date cycle where s veral usabilityand user-centred methods were used. The important lesson learnt is that a better (more complete) spec ification before programming results in fewer errors in the code and that such a specification can be reached by user tests of inter ac ive mockups.	hough transform;software quality;spec#;usability	John Sören Pettersson;Jenny Nilsson	2009			reliability engineering;long-term support;verification and validation;regression testing;software performance testing;system integration testing;systems engineering;package development process;software reliability testing;software development;software engineering;software construction;software testing;software deployment;software quality control;software quality;software quality analyst	SE	-65.2569831644566	28.539938757439295	68371
e0eeb3e44cee9d77b3db50ac03bf0759addb003a	standards should be set by officially recognized bodies	computer society;art;standards organizations;iso standards;software engineering;software engineering software standards iec standards iso standards best practices computer society standards development standards organizations us government art;standards development;iec standards;best practices;software standards;us government	0 7 4 0 7 4 5 9 / 9 8 / $ 1 0 . 0 0 © 1 9 9 8 oftware engineering standards are simultaneously some of the most useful and most maligned intellectual tools in the software profession. Professional organizations have defined standards for project management plans, quality assurance plans, configuration management plans, test documentation, user documentation, and many other software engineering work products. Some practitioners find these standards to be an indispensable aid to their day-to-day work. Others disagree; one Internet developer recently referred to such standards as “out-of-touch roadkill.” In this roundtable, five contributors from around the world take up the issue of who should define standards and whether standards should even be defined at all. What is the right answer? You tell us—drop an e-mail to software@computer.org. —Steve McConnell S Steve McConnell Moderator	configuration management;email;google moderator;software documentation;software engineering;steve mcconnell	Steve McConnell	1998	IEEE Software	10.1109/MS.1998.730853	iso/iec 9126;generally accepted auditing standards;engineering ethics;iso 29110;iso 9000;computer science;systems engineering;software engineering;iso/iec 15504;management;best practice	SE	-67.5170692868339	25.492453219681988	68483
72f99ec1d8b7bdc5c56f14f35767eac0985e2785	decision support systems: a summary, problems, and future trends	decision support system	Abstract   This paper critically examines issues confronting Decision Support Systems (DSS) in the business/management area. Due to the lack of acceptable definition of DSS, the characteristics and components of DSS are discussed in detail. It is pointed out that work activities that require decision making form a spectrum of problems ranging from structured problem to unstructured problem. It is further pointed out that personality and cognitive style can influence individuals' decision styles, and thus different decision aids will be sought. DSS development and applications are briefly described. Finally, the major problems facing current DSSs are outlined, and the future trends of DSS are described.	decision support system	M. C. Er	1988	Decision Support Systems	10.1016/0167-9236(88)90022-X	decision support system;computer science;artificial intelligence;operations management;data mining;management science;management;operations research;business decision mapping	OS	-65.84401880985305	7.883578021371024	68618
79c1324a77c12a2a9a2d5dca47a3eff1f72296f7	first international workshop on requirements engineering practices on software product line engineering (repos 2012)	first international workshop on requirements engineering practices on software product line engineering repos 2012;articulo;product line;requirements engineering;software product line engineering;empirical evidence;requirement engineering;discussion forum;software product line	The objective of this workshop is to attract professionals from academia and industry to discuss the role of requirements engineering in product line developments, including new techniques, methods and tools that will help practitioners to improve their current requirements engineering practices. The workshop will address experiences from practitioners, empirical evidence about current practices, successful deployment of novel approaches, and current obstacles and proposed solutions. Nevertheless, REPOS also aims to become a discussion forum about the state of the art and practice for practitioners and researchers on Requirements Engineering for Software Product Lines.	experience;requirement;requirements engineering;software deployment;software product line	Emilio Insfrán;Gary J. Chastek;Patrick Donohoe	2012		10.1145/2362536.2362575	engineering management;requirements analysis;market requirements document;empirical evidence;software engineering process group;systems engineering;engineering;social software engineering;requirement;software engineering;requirements engineering;management;software requirements;product engineering	SE	-66.31242649722962	23.693051995027552	68681
e709c59aa1d072038d397a57b1fb8b5e0b959892	legal implications of blown schedules	legislation;law legal factors costs financial management project management software development management programming delay;software development;software launch delay legal implications blown schedules management perspective software development project lost public credibility;legislation software development management;software development management	From a management perspective there may be nothing more frightening than to learn that your software development project is going over schedule and budget. The implied cost increase to complete the project may pale in comparison to the costs of lost public credibility and delaying the software launch. Worse yet, it's usually difficult if not impossible to predict just when the project will be done. It may be tomorrow or next year. Taking control of the situation and achieving the original objective can seem impossible.		Karl Dakin	1996	IEEE Software	10.1109/52.536467	project management;team software process;extreme project management;software quality management;time management;software project management;systems engineering;engineering;operations management;software development;software engineering;software asset management;management science;project management triangle;management;project planning	Visualization	-69.40162034653939	26.12314817482471	68730
255d38d0fef3b3d736caecc1501087a06a30d067	meta-level modelling of e-education ecosystem in multicultural context		A sociotechnical system is a complex inter-relationship of people and technology, including hardware, software, data, physical and virtual surroundings, people, procedures, laws and regulations. An e-Education environment is a particularly complex example of a sociotechnical system that requires equal support for user needs and technological innovations. The challenge for eEducation environment development is that in addition to the producers, users, domain experts and software developers, pedagogical experts are also key stakeholders. In our paper, we discuss different meta-aspects and components of modelling e-Education ecosystems in multicultural contexts.		Anneli Heimbürger	2016		10.3233/978-1-61499-720-7-182	multiculturalism;environmental resource management;ecosystem;geography	HCI	-72.99862940329265	11.139399287133575	68743
370ebdeca4659bd23e6de6af845ba5f90d692f22	towards an enterprise architecture for public administration using a top-down approach	information management system;eur j inform syst;information systems security;mis systems;information systems research;information security;case studies;information science;top down;ejis special issue;information security systems;information technology;business information technology;security information systems;european journal of information systems;information systems management;operational research society;data model;business model;enterprise integration;computer information systems;geographic information systems;information technology journal;information management;european journal of is;information systems journals;european journal information systems;information systems technology;managing information systems;accounting information systems;reference data;information and management;management information systems;define information systems;strategic information systems;business information management;process model;private sector;soft system methodology;ejis journal;information system;health information systems;computer information technology;business information systems;business systems analyst;ejis;reference architecture;european journal;enterprise architecture;management science;journal of information systems;european journal of information system;information technology journals;public administration	The use of Enterprise Architectures is becoming increasingly widespread in the private sector. Borrowing insights from enterprise reference architectures developed during the last decade, IT vendors and companies belonging to specific industries are establishing reference data and process models advancing the standardization of their businesses and creating a more integrated environment for their activities. Although public administrations share the same problem of non-standardization, which is being magnified rapidly in a changing and demanding environment, little has been done so far in the direction of integration. This article builds a basis, shows initial directions and attempts to stimulate interest in a PA enterprise framework. Following a top-down approach and employing concepts from the fields of public administration, enterprise integration and generic process and data modeling the outline of the ArchPad enterprise architecture for Public Administration is presented.	comparison of command shells;data modeling;efx factory;enterprise architecture;enterprise integration;top-down and bottom-up design	Vassilios Peristeras;Konstantinos A. Tarabanis	2000	EJIS	10.1057/palgrave.ejis.3000378	functional software architecture;public relations;enterprise system;enterprise systems engineering;enterprise software;computer science;engineering;knowledge management;architecture domain;electrical engineering;integrated enterprise modeling;management information systems;enterprise architecture management;management science;enterprise architecture;enterprise integration;information technology;enterprise planning system;information system;enterprise information security architecture;enterprise information system;business architecture;enterprise life cycle	Web+IR	-71.6514688433652	7.830039866385921	68810
e05e6da1dab79323aa9b371b986f6009581cdc3f	use of information visualization techniques in a collaborative context	groupware;computer supported cooperative work;collaboration;3c collaboration model;software development information visualization technique computer supported cooperative work cscw 3c collaboration model;information visualization;software engineering;software engineering data visualisation groupware;data visualisation;visualization;collaborative visualization;computational modeling;three dimensional displays;information visualization technique;software development;cscw;visualization computational modeling collaboration three dimensional displays;3c collaboration model collaborative visualization information visualization computer supported cooperative work	Collaborative visualization is a study field that emerged by combining computer-supported cooperative work (CSCW) and information visualization. The main goal is to analyze how visualization techniques can be used to support collaboration. One of the key aspects for the effective use of visual representations as a facilitator for collaboration is the analysis of how this feature influences collaborative work The aim of this paper is to present a study which considered the use of visualization techniques for supporting the collaboration process from a 3C collaboration model perspective. More specifically, we analyzed the influence of techniques from the user's perspective. A case study was conducted to investigate the relationship of visualization techniques with activities performed by users in a software development scenario. The results of the case study show that visualization techniques influence collaborative tasks.	computer-supported cooperative work;information visualization;software development	Fernanda C. Ribeiro;Jano Moreira de Souza;Melise M. Veiga de Paula	2015	2015 IEEE 19th International Conference on Computer Supported Cooperative Work in Design (CSCWD)	10.1109/CSCWD.2015.7230937	visual analytics;information visualization;human–computer interaction;computer science;knowledge management;computer-supported cooperative work;management	Visualization	-65.77760221103601	15.842950585894535	68823
199c68d44573adf4171bf72d1d2e3aeaf5b19d88	inferring comprehensible business/ict alignment rules	economie;extraction information;modelizacion;economia;alignement;multidisciplinaire;swarm intelligence;rule induction;intelligence en essaim;analisis datos;information extraction;inversion;processus metier;information technology;hd28 management industrial management;data mining;strategic alignment;universiteitsbibliotheek;investment;practical guidelines;alignment rule set;modelisation;data analysis;induccion;induction;technology and engineering;fouille donnee;investissement;alineamiento;proceso oficio;analyse donnee;multidisciplinary;prediction model;economy;multidisciplinar;practice guideline;modeling;inteligencia de enjambre;busca dato;artificial ant systems;extraccion informacion;business rules;alignment;business process;business ict alignment;ant system	We inferred business rules for business/ICT alignment by applying a novel rule induction algorithm on a data set containing rich alignment information polled from 641 organisations in 7 European countries. The alignment rule set was created using AntMiner+, a rule induction technique with a reputation of inducing accurate, comprehensible, and intuitive predictive models from data. Our data set consisted of 18 alignment practices distilled from an analysis of relevant publications and validated by a Delphi panel of experts. The goal of our study was to describe practical guidelines for managers in obtaining better alignment of ICT investments with business requirements. Our obtained rule set showed the multi-disciplinary nature of B/ICT alignment. We discuss implication of the alignment rules for practitioners.		Bjorn Cumps;David Martens;Manu De Backer;Raf Haesen;Stijn Viaene;Guido Dedene;Bart Baesens;Monique Snoeck	2009	Information & Management	10.1016/j.im.2008.05.005	inversion;investment;swarm intelligence;computer science;engineering;knowledge management;artificial intelligence;marketing;operations management;data mining;business process;data analysis;management;information technology;world wide web;strategic alignment;information extraction	AI	-83.09358901222093	9.87216995007466	68864
8721787b480727ecb3f60797b8c0e58f852d4ac3	estimating design effort using parametric models: a case study at pratt & whitney canada		Aerospace is very important to the Canadian economy, with over 80,000 employees and generating over $20 billion dollars in revenue. However, this industry like many others is facing many challenges. One of them is the difficulty in being able to estimate design effort required in a design project, which impacts not only resource requirements and lead-time but also the final cost. This article presents the findings of a case study conducted for Pratt u0026 Whitney Canada, recognized as a global leader in the design and manufacturing of aircraft engines. The study models parametric cost estimation relationships to estimate the design effort of integrated blade-rotor low-pressure compressor fans. Several effort drivers are selected to model the relationship. Comparative analyses of three types of models are conducted. The model with the best accuracy and significance in design estimation is retained.	knuth–morris–pratt algorithm	Adil Salam;Nadia Bhuiyan	2016	Concurrent Engineering: R&A	10.1177/1063293X16631800	engineering;operations management;operations research	SE	-85.5848922013926	8.887461005716476	68867
12c7e4fe0911c229e0fce97ff8821e566b764a65	project graal: towards operational architecture alignment	application architecture;case study research;governance;strategic alignment;it infrastructure;software architecture;architecture alignment;enterprise architecture	This paper presents a framework for architecture alignment that can be positioned between approaches for software architecture (which concern software artefacts only) and strategic alignment models. The framework is currently applied in case study research to find alignment patterns used in practice. First results presented in this paper indicate that the framework might yield an operationalization of strategic architecture alignment models. We also present an alignment pattern found that shows a difference between how architectures are designed at the application level and the infrastructure level. We think this difference is significant for practical alignment models.	graal;software architecture	Pascal van Eck;Henk M. Blanken;Roel Wieringa	2004	Int. J. Cooperative Inf. Syst.	10.1142/S0218843004000961	enterprise architecture framework;corporate governance;reference architecture;software architecture;the open group architecture framework;space-based architecture;information technology management;database-centric architecture;operational view;knowledge management;architecture domain;applications architecture;service-oriented modeling;enterprise architecture management;solution architecture;software architecture description;enterprise architecture;view model;management;strategic alignment;data architecture;business architecture	SE	-73.38506618178178	8.977698166203915	68883
23465a4bba29498a3f011f8a818131794662c4c0	qtier-rapor: managing spreadsheet systems & improving corporate performance, compliance and governance	legislation;best practice;corporate performance;corporate governance	Much of what EUSPRIG discusses is concerned with the integrity of individual spreadsheets. In businesses, interlocking spreadsheets are regularly used to fill functional gaps in core administrative systems. The growth and deployment of such integrated spreadsheet SYSTEMS raises the scale of issues to a whole new level. The correct management of spreadsheet systems is necessary to ensure that the business achieves its goals of improved performance and good corporate governance, within the constraints of legislative compliance – poor management will deliver the opposite ! This paper is an anatomy of the real-life issues of the commercial use of spreadsheets in business, and demonstrates how Qtier-Rapor has been used to instil best practice in the use of integrated commercial spreadsheet systems.	best practice;corporate governance;real life;software deployment;spreadsheet	Keith Bishop	2008	CoRR		corporate governance;best practice	OS	-75.89357137090175	10.276306244455846	68925
a70ea0ff4648e6f33ed959b39b17b9b54e992a61	a standard based adaptive path to teach systems engineering: 15288 and 29110 standards use cases	iso standards;industries;iec standards;modeling	This paper discusses the use of two different standards for teaching Systems Engineering (SE): ISO/IEC/IEEE 15288 and ISO/IEC 29110. The first one is a general and widely-used standard describing the lifecycle processes of the entire system, whereas the second one is a relatively new standard based on a reduced set of standards elements focused on lifecycle profiles for Very Small Entities (VSEs). We are especially interested in the impact that SE standards can have on teaching this discipline to engineering students. We consider the teaching of fundamental principles of systems engineering. In this paper we illustrate how our, previously developed, standard based solution for systems engineering education can be used as a framework to support these standard-based teaching paths. We mainly focus on illustrating how adapting standard processes can be done, considering not only the learning goals, but also projects size and complexity, in a project-based learning environment. This paper shows that, thanks to it's adaptation from the ISO/IEC/IEEE 15288, and to it's reduced size, the ISO/IEC 29110 standard is particularly suitable for teaching systems engineering fundamental knowledge to undergraduate students, new to the discipline. While the ISO IEC/IEEE 15288 might be more suited for students that already have a good grounding in systems engineering fundamentals, especially thanks to the ability to use some from its various processes to separately teach different topics of systems engineering.	complexity;entity;iso/iec 42010;systems engineering	Mohammed Bougaa;Stefan Bornhofen;Rory O'Connor;Alain Rivière	2017	2017 Annual IEEE International Systems Conference (SysCon)	10.1109/SYSCON.2017.7934712	iso/iec 9126;iso/iec 42010;iso/iec 12207;systems engineering;engineering;software engineering;mechanical engineering	SE	-64.49483179606062	19.84203606756104	68978
40cdb3a4aebc05a0b1544e6e44cdf19d441b728d	volume discovery: leveraging liquidity in the depth of an order driven market	iceberg orders;quantity discovery;otc;market microstructure;block trading;order book	Electronic order book trading has evolved in being recognized as the best-practice for trading small and mid-sized orders. Yet, this mechanism does not properly address the needs of large-sized orders which tend to execute off order book in over-the-counter markets. Order book trading provides for public price discovery but not for quantity discovery. Off book executions generally fragment the order flow which again adversely impacts price discovery. We propose a market model innovation to close this gap: ‘Volume Discovery’ introduces the new order type ‘volume order’ to integrate large sized orders into the book. The volume order builds on the concept of iceberg order but is enhanced by two parameters ‘hidden limit’ and ‘minimum volume’, which continuously search order book depth for matching quantity. For large orders, Volume Discovery leverages already existent liquidity to benefit block trading by increased likelihood of execution and reduced opportunity costs. For all orders, Volume Discovery promotes the integration of OTC markets and order book trading in order to improve liquidity while protecting price/time priority.	algorithmic trading;over-the-counter (finance)	Peter Gomber;Miroslav Budimir;Uwe Schweickert	2006	Electronic Markets	10.1080/10196780600643688	financial economics;market microstructure;economics;block trade;order;operations management;finance;commerce	ECom	-72.70432166970718	4.8797718714576055	69056
0c6bd7463d945d56c3a165d93b291b9bbcc80a27	on managing the enterprise information systems transformation: lessons learned and research challenges		Contemporary standardized Enterprise Information Systems, like SAP or Oracle, are increasingly setting the agenda and the pace for the development of almost any organization. Most organizations today are required not only to establish effective business processes but they are also required to accommodate for changing business conditions at an increasing rate. The major vendors are proposing new breed of technology, sometime denoted extended ERP or ERP/II. Consequently the time is ripe to reflect on the status of Enterprise Information Systems and their application in practice. This paper reflects on the lessons learned from managing the Enterprise Information Systems transformation and outlines future challenges for research on Enterprise Information Systems. The paper concludes that the ERP industry has a significant and increasing impact on enterprises due to the EIS enable transformation of the business. Consequently, the practical implications of these results are the continued management of EIS, after going-live, but most important, EIS research need to refocus and embrace this new tendency. The paper takes an exploratory perspective on managing the EIS transformation by examine the evidence from the experience gained in seven different Danish organizations. The paper finally outlines the critical implications for research in EIS.	enterprise information system	Charles Møller	2006		10.1007/0-387-34456-X_31	systems engineering;knowledge management;management science	DB	-75.4606178227253	8.727041453078256	69063
ef716a489837199ec5f363bc1a7d6ea4209e4652	overview of the khronos group	working group	The Khronos Group is a large and extremely successful standards organization whose mission is to define the APIs and technology required to support the authoring and playback of dynamic media on a wide variety of platforms and devices. Over the past several years, this organization has focused primarily on sorting out the issues to support developers in the wild and wooly mobile media market.This presentation describes the organization of the Khronos Group, the current set of working groups, how the technologies fit together to create a cohesive platform for development and deployment, and how your organization can use these technologies and become involved in the effort.	application programming interface;mobile media;software deployment;sorting	Randi J. Rost	2006		10.1145/1185657.1185746	working group;simulation;computer science;world wide web	HCI	-67.85242557264665	19.127838227753326	69077
2e6db25b1eec6e33999dd647834447bf5636dc9a	connotations of problem solving	patterns references;information system design;problem solving	"""A lot of interest has been focused on analysis and design models during information system (IS) design. But fundamentally, information systems are tools of """"problem solving"""" where the term """"problem solving"""" can take on different connotations. In this paper, we introduce seven different connotations of problem solving that describe IS functionality. Such a knowledge can help the system designer in transitioning between the analysis and design phases."""	information system;problem solving;systems design	Srinath Srinivasa	2001	ACM SIGSOFT Software Engineering Notes	10.1145/505532.505555	computer science;artificial intelligence;general group problem solving (ggps) model;algorithm	SE	-65.58500815300333	7.240777486582434	69276
65029d85c2e31803c7ee283b87f83383fe7226c6	is extreme programming just old wine in new bottles: a comparison of two cases	agile methods;information systems development;extreme programming;xp	This paper explores Extreme Programming (XP) as an information systems development approach and argues that it is mainly old wine in new bottles. We take an interpretive and critical view of the phenomenon. We made an empirical study of two companies that apply an XP style development approach throughout the information systems development lifecycle. The results of our research suggest that XP is a combination of best practices of traditional information systems development methods. It is hindered by its reliance on talented individuals, which makes its large scale deployment as a general purpose method difficult. We claim that XP can be useful for small teams of domain experts, who are physically close to and able to communicate well with the endusers and who are good designers and implementers. However, these skilled and motivated individuals with high working moral can exhibit high productivity regardless of the methods used, if they are not overly constrained by bureaucracy.	best practice;bureaucracy;extreme programming;information system;software deployment;software development process;systems development life cycle	Hilkka Merisalo-Rantanen;Tuure Tuunanen;Matti Rossi	2005	J. Database Manag.	10.4018/jdm.2005100103	simulation;extreme programming;artificial intelligence;data mining;agile software development;database;management	HCI	-70.66526063809914	24.271074465102135	69319
2e4a54032146b4bc56e367e1fe5746cacbbd0a12	on the importance of data quality in services: an application in the financial industry	financial data processing;investments;industries;financial services;stock markets;service industries;data quality;data handling;machinery;security;context	In different service industries, the attention to data quality has become an immediate necessity to remain successful in business activities. This paper shows a framework on data quality for financial services as well as the rising importance of the concept for every participant in the data value chain.	accessibility;data mart;data quality;display resolution;inventory;microsoft outlook for mac;relevance;traders	Kathrin Klaus	2011	2011 International Conference on Emerging Intelligent Data and Web Technologies	10.1109/EIDWT.2011.31	market data;marketing;financial system;business;commerce	Robotics	-72.02128057563684	4.423099961879615	69379
7f616ff0186fdd40eb849a3dcf17234ec6340e72	vendor: vidi, vici	installation management;project and people management;software management	Some hidden costs of outsourcing.	outsourcing	Phillip G. Armour	2014	Commun. ACM	10.1145/2661053	programming language;software engineering;vendor;computer science	Security	-64.25101153615093	21.663016999842863	69380
5fc1ec1dc5a156604e9f6ff73e24a660028e9cfb	estimating the effort of implementing iso 9001 in software organizations	data collection	A major concern for organizations who are seeking registration to ISO 9000 [11], or seeking the implementation of any quality software related framework [22][24], is the ability to estimate the required effort for meeting the stated requirements. This paper presents a model based on data collected using a survey of software organization that have achieved ISO 9001 registration.		Silas F. Rahhal;Nazim H. Madhavji	1996		10.1007/BFb0017738	iso/iec 12207;iso 9000;systems engineering;software engineering;database	SE	-69.02431105304267	20.910332341310784	69381
98f00e3b5d9338fc1f990069803735f33e54fe21	the requirements entropy framework in systems engineering	volatility;measurement;leading indicator;simulation;metric;requirements engineering;quality;theory;model;engineering effort;entropy;information;trends;framework	This paper introduces a requirements entropy framework (REF) for measuring requirements trends and estimating engineering effort during system development. The REF treats the requirements engineering process as an open system in which the total number of requirementsR transition from initial states of high requirements entropy H R , disorder and uncertainty toward the desired end state of H Rmin as R increase in quality. The cumulative requirements quality Q reflects the meaning of the requirements information in the context of the SE problem. The distribution of R among N discrete quality levels is determined by the number of quality attributes accumulated by R at any given time in the process. The number of possibilities P reflects the uncertainty of the requirements information relative to H Rmin. The HR is measured or estimated using R, N and P by extending principles of information theory and statistical mechanics to the requirements engineering process. The requirements information I increases as H R and uncertainty decrease, and ΔI is the additional information necessary to achieve the desired state from the perspective of the receiver. The H R may increase, decrease or remain steady depending on the degree to which additions, deletions and revisions impact the distribution of R among the quality levels. Current requirements volatility metrics generally treat additions, deletions and revisions the same and simply measure the quantity of these changes over time. The REF measures the quantity of requirements changes over time, distinguishes between their positive and negative effects in terms of Q, H R , and ΔI , and forecasts when a specified desired state of requirements quality will be reached, enabling more accurate assessment of the status and progress of the engineering effort. Results from random variable simulations suggest the REF is an improved leading indicator of requirements trends that can be readily combined with current methods. The additional engineering effort ΔE needed to transition R from their current state to the desired state can also be estimated. Simulation results are compared with measured engineering effort data for Department of Defense programs, and the results suggest the REF is a promising new method for estimating engineering effort for a wide range of system development programs. C⃝ 2014 Wiley Periodicals, Inc. Syst Eng 17: 462–478, 2014	audio engineer;barry boehm;display resolution;experiment;information theory;interaction;john d. wiley;list of system quality attributes;r language;requirement;requirements engineering;simulation;systems design;systems engineering;volatility	Michael W. Grenn;Shahram Sarkani;Thomas A. Mazzuchi	2014	Systems Engineering	10.1111/sys.21283	reliability engineering;entropy;information;volatility;economics;metric;systems engineering;engineering;operations management;software framework;economic indicator;requirements engineering;theory;measurement;statistics	SE	-70.77618528077893	29.444979822048428	69391
c48fa9a4843fcadfc5ce684dabd0fcad180a5668	considerations on risk communication for it systems and development of support systems	support systems;comunication;of;doi;情報処理学会論文誌 doi;issn;risk management;it risk;computer;journal;ジャーナル;risk communication;一般論文 it risk	Many of society’s systems are dependent on information technology (IT), which means that securing the safety of IT systems is of the utmost importance. Furthermore, numerous stakeholders (managers, customers, employees, etc.) exist in the risk measures decision-making process for these IT systems, which makes it necessary to have a means of communicating risk measures so that stakeholders can easily form a consensus when necessary. For this purpose, we have developed a Multiple Risk Communicator (MRC) to assist in consensus formation within organizations and a Social-MRC system to support social consensus formation, which we have applied to various problems. This paper describes the considerations that IT system risk communication should take, describes the development of the necessary support systems, and provides information on the results of their application.	adobe flash lite;information filtering system;mathematical optimization;risk management;risk measure;semiconductor industry	Ryôichi Sasaki	2012	JIP	10.2197/ipsjjip.20.814	risk management;computer science;it risk;operations research;computer security	SE	-70.14097224261789	5.016980882482084	69427
e1aad42dd27b1fccc532576c2317caa0bb1bf0dc	a systematic literature review of best practices and challenges in follow-the-sun software development	challenges;project management;info eu repo semantics conferenceobject;globalisation;software best practices cultural differences electronic mail global communication testing software engineering;best practice;software houses globalisation project management software development management;software houses;follow the sun fts;global software development;challenges global software development follow the sun fts best practice;software development management;fts implementation follow the sun software development strategy fts software development software projects globally distributed locations communication challenges collaboration challenges software companies task allocation daily project handovers systematic literature review slr	Follow-the-sun (FTS) software development is a strategy used to reduce the length of software projects that are developed across globally distributed locations. However, due to communication and collaboration challenges, software companies find it difficult to adopt this development strategy during task allocation and daily project handovers. In this study, we present results from a Systematic Literature Review (SLR) performed on papers published between 1990 and 2012. Our goal was to identify best practices and challenges for FTS implementation. We found 36 best practices and 17 challenges for FTS. These results are discussed in this paper in order to indicate opportunities for future research and make our results useful for the project managers.	best practice;fleet telematics system;futures studies;software development;software industry;systematic review	Josiane Kroll;Sajid Ibrahim Hashmi;Ita Richardson;Jorge Luis Nicolas Audy	2013	2013 IEEE 8th International Conference on Global Software Engineering Workshops	10.1109/ICGSEW.2013.10	project management;software review;personal software process;long-term support;team software process;simulation;software engineering process group;economics;extreme programming practices;software project management;systems engineering;engineering;social software engineering;software development;software engineering;software as a service;software technical review;software walkthrough;software analytics;management;software deployment;software development process;best practice;software peer review	SE	-67.6605774007731	21.114790767702374	69540
7c5a538fbe809f45d1326e1b58e74bcb94d5f953	software maintenance and reengineering in the days of software agents	yarn;software maintenance;software agent;collaboration;agent oriented software engineering;data engineering;software engineering;software agents;paradigm shift;monitoring;object oriented;artificial intelligence;computer science;software maintenance software agents software engineering data engineering artificial intelligence yarn monitoring collaboration computer science knowledge engineering;knowledge engineering	There is an ongoing paradigm shift in Software Engineering from object-orientation to agent-orientation. We review some of the reasons for this, and briefly overview the state-of-the-art in Agent-Oriented Software Engineering (AOSE). We then sketch some threads of long-term research on autonomic software, software monitoring and diagnosis, and requirements evolution. In addition, we discuss the impact this research may have on how software maintenance and reengineering is done in the future. The research reported is the result of collaborations with colleagues at the Universities of Toronto, Trento and a number of other academic institutions..	agent-oriented software engineering;autonomic computing;code refactoring;programming paradigm;requirement;software agent;software maintenance	John Mylopoulos	2008	2008 12th European Conference on Software Maintenance and Reengineering	10.1109/CSMR.2008.4493293	personal software process;verification and validation;computing;software engineering process group;search-based software engineering;computer science;systems engineering;engineering;social software engineering;component-based software engineering;software development;software agent;software engineering;knowledge engineering;software construction;software walkthrough;software analytics;resource-oriented architecture;software maintenance;software deployment;software requirements;software system;computer engineering;software peer review	SE	-64.51055943119235	25.690262167807944	69588
89b0ad9f33309461b9d2a154a151ddcbeef9db4b	knowledge prioritisation for erp implementation success: perspectives of clients and implementation partners in uk industries		Purpose: Knowledge management (KM) is crucial for Enterprise Resource Planning (ERP) systems implementation in real industrial environments, but this is a highly demanding task. The primary purpose of this study is to examine the effectiveness of knowledge identification, categorisation and prioritisation that contributes to achieving ERP implementation success. Design/methodology/approach: This study adopts a mixed methods approach; a qualitative phase to identify and categorise knowledge types and sub-types; conducting in-depth interviews with ERP clients and implementation partners; plus a quantitative phase to prioritise knowledge types and sub-types based on their contribution to achieving ERP success for business performance improvement. An Analytic Hierarchy Process (AHP) based questionnaire was used to collect empirical data for the quantitative phase.		Uchitha Jayawickrama;Shaofeng Liu;Melanie Hudson Smith	2017	Industrial Management and Data Systems	10.1108/IMDS-09-2016-0390	systems engineering;knowledge management;management science;business	SE	-78.08565597224907	9.340735707091362	69596
5a65065f7521bd862a463823badb1ede11fdbcdf	an empirical study of the effect of complexity, platform, and program type on software development effort of business applications	empirical study;computer platform;online and batch programs;software complexity;function point;empirical studies of programming and software engineering;multiple comparisons;software development;cost estimation;software development effort	Several popular cost estimation models like COCOMO and function points use adjustment variables, such as software complexity and platform, to modify original estimates and arrive at final estimates. Using data on 666 programs from 15 software projects, this study empirically tests a research model that studies the influence of three adjustment variables—software complexity, computer platform, and program type (batch or online programs) on software effort. The results confirm that all the three adjustment variables have a significant effect on effort. Further, multiple comparison of means also points to two other results for the data examined. Batch programs involve significantly higher software effort than online programs. Programs rated as complex have significantly higher effort than programs rated as average.	artificial neural network;batch processing;cocomo;cost estimation in software engineering;data mining;decision tree learning;device driver;function point;futures studies;programming complexity;software development	Girish H. Subramanian;Parag C. Pendharkar;Mary Wallace	2006	Empirical Software Engineering	10.1007/s10664-006-9023-3	personal software process;verification and validation;simulation;software sizing;computer science;systems engineering;engineering;package development process;software development;function point;software engineering;analysis effort method;software construction;empirical process;empirical research;software deployment;multiple comparisons problem;goal-driven software development process;programming complexity;cost estimate;software metric	SE	-66.03988480178513	30.514467285528806	69704
1b8389c99664dcfeab095f7c0285fc07e7ceb624	digital business ecosystem prototyping for smes	information systems;digital business ecosystem;business ecosystem;lisrel model;small to medium sized enterprises;computer software;interoperability;open source	Purpose – The purpose of this paper is to survey, systemize and analyse the e-readiness of small and medium-sized enterprises (SMEs) and determine which new technologies can be applied to build a digital business ecosystem (DBE) for SMEs. Based on the results of a survey, the authors’ objectives are to propose an adequate solution system that uses open source solutions for the SME through the development of a prototype based on a DBE concept. Design/methodology/approach – On the resulting principal components, the authors applied the variance analysis and built two LISREL (a linear structural equation system involving multiple indicators of unmeasured variables) models. LISREL can handle a wide array of problems and models. Based on a survey, an open source prototype solution was developed based on DBE philosophy. Findings – The authors defined different clusters. An SME can then find within the clusters further methods that could be important for their business. The authors developed two LISREL models, aiming to examine the factors that impact the use of the community applications and how they impact them, as well as the factors that impact the increase of the on-line sales. Based on the survey, a prototype system was designed and created that can be used for the set-up of digital business networks. Practical implications – The paper proposes an adequate solution system that uses open source solutions for the SMEs through the development of a prototype based on a DBE concept. Originality/value – Building DBEs helps to create and operate value chains that help enterprises to extend their markets. Through their help, the inquirers and buyers get to know the elements of the value chain, and can therefore make decisions much easier. The paper shows how to define e-attributes of SMEs and clusters to build and use a DBE system for them.	business ecosystem;business process;lisrel;online and offline;open-source software;principal component analysis;prototype;structural equation modeling;transmitter;value (ethics)	Miklos Herdon;Laszlo Varallyai;Adam Pentek	2012	J. Systems and IT	10.1108/13287261211279026	interoperability;simulation;telecommunications;computer science;systems engineering;engineering;knowledge management;marketing;operations management;business ecosystem;management;information system	HCI	-77.85000507841337	6.080964724444002	69748
e7dcbbf4c27377107e7343fe2701dd90c9f559c8	customer relationship management mechanisms: a systematic review of the state of the art literature and recommendations for future research	electronic;information systems;crm;knowledge management;data mining;data quality	In the information systems, customer relationship management (CRM) is the overall process of building and maintaining profitable customer relationships by delivering superior customer value and satisfaction with the goal of improving the business relationships with customers. Also, it is the strongest and the most efficient approach to maintaining and creating the relationships with customers. However, to the best of our knowledge and despite its importance, there is not any comprehensive and systematic study about reviewing and analyzing its important techniques. Therefore, in this paper, a comprehensive study and survey on the state of the art mechanisms in the scope of the CRM are done. It follows this goal by looking at five categories in which CRM plays a significant role: E-CRM, knowledge management, data mining, data quality and, social CRM. In each category, a couple of studies are presented and determinants of CRM are described and discussed. The major development in these five categories is reviewed and the new challenges are outlined. Also, a systematic literature review (SLR) in each of these five categories is provided. Furthermore, insights into the identification of open issues and guidelines for future research are provided. © 2016 Elsevier Ltd. All rights reserved.	customer relationship management;data mining;data quality;information system;knowledge management;systematic review	Zeynab Soltani;Nima Jafari Navimipour	2016	Computers in Human Behavior	10.1016/j.chb.2016.03.008	customer relationship management;electronics;data quality;computer science;knowledge management;management science;information system	AI	-79.05435284280492	8.307229362805199	69934
5480e594b01956e3e64ef7665f25fde850e98410	high-tech sector in the conditions of institutionalization of the smart economy (on the example of the telecommunication industry)		The article contains a theoretical analysis of the institutionalization of the Smart Economy as a new type economy on the materials of communications industry. The analysis is based on the research of interrelations between the concepts of “cluster”, “network transactions” and “collaboration”. The perspectives of participation of the telecommunications industry enterprises in the clusters formation are shown. The conjunction of the merits of the hierarchical and market structures in the clusters is due to the network nature of clusters, the possibility of combining of the cooperation and the competition between participants. The proliferation of clusters in the high-tech sectors of economy is associated with the advantages of network transactions. Specific characteristics of network type transactions and their advantages to the interacting parties are identified. The awareness of the levels of institutionalization of the Smart Economy and the consistent movement on the chain “clusters - network transactions - collaboration” when building relations between the cluster participants will allow a qualitative solution of a number of problems facing the Russian communications industry.		N. V. Vasilenko;A. J. Linkov;Vladimir V. Glukhov	2017		10.1007/978-3-319-67380-6_29	market structure;economy;telecommunications;business;institutionalisation;high tech	ML	-86.2245865679736	10.812310155881475	69952
ffa58300087245754822acb5c6fc39ab21d96562	a survey of experienced user perceptions about software design patterns	software design;survey;software design patterns	0950-5849/$ see front matter 2012 Elsevier B.V. A http://dx.doi.org/10.1016/j.infsof.2012.11.003 ⇑ Corresponding author. E-mail address: david.budgen@durham.ac.uk (D. B Context: Although the concept of the software design pattern is well-established, there is relatively little empirical knowledge about the patterns that experienced users consider to be most valuable. Aim: To identify which patterns from the set catalogued by the ‘Gang of Four’ are considered to be useful by experienced users, which ones are considered as not being useful, and why this is so. Method: We undertook a web-based survey of experienced pattern users, seeking information about their experiences as software developers and maintainers. Our sampling frame consisted of the authors of all of the pattern papers that we had identified in a preceding systematic review of studies of patterns. Results: We received 206 usable responses, corresponding to a response rate of 19% from the original sampling frame. Most respondents were involved with software development rather than maintenance. Conclusion: While patterns can provide a means of sharing ‘knowledge schemas’ between designers, only three patterns were widely regarded as valuable. Around one quarter of the patterns gained very low approval or worse. These observations need to be considered when using patterns; teaching students about the pattern concept; and planning empirical studies about patterns. 2012 Elsevier B.V. All rights reserved.	sampling (signal processing);software design pattern;software developer;software development;systematic review;web application	Cheng Zhang;David Budgen	2013	Information & Software Technology	10.1016/j.infsof.2012.11.003	computer science;knowledge management;software design;software engineering;data mining;database;world wide web	SE	-73.01489285746086	24.495601836205665	69957
98a7d71bbc4b047bdc23550e4ca4a50d70b5117f	systemic decision making for liquidity risk management in banks		The outbreak of the recent financial crisis reveals significant problems in current bank practices in conventional liquidity risk management. To avoid catastrophic consequences, a holistic view, which captures the dynamic interactions between liquidity and other financial variables, should be taken to help banks make business decisions. However, few studies in the literature have addressed this problem. To fill the research gap, we present a Systemic decision making approach for Liquidity Risk Management (SLRM) as a more advanced alternative to Conventional Liquidity Risk Management (CLRM) by capturing dynamic factors, offering logic visibility, and considering rare but fatal events. We show that SLRM can be used to support managerial decisions in developing contingency plans for liquidity management. SLRM is validated by using real data from Washington Mutual, a US bank failed during the 2008 financial tsunami. Further, we demonstrate that SLRM can also help banks conform to regulatory changes in Basel III.	contingency plan;holism;interaction;risk management	Xiaoyu Wu;Leon Zhao	2012			computer science;basel iii;financial crisis;financial risk management;system dynamics;liquidity risk;liquidity crisis;finance;financial system;contingency plan;market liquidity	HCI	-75.06861251730851	11.094280278680362	69961
68046003361147d0c26084a122bd6fc91ed8eb27	the future of java-based simulation	internet;java;digital simulation;object-oriented programming;software reusability;software tools;internet browser software;java-based simulation;development environments;model development;object-oriented components;simulation components;simulation models;simulation software firms;software reuse	Java-based simulation presents a unique opportunity revolutionary changes in the process of develop simulation models and in the mission of the simulati software firms that provide tools to support the mod development process. Java enables a new vision o simulation industry populated by application-specif simulation specialists who generate compatible a reusable simulation components. These object-orien components can be developed using inexpens professional-quality Java development environments a executed using Internet browser software. This discuss is an overview of the features and future benefits of Ja based simulation. It is targeted at experienced simula practitioners who understand the limitations of existi tools and the need for object-oriented, standardized reusable modeling software.	java;population;simula;simulation	Richard A. Kilgore;Kevin J. Healy;George B. Kleindorfer	1998			reusability;personal software process;verification and validation;software sizing;computer science;systems engineering;package development process;backporting;software design;social software engineering;software framework;component-based software engineering;software development;software engineering;software construction;real time java;software walkthrough;programming language;object-oriented programming;resource-oriented architecture;java;software deployment;software development process;software system;software peer review	HPC	-63.706433303374595	25.855664403488802	69968
6eed2c5f3405ea568849d596e88997184464e0f5	debugging-workflow-aware software reliability growth analysis		Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione, Università degli Studi di Napoli Federico II, Naples, 80125, Italy Correspondence Stefano Russo, DIETI Università degli Studi di Napoli Federico II, Via Claudio 21, Naples 8015, Italy. Email: sterusso@unina.it Funding information ICEBERG, Grant/Award Number: (324356); GAUSS, Grant/Award Number: (CUP E52F16002700001) Summary Software reliability growth models support the prediction/assessment of product quality, release time, and testing/debugging cost. Several software reliability growth model extensions take into account the bug correction process. However, their estimates may be significantly inaccurate when debugging fails to fully fit modelling assumptions. This paper proposes debugging-workflow-aware software reliability growth method (DWA-SRGM), a method for reliability growth analysis leveraging the debugging data usually managed by companies in bug tracking systems. On the basis of a characterization of the debugging workflow within the software project under consideration (in terms of bug features and treatment phases), DWA-SRGM pinpoints the factors impacting the estimates and to spot bottlenecks, thus supporting process improvement decisions. Two industrial case studies are presented, a customer relationship management system and an enterprise resource planning system, whose defects span a period of about 17 and 13 months, respectively. DWA-SRGM revealed effective to obtain more realistic estimates and to capitalize on the awareness of critical factors for improving debugging.	bug tracking system;cups;customer relationship management;debugging;digital watermarking alliance;email;enterprise resource planning;gauss;population dynamics;software project management;software quality;software reliability testing	Marcello Cinque;Domenico Cotroneo;Antonio Pecchia;Roberto Pietrantuono;Stefano Russo	2017	Softw. Test., Verif. Reliab.	10.1002/stvr.1638	software quality;software engineering;software construction;software regression;reliability engineering;debugging;software;software reliability testing;software metric;workflow;computer science	SE	-64.90645884493267	31.08039453846385	69973
6ecb441b4db5544b86cfd66f2b7aa5bcab1ec01d	investigating benefits and limitations of e-procurement in b2b automakers companies in iran	skk e procurement b2b automaker companies foreign automakers iranian automakers iranian economy wto automotive industry zamyad topco saze gostare saipa sanaye khodrosazi kerman sapco sgs;automobile manufacture;production engineering computing automobile industry automobile manufacture industrial economics procurement;procurement;industrial economics;companies procurement decision making manufacturing law;exploring;production engineering computing;e procurement;beteendevetenskap;samhalls;e procurement benefits limitations;social behaviour law;benefits;process;juridik;automobile industry;limitations	There is severe competition among big foreign automakers who already have best management systems as well as the minimum finished cost by using high technology. So, it is necessary for Iranian automakers to replace ancient managerial methods with newest ones. Moreover, joining to WTO which is another target of Iranian economy makes this situation clear that we have to present new methods. Eprocurement will be one of the solutions for improving competition capability in automotive industry in Iran. This research aims to explore the degree of e-procurement usage and finding benefits and limitations of applying it in five main automotive Iranian companies included: 1. Zamyad (Z), 2. Topco (T), 3. Saze Go stare SAIPA (SGS), 4. Sanaye Khodrosazi Kerman (SKK) 5. Sapco(S).	best practice;e-procurement;iranian.com;procurement;strong generating set	Behnam Bahreman	2013	2013 10th International Conference on Information Technology: New Generations	10.1109/ITNG.2013.119	procurement;computer science;e-procurement;management;process	Robotics	-72.96679417933903	5.38730356011156	70028
bc70aa90d3353fdcf653092c6db892d9d2237ddb	exploring expectations about risk-based testing: towards increasing effectiveness and efficiency		Risk-based testing is sometimes reduced to an approach that focuses on cutting costs and time in testing. While the high effort involved in testing makes efficiency an important issue, for many companies the main concern is still to find the critical defects in their software products. Such defects can cause costly remedial upgrades and fixes, and they threaten the company’s long-term business success. In this paper we explore how the two goals “effectiveness” and “efficiency” motivate a risk-based testing approach in different organizations. Furthermore, we identify a third goal, summarized as “management support”. In a survey conducted as part of a tutorial on risk-based testing we investigated common expectations and potential benefits associated with these three goals. The results indicate that the main motivation for a risk-based approach is making testing more efficient. Nevertheless, efficiency and effectiveness are not conflicting goals and the main challenge is therefore finding strategies that increase the overall benefit of including risk information in testing.	risk-based testing	Michael Felderer;Rudolf Ramler	2016		10.1007/978-3-319-49094-6_56	management science;risk-based testing;test management;software;engineering;remedial education	SE	-69.6627104261431	22.625669490544166	70048
d066d678b2f04af3fa3e6a470204e9968445ba7d	models for improving software system size estimates during development	software size;ada;software systems;metrics;re estimation;regression;estimation	This paper addresses the challenge of estimating eventual software system size during a development project. The approach is to build a family of estimation models that use information about architectural design characteristics of the evolving software product as leading indicators of system size. Four models were developed to provide an increasingly accurate size estimate throughout the design process. Multivariate regression analyses were conducted using 21 Ada subsystems, totaling 183,000 lines of code. The models explain from 47% of the variation in delivered software size early in the design phase, to 89% late in the design phase.	a* search algorithm;ada;coefficient of determination;general linear model;mathematical model;software development;software sizing;software system;source lines of code;unit of observation;visual intercept	William W. Agresti;William M. Evanco;William M. Thomas	2010	JSEA	10.4236/jsea.2010.31001	reliability engineering;estimation;regression testing;real-time computing;ada;regression;software sizing;computer science;systems engineering;software reliability testing;software engineering;programming language;software regression;metrics;goal-driven software development process;use case points;software metric;statistics;software system	SE	-63.06062164427372	30.95005348073459	70060
b74818ca119a863301f80451bbf104c11b8c30d7	the linkages among horizontal strategy, person-environment fit and strategic human resource management	competitive potential hr strategy horizontal strategy person environment fit strategic human resource management aligned corporate strategy recruitment strategy systematic fit;systematics;resource management;bibliographies;person environment fit and strategic human resource management horizontal strategy;organizations;couplings;organizations human resource management systematics couplings bibliographies resource management;human resource management;recruitment	The main purpose of this paper is to identify relationship among horizontal strategy, person environment fit and strategic human resource management. This paper shows that how important to aligned corporate strategy to recruitment strategy or HR strategy for organization's long-term success and performance. Horizontal fit act as the sustainable competitive advantage of horizontal strategy for organization. More specifically, we present a conceptual framework, in which illustrates how person-environment fit, systematic fit and competitive potential HR strategy have impact on horizontal strategy through horizontal alignment.	strategic management	Pratima Verma;R. R. K. Sharma	2015	2015 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)	10.1109/IEEM.2015.7385624	strategic planning;economics;organization;knowledge management;resource management;operations management;competitor analysis;human resource management;systematics;coupling;technology strategy;management;strategic fit	Robotics	-80.41963567863317	4.667341363020898	70100
b3d22503ec549a89de71bac0cde7a761b0c4252b	adoption of social responsibility through the expansion of existing management systems	corporate social responsibility;management system;socially responsible;iso 9000 series;quality management system;standardization;supply chain management;quality management;internal standard;design methodology	Purpose – The paper's aims are: to scrutinize International Organization for Standardization (ISO) 26000 international guidance standard for social responsibility; to discuss the role of this standard in the adoption of social responsibility in organisations with existing quality management systems in line with ISO 9000.Design/methodology/approach – Building on academic literature and recent resolutions and working documents from ISO, the paper discusses ISO 9000 and ISO 26000 in five areas: general description; scope; approach to supply chain management; principles and content.Findings – Provides a comparison between ISO 9000 and ISO 26000 in each area listed above. Argues for ISO 9000 to provide a structural and infrastructural platform for organisations to develop and adopt corporate social responsibility (CSR) and for ISO 26000 to facilitate a shift from customer focus to stakeholder focus hence creating a business‐to‐society orientation in organisations.Practical implications – A very useful source f...		Pavel Castka;Michaela A. Balzarova	2008	Industrial Management and Data Systems	10.1108/02635570810858732	iso/iec 9126;public relations;quality assurance;quality management;supply chain management;quality management system;information technology infrastructure library;economics;design methods;iso 9000;environmental resource management;marketing;operations management;social responsibility;internal standard;management system;management;quality of analytical results;standardization;corporate social responsibility	DB	-70.77513542551476	13.306898055091677	70200
eabab23c2e7aeff1a5cd10d817f09a69dbf0e577	in search of software maintenance productivity and quality: does software complexity matter?	software development;software reliability;data collection;life cycle;software maintenance;software complexity;error rate;software measurement;programming language;three dimensions;cobol	Over the past several decades, software maintenance has been absorbing a large and rising proportion of Information Systems (IS) resources with expenditures often as high as 80% to 95% of the total IS budget (Nosek and Palvia 1990). On a life-cycle basis, about three-fourths of the investment in software occurs a#er the system has been implemented. Thus, support for existing software represents a significant investment of resources for most firms and there is considerable ititerest in understanding and improving productivity and quality in the software maintenance task. Software complexity is believed to be a major factor influencing performance in software maintenance, impacting maintenance costs (Banker, et at. 1993) and software error rates (Shen, et al. 1985). A principal objective of our research program, therefore, is to analyze the implications of software complexity for the support of an extensive application portfolio. Since software complexity is recognized as a psychological phenomenon, we draw upon Wood's (1986) psychological framework for task complexity to conceptualize three dimensions of software complexity: component, coordinative and dynainic complexity. We test our model of software complexity in a COBOL environment at a major national mass merchandising retailer. The study of software maintenance in this kind of environment is important. Although more than 60% of business expenditures for computing are devoted to maintenance of software written in COBOL, the majority of prior research iii software maintenance focusses on non-COBOL programming languages (Hale and Haworth 1988). Fieldwork is complete, but data analysis and conclusions are still at an early stage. Data collection has been accomplished in two phases. In the first phase, we collected productivity anc] software measures for a number of software maintenance projects that were coinpleted over a two year timeframe. In the second phase of data collection, we obtained thirty months of software error measures for a 5,000 module application portfolio, which we are analyzing to investigate the relationship between software complexity and software reliability. Preliminary results suggest that. controlling for project team skill and experience, Ihere is a strong effect for software complexity on software maintenance effort and software reliability. We also find that, contrary to the initial expectations of IS managers at the research site, the use of a software development productivity tool for generating code does not translate into a reduction in maintenance project effort. These results have interesting implications for software maintenance practice and research.	cobol;cycle basis;datar;field research;programming complexity;programming language;software bug;software development;software maintenance;software reliability testing	Rajiv D. Banker;Sandra Slaughter	1994			software peer review;marketing;backporting;software construction;software design description;package development process;software maintenance;social software engineering;computer science;software sizing	SE	-66.50109489573187	32.139995524791686	70263
a7fe8359821113e3fdf8ae4be821c54dbc50addd	special issue: organizational design: multiple faces of codification: organizational redesign in an it organization	outsourcing;core rigidity;ethnography;objectification;qualitative methods;information technology;organizational design;redesign;organizational change;it implementation;codification;organizational theory	This paper details a longitudinal interpretative field study of an information technology (IT) organization in which a new chief information officer (CIO) implemented a major organizational redesign. The redesign increased the degree of codification in activities of the IT organization so as to control, coordinate, and deliver services more cost effectively to its business clients. We examine different stakeholders views of the change, the implementation processes, and the consequences of the redesign.#R##N##R##N#The case analysis emphasizes specific challenges that designers of support organizations face when increasing the degree of codification. Key implications include the need for these designers to (1) pay as much, if not more, attention to the local organizational context as they do to the external environmental conditions; (2) communicate and negotiate constantly with various stakeholders concerning the appropriate degree of codification and control; (3) be wary of how a strict alignment of all design elements can blind the designer to important, unrecognized issues; and (4) consider that increased codification may help support organizations compete more efficiently with external vendors, but may also ease the process of outsourcing.		Emmanuelle Vaast;Natalia Levina	2006	Organization Science	10.1287/orsc.1050.0171	organizational theory;economics;knowledge management;qualitative research;operations management;ethnography;management;information technology;organizational architecture;outsourcing	Robotics	-78.006517388556	4.854486379783095	70274
6ed67a876b3afd2f2fb7b5b8c0800a0398c76603	erp integration in a healthcare environment: a case study	inventory management;medical informatics;distribution and inventory management;manufacturing resource planning;procurement;process integration;greece;enterprise integration;organizational change;erp implementation;health services;medical informatic;action research;software implementation;design methodology	Purpose – Aims at providing a classification of the ERP integration concept in a healthcare organization and at presenting a real world example of process integration using SAP R/3.Design/methodology/approach – Research is based on a case study involving in‐depth semi‐structured interviews with key stakeholders and action research conducted in the hospital during the ERP implementation period.Findings – Findings suggest that an apparently simple software implementation of an ordering process can have a considerable impact on stakeholders in a complex environment operating ERP software. Organizational change issues, implementation and integration issues of SAP R/3 with other non‐SAP systems and SCM considerations are discussed.Originality/value – Analyzes enterprise integration concept specifically in a healthcare environment and describes a real world process integration solution (missing from the literature) achieved by using SAP R/3 software.	erp	Constantinos J. Stefanou;Andreas Revanoglou	2006	J. Enterprise Inf. Management	10.1108/17410390610636913	health informatics;procurement;design methods;systems engineering;engineering;knowledge management;marketing;operations management;action research;enterprise integration;management;process integration	DB	-78.47935385098343	8.702580924457436	70306
2aadaebcc7d3b5b3cf3d7b9275eb9de8e2f6b380	a qualitative study of devops usage in practice		Organizations are introducing agile and lean software development techniques in operations to increase the pace of their software development process and to improve the quality of their software. They use the term DevOps, a portmanteau of development and operations, as an umbrella term to describe their efforts. In this paper, we describe the ways in which organizations implement DevOps and the outcomes they experience. We first summarize the results of a systematic literature review that we performed to discover what researchers have written about DevOps. We then describe the results of an exploratory interview-based study involving 6 organizations of various sizes that are active in various industries. As part of our findings, we observed that all organizations were positive about their experiences and only minor problems were encountered while adopting DevOps.	devops	Floris Erich;Chintan Amrit;Maya Daneva	2017	Journal of Software: Evolution and Process	10.1002/smr.1885	software development process;systems engineering;engineering;devops;agile software development;software;systematic review;qualitative research;lean software development;systems development life cycle	SE	-69.25596048257025	22.294569433150713	70441
f0df15646dd49a15532c49e37b65acc91da14ee5	strategic information systems and financial performance	subsample analysis;sis result;support growth;long-term competitive advantage;sis firm;successful employment;sis implementation;financial performance;competitive advantage;long-term financial success;strategic information system;empirical evidence	Investment in strategic information systems (SIS) is advocated by numerous authors as an important way for firms to seek competitive advantage. Yet there is still little empirical evidence that implementation of SIS results in long-term competitive advantages. This is primarily due to the difficulty of isolating economic benefits attributable to SIS implementation. In this research, thirty-five sample firms identified in the media for successful employment of SIS are analyzed for evidence of long-term financial success. This analysis is conducted over a thirteen-year period, centered around the year in which firms were identified for employment of SIS to either support growth, control costs, form alliances, differentiate products, or provide innovation of products/processes. Results show that the stock market reacted favorably to announcements that firms were using SIS, and in subsequent years those firms tended to be more productive and more profitable than their industries and than firms in their respective industries. The greatest profitability differences occurred in the years after SIS firms were initially recognized as successfully employing SIS or investing in SIS. Further, a subsample analysis revealed that the positive results were primarily driven by those firms employing SIS to support growth.	information systems;strategic information system	Robert M. Brown;Amy W. Gatian;James O. Hicks	1995	J. of Management Information Systems		productivity;economics;marketing;operations management;strategic information system;management;information system;competitive advantage;commerce;profitability index	OS	-83.16285895152605	6.036022889473322	70504
101d0cc373a55c06b91b18c786cd7145591c7ea2	understanding users' behavior with software operation data mining	software analytics;data mining;log data;software operation knowledge;user behavior;software usage	Software usage concerns knowledge about how end-users use the software in the field, and how the software itself responds to their actions. In this paper, we present the Usage Mining Method to guide the analysis of data collected during software operation, in order to extract knowledge about how a software product is used by the end-users. Our method suggests three analysis tasks which employ data mining techniques for extracting usage knowledge from software operation data: users profiling, clickstream analysis and classification analysis. The Usage Mining Method was evaluated through a prototype that was executed in the case of Exact Online, the main online financial management application in the Netherlands. The evaluation confirmed the supportive role of the Usage Mining Method in software product management and development processes, as well as the applicability of the suggested data mining algorithms to carry out the usage analysis tasks. 2013 Elsevier Ltd. All rights reserved.	algorithm;clickstream;data mining;prototype;software product management;usage analysis	Stella Pachidi;Marco R. Spruit;Inge van de Weerd	2014	Computers in Human Behavior	10.1016/j.chb.2013.07.049	software visualization;personal software process;long-term support;verification and validation;software sizing;software mining;computer science;package development process;data science;software development;software design description;software construction;software as a service;data mining;database;software walkthrough;software analytics;software deployment;world wide web;software system;software peer review	SE	-72.78924111257062	26.131286178843116	70632
34efa5f8bf52216660dfc6d919121f14da16bad1	commercial internet adoption in china: comparing the experience of small, medium and large businesses	comparative analysis;computacion informatica;cost saving;small firms;foreign countries;internet;statistical analysis;ciencias basicas y experimentales;business;small to medium sized enterprises;cost effectiveness;grupo a;china;competitive advantage;empirical research	Earlier research studies predicted that it would be small and medium‐sized businesses that were more likely to adopt and benefit from the use of the Internet because of their greater flexibility. Anecdotal evidence appears in the literature to support this claim; however, little systematic empirical research has been done among SMEs to test this speculation. A sample of 248 companies in Shanghai, China, was divided into small, medium and large groups. The statistical analysis indicates that there are significant differences between large and small companies. Large companies have benefited considerably more from the Internet than small companies not only in their increased sales (derived from the Internet) but also from cost savings. Although the whole sample confirms the main reason for establishing an Internet connection, to gain a competitive advantage, companies also think that the Internet does not work equally for all players.		Hernan Riquelme	2002	Internet Research	10.1108/10662240210430946	social science;marketing;sociology;empirical research;china;competitive advantage;commerce	OS	-85.22765453099251	6.664101351370411	70742
8b49aa18a01e6d2a47c1a61c709403cd18ae1b46	managing risks in distributed software projects: an integrative framework	software;project management;systematic review;web based tool;software management;risk management;collaboration;software management project management risk management;distributed software projects;data mining;web based tool geographically distributed software project risk management software development risk resolution technique;software development;geographically distributed software project;book reviews;risk management communication and collaboration distributed software projects;organizations;risk resolution technique;empirical evaluation;programming;face to face;communication and collaboration;geographic distribution;software collaboration book reviews organizations programming risk management data mining	Software projects are increasingly geographically distributed with limited face-to-face interaction between participants. These projects face particular challenges that need careful managerial attention. While risk management has been adopted with success to address other challenges within software development, there are currently no frameworks available for managing risks related to geographical distribution. On this background, we systematically review the literature on geographically distributed software projects. Based on the review, we synthesize what we know about risks and risk resolution techniques into an integrative framework for managing risks in distributed contexts. Subsequent implementation of a Web-based tool helped us refine the framework based on empirical evaluation of its practical usefulness. We conclude by discussing implications for both research and practice.		John Stouby Persson;Lars Mathiassen;Jesper Boeg;Thomas Stenskrog Madsen;Flemming Steinson	2009	IEEE Transactions on Engineering Management	10.1109/TEM.2009.2013827	project management;programming;systematic review;economics;risk management;systems engineering;organization;engineering;knowledge management;software development;data mining;management;collaboration	SE	-71.76265202922937	20.143106932124486	70761
6c4bff0030fc81f0210702ca270a14fc48ed5e96	experimental software engineering: a report on the state of the art	research needs;probability density function;common ground;software systems;data mining;software engineering;professional development;experimental software engineering;software development;electrical engineering	The goal of this session is to make the software engineering community aware of the opportunities that exist to pursue such an experimental approach. In the remainder of the essay, we describe an emerging model for empirical work and the language for discussing it. We then focus on the current state of experimental software engineering, the road blocks barring effective progress, and what developers and researchers can do to remove them.	experimental software engineering	Lawrence G. Votta;Adam A. Porter	1995	1995 17th International Conference on Software Engineering	10.1145/225014.225040	professional development;personal software process;probability density function;verification and validation;software engineering process group;software sizing;software verification;search-based software engineering;computer science;systems engineering;engineering;package development process;social software engineering;component-based software engineering;software development;software engineering;software construction;software walkthrough;software analytics;software deployment;software development process;software requirements;software system;computer engineering;software peer review	SE	-65.85047721489485	24.326038829220014	70764
5975efd122f8a8c40a0f1da7a17af7f4e9c0d153	introducing agile software development at sap ag - change procedures and observations in a global software company	agile software development	This paper describes the change process that is taking place at SAP AG to move the software development processes from a waterfall-like approach to agile methodologies. This change affects about 18,000 developers working at 12 global locations. The paper outlines the procedure to introduce Scrum as an implementation of lean development, and the model chosen to scale Scrum up to large product development projects. The most important observations are described, and an outlook on future improvement is given.	agile software development;microsoft outlook for mac;new product development;scrum (software development);software development process	Joachim Schnitter;Olaf Mackert	2010			p-modeling framework;personal software process;verification and validation;agile unified process;extreme programming practices;agile usability engineering;computer science;systems engineering;engineering;social software engineering;software development;software engineering;release management;software construction;agile software development;empirical process;lean software development;software deployment;software development process	SE	-67.14991581369328	22.02999943640077	70789
84778a22673f192ae1e0dabb64b6df80084258c7	embracing technical debt, from a startup company perspective		"""Software startups are typically under extreme pressure to get to market quickly with limited resources and high uncertainty. This pressure and uncertainty is likely to cause startups to accumulate technical debt as they make decisions that are more focused on the short-term than the long-term health of the codebase. However, most research on technical debt has been focused on more mature software teams, who may have less pressure and, therefore, reason about technical debt very differently than software startups. In this study, we seek to understand the organizational factors that lead to and the benefits and challenges associated with the intentional accumulation of technical debt in software startups. We interviewed 16 professionals involved in seven different software startups. We find that the startup phase, the experience of the developers, software knowledge of the founders, and level of employee growth are some of the organizational factors that influence the intentional accumulation of technical debt. In addition, we find the software startups are typically driven to achieve a """"good enough level,"""" and this guides the amount of technical debt that they intentionally accumulate to balance the benefits of speed to market and reduced resources with the challenges of later addressing technical debt."""	principle of good enough;technical debt;tree accumulation	Terese Besker;Antonio Martini;Rumesh Edirisooriya Lokuge;Kelly Blincoe;Jan Bosch	2018	2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)	10.1109/ICSME.2018.00051	process management;new product development;systems engineering;computer science;software;software development;codebase;technical debt	SE	-69.59119480121225	23.195919464775105	70815
3e18d7a7acb24ade47878c856310c415abfd5d56	towards a global software development maturity model	maturity model	Build softwares have always been a challenge. To shape and to implement a computational viable solution involves a lot of technical and social questions (referring to the interaction between stakeholders). This complexity increases, significantly, when dispersed global teams are used. The necessity to have a set of processes better to organize the development strategy appears as one of the main challenges to be explored. The objective of this article is to present a proposal of structure for a maturity model for global software development. The study is based on an ample theoretical revision on the structures of the main maturity and government models of information technology. The empirical base of this study will involve a multinational organization of software development with branch offices in Brazil, Russia and India.	capability maturity model;computation;computational biology;software development	Leonardo Pilatti;Jorge Luis Nicolas Audy	2005			knowledge management;systems engineering;computer science;service integration maturity model;software development;capability maturity model	SE	-73.16755125693544	16.951589109429037	70887
18cb3c162969ec65e5c2097b3f266197a6ba67fc	supply chain performance sustainability through resilience function	wider supply chain;manufacturing enterprise;resilience function;massive improvement;sd model;supply chain performance sustainability;supply chain;strength point;business world;critical issue;recent year;supply chain risk;resilience;risk management;supply chains;supply chain management	Today's business world faces challenges and pressures on an unprecedented scale. Many of these obstacles have the potential to severely affect the continuity of a manufacturing enterprise, in particular, through disruption to the wider supply chain. Indeed, it can be argued that supply chain risk is currently greater now than ever before. Resilience is one of the ways to combat disruptions in the supply chain. In this paper the behavior of a Supply chain is studied using a SD model built with Powersim. The paper describes the process of building the model and utilizes the model to demonstrate the massive improvement that resilience can bring in a manufacturing enterprise. The critical issues and strength points in a supply chain are analyzed, in particular, trying to improve their resilience, a feature that has gained even more importance in recent years.	denial-of-service attack;markov chain;scott continuity;supply chain attack	Teresa Murino;Elpidio Romano;Liberatina Carmela Santillo	2011	Proceedings of the 2011 Winter Simulation Conference (WSC)		supply chain risk management;demand chain;supply chain management;risk management;service management;systems engineering;supply chain	AI	-74.49649578735561	7.269865784659028	70889
90d1a3f2cef885e82948babe5305bdd6cd45c4db	intelligent service for finding implicit factors		"""The issues of identifying, accounting for, and interpreting implicit factors, as well as assessing their impact on an organization's performance, is multifaceted and requires rigorous scientific research as they grow in number and importance for the decision-making process in the context of digital economy. The paper proposes solutions to the identified issues that will make an organization's management more efficient and, ultimately, will improve the economic position of an organization in the relevant market segment with minimal costs by using an intelligent web service to find implicit factors - """"Implicit influences Joomla component"""""""	joomla!;mechatronics;web service	Nazarov Dmitrii	2018	2018 IEEE 20th Conference on Business Informatics (CBI)	10.1109/CBI.2018.10063	management science;web service;data modeling;service-oriented architecture;relevant market;business;digital economy	Visualization	-72.63176945160488	11.28068496926299	70895
8e43c5b84d9f71181bcebc0daf411afa9ed488fe	conflicts of interest, information quality and management decision		The existence of conflict of interests (COI) within different firms’ level of management is often cited as the reason for the relationship for low information quality and ineffective decision making. We reveal how COI affects negatively decision making through low information quality longer time to produce information quality, shrinking optimal information quantity, and increasing in the information cost. Although, COI has been studied extensively by different organizational studies, there is no research showing how to measure the effect of COI on information quality from mathematical modeling perspective. Since effective and efficient management decisions closely depend on information quality, this paper provides the solutions to this problem.	information quality	Saeed Askary;Amrendra Singh Yadav	2018		10.1007/978-3-030-02131-3_27	environmental economics;conflict of interest;information quality;resizing;organizational studies;business	Theory	-84.1319033584167	4.604080902158154	70910
d5715f805a418faefb63d13464005a64e0092776	improving business processes through mobile apps - an analysis framework to identify value-added app usage scenarios	analysis framework;mobile application;business processes	Mobile apps offer new possibilities to improve business processes. However, the introduction of mobile apps is typically carried out from a technology point of view. Hence, process improvement from a business point of view is not guaranteed. There is a methodological lack for a holistic analysis of business processes regarding mobile technology. For this purpose, we present an analysis framework, which comprises a systematic methodology to identify value-added usage scenarios of mobile technology in business processes with a special focus on mobile apps. The framework is based on multi-criteria analysis and portfolio analysis techniques and it is evaluated in a case-oriented investigation in the automotive industry.	action potential;business process;fits;holism;mobile app;modern portfolio theory;point of view (computer hardware company);programming tool;synergy	Eva Hoos;Christoph Gröger;Stefan Kramer;Bernhard Mitschang	2014		10.5220/0004897100710082	simulation;knowledge management;artifact-centric business process model;mobile business development;business process;business process discovery;management;business process modeling	HCI	-72.42561315996028	10.412852931154472	70968
5d783e34b221e77fa4f821ae4870a3fa06cd90f5	project report: high-reliable object-oriented embedded software design	science and technology;uml;educational technology;embedded systems;formal verification;embedded software;software design;software engineering;software reliability;formal specification;software development;model checking;object oriented;object oriented programming	The high-reliable object-oriented embedded software design project is an industry-university joint research project and has been launched in 2003 as a part of e-society project, supported by Ministry of Education, Culture, Sports, Science and Technology, Japan. The target of this five-year project is to develop the method and environment for high-reliable embedded software development for civilian industry such as automobile, communication, and control and consumer-electronics fields. The challenge of the project is to establish the practical usage of the latest achievement of software science and software engineering at the reasonable cost for actual development practice. In this paper, we introduce the approach, vision and plan of the project.	deadlock;e-society;embedded software;embedded system;finite-state machine;model checking;promela;refinement (computing);spin;sequence diagram;software design;software development;software engineering;unified modeling language;verification and validation;xojo	Tomoji Kishi;Toshiaki Aoki;Shin Nakajima;Natsuko Noda;Takuya Katayama	2004	Second IEEE Workshop on Software Technologies for Future Embedded and Ubiquitous Systems, 2004. Proceedings.	10.1109/WSTFEUS.2004.10005	project management;personal software process;verification and validation;extreme project management;work breakdown structure;project;software project management;opm3;computer science;systems engineering;social software engineering;software development;software design description;software engineering;software construction;systems development life cycle;project management 2.0;project management triangle;project charter;software development process;project planning;computer engineering	SE	-63.704774641513964	25.56616413455519	71019
dbdf614799e540cfe6945d2eaf633a958455a92b	repository mining and six sigma for process improvement	repositories;gqm;six sigma;process improvement	In this paper, we propose to apply artifact mining in a global development environment to support measurement based process management and improvement, such as SEI/CMMI's GQ(I)M and Six Sigma's DMAIC. CMM has its origins in managing large software projects for the government and emphasizes achieving expected outcomes. In GQM, organizational goals are identified. The appropriate questions with corresponding measurements are defined and collected. Six Sigma has its origins in manufacturing and emphasizes reducing cost and defects. In DMAIC, a major component of a Six Sigma approach, sources of waste are identified. Then changes are made in the process to reduce effort and increase the quality of the product produced. GQM and Six Sigma are complementary. Both approaches rely heavily on the measurement of input and output metrics. Mining development artifacts can provide usable metrics for the application of DMAIC and GQM in the software domain.		Michael VanHilst;Pankaj Kumar Garg;Christopher Lo	2005	ACM SIGSOFT Software Engineering Notes	10.1145/1082983.1083157	design for six sigma;reliability engineering;dmaic;systems engineering;engineering;gqm;development environment;six sigma	SE	-69.08075193065632	21.54657691692165	71059
1ca46d03d9d2456ce11e0227f6aa74dc6ab516cf	usage-centric adaptation of dynamic e-catalogs	navegacion;site web;catalogue electronique;concepcion sistema;metodo formal;methode formelle;systeme integre;sistema integrado;data mining;catalogue web;formal method;dynamic environment;navigation;fouille donnee;system design;interaction pattern;sitio web;integrated system;busca dato;conception systeme;web site	Although research into the integration of e-catalogs has gained considerable momentum over the years, the needs for building adaptive catalogs have been largely ignored. Catalogs are designed by system designers who have a priori expectations for how catalogs will be explored by users. It is necessary to consider how users are using catalogs since they may have different expectations. In this paper, we describe the design and the implementation of a system through which integrated product catalogs are continuously adapted and restructured within a dynamic environment. The adaptation of integrated catalogs is based on the observation of customers’ interaction patterns.		Hye-Young Paik;Boualem Benatallah;Rachid Hamadi	2002		10.1007/3-540-47961-9_25	navigation;formal methods;computer science;data mining;systems design	Mobile	-67.42116985605959	7.620546063856682	71081
0c89b8b0f621e5a00b53991e5370d135fc9a54d9	a tqm approach to the improvement of information quality	quality function deployment;total quality management;information quality	"""There is a consistent gap between users expectations regarding Information Quality (IQ) and the perceived quality of the information they are using. An explicit approach to IQ is required, meaning that all stakeholders should specify in detail the IQ requirements, design them into the information solutions and track their fulfillment. A Total Quality Management (TQM) based framework for the IQ improvement process is proposed. The framework employs six TQM concepts, namely Customer Focus, Leadership, Teamwork, Continuous Improvement, Measurement and Benchmarking. A case study about an initiative to improve information on Project Status is discussed. IQ dimensions are at the centre of this framework. They are organized in a three level hierarchy. User satisfaction is decomposed into """"Customer Needs"""" which are translated into """"IQ metrics"""" These dimensions are treated as objects. The paper lists the set of operations that should be performed on these objects including selection, scaling and prioritization. InfoQual, a methodology designed to facilitate the manipulation of IQ dimensions in the improvement process, is described. This methodology is based on the TQM framework and uses three specific tools: • QFD (Quality Function Deployment) to translate Customer Needs into metrics. • IQ dimensions and metrics database to preserve and reuse experience gained during the improvement process • IQ metrics graphical representation to communicate metrics information."""	business process;computer user satisfaction;dfa minimization;graphical user interface;image scaling;information quality;information system;quality function deployment;real life;requirement;side effect (computer science);software deployment;software metric	Ron Dvir;Stephen Evans	1996			information quality;total quality management;prioritization;reuse;hierarchy;benchmarking;teamwork;quality function deployment;systems engineering;computer science	DB	-72.25029301189745	9.56431032717807	71107
701bc901e36805bb0541bd4e55a7f7e226813f45	an introduction to ibm general business simulation environment	ibm china research lab;order handling process;desktop software tool;controller layer;decision making;supply chain management;multiple supply chain;inventory control process;transportation process;ibm general business simulation;procurement process;eclipse platform;ibm general business simulation environment;risk analysis;supply chain dynamic;supply chain what-if analysis;tactical level decision making;what-if analysis;business data processing;supply chain;supply chain simulation tool;dp industry;simulation;inventory control;mean squared error	IBM General Business Simulation Environment (GBSE) is a supply chain simulation tool developed by IBM China Research Lab. It can capture supply chain dynamics with finest level of granularity and provides great insights to a supply chain's real operations. GBSE is designed for tactical level decision making; it is proper for supply chain what-if analysis and risk analysis. GBSE implements multiple supply chain processes to considerable details, such as order handling process, inventory control process, manufacturing process, transportation process, procurement process, and planning. The environment is created as a desktop software tool based on Eclipse platform. The backbone framework consists of Presentation Layer, Controller Layer, Service Layer, and Data Layer.	desktop computer;eclipse;it risk management;internet backbone;inventory control;procurement;programming tool;service layer;simulation	Wei Wang;Jin Dong;Hongwei Ding;Changrui Ren;Minmin Qiu;Young M. Lee;Feng Cheng	2008	2008 Winter Simulation Conference		inventory control;supply chain management;simulation;risk analysis;systems engineering;engineering;mean squared error;supply chain;manufacturing engineering	Robotics	-63.51489268441779	5.561296390054499	71131
b47d977827e74928db6f2e29aca2c46f16100f51	cartographie de réseaux d'alliances et analyse stratégique	graph theory;teoria grafo;cartographie;theorie graphe;network analysis;firm cooperation;cartografia;innovation;cartography;cooperation entreprise;information system;innovacion;analyse circuit;systeme information;cooperacion empresa;analisis circuito;sistema informacion	Our aim is to explain the links between innovation and interfirm network structure and dynamics. We therefore need to understand first the mechanisms that lead to the formation and evolution of real networks. Network analysis was conducted using Tetralogie software for network mapping and graph-theoretic concepts such as density and centrality. Knowledge at all times of the macroscopic environment of a company should allow the company to position itself within this macro-network and to adapt dynamically its strategy so as to have an optimal position and therefore performance of innovation in the network. Mots-clés: Réseau, Innovation, alliances, centralité, cartographie, position, structure, stratégie	centrality;graph theory;network governance;network mapping	Brigitte Gay;Bernard Dousset	2006	Ingénierie des Systèmes d'Information	10.3166/isi.11.2.37-51	innovation;network analysis;computer science;artificial intelligence;graph theory;information system	AI	-89.39245031239577	8.39496233153795	71181
b5d0eba5d808f8880b2ccfa4b34670f6fbb98bf1	engineering of building automation systems — state-of-the-art, deficits, approaches	energy efficiency;software;frequency modulation;energy efficient;employee security;barium;structural engineering computing building management systems;buildings automation frequency modulation planning software knowledge based systems barium;structural engineering computing;building management systems;integrated building automation system;energy efficiency integrated building automation system employee productivity employee security;building automation systems;employee productivity;planning;knowledge based systems;buildings;automation	Growing flexibility along with conversion, growing employee productivity and improvements in security are increasingly become the focus of interest in state-of-the-art buildings. Energy efficiency is a further essential aspect, which is associated with factors such as lower utilization costs. A modern and integrated building automation system is necessary to fulfill these demands. The engineering process provides the basis for developing such systems. However, today's engineering process exhibits a number of shortcomings in the demands for implementing such systems. This paper focuses on the topic, describes today's engineering process of building automation systems and identifies its deficits. It also reports on two approaches designed to largely eliminate the deficits. These approaches have been proved and tested in a reference project, which is also outlined.	assembly language;deficit round robin;fm broadcasting;requirement;requirements engineering;solution architect;system requirements	Stefan Runde;Achim Heidemann;Alexander Fay;Peer Schmidt	2010	2010 IEEE 15th Conference on Emerging Technologies & Factory Automation (ETFA 2010)	10.1109/ETFA.2010.5641173	construction engineering;computer science;systems engineering;engineering;artificial intelligence;knowledge-based systems;efficient energy use	SE	-63.85040813831727	8.843290679055809	71367
0404179c2119a6f13fc5e87a424dfda38a0ae683	the use of a systems engineering process guide to accelerate improvement in systems engineering application and expertise	engineering and training;intranets;product life cycle management;systems engineering;case stud systems engineering training system dynamics modeling systems engineering case study systems engineering workforce improvement intervention;production engineering computing;on the job training;systems engineering and theory training monitoring companies context flowcharts;systems engineering competency;product design;company intranet systems engineering process systems engineering application systems engineering expertise bristol site rolls royce plc on the job training tool company advocated technique structured product characterisation optimisation process analysis process improvement local engineering process on site systems engineering mentor;systems engineering intranets on the job training product design product life cycle management production engineering computing	A case study is presented of the use of a simple and intuitive guide at the Bristol site of Rolls-Royce plc as an on the job training tool for those new to using particular company advocated techniques. The set of techniques range from those that help develop a systemic problem understanding through to structured product characterisation and optimisation, and also process analysis and improvement. Use of the guide is supported by local engineering processes, on site systems engineering mentors, and guidance on individual technique use on the company intranet. It is intended that the guide be used as a discussion aid to critically reflect on which techniques to use on a particular task and will: increase the rate of use of the techniques by engineers, increase the quality of technique use by engineers, and accelerate the increase in the number of experienced systems engineering practitioners. The continuous gathering and sharing of evidence of the need for and benefit gained from the intervention, combined with the participative nature in which the intervention has been developed, has resulted in support for guide from all levels of the company and has resulted in its use spreading across all company sites within months of release. A validation study is ongoing to ensure the guide is having the intended effect, and to find ways the intervention can be improved.	embedded system;intranet;iterative method;mathematical optimization;storyboard;systemic problem;systems engineering;winston w. royce	Andrew Parsley;Darren M. York;Charlotte N. Dunford;Mike Yearworth	2013	2013 IEEE International Systems Conference (SysCon)	10.1109/SysCon.2013.6549962	electrical engineering technology;health systems engineering;information engineering;system of systems;system of systems engineering;systems engineering;engineering;knowledge management;methods engineering;operations management;applied engineering;transport engineering;requirements engineering;production engineering;systems design	SE	-71.10953940166331	19.027698686584266	71496
c4d35ae11db500c0cc8eb2c094ccc8825d4c3c3a	towards a definition of large scale products		The range of structure sizes for industrial products produced today is increasingly expanding. This trend is evident both at the bottom end of the scale as well as the top end. Examples include the ever-smaller miniaturization of devices in the semiconductor industry and the increase in rotor diameter of wind turbines. While definitions already exist for smaller scale device structures e.g. nanotechnology, the conceptual distinction between conventional large products and large scale products is currently insufficient. In this study, we present a potential basis for the definition of large scale products. To achieve this, we first of all derive hypotheses and examine these in the context of an empirical study using the examples of threaded nuts, screw presses and passenger aircraft. The study shows that the transition from conventional products to large scale products is characterized by a disproportionate increase in product costs due to the augmentation of a characteristic product feature. Based on the findings described, we then derive a proposed definition which characterizes large scale products on the basis that man encounters his technical, organizational and economic limits with the methods and tools available at the time of observation, in the context of product creation.	data acquisition;high-level programming language;r.o.t.o.r.;semiconductor industry;threaded code;vergence	Bernd-Arno Behrens;Peter Nyhuis;Ludger Overmeyer;Aaron Bentlage;Tilmann Rüther;Georg Ullmann	2014	Production Engineering	10.1007/s11740-013-0503-1	engineering;operations management;nanotechnology;engineering drawing;mechanical engineering	HCI	-78.75357262622994	17.542027523673745	71568
1bd65a43a820ad20d2d01c21c5bf1054362c537e	deploying static application security testing on a large scale	secure development life cycle;sast;sdlc;static application security testing;static code analysis	Static Code Analysis (SCA), if used for finding vulnerabilities also called Static Application Security Testing (SAST), is an important technique for detecting software vulnerabilities already at an early stage in the software development lifecycle. As such, SCA is adopted by an increasing number of software vendors. The wide-spread introduction of SCA at a large software vendor, such as SAP, creates both technical as well as non-technical challenges. Technical challenges include high false positive and false negative rates. Examples of non-technical challenges are the insufficient security awareness among the developers and managers or the integration of SCA into a software development life-cycle that facilitates agile development. Moreover, software is not developed following a greenfield approach: SAP’s security standards need to be passed to suppliers and partners in the same manner as SAP’s customers begin to pass their security standards to SAP. In this paper, we briefly present how the SAP’s Central Code Analysis Team introduced SCA at SAP and discuss open problems in using SCA both inside SAP as well as across the complete software production line, i. e., including suppliers and partners.	agile software development;application security;security awareness;security testing;sensor;software development process;software product line;static program analysis;technical standard;vulnerability (computing)	Achim D. Brucker;Uwe Sodan	2014			software security assurance;development testing;computer science;database;world wide web;computer security	SE	-67.0066643855564	23.33775439371003	71603
a2abdf8745a9480dabae7e5838d8c445d5d1c17b	open source software for personal information managers and personal knowledge management	content management;human computer interaction;information management;knowledge management;personal information systems;public domain software;enterprise content management software;human-computer interaction;individual skills;knowledge delivery;knowledge distribution;open source software;personal information management;personal knowledge management;social personal management;users desktop	Technology is common in the domain of knowledge distribution, but it rarely enhances the process of knowledge use. Distribution delivers knowledge to the potential user's desktop but cannot dictate what he or she does with it thereafter. It would be interesting to envision technologies that help to manage personal knowledge as it applies to decisions and actions. The viewpoints about knowledge vary from individual, community, society, personnel development or national development. Personal Knowledge Management (PKM) integrates Personal Information Management (PIM), focused on individual skills, with Knowledge Management (KM). KM Software is a subset of Enterprise content management software and which contains a range of software that specialises in the way information is collected, stored and/or accessed. This article focuses on KM skills, PKM and PIM Open Sources Software, Social Personal Management and also highlights the Comparison of knowledge base management software and its use.	desktop computer;enterprise content management;knowledge base;open-source software;personal digital assistant;personal information management;personal knowledge management	Khaisar Muneebulla Khan;Umesha Naik	2009	2009 International Conference for Internet Technology and Secured Transactions, (ICITST)		knowledge base;information technology management;enterprise software;time management;content management;software project management;data management;computer science;knowledge management;digital firm;personal information management;software as a service;data mining;group information management;database;risk management information systems;structure of management information;information management;personal knowledge management;human resource management system;personal information manager;domain knowledge	DB	-69.20263360658844	10.64638215354365	71651
f08d6b49e5537c3be705fc4a467a2b4db253f3c3	methodology for the evaluation and selection of the suitability of highly iterative product development methods for individual segments of an overall development project	software;uncertainty;iterative methods companies product development manufacturing software uncertainty face;project management iterative methods manufacturing industries product quality;companies;marketable products highly iterative product development methods overall development project manufacturing companies heterogeneous customer requirements quality standards customer specific products;scrum flexibility highly iterative product development;iterative methods;manufacturing;face;product development	Today's challenge for manufacturing companies is the fulfillment of constantly growing heterogeneous customer requirements by simultaneously delivering highest quality standards within shortest time. One way to face this challenge is the highly iterative product development approach. Its objective is the distribution of the current development process in many short and iterative sub-processes. This procedure enables a regular involvement of customers in the development process and an early validation of the development status, which jointly results in customer-specific and marketable products. However, most companies do not know how this short-cycle concept can be used in development projects. This is due to the fact that most published papers just describe the highly iterative methods, however, without considering the suitability of these methods for individual development scopes. This paper aims at the presentation of a methodology for the evaluation and selection of highly iterative product development methods depending on the individual development scopes.	iterative method;new product development;requirement	G. Schuh;S. Rudolf;F. Diels	2015	2015 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)	10.1109/IEEM.2015.7385730	face;uncertainty;systems engineering;engineering;marketing;operations management;iterative and incremental development;scrum;mathematics;iterative method;manufacturing;product management;new product development;statistics	SE	-66.65060360728748	18.05091586311661	71910
da0b04bb37b333ef4394a344b67cad7cb41e656b	reorganization of the work system for successful information systems implementation: a case study	puesta en obra;organization management;concepcion sistema;implementation;direction organisation;direccion organizacion;system design;information system;gestion organizacion;mise en œuvre;top management;conception systeme;systeme information;gestion organisation;sistema informacion	This paper presents a case study of two attempts to implement a computer based materials management system in a large engineering company. It presents the details of the failure of the first attempt, and an analysis of the problems encountered in the first attempt. The details of the corrective action undertaken, and the successful second attempt are also presented. The analysis of the two implementation attempts suggests the important role of personnel with functional expertise and cross-functional business perspective on design teams. This should facilitate the integrated work-information system design stressed in this paper. The case study illustrates the nature of interrelationships between the work system and the information system and the dependence of the information system on the work system for successful operation. Thus, in some instances the need for reorganization of the work system for successful implementation is highlighted. The analysis also identifies the role of top management in creating the necessary organizational structures for involving users and assigning the responsibility for the success of the implementation to them. The paper also makes recommendations for the development of systems designers through appropriate recruitment, training, and job rotation programs. The paper concludes with a recommendation that systems designers should focus on the integrated work-information system design, rather than limiting the boundary of the systems design to the information processing system and information technology related tasks.	information system;work systems	Kailash Joshi	1990	Information & Management	10.1016/0378-7206(90)90036-H	computer science;engineering;operations management;management information systems;implementation;management;operations research;information system;systems design	HCI	-80.50928580227037	9.084843958931762	71988
f598501f92e151dee0c9e834c4b9c4893293d0f4	spinoffs and the mobility of u.s. merchant semiconductor inventors	clusters;grupo de excelencia;spinoffs;agglomeration economies;administracion de empresas;inventor mobility;economia y empresa;grupo a	Data on assignees of patenters are used to analyze the mobility of semiconductor inventors. A model of the entry and staffing of spinoffs of incumbent producers is developed to explain the higher mobility of semiconductor inventors in Silicon Valley. Exploiting data on the origins of semiconductor producers with larger sales, we argue that the greater mobility of semiconductor inventors in Silicon Valley was due primarily to spinoffs and not the clustering of the semiconductor industry there or California?s ban on the enforcement of employee non-compete covenants. To the extent that greater inventor mobility benefited Silicon Valley semiconductor producers, the benefits were mainly experienced by entrants and not incumbent producers. Jelcodes:R23,L63 1 Spinoffs and the Mobility of US Merchant Semiconductor Inventors	cluster analysis;semiconductor industry	Cristobal Cheyre;Steven Klepper;Francisco Veloso	2015	Management Science	10.1287/mnsc.2014.1956	economics;operations management;economies of agglomeration;economy;management	SE	-85.71984606849799	5.655952694041094	72106
b9110d91800ee5f952a1f4d17948251e4de3decb	beyond gss: fitting collaboration technology to a given work practice	collaboration support systems css;group support system;group support systems gss;collaborative engineering;component based groupware;limit set;requirement engineering;group work;collaboration engineering;critical success factor;free riding;work practice	Collaboration has become a critical success factor for many organizations. Collaboration is however not without challenges. Free riding, dominance, group think or hidden agendas are but a few phenomena in group work that make it a non straight effort. In addition, tools and technology that supports collaboration exists in a variety of shapes from complex group support systems (GSS) to simple boxes with cards and pencils. GSS often only offer a limited set of tools with a limited set of configurable features. Organizations, however, face an unlimited variety of problems with an unlimited variety of structures. In this article, we present a component-based groupware approach that goes beyond current GSS and allows collaboration engineers to fit the collaboration technology to a given work practice. We illustrate the feasibility of our approach by reporting on first experiences on supporting a requirements engineering work practice.	application programming interface;collaborative software;component-based software engineering;curve fitting;groupthink;requirements engineering;software engineer;usability	Tanja Buttler;Jordan Janeiro;Stephan Lukosch;Robert O. Briggs	2011		10.1007/978-3-642-23801-7_11	systems engineering;engineering;knowledge management;operations management	HCI	-68.46476675140181	17.50140939427194	72112
691b8c1e933b1d615bb3e438ec6acb0496de4960	internal resources matter more when there is intense competition: a resource-based assessment of e-business implementation	it capability;commerce electronique;rbv resource based view;gestion entreprise;theoretical model;information communication technology;comercio electronico;competition;competitividad;implementation;resource based view;avantage competitif;resource management;firm management;e business implementation;modele theorique;gestion recursos;taiwan;asie;competitiveness;gestion ressources;administracion empresa;adopcion;electronic business;implementacion;adoption;competitivite;electronic trade;nueva tecnologia informacion comunicacion;technologie information communication;rbv;modelo teorico;asia;competitive advantage;travel industry	Drawing upon the resource-based view of the firm (RBV), we argue that e-business implementation is path-dependent by nature. Successful implementation is a function of a firm's financial, technological, and human resources accumulated over time. Additionally, innovation-facilitating resources have a greater impact on e-business implementation when a company is faced with intense competition. Using a sample of travel agencies in Taiwan, we found a positive relation between a company's IT capability, its CEO's openness to new experience and e-business implementation. This was particularly true for the companies facing intense competition. The theoretical and managerial implications of these findings are discussed.	electronic business	Sophia Wang	2006	IJEB	10.1504/IJEB.2006.011698	information and communications technology;competition;economics;marketing;resource management;operations management;electronic business;implementation;management;tourism;competitive advantage	NLP	-82.32680032511793	8.326205230300431	72131
698964cfed59d0eafe3872a13bc74ae7aa60b27a	emergent consequences: unexpected behaviors in a simple model to support innovation adoption, planning, and evaluation		Many proven clinical interventions that have been tested in carefully controlled field settings have not been widely adopted. We study an agentbased model of innovation adoption. Traditional statistical models average out individual variation in a population. In contrast, agent-based models focus on individual behavior. Because of this difference in perspective, an agent based model can yield insight into emergent system behavior that would not otherwise be visible. We begin with a traditional logic of innovation, and cast it in an agent-based form. The model shows behavior that is relevant to successful implementation, but that is not predictable using the traditional perspective. In particular, users move continuously in a space defined by degree of adoption and confidence. High adopters bifurcate between high and low confidence in the innovation, and move between these groups over time without converging. Based on these observations, we suggest a research agenda to integrate this approach into traditional evaluation methods.	agent-based model;bifurcation theory;emergent;population;statistical model;usability	H. Van Dyke Parunak;Jonathan A. Morell	2014		10.1007/978-3-319-05579-4_22	simulation;knowledge management;artificial intelligence;management science	AI	-76.18492482047883	12.810681886209666	72164
7f052a33e181ced45609348ce1dfef929a1c743a	research on incentive mechanism of supply chain with knowledge association between suppliers	supply chain management groupware incentive schemes knowledge acquisition knowledge management production engineering computing;incentive mechanism;groupware;group knowledge sharing incentive mechanism supply chain supplier knowledge association principal agent model production enterprise;production enterprise;group incentive supply chain knowledge association individual incentive;principal agent model;knowledge management;group knowledge sharing;biological system modeling;supplier knowledge association;knowledge association;supply chains;ethics;production engineering computing;group incentive;individual incentive;knowledge acquisition;knowledge sharing;supply chains economics educational institutions supply chain management ethics biological system modeling;supply chain;incentive schemes;economics;supply chain management	Aimed at the knowledge association relation between suppliers in complex supply chain, the principal-agent models with individual incentive and group incentive under knowledge associations were established respectively. Additionally, the core enterprise’ optimal incentive strategies and knowledge sharing between suppliers were researched. The results show that combing individual incentive and group incentive can promote production effort and coordinate knowledge sharing behavior among suppliers effectively. Besides, learning capability of suppliers has a significant impact on group incentive and the knowledge-sharing effort.		Deng Liusheng;Zhang Xumei;Chen Wei;Shen Nali	2010	2010 International Conference on E-Business and E-Government	10.1109/ICEE.2010.1106	supply chain management;economics;knowledge management;environmental resource management;marketing;supply chain;commerce;incentive program	AI	-79.68820607016148	4.534975314580248	72182
0f5031c270c4c7c1fac3c3ed894d12fdbaf7a3eb	software architecture tool demonstrations	software architecture;tools;demo	In this paper, we describe the short summary of the tool demonstrations session at /ECSA 2015. The session aimed to attract both tools in practice and research tools. We describe the targeted topics for the tool support, and report on the program.	software architecture	Bedir Tekinerdogan	2012		10.1145/2797433.2797500	software architecture;simulation;human–computer interaction;computer science;engineering;software engineering	HCI	-64.51251926783861	23.97048885136698	72366
1ef9e8fc6df054cd439e865d8105dfb7337227d0	test optimization using combinatorial test design: real-world experience in deployment of combinatorial testing at scale	heart;testing;optimization;insurance;conferences;throughput	Clients today want more for less and the IBM test mantra of Test Less Test Right helps address this ask by placing Combinatorial Test Design (CTD) at the heart of the solution. This document presents two case studies of CTD implementation in client engagements and focuses on the approach, process and challenges addressed to scale up the implementation and make CTD a mainstream activity. The IBM Focus tool was used in both cases to implement Combinatorial Test Design for optimization of tests and for reducing test effort while increasing test coverage.	comparative toxicogenomics database (ctd);fault coverage;mathematical optimization;socialization;software deployment;test design;test effort;turing test	Saritha Route	2017	2017 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)	10.1109/ICSTW.2017.50	throughput;simulation;insurance;computer science;systems engineering;engineering;operating system;software engineering;test management approach;heart	SE	-64.88292671698927	26.019834746225644	72369
ead93ec0790cb3598f62f2f6e9cd36bfa2f70df8	monitoring organizational transactions in enterprise information systems with continuous assurance requirements	enterprise information systems;continuous assurance;risk profiles;management information system;organizational transactions	This work focuses on issues typically encountered in organizations whose core business largely depends on ICT: continuous monitoring, continuous auditing, controlling and assessment of transactions risk. Organizations have been making efforts to implement methods and systems which enable them to increase reliability of their business and, simultaneously, to be in accordance with their organizational objectives and compliant with external regulations. Thus, this work presents and validates an innovative solution to implement Continuous Assurance services in information systems applicable to any organizational transaction, regardless of its type, dimension, business area or even its information system support technology. This last objective is pursued having as support an ontological model at an abstraction level that guarantees that independence. This research culminated with the development of a prototype and consequent results analysis, using data collected from the near-real implementation, allowing us to ensure the feasibility and the effective use of the proposal. Monitoring Organizational Transactions in Enterprise Information Systems with Continuous Assurance Requirements	abstraction layer;enterprise information system;ontology (information science);prototype;requirement	Rui Pedro Marques;Henrique Santos;Carlos Santos	2015	IJEIS	10.4018/ijeis.2015010102	computer science;systems engineering;knowledge management;marketing;operations management;management information systems;database;management;enterprise information system	DB	-69.9676807253637	12.308706957797208	72445
55d9c4fa852bd2f38a9f1af7a4d3e5cd00135449	deriving enterprise engineering and integration frameworks from supply chain management practices		Enterprise Engineering and Enterprise Integration have been leveraged as key topics in Enterprise Management. Since the 80s  multiple approaches, methodologies, languages and, frameworks have been proposed. Despite the numerous results currently existing,  new trends and solutions are continuously emerging. This paper provides a landscape of the current problems on Enterprise  Engineering and Integration, the strategies, solutions and our vision about future trends.  	enterprise engineering	Ángel Ortiz Bas;Víctor Anaya;Rubén Darío Franco	2004		10.1007/0-387-29766-9_23	supply chain management;enterprise systems engineering;enterprise software;business administration;digital firm;process management;enterprise integration;enterprise information system;enterprise life cycle	DB	-73.6199904323709	7.6460137295920045	72490
ad1d0cda6548f00b7d8342a6c7241867d722d9a1	the development of a knowledge based front end for a computational fluid dynamics package			computation;computational fluid dynamics	Stuart Lee Hartle	1993				HPC	-91.41204120506765	21.838340423312793	72500
7fab9e15794a7682b0ab9a09a3c907ed99ef8a84	measurement and benchmarking foundations: providing support to organizations in their development and growth using dashboards	benchmarking;stage models;improvement;measuring;stages of growth;maturity;dashboard	article i nfo Available online 6 December 2012 Growth and stage models often lack a sound empirical and theoretical base and do not provide any help for organizations to improve. Measuring and benchmarking (Mu0026B) is necessary for understanding an organizationu0027s position and identifying growth opportunities. Yet Mu0026B methods are often not based on gen- eralizations of practice and measure only what is directly visible. They are missing relevant elements that can help further development. In this paper, we propose a multi-level measurement framework utilizing a mix of measurement methods to look deep inside organizations. Whereas benchmarking is often based on a single number, deep insight is given by showing the performance in a broad range of areas and views using a dash- board. Guidance for improvement is created by identifying those elements that need improvements. The il- lustration of the framework in a case study shows that the process of measuring deep inside organizations might be more important than the actual outcomes and that per area different maturity levels might be pos- sible. We provide seven principles that can serve as a foundation for developing Mu0026B and stage models.		Devender Maheshwari;Marijn Janssen	2013	Government Information Quarterly	10.1016/j.giq.2012.11.002	simulation;computer science;operations management;management;maturity;measurement;dashboard;benchmarking	Theory	-78.25289246804213	10.216124363088053	72576
d9e96a48af0c1b60edfd6f08427ec27f9cec3032	an empirical investigation of the contribution of is to manufacturing productivity	selected works;small manufacturing firms;manufacturing;bepress;productivity	Next to the financial sector, the manufacturing industry is the second largest investor in information systems (IS) and technologies (IT). However, in recent years, many CEOs have expressed skepticism and disappointment on the returns from investments in IS. There is reluctance on the part of these CEOs to continue to invest in IS because of a nagging feeling that existing systems are often under-utilized. The dissatisfaction of CEOs with returns on them combined with the suspicion that existing IS are under-utilized clearly has serious implications for the IS community. Using empirical survey data of small and large manufacturing firms in the electronic industry, this paper investigates whether, in fact, IS are under-utilized and, if so, explores the underlying reasons for this situation. In particular, we explore the differences, if any, between small and large manufacturing firms in their ability to deploy and utilize IS to achieve organizational excellence.	information system	Uma G. Gupta;Margaret Capen	1996	Information & Management	10.1016/S0378-7206(96)01081-6	productivity;economics;engineering;marketing;operations management;manufacturing;management;commerce	HCI	-82.11873572578665	5.818293176425211	72600
08dc0e0893716c813e4aeeffc2c2862fc5201b2c	a method to identify and correct problematic software activity data: exploiting capacity constraints and data redundancies	mining software repositories;data quality;capacity constraint;data redundancy	Mining software repositories to understand and improve software development is a common approach in research and practice. The operational data obtained from these repositories often do not faithfully represent the intended aspects of software development and, therefore, may jeopardize the conclusions derived from it. We propose an approach to identify problematic values based on the constraints of software development and to correct such values using data redundancies. We investigate the approach using issue and commit data of Mozilla project. In particular, we identified problematic data in four types of events and found the fraction of problematic values to exceed 10% and rapidly rising. We found the corrected values to be 50% closer to the most accurate estimate of task completion time. Finally, we found that the models of time until fix changed substantially when data were corrected, with the corrected data providing a 20% better fit. We discuss how the approach may be generalized to other types of operational data to increase fidelity of software measurement in practice and in research.	commit (data management);software development;software measurement;software repository	Qimu Zheng;Audris Mockus;Minghui Zhou	2015		10.1145/2786805.2786866	data quality;computer science;data science;data mining;database;data redundancy	SE	-66.16429863307151	32.26391909740214	72606
8e87daefbbd0f22efaacf7eb2ba73f98189a58fc	descriptive vs prescriptive models in industry	network simulation;co simulation;modeling in industry;protocol verification;models of platforms;descriptive models;prescriptive models;wireless sensor network modeling;embedded software	To understand the importance, characteristics, and limitations of modeling we need to consider the context where models are used. Different organizations within the same company can use models for different purposes and modelling can involve different stakeholders and tools. Recently, several papers discussing how industries use MDE have been published and they have contradictory findings.  In this paper we report lessons learned from our collaborations with three large companies. We found that it is important to distinguish between descriptive models (used for documentation) and prescriptive models (used for development) to better understand the adoption of modelling in industry. Our findings are valuable for both academia and industry. A better understanding of modeling in large companies can help academia conceiving innovative MDE solutions that can have a real impact in industry. On the other hand, industry can better understand how to properly exploit MDE and what to expect from it.	documentation;model-driven engineering	Rogardt Heldal;Patrizio Pelliccione;Ulf Eliasson;Jonn Lantz;Jesper Derehag;Jon Whittle	2016		10.1145/2976767.2976808	simulation;embedded software;computer science;systems engineering;engineering;network simulation;management science	SE	-70.80073250149505	21.050235748872065	72630
21c0ec8dfcfff6df192eac69648e99dacf913a42	robust solutions for the software project scheduling problem: a preliminary analysis	sbse;search based software engineering;robustness;software project scheduling;sps	The software project scheduling problem relates to the decision of who does what during a software project lifetime. This problem has a capital importance for software companies. In the software project scheduling problem, the total budget and human resources involved in software development must be optimally managed in order to end up with a successful project. Two are the main objectives identified in this problem: minimising the project cost and minimising its makespan. However, some of the parameters of the problem are subject to unforeseen changes. In particular, the cost of the tasks of a software project is one of the most varying parameters, since it is related to estimations of the productivity of employees. In this paper, we modify the formulation of the original bi-objective problem to add two new objectives that account for the robustness of the solutions to changes in the problem parameters. We address 36 instances of this optimisation problem using four state-of-the-art metaheuristic algorithms and compare the solutions with those of the original non-robust bi-objective problem.	algorithm;makespan;mathematical optimization;metaheuristic;multi-objective optimization;objective-c;pareto efficiency;relevance;scheduling (computing);search-based software engineering;software development;software project management	Francisco Murilo Tavares Luna;Francisco Chicano;Enrique Alba	2012	IJMHeur	10.1504/IJMHEUR.2012.048218	nurse scheduling problem;mathematical optimization;team software process;test data generation;software sizing;search-based software engineering;computer science;software development;software design description;software construction;management science;project management triangle;software development process;software metric;robustness	SE	-66.86019342553271	28.90123144803633	72636
e1278891f16e8788d5f2affa4b0629ae2a4219e1	using a process assessment model to prepare for an iso/iec 20000-1 certification: iso/iec 15504-8 or tipa for itil?		Nowadays, whatever their domain of activity, the interest of the organizations for IT service management is growing. In that context, a certification of their service management system is seen as a good means to demonstrate their excellence in this discipline. Amongst all the existing process (assessment or maturity) models that could be used to support such a certification, two of them have been studied. This paper presents and compares the ISO/IEC 15504-8 and the TIPA for ITIL process assessment models, in order to determine their respective strengths and weaknesses when used for preparing an ISO/IEC 20000-1 certification.	iso/iec 15504;itil;tudor it process assessment	Stéphane Cortina;Béatrix Barafort;Michel Picard;Alain Renault	2016		10.1007/978-3-319-44817-6_7	iso/iec 9126;reliability engineering;iso 13567;iso 29110;iso/iec 15288;iso/iec 15504	Vision	-70.2634540104534	14.53657941225328	72671
f183653ea27a20875e9a98dd63633c45a1006c2b	introduction to software engineering with computational intelligence	computacion informatica;computational intelligence;grupo de excelencia;software engineering;ciencias basicas y experimentales	There are two distinct features that characterize software engineering: (1) it is an application of engineering to software, that is, the application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software; and (2) it is a transformation process, that is, the transformation that models the real world into its corresponding software world.	computational intelligence;software engineering	Jonathan Lee	2003	Information & Software Technology	10.1016/S0950-5849(03)00009-0	computational science;computer science;artificial intelligence;software engineering;computational intelligence	SE	-64.48600556649042	25.04527744656983	72725
c7249b1b65ae11c95ef90e755eba95c222510f5b	the impact of lack in domain or technology experience on the accuracy of expert effort estimates in software projects	project risk;effort estimation;software development;software projects;expert estimation	The study examines the impact of lack of experience in the domain problem or lack of experience with the technologies used in a software development project on the accuracy of single expert estimation of task effort, as measured by estimated versus actual effort. Expert judgment in the estimation of task effort is the most frequently used estimation technique for software projects. Estimators rely on their experience, business domain knowledge, and technical expertise. Occasionally, organizations lack experts on staff that have relevant prior experience on some business or technology related aspects of the project. This research investigates the impact of such incomplete expertise on the reliability of estimates.		Susanne Halstead;Rosario Ortiz;Mario Córdova;Miguel Seguí	2012		10.1007/978-3-642-31063-8_19	software project management;systems engineering;engineering;knowledge management;software development;software engineering;analysis effort method;project risk management;management science;subject-matter expert;management	SE	-70.47366542684476	23.319228911776484	72839
5c85256f14dda6019abca65d082169fe485485c1	mean and bold? on separating merger economies from structural efficiency gains in the drinking water sector	benchmarking;metodo cuadrado menor;modelizacion;developpement entreprise;forecasting;evaluation performance;gestion entreprise;methode moindre carre;reliability;project management;firm development;information systems;performance evaluation;least squares method;maintenance;robustesse test;efficiency;evaluacion prestacion;soft or;information technology;agua potable;specification;estimation non parametrique;non parametric and parametric estimation;packing;evaluacion comparativa;nonparametric and parametric estimation;fusiones y adquisiciones;firm management;scale economies;operations research;location;universiteitsbibliotheek;investment;journal;intelligence economique;journal of the operational research society;inventory;purchasing;modelisation;non parametric estimation;estrategia empresa;history of or;col;desarrollo empresa;parametric estimation;logistics;eau potable;especificacion;marketing;merger and acquisition;scheduling;test robustness;drinking water;least square;water sector;production;competitive intelligence;fusion acquisition;robustez prueba;communications technology;administracion empresa;computer science;estimacion no parametrica;operational research;efficient estimation;economy of scale;inteligencia economica;firm strategy;modeling;economie d echelle;strategie entreprise;mergers and acquisitions;free disposal hull;applications of operational research;or society;jors;management science;infrastructure	The Dutch drinking water sector experienced two drastic changes over the last 10 years. Firstly, the sector association started a voluntary benchmarking aimed to increase the efficiency and effectiveness of the sector. Secondly, merger activity arose. This paper develops a nonparametric model to dissect and distinguish the effects on efficiency of these evolutions. Parametric corrected ordinarily least squares (Fourier) tests show the robustness of the model with respect to the specification and its variables. Although detecting an efficiency enhancing effect of benchmarking, we find insignificant merger economies due to the absence of scale economies and the absence of increased incentives to fight inefficiencies.		Kristof De Witte;E. Dijkgraaf	2010	JORS	10.1057/jors.2008.129	project management;competitive intelligence;economics;economies of scale;marketing;operations management;management;operations research;information technology;least squares;statistics	Robotics	-83.5024959602287	9.125210590450848	72881
7eeda8b04aad186df8557a6d4bbc6dac6439280e	information security incident management: planning for failure	information security incident management;control systems;documented plans;security of data control engineering computing electricity supply industry failure analysis industrial control power distribution power engineering computing;information security interviews training control systems organizations iso standards;smart grids information security incident management failure planning organization operating industrial control systems distribution service operators power industry incident management preparation activities documented plans;information security;iso standards;distribution service operators;incident management;information technology;training;power distribution;incident management preparation activities;failure analysis;smart grids;power engineering computing;smart grids industrial control systems information security information technology incident management power industry;industrial control;power industry;interviews;industrial control systems;control engineering computing;failure planning;organizations;electricity supply industry;organization operating industrial control systems;security of data	This paper reports on an interview study on information security incident management that has been conducted in organizations operating industrial control systems that are highly dependent on conventional IT systems. Six distribution service operators from the power industry have participated in the study. We have investigated current practice regarding planning and preparation activities for incident management, and identified similarities and differences between the two traditions of conventional IT systems and industrial control systems. The findings show that there are differences between the IT and ICS disciplines in how they perceive an information security incident and how they plan and prepare for responding to such. The completeness of documented plans and procedures for incident management varies. Where documentation exists, this is in general not well-established throughout the organization. Training exercises with specific focus on information security are rarely performed. There is a need to create amore unified approach to information security incident management in order for the power industry to be sufficiently prepared to meet the challenges posed by Smart Grids in the near future.	computer security incident management;control system;documentation;information security	Maria B. Line;Inger Anne Tøndel;Martin Gilje Jaatun	2014	2014 Eighth International Conference on IT Security Incident Management & IT Forensics	10.1109/IMF.2014.10	itil security management;knowledge management;operations management;business;computer security;incident management	DB	-72.83110009804442	16.099010770321886	72892
d9cbf915a8f6cbe46ce22063ef160b18beadcbde	enterprise systems implementation failure: the role of organizational defensive routines	enterprise system	This paper discusses some of the challenges that organizations face when implementing enterprise systems. A small-medium sized enterprise within a large conglomerate within the Asia-Pacific region was studied using critical ethnography. In this study it was found that organizational learning around strategically important issues failed to occur. This paper suggests the theory of organizational defensive routines for understanding how organizational learning may be hindered during the implementation of enterprise systems.	erp;enterprise integration;enterprise resource planning;enterprise system;timer	Jiunn-Chieh Lee;Michael D. Myers	2004			enterprise system;enterprise systems engineering;enterprise software;computer science;knowledge management;business administration;integrated enterprise modeling;enterprise architecture management;process management;enterprise architecture;enterprise planning system;enterprise information security architecture;enterprise information system;enterprise life cycle	AI	-74.11009640450953	8.507811792408061	73021
26cf2f319e2170ed3c1b120d96339d7b0b8544e1	productivity measurement and improvements: a theoretical model and applications from the manufacturing industry	kpi;productivity;performance measurement;oee	"""At many companies, workers associate productivity or efficiency in- crease with something negative, it is interpreted as an increase in speed and the """"sweat factor"""". Productivity is not only made up of the speed factor, but these misconceptions and lack of knowledge tend to put """"a wet blanket"""" on all at- tempts to increase productivity. It is therefore important to clarify what produc- tivity is and especially how it can be improved. In general, the productivity at shop-floor level can be improved through im- proving the method, increasing the performance, and increasing the utilization. The design of the products and the amount of scraped products also affects the productivity in both manual tasks as well as work performed by machines. These aspects of productivity will be elaborated in the theoretical model and the industrial applications presented in this article."""	theory	Peter Almström	2012		10.1007/978-3-642-40361-3_38	productivity;engineering;operations management;industrial engineering;productivity model;partial productivity	Vision	-77.91817192139183	12.043729058153602	73122
6faa97e2eeed4579e6ea0246ba9de086e97946c5	strategic alignment maturity model (samm) in a cascading balanced scorecard (bsc) environment: utilization and challenges		SAMM is a useful tool for measuring the maturity of business/IT alignment in an organization at the macro level. However, at the micro level, organizations use several frameworks including cascading BSC, ITIL, COBIT, etc. to align business and IT processes. The complexity of alignment increases with the existence of more than one tier of cascading and usage of different tools or frameworks. Studies have shown that measuring business/IT alignment at the micro level is difficult. Therefore, in order to accurately measure outcomes, mapping between metrics at all levels is required. It is also important to establish metrics that are aligned with those prescribed by SAMM. Using a multi-level cascading BSC that was previously published in BUSITAL by this author, this study attempts to apply the underlying components of SAMM and to establish relevant alignment metrics. It also highlights some applicability problems and suggests appropriate solutions for future implementations.	binary symmetric channel;capability maturity model	Suchit Ahuja	2012		10.1007/978-3-642-31069-0_47	systems engineering;engineering;knowledge management;management	HPC	-77.96824510835052	7.800358608442745	73255
611bdc652c87b571f48529f1823a7d7e75c31d9a	a case study in software reuse	domain engineering;reuse metrics and waiting queues simulations;cocomo;domain architecture;software reuse	We report on a term project to conduct domain engineering followed by application engineering for a specific set of applications. We discuss some of the observations we have made on this project, as well some of the lessons we have learned.	code reuse;domain engineering	Edward A. Addy;Ali Mili;Sherif M. Yacoub	1999	Software Quality Journal	10.1023/A:1008963424886	domain analysis;real-time computing;domain;computer science;systems engineering;architecture domain;feature-oriented domain analysis;software engineering;domain engineering;cocomo;management	SE	-64.83395103012124	23.1675053685895	73412
7d66b5259f2e37f623f1e6c7610c1482358993c6	an ecological perspective towards the evolution of quantitative studies in software engineering	food chains;meta analysis;ecology;experimental chains;quasi experiments;evidence based software engineering;population sampling;quantitative survey;threats to validity	Context: Two of the most common external threats to validity in quantitative studies in software engineering (SE) are concerned with defining the population by convenience and nonrandom sampling assignment. Although these limitations can be reduced by increasing the number of replications and aggregating their results, the acquired evidence rarely can be generalized to the field.  Objective: To investigate the state of practice of meta-analysis in SE and its limitations, intending to propose an alternative perspective to understand the relationships among experimentation, production, threats to validity and evidence. To propose and evaluate means to strengthen quantitative studies in software engineering and making them less risky due to population and sampling issues.  Method: To use the underlying idea from the Theory of Food Chains to alternatively understand the impact of external threats to validity in the SE experimental cycle (experimental chains). Next, to accomplish an initial technical literature survey to observe basic features of secondary studies aggregating primary studies results. Third, to organize a set of experimental chain's concepts and make initial discussions regarding the observed secondary studies concerned with this metaphor.  Results: By applying the experimental chains concepts it was initially observed that, although important and necessary, most of the current effort in the conduction of quantitative studies in SE does not produce (mainly due to population/sampling constraints) results strong enough to positively impact the engineering of software. It promotes an imbalance between research and practice. However, more investigation is necessary to support this claim.  Conclusion: We argue that research energy has been lost in SE studies due to population/sampling constraints. Therefore, we believe more investigation must be undertaken to understand how better organizing, enlarging, setting up and sampling SE quantitative studies' population by using, for instance, alternative technologies such as social networks or other crowdsourcing technologies.	crowdsourcing;organizing (structure);sampling (signal processing);social network;software engineering	Rafael Maiani de Mello;Guilherme Horta Travassos	2013		10.1145/2460999.2461031	simulation;engineering;operations management;management science	SE	-73.58765546220901	21.28564451064556	73454
90db109e81b13709886c35f198aedd4636096745	requirements composition table explained	banking;project management;formal specification;aspect oriented requirements engineering;software testing context floors investments conferences;investment;software development management banking formal specification investment project management;requirements composition table;crosscutting concerns;change impact analysis aspect oriented requirements engineering requirements composition table rct crosscutting concerns core features project tasks aore software development wall street applications investment banks software project;change impact analysis;change impact analysis aspect oriented requirements engineering crosscutting concerns requirements composition table;software development management	Aspect-oriented requirements engineering (AORE) introduced an artifact called Requirements Composition Table (RCT). RCT presents a holistic view of an application's functionality structured by core features and crosscutting concerns. This artifact can effectively support various project tasks and serve as a common frame of reference for all parties on a project team. As AORE remains little-known to most practitioners in the software development field, the purpose of this paper is to explain the RCT concept to practitioners and discuss its benefits. The RCT technique has been implemented for a number of Wall Street applications at various investment banks. RCT can help us perform important project tasks and has proven to be one of the most valuable artifacts of a software project. This paper discusses the steps to develop an RCT, provides an example of how to use it to perform change impact analysis for releases, describes experiences using RCTs in practice, and discusses lessons learned on projects implementing the RCT technique.	aspect-oriented software development;cross-cutting concern;experience;holism;requirement;requirements engineering;software project management	Yuri Chernak	2012	2012 20th IEEE International Requirements Engineering Conference (RE)	10.1109/RE.2012.6345814	project management;reliability engineering;requirements analysis;software requirements specification;economics;software project management;investment;systems engineering;engineering;software engineering;formal specification;management;change impact analysis	SE	-66.4933629235742	22.83966062755568	73601
47af9b5c48eb800a8ca010cf93bf34a648842ad9	design of vip customer database and implementation of data management platform		Accurate and personalized analysis management for VIP Customer data are an important means for enterprises to make business decisions and win benefits. In this paper, take telecom industry as an example, SQL Server 2008 is applied to design the database according to the problem of traditional data anal- ysis and management methods of VIP customers. Combined with Brows- er/Server structure, an intelligent VIP customer data management platform is designed and implemented, which achieves rapid extraction, effective analysis, early warning and graphic display for the data. In addition, it effectively guides customer managers to make business decisions, and provides managers with first-hand data on the customer managers' performance evaluation. The actual operation in a large domestic telecom company proves that the platform is sta- ble, efficient and safe.		Rui Li;Miao-Jie Sang;Kebin Jia	2014		10.1007/978-3-319-07773-4_34	customer to customer;marketing;operations management;database;customer intelligence;business;customer advocacy	DB	-80.0509528746784	12.943209200651655	73690
2254b08c3d29ed7a215bf01c57fffeac998517a5	daily questionnaire to assess self-reported well-being during a software development project		According to authors best knowledge, this workshop paper makes two novel extensions to software engineering research. First, we create and execute a daily questionnaire monitoring the work well-being of software developers through a period of eight months. Second, we utilize statistical methods developed for discovering psychological dynamics to analyze this data. Our questionnaire includes elements from job satisfaction surveys and one software development specific element. The data were collected every day for a period of 8 months in a single software development project producing 526 answers from eight developers. The preliminary analysis shows the strongest correlations between hurry and interruptions. Additionally, we constructed temporal and contemporaneous network models used for discovering psychological dynamics from the questionnaire responses. In the future, we will try to establish links between the survey responses and the measures collected by conducting software repository mining and sentiment analysis.		Miikka Kuutila;Mika Viking Mäntylä;Maëlick Claes;Marko Elovainio	2018	2018 IEEE/ACM 3rd International Workshop on Emotion Awareness in Software Engineering (SEmotion)	10.1145/3194932.3194942	empirical research;time series;data mining;network model;software repository;software development;sentiment analysis;software;job satisfaction	SE	-72.92761604913758	26.286250094152223	73731
aa89e1b0f0b2ef3eb39b5ca3a246f30571e2664b	boundary spanning by design: toward aligning boundary-spanning capacity and strategy in it outsourcing	outsourcing;dp industry;indium tin oxide organizations interviews data models context lenses bridges;project management boundary spanning case study it outsourcing ito;data analysis;rational planning boundary spanning capacity alignment boundary spanning strategy it outsourcing ito engineering management client vendor boundaries secure relationship offshore sourcing national boundaries client perspective systematic analyses vendor perspective ito vendors data collection data analysis alignment form model alignment path model vendor boundary spanning rational deliberation;outsourcing data analysis dp industry	IT outsourcing (ITO) has created new issues for engineering management. Of these, a persistent problem concerns the boundaries between clients and vendors, which have the potential to damage even the most trusting and secure relationship. The rising trend of offshore sourcing has further exacerbated the issue, because of the national boundaries that ITO traverses in the offshore context. However, research on this issue has been limited; existing studies have mainly focused on the client perspective, whereas systematic analyses from the vendor perspective have been limited. In this study, we bridge this gap by studying boundary spanning of two ITO vendors. The data collection and analysis are guided by a novel and sound theoretical lens-alignment between boundary-spanning capacity and strategy. Two alignment models are derived. The alignment-form model depicts the outlook of the alignment and the alignment-path model depicts the process of achieving it. Based on these two models, we further conceptualize that vendor boundary spanning happens by design, with rational deliberation and planning. The study complements the ITO literature by providing a more complete picture of boundary spanning and the boundary-spanning literature by integrating the two diverse yet relevant research streams of boundary spanners and boundary-spanning strategies.	citizen sourcing;client (computing);file spanning;indium tin oxide;microsoft outlook for mac;os-tan;outsourcing;trust (emotion)	Wenyu Du;Shan Ling Pan	2013	IEEE Transactions on Engineering Management	10.1109/TEM.2012.2206114	economics;knowledge management;marketing;operations management;data analysis;management;outsourcing	SE	-80.96479444201066	4.688154302044983	73848
9737c63d6846a44a6a9def4a5e0d0b62d02da8fc	proposal of a framework of lean governance and management of enterprise it	enterprise it governance;lean thinking;enterprise it management;processes management;work system;lean it;projects management	Technology and Information are vital to the success of companies. To leverage the successes in IT projects, companies have at their disposal, references globally accepted as good practices (COBIT, ITIL, PMBOK, ISO, TOGAF, etc.). In spite of this, it is still great the magnitude of spending on IT projects poorly designed or improperly implemented. This paper presents a brief description of standards and good practices related to governance and management of enterprise IT, defines the Lean Thinking, Lean IT, the Processes Management, the Portfolio, Program and Project Management, and the Work System Theory, and highlights the purpose of them, showing their characteristics and suggests a Framework of Lean Governance and Management of Enterprise IT, by demonstrating how the standards and good practices presented can work together, because it advocates that the Lean Thinking, the Process, Portfolio, Program, and Project Management, and the Work System Theory complement the standards and good practices of Governance and Management of Enterprise IT with an approach not referenced in these standards and good practices.	cobit;itil;the open group architecture framework;work systems	Mauro Gonçalves Pinheiro;Mehran Misaghi	2014		10.1145/2684200.2684367	lean project management;human performance technology;enterprise software;knowledge management;operations management;enterprise architecture management;enterprise data management;business;management;it portfolio management;enterprise planning system;lean it;enterprise information system;enterprise life cycle	OS	-75.006240233785	10.06917784140075	73906
d4c15c486310db19e1030c045a9277a02c418e17	statistical analysis of metrics for software quality improvement		Software product quality can be defined as the features and characteristics of the product that meet the user needs. The quality of any software can be achieved by following a well defined software process. These software process results into various metrics like Project metrics, Product metrics and Process metrics. Software quality depends on the process which is carried out to design and develop software. Even though the process can be carried out with utmost care, still it can introduce some error and defects. Process metrics are very useful from management point of view. Process metrics can be used for improving the software development and maintenance process for defect removal and also for reducing the response time. This paper describes the importance of capturing the Process metrics during the quality audit process and also attempts to categorize them based on the nature of error captured. To reduce such errors and defects found, steps for corrective actions are recommended.	categorization;point of view (computer hardware company);response time (technology);software bug;software development process;software quality;software technical review;stepwise regression	Karuna Prasad;M. G. Divya;N. Mangala	2018	CoRR		software quality;reliability engineering;engineering;systems engineering;software development process;quality audit;categorization;software development;response time;software	SE	-62.89411465116088	28.495088468924244	74111
75f30ceef5b289015a3668f3f867afec760bf977	predicting software revision outcomes on github using structural holes theory	structural holes theory;predictive modeling;software development;survival analysis;social network analysis	Many software repositories are hosted publicly online via social platforms. Online users contribute to the software projects not only by providing feedback and suggestions, but also by submitting revisions to improve the software quality. This study takes a close look at revisions and examines the impact of social media networks on the revision outcome. A novel approach with a mix of different research methods (e.g., ego-centric social network analysis, structural holes theory and survival analysis) is used to build a comprehensible model to predict the revision outcome. The predictive performance is validated using real life datasets obtained from GitHub, the social coding website, which contains 32,962 pull requests to submit revisions, 20,399 distinctive software project repositories, and a social network of 234,322 users. Good predictive performance has been achieved with an average AUC of 0.84. The results suggest that a repository host's position in the ego network plays an important role in determining the duration before a revision is accepted. Specifically, hosts that are positioned in between densely connected social groups are likely to respond more quickly to accept the revisions. The study demonstrates that online social networks are vital to software development and advances the understanding of collaboration in software development research. The proposed method can be applied to support decision making in software development to forecast revision duration. The result also has several implications for managing project collaboration using social media.	structural holes	Libo Li;Frank Goethals;Bart Baesens;Monique Snoeck	2017	Computer Networks	10.1016/j.comnet.2016.08.024	predictive analytics;social network analysis;computer science;data science;software development;operating system;data mining;survival analysis;world wide web;computer security;software peer review	SE	-73.30129707009986	26.82438119113998	74113
0126ca3667847936dfbf5b3c4195124222af6ec0	capitalizing on multiple market opportunities	innovative software developers;investments;project management;dp industry;resource management;market opportunities project management recruitment law costs marketing and sales investments commercialization humans resource management;law;funding multiple market opportunities innovative software developers commercial technology application resource demands projects profits;marketing;funding;market opportunities;software development;project management software development management marketing dp industry economics;profitability;humans;economics;projects;commercialization;resource demands;profits;multiple market opportunities;software development management;recruitment;commercial technology application;marketing and sales	Innovative software developers are often unable to take advantage of the multiple opportunities presented by the commercial application of their technology. Because each market opportunity demands its own resources, most developers are fortunate if they can find a single project. Other markets have to wait while the developer hopes the first venture yields enough profit to fund more. >		Karl Dakin	1995	IEEE Software	10.1109/52.406763	project management;profit;project;marketing;resource management;software development;management;commerce;profitability index	Visualization	-74.31161572222162	17.577339881273222	74223
0d1fa0bdc2ffc05fb93d0926ba3389269828a697	the wizard of oz in crmland: crm's need for business process management	consumidor;gestion entreprise;entreprise;north america;america del norte;amerique du nord;amerique;relation client fournisseur;consommateur;implementation;empresa;firm management;etats unis;estados unidos;ejecucion;wizard of oz;processus entreprise;voice recognition;web portal;consumer;firm;business process management;relacion cliente proveedor;administracion empresa;profitability;information system;america;systeme information;business process;sistema informacion;supplier customer relationship	Abstract For several years now the focus of CRM has been to make it easier for companies to “reach out and touch” their customers, and vice versa. We have seen an explosion of technologies (wireless, chat, Web portals, voice recognition, analytics, and so on) that have made it convenient for customers to contact vendors, for employees to obtain information, and for companies to hone in on their most valuable assets – profitable customers. Over the last year, CRM company revenues have soared as the big get huge, and the small pick up sizable crumbs left over from the CRM table. Sure, the market is consolidating; but the CRM money machine rolls on.		Randy Davis	2002	IS Management	10.1201/1078/43202.19.4.20020901/38833.5	customer relationship management;consumer;computer science;business process management;database;business process;implementation;management;information system;profitability index	DB	-69.40262947856124	6.328785646098959	74278
6df7bbf79d51f617d40e3daed658e6c681177f2d	designing project management for global software development - informality through formality	trust;project management;distributed teams;formal processes;informal processes;control;global distributed software development;global software development;communication;coordination	Software development in distributed teams remains challenging despite rapid technical improvement in tools for communication and collaboration across distance. The challenges stem from geographical, temporal and sociocultural distance and manifest themselves in a variety of difficulties in the development projects. This study has identified a range of difficulties described in the literature of global software development, lacking sufficient solutions. In particular, advice for project managers is lacking. Design science research has been applied to design a model to guide project managers of distributed software teams, based on a practice study and informed by well-known theories. Our work pinpoints the difficulties of handling the vital informal processes in distributed collaboration that are so vulnerable because the distances risk detaining their growth and increasing their decay rate. The research suggest to support and securing these informal processes through explicit and formal means and to ensure management’s continuous focus on this effort to succeed.	software development	Gitte Tjørnehøj;Maria B. Balogh;Cathrine Iversen;Stine Sørensen	2014		10.1007/978-3-662-43459-8_8	software project management;systems engineering;engineering;knowledge management;software development;management science;project management 2.0;software development process	SE	-72.20459051757251	20.898971596483804	74360
495a720211bb096b4260a298ccf4b52855c39e1a	on business intelligence systems	life style;architecture systeme;procesamiento informacion;information technology;business strategy;technologie information;data mining;management data processing;information processing;data warehousing;informatique gestion;arquitectura sistema;business intelligence;information system;system architecture;traitement information;tecnologia informacion;systeme information;standard of living;informatica gestion;sistema informacion	Business enterprises today face unprecedented competitive pressures, as the pace of advances in computer and information technologies has become maddeningly fast and the standard of living and life styles of consumers have undergone significant changes. Enterprises can gain competitive edges if they can set their business strategies for winning new customers, retaining existing customers, and reducing the cost of doing business better. One promising way is for enterprises to base these strategies on business intelligence that can be anaJyzed and deduced from the vast amounts of data at their disposal. Today, advances in information technology have made available such technologies as data mining, online analytical processing, and data warehousing. It is now possible to construct business intelligence systems using these technologies. In this paper, we will describe what business intelligence systems are, examine the enabling technologies, and outline several key issues that need to be given serious considerations in successfully building and using business intelligence systems.	data mining;online analytical processing	Won Young Kim	1998		10.1007/3-540-64216-1_59	business model;business analysis;business transformation;standard of living;information processing;computer science;artifact-centric business process model;artificial intelligence;business case;electronic business;database;business analytics;business intelligence;business process discovery;business rule;operations research;new business development;information technology;computer security;business process modeling;information system;business activity monitoring	AI	-69.3648590696084	6.431239124244476	74399
81d7238e2d650039bc328337b71f87471cd93d8c	examining trust within the team in it startup companies--an empirical study in the people's republic of china	empirical study;first year;dp industry;team working dp industry;companies;team working;indexes;innovation management;indexation;people s republic of china;companies innovation management context indexes educational institutions;context;economic growth;relational capital trust examination it startup companies people s republic of china economic growth rates entrepreneurial teams	The People's Republic of China has shown strong economic growth rates within the last years, which can in part be attributed to the increasing foundation of startup companies, especially in the IT sector. Due to a lack of familiarity with novel situations, it is difficult for entrepreneurial teams to operate successfully during the first years after starting out. Previous research suggests that high levels of trust within the team can mitigate these difficulties and positively influence the relational capital that in turn is beneficial for the success of startup companies. The present study analyzes influencing variables on the relational capital dimension trust within IT startup companies in China. The results show that the innovativeness of a business idea as well as the business growth potential have a positive impact on trust within the entrepreneurial team, especially if the team consists of Chinese - as opposed to non-Chinese - startup founders.		Oliver Oechslein;Andranik Tumasjan	2012	2012 45th Hawaii International Conference on System Sciences	10.1109/HICSS.2012.261	database index;economics;innovation management;computer science;marketing;empirical research;management;commerce	SE	-83.66852294326027	5.222926534615306	74451
a3bb5ad5a4c9588b94fac4fd8b2ad7969bdbcfea	mining resource assignments and teamwork compositions from process logs		Process mining aims at discovering processes by extracting knowledge from event logs. Such knowledge may refer to different business process perspectives. The organisational perspective deals, among other things, with the assignment of human resources to process activities. Information about the resources that are involved in process activities can be mined from event logs in order to discover resource assignment conditions. This is valuable for process analysis and redesign. Prior process mining approaches in this context present one of the following issues: (i) they are limited to discovering a restricted set of resource assignment conditions; (ii) they are not fully efficient; (iii) the discovered process models are difficult to read due to the high number of assignment conditions included; or (iv) they are limited by the assumption that only one resource is responsible for each process activity and hence, collaborative activities are disregarded. To overcome these issues, we present an integrated process mining framework that provides extensive support for the discovery of resource assignment and teamwork patterns.	business process;fundamental fysiks group;microsoft outlook for mac;mined;real life	Stefan Schönig;Cristina Cabanillas;Claudio Di Ciccio;Stefan Jablonski;Jan Mendling	2016	Softwaretechnik-Trends		business process;computer science;systems engineering;business process discovery;management science;work in process;process mining;teamwork;knowledge management;process modeling;human resources	ML	-66.86878729766465	13.86270434060043	74465
2217123023b31e1505ec1858d153f9fecab017c5	the future of telecommunications		For two decades the telecommunications industry has been facing severe changes caused by the growing dominance of the Internet architecture as general purpose communications infrastructure, mass media, and also by new generations for mobile communications. This development will continue: new network technologies (e.g. Next Generation Networks, Long Term Evolution) will change cost and market structures, innovation cycles for products and technologies will become shorter, value chains will change. This is illustrated by vendors entering online service markets or offering their own network communication through so-called managed services. The change of operations system platforms and the emergence of application market places, especially in the mobile domain with the so-called apps, influence the future development considerably. Established network operators, on the other hand, start to specify individual consumer devices; classic computer manufacturers break into the telecommunications market by way of consumer devices as well as new services. The acquisition of Motorola Mobility by Google is only one further indicator that this ecosystem is subject to fundamental change. As sign of emancipation from services and infrastructure, numerous new enterprises offer carrier agnostic telecommunications and data services without owning the infrastructure, so-called over the top providers. Along with this a gradual shift of shares of the value chain with neighboring industries such as the media industry can be observed. Information technology (IT) and the next generation of telecommunications – based on the Internet protocol IP supporting services as well as the infrastructure – will get more and more similar. This is why many IT paradigms will become transferable to the telecommunications infrastructure. The growing convergence is coined by the term information and communications technology (ICT). Operators are often faced with the decision if they should build their future on pure network operations or seek premiums through new types of network specific services (using so-called enablers) or network agnostic services, respectively. The development of the telecommunications industry and the impulses and innovations caused by it in business and society depend considerably from the institutional guidelines, especially from regulations. The controversially discussed topic of network neutrality, i.e. the equal treatment of all Internet traffic allows for different views on the same thing: network neutrality can be interpreted as productive or counterproductive to innovation. In any case the internal operations of the involved players have to be designed very effectively and efficiently to reach a higher degree of automation. This is where reference models such as the Enhanced Telecom Operations Map (eTOM) help to reach a standardized course of action when needed. The harmonization between business processes and enterprise information systems along with a well tuned enterprise architecture plays a crucial role here. Further new challenges offer opportunities, but require a thorough techno-economical investigation. The current network transformation to an IP-based infrastructure of the Next Generation Networks offers potential for increased efficiency, but also requires high investments of billions of Euros with various decision alternatives. The started rollout of fiber optics connection right into homes (Fiber To The Home) will not only offer bandwidth of well over 100 Mbit/s to end customers, but will also give operators new opportunities to shape the network architecture, as well as to further automate the internal business processes up to a zero touch (i.e. fully automated) provisioning of services. This enormous build-out will not be done by former monopolies (so-called incumbents) alone, but will be shared. The business models of infrastructure suppliers will require a mutual ability for wholesale. This implies that in the future, incumbents might also lease some access capacity from other operators, which was not the case before. In addition to that it turns out that in some geographical regions public spending might be necessary (e.g. by means of Public-Private-Partnerships) in order to achieve full coverage of high speed broadband access. It all sums up to the fact that innovative products and applications have to be found that are convincing to the customer both in functionality and price. As a matter of	business process;carrier preselect;communications protocol;ecosystem;emergence;enterprise architecture;enterprise information system;fiber to the x;internet access;megabit;net neutrality;network architecture;next-generation network;online service provider;optical fiber;provisioning	Udo Bub;Arnold Picot;Helmut Krcmar	2011	Business & Information Systems Engineering	10.1007/s12599-011-0178-0	telecommunications service;telecommunications equipment	Web+IR	-72.94531688013956	6.901518685774907	74467
1d35d36ad738e5a5b91bffe9a1688ae91331b6e1	relationships among factors related to employee perceptions of the green movement in organisations	corporate culture;gestion de calidad;environmental issues;employe;desarrollo sostenible;firm size;developpement durable;employee perceptions;cultura de empresa;percepcion;qualite service;organisational performance;services and standards;estrategia empresa;maturite;maturity;green movement;environmental movement;gestion de la qualite;organisational culture;employee;perception;quality management maturity;firm strategy;strategie entreprise;empleado;culture d entreprise;green orientation;service quality;sustainable development;madurez;quality management;calidad servicio	In this research, we examine whether organisations of differing size (large vs. small) and focus (regional vs. national/international) differ on several variables related to the green movement and Quality Management (QM). The variables examined include organisational green orientation, organisational culture, employee perceptions of organisational green orientation, QM maturity, and outcomes, in terms of positive impacts of the green movement and organisational performance. We find evidence for differences in use of traditional QM tools for large vs. small organisations and for both traditional and advanced QM tools in organisations differing in focus.	capability maturity model	Lillian Y. Fok;Susan M. L. Zee;Sandra J. Hartman	2009	IJSS	10.1504/IJSS.2009.032175	organizational culture;economics;operations management;sociology;management;perception;operations research;maturity;service quality;sustainable development	HCI	-85.90914048023238	4.472803392166207	74470
c3bad738fb8ad0d1630359c9e328548927ced45c	developing and codifying business models and process models in e-business design	system dynamics;business strategy;modelling;business models	The development of business models for eBusiness has become increasing popular within both the academic and business arena. We believe that many of the business models for eBusiness are static in nature and only provide a historical view. In this paper we draw upon the emergent knowledge of eBusiness together with the traditional strategy theory and provide a simple framework for the evaluation of business models for eBusiness. Central to this paper we use dynamic modelling techniques of systems dynamics to evaluate an eBusiness model using the triple pair approach in an effort to capture the casual relationships and rationalise the complexity of organisation’s resources and the environment it must compete in.	business requirements;electronic business;emergence;system dynamics	Philip Joyce;Graham Winch	2004			function model;business process discovery;electronic business;business process modeling;business process model and notation;management science;computer science;process management;process modeling;business model;artifact-centric business process model	SE	-74.27773243942256	9.007875095259859	74474
3a8a5df05c164368eb259f40e61e90f867e1f5b7	externalization of software behavior by the mining of norms	empirical study;software libraries;contracts;data mining;exception handling anti patterns;exception handling;exception flows analysis;static analysis tool;computer bugs;context;australia;open source software	Open Source Software Development (OSSD) often suffers from conflicting views and actions due to the perceived flat and open ecology of an open source community. This often manifests itself as a lack of codified knowledge that is easily accessible for community members. How decisions are made and expectations of a software system are often described in detail through the many forms of social communications that take place within a community. These social interactions form norms which are influential in dictating what behaviors are expected in a community and of the system. In this paper, we provide a tool which mines these social interactions (in the form of bug reports) and extract norms of the system, externalizing this information into a codified form that allows others within the community to be aware of without having witnessed the social interactions.	ecology;interaction;open-source software;software development;software system	Daniel Avery;Khanh Hoa Dam;Bastin Tony Roy Savarimuthu;Aditya K. Ghose	2016	2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR)	10.1145/2901739.2901744	exception handling;software bug;computer science;engineering;operating system;data mining;programming language;empirical research;management;world wide web;computer security	SE	-74.74372615325224	22.442979824470893	74475
357cdcfe4ac37f7cbdc7f4bbceadef104e955b7e	action research: a research approach for cooperative work	research and development management groupware information systems planning;groupware;information systems;qualitative research;collaborative work;multiview;design and development;collaboration;research and development management;testing;collaborative work information systems management information systems collaboration testing process control process planning;synergistic relationship;practitioners action research multidisciplinary research teams multiview information systems action planning collaborative work synergistic relationship;process control;management information systems;planning;action planning;process planning;information system;multidisciplinary research teams;practitioners;action research;cooperative work	Action research is a qualitative research approach, which is not only suitable for multidisciplinary research teams. but also mixed teams of researchers and practitioners. It aims to improve practice through the collaborative work of researchers and practitioners. It is a synergistic relationship because practitioners, and the improvement of practice, inform research and researchers whilst, at the same time, researchers apply their theories to practice. In this paper we discuss action research and how Multiview, an approach to the design and development of information systems, was itself defined through action research.		David E. Avison	2002		10.1109/CSCWD.2002.1047641	knowledge management;action research;process control;management science;management;information system	HCI	-65.54427683850618	16.479394258959427	74537
803a0eaa3993a5d8a7a3522700e4a022f4c70f08	the business process management way of training and coaching	process professionals;training;coaching;way of training and coaching;skills gap;cross disciplinary competencies	In this chapter we explore a standardized and common way to train process professionals with a detailed career path for process experts, process architects, and process engineers. The result is that we turn traditional process education into performance-based project coaching that provides a different set of cost–benefit value ratios, representing the most effective way for organizations to build their process skills.	business process	Mark von Rosing;Henrik von Scheel;August-Wilhelm Scheer	2015		10.1016/B978-0-12-799959-3.00034-3	simulation;engineering;knowledge management;pedagogy	HCI	-76.45414569147313	9.0096188229441	74569
af7c279444ab6ab816f28ecc31da0035b879eddb	knowledge management strategy and service firm performance: a comparison of firms competing on low-cost versus high-quality	hospitality;information technology;knowledge management;service;performance	This study contributes to the knowledge management literature by comparing the effects of four knowledge strategy configurations on the performance of service firms competing on low-cost versus high-quality. Data was collected from 107 hospitality establishments operating in South Africa. Firms competing on low-cost and high-quality were classified into one of four groups based on their approach to knowledge management and were then compared on two dimensions of service firm performance. Results suggest that information (IT) based approaches to knowledge management are associated with high performance amongst hospitality services firms competing on low-cost whilst human capital based approaches are associated with high performance amongst firms competing on quality. Implications for knowledge management are discussed.	americas conference on information systems;causal filter;information;interaction;knowledge base;knowledge management;quality of service;strategic management;while	Jason F. Cohen;Karen Olsen	2013				HPC	-82.7637248048465	5.597556536413029	74577
08366cbc5ad8e6eb7696bc1e3f9edb41c1b5b0d2	advancements in cloud computing for logistics	semiautomated on demand interface creation service cloud computing integrated ict infrastructure integrated ict services logistic service providers it capacity demands r and d cc eu project logical cloud logistic sector logistic cloud architecture;software architecture cloud computing innovation management logistics data processing service industries;logistics cloud computing collaboration communities synchronization;logistics data processing;software architecture;innovation management;service industries;cloud computing	Adequate integrated ICT infrastructure and services are a prerequisite for keeping pace with the rapid rise of complexity and service levels in logistics. Recent studies indicate a high attractiveness and impact perspective of cloud computing for logistics service providers within few years in order to cope with the growing IT capacity demands. Within this paper, a comprehensive overview is given on R&D with relation to CC for logistics. Among these, the EU-project LOGICAL is presented in detail since it combines different aspects and benefits of CC for the logistics sector. A generic system of CC use cases in logistics and the corresponding needs for a logistics cloud architecture are discussed and compared with the implementation status of the LOGICAL cloud. Special attention is given to the problem of incompatible data and service interfaces. Instead of following the single-window, single-document concept, a semi-automated on demand interface creation service is presented as an intermediate alternative for the practitioning logistics sector.	cloud computing;complexity;itil;logistics;semiconductor industry	Uwe Arnold;Jan Oberländer;Björn Schwarzbach	2013	2013 Federated Conference on Computer Science and Information Systems		software architecture;simulation;cloud computing;innovation management;computer science;operating system;integrated logistics support;database	HPC	-72.32780486559341	13.337743114712124	74589
1aedced8b26b7395a3c81e8b51939590435c646a	modelling and search: efficient, practical, and innovative approaches to software engineering	search problems;software engineering;cmsbse 2013;combining modelling and search-based software engineering;first international workshop;innovative approaches	An overview of the invited tutorial 'Modelling and Search: Efficient, Practical, and Innovative Approaches to Software Engineering', presented at the First International Workshop on Combining Modelling and Search-Based Software Engineering (CMSBSE 2013).	search-based software engineering	Simon M. Poulding	2013			verification and validation;software engineering process group;software sizing;information engineering;software verification;search-based software engineering;computer science;systems engineering;software design;social software engineering;component-based software engineering;software development;feature-oriented domain analysis;software engineering;software construction;computer-aided engineering;management science;requirements engineering;software walkthrough;software requirements;software system;computer engineering	SE	-63.0166903653531	24.164218505921415	74625
ab35d1e8125bed4a5b69f0cd7609eaefbdc0b907	architecture prototyping in the software engineering environment	developpement logiciel;computer aided design;architecture systeme;formal specification;technology;software systems;software engineering;software requirements;software architecture;software engineering environment;design and implementation;system design;software development;technologie;genie logiciel;software life cycle;product design;system architecture;program development;conception systeme;software quality;dynamic behavior	This technical essay presents a perspective on the evolution and problems of the software development craft and how software engineering techniques show promise to solve these problems. It introduces architecture prototyping as a program development technique for improving software quality. Experience with large software systems shows that over half of the defects found after product release are traceable to errors in early product design. Furthermore, more than half the software life-cycle costs involve detecting and correcting design flaws. In this paper, we explore a disciplined approach to software development based on the use of formal specification techniques to express software requirements and system design. As a consequence, we can use techniques like rapid prototyping, static design analysis, design simulation, and dynamic behavior analysis to validate system design concepts prior to element design and implementation. We explore how these techniques might be organized in a software architecture prototyping facility that would be similar to the Computer-Aided Design and Manufacturing (CADAM) tools used in other engineering disciplines. We also examine the process by which software engineers might use these facilities to create more reliable systems.	software engineering	William E. Beregi	1984	IBM Systems Journal	10.1147/sj.231.0004	software architecture;requirements analysis;personal software process;verification and validation;software engineering process group;software sizing;systems engineering;engineering;package development process;software design;social software engineering;software development;software design description;operating system;software engineering;software construction;formal specification;systems development life cycle;software walkthrough;product design;resource-oriented architecture;software deployment;goal-driven software development process;software development process;software requirements;software quality;systems architecture;software system;computer engineering;technology;software peer review	SE	-63.342453923492435	26.751976374340728	74749
322f354637feaa89e89d4804f3f50a8472bfd8d0	associations rules between sector indices on the warsaw stock exchange		In this paper an Association Rules data mining technique is adopted to explore the co-movement between sector indices listed on the Warsaw Stock Exchange. The sector indices describe various parts of the Polish economy as well as Ukrainian companies and are not as sensitive to individual random events as single companies are. The measures describing discovered rules are calculated and strong rules are selected. Based on the strong rules the relations between parts of the Polish economy are presented. The interesting mutual interrelations between parts of Polish and Ukrainian economies are also observed.		Krzysztof Karpio;Piotr Lukasiewicz;Arkadiusz Orlowski	2016		10.1007/978-3-319-45321-7_22	business;commerce;monetary economics	DB	-84.4869136635705	10.090236542339614	74784
eac42e89b6879d09f68c1915c2a39be426d7c1a4	integrating hcd into bizdevops by using the subject-oriented approach		The DevOps-approach becomes more and more important because of the success of agile software development in conjunction with the continuously changing reality. It aims at unifying development and operations. A common team is responsible for both domains. Additionally, there are approaches like Continuous Software Engineering with the intention to unify business administration (Biz) and development. Even tool chains for BizDevOps are possible. The paper discusses aspects of BizDev and BizDevOps using a subject-oriented approach for supporting aspects of HCD. The focus lies on modeling user activities and business processes. Additionally, the role of domain-specific textual languages is discussed. Most important is the fact that methods from HCI like task modeling or storytelling can support BizDev and BizDevOps.		Peter Forbrig;Anke Dittmar	2018		10.1007/978-3-030-05909-5_21	software engineering;business process;devops;storytelling;agile software development;computer science	NLP	-62.99745608198935	18.88085428083561	74858
17afe119710bfc406101224c5f42ee298e570e15	a horizontal approach for software process improvement	horizontal change approach horizontal approach software process improvement vertical approach horizontal change hoc a software development organizations vertical process improvement;software process improvement;software development;management of change software development management;management of change;process improvement;programming quality management monitoring manufacturing industries process control phase measurement computer science instruments statistical analysis management accounting;software development management	Software process improvement practice and research have predominantly assumed a vertical approach to definition and change. Our studies show that vertical approaches have inherent difficulties. We have developed a new approach called horizontal change (HOC-A) that provides a new environment for change in software development organizations. We describe some of the difficulties of the vertical process improvement approaches and demonstrate, with examples. how the horizontal change approach addresses these difficulties.	software development process	Onur Demirörs;Dennis J. Frailey	1995		10.1109/CMPSAC.1995.524797	personal software process;change management;verification and validation;team software process;software engineering process group;software sizing;software project management;systems engineering;engineering;business process management;social software engineering;software development;change management;iterative and incremental development;software construction;management science;empirical process;management;software deployment;goal-driven software development process;software development process;software metric	SE	-68.02390255675105	21.39805512719865	74899
55a29cbd7ccbd869bb82fd00ecbf2955bc9a949a	evaluating human work in the digital factory - a new german guideline -		Over a period of several years a Guideline Committee of the Association of German Engineers (VDI) engaged in the development of a part of a guideline for computer-aided modelling and ergonomic evaluation of working people in digital factory tools. The subject of this part is to document the state of the art in analysis, evaluation and assessment of stresses and strains through work tasks. Broken down according to the separate aspects of engineering mechanics, it looks at existing anthropometric and physiological functionalities of evaluating human labour, with a distinction made between those that are already required as standard and those that are still considered to be optional and are only applied in a few procedures. It appears that the evaluation methods used often lead to different assessments, and that some necessary functions are not yet realized. Further progress is therefore required from an ergonomic perspective.		Gert Zülch	2014		10.1007/978-3-662-44733-8_5	systems engineering;engineering;software engineering;manufacturing engineering	HCI	-71.72738266936433	16.831109572676326	74955
1b6723db23745014e873efaaf8cb90044406d625	contract as a source of trust--commitment in successful it outsourcing relationship: an empirical study	empirical study;outsourcing;outsourcing contracts dp management;contracts;it outsourcing;contractual mechanism contract it outsourcing relationship relational governance interorganizational performance service level agreements;service level agreement;dp management;contracts outsourcing quality management performance analysis joining processes shape marketing management costs;relationship quality	Existing studies on IT outsourcing have mainly examined partnership factors that influence IT outsourcing effectiveness. This stream of IT outsourcing research has largely ignored how to foster and manage necessary attributes of partner relationships that promote relationship quality, leaving the role of the formal contract in the context of managing outsourcing relationships untouched. This research extends such studies and integrates the extant views that contracts and relational governance function as a complement and that relationship commitment and trust are the key attributes in the relational governance impacting on the interorganizational performance. Using service level agreements (SLA) we examine how specific elements of formal contracts help firms to shape their relational attributes such as trust and commitment that leads to the success of IT outsourcing. The results show that the effects of well-structured SLA in managing the relational aspects of IT outsourcing contracts are significant. This paper also provides insight into the development of relational governance through a contractual mechanism in IT outsourcing engagements	outsourcing;service-level agreement	Jahyun Goo;Kichan Nam	2007	2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07)	10.1109/HICSS.2007.148	business administration;knowledge process outsourcing;management;commerce;outsourcing	SE	-81.53609767411992	4.450177018294536	75023
ee0db189e3b1f7d748aa1e2e7f937a291525dabd	ai tools for business-process modeling	knowledge based system;building block;business process model;artificial intelligent;global economy;ai tool ai tools business process modeling artificial intelligence techniques cutting edge business applications knowledge based systems fortune 1000 companies manufacturing operations investment portfolios competitive global economy business management business process reengineering productivity business managers kbs techniques kbs tools art enterprise prokappa rethink;business data processing;artificial intelligence business process re engineering knowledge based systems companies productivity costs turning pressing job shop scheduling manufacturing;knowledge based systems systems re engineering business data processing;business process reengineering;knowledge based systems;systems re engineering	For more than a decade, artificial intelligence techniques have served as critical building blocks for cutting edge business applications. Knowledge based systems (KBS) in particular have helped numerous Fortune 1000 companies solve pressing business problems-everything from scheduling their manufacturing operations to managing their investment portfolios. AI has helped many companies improve productivity and reduce costs to meet the demands of today's competitive global economy. Today, business management itself is undergoing fundamental change. For the past several years, business process reengineering (BPR) has become the watchword. This move to rethink and redesign the way a company works aims at further boosting productivity and cutting costs. No wonder then that business managers worldwide are turning to explicit KBS techniques, long proven to achieve the very goals of BPR, to model change. The first wave of AI based tools and applications for business process modeling (BPM) is just hitting the shore. Organizations such as IBM, EDS, the US Army, and Swiss Bank are among the first to adopt AI for BPM. Some are using traditional KBS tools such as ART*Enterprise and ProKappa, while others are turning to ReThink, the first AI tool designed specifically for BPM.	process modeling	Sara Reese Hedberg	1996	IEEE Expert	10.1109/64.511772	business analysis;business process reengineering;computer science;knowledge management;artifact-centric business process model;business process management;artificial intelligence;knowledge-based systems;process modeling;business process model and notation;process management;business process;business process discovery;business rule;new business development;business process modeling;business activity monitoring;business architecture	Vision	-72.13186906399787	8.06697364763472	75074
226d3118cbfe620b6f310d40e1a973ca32c9543e	an ontological analysis of threat and vulnerability	mereotopology;cognitive science;threat assessment;formal threat ontology;opportunity intent;metaphysical framework;ontologies cognitive science educational institutions predictive models character recognition target recognition event detection target tracking economic forecasting psychology;upper level framework;economic forecasting;formal ontological analysis;integrated whole;threat prediction;event detection;vulnerability;psychology;basic formal ontology;ontologies artificial intelligence;target recognition;threat mitigation formal ontological analysis vulnerability metaphysical framework upper level framework formal threat ontology thro threat prediction;relations;predictive models;ontologies;target tracking;thro;capability;character recognition;ontology;mereotopology ontology threat assessment capability opportunity intent vulnerability relations basic formal ontology bfo integrated whole;basic formal ontology bfo;threat mitigation	The overall goal of this paper is to provide a formal ontological analysis of threat. In particular, this paper discusses the formal ontological structure of threats as integrated wholes possessing three interrelated parts: intentions, capabilities and opportunities, and shows how these elements stand to one another, as well as to states of vulnerability. This discussion offers a means for understanding variations of threat conditions such as potential vs. viable threats and dispersed threats. A general, metaphysical, upper-level framework for the development of a formal threat ontology (ThrO) offers a necessary foundation for designing consistent and comprehensive models for threat prediction and mitigation	ontology (information science);process (computing);requirement;threat (computer);threat model	Eric G. Little;Galina L. Rogova	2006	2006 9th International Conference on Information Fusion	10.1109/ICIF.2006.301716	psychology;knowledge management;data mining;social psychology	SE	-66.20181517295215	8.84761041523812	75112
12a9b98baaae20ac99e2239a4cf79ff414438b5a	customization of a commercial cm system to provide better management mechanisms	software development	Consider the question often asked in software development: Whyproduct 10 times faster than one engineer? Effort and schedule arerelationship. What function is the additional effort serving ifThis extra effort is generally referred to as overhead. The primarycomplexity of developing many pieces concurrently and integrating		Karen Parker	1995		10.1007/3-540-60578-9_27	simulation;mass customization;systems engineering;engineering;operations management	HCI	-70.69613251249301	28.572534747450927	75132
413ef3454e59f061569abbc98634e38e149a0603	sla management process model	service level;service requirements;contractor profile;support line;lessons learned;support;sla documents;process model;experience and lessons learned;service catalogue;service delivery	To make customers happy, one must continuously revise support provided to them and make decisions on how to change it in the next coming service delivery period. In this paper, we evaluate EM3: SLA Management process model within eleven software companies. Our results show that our model is realistic and correctly reflects the industrial practice.	itil;process modeling;service-level agreement	Mira Kajko-Mattsson	2009		10.1145/1655925.1655968	service level requirement;service catalog;support;service level;knowledge management;service delivery framework;operations management;process modeling;database;world wide web	SE	-71.22701584775488	15.34357457269035	75142
cf53e8f59bd45d8b9b51845722c3dee7bbf37901	the awareness network, to whom should i display my actions? and, whose actions should i monitor?	software;ethnographic data;project age;groupware;programming environments;tool support;programming environment;computer supported cooperative work;collaboration;tools computer supported cooperative work organizational management and coordination programming environments programming teams;software engineering;awareness network;software architecture;servers;programming teams;software programming monitoring interviews collaboration servers;monitoring;organizational setting;cscw awareness network computer supported cooperative work software development social actor ethnographic data organizational setting project age software architecture;social actor;media space;software development;interviews;software development tools;cscw;software architecture groupware;tools;organizational management and coordination;programming;work practice	The concept of awareness plays a pivotal role in research in Computer-Supported Cooperative Work. Recently, software engineering researchers interested in the collaborative nature of software development have explored the implications of this concept in the design of software development tools. A critical aspect of awareness is the associated coordinative work practices of displaying and monitoring actions. This aspect concerns how colleagues monitor one another's actions to understand how these actions impact their own work and how they display their actions in such a way that others can easily monitor them while doing their own work. In this paper, we focus on an additional aspect of awareness: the identification of the social actors who should be monitored and the actors to whom their actions should be displayed. We address this aspect by presenting software developers' work practices based on ethnographic data from three different software development teams. In addition, we illustrate how these work practices are influenced by different factors, including the organizational setting, the age of the project, and the software architecture. We discuss how our results are relevant for both CSCW and software engineering researchers.	computer-supported cooperative work;programming tool;software architecture;software developer;software development;software engineering	Cleidson R. B. de Souza;David F. Redmiles	2007	IEEE Transactions on Software Engineering	10.1109/TSE.2011.19	human–computer interaction;computer science;systems engineering;engineering;knowledge management;software engineering;computer-supported cooperative work;management	SE	-74.38224269608645	22.600903523458044	75148
f42b41f1942e154cea4dbb00a60e3dd1606057ee	an empirical analysis of the contractual and information structures of business process outsourcing relationships	information structure;outsourcing;information systems;bpo;performance;contractual structure;governance;information processing;coordination	The emergence of information-intensive business process outsourcing (BPO) relationships calls for the study of exchange performance beyond traditional considerations of the contractual structure that facilitates cooperative intent to include the information structure that facilitates the mutual exchange of information to enact cooperative intent and coordinate actions between the user firm and the service provider. Yet, there has been little analysis of the drivers and performance effects of the information structure of BPO relationships, including its linkages to the underlying contractual structure. This study integrates perspectives in neo-institutional economics and information processing to develop and test the theoretical argument that the extent of use and performance effects of the information structure of the BPO relationship are greater in time and materials BPO contracts than in fixed-price BPO contracts. Survey data on 134 BPO relationships provide empirical support for our hypotheses. The synergistic impact of incentives and information on BPO performance emphasizes that their joint assessment is necessary to enhance the explanatory power of extant theories of organization. This result also has implications for achieving maximum benefits from complex BPO arrangements that are more likely to be characterized by time and material contracts.	business process;outsourcing	Deepa Mani;Anitesh Barua;Andrew B. Whinston	2012	Information Systems Research	10.1287/isre.1110.0374	corporate governance;economics;information processing;performance;knowledge management;operations management;broker's price opinion;management;law;information system;outsourcing	DB	-82.41818519801961	4.286474292171788	75166
971dc61182fdc6215457d24e34f58d0ccd6959df	influence of transport junction on market potential of logistics	transportation logistics;public infrastructure;market potential transport junction logistics;logistics demand transport junction market potential transport mode cargo passenger transportation logistics service customer universality logistics enterprise public infrastructure information facility market demand sensitive analysis logistics transaction efficiency;logistics;sensitivity analysis;transportation;potential function;market potential	The transport junction takes kinds of transport mode as an integration, which completes the cargo and the passenger transportation. Since the transport junction enhances the scope of logistics service and the customer universality, reduces cost for the logistics enterprise to gain the public infrastructure and the information facility, therefore, has the inevitable influence to the potential market demand of logistics. This article establishes the market demand potential function of logistics, makes four conclusions by sensitive analysis. First, the market potential of logistics is decided by the logistics transaction efficiency. Second, the market potential of logistics is decided by the labor level division of logistics and society. Third, the market potential of logistics is decided by the network degree of logistics, particularly, important transport junction. Fourth, as the logistics cost is high, the transaction efficiency, the logistics demand, the labor level division and the network degree of logistics have more obvious influence to potential market demand of logistics.	logistics;transport layer security;universality probability	Wei Chen;Yan Zhang	2011	2011 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2011.6016807	logistics;transport;humanitarian logistics;integrated logistics support;traffic management;sensitivity analysis	ML	-72.71729716772009	6.750536037061856	75170
57924a983ca1f62c3ce5999b2824bb80962beafe	intersecting cultures and ecommerce: a case study	ecommerce;organizational culture;global management;internet;culture;structural features;information service;situated culture;business process	"""How do managers of global eCommerce services accommodate to the structural conditions and unique situated cultures in different locations where they do business, as they attempt to control business processes and shape organizational culture? This paper reports on a case study of an eCommerce company, """"EmergeInc,"""" that operated from offices in several different countries, selling online information services to customers in many different countries.The case illustrates management's role in attempting to accommodate to cultural and structural influences, while actively shaping organizational norms and practices and encouraging conformance to them. Three main techniques were used by the management team to shape norms and practices: using the Internet to disseminate stories, transferring employees, and creating new integrative roles and reporting relationships."""	business process;conformance testing;e-commerce;electronic business;email;emergence;internet;noise shaping;process (computing);shrink wrap contract;situated;transcription (software)	Janis L. Gogan	2006		10.1145/1125170.1125245	public relations;knowledge management;marketing;business	AI	-75.82808165777345	4.377623208102815	75201
2d9a0c0c33e16714406f317ba7fb5f3d0e6ff973	software requirements engineering - preface	software requirements		requirements engineering;software requirements	Nancy R. Mead	1997	Ann. Software Eng.		software engineering;software peer review;systems engineering;software requirements;requirement;software construction;requirements engineering;social software engineering;software requirements specification;computer science;requirements analysis	SE	-63.266380712628425	25.075275010942875	75224
4946f000166261f18e622c49bb5584f243f78d48	the data and analysis center for software(presentation abstract)	representation of solids;computer graphics;geometric data bases;computational geometry;geometric modelling;cad cam	An in-house study was performed by RADC between August 1974 and May 1978 to validate the predictions of several mathematical models for predicting the reliability and error content of a software package. This study used actual software error data that had been extracted from the formalized testing phases of four large software development projects encompassing command and control, avionics and radar control applications. Model predictions were compared on a project vs. project basis, as well as on an intra-project basis using total system, error criticality and functional subsystem categorizations. The results of this empirical analysis will be discussed in terms of the conclusions that were tentatively drawn as to potential model applicability. Results will also be discussed in terms of the error data available and the techniques used to extract the necessary information for input into the analyzed models.	avionics;categorization;criticality matrix;mathematical model;radar;software bug;software development	Lorraine Duvall	1978		10.1145/800178.810131	data modeling;computer science;software development;data administration;software construction;data mining;database;software system;software peer review	SE	-64.21156250916304	30.25765825282318	75228
fbc8b5616bc553f8771042912b206bf848560b07	an experimental investigation of the impact of individual, program, and organizational characteristics on software maintenance effort	software maintenance effort;human information processing;performance test;software maintenance;resource allocation;data collection;maintenance cost;variable name mnemonicity;time pressure;controlled laboratory experimentation;dynamic environment;semantic knowledge;information processing;control flow;experimental evaluation;variable name mnemoncity;control flow complexity	Resources allocated to software maintenance constitute a major portion of the total lifecycle cost of a system and can eect the ability of an organization to react to dynamic environments. A major component of software maintenance resources is analyst and programmer labor. This paper is an experimental evaluation of how the Human Information Processing (HIP) model can serve as a framework for examining the interaction of an individual's information processing capability and characteristics of the maintenance task. Independent variables investigated include program size, control  ̄ow complexity, variable name mnemonicity, time pressure, level of semantic knowledge and some of their interactions on maintenance eort. Data collection was done using the Program Maintenance Performance Testing System (PROMPTS) designed especially for the experiment. The results indicate that a HIP perspective on software maintenance may contribute to a decrease in maintenance cost and increase the responsiveness of maintenance to changing organizational needs. Ó 2000 Elsevier Science Inc. All rights reserved.	information processing;interaction;position-independent code;programmer;responsiveness;software maintenance	Sam Ramanujan;Richard W. Scamell;Jaymeen R. Shah	2000	Journal of Systems and Software	10.1016/S0164-1212(00)00033-9	reliability engineering;simulation;information processing;semantic memory;resource allocation;computer science;systems engineering;engineering;software engineering;programming language;control flow;software maintenance;management;statistics;data collection	SE	-68.17791285735122	31.75094058879288	75266
973ab6e1edd767cde0a82ffef6812008eba4cdfb	optimized software process for fault handling in global software development	six sigma;process capability;distributed team;software development;global software development;business value;software process	Software development organizations are turning to global softwaredevelopment (GSD) to reach a competitive lead on the global market. This paperpresents experiences and results of an Six Sigma based improvement projectin a GSD organization. The improvements address better process definition, increaseof awareness for different levels of expectations in globally distributedteams, and introduction of regular scanning mechanisms. Success indicators aredefined to connect process capability to business value, and are used to measureimprovement success by applying SPC techniques.	software development process	Dirk Macke;Tihana Galinac Grbac	2008		10.1007/978-3-540-79588-9_34	reliability engineering;personal software process;verification and validation;team software process;software engineering process group;software sizing;systems engineering;engineering;package development process;social software engineering;software development;software design description;software engineering;software construction;software walkthrough;empirical process;software deployment;goal-driven software development process;software development process;software peer review	SE	-68.28248896976905	21.46529418109046	75349
6a9e81ae31c50b44abdd6b8efc0d787e5de3116f	simple modeling of executable role-based workflows: an application in the healthcare domain	workflow management;healthcare clinical processes;process modeling;model driven software development;simplicity	Process modeling has become an established technique to document, analyse and automate workflows. Workflows that model the procedures followed by professionals require the immediate contribution of these professionals, who usually lack broad knowledge of formal models as well as software engineering skills. Simplicity of the selected modeling approach throughout each step of the design phase is therefore a key factor for the success of a workflow management project. In this paper we show a process modelling framework that combines simplicity and intuitiveness in the modeling phase with immediate evaluation of the models via execution. This combination allows practitioners and professionals to play a key role in the rapid development of IT applications and systems that support role-based workflows. We discuss the practicability and benefits of this approach on an exemplary case study from the healthcare domain: analysis and optimization of outpatient clinic processes of a leading German hospital.	business process model and notation;electronic product code;executable;mathematical optimization;modeling language;process modeling;programming language;rapid prototyping;round-trip engineering;software engineering	Tiziana Margaria;Steve Boßelmann;Bertold Kujath	2013	J. Integrated Design & Process Science	10.3233/jid-2013-0017	computer science;systems engineering;knowledge management;management science	SE	-65.39715380160597	15.186127536523411	75456
c1ac7c88bda672c344c614551f2bb3bb529e0af1	a model driven engineering approach for business continuity management in e-health systems	computers;formal specification;medical administrative data processing;standards organizations;business continuity plan model driven engineering approach business continuity management e health systems information and communication technology healthcare services healthcare sector system downtime patient safety formal description;organic computing;health system;medical services;business continuity;unified modeling language organizations business continuity medical services standards organizations computers;unified modeling language;delivery of healthcare;model driven engineering;model;business continuity plan;patient safety;e health business continuity plan model meta model model driven engineering;e health;organizations;new information and communication technologies;medical administrative data processing business continuity formal specification health care;meta model;business continuity management;health care	Nowadays e-Health systems recognize the benefits of new Information and Communication Technology (ICT) in the delivery of healthcare services. While these modern technologies have enhanced practices in the healthcare sector, the potential of failures to interrupt a process is still important. This risk of system downtime may induce significant impacts on patient safety. Thus, ensuring business resilience has proven to be increasingly challenging as the healthcare field employs more ICT applications and all signs for the future point to even more reliance on digital data. In this context, we propose an approach based on Business Continuity Management (BCM) to ensure the ability to operate in spite of unforeseen events and to quickly recover from any type of business interruption. The basic purpose of this paper is to define a formal description of a Business Continuity Plan (BCP), following an approach grounded in Model-Driven Engineering (MDE).	bulk copy program;business continuity planning;digital data;downtime;interrupt;model-driven engineering;model-driven integration;scott continuity	Olfa Rejeb;Rémi Bastide;Elyes Lamine;François Marmier;Hervé Pingaud	2012	2012 6th IEEE International Conference on Digital Ecosystems and Technologies (DEST)	10.1109/DEST.2012.6227931	recovery time objective;systems engineering;engineering;knowledge management;artifact-centric business process model;management science	DB	-64.40164023668274	10.777996528678408	75467
6c6e9b91488c39d72da1eff81ffe19c27d6f56eb	modeling the evolution of generativity and the emergence of digital ecosystems	simulation and modeling is;complexity theory;digital business ecosystems;bepress selected works;platform design;innovation;complexity theory digital business ecosystems innovation platform design simulation and modeling is	Recent literature on sociotechnical systems has employed the concept of generativity to explain the remarkable capacity for digital artifacts to support decentralized innovation and the emergence of rich business ecosystems. In this paper, we propose agent-based computational modeling as a tool for studying the evolution of generativity, and offer a set of building blocks for constructing agent-based models in which generativity evolves. We describe a series of models that we have created using these building blocks, and summarize the results of our computational experiments to date. We find in several different settings that key features of generative systems can themselves evolve endogenously, including “core” components and reusable parts. Moreover, we find that boundedly rational designers without coordination or foresight can evolve business ecosystems that satisfy a diverse range of consumer preferences and exhibit robustness to changes in these preferences over time.	agent-based model;business ecosystem;capability maturity model;causal filter;computation;digital artifact;emergence;evolution;experiment;game theory;generative systems;imperative programming;information systems;list of information schools;object type (object-oriented programming);relevance;sociotechnical system;software system;unified model	C. Jason Woodard;Eric K. Clemons	2014			innovation;simulation;computer science;knowledge management;management	AI	-76.99022476118769	6.8485101628184335	75539
84c26f71aba3c4966d20574017c37181fa6c2872	quality characteristics of collaborative systems	performance measure;software metrics;software;groupware;reliability;complexity theory;probability density function;collaboration;metric;collaborative system;data mining;collaborative work collaborative software international collaboration computer science software quality application software medical simulation maintenance performance evaluation software measurement;quality evaluation;software metrics groupware;collaborative systems;metric collaborative systems quality characteristics;quality characteristics;specially designed software;conferences;specially designed software collaborative systems quality evaluation	This paper describe the new concepts of collaborative systems quality evaluation. There are identified structures of collaborative systems. The paper define the quality characteristics of collaborative systems. There are proposed a metric to estimate the quality level of collaborative systems. There are performed measurements of collaborative systems quality using a specially designed software.	collaborative software;concatenation;database;display resolution;knowledge-based systems;relevance	Ion Ivan;Cristian Ciurea	2009	2009 Second International Conferences on Advances in Computer-Human Interactions	10.1109/ACHI.2009.53	computer science;software engineering;management;collaboration	SE	-64.01164998450506	27.444326745439206	75552
b86717001c0fc1e76e9436f8c31ea3eeacf7b192	what are problem causes of software projects? data of root cause analysis at four software companies	software testing;project management;software process improvement;software management project management;software software engineering software testing companies context programming monitoring;software management;software engineering;task difficulty;grounded theory root cause analysis problem prevention software process improvement;software development tools software projects root cause analysis four software companies rca software engineering;root cause analysis;datavetenskap datalogi;grounded theory;software development tools;problem prevention;work practice	Root cause analysis (RCA) is a structured investigation of a problem to detect the causes that need to be prevented. We applied ARCA, an RCA method, to target problems of four medium-sized software companies and collected 648 causes of software engineering problems. Thereafter, we applied grounded theory to the causes to study their types and related process areas. We detected 14 types of causes in 6 process areas. Our results indicate that development work and software testing are the most common process areas, whereas lack of instructions and experiences, insufficient work practices, low quality task output, task difficulty, and challenging existing product are the most common types of the causes. As the types of causes are evenly distributed between the cases, we hypothesize that the distributions could be generalizable. Finally, we found that only 2.5% of the causes are related to software development tools that are widely investigated in software engineering research.	experience;software development;software engineering;software industry;software testing;superuser	Timo O. A. Lehtinen;Mika Viking Mäntylä	2011	2011 International Symposium on Empirical Software Engineering and Measurement	10.1109/ESEM.2011.55	project management;reliability engineering;root cause analysis;computer science;systems engineering;engineering;software engineering;software testing;grounded theory	SE	-69.8923127838706	23.133831519795176	75561
699c74762364fa4998eecc25fd32912f4528f70b	service value broker patterns: an empirical collection	broker;electronic commerce;business modeling;ip networks contracts optimization computational modeling economics security;contracts;会议论文;economics service value design pattern business modeling broker;software engineering;service value;computational modeling;design pattern;ip networks;optimization;economics;electronic service service value broker pattern svb pattern business modeling knowledge management economic analysis svb study e service economics era;software engineering electronic commerce;security	The service value broker(SVB) pattern integrates business modeling, knowledge management and economic analysis with relieved complexity, enhanced reusability and efficiency, etc. The study of SVB is an emerging interdisciplinary subject which will help to promote the reuse of knowledge, strategy and experience in service based designs and solutions. In this paper, we focus on enumerating collected SVBs empirically with initial analysis on their composition manners. The results from this paper will play a dominating role in fueling a coming E-service Economics era.	e-services;knowledge management	Yucong Duan;Ajay Kattepur;Hui Zhou;Ying Chang;Mengxing Huang;Wencai Du	2013	2013 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing	10.1109/SNPD.2013.87	e-commerce;business model;computer science;information security;software engineering;database;management science;design pattern;computational model;computer security;computer network	SE	-73.20804064193467	6.890397212014925	75572
01ef817b5e2c47ce3af4679dd20e6a8a990a01fe	advances in supply chain simulation	analytical models;simulation based analysis;supply chain dynamics simulation based analysis supply chain management green logistics supply chain resilience sustainable logistics logistics security logistics safety logistics bottleneck saturation;environmental factors;supply chains logistics analytical models computational modeling supply chain management computer simulation project management risk analysis security safety;modeling and simulation;simulation;biological system modeling;logistics safety;supply chains;sustainable logistics;computational modeling;logistics;green logistics;supply chain resilience;supply chain;logistics security;supply chain management environmental factors logistics simulation;supply chain dynamics;logistics bottleneck saturation;supply chain management	The lecture aims to provide an insight into modern approaches to simulation-based analysis of supply chains. It gives an overview of using Modeling and Simulation (M&S) for facing different types of activities in supply chain management. In particular, different issues are considered, starting with new criticality such as: • Green Logistics “Green Logistics” concept born several years ago and originally was very strongly related to qualitative considerations devoted to improve the sustainability of logistics. A new approach to Green Logistics is devoted to developing new models to support quantitative analysis and to measure the real impact of the whole supply chain; such an impact affects sustainability and represents indirect costs and consumption of resources.	criticality matrix;logistics;simulation	Agostino G. Bruzzone;Yuri Merkuryev	2009	2009 Third Asia International Conference on Modelling & Simulation	10.1109/AMS.2009.151	systems engineering;operations management;process management;business	Robotics	-78.96163281300068	16.992150198507723	75585
aef15ce0c9f16e4588ed519f05a2442de096eb73	impact of process conformance on the effects of test-driven development	regression models;test driven development;developers productivity;software quality	Context: One limitation of the empirical studies about test-driven development (TDD) is knowing whether the developers followed the advocated test-code-refactor cycle. Research dealt with the issue of process conformance only in terms of internal validity, while investigating the role of other confounding variables that might explain the controversial effects of TDD. None of the research included process conformance as a fundamental part of the analysis.  Goal: We aim to examine the impact of process conformance on the claimed effects of TDD on external quality, developers' productivity and test quality.  Method: We used data collected during a previous study to create regression models in which the level of process conformance was used to predict external quality, productivity, and tests thoroughness.  Result: Based on our analysis of the available data (n = 22), we observe that neither quality (p -- value = 0.21), productivity (p -- value = 0.80), number of tests (p -- value = 0.39) nor coverage (p -- value = 0.09) was correlated with the level of TDD process conformance.  Conclusion: While based on a small sample, we raise concerns about how TDD is interpreted. We also question whether the cost of strictly following TDD will pay-off in terms of external quality, productivity, and tests thoroughness.	code coverage;code refactoring;conformance testing;fault coverage;internal validity;test-driven development	Davide Fucci;Burak Turhan;Markku Oivo	2014		10.1145/2652524.2652526	reliability engineering;test-driven development;computer science;systems engineering;engineering;operations management;software engineering;management;software quality;regression analysis	SE	-65.94383948176686	29.986102021820837	75606
8e74fe8f9b16f3af554f718e13c4fe0dd97293bd	building a knowledge base for guiding users through the cloud life cycle		Cloud Computing is still a hype to provide IT services as customers demand. Especially small and medium size businesses can benefit of this new trend because of the flexibility Cloud Computing provides for the consumption of its services. Nevertheless the step into the cloud must be carefully prepared and setup to ensure a successful usage and an appropriate return on investment. Currently the lack of experience with cloud projects and the uncertainty of its predestined application areas are detaining companies to use cloud computing services. No best practices, rules and use cases are in place yet to help to convince potential users to go their way to the cloud. With the CLiCk project the authors therefore propose a personalized webbased platform for Small and Medium Enterprises to support them over an entire Cloud Life Cycle.	best practice;cloud computing;knowledge base;personalization	Claudio Giovanoli;Stella Gatziu Grivas	2013			simulation;knowledge management	HCI	-71.91841351566802	12.177121664776411	75612
e2f71f429a7f93a3c0149ab71987f6b3149ec7b8	bp: security concerns and best practices for automation of software deployment processes: an industrial case study		SecDevOps is a paradigm for integrating the software development and operation processes considering security and compliance requirements. Organizations are reluctant to transform their development and operation processes to SecDevOps because of the expectation of incompatibility between security and DevOps. This paper reports about a study performed at IBM on transformation of five Business Intelligence (BI) projects to SecDevOps. The study revealed that main security concerns for the automation of the deployment process are: separation of roles, enforcement of access controls, manual security tests, audit, security guidelines, management of security issues, and participation of the security team. The major recommended best practices for a transformation of current processes to SecDevOps are: good documentation and logging, strong collaboration and communication, automation of the processes, and enforcement of separation of roles. Based on the empirical results, we conclude that separation of roles is the main aspect to be considered when planning to automate deployment processes. The results of the study are being used by IBM BI Unit and may be used by other organizations when planning to migrate to SecDevOps, especially for BI projects.	access control;automation;best practice;devops;documentation;programming paradigm;requirement;software deployment;software development;software incompatibility	Vaishnavi Mohan;Lotfi ben Othmane;Andre Kres	2018	2018 IEEE Cybersecurity Development (SecDev)	10.1109/SecDev.2018.00011		SE	-72.39761201969286	16.507710974828207	75649
75072b0b8b6beeb58f84a63411db0fdd1b3dd9a3	software project development cost estimation	cost estimation	This paper reports the results of an empirical investigation of the relationships between effort expended, time scales, and project size for software project development. The observed relationships were compared with those predicted by Lawrence Putnam's Rayleigh curve model and Barry Boehm's COCOMO model. The results suggested that although the form of the basic empirical relationships were consistent with the cost models, the COCOMO model was a poor estimator of cost for the current data set and the data did not follow the Rayleigh curve suggested by Putnam. However, the results did suggest that it was possible to develop cost models tailored to a particular environment and to improve the precision of the models as they are used during the development cycle by including additional information such as the known effort for the early development phases. The paper finishes by discussing some of the problems involved in developing useful cost models.		Barbara A. Kitchenham;N. R. Taylor	1985	Journal of Systems and Software	10.1016/0164-1212(85)90026-3	putnam model;simulation;computer science;engineering;cocomo;management;cost estimate;statistics	SE	-66.14633502060359	31.093026957905284	75675
fabf866713fe5a4cd2cf81fd2b2f2b7507a11e7f	critical success factors to improve the game development process from a developers perspective		The growth of the software game development industry is enormous and is gaining importance day by day. This growth imposes severe pressure and a number of issues and challenges on the game development community. Game development is a complex process, and one important game development choice is to consider the developer perspective to produce good-quality software games by improving the game development process. The objective of this study is to provide a better understanding of the developer’s dimension as a factor in software game success. It focusses mainly on an empirical investigation of the effect of key developer factors on the software game development process and eventually on the quality of the resulting game. A quantitative survey was developed and conducted to identify key developer factors for an enhanced game development process. For this study, the developed survey was used to test the research model and hypotheses. The results provide evidence that game development organizations must deal with multiple key factors to remain competitive and to handle high pressure in the software game industry. The main contribution of this paper is to investigate empirically the influence of key developer factors on the game development process.	capability maturity model;game engine;software development process;test management;video game design;video game developer;video game development;warez	Saiqa Aleem;Luiz Fernando Capretz;Faheem Ahmed	2017	CoRR	10.1007/s11390- 016-1673-z		HCI	-71.88503097919246	22.858572025866927	75740
41ffe81d0ff7423a72bcc3ef65ff76c347a5d9f0	a government it-focused e-procurement strategy for colombia	government procurement;e procurement;framework agreements;it procurement	In Colombia, public agencies execute independent contractual processes to acquire the same information technology (IT) goods or services, suppliers are mostly selected based on the minimum price, and the current procurement process is lengthy and inefficient. There is a need for allowing the Colombian Government to select suppliers considering other criteria (e.g. quality assurance) and also make purchases more efficient and transparent.  In this paper we present an initiative aimed at facilitating and standardizing the purchase of IT goods and services across public agencies, through the implementation of a government IT-focused e-procurement strategy. This strategy is mainly focused on the establishment and implementation of an IT e-procurement process, based on Pricing Framework Agreements and supported by a set of tools and training. The implementation of this strategy will enable the Colombian State to achieve a higher level of rationalization over the investment in IT and a reduction in time, effort and cost of the procurement process.	e-procurement;procurement;purchasing	Lina Marcela Morales Moreno;Javier Orlando Torres Paez;Adolfo Serrano Martínez	2014		10.1145/2691195.2691213	procurement;environmental resource management;business;commerce	HCI	-77.54839809423912	11.183033998826915	75743
e90dd4a2750df4d52918a610ba9fb2b013153508	function points as a universal software metric	business process model;configuration;dependency structure	"""Function point metrics are the most accurate and effective metrics yet developed for software sizing and also for studying software productivity, quality, costs, risks, and economic value. Unlike the older """"lines of code"""" metric function points can be used to study requirements, design, and in fact all software activities from development through maintenance. In the future function point metrics can easily become a universal metric used for all software applications and for all software contracts in all countries. The government of Brazil already requires function points for all software contracts, and South Korea and Italy may soon follow. However, there are some logistical problems with function point metrics that need to be understood and overcome in order for function point metrics to become the primary metric for software economic analysis. Manual function point counting is too slow and costly to be used on large software projects above 10,000 function points in size. Also, application size is not constant but grows at about 2% per calendar month during development and 8% or more per calendar year for as long as software is in active use. This paper discusses a method of high-speed function point counting that can size any application in less than two minutes, and which can predict application growth during development and for five years after release. This new method is based on pattern matching and is covered by U.S. utility patent application and hence is patent pending."""	benchmark (computing);capability maturity model integration;design by contract;function point;logistics;niche blogging;pattern matching;programming language;requirement;software bug;software industry;software metric;software sizing;source lines of code;total cost of ownership;use case points	Capers Jones	2013	ACM SIGSOFT Software Engineering Notes	10.1145/2492248.2492268	reliability engineering;software quality management;software sizing;computer science;systems engineering;engineering;function point;software engineering;weighted micro function points;configuration;business process modeling;use case points;software metric;avionics software	SE	-66.39439558620212	28.48505663171883	75790
7b1064792942df2d57caa11cf8aa5868a6880435	managing the performance of software engineering professional	software engineering management;softwareprofessional performance;software engineering;performance review process;software engineering software development management human resource management;engineering management software engineering software performance software tools psychology appropriate technology software measurement management training;corporate strategy software engineering management performance review process software professional performance software professionals;human resource management;software development management	This tutorial is directed at Software Engineering Managers and Senior Software Engineers in position of leadership, essentially anyone who is charged with evaluating and improving the performance of Software Professionals (both Software Engineering Managers and Software Engineers). It aims to provide attendees with a method they can employ in their own organizations at whatever level they deem appropriate in order to improve the performance of the high technology professionals reporting to them.	software engineer;software engineering	Lawrence J. Peters	2003		10.1109/CSEE.2003.1191400	personal software process;medical software;long-term support;verification and validation;software engineering process group;software verification;software project management;systems engineering;engineering;knowledge management;package development process;social software engineering;software development;software engineering;software construction;software walkthrough;software analytics;software deployment;software requirements;software system;software peer review	SE	-65.80053174160659	25.581374034695546	75876
f6cc8ff2d49c509850f7ff9edb6d9caf3fdbb9dd	why haven't ft systems been more successful?		technology learning series featured on www­dot­halliburton­dot­com­slash­redtech. This series of podcasts has been designed to highlight particularly significant products that Halliburton has recently brought to market. These technologies represent innovations that deliver reliability by simplifying complex completions, optimizing hydrocarbon recover or economically optimizing production.	podcast	Omri Serlin	1985			database;computer science;haven	HCI	-73.61073295441906	4.247018248780549	75902
5fec842f816f84bfe037db7db9f0df353c82449e	tool-based risk management made practical	formal specification;requirements management;e commerce;risk management;risk management aerospace industry costs computer industry aerospace safety medical services software safety software engineering project management protocols;requirements management tool based risk management it industry economic crisis safety critical industries web agencies e commerce requirements engineering;systems analysis;requirement engineering;economic crisis;formal specification risk management software development management systems analysis;point of view;software development management	Risk management has become one of the buzz words in the IT industry. The reasons can especially be found in the current economic crisis. While risk management is highly accepted in safetycritical industries (aerospace, healthcare), more and more branches (web agencies, e-commerce, software in general) see the value of establishing risk management processes. As risk management seems to be clear from a theoretical point of view, it is not from a practical standpoint. This is the challenge for project leaders in real life projects. This one-page abstract summarizes the main aspects of the industrial presentation held at Requirements Engineering 2002 (RE02) in Essen/Germany. The complete presentation deals with how risk management can be grounded as an integral part of a requirements management process. A full paper (German and English) describing this approach is available from the author by email.	e-commerce;email;floor and ceiling functions;real life;requirement;requirements engineering;requirements management;risk management	Holger Doernemann	2002		10.1109/ICRE.2002.1048523	project management;reliability engineering;engineering management;systems analysis;requirements management;requirement prioritization;systems management;program management;it risk management;risk management;software project management;data management;computer science;systems engineering;engineering;technology management;software engineering;applied engineering;formal specification;risk management information systems;requirements engineering;application lifecycle management;management;risk management framework;risk management plan	SE	-72.3366904381205	17.60252382932721	76199
fa8c108b17657f332a47bb23d6d24e0753c6c0ec	evaluating the effect of a delegated versus centralized control style on the maintainability of object-oriented software	control styles;design principle;object oriented design;software maintenance;responsibility delegation;object oriented software;controlled experiment;object oriented programming;indexing terms;65;object oriented programming object oriented software maintainability object oriented design professional java tool java design centralized control style delegated control style object oriented development;object oriented systems;index terms design principles;undergraduate student;object oriented;software development management software maintenance object oriented programming java;expert opinion;software maintainability;software development management;software maintenance centralized control software design java design methodology unified modeling language business communication logic electrical equipment industry object oriented programming;controlled experiment index terms design principles responsibility delegation control styles object oriented design object oriented programming software maintainability;design principles;java;cognitive complexity	"""A fundamental question in object-oriented design is how to design maintainable software. According to expert opinion, a delegated control style, typically a result of responsibility-driven design, represents object-oriented design at its best, whereas a centralized control style is reminiscent of a procedural solution, or a """"bad"""" object-oriented design. We present a controlled experiment that investigates these claims empirically. A total of 99 junior, intermediate, and senior professional consultants from several international consultancy companies were hired for one day to participate in the experiment. To compare differences between (categories of) professionals and students, 59 students also participated. The subjects used professional Java tools to perform several change tasks on two alternative Java designs that had a centralized and delegated control style, respectively. The results show that the most skilled developers, in particular, the senior consultants, require less time to maintain software with a delegated control style than with a centralized control style. However, more novice developers, in particular, the undergraduate students and junior consultants, have serious problems understanding a delegated control style, and perform far better with a centralized control style. Thus, the maintainability of object-oriented software depends, to a large extent, on the skill of the developers who are going to maintain it. These results may have serious implications for object-oriented development in an industrial context: having senior consultants design object-oriented systems may eventually pose difficulties unless they make an effort to keep the designs simple, as the cognitive complexity of """"expert"""" designs might be unmanageable for less skilled maintainers."""	centralized computing;cognitive complexity;correctness (computer science);jsp model 2 architecture;java	Erik Arisholm;Dag I. K. Sjøberg	2004	IEEE Transactions on Software Engineering	10.1109/TSE.2004.43	computer science;systems engineering;knowledge management;operating system;software engineering;programming language;object-oriented programming	SE	-71.36141517076724	24.75337359816042	76236
20835a205f77b5437fb9d8c3c321507ee5275333	the importance of ict: an empirical study in swiss smes	it strategy;empirical study;sme;ict use;switzerland	The following paper presents results from a longitudinal study about the importance and use of information and communication technology in Swiss small and medium-sized companies. In an empirical survey, 989 questionnaires were collected and analysed (return rate 17 %). The results were weighted according to company size and industry sector and are thus representative for Switzerland. The findings show that Swiss SMEs – in their self assessment – manage to gain (competitive) advantages from the deployment of information and communication technology. ICT is universally used (even) in (small and medium-sized) companies. There is a high degree of inter-organisational ICT use. ICT know-how and awareness are strongly rooted in management. The general conclusion is that IT matters for Swiss SMEs.		Petra Schubert;Uwe Leimstoll	2006			marketing;business;commerce	HCI	-83.38246651828827	5.207647809901692	76249
d6ab2251a32236ac65439b538a8aea4a9ec09f67	datamining: roadmap to extract inference rules and design data models from process data of industrial applications	knowledge rule extraction;datamining roadmap;industrial applications	The objectives of this study were to introduce the easiest and most proper applications of datamining in industrial processes. Applying datamining in manufacturing is very different from applying it in marketing. Misapplication of datamining in manufacturing system results in significant problems. Therefore, it is very important to determine the best procedure and technique in advance. In previous studies, related literature has been introduced, but there has not been much description of datamining applications. Research has not often referred to descriptions of particular examples dealing with application problems in manufacturing. In this study, a datamining roadmap was proposed to support datamining applications for industrial processes. The roadmap was classified into three stages, and each stage was categorized into reasonable classes according to the datamining purposed. Each category includes representative techniques for datamining that have been broadly applied over decades. Those techniques differ according to developers and application purposes; however, in this paper, exemplary methods are described. Based on the datamining roadmap, nonexperts can determine procedures and techniques for datamining in their applications.	data mining;data model	Hyeon Bae;Youn Tae Kim;Sungshin Kim;George J. Vachtsevanos	2005	Int. J. Fuzzy Logic and Intelligent Systems	10.5391/IJFIS.2005.5.3.200	systems engineering;engineering;data mining;management science	Robotics	-65.76934767640861	11.840978924392598	76300
c35663458e20553c95c8a5314287ac7342a2a2bb	pricing software development services	pricing;outsourcing;software development;agency theory;transaction cost economics;risk premium;transaction cost	This paper studies the pricing of software development outsourcing. Two pricing techniques – time and material and fixed price – are described and the economic conditions for selecting between them are discussed. Using agency theory and transaction cost economics, it is predicted that risky and specific systems will be priced on time and material basis while other projects will be fixed price. An additional prediction is that confidence in the vendor’s auditing of resources is essential for time and material contracts. The predictions are tested on fourteen external software development projects in two large corporations. Quantitative measures of risk, specificity and confidence are utilised, but the data-set does not support the theoretical predictions. In order to explain this result, interviews with senior managers at the two corporations have been conducted. Both disagree with the theoretical prescriptions: one contracts risky projects on fixed price basis, preferring to pay a risk-premium rather than to rebudget. The second expert allows fixed price only with trusted vendors, preferring time and material with all other vendors.	agile software development;outsourcing;sensitivity and specificity	Yossi Lichtenstein;Alan McDonnell	2003			fixed price;software development;marketing;vendor;risk premium;principal–agent problem;transaction cost;audit;outsourcing;economics	SE	-72.31217041108513	25.75206604283737	76369
fdbde329044f4a7c661171a8d3b5161606a1d7ab	management-aided software engineering	engineering management software engineering best practices time division multiplexing probes performance analysis vents;software management;best practice;software management management aided software engineering veteran software managers management best practices organizations;veteran software managers;small samples;software engineering;management best practices;organizations;management aided software engineering	Two veteran software managers examine some of today's management best practices for signs of what might become generally accepted practice in the near future. They present these practices from a small sample of healthy organizations. In addition, they envision how software management might mature over the next few decades to produce a new generation of best practices.<<ETX>>	best practice;software engineering	Sheila Brady;Tom DeMarco	1994	IEEE Software	10.1109/52.329398	engineering management;personal software process;verification and validation;software engineering process group;extreme programming practices;software configuration management;software project management;systems engineering;organization;engineering;package development process;social software engineering;software development;software engineering;software construction;software as a service;software walkthrough;application lifecycle management;management;software deployment;software quality control;software requirements;best practice;software peer review	SE	-68.24388927985966	25.90407254527068	76400
53c5fd8e35c91b041f6fde039aac6af2913fa8fe	static inspection: tapping the wheels of software	programming language;inspection;software engineering;lines of code;inspection wheels programming profession software testing software engineering computer languages software reliability software safety software quality automobiles;software quality static inspection software fault removal inconsistencies product testing software engineering standards programming languages safe complete unambiguous language definition programming limitations failures inaccuracies specification capture software reliability software safety;automatic detection;programming languages software quality safety inspection software reliability;safety;software reliability;software quality;programming languages	"""Static inspection is about the removal of obvious fault or inconsistency before a prod uct is even tested. It forms an indispensable part of all conventional engineering disciplines except for software engineering. For example, when a microcomputer is reviewed, the reviewer will inevitably remove the case, exposing the boards to inspect their quality of build, and make astute judgemen t s as to the long-term behaviour of the machine. Before a person buys a house, it is inspected for struct ur al and many other problems. It would be nice to think that all software engineering was subject to the same degree of care. It would be nice to think that after careful rumination by an impressive number of qualified people who together comprise a standar d s committee, the resulting program ming language came into the world in a safe, well-defined and unambiguous manner, suitable for use by program m e r s necessarily less knowledgeable about the language. It would be nice to think that any resulting transgressions to this safe, complete and unambiguous language definition, would be immediately detected by a compiler before they can cause any damage, and the program m er responsible informed of this fact in a straightforwar d manner. In such a world, elephant s fly and dreams always become reality. In our world, nothing could be farther from the truth. In spite of the best efforts of our language experts, we continue to produce program ming languages, which are not only unsafe, but frequently fail to acknowledge the fact. Compiler-writers compoun d the misery by providing compilers which are allowed to turn a blind eye to safety in obeisance to the great one-eyed god """"Performance"""". Modern languages such as Ada, Fortran 90 and C+ + further compoun d the misery by being very complex, and consequently exceedingly difficult to understa nd, and therefore difficult for which to write high-quality dependable compilers. As a result, much of the world's software in various languages is released full of inconsistencies, waiting for the right opport u nity to develop into a real failure. These work in consort with inaccuracies in capturing the specification to reduce the reliability and safety of our software in an era when software becomes ever more pervasive with many tens of thousand s and sometimes millions of lines of code in autom obiles, televisions, fire alarms, medical scanners and aircraft. To …"""	ada;compiler;dreams;fortran;microcomputer;our world;pervasive informatics;software engineering;source lines of code;struct (c programming language);television;turned a;wheels	Les Hatton	1995	IEEE Software	10.1109/52.382193	reliability engineering;medical software;long-term support;verification and validation;real-time computing;software sizing;computer science;engineering;package development process;software design;social software engineering;software reliability testing;software framework;component-based software engineering;software development;software engineering;software construction;software walkthrough;programming language;software deployment;software quality;software quality analyst;software system;avionics software;software peer review	SE	-68.89402619439797	27.919466996105598	76444
c18d1d8876f9bd1b97ddb5ffa5ceec9b23f0fa1e	technological uncertainty and firm boundaries: the moderating effect of knowledge modularity	technological uncertainty;technological innovation;uncertainty;biological system modeling;industries;aging;technology management;complex technological products;industry emergence;modularity;atmospheric modeling;firm boundary;knowledge engineering	One fundamental challenge of technology and innovation management is for firms to decide which future technologies to develop in-house versus buying them outside. This challenge is particularly pertinent when industries emerge, typically a time of high levels of technological uncertainty. It has been long understood that technological uncertainty functions as important stimulus for firms to manage their boundaries. However, two to some extent competing strategy perspectives—governance and competence—predict that firms faced with uncertainty would either increase or decrease their scope of activities, respectively. To reconcile these conflicting positions, we propose a model in which knowledge modularity moderates the effect of technological uncertainty on firms’ research and development (R&D) scope decisions. We develop new measures for R&D scope, knowledge modularity, and technological uncertainty, drawing on population ecology, network theory, and technology management. We test our model empirically using data on patenting activity and firm boundary location in the emerging automotive air bag industry. Our results generally support our model and show that in case of knowledge-generating activities such as R&D, scope decisions under technological uncertainty are more driven by concerns about the risk of obsolescence than the risk of opportunistic behavior. We discuss implications for managerial practice and future research.	ecology;network theory;relevance	Sebastian K. Fixson;Davit Khachatryan;Won Hee Lee	2017	IEEE Transactions on Engineering Management	10.1109/TEM.2016.2638847	atmospheric model;uncertainty;economics;systems engineering;engineering;knowledge management;marketing;technology management;knowledge engineering;modularity;management science;management;statistics	HCI	-82.31152441129385	5.027352268795316	76509
f1ca156bb95bf67f9600a50d81bcc3423a3c720d	demand-oriented competency development in a manufacturing context: the relevance of process and knowledge modeling		Competency management is a crucial success factor for organizations in the area of tension between knowledge management, human resource management, and process management, and has to be considered from a knowledge economy perspective. A basis for developing appropriate qualification measures in organizations is the comparison of necessary and available competencies. Given the time and cost intensity of the comparison process, the use of appropriate methods is of particular relevance for enterprises. This paper presents a procedural method and a software tool which enable resource-saving comparisons. Usually, employees’ “to competencies” are determined on a strategic level. Currently available “is competencies” can be derived from the actual knowledge transfer or from existing competence profiles. The method and tool first allow for the appropriate visualization of both competencies. After an automatized comparison of both contents, an overview of given and missing “to competencies” will be provided. Not available competencies can be addressed as qualification requirements and reflections regarding staffing or task allocation can be conducted.	amiga reflections;knowledge management;knowledge modeling;procedural programming;programming tool;relevance;requirement	Gergana Vladova;André Ullrich;Eldar Sultanow	2017			knowledge management	Web+IR	-76.8869554269049	9.358801375258048	76542
73f248fdc67065b89dbfcd8bb80aecd6d2225620	a business process management approach to perioperative supplies/instrument inventory and workflow	perioperative supply workflow;instruments;inventory management;medical administrative data processing;medical administrative data processing health care inventory management;hospitals;perioperative supply workflow business process management perioperative supply inventory;surgery;business process management;financial cost effectiveness business process management perioperative supplies instrument inventory perioperative supply workflow inventory management dynamic technological activities external organizational data hospital processes subsequent contextual understanding hospital perioperative process integrated information systems business analytics perioperative efficiency;perioperative supply inventory;hospitals surgery educational institutions inventory management instruments;health care	This study examines business process management practices applied to monitor, measure, and improve a hospital's perioperative supply workflow and corresponding inventory management. This paper identifies how dynamic technological activities of analysis, evaluation, and synthesis applied to internal and external organizational data can highlight complex relationships within integrated hospital processes to target opportunities for improvement and ultimately yield improved process capabilities. The identification of existing limitations, potential capabilities, and the subsequent contextual understanding are contributing factors that yield measured improvement within a hospital's perioperative process. Based on a 10-year longitudinal study of a large 909 registered-bed teaching hospital, this case study investigates the impact of integrated information systems to identify, qualify, and quantify business analytics used to improve perioperative efficiency and effectiveness across patient quality of care, operational efficiency, and financial cost effectiveness. The theoretical and practical implications and/or limitations of this study's results are also discussed with respect to practitioners and researchers alike.	business analytics;business process;capability (systems engineering);information system;inventory	Jim Ryan;Carmen Lewis;Barbara Doster;Sandra Daily	2014	2014 47th Hawaii International Conference on System Sciences	10.1109/HICSS.2014.359	computer science;knowledge management;business process management;operations management;database;management science;process management;health care	SE	-78.2226679631447	10.268862336122362	76633
2261eb80a9244fe0616acef4b411fc6a7aca2a5b	software test program: a software residency experience	software test program;relevant concept;great benefit;formal teaching;software residency;software testing;medical residency;federal university;software development practice;deep practice;software residency experience;information systems;information science;software engineering;biomedical informatics;computer science education;software development;human factors;programming	The Software Test Program (STP) is a cooperation between Motorola and the Center for Informatics of the Federal University of Pernambuco. It has been conceived with inspiration on the Medical Residency, adjusted to the software development practice. A Software Residency includes the formal teaching of the relevant concepts and deep practice, with specialization on some specific subject; here the focus is on software testing. The STP has been of great benefit to all parties involved.	software testing	Augusto Sampaio;Carlos Albuquerque;João Vasconcelos;Luckerson Cruz;Luis Figueiredo;Sérgio Cavalcante	2005		10.1109/ICSE.2005.1553611	software review;programming;personal software process;medical software;software engineering process group;information science;software project management;computer science;systems engineering;engineering;social software engineering;software development;software engineering;software testing;software technical review;software walkthrough;information system;computer engineering;software peer review	SE	-66.02651398687178	25.408929595544606	76754
54db418fed43e535ff9d021720dd7534cfdb2121	defect prediction: accomplishments and future challenges	trends and future directions software quality assurance defect prediction;software predictive models data models complexity theory market research power measurement;software systems complexity software prediction models software defect prediction software quality assurance;software quality software metrics	As software systems play an increasingly important role in our lives, their complexity continues to increase. The increased complexity of software systems makes the assurance of their quality very difficult. Therefore, a significant amount of recent research focuses on the prioritization of software quality assurance efforts. One line of work that has been receiving an increasing amount of attention for over 40 years is software defect prediction, where predictions are made to determine where future defects might appear. Since then, there have been many studies and many accomplishments in the area of software defect prediction. At the same time, there remain many challenges that face that field of software defect prediction. The paper aims to accomplish four things. First, we provide a brief overview of software defect prediction and its various components. Second, we revisit the challenges of software prediction models as they were seen in the year 2000, in order to reflect on our accomplishments since then. Third, we highlight our accomplishments and current trends, as well as, discuss the game changers that had a significant impact on software defect prediction. Fourth, we highlight some key challenges that lie ahead in the near (and not so near) future in order for us as a research community to tackle these future challenges.	algorithm;common platform;data pre-processing;just-in-time compilation;mobile app;open sound system;open-source software;openness;performance evaluation;preprocessor;software bug;software quality assurance;software repository;software system;weka	Yasutaka Kamei;Emad Shihab	2016	2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)	10.1109/SANER.2016.56	software security assurance;reliability engineering;personal software process;verification and validation;software sizing;software verification;computer science;systems engineering;social software engineering;software reliability testing;software development;software engineering;software construction;software analytics;software deployment;software quality control;software quality;software metric;software quality analyst;software peer review	SE	-64.96731540571987	25.965357964195505	76955
40e1120bd4c1432643281337e32336ec5293429b	security threats to internet: a korean multi-industry investigation	interruption;fabrication;corea;entreprise;banking;threat;fabricacion;red www;securite informatique;empresa;reseau web;besoin utilisateur;necesidad usuario;secteur bancaire;protocole tcp;coree;transmission control protocol;computer security;protection;internet;protocolo tcp;asie;interrupcion;intercepcion;user need;seguridad informatica;firm;manufacturing;korea;interception;institution financiere;proteccion;world wide web;financial institution;security;asia;modification	In recent years, a number of security problems with the Internet became apparent. New and current Internet users need to be aware of the likelihood of security incidents and the steps they should take to secure their sites. Before designing a secure system, it is advisable to identify the specific threats against which protection is required. The threats may be classified into interruption, interception, modification, and fabrication. The extents of these threats are examined across four industries in Korea — manufacturing, banking/financial, research institution/university, and distribution/service. Banking/financial firms generally perceive the four categories of threats more seriously than other industries. The companies in the manufacturing industry consider interruption to be the most serious. Research institutions/universities and distribution/service companies regard modification and interruption as critical threats. The appropriate security technique is recommended for each threat.	internet	Bumsuk Jung;Ingoo Han;Sangjae Lee	2001	Information & Management	10.1016/S0378-7206(01)00071-4	the internet;computer science;engineering;electrical engineering;threat;information security;marketing;operations management;transmission control protocol;interception;advertising;manufacturing;fabrication;management;social psychology;world wide web;computer security	HCI	-67.85371658124583	5.615614395001225	77021
bffdd687df6073dcbe2549eb3be413f6d3c14067	the impact of information technology on financial performance: the importance of strategic choice	information management system;eur j inform syst;information systems security;mis systems;information systems research;information security;case studies;information science;financial performance;ejis special issue;information security systems;information technology;business information technology;security information systems;european journal of information systems;information systems management;operational research society;business model;computer information systems;geographic information systems;information technology journal;information management;european journal of is;information systems journals;european journal information systems;information systems technology;managing information systems;accounting information systems;information and management;management information systems;define information systems;strategic information systems;business information management;soft system methodology;ejis journal;information system;health information systems;computer information technology;business information systems;business systems analyst;ejis;european journal;management science;journal of information systems;european journal of information system;information technology journals	Information technology (IT) may not automatically improve firm profitability. It is an essential tool, but not sufficient in itself, and should therefore be coupled with organisational factors such as business strategies. A firm can maximise the value from its IT investments by aligning them with business strategies because IT improves scope economies and coordination. This paper examines empirically the contribution of IT to financial performance as measured by net profit, ROA, and ROE by focusing on the alignment of IT with business strategies such as vertical disintegration and diversification. Empirical analysis shows that IT does not directly improve financial performance. In conjunction with vertical disintegration and diversification, however, it does improve financial performance as measured by net profit. Financial performance ratios such as ROA and ROE, however, are not correlated with the alignment (or interaction) factor of IT with vertical disintegration and diversification. The results indicate that increased IT spending improves net profit, but not performance ratios such as ROA and ROE, of firms with decreased vertical integration and higher diversification.		Namchul Shin	2001	EJIS	10.1057/palgrave.ejis.3000409	economics;engineering;knowledge management;electrical engineering;marketing;management information systems;management science;information technology;information system	ECom	-81.30540068367387	4.526610582117354	77059
38988ed8a760987ae10ec205fb7aa4c4ebb4aa44	evaluating adaptability in frameworks that support morphing collaboration patterns	collaborative information systems;groupware;project management;object oriented methods;information systems;collaborative work;information technology;system adaptability assessment;collaborative system;project lifecycle;collaborative tools;system adaptability assessment project lifecycle collaborative information systems;software development management project management groupware management information systems workflow management software object oriented methods;workflow management software;management information systems;world wide web;humans;computer science;information system;usability;software development management;collaborative software;collaborative work information systems collaborative tools humans collaborative software international collaboration usability computer science information technology world wide web	This paper emphasizes the need for systems to be adaptable due to the manner in which groups collaborate over the lifetime of a project. It is observed that humans tend to adopt patterns of behavior when collaborating and these patterns tend to morph over a project's lifecycle. We identify the factors that cause such change along with the challenges that such evolving patterns pose on existing collaborative information systems. We propose a method to assess the adaptability of systems in ensuring that they can accommodate this change. Adaptability assessment is an essential criterion in the evaluation of a collaborative system.	information system;morphing	Vijayanand Bharadwaj;Y. V. Ramana Reddy;Kankanahalli Srinivas;Sumitra Reddy;Sentil Selliah;Jinqiao Yu	2004	13th IEEE International Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises	10.1109/ENABL.2004.28	project management;adaptability;human–computer interaction;computer science;knowledge management;management;information technology;information system;collaborative software	SE	-74.59419905514312	15.379039835628815	77073
0c34933c89afa2aa834ebc3e15f213d7f03a1d14	database cost-benefit analysis: the case of quality control for a computer manufacturer.	selected works;bepress;quality control;cost benefit analysis			G. K. Webb	2001			quality control;cost–benefit analysis;data mining;operations research	Theory	-66.0748889964612	5.4385153530021855	77092
8a0a8c79e76613c24cc7acc9d963608ee0b0567f	defining requirements for an entrepreneurship marketplace: business and it alignment in practice		e-novate is an IT consulting company based in Geneva, Switzerland. They recently made a radical change to an IT product they were developing. The change compelled e-novate to define a new business model for the product and to align it with the existing IT architecture. Through e-novate’s episodic relationship with a research team, they learned about the SEAM Enterprise Architecture method. Based on a set of research papers downloaded from the web, e-novate’s models were created as prescribed by SEAM for defining early requirements, they were validated by their stakeholders and the required changes were implemented. In this paper we present the project, the reasons for selecting SEAM, the models that were created, the difficulties in creating and applying them, and the lessons learned for both practice and research.	requirement	Lucien Etzlinger;Pierre Castori;Gil Regev;Alain Wegmann	2011		10.1007/978-3-642-23388-3_6	business requirements;business administration;business;management	SE	-67.65709055861512	20.25892231695586	77097
19fe966825fed6a70935b6b9eb06c4f5b62e44a9	codifiability, relationship-specific information technology investment, and optimal contracting	b2b exchange;fundamental role;analytical model;idiosyncratic investment;it investment;transaction codifiability;b2b arena;relationship-specific information technology investment;case study;electronic exchange;underlying factor	The past few years have seen an explosion in the number of e-marketplaces, including a variety of electronic exchanges in the B2B arena, but many of these have also collapsed (e.g., Chemdex/Ventro). The question addressed in this paper is what are the underlying factors that affect which transactions are likely to be supportable by B2B exchanges. In particular, we identify and study three factors: supplier management, idiosyncratic investments in information systems, and codifiability (i.e., digitalizability) of product and order-fulfillment specifications underlying transactions. We show that transaction codifiability plays a fundamental role in influencing the nature of sustainable contracting and IT investments in e-markets. Hypotheses are derived from an analytical model of codifiability in e-marketplaces; these hypotheses are supported by several case studies by the authors and others on the key success factors underlying B2B exchanges.		Moti Levi;Paul Kleindorfer;D. J. Wu	2003	J. of Management Information Systems		economics;marketing;operations management;supplier relationship management;management;information system;commerce	HPC	-81.34766830440721	6.3118499712872635	77197
8880a77bb902d6f8ccaf140b210824b89882eb71	ship routing and scheduling in the new millennium	tramp shipping;inventory management;maritime transportation;routing;liner shipping	We review research on ship routing and scheduling and related problems during the new millennium and provide four basic models in this domain. The volume of research in this area about doubles every decade as does the number of research outlets. We have found over a hundred new refereed papers on this topic during the last decade. Problems of wider scope have been addressed as well as more specialized ones. However, complex critical problems remain wide open and provide challenging opportunities for future research.	routing;scheduling (computing)	Marielle Christiansen;Kjetil Fagerholt;Bjørn Nygreen;David Ronen	2013	European Journal of Operational Research	10.1016/j.ejor.2012.12.002	routing;simulation;computer science;operations management;operations research	Theory	-64.40914957792928	4.648627241359578	77310
446f020ad3ea1f8d9ac83063ea5ec010ce726764	towards introducing agile architecting in large companies: the caffea framework	agile software development;software process improvement;computer and information science;software engineering;architect roles;natural sciences;organizational framework;architect role;agile architecture;organizational frameworks;agile architectures;software design	To continuously deliver value both in short-term and long-term, a key goal for large product lines companies is to combine Agile Software Development with the continuous development and management of software architecture. We have conducted interviews involving several roles at 3 sites from 2 large companies employing Agile. We have identified current architect roles and gaps in the practices employed at the organizations. From such investigation, we have developed an organizational framework, CAFFEA, for Agile architecting, including roles, teams and practices.	agile software development	Antonio Martini;Lars Pareto;Jan Bosch	2015		10.1007/978-3-319-18612-2_20	p-modeling framework;personal software process;agile unified process;extreme programming practices;agile usability engineering;systems engineering;engineering;knowledge management;software development;requirement;software engineering;agile software development;software documentation;empirical process;lean software development;software development process	Vision	-68.37160739385148	21.3499279206397	77406
5a4f5addc78a75b74854793103f2720ca472096f	intranet technology as an enabler of bpr: an exploratory study in public healthcare	exploratory study;public sector;level of service;top down;bottom up;e commerce	The growth and popularity of e-commerce has both challenged and enabled public sector organisations to redefine their levels of service. In the early 1990’s BPR was proposed as a mechanism for change. However, after reports of successive BPR failures the momentum for BPR abated. This paper explores the relationship between Intranet Technology and BPR and, by means of case studies conducted in two organisations in the Irish public sector, investigates the potential of Intranet Technology to become an enabling technology for BPR.. The paper uses the Venkatraman framework for IT enabled change as the framework for the investigation. The paper demonstrates that in accordance with the framework BPR can be approached by top down strategic or bottom up evolutionary methodology. The main finding is that although each organisation took alternative approaches neither actually achieved BPR due to lack of open senior management commitment but that Intranet Technology was considered a key enabler of BPR.	business process;code refactoring;e-commerce;intranet;iterative and incremental development;legacy system;marginal model;top-down and bottom-up design;usability	Martin Hughes;William Golden	2001				HCI	-74.13840818566449	7.95136400646922	77465
19203722f9ecc636882c5cb3b51eac9ab5d12b83	knowledge management in requirement elicitation: situational methods view		Abstract In small-scale software development organizations, software engineers are beginning to realize the significance of adapting software development methods according to project conditions. There is a requirement to proliferate this know-how to other developers, who may be facing the same settings/context, so that they too can benefit from others’ experiences. In this paper, the application of situational method engineering in requirements elicitation phase is investigated. A novel, simple and dynamic web-based tool, Situational Requirement Method System (SRMS), is developed which can aid in conception/formulation, repository, and elicitation/derivation of methods related with this stage. The proposed approach and tool are validated by distributing a questionnaire among software professionals working in large software companies, and making SRMS accessible to them. The results indicate that a majority of the participants finds SRMS useful and provides various suggestions to improve it.	knowledge management	Deepti Mishra;Seçil Aydin;Alok Mishra;Sofiya Ostrovska	2018	Computer Standards & Interfaces	10.1016/j.csi.2017.09.004	requirement;requirements engineering;dynamic web page;computer science;software development;situational ethics;software;requirements elicitation;knowledge management;method engineering	HCI	-73.24001624709761	23.43065412072782	77466
6e059346e6865bbd07726b9bc247decea4f95106	mmpro: a methodology based on iso/iec 15939 to draw up data quality measurement processes	data quality	Nowadays, data plays a key role in organizations, and management of its quality is becoming an essential activity. As part of such required management, organizations need to draw up processes for measuring the data quality (DQ) levels of their organizational units, taking into account the particularities of different scenarios, available resources, and characteristics of the data used in them. Given that there are not many works in the literature related to this objective, this paper proposes a methodology -abbreviated MMPROto develop processes for measuring DQ. MMPRO is based on ISO/IEC 15939. Despite being a standard of quality software, we believe it can be successfully applied in this context because of the similarities between software and data. The proposed methodology consists of four activities: (1) Establish and sustain the DQ measurement commitment, (2) Plan the DQ Measurement Process, (3) Perform the DQ Measurement Process, and (4) Evaluate the DQ Measurement Process. These four activities are divided into tasks. For each task, input and output products are listed, as well as a set of useful techniques and tools, many of them borrowed from the Software Engineering field.	artifact (software development);business process;data quality;iso/iec 42010;input/output;psychometric software;qp state machine frameworks;software developer;software engineering;software measurement;software quality;test automation	Ismael Caballero;Eugenio Verbo;Coral Calero;Mario Piattini	2008			engineering;systems engineering;data quality	SE	-68.77519426370127	19.083747122584967	77491
30cb1ccfb85812026180c8bf214a05c91f0eee8c	cots acquisition process: incorporating business factors into cots vendor evaluation taxonomies	metrics;acquisition process;cots;business factors;process re engineering	Abstract#R##N##R##N#The increasingly prevalent use of Commercial-Off-The-Shelf (COTS) components in software development has attracted a huge capital pool to the industry. The result is an industry that is characterized by strong forces of change and weak resistance. Under such environment, weaker players are constantly displaced by stronger players, and older technologies are constantly displaced by emerging technologies. This phenomenon has brought about a new class of risk, namely, vendor business factors, to the COTS acquisition community. However, the existing COTS vendor evaluation taxonomies remain product centric, focusing only on product and cost-related factors. This article extends the taxonomies to incorporate Vendor Business Factors into COTS selection process. The resulting model is named VERPRO (Vendor Economics and Risk Profiler). The foundation of VERPRO lies within a measurement-based vendor evaluation taxonomy that categorizes the evaluation criteria into four main factors: product, cost, service, and business. VERPRO enables the acquisition community to incorporate business factors into vendor selection process and provides a revolutionary approach that merges the knowledge of financial analysis into software engineering. Copyright © 2006 John Wiley & Sons, Ltd.	taxonomy (general)	James Miller;Hean Chin Yeoh	2006	Software Process: Improvement and Practice	10.1002/spip.297	computer science;systems engineering;engineering;marketing;operations management;software engineering;management;metrics	HCI	-80.26236314872948	7.4934152317750895	77525
a3898f26f47bf99de495cf584c2b0a12a841d93f	collaborative modeling techniques to facilitate communication among end-users and analysts	business process redesign;modeling technique;system modeling;user participation;system design;natural language;field study;information system development	Activity-based system modeling is a part of many information system development methodologies and business process redesign projects. End-Users may become effective participants in a group modeling process under the leadership of specialists (i.e., meeting facilitators, systems analysts, technical consultants). The analysts typically have the responsibility to synthesize a model that combines the functional expertise of all of the system stakeholders, Natural language descriptions are difficult for analysts to understand and the abstract modeling notation used for systems design is difficult for end-users to understand. The end-users usually have difficulty communicating with each other and reaching consensus on system requirements. Field studies have demonstrated that many of the problems associated with participative modeling can be alleviated with customized software. A group tool for activity-based (IDEFO) modeling incorporates features that specifically address problems identified during field studies conducted with previous prototypes. The specialists are freed from data entry responsibilities and can concentrate on coaching the end-user participants on the modeling process. Analysts can identify inconsistencies and violations of modeling rules dynamically, minimizing the participants’ need for formal advanced training in the methodology. Ideally, the analysts can supply the abstraction needed for technical methodological requirements (i.e., level balancing of constraints) without threatening the end-user’s sense of ownership of the model.	business process;consensus (computer science);information system;natural language;requirement;software development process;system requirements;systems design;systems modeling	Robert M. Daniels;Glenda S. Hayes	1995		10.1145/212490.212585	user modeling;human–computer interaction;systems engineering;engineering;knowledge management;service-oriented modeling;process modeling;modeling language;business process modeling	SE	-63.10251114027099	15.319067210111665	77538
731688c8dd7ddc02cc9e59378069704b446da1f9	long-term effects of test-driven development a case study	long term effect;team production;test driven development;software maintenance;continuous improvement;software quality	Test-Driven Development (TDD) is one of the most widely debated agile practices. There are a number of claims about its effect  on the software quality and team productivity. The current studies present contradicting results and very little research  has been performed with industrial projects, which have used TDD over an extensive period of time. This paper is reporting  the long-term effects on a three year-long application of TDD in a Nokia Siemens Networks team. We present qualitative findings  based on interviews with the team members. We conclude that TDD has been found to improve the team confidence in the code  quality and simplify significantly the software maintenance. The examined team did not notice any significant negative effects  over the long-term TDD application and is eager to continue improving the practice application. The authors suggest that results  bear direct relevance to the industry and academia. Further research avenues are indicated.  	test-driven development	Artem Marchenko;Pekka Abrahamsson;Tuomas Ihme	2009		10.1007/978-3-642-01853-4_4	reliability engineering;systems engineering;engineering;operations management	HCI	-69.79932891822496	23.393200327717942	77559
5d33db529e8b99d198b3a415803d15e29d068958	developing is enabled capabilities for a bpo vendor: a case study	is enabled capabilities;business process outsourcing;sense making;information system;enterprise architecture;service provision;business process	The outsourcing literature has offered a plethora of perspectives and models for understanding decision determinants and outcomes of outsourcing of business processes. While past studies have contributed significantly to scholarly research in this area, there are an insufficient number of studies that explore how information systems can be used to facilitate service provisioning. Consequently, there is a need to understand how vendors develop IS enabled capabilities that allow them to address a core challenge: to achieve scalable growth by developing standardized offerings that can be sufficiently customized to meet the unique demands of individual customers. This in-depth case study leverages an organizational sensemaking framework to explore IS enabled capability development in one of the largest business process outsourcing (BPO) vendors in the world.	americas conference on information systems;business process;embedded system;information system;outsourcing;provisioning;scalability;sensemaking;social system;socio-cognitive	Mark O. Lewis;Lars Mathiassen;Arun Rai	2009			knowledge management;business administration;process modeling;process management;business;business process modeling;business architecture	Metrics	-77.30381934214168	6.000985673910046	77563
0cfbd123eb52a29f0fda41c240d49f68ce73f971	ein betriebstypologisches verfahren zur segmentierung des anwendungssoftware-marktes		Every manufacturer of software has to make a compromise between the production of cheap standard and expensive specialized software. One way to tackle this problem is the use of a typification scheme for industry specific modular programs. The classification of firms as a means of market segmentation thus combines a customer orientation for the software development with the distribution of product development and maintenance costs on a higher multitude of buyers		Katja Steinbeißer;Uwe Dräger	1990	Inform., Forsch. Entwickl.		software development;operations management;market segmentation;software engineering;new product development;software;software portability;typification;modular design;operations research;computer science;compromise	Crypto	-91.9653336250758	26.81823236648671	77652
e464e78e64edc7dfd1d169162d8c1d1cca879840	understanding modelling practices in manufacturing	enterprise modelling;ethnography	This paper is about describing and analysing modelling practices in automotive industry and electronics design. Based on ethnographic observations in modelling sessions in three case studies, we tried to describe the modelling scope in real work environments. This helped to identify problem areas or areas of change, articulation work in decision making activities during modelling, models as shared objects and issues of accessing the models by different communities of practice. The focus of our investigations was on a specific modelling environment called Active Knowledge Modelling (AKM).	biconnected component;electronic design automation	Hilda Tellioglu;Gian Marco Campagnolo	2008			enterprise modelling;management science;business;automotive industry;systems engineering	AI	-65.12021521001304	15.112527156842543	77724
c88fd23c51f18c264b98a87b424101f7fdf04fc6	study on core capabilities of tourism networks	core capabilities;tourism networks;tourism industry tourism business networks value system continuum hypothesis global competition tourism network management network brand identity relational value production cooperative activities in tourism business networks tourism network core capabilities;groupware;marketing management environmental economics conference management environmental management technology management electronic mail industrial economics identity management systems knowledge management production;value system;global competition;tourism network core capabilities;industries;set theory;data mining;network brand identity;travel industry groupware management set theory;value system continuum hypothesis;tourism network management;business;relational value production;network core capabilities tourism networks;production;cooperative activities in tourism business networks;business networks;management;context;tourism industry;network;value added;asia;tourism business networks;travel industry;knowledge engineering	Tourism business networks are becoming more and more popular in the development of tourism industry. This article analyses the coordination of cooperative activities in tourism business networks by challenging the sustainability of the ‘manipulating’ demand approach in favor of the Value System Continuum in them. It is hypothesized that local tourism businesses must develop new key capabilities in order to face future global competition. The paper focuses on five key capabilities required to manage tourism business networks. The author put forward that capabilities need in tourism business network managing includes: the coordination of cooperation in tourism business networks, value adding relational value production, building the network brand identity, accumulating necessary knowledge and skills, relative independence and multirole of network node, etc.	apache continuum	Xingqi Wen	2009	2009 International Conference on Environmental Science and Information Application Technology	10.1109/ESIAT.2009.257	computer science;marketing;knowledge engineering;tourism;commerce	DB	-79.4812752083791	4.636792614062151	77790
235d34c528078bff9565ecca6616b2c730107d8e	choosing a test modeling language: a survey	language use;automatic testing;best practice;system under test;modeling language;model based testing;domain specificity	Deployment of model-based testing involves many difficulties that have slowed down its industrial adoption. The leap from traditional scripted testing to model-based testing seems as hard as moving from manual to automatic test execution. Two key factors in the deployment are the language used to define the test models, and the language used for defining the test objectives. Based on our experience, we survey the different types of languages and sketch solutions based on different approaches, considering the testing organization, the system under test, etc. The types of languages we cover include among others domain-specific, test-specific as well as generic design languages. We note that there are no best practices, but provide general guidelines for various cases.	best practice;formal language;model-based testing;modeling language;software deployment;software testing;system under test;test script	Alan Hartman;Mika Katara;Sergey Olvovsky	2006		10.1007/978-3-540-70889-6_16	test strategy;natural language processing;keyword-driven testing;exploratory testing;model-based testing;simulation;white-box testing;manual testing;computer science;functional testing;test suite;session-based testing;risk-based testing;system under test;modeling language;programming language;test management approach;best practice	SE	-64.91626977664843	27.55499823787839	78016
d73a88b57001cc288c2b842985a68e779d674944	software development education based on uec software repository		Most of the software developed in universities is primarily used by the developers themselves. Normally, most of this software is managed and stored on servers in the research laboratories, but since the software is generally lacking in documentation and is not developed with third-party use in mind, it tends to be used only by the original developers. It is seldom reused after the developers have graduated, and is often not in a fit state for use by third parties. Today’s information systems graduates have not been provided with sufficient education with regard to the knowledge, techniques and experience needed for the usual software development process of actual software development businesses from project planning through to execution and management (requirements analysis, development, implementation, testing, etc.) and lack the basic skills for handling actual business situations. In this paper, we report on our approach to software management using the UEC software repository to consolidate software assets, and on practical software development education based on this repository.	software development;software repository;ubuntu	Takaaki Goto;Takahiro Homma;Kensei Tsuchida;Tetsuro Nishino	2012		10.1007/978-3-642-28670-4_5	systems engineering;software engineering;computer engineering	SE	-67.84246998383594	25.102243868093833	78021
5a1c3c08300b6eff03f28a052e7a46d92ec25456	design with uncertainty: the role of future options for infrastructure integration		Decision making for effective infrastructure integration is challenging because the performance of long-lasting facilities is often difficult to foresee or well beyond the designer’s control. We propose a new approach for integrating the construction/retrofitting of two or more types of facilities. Infrastructure integration has many perceived benefits, but practitioners also express serious doubts, particularly when it comes to civil engineering works. To substantiate this approach, we test all of the major options for integrating a ground source heat pump system with the construction/retrofitting of an archetypal office building. We use actual data from the United Kingdom, which represent a middle-of-the-road setting among major developed countries. The model highlights the sensitivity of the range of cost-effective solutions to the embedding of future options. The findings point to a clear need for appropriate standards for managing infrastructure integration. We expect this kind of model to find increasing applications among infrastructure complexes, particularly as cities become denser and more		Claudio Martani;Ying Jin;Kenichi Soga;Stefan Scholtes	2016	Comp.-Aided Civil and Infrastruct. Engineering	10.1111/mice.12214	simulation;systems engineering;engineering;electrical engineering;civil engineering;management	DB	-63.34891818368909	8.524891640957108	78029
59ef43ecfd54cb342f8ca09bdb79f3aa7cd91059	it innovation strategy: managing the implementation communication and its generated knowledge through the use of an ict tool		Purpose: Without effective implementation, no IT strategy can succeed. There has been much research into IT planning, but few studies have developed one of the most important phases of IT strategy: IT Implementation. IT implementation can be improved at Information and Communication Technology (ICT) Organizations through the use of organization learning models (OLM) and the implementation of ICT tools. This paper has two purposes 1) Define an OLM framework that determines the best practices to increase knowledge at individual, group and/organizational levels, and 2) Define and implement an ICT Tool to facilitate the integration and institutionalization of the OLM. The ICT tool is based on the Technology Roadmapping technique that allows an organization to manage at an executive level what, when and how the IT strategy is going to be implemented. Design/methodology/approach: This paper is based on a case study performed at an ICT Organization that provides ICT services to financial institutions. The study was carried out in 2014. It analyzed over 24,000 projects, which translated into an equivalent of more than 18 million man-hours. The proposal was assessed at a very large ICT Organization. Findings/Originality/Value: This paper proposes a framework called SPIDER to implement effectively organizational learning models based on big data management principles for monitoring and reporting current status of IT Innovation strategies. This kind of approaches contribute to solve the problems identified in the state of the art regarding the communication and monitoring the implementation status of IT innovation strategies. During this research work several factors that are essential to implement this kind of approaches in large banking organizations were identified. These factors include: i) Effort required to elaborate the monitoring and reporting activities; ii) Easiness to understand the reported information; iii) Detailed planning of the implementation program; and iv) Focus on communication efficiency.	best practice;big data	José Francisco Landaeta Olivo;Javier García Guzmán;Ricardo Colomo Palacios;Vladimir Stantchev	2016	J. Knowledge Management	10.1108/JKM-06-2015-0217	knowledge management;management science;management	HCI	-72.01982674257826	10.630552292595793	78037
a25eae1fe583e2cd117cf062e682db7d538dcc14	a method to assess expected net benefits of edi investments	benefits of information technology;cost benefit analysis;electronic data interchange	In this paper we analyze how small and medium-sized enterprises (SMEs) operating in the Port of Rotterdam can be supported in the assessment of electronic data interchange (EDI) investments in terms of expected net benefits. In contrast to many other industry sectors, where a dominant hub actor is able to enforce wide-scale adoption of EDI by spoke SMEs, port communities need more ex-ante insights into the benefits that may accrue from EDI before they decide to allocate part of their scarce monetary resources to investments in this technology. There is a need for a cheap method of quantifying bottom-line savings from EDI in a traceable way.A literature survey shows that none of the existing IT evaluation methods satisfies these criteria. Therefore, we developed a new tool, called Edialysis, which aids in the decision-making process in terms of whether investments in EDI may yield a positive net present value, given a context-specific set of interorganizational communication scenarios.This paper discusses the method's scope, design, and underlying plan. In addition, a case study conducted at a forwarder operating in the Port of Rotterdam illustrates the results one may obtain from the Edialysis application.	electronic data interchange	Martijn R. Hoogeweegen;René W. Wagenaar	1996	Int. J. Electronic Commerce	10.1080/10864415.1996.11518277	economics;computer science;cost–benefit analysis;marketing;operations management;electronic data interchange;world wide web;commerce	ECom	-79.18417829883374	10.037844553787897	78081
fce3abc73996a56516d28e11376ca794cc6581db	an innovative model for optimizing software risk mitigation plan: a case study	optimum decisions;risk management;software engineering;dangerous risks;risk mitigation decisions;active software project innovative model software risk management process software risk mitigation plan circumferential opportunities;circumfrential risks;maintenance engineering data security software risk management companies programming customer relationship management;opportunities;optimum decisions risk mitigation decisions dangerous risks circumfrential risks opportunities;software engineering risk management	Software risk management process like any other process faces some threats. Ignoring such threats may influence the efficiency of software systems. The risk mitigation process would not be proper if the unseen risks neglected. The model in this research presents the best combination of solutions for integration of risk mitigation plan. The proposed model facilitates the identification of circumferential opportunities and risks in mitigation process. In addition, hidden competencies will be identified and the effectiveness of risk mitigation decisions will be optimized. By applying the method on the active software project, the circumferential risks and opportunities were detected and prioritized based on the obtained values. The case study verifies the model in practice and shows its applicability.	optimizing compiler;risk management;software project management;software system;threat (computer)	Ahdieh Sadat Khatavakhotan;Siew Hock Ow	2012	2012 Sixth Asia Modelling Symposium	10.1109/AMS.2012.55	reliability engineering;risk analysis;software engineering process group;it risk management;risk management;systems engineering;engineering;risk analysis;risk management plan	SE	-70.25060023603795	26.011113457924814	78149
20c660e1903b193d435f8f22272ee92c10bc3b85	using early quality assurance metrics to focus testing activities		Testing of software or software-based systems and services is considered as one of the most effort-consuming activities in the lifecycle. This applies especially to those domains where highly iterative development and continuous integration cannot be applied. Several approaches have been proposed to use measurement as a means to improve test effectiveness and efficiency. Most of them rely on using product data, historical data, or in-process data that is not related to quality assurance activities. Very few approaches use data from early quality assurance activities such as inspection data in order to focus testing activities and thereby reduce test effort. This article gives an overview of potential benefits of using data from early defect detection activities, potentially in addition to other data, in order to focus testing activities. In addition, the article sketches an integrated inspection and testing process and its evaluation in the context of two case studies. Taking the study limitations into account, the results show an overall reduction of testing effort by up to 34%, which mirrors an efficiency improvement of up to about 50% for testing.	algorithm;continuous integration;focus group;iterative and incremental development;iterative method;software bug;test effort	Frank Elberzhager;Jürgen Münch	2013	CoRR		test strategy;reliability engineering;development testing;black-box testing;test data generation;software performance testing;white-box testing;system integration testing;systems engineering;engineering;software reliability testing;functional testing;data mining;operational acceptance testing;software testing;system testing;software quality analyst	SE	-64.08042687787774	31.24953296961208	78169
e9676888adb21fd336685be4412bcec3eea57d7e	collaboration and multitasking in networks: prioritization and achievable capacity	collaboration;teams;stability;resource sharing;multitasking;control;architectures;productivity;capacity;priority	Motivated by the trend toward more collaboration in workflows, we study networks where some tasks require the simultaneous processing by multiple types of multitasking human or indivisible resources. The capacity of such networks is generally smaller than the bottleneck capacity. In Gurvich and Van Mieghem [Gurvich I, Van Mieghem JA (GVM1) Collaboration and multitasking in networks: Architectures, bottlenecks, and capacity. Manufacturing Service Oper. Management 17(1):16–33], we proved that both capacities are equal in networks with a hierarchical collaboration architecture, which define a collaboration level for each task depending on how many types of resources it requires. This paper studies how task prioritization impacts the achievable capacity of such hierarchical networks using a conceptual queueing framework that formalizes coordination and switching idleness. To maximize the capacity of a collaborative network, highest priority must be given to the tasks that require the most collaboration. Other...	computer multitasking	Itay Gurvich;Jan A. Van Mieghem	2018	Management Science	10.1287/mnsc.2017.2722	shared resource;productivity;simulation;stability;economics;human multitasking;operations management;distributed computing;scientific control;statistics;collaboration	Theory	-82.14579618761884	11.897924941505527	78206
3a4f99dab6efd643bbd5153ae8a94d0e71ad8cbc	how to make process model matching work better? an analysis of current similarity measures		Process model matching techniques aim at automatically identifying activity correspondences between two process models that represent the same or similar behavior. By doing so, they provide essential input for many advanced process model analysis techniques such as process model search. Despite their importance, the performance of process model matching techniques is not yet convincing and several attempts to improve the performance have not been successful. This raises the question of whether it is really not possible to further improve the performance of process model matching techniques. In this paper, we aim to answer this question by conducting two consecutive analyses. First, we review existing process model matching techniques and give an overview of the specific technologies they use to identify similar activities. Second, we analyze the correspondences of the Process Model Matching Contest 2015 and reflect on the suitability of the identified technologies to identify the missing correspondences. As a result of these analyses, we present a list of three specific recommendations to improve the performance of process model matching techniques in the future.	pattern matching;process modeling	Fakhra Jabeen;Henrik Leopold;Hajo A. Reijers	2017		10.1007/978-3-319-59336-4_13	knowledge management;computer science;performance improvement;data mining;contest;process modeling	SE	-71.92551543471353	23.139870702353175	78254
726e0ad75ac824c1dc679bdb2224f78e599f9840	standardizing domain-specific components: a case study	domain specificity	m Thera S.p.A. is a software house that produces finished and semi-finished software and provides smaller software organizations in Northern Italy with base application components on which they can build and specialize new products. Thera senior management is strongly committed to improving software development, in pursuit of business objectives. A key factor for improving software production is the introduction of standard domain analysis methods. Its success will be a cornerstone in the global development process in which all the software production will be redesigned and standardized on the basis of standard domain analysis and on the software reuse experience gained from it. hera S.p.A. is a software house, one of whose main business goals is to develop software products for the rational management of firms and their resources by evolving software systems that introduce well-defined, manageable, and flexible solutions for business, management, and production problems. Thera’s target customers are mainly manufacturers (of production management systems), insurance companies (actuarial management systems) and commercial organizations (accounting and commercial systems). In addition, Thera develops “ad hoc” products for clients with specific needs. It also produces semi-finished software products, acting as a supplier to smaller software organizations in Northern Italy by providing them with application base components. Even though Thera’s products already enjoy commercial success, senior management is strongly committed to improving the development process in order to pursue the following business objectives:	code reuse;component-based software engineering;domain analysis;domain-specific language;hoc (programming language);semiconductor industry;software development;software house;software system	Massimo Fenaroli;Andrea Valerio	1997	ACM StandardView	10.1145/260558.260563	computer science	SE	-66.54679354251952	20.87016256898736	78270
208cd239e2f68e0ac8b1116fd4a64d91b5a1fe22	a research of value-net based business model and operating of m-commerce	system modeling;e commerce;business model;mobile commerce;value chain;virtual environment;organizational structure	Recent advances in Internet and e-commerce have led industry to reengineer their organizational structures and value chains. One of the most prominent trends in this change is to conduct work and operation in the distributed or virtual environments. A new kind of business model, termed as value-net, is emerging. In this paper, the business and system mode from the value-net system is studied and proposed. The feature of this business model suitable for Mobile Commerce (M-Commerce) is analyzed on the basis of value-net. The ecosystems (ecology system) model based on value-net is presented for the M-Commerce with systematic structure, collaborate mode and working mechanism. A modified value-net based ecosystem for MCommerce operators is proposed in this paper with practical case analysis.	e-commerce;ecology;ecosystem;internet;mobile commerce;virtual reality	Yingliang Wu;Chin E. Lin;Haosu Wu	2007		10.1007/978-0-387-75466-6_65	artifact-centric business process model;process modeling;electronic business;process management;business;mobile business development;new business development;business process modeling;commerce;consumer-to-business	Web+IR	-75.2630166899854	6.008604727126117	78298
3144a1e5e6cdd9957cc59617bdbd6efc58e15ab9	management issues in cooperative computing	costs and benefits;distributed system management;computer network;numerical computation;resource sharing	There is a growing t rend toward cooperative comput ing a r r angemen t s tha t span several independen t organizations. P rominen t among these are the numerous compu te r ne tworks set up to permi t inter-organizational shar ing of compute r -based resources Cooperat ive a r r angemen t s appear to share m a n y c o m m o n problems in their development , financing, and managemen t . These problems are revmwed, and the cons ldera tmns involved m dealing with t h e m discussed. It is shown tha t the problem areas described have a fundamen ta l influence on the success of any cooperative comput ing venture , and m u s t be taken into account in assessing the potential costs and benefits associated with such a venture , as well as in its subsequen t managemen t . The re is an extensive set of references to the li terature.		Dan Bernard	1979	ACM Comput. Surv.	10.1145/356757.356759	shared resource;element management system;computer science;knowledge management;cost–benefit analysis;resource management;distributed computing;management science;structure of management information	Theory	-82.04513443586337	11.615009575804631	78315
bec7f9de0ec5bef2ccd1d4f518e0152fa80b9ed5	kdd meets big data	standards organizations;data science;data mining;big data;capability maturity model;organizations;data models	Cross-Industry Standard process model (CRISP-DM) was developed in the late 90s by a consortium of industry participants to facilitate the end-to-end data mining process for Knowledge Discovery in Databases (KDD). While there have been efforts to better integrate with management and software development practices, there are no extensions to handle the new activities involved in using big data technologies. Data Science Edge (DSE) is an enhanced process model to accommodate big data technologies and data science activities. In recognition of the changes, the author promotes the use of a new term, Knowledge Discovery in Data Science (KDDS) as a call for the community to develop a new industry standard data science process model.	big data;data mining;data science;database;end-to-end encryption;process modeling;software development;technical standard	Nancy W. Grady	2016	2016 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2016.7840770	data modeling;data governance;engineering;data science;data mining;database	DB	-69.75294130575836	13.432642524202423	78481
8c482425dadbf1173608f75573253eb192549867	a reliability assessment guide for the transition planning to lead-free electronics for companies whose products are rohs exempted or excluded	tecnologia electronica telecomunicaciones;rohs excluded;lead free;rohs exempt;supply chain;tecnologias;solder;grupo a	Article history: Received 21 October 2015 Received in revised form 17 February 2016 Accepted 14 March 2016 Available online xxxx While amajority of electronicmanufacturers have transitioned to lead-freematerials and processes, both to comply with government legislation and to be compatible with the evolving supply-chain infrastructure, there are still many electronic manufacturers who are not using lead-free technologies, because they produce products that are either currently exempted or excluded from the government-imposed restrictions. Nevertheless, at some time, these manufacturers will need to have a lead-free transition plan, which includes compliance identification of the materials used, supplier compliance, process updates, and reliability assessment and qualification to the targeted application. This paper first briefly overviews the state of the knowledge as it relates to the reliability of lead-free solders. Then the paper provides a guide to help the planning team to assess the reliability issues needed for scheduling and resource identification to ensure a cost-effective and timely transition to leadfree products. The focus is on a risk matrix developed to help companies determine the general category of risk that they will encounter if they are currently exempt or excluded from making lead-free electronics. © 2016 Elsevier Ltd. All rights reserved.	change control;enterprise life cycle;microsoft outlook for mac;plan 9 from bell labs;requirement;scheduling (computing);traceability	Michael G. Pecht;Tadahiro Shibutani;Lifeng Wu	2016	Microelectronics Reliability	10.1016/j.microrel.2016.03.020	reliability engineering;engineering;electrical engineering;nanotechnology;supply chain;soldering;operations research	AI	-63.23633094902296	9.757560231619923	78533
362eebe373ec8aeb5d4249678382c0716cce746c	using actor object operations structures to understand project requirements complexities	analytical models;software;project requirements complexity;casse requirements engineering project management software modeling;project management;formal specification;complexity theory;casse;software modeling;requirements management;project manager;dp industry;software management;complex adaptive system software engineering;complex adaptive system;development process;software engineering;requirements engineering;requirements volatility;engines;systems analysis;requirement engineering;unified modeling language;actor object operation structures;software industry;object oriented modeling project management job shop scheduling costs computer science cities and towns africa computer industry optimal scheduling outsourcing;system development;use case;object oriented modeling;systems development;project management project requirements complexity software industry requirements volatility requirements management complex adaptive system software engineering systems development actor object operation structures;systems analysis dp industry formal specification project management software management	Some of the major challenges still facing the software industry are use of inappropriate methodology, lack of customer involvement, project complexity, and requirements volatility and management. In this work we propose the use of the complex adaptive system software engineering framework (CASSE) that analyzes value-related aspects of systems development - addressing satisfaction of all project requirements as a measure for project success. We specifically look at actor object operation structures (AOOS) and how they impact on the complexity of project requirements. We demonstrate our approach by analyzing use-case artifact properties and how a change in any properties can impact on project management during the development process. Our results show that the hidden interrelationships enhance implementation prioritization and schedule optimization.Our major contribution is in demonstrating that alternative engineering approaches that analyze AOOSs can enhance the way we understand project complexities, increase customer commitment and generate project value and success.	complex adaptive system;genetic algorithm scheduling;mathematical optimization;requirement;software development process;software engineering;software industry;volatility	Joseph Kibombo Balikuddembe;Anet Potgieter	2008	2008 The Third International Conference on Software Engineering Advances	10.1109/ICSEA.2008.83	level of effort;use case;project management;complex adaptive system;reliability engineering;unified modeling language;systems analysis;requirements management;extreme project management;work breakdown structure;software project management;computer science;systems engineering;engineering;software engineering;formal specification;requirements engineering;modeling language;project management 2.0;project management triangle;software development process;project portfolio management	SE	-66.30636334448374	19.780390525607455	78563
38f2ac3666ffc8cbb1b9aabe39ec7ce4b0b0c352	smart business networks: how the network wins	gestion integrada;gestion integree;gestion entreprise;processus metier;firm management;integrated management;calculateur processus;proceso oficio;administracion empresa;calculador proceso;business networks;process computer;business process	Realizing scenarios in which business is conducted through a rapidly formed network with anyone, anywhere, anytime regardless of different computer systems and business processes.	anytime algorithm;business process	Eric van Heck;Peter H. M. Vervest	2007	Commun. ACM	10.1145/1247001.1247002	business process;management	Graphics	-69.05536281816397	6.738498774422743	78584
2609ae36b623ac46ce2eee8b1c5fdef7fe66cd59	requirements engineering for cross-organizational erp implementation undocumented assumptions and potential mismatches	coordination mechanisms;conceptual framework;enterprise resource planning business communication companies human resource management computer science application software software libraries materials requirements planning software packages authorization;erp system;systems analysis enterprise resource planning;systems analysis;requirement engineering;enterprise resource planning;erp implementation;literature survey;cross organizational erp implementation undocumented assumptions requirements engineering enterprise resource planning business coordination requirements analysis;empirical research	A key issue in requirements engineering (RE) for enterprise resource planning (ERP) in a cross-organizational context is how to find a match between the ERP application modules and requirements for business coordination. This paper proposes a conceptual framework for analyzing coordination requirements in inter-organizational ERP projects from a coordination theory perspective. It considers the undocumented assumptions for coordination that may have significant implications for ERP adopting organizations. In addition, we build a library of existing coordination mechanisms supported by modern ERP systems, and use it to make a proposal for how to improve the match between ERP implementations and supported business coordination processes. We discuss the implications of our framework for practicing requirements engineers. Our framework and library are based on a literature survey and the experience with ERP implementation of one of us (Daneva). We further validate and refine our framework.	erp;enterprise resource planning;requirement;requirements engineering;undocumented feature	Maya Daneva;Roel Wieringa	2005	13th IEEE International Conference on Requirements Engineering (RE'05)	10.1109/RE.2005.59	systems analysis;systems engineering;engineering;knowledge management;software engineering;conceptual framework;requirements engineering;empirical research	SE	-72.61541129608516	15.706420886520885	78626
d8f6d4d56340e3eb84459a57b6b5ed0e4eacb4ad	organizations non gratae? the impact of unethical corporate acts on interorganizational networks	networks;social responsibility;illegal activities;business administration;unethical acts;legitimacy;network structure	In this study, we examine the effects of illegal/unethical acts on interfirm networks. We hypothesize that the quality of network partners will decline and overall network structure will change after a firm commits an unethical act. More specifically, we posit that the decline in partner quality is primarily driven by higher quality firms leaving the network, and the focal firm being forced to replace them with lower quality ones. We also propose that partner prominence and network cohesion will be affected after these acts, and that the changes in partner quality and network structure will be greater for those acts perceived as more illegitimate. We test these hypotheses using a sample of 200 large firms in the United States and data on unethical acts by these firms. Our results show that the quality of a firm's network partners declines after the firm's commission of an unethical act, and that the deterioration in partner quality tends to be greater for acts of greater illegitimacy. Our results also show declines in network prominence and cohesion for firms involved in these activities. We discuss the implications of our results for the literatures on interfirm networks and unethical corporate activities.	semantic network	Bilian Ni Sullivan;Pamela R. Haunschild;Karen Page	2007	Organization Science	10.1287/orsc.1060.0229	public relations;economics;social responsibility;sociology;legitimacy;management;social psychology;law	NLP	-84.65451373856459	4.632748310445762	78647
73338ade489e336df422755a45f8df84dbaee1de	exploring students' happiness in distributed software projects	distributed software development;polls;global software engineering	Teaching realistic Global Software Engineering in a university setting requires exposing students to intricacies of the approach by immersing them into simulated or real distributed projects. However, such an approach requires careful student and course management by the university staff, additionally hampered by reduced visibility of distributed teams. In this paper we describe the concept of happiness polls aimed at gaining better insight into dynamics of distributed student projects, and some preliminary results identifying the key process properties influencing students perception of development process.	software engineering	Igor Cavrak;Ivana Bosnic;Mario Zagar	2015		10.1145/2797433.2797470	team software process;simulation;computer science;systems engineering;engineering;knowledge management;software engineering	SE	-72.36970308220342	22.303535750583627	78768
ad1b94b2330f5cb8cd3c3def40b6afc1dec6778e	practice prize paper - applying a dynamic model of consumer choice to guide brand development at jetstar airways	bayesian estimation;consumer choice;service management;journal article;marketing strategy;airlines;keywords airlines	This paper describes the use of a marketing science model by Jetstar, a subsidiary of Australia's leading airline, Qantas, to effectively and profitably compete in the low-cost carrier marketplace. We trace the evolution of the Jetstar strategy from a baseline calibration of its initial position, to its efforts to attain price competitiveness and service parity, followed by its highly focused, cost-effective service delivery strategy. We develop a hierarchical model with parameters estimated at the individual level. This allows us to study not only how service design and pricing initiatives shift the perceived performance of Jetstar relative to its competitors but also how the airline can move market preferences toward areas in which it has competitive advantage. The contribution of the research is substantial. The Jetstar market share went from 14.0% to 18.1% during the first five quarterly waves of the research, and profits went from $79 million in 2006--2007, before the study was commissioned, to $124 million in 2008--2009.	choice modelling;feedback	Peter J. Danaher;John H. Roberts;Ken Roberts;Alan Simpson	2011	Marketing Science	10.1287/mksc.1100.0619	bayes estimator;economics;service management;marketing;operations management;marketing strategy;advertising	Logic	-83.4259849787856	7.052322321044001	79186
c02f2d4bc7955eb43d7de213d42f5e3b0204f052	comparing value-driven methods: an experiment design		A business model is a representation of an organization with a particular point of view. It is common to find different types of models to describe the business. However, methods to create business models representing an economic point of view have only emerged over the last few years in the scientific community. Such methods aid business specialists improving the economics understanding of the business, helping both defining more efficient business strategies and better aligning the information technology systems with the business. This paper aims at describing the design of an experiment to compare two methods to specify economic values (evalue and value-driven development). Our experiment design allows predicting the acceptance of a particular method in practice, based on the effort of applying the method, the quality of the artifacts produced, and the user perceptions with regard to the quality of the method.	design of experiments	Eric Souza;Silvia Mara Abrahão;Ana Moreira;João Araújo;Emilio Insfrán	2016			design of experiments;simulation;computer science	HCI	-77.17742711849509	8.213454053749965	79252
ccc4f2cce64fabdddf4ed7f6fefa16de248e0ede	the impact of tailoring criteria on agile practices adoption: a survey with novice agile practitioners in brazil		Abstract The software development industry adopts agile methods in different ways by considering contextual requirements. To fulfill organizational needs, adoption strategies consider agile methods tailoring. However, tailoring according to the context of the organization remains a problem to be solved. Literature on criteria for adopting software development methods exists, but not specifically for agile methods. Given this scenario, the following research question arises: what is the impact of software method tailoring criteria on the adoption of agile practices? To answer this question, we conducted a survey among agile practitioners in Brazil to gather data about importance of the tailoring criteria and agile practices adopted. A model for agile practices adoption based on the tailoring criteria is proposed using the results of the survey with a majority of novice agile practitioners. The proposed model was validated using PLS-SEM (partial least squares structural equation modeling) and the survey data. Results show that adoption of agile practices was influenced by criteria such as external environment, previous knowledge and internal environment. Results also indicate that organizations tend to use hybrid/custom software methods and select agile practices according to their needs.	agile software development	Amadeu Silveira Campanelli;Ronaldo Darwich Camilo;Fernando Silva Parreiras	2018	Journal of Systems and Software	10.1016/j.jss.2017.12.012	systems engineering;software;computer science;survey data collection;software development;agile software development;research question;custom software	Graphics	-70.87140755835216	21.244826319860337	79338
1d1733eb5c89bfe5fd6d4ff0bfab5e55e1548575	openness and requirements: opportunities and tradeoffs in software ecosystems	mixed method requirements engineering software ecosystem;team working ecology formal specification industrial property project management software development management;requirements engineering;mixed method;end users context continuous evolution systems interdependence innovation power consumer oriented software services project management reputation management requirements engineering mixed method study ibm clm ecosystem open commercial development model ecosystem actors participatory observation software repositories product requirements information flow just in time re software ecosystems openness intellectual property product teams;software ecosystem;ecosystems software context interviews automotive engineering organizations collaboration	A growing number of software systems is characterized by continuous evolution as well as by significant interdependence with other systems (e.g. services, apps). Such software ecosystems promise increased innovation power and support for consumer oriented software services at scale, and are characterized by a certain openness of their information flows. While such openness supports project and reputation management, it also brings some challenges to Requirements Engineering (RE) within the ecosystem. We report from a mixed-method study of IBM®'s CLM® ecosystem that uses an open commercial development model. We analyzed data from from interviews within several ecosystem actors, participatory observation, and software repositories, to describe the flow of product requirements information through the ecosystem, how the open communication paradigm in software ecosystems provides opportunities for `just-in-time' RE, as well as some of the challenges faced when traditional requirements engineering approaches are applied within such an ecosystem. More importantly, we discuss two tradeoffs brought about the openness in software ecosystems: i) allowing open, transparent communication while keeping intellectual property confidential within the ecosystem, and ii) having the ability to act globally on a long-term strategy while empowering product teams to act locally to answer end-users' context specific needs in a timely manner.	bottom-up proteomics;channel length modulation;confidentiality;continuous delivery;ibm research;interdependence;openness;programming paradigm;reputation management;requirement;requirements engineering;software deployment;software ecosystem;software engineering;software repository;software system;victoria (3d figure)	Eric Knauss;Daniela E. Damian;Alessia Knauss;Arber Borici	2014	2014 IEEE 22nd International Requirements Engineering Conference (RE)	10.1109/RE.2014.6912263	requirements analysis;personal software process;software engineering process group;economics;software project management;systems engineering;engineering;knowledge management;environmental resource management;social software engineering;software development;requirement;software engineering;software as a service;requirements engineering;management;software deployment;software development process;software requirements	SE	-67.15517775143582	21.94349573870987	79348
b8569dece74d31bacf055cb31d6589188f4b357a	business-it alignment in pss value networks: a capability-based framework		Advanced information technology (IT) is regarded as a foundation for the operation of product-service system (PSS) value networks. This requires alignment between IT and PSS business strategy. Business-IT alignment (BIA) in a value network can raise the ability of partners to collaborate effectively and improve network performance. However, the theory of traditional firm-level BIA is not tailored to the specific situations of PSS value networks. In this paper we investigate the applicability of BIA concepts and definitions at a PSS value network level. Alignment in firm-level literature looked at fit between business and IT capabilities. To substantiate this at a PSS value network level, we identified and classified generic value network business capabilities on the one hand and IT capabilities on the other hand. By exploring and discussing the interrelations between the two types of capability, a conceptual framework for understanding BIA in a PSS value network is derived.	physical symbol system;value network	Samaneh Bagheri;Rob J. Kusters;Jos J. M. Trienekens	2014		10.1007/978-3-662-44745-1_27	simulation;engineering;artificial intelligence;operations management	Vision	-75.73669456403822	8.137633715199984	79370
3b9ecaf91dacfbe62db268256b3df9a88efade4f	supporting requirements elicitation through goal/scenario coupling	requirements elicitation;requirement engineering	Goals have long been recognized to be an essential component involved in the Requirements Engineering (RE) process. They have proved to be an effective way to support a number of requirements engineering activities such as requirements elicitation, systematic exploration of design choices, checking requirements completeness, ensuring requirements pre-traceability and helping in the detection of threats, conflicts, obstacles and their resolution. The leading role played by goals in the RE process led to a whole stream of research on goal modeling, goal specification/formulation and goal-based reasoning for the multiple aforementioned purposes. On the other hand, there is evidence that dealing with goal is not an easy task and presents a number of difficulties in practice. To overcome these difficulties, many authors suggest combining goals and scenarios. The reason is that they complement each other: there is a mutual mitigation of difficulties encountered with one by using the other. The paper reviews various research efforts undertaken in this line of research. It uses L’Ecritoire, an approach that guides the requirements elicitation and specification process through interleaved goal modelling and scenario authoring to illustrate the combined use of goals and scenarios to reason about requirements for the system To-Be.	code coverage;goal modeling;interaction;potts model;refinement (computing);requirement;requirements elicitation;requirements engineering;traceability	Colette Rolland;Camille Salinesi	2009		10.1007/978-3-642-02463-4_21	reliability engineering;requirements analysis;software requirements specification;requirements management;goal modeling;systems engineering;engineering;requirement;system requirements specification;requirements elicitation;management science;non-functional requirement;requirements traceability	SE	-63.8918426916949	17.672989812510956	79388
0a801703427f5d753f1ad914f04476a6c0c94d71	integrated risk management process assessment model for it organizations based on iso 31000 in an iso multi-standards context		Managing processes remain a key challenge for most organizations which need to preserve competitiveness. Process assessment frameworks can help by providing instruments guiding process improvement and regulation alignment. Several process assessment frameworks such as TIPA® are based on the ISO Process assessment standard series ISO/IEC 15504, currently revised in the ISO/IEC 330xx family of standards. Following a Design Science Research (DSR) methodology, this paper visits the TIPA Framework evolution throughout iterative cycles in terms of design, rigour and relevance. It investigates how original and new artefacts are being developed and improved, in particular the new strategic move towards the automation of the assessment process represented by a new software artefact. By demonstrating the evolution of the TIPA framework using a DSR perspective combined with Action Design Research for the new software artefact, this paper explicates design knowledge regarding the role and value of the framework within the ISO standards community and in practice.		Béatrix Barafort;Antoni Lluís Mesquida;Antònia Mas	2018	Computer Standards & Interfaces	10.1016/j.csi.2018.04.010	computer science;real-time computing;iso 31000;process management;risk management;business process;design science research;process modeling	HCI	-70.31502502252	14.588706684777037	79529
8e72a49cabf310765a9a9fb8a724936db8e6cf5e	e-health initiative and customer's expectation: case brunei		services in Brunei Darussalam (Brunei) from customers' perspective. It is to identify, understand, analyze and evaluate public's expectation on e-health in Brunei. A questionnaire was designed to gather quantitative and qualitative data to survey patients, patient's family, and health practitioners at hospitals, clinics, or home care centers in Brunei starting from February to March, 2011. A 25-item Likert-type survey instrument was specifically developed for this study and administered to a sample of 366 patients. The data were analyzed to provide initial ideas and recommendation to policy makers on how to move forward with the e-health initiative as a mean to improve healthcare services. The survey revealed that there exists a high demand and expectation from people in Brunei to have better healthcare services accessible through an e-health system in order to improve health literacy as well as quality and efficiency of healthcare. Regardless of the limitations of the survey, the general public has responded with a great support for the capabilities of an e-health system listed from the questionnaires. The results of the survey provide a solid foundation for our on going research project to proceed further to develop a model of e-health and subsequently develop a system prototype that incorporate expectations from the people.	confidentiality;customer relationship management;e-commerce;e-government;erp;electronic business;email;information retrieval;information security;internet;management information system;prototype;system administrator;while	Mohammad Nabil Almunawar;Zaw Wint;Patrick Kim Cheng Low;Muhammad Anshari	2012	CoRR		management science	HCI	-70.51210017287099	9.651312167687406	79663
3a4228bce49946599bf8dfb2e4c3f0a4515c30b0	extracting high quality scenario for consensus on specifications of new products	human centric process;scenario map;objective data;chance discovery;helix loop process;undefined defect;undefined name;new product;group discussion;subjective data;high quality scenario	In this study, analysis has been executed on the scenarios for consensus on specifications of new products, which were extracted with the refinement of Double Helix Loop process for Chance Discovery. The aim of this analysis was to confirm that SCENARIO MAP, which refines KeyGraph by attaching macro photography of defects on the undefined names of the defects on its nodes, helps examinees to extract high quality scenarios through group discussion and to find the roles of SCENARIO MAP and Group discussion with its effect. 11 high quality scenarios for future proposal were extracted successfully after 104 scenarios. We found that SCENARIO MAP played the role to help examinees to identify the undefined defects and interpret the context of objective data and subjective data easily and the group discussion accelerated them to exchange their inherent experiences, know how and knowledge. The human centric process of SCENARIO MAP and group discussion can be regarded to give effect to extract these high quality scenarios.		Kenichi Horie;Yukio Ohsawa	2006		10.1007/978-3-540-34353-0_16	simulation;consensus;computer science;artificial intelligence;photography;product design specification;knowledge extraction;algorithm;new product development	DB	-63.13851266253796	19.40590271680412	79749
fbd95b840b5edeca3f371018fac4b05611e76023	software reuse in open source: a case study	empirical study;software evolution;component based software development;software reuse;open source software;quantitative study	"""A promising way to support software reuse is based on Component-Based Software Development CBSD. Open Source Software OSS products are increasingly available that can be freely used in product development. However, OSS communities still face several challenges before taking full advantage of the """"reuse mechanism"""": many OSS projects duplicate effort, for instance when many projects implement a similar system in the same application domain and in the same topic. One successful counter-example is the FFmpeg multimedia project; several of its components are widely and consistently reused in other OSS projects. Documented is the evolutionary history of the various libraries of components within the FFmpeg project, which presently are reused in more than 140 OSS projects. Most use them as black-box components; although a number of OSS projects keep a localized copy in their repositories, eventually modifying them as needed white-box reuse. In both cases, the authors argue that FFmpeg is a successful project that provides an excellent exemplar of a reusable library of OSS components."""	code reuse	Andrea Capiluppi;Klaas-Jan Stol;Cornelia Boldyreff	2011	IJOSSP	10.4018/jossp.2011070102	social science;quantitative research;systems engineering;engineering;software evolution;component-based software engineering;software engineering;software construction;sociology;empirical research;world wide web	SE	-64.5427712178268	23.520831446193895	79751
9dee03c4948df874538ea84676a070e1abbc0194	transferring knowledge from research to industry: experiences from germany	engineering science;software development;knowledge transfer	In the past 10 years a research cluster in engineering science has been set up to create completely new materials as well as appropriate machines and production processes. To support the collection, storage and analysis of complex process data within that cluster several IT & process management methods and tools have been developed. Now entering the final stage of the cluster, the focus changes to the efficient usage of the created knowledge. To support the technology transfer process the previously created methods and tools are enhanced. In this paper, a software system supporting the controlled and secure transfer of engineering knowledge from research to industry is presented.		Michaela Helbig;Hajo Wiemer;Jens Weller	2015			knowledge integration;computer science;knowledge management;software development;knowledge engineering;management	HPC	-66.65432191161665	20.05719310687582	79763
1d844d29f798f71c688b6ea6ff63460b3793df33	digital fitness: four principles for successful development of digital initiatives	software;project management;information systems;technology trajectories;strategic alignment;digital economics digital fitness it business value strategic alignment technology trajectories;media;strategic planning business data processing economics organisational aspects;organizational leaders digital fitness noncio organizational leader digital initiatives business value generation organizational strategies business strategies strategic alignment technology trajectories digital economics;organizations;digital economics;economics;it business value;digital fitness;project management economics media information systems organizations software	Digital initiatives such as big data analytics at Progressive Insurance can drive competitive advantage. However, digital initiatives often fail to meet their objectives in terms of budget, time, or performance. Why is this the case and what can be done about it? Complementing existing studies of software development methodologies, project management techniques, and the role of the CIO, we focus on the role of the non-CIO organizational leader in successful development of digital initiatives. Synthesizing our own original research with existing scholarship, we introduce four principles for successful development of digital initiatives: generate business value, maintain strategic alignment with organizational and business strategies, leverage technology trajectories, and apply digital economics. Understanding the four principles raises the digital fitness of organizational leaders, thereby reducing risk and raising the chances of success in digital initiatives.	big data;chief information officer;health insurance portability and accountability act;software development process	Nigel P. Melville	2015	2015 48th Hawaii International Conference on System Sciences	10.1109/HICSS.2015.526	project management;digital transformation;media;organization;knowledge management;marketing;management science;management;strategic alignment;information system	HCI	-74.6478340279963	8.288280852707734	79813
f2ed6f330e5b07bf9cc585ea8c4ae2ed072b67bd	quality assessment in devops: automated analysis of a tax fraud detection system	tax fraud detection;data intensive applications;unified modeling language;model based quality of service;devops	The paper presents an industrial application of a DevOps process for a Tax fraud detection system. In particular, we report the influence of the quality assessment during development iterations, with special focus on the fulfillment of performance requirements. We investigated how to guarantee quality requirements in a process iteration while new functionalities are added. The experience has been carried out by practitioners and academics in the context of a project for improving quality of data intensive applications.	data-intensive computing;devops;iteration;requirement	Diego Perez-Palacin;Youssef Ridene;José Merseguer	2017		10.1145/3053600.3053632	unified modeling language;computer science;devops;computer security	SE	-66.97028284406534	22.48833169290054	79944
809b1a44bfa0314e4c079c9a1b1cdd12257cac08	challenges in efficient customer recognition in contact centre: state-of-the-art survey by focusing on big data techniques applicability	databases;customer relationship management;companies;big data;next generation networking	The pulse of today's business processes, competition and technology can be well observed in the increasingly available information companies are confronted with. Particularly, the importance of information leads to big data which makes it even more difficult for companies to manage their information and costumers. Contact centers (CCs) as the main touch point of organizations face the major issues in managing their costumer efficiently. This paper will focus on the role of big data in empowering the current contact centers to shift to the next generation of CCs. Based on the current literature on CCs, four gaps are identified that by addressing them can be the first step in this shift.	big data;business process;next-generation network	Morteza Saberi;Anne Karduck;Omar Khadeer Hussain;Elizabeth Chang	2016	2016 International Conference on Intelligent Networking and Collaborative Systems (INCoS)	10.1109/INCoS.2016.136	next-generation network;big data;computer science;knowledge management;data mining;database;management;computer security	Robotics	-72.82743768635471	6.181520613949689	79970
ad422b6705d2e2ee38b5b0ccdc45c3a972b127e4	plm components selection based on a maturity assessment and ahp methodology		The benefits of Product Lifecycle Management (PLM) have been noted for improving business, creating collaboration, and reducing energy and time by making transcendent decisions through the process of product life cycle. This work aims to propose a PLM Components Maturity Assessment (PCMA) model to gain comprehensive maturity results and reduce the complexity in obtaining maturity scores. According to PLM functionalities, we divide PLM into fifteen components. PLM components can be cataloged into five main fields: „TechnoWare‟, „InforWare‟, „FunctionWare‟, „OrgaWare‟, and „SustainWare‟ (TIFOS Framework). With PCMA model we analyzed PLM components and proposed mature content of each dimension, obtaining specific key performance indicators for each dimension. This work has been also useful to solve decision-making issues based on AHP methodology, such as: selecting the optimal PLM components in TIFOS Framework, obtaining the components ranking weight, getting components maturity score, and comparing it with the actual situation to give constructive business suggestions. These business suggestions include strengths and weakness of PLM components and conducting selection of PLM components. Experimental studies have been conducted to verify maturity scores for each component and to achieve component-ranking weights.	capability maturity model;experiment	Haiqing Zhang;Yacine Ouzrout;Abdelaziz Bouras;Antonio Mazza;Matteo Mario Savino	2013		10.1007/978-3-642-41501-2_44	reliability engineering;systems engineering;operations management	Web+IR	-66.52644012925089	17.328900576047733	80037
8af6eb6e37d753b875db1e9f8f844e980df38eae	conceptual design of a service level agreement management system	business management;information management;service level agreement	Due to customer demand, market competition and technological development of multiple background, SLA (Service Level Agreement) is playing more and more important roles in telecommunications industry. In this paper, we give an overall system implementation method, architecture, technology roadmap and outline design of each module to guide the detailed design and development. This work can be used by the implementation of SLA system and referred by related researched in service management domain. © 2011 Springer-Verlag Berlin Heidelberg.	management system;service-level agreement	Puyu Yuan;Wuliang Peng	2011		10.1007/978-3-642-23753-9_66	reliability engineering;service level objective;systems engineering;knowledge management	EDA	-69.9717833855304	10.706894752022944	80057
5712278ca9ca553f25971caddda17451d2dccfc8	engineering multi-agent systems	multi-agent systems;peer-to-peer computing;security of data;fipa-compliant agent system;application-specific threats;cryptographic techniques;information flow;interlayer security;key maintenance;multiagent system engineering;multilayered system;multilevel security threats;peer-to-peer content lookup;secure groups;security engineering;system security	Autonomous systems are rapidly transitioning from labs into our lives. A crucial question concerns trust: in what situations will we (appropriately) trust such systems? This paper proposes three necessary prerequisites for trust. The three prerequisites are defined, motivated, and related to each other. We then consider how to realise the prerequisites. This paper aims to articulate a research agenda, and although it provides suggestions for approaches to take and directions for future work, it contains more questions than answers.	autonomous system (internet);multi-agent system;norm (social)	Amal El Fallah-Seghrouchni;Alessandro Ricci;Tran Cao Son	2017		10.1007/978-3-319-91899-0	biological systems engineering;electrical engineering technology;mechatronics;system of systems engineering;systems engineering;methods engineering;civil engineering software;railway engineering;requirements engineering;biosystems engineering;mechanical engineering	AI	-64.53784222169824	7.456521371678329	80080
12146fdb743b8d7fc6e59d7db31c75346d49aad5	understanding the adoption of use case narratives in the unified modeling language	bepress selected works;technology acceptance model;unified modeling language;use case driven;use case;unified modeling language use case use case driven technology acceptance model	This research examines the adoption of Use Case Narratives within the Unified Modeling Language (UML).Using the Technology Acceptance Model (TAM) as a framework, practitioners with UML experience were asked questions to measure their Perceived Ease of Use and Perceived Usefulness of Use Case Narratives and their Intentions to Adopt them. We extend Perceived Usefulness in the context of UML adoption to address the question “usefulness for what purpose(s)?” Generally, we find that TAM explains Use Case Narrative acceptance. More importantly, we find that Perceived Usefulness is explained by usefulness for specific software development tasks. This research provides three main contributions, beginning with an improved understanding of the role of Use Case Narratives in UML projects. Second, the study extends TAM by explaining how a technology is used rather than simply whether it is used. Third, this study provides a framework for future studies into other UML diagrams.	diagram;futures studies;ibm tivoli access manager;software development;unified modeling language;usability	Brian Dobing;Joerg Evermann;Jeffrey Parsons	2010			use case;unified modeling language;simulation;computer science;knowledge management;process driven development;database;management	SE	-72.13635746371907	23.548500234143273	80150
dc901fc6c465e53f85cb9720718b6ae242c89e07	integrated product team in large scale and complex systems		The development process of Large and complex systems are a highly complex process involving different disciplines which shall take decision and develop interrelated subsystems. Integrated Product Team is a management tool used to improve system development performance. Among the potential benefits of implementing the integrated product team principles are reduced development time, reduced risk of failure, enhanced quality, flexibility and better knowledge sharing. This paper provides an overview of the integrated product team’s principles and evaluates the lessons and guidance in existing literature applicable to develop large and complex systems. Recommendations based on own experience and lessons learned developing large and complex systems are presented here.	complex systems	Sorin Aungurenci;Aurel Chiriac	2013		10.1007/978-3-319-02812-5_24	systems engineering;knowledge management;process management	HPC	-63.392615968753496	14.461442831229457	80165
77b4178815b943b0073b9c98e1b6bd280f80cc10	impact of business intelligence and it infrastructure flexibility on competitive performance: an organizational agility perspective				Xiaofeng Chen;Keng Siau	2011			knowledge management;process management;information technology management;computer science;business intelligence	AI	-72.93752090330413	7.821633860914053	80409
ed5b7a83aa2af63b62ece02eea912c130f0b2612	handling uncertainty in agile requirement prioritization and scheduling using statistical simulation	release planning;software;statistical simulation release planning uncertainty requirements prioritization scheduling;statistical simulation;agile requirement prioritization;uncertainty;agile planning;agile planning uncertainty handling agile requirement prioritization agile requirement scheduling statistical simulation constraints management;prioritization;uncertainty handling planning scheduling software development management statistical analysis;uncertainty handling;uncertainty planning business software genetic algorithms estimation programming;requirements;statistical analysis;estimation;scheduling;business;planning;genetic algorithms;resource availability;constraints management;business value;programming;software development management;agile requirement scheduling	In the creation of a release plan developers must attempt to maximise business value while maintaining a high degree of certainty that the product will be completed on time and to budget. As a result of these constraints management is often forced to make the difficult decision as to which stories to complete and which to ignore. The difficulty of this decision is compounded by a high degree of uncertainty within the business value of each story, the story size and also the resources available. This paper proposes a relatively simple statistical methodology that allows for uncertainty in these areas. In so doing it provides key stakeholders with the opportunity to manage uncertainty in the planning of what functionality to include in upcoming releases. The technique is lightweight in nature and consistent with existing agile planning practices. A case study is provided to demonstrate how the method may be used.	agile software development;requirement prioritization;scheduling (computing);simulation;software release life cycle;user story	Kevin Logue;Kevin McDaid	2008	Agile 2008 Conference	10.1109/Agile.2008.79	reliability engineering;systems engineering;engineering;management science	Robotics	-71.74403293918733	26.695096743901406	80524
a51ba0028e487af5a49c90eacf8e6ef5e2fa61e0	exploring software project effort versus duration trade-offs	software metrics;software;schedule and organizational issues;investments;size measurement;mathematical model schedules software development product development estimation costs data models investments;software project management;project control and modeling;estimation;software development;performance measures;schedules;mathematical model;performance measures software project management software metrics cost estimation time estimation schedule and organizational issues project control and modeling;cost estimation;time estimation;data models;product development	Estimates of effort and duration for a new software project often have to be adjusted to deal with an imposed target delivery date or a constraint on staffing. Estimating methods assume an effort/duration trade-off relationship based mostly on theory or expert judgment. This paper describes a process for analyzing actual project effort and duration data which is designed to explore the trade-off relationship. I assume a reference relationship of a simple power-curve with variable power &amp;#x2018;N&amp;#x2019; and use this (a) as a means of comparing the trade-off relationships assumed by four well-known estimating methods, and (b) as the basis for a process to analyze actual project data. Results are presented of applying the process to 16 sub-sets of project data. These suggest, for example, that the value of &amp;#x2018;N&amp;#x2019; differs between new development projects and enhancement projects. The Web Extra presents more results for each step in the effort-duration trade-off process described in the main article.	emoticon;software project management;world wide web	Charles Symons	2012	IEEE Software	10.1109/MS.2011.126	reliability engineering;data modeling;estimation;schedule;software project management;computer science;systems engineering;engineering;operations management;software development;software engineering;analysis effort method;mathematical model;management;new product development;cost estimate;software metric	SE	-65.97688724000369	29.538674252263252	80561
e0596f82274cc1541020ad49a667476c423e5454	kmpi: measuring knowledge management performance	corea;kmpi;evaluation performance;gestion entreprise;entreprise;gestion des connaissances;analisis factorial;performance evaluation;methode mesure;financial performance;evaluacion prestacion;empresa;knowledge management;performance index;firm management;metodo medida;ingenierie des connaissances;coree;questionnaire survey;enquete;knowledge management performance;analyse factorielle;asie;stock price;factor analysis;firm;knowledge circulation process;knowledge sharing;korea;administracion empresa;encuesta;measurement method;logistic function;survey;gestion conocimiento;knowledge creation;economic value;asia;competitive advantage	This paper provides a new metric, knowledge management performance index (KMPI), for assessing the performance of a firm in its knowledge management (KM) at a point in time. Firms are assumed to have always been oriented toward accumulating and applying knowledge to create economic value and competitive advantage. We therefore suggest the need for a KMPI which we have defined as a logistic function having five components that can be used to determine the knowledge circulation process (KCP): knowledge creation, knowledge accumulation, knowledge sharing, knowledge utilization, and knowledge internalization. When KCP efficiency increases, KMPI will also expand, enabling firms to become knowledgeintensive. To prove KMPI’s contribution, a questionnaire survey was conducted on 101 firms listed in the KOSDAQ market in Korea. We associated KMPI with three financial measures: stock price, price earnings ratio (PER), and R&D expenditure. Statistical results show that the proposed KMPI can represent KCP efficiency, while the three financial performance measures are also useful. # 2004 Elsevier B.V. All rights reserved.	knowledge management;tree accumulation	Kun Chang Lee;Sangjae Lee;Inwon Kang	2005	Information & Management	10.1016/j.im.2004.02.003	logistic function;questionnaire;process performance index;economics;economic value added;computer science;marketing;operations management;factor analysis;management;competitive advantage;statistics	AI	-83.95262245101375	8.442933035802326	80616
cb6c2b8bf515a52e61c3fb95fd718659c8d9f768	capital budgeting in information systems development	administracion financiera;capital;corporate planning and strategy;information system investment;analisis datos;financial management;inversion;information technology;cost of capital;estrategia;technologie information;investment;costo;strategy;data analysis;planificacion;investissement;presupuesto;analyse donnee;planning;budget;information system;planification;gestion financiere;tecnologia informacion;strategie;systeme information;capital budgeting;information system development;cout;sistema informacion	The results of an empirical study on the current usage of capital budgeting techniques for evaluating, terminating, and auditing information system investments are presented. Findings based on 134 senior MIS personnel and management executives indicate that capital budgeting has little impact on IS investment, and simple techniques such as payback period and cost benefit ratio are preferred over more sophisticated discount cash-flow models. Problems with cost and return estimations are shown to be the key factors that limit their use. It is suggested that the decision authority varies according to the project value and the type of decisions being undertaken.	information system;software development process	Kar Yan Tam	1992	Information & Management	10.1016/0378-7206(92)90016-9	inversion;cost of capital;planning;modified internal rate of return;economics;capital;strategy;investment;operations management;economy;capital budgeting;data analysis;management;information technology;information system	DB	-83.07318668186072	9.070784929985564	80625
3bd6a38adf5479e666ba8465f2392f9bd1fe20f8	on the future of modelling - why current case tools insist on supporting 20 years old methods and techniques			computer-aided software engineering	Colette Rolland	1989			systems engineering;computer science;computer-aided software engineering	HCI	-69.75723174386164	25.73697229330456	80795
8df0e451569760657b08035c5ae613f9ee75a292	lean/agile software development methodologies in regulated environments - state of the art	agile software development;it professional;regulated environment;agile development;software development methodology;conference item;software development;systematic literature review;medical device;lean;safety critical;software quality;embedded software	Choosing the appropriate software development methodology is something which continues to occupy the minds of many IT professionals. The introduction of “Agile” development methodologies such as XP and SCRUM held the promise of improved software quality and reduced delivery times. Combined with a Lean philosophy, there would seem to be potential for much benefit. While evidence does exist to support many of the Lean/Agile claims, we look here at how such methodologies are being adopted in the rigorous environment of safety-critical embedded software development due to its high regulation. Drawing on the results of a systematic literature review we find that evidence is sparse for Lean/Agile adoption in these domains. However, where it has been trialled, “out-of-the-box” Agile practices do not seem to fully suit these environments but rather tailored Agile versions combined with more planbased practices seem to be making inroads.	agile software development;embedded software;lean integration;out of the box (feature);scrum (software development);software development process;software quality;sparse matrix;systematic review	Oisín Cawley;Xiaofeng Wang;Ita Richardson	2010		10.1007/978-3-642-16416-3_4	personal software process;verification and validation;agile unified process;extreme programming practices;agile usability engineering;systems engineering;engineering;package development process;social software engineering;software development;software engineering;release management;software construction;agile software development;software walkthrough;software documentation;empirical process;lean software development;software deployment;software development process;computer engineering;software peer review	SE	-65.93463586789878	23.531660522875328	80836
7d0fc7e85aa4ef46d485a8078890bdde7ebb5a40	a method for tridimensional process assessment using modelling theory	analytical models;software;capability maturity model organizations analytical models software book reviews microprogramming planning;modeling theory;software process improvement;performance indicator;three dimensions;process capability;process capability analysis;pro2pi methodology software process improvement process assessment process modeling;process assessment;process capability profile model;process enactment description model;pro2pi methodology;process modeling;innovative process assessment method;continuous improvement;capability maturity model;planning;book reviews;process model;process improvement;organizations;it evaluation;microprogramming;tridimensional process assessment;process performance indicator model;software process improvement process capability analysis;process improvement cycle;model theory;process performance indicator model tridimensional process assessment modeling theory innovative process assessment method process improvement cycle process capability profile model process enactment description model	This article introduces a process assessment method that uses modeling theory to consider the continuously improved process under three dimensions. These dimensions are related with three types of process models used during a process improvement cycle: Process Capability Profile model, Process Enactment Description model and Process Performance Indicator model. The article presents the motivations for this work, the objectives, methodology and process for this work, a description of the proposed method, experiences using the method and considerations about its evaluation.	definition;semiconductor consolidation	Clenio F. Salviano;Márcia Regina Martins Martinez;Edgar Lopes Banhesse;Angela Enelize;Alessandra Zoucas;Marcello Thiry	2010	2010 Seventh International Conference on the Quality of Information and Communications Technology	10.1109/QUATIC.2010.95	reliability engineering;process capability;idef3;systems engineering;engineering;software engineering;process modeling;management science;model theory	Robotics	-69.77594837771049	15.219460458373616	80872
da5ea6528c703b39c49302a163e98cfd09ecde45	workshop on human aspects of software engineering	reflection and retrospective processes;project management;stakeholder involvement;project manager;software development process;software engineering;cognitive process;human aspects;social processes;organizational aspects;cognitive processes;teamwork	This workshop focuses on human aspects of software engineering. The importance of this topic stems from the recognition that the more the software world is developed, the more it is accepted by the software engineering community that the people involved in software development processes deserve further attention, not the processes themselves or technology. In this spirit, this workshop attempts to highlight the world of software engineering from the perspective of the main actors and stakeholders involved in software development processes: the individuals, the team, the customer, the organization, and others. Needless to say, the code and technology are main actors in this process as well; indeed, in the workshop they are addressed and analyzed from the human perspective. Since the OOPSLA community deals with software engi-neering processes, it should not neglect their human aspects. The workshop offers one venue in which the topic can be discussed.	software development process;software engineering;venue (sound system)	Orit Hazzan;Yael Dubinsky	2009		10.1145/1639950.1639984	project management;software review;personal software process;team software process;cognition;software engineering process group;software project management;knowledge management;social software engineering;software development;management science;software walkthrough;software development process;software peer review	SE	-66.27047730980884	24.323373823856027	80887
4b131a486bc3aa1130543a01113363d73d541db4	factor analysis of the advanced manufacturing mode diffusion based on system dynamics	advanced manufacturing mode diffusion;system dynamics;factor analysis;sensitivity analysis	In order to reveal the factor impact on the advanced manufacturing mode diffusion, the influence of various factors in the diffusion process is studied from the perspective of system dynamics. To this end, firstly, factors in the advanced manufacturing mode diffusion are identified, which include both macro and micro factors from the perspective of whole industry and single enterprise respectively. Secondly, based on the factor analysis and system dynamics, both a macro and a micro system dynamic model of the advanced manufacturing mode diffusion are established. Then, the models are simulated by Vensim, and different parameter settings of the models are discussed. And then, the sensitivity analysis of factors is conducted. Finally, based on the results of system dynamics analysis, conclusions about the impact of these factors on the advanced manufacturing mode diffusion are drawn, which not only verifies the proposed models, but provides foundation for understanding the diffusion mechanism of the advanced manufacturing mode.		Chaogai Xue	2013	JSW	10.4304/jsw.8.10.2562-2568	computer science;system dynamics;factor analysis;sensitivity analysis;statistics	Robotics	-84.16305948864235	9.981073845863396	80933
2251b3eeacaf717c94ceb7bd145e60954a8fcb8d	implementation of lean practices based on malaysian manufacturers' perspective: a confirmatory factor analysis	lean manufacturing;lean implementation;confirmatory factor analysis;work organisation;quality improvement;process optimisation;lean production;operational efficiency;lean indicators;layout design;malaysia;cfa;lean practices;productivity;product quality	This article aims to explore the most significant indicator and factor that drive the implementation of lean production, primarily by Malaysian manufacturers. Using a survey data from 107 respondents, the analysis disclosed that the ability to improve the quality of products, increase the productivity and the operation efficiency was the greatest impact that was achieved from the implementation of lean production practices by Malaysian manufacturers. From an exploratory factor analysis, process optimisation, layout arrangement and work organisation were understood as the main component that encourages the implementation of lean production practices. The findings was confirmed using confirmatory factor analysis, showing that all the practice indicators that were assessed and the factors suggested is consistent with the current practice of lean production in the population studied. This provides a valuable input in making a strategic decision in designing and implementing the realistic systems of lean produ...	confirmatory factor analysis	Muhamad Zaki Yusup;Wan Hasrulnizzam Wan Mahmood;Mohd Rizal Bin Salleh	2016	IJAOM	10.1504/IJAOM.2016.079680	lean project management;lean laboratory;page layout;productivity;economics;confirmatory factor analysis;systems engineering;engineering;operations management;manufacturing engineering;lean manufacturing	HCI	-80.86489158344655	7.640503504048297	80965
956bc7863c705a0dbeb35089944391f2c4b3237b	collaborative model merging	rationale management;collaboration;issue;merging;model;software development life cycle;conflict resolution;operation based	Models are important artifacts in the software development life-cycle and are often the result of a collaborative activity of multiple developers. When multiple developers modify the same model, conflicts can occur and need to be resolved by merging. Existing approaches for model merging require developers to solve all conflicts before committing. The later a developer commits the higher the probability for even more conflicts. This forces the developers to solve every conflict as soon as possible and without consulting the other developer. However, we claim that developers often need to discuss their choice of conflict resolution with another developer in case of a complex conflict, since a conflict also expresses differences in opinion about the model. In this paper we propose to allow developers to postpone a decision of a modeling conflict. We present an approach to make conflicts part of the model and represent them as first-level entities based on issue modeling from the field of Rationale Management. This facilitates the possibility for collaborative conflict resolution and merging. Furthermore, it allows for a complete batch merge instead of interactive merging, where all conflicts are added to the model and then resolved later. To substantiate our claim that developers favor to discuss complex conflicts we conducted a case study.	batch processing;design rationale;entity;software development process	Maximilian Kögel;Helmut Naughton;Jonas Helming;Markus Herrmannsdoerfer	2010		10.1145/1869542.1869547	simulation;conflict resolution research;computer science;knowledge management;conflict resolution;management science;systems development life cycle;collaboration	SE	-64.10812966804998	14.729139144280923	81060
601a09f86b95324e52f4d3dbe0f77d0b9efbb1e6	quality in software digital ecosystems the users perceptions	quality in software ecosystem;e government;user perception;product model;digital content;quality;software component;cooperative engineering;public good;free software;digital emerging ecosystem;public administration	Brazilian Public Software (BPS) is an innovative experience in public administration. It combines features of the free software production model with the concept of public goods and is delivered by a portal that links different people and interests. This paper is about the perception of quality by the participants of the digital ecosystem BPS -Brazilian Public Software. The concept of digital ecosystems employed here is ecosystems which digital environment is populated by digital species (software components, applications, online services, etc.). These ecosystems can be devoted to digital content production, business, and academic research. [5].		Giancarlo Nuti Stefanuto;Maiko Spiess;Angela Maria Alves;Paula F. D. Castro	2011		10.1145/2077489.2077504	e-government;public good;computer science;knowledge management;social software engineering;component-based software engineering;software development;software as a service;database;multimedia;software deployment;world wide web;software quality control;computer security;software quality	HCI	-63.634881340745764	22.022644795152114	81089
82c2183a465c3924d6a0579fbdf64bb8a4f84274	systematic review of statistical process control: an experience report	systematic review;empirical software engineering;statistical process control	Background: A systematic review is a rigorous method for assessing and aggregating research results. Unlike an ordinary literature review consisting of an annotated bibliography, a systematic review analyzes existing literature with reference to specific research questions on a topic of interest. Objective: Statistical Process Control (SPC) is a well established technique in manufacturing contexts that only recently has been used in software production. Software production is unlike manufacturing because it is human rather than machine-intensive, and results in the production of single one-off items. It is therefore pertinent to assess how successful SPC is in the context of software production. These considerations have therefore motivated us to define and carry out a systematic review to assess whether SPC is being used effectively and correctly by software practitioners. Method: A protocol has been defined, according to the systematic literature review process, it was revised and refined by the authors. At the current time, the review is being carried out. Results: We report our considerations and preliminary results in defining and carrying out a systematic review on SPC, and how graduate students have been included in the review process of a first set of the papers. Conclusions: Our first results and impressions are positive. Also, involving graduate students has been a successful experience.	application domain;complete (complexity);documentation;experimental software engineering;relevance;systematic review	Maria Teresa Baldassarre;Danilo Caivano;Barbara A. Kitchenham;Giuseppe Visaggio	2007			computer science;systems engineering;data mining;management science;software technical review	HCI	-68.34245561229916	30.345027393731016	81365
2a772e092a8a2b452745149c733cfc37d51f2a2d	introduction to the special issue on aviation operations research: commemorating 100 years of aviation	gestion personnel;staff management;gestion trafic;air transportation;traffic management;operations research;gestion personal;transport aerien;transporte aereo;scheduling;personal de navegacion;personnel navigant;gestion trafico;crew;ordonnancement;reglamento	One hundred years ago amid the dunes on the Outer Banks of North Carolina, Orville Wright piloted a gasoline-powered aircraft down a single iron track. That aircraft lifted into the air, flew 120 feet, and came to a controlled landing. This flight represented the result of five years of research and experiments by two bicycle makers, Wilbur and Orville Wright. The Wright brothers developed fundamental aviation technologies that are still in use in more advanced form today, including an aileron, rudder, and elevator. Probably more important to their ultimate success was that they viewed the aircraft as a system that had to dynamically adjust to variations in conditions that could arise during flight. Competing, better-funded efforts to develop heavier-than-air flying machines did not meet the Wright brothers’ success, largely because of their lack of a comprehensive dynamic control system. The Wright brothers used the control of a bicycle as their model, noting that one must constantly adapt weight distribution, steering, and speed to keep the bike upright during travel. Thus, in many respects the Wright brothers’ achievement can be viewed as a success for systems engineering. Of course, over the years the aircraft has developed substantially, evolving into today’s large, powerful jet. Yet, more impressive than the aircraft itself may be the systems surrounding the provision of services based on it. These include the passenger and freight air transport networks; airline-reservation systems; passenger-, crew-, and fleet-scheduling systems; maintenance operations; package-sorting systems; baggage-handling systems; etc. Operations research has played a fundamental role in the development of these systems, and their analysis has led to many important developments in the field of operations research. In fact, it can be argued that air transport systems have played a leading role in fostering the development of operations research. Air crew scheduling was one of the first real applications of integer programming. By the early 1980s, most air carriers had implemented crew-scheduling systems based on the exact or approximate solutions of integer-programming representations of this problem. Air crew scheduling motivated much of the early developments on the set-partitioning problem and continues to challenge research in the area today. The airline industry pioneered the development of the field of yield management. Yield management is credited with increasing the revenues of the airline industry by as much as 8%. Today yield management represents one of the most vibrant research areas within operations research, and applications within a broad range of industries are now underway. The importance of operations research to aviation spurred the early development of OR groups within most major airlines. In fact, the airlines have their own operations research society: AGIFORS. The strong link between operations research and aviation makes it very natural and perhaps mandatory to commemorate the Wright brothers’ first flight with a special issue of Transportation Science. Transportation Science is the leading operations research journal dedicated to the analysis of transportation systems. No special call for papers was released for this special issue. Rather, one overview paper was commissioned, and a set of papers was selected from the publication queue. It is a testament to the vibrancy of the field of aviation operations research that such a high-quality set of papers could be assembled from the publication queue.		Michael O. Ball	2003	Transportation Science	10.1287/trsc.37.4.366.23278	active traffic management;computer science;engineering;operations management;operating system;transport engineering;operations research;scheduling;aviation	DB	-64.00409872114473	4.293636308833342	81426
f848c61c07e22bf22bd05c69e97dab6c70c49af6	bim-fm and information requirements management: missing links in the aec and fm interface		A steady shift in the value added from building information modelling (BIM) to architectural, engineering and construction (AEC) activities to those of facilities management (FM) is seeing increasing emphasis on whole-of-life thinking and associated information requirements management practices. Little is known about the process of identifying, documenting, generating and harmonizing BIM data inputs with FM data outputs in the Australian construction industry. Grounded on empirical evidence from a case study that transverses client and project team perspectives, this exploratory paper identifies missing links in the AEC and FM interface. The study describes the issues surrounding the collection and harmonization of BIM data inputs (as-built deliverables at handover) and the identification of (and connection to) FM data outputs. With the limitation of an exploratory and interpretive case study, the intention is to provide a contribution to academics and practitioners with grounded, stakeholder-related insights.	bim;echo suppression and cancellation;fm broadcasting;requirements management	Julie Jupp;Ramsey Awad	2017		10.1007/978-3-319-72905-3_28	systems engineering;building information modeling;requirements management;handover;business;deliverable;project team;facility management;empirical evidence	HPC	-69.85268591188272	4.5985053014780055	81445
c4bd582687bf029b0a82f08363b0d283821f362b	evaluation of code review methods through interviews and experimentation	datavetenskap datalogi	This paper presents the results of a study where the effects of introducing code reviews in an organisational unit have been evaluated. The study was performed in an ongoing commercial project, mainly through interviews with developers and an experiment where the effects of introducing code reviews were measured. Two different checklist based review methods have been evaluated. The objectives of the study are to analyse the effects of introducing code reviews in the organisational unit, and to compare the two methods. The results indicate that many of the faults that normally are found in later test phases or operation are instead found in code reviews, but no difference could be found between the two methods. The results of the study are considered positive, and the organisational unit has continued to work with code reviews.	test case	Martin Höst;Conny Johansson	2000	Journal of Systems and Software	10.1016/S0164-1212(99)00137-5	computer science;systems engineering;software engineering;management science	HCI	-70.55636165091907	21.956044619509605	81479
a0d1e8513c37cc53268b0dea7cf364edcca148c6	rendex: a method for automated reviews of textual requirements		Conducting requirements reviews before the start of software design is one of the central goals in requirements management. Fast and accurate reviews promise to facilitate software development process and mitigate technical risks of late design modifications. In large software development companies, however, it is difficult to conduct reviews as fast as needed, because the number of regularly incoming requirements is typically several thousand. Manually reviewing thousands of requirements is a time-consuming task and disrupts the process of continuous software development. As a consequence, software engineers review requirements in parallel with designing the software, thus partially accepting the technical risks. In this paper we present a measurement-based method for automating requirements reviews in large software development companies. The method, Rendex, is developed in an action research project in a large software development organization and evaluated in four large companies. The evaluation shows that the assessment results of Rendex have 73%-80% agreement with the manual assessment results of software engineers. Succeeding the evaluation, Rendex was integrated with the requirements management environment in two of the collaborating companies and is regularly used for proactive reviews of requirements.	requirement	Vard Antinyan;Miroslaw Staron	2017	Journal of Systems and Software	10.1016/j.jss.2017.05.079	systems engineering;software peer review;goal-driven software development process;non-functional testing;software requirements specification;software technical review;computer science;software requirements;social software engineering;requirements analysis	SE	-69.311432317136	22.647121069680054	81524
f12d25ef9591896925b26376751611794137c264	tipster phase iii goals	phase iii framework;capabilities platform development;advanced research;primary goal;tipster program;metrics-based evaluations;iii goal;phase ii;tipster phase;balanced overall program;implementation projects	The primary goal of TIPSTER Phase III is to promote advancements in text processing technologies. To accomplish this goal, the TIPSTER Program will continue to encourage the cooperation of researchers and developers in government, industry and academia to achieve a balanced overall program. The Phase III framework is modeled on that of Phase II and will consist of four basic components: (1) Advanced Research, (2) Architecture and Capabilities Platform Development, (3) Metricsbased Evaluations and (4) Demonstration and Implementation Projects.		F. Ruth Gee	1996			simulation;systems engineering;engineering;management science	SE	-69.76846256465286	16.435116370388418	81540
b6da1e61f4b8fb37589ef44798930f8222bf6d7f	quality time: can a manufacturing quality model work for software?	software metrics;six sigma;software engineering 6 sigma model manufacturing quality model standard deviations statistical models hardware quality software quality measurement;standard deviation;software systems;software engineering;statistical model;statistics;virtual manufacturing software quality six sigma hardware software systems software measurement manufacturing processes software maintenance maintenance engineering measurement standards;quality model;software metrics statistics software quality;software quality	Six-sigma (i.e. 6 standard deviations) is a parameter that is used in statistical models of the quality of manufactured goods (including computer hardware). It also serves as a slogan that suggests high quality. Some attempts have been made in the past to apply 6-sigma to software quality measurement. Software engineers often look to hardware analogies to suggest techniques that are useful in building, maintaining or evaluating software systems. The author explains why the 6-sigma approach to hardware quality simply does not work when applied to software quality.		Robert V. Binder	1997	IEEE Software	10.1109/52.605937	reliability engineering;statistical model;personal software process;verification and validation;software sizing;software verification;computer science;systems engineering;engineering;package development process;backporting;social software engineering;software development;software engineering;software construction;software walkthrough;standard deviation;software measurement;software deployment;software quality control;six sigma;software requirements;software quality;software metric;software quality analyst;software system;software peer review	SE	-63.353190879830194	31.33330769198435	81721
abcad2ed09f27c94a3bda7960ec18da22934b61f	object-oriented analysis and design with applications, third edition	embedded systems;arm;linux;object oriented analysis and design;scilab;programming;open source software	Object Oriented Analysis and Design with Applications (3 ed.) is written by Grady Booch; Robert A. Maksimchuk; Michael W. Engle; Bobbi J. Young, Ph.D; Jim Conallen; and Kelli A. Houston. Published by Addison-Wesley, © 2007, ISBN 0-20189551-X, 691 pages, $64.99 US. This 3 edition is the eagerly and long awaited update to the 2 edition which was published in 1994. It provides thorough and practical coverage of concepts, techniques, notations and examples for modern object-oriented analysis and design. The material covered draws upon a solid foundation of theoretical work but is consistently pragmatic in approach. This book provides an essential body of knowledge for professionals responsible for the analysis and design of complex systems. As with the second edition, the book is organized into three major sections – Concepts, Method and Applications. Concepts introduces the fundamental principles of object-oriented analysis and design (OOAD) such as creating abstractions, objects and classes, and how to address the complexities found in a variety of systems. The Method section focuses on how to analyze and design complex systems with an emphasis on using UML 2. This section also gives some coverage of OOAD as part of a development cycle and from the project management perspective. Finally, the Applications section provides five in-depth examples from different domains which provide illustrative approaches to sets of problems commonly faced by practitioners. This edition follows a similar format and addresses many of the same topics as its predecessor but varies in several areas. Most noticeably, the famous “clouds” and other Booch notations used in the 2 edition have all been replaced with UML. The UML diagrams also make frequent use of the newer UML 2 notations such as frames on sequence diagrams and ports on component diagrams. The applications examples have also been updated. The weather monitoring and cryptanalysis examples remain but the class library and inventory tracking examples have been replaced with satellite-based navigation, traffic management and a vacation tracking system. As a whole, the new set of applications nicely cover a variety of challenges found in modern systems design. There are also fewer code examples in this edition. However, as the frequent use of Courier font suggests, the text still sits conceptually just slightly above code level when that is necessary. This book is very well organized, written and edited. For example, in the Methods section, the chapter on Notation doesn’t merely plod through the syntax of various shapes and line styles but explains each diagram set with regard to intended use and contribution to object-oriented models. There are also clear and informative distinctions between essential techniques and more advanced concepts. In addition to the substantive case studies in the Applications sections, there are effective supporting examples used throughout the text. Though some concepts clearly build upon each other, the reader is not forced to read the material in a certain order – e.g., references to material in other chapters are clearly marked and summarized. The bibliography is nicely organized into major topics, there are extensive end-notes by chapter, and foot notes are used as needed. Diagram styles vary somewhat from chapter to chapter but, as explained in the preface, this is deliberate in order to familiarize the reader with the output of commonly used tools. These variances in style also help one discern between superficial and substantive differences in notation. The process material, both in the Methods section and in the organization of the Applications examples, naturally draws from the Rational Unified Process (RUP). However, RUP is used effectively to convey ideas and not in a way that would prevent practitioners of another process from applying the core OOAD material being discussed. More extensive or sharply defined distinctions between what is common practice as compared to alternative approaches, including the risks or benefits of either, would have been nice to have but this omission doesn’t detract from what is a great book overall. Though intended primarily for developers and architects of software systems, the material presented would also be highly valuable to analysts in non-engineering roles such as business systems analysts. It is also a worthwhile read for those working on systems without a software emphasis. Though obviously focused on analysis and design activities, it is a excellent supplemental text for requirements gatherers, developers and project managers. Analysts, designers and architects of complex systems, will find this text provides broad and deep coverage in the current practice of OOAD. As a result, it should regarded as mandatory reading for professionals in those fields. Reviewed by Brian A. Lawler, Association of Computing Machinery and Johns Hopkins University, brian.lawler@acm.org and brian.lawler@jhu.edu.	brian;complex systems;cryptanalysis;information;international standard book number;library (computing);nice (unix);rational unified process;requirement;sequence diagram;software system;systems design;the superficial;tracking system;unified modeling language	Grady Booch;Robert A. Maksimchuk;Michael W. Engle;Bobbi J. Young;Jim Conallen;Kelli A. Houston	2007	ACM SIGSOFT Software Engineering Notes	10.1145/1402521.1413138	object-oriented analysis and design;programming;computer science;engineering;artificial intelligence;theoretical computer science;software engineering;programming language;arm architecture;linux kernel	SE	-71.82384710470862	28.32702136942721	81743
1572f51dfcb69a5e5805845a147bcd8d785de7d4	integration of electronic services in the execution of business transactions	electronic services;business transactions	Although most enterprises are using the Internet for business purposes today, the execution of business transactions remains a challenge. Areas where electronic services have established themselves include eLogistics, ePayments, and eFulfillment. However, solutions which integrate physical, financial and information logistics are still in their infancy. Starting from a definition of the business requirements and a review of the current situation in eLogistics, eFulfillment and ePayments we identify the invoice document as integration basis and describe Electronic Bill Presentment and Payment services as one possible approach towards creating integrated services for transaction execution.		Rainer Alt;Stefan Zbornik	2002			marketing;business;services computing;commerce	DB	-70.49472242112718	11.541046908417682	81807
dfaa4813148692ff171be50ba5f5b978740bc615	characterizing software engineering work with personas based on knowledge worker actions		Mistaking versatility for universal skills, some companies tend to categorize all software engineers the same not knowing a difference exists. For example, a company may select one of many software engineers to complete a task, later finding that the engineer's skills and style do not match those needed to successfully complete that task. This can result in delayed task completion and demonstrates that a one-size fits all concept should not apply to how software engineers work. In order to gain a comprehensive understanding of different software engineers and their working styles we interviewed 21 participants and surveyed 868 software engineers at a large software company and asked them about their work in terms of knowledge worker actions. We identify how tasks, collaboration styles, and perspectives of autonomy can significantly effect different approaches to software engineering work. To characterize differences, we describe empirically informed personas on how they work. Our defined software engineering personas include those with focused debugging abilities, engineers with an active interest in learning, experienced advisors who serve as experts in their role, and more. Our study and results serve as a resource for building products, services, and tools around these software engineering personas.	categorization;debugging;fits;persona (user experience);software engineer;software engineering	Denae Ford;Tom Zimmermann;Christian Bird;Nachiappan Nagappan	2017	2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)	10.1109/ESEM.2017.54	computer science;debugging;persona;software engineering;systems engineering;software;categorization;knowledge engineering;knowledge worker;autonomy	SE	-71.06039845330042	24.30464666580856	81835
01b80e95ad68e1522072964a943c96bff33c097c	toward a mature industrial practice of software test automation	publikationer;konferensbidrag;artiklar;rapporter	Since the launch of the first IEEE/ACM International Workshop on Automation of Software Test eight years ago, the research in the area has developed rapidly as test automation has become a mature practice widely adopted by the IT and software industry. One of the goals that has been continuously pursued is to bridge the gap between research and practice. Many of the recent papers are closely related to the current practice in the industry reflecting the trend toward a mature community unifying both researchers and practitioners. This is also reflected in the diversity of the topics related to the practical industrial problems covered in this special section. Authorization systems are at the heart of computer security. Antonia Bertolino et al.’s paper entitled Testing of PolPA-based Usage Control Systems is devoted to the testing of the Policy Decision Point (PDP) within the authorization system called PolPA, which enables both history-based and usage-based control of accesses. They propose two testing strategies for validating the history-based access control and the usage control functionalities, respectively. The former is based on a fault model being able to highlight the problems and vulnerabilities that could occur during the PDP implementation. The latter combines the standard technique for condition coverage with a methodology for simulating the continuous control of the PDP	access control;authorization;common open policy service;computer security;control system;fault model;simulation;software industry;software testing;test automation;xacml	Hong Zhu;Daniel Hoffman;John Hughes;Dianxiang Xu	2014	Software Quality Journal	10.1007/s11219-014-9232-8	computer science;operations management;software engineering;operations research	SE	-65.84527664903709	20.846854969874762	82080
bb26cfa719483b398b57bdc6d0568b5c0232717a	a state-of-the-art review on scheduling with learning effects	learning effectiveness;learning;learning by doing;inversion;proactive service;efecto medio;environmental effect;efecto medio ambiente;investment;saber hacer;planificacion;sevicio proactivo;know how;medium effect;scheduling;savoir faire;investissement;planning;planification;point of view;effet environnement;autonomous learning;ordonnancement;reglamento;effet milieu;service proactif	Recently learning effects in scheduling have received considerable attention in the literature. All but one paper are based on the learning-by-doing (or autonomous learning) assumption, even though proactive investments in know how (induced learning) are very important from a practical point of view. In this review we first discuss the questions why and when learning effects in scheduling environments might occur and should be regarded from a planning perspective. Afterwards we give a concise overview on the literature on scheduling with learning effects. 2007 Elsevier B.V. All rights reserved.	algorithm;autonomous robot;hall-effect thruster;scheduling (computing)	Dirk Biskup	2008	European Journal of Operational Research	10.1016/j.ejor.2007.05.040	inversion;planning;proactive learning;error-driven learning;simulation;economics;investment;artificial intelligence;operations management;active learning;scheduling	AI	-82.87890402143192	10.5186507895484	82134
8685cc9f71442fa02cd1f52630eda344b1daeb06	systems-based process reengineering in demand chains	selected works;bepress	Demand chain management is a set of practices aimed at managing and coordinating the entire chain, motivated by the final customer and working back to raw material suppliers. The Internet enables the consumer and provides the consumer with greater knowledge. By matching products to the appropriate value stream, product development time can be reduced, manufacturing costs can be reduced, and delivery times can be dramatically reduced. Analysis and design of focused demand chains involve a number of alternative channels and forms. In this paper, we propose a soft systems version of goal analysis with the intent of viewing information systems design as a system with interacting parts with a shared purpose. Identification of goals can be a way to develop conceptual models to better understand problems in interaction among system parts. Analysis of systems models can lead to identification of weak points, which can lead to redesign to overcome such problems, thus leading to the analysis of the system leading to better system solutions.	algorithm;business process;code refactoring;fits;failure;information system;interaction;internet;iteration;markov chain;mathematical optimization;new product development;program optimization;soft systems methodology;systems design	Sang-Jun Lee;Silvana Trimi;Kris Rosacker;David L. Olson	2003			simulation;systems engineering;engineering;operations management	SE	-64.31538766527432	12.577278362240337	82173
d9a827c2f539c7398eb8c065e0d819ff837ca6bc	performance impacts of big data analytics	data driven decisionmaking;big data analytics;big data;economics of it;organizational performance	Big Data Analytics has been a ‘hot topic’ for industry and academic during the past few years. This paper examines what constitutes Big Data Analytics (BDA) and how it relates to organizational performance. It also investigates what other factors influence this relationship, whether BDA leads to more data-driven decision-making (DDDM) and whether the latter is really superior to less informed decision-making. The study first operationalizes Big Data Analytics, and then develops a research model which manifests the direct and indirect relationships between analytic capability, DDDM, and organizational performance.		Usarat Thirathon	2016			analytics;organizational performance;big data;computer science;data science;data mining;database;business analytics;business intelligence;software analytics	DB	-78.91917915573276	7.789051016687221	82176
e043fd4ebc04b035c7d5e530e8654a20e16b94c5	entrepreneurship in mobile application development	application development;futures market;search engine;j2me;entrepreneurship;market structure;demand and supply;mobile commerce;brew;mobile application;technology choice	The growth of mobile commerce will require development of both demand and supply sides of the market. On the supply side, the development of mobile applications (e.g. games, calendars, search engines) via cellular technologies will occur in a context partially defined by technical development dependencies that arise from the layered nature of mobile hardware and software technologies. This research finds that these technical application development dependencies shape both the strategies employed by entrepreneurs as well as the evolution of the mobile market. In particular, entrepreneurs are faced with technology choices and device uncertainties, while the market experiences fragmentation, intermediation and distribution integration. These results explain, in part, the slow development of mobile commerce in general, provide a basis for understanding future market developments, and are further evidence of the complex interdependency between technological and market evolution.	fragmentation (computing);interdependence;mobile app;mobile commerce;mobile computing;web search engine	Ankur Tarnacha;Carleen F. Maitland	2006		10.1145/1151454.1151466	marketing;economy;business;mobile business development;commerce	HCI	-74.95200453732029	5.701729133036074	82247
e3816fdd16b5a042712a7ee4a6d25160746084d1	overview of the modelling of the physical world (motpw) workshop at models 2012	2611 modelling and simulation	Since the popularity of models and related tools for the development of software has become more mature, the applications of approaches from the modelling community has begun to be used on models of physical objects, such as buildings, machines and biological systems. This has revealed to software tool researchers that modelling and model manipulation from physical world perspectives has been applied a lot longer in many of these domains than in software. However, as software modelling follows software trends, software modellers and tool builders have been able to take advantage of new developments such as the open source movement, and agile software methodologies. This has increasingly drawn architects, engineers, biologists and others concerned with physical systems to the software modelling community. This workshop aimed to meet two complimentary goals: providing a focus on physical modelling and its use of software engineering approaches; and attempting to learn lessons in the software community from the longer established disciplines of modelling real world objects and systems which often requires more rigour and narrower margins for failure than software typically achieves. We also hoped that some islands in the IT community may be bridged through the example of bridges between models used in physical disciplines and software models.	agile software development;biological system;modeling language;open-source software;programming tool;software development process;software engineering	Keith Duddy;Jim Steel	2012		10.1145/2491617.2491618	personal software process;simulation;software engineering process group;computer science;systems engineering;software design;social software engineering;software framework;software development;software design description;management science;software walkthrough;software analytics;software development process;software requirements;software metric	SE	-64.85179314735086	24.789962499789166	82269
2dd36941f2a22c5e382300b1616e6c98a0781fd7	evaluation of electronic commerce adoption within smes	electronic commerce;small and medium enterprise;exponential growth;organizational structure	Copy right Idea Grou p Inc . Copy right Idea Grou p Inc . The exponential growth that has recently characterized the diffusion of electronic commerce (EC) applications could lead companies of any size to plan new investments, in order to compete in an increasingly dynamic market. The first business experiences show that EC should be considered a competitive instrument not simply affecting economic transactions, but significantly influencing the business organizational structure and strategic objectives. Although this issue has recently been object of considerable attention, the research on the evaluation of EC adoption for Small and Medium Enterprises (SMEs) is still relatively new. This chapter aims at supporting SMEs in choosing the most suitable EC approach according to their peculiarities and strategic goals. First, it identifies five EC approaches supporting different business activities. Then, it describes the business variables involved in any EC project and identifies four SME profiles characterized by different values of these variables. Finally, a cross analysis between EC approaches and SME profiles allows developing a framework suggesting the most suitable EC solution for each business profile.	e-commerce;time complexity	Alessandro Antonelli;Aurelio Ravarini;Marco Tagliavini	2000			marketing;business;commerce	Web+IR	-81.24802427784572	6.619883983171979	82331
66a545a7c6de0880e88aaf7cdc8373dec8584ec9	industrial applications of holonic and multi-agent systems		Our modern life has grown to depend on many and nearly ubiquitous large complex engineering systems. Many disciplines now seemingly ask the same question: “In the face of assumed disruption, to what degree will these systems continue to perform and when will they bounce back to normal operation”. This presentation argues that multi-agent systems (MAS), as decentralized and intelligent control systems, have an indispensable role to play in enabling the overall resilience of the combined cyber-physical engineering system. To that effect, it first draws from recently published work that provides measures of resilience for large flexible engineering systems. These measures define the system’s actual & latent resilience as it goes through physical disruptions. The role of a multi-agent system is then introduced so as to intelligently bring about reconfigurations that restore the system performance back to its original level. Naturally, the implementation of such a multi-agent system requires a distributed architecture. To this effect, the recent literature has used the quantitative resilience measures to distill a set of principles that design resilience into the multi-agent system. These are specifically discussed in the context of production systems and power grids. The presentation concludes with several avenues for advancing multi-agent systems to support resilient engineering systems.	control system;cyber resilience;denial-of-service attack;distributed computing;holon (philosophy);intelligent control;mit engineering systems division;multi-agent system;production system (computer science)	Wolfgang Wahlster;Thomas Strasser;Petr Kadera	2013		10.1007/978-3-319-22867-9		AI	-63.77530234002363	7.057759705289257	82519
8e8bba996e342fc3823672b5cf0da79cb97461a1	designing a lifecycle integrated data network for remanufacturing using rfid technology		With the emergence of concerns regarding pollution and the exhaustion of resources, original equipment manufacturers have begun to take responsibility for environmentally sound manufacturing according to regulations that have been established. Manufacturers thus need to decide how much they will recycle and which options to pursue for minimizing operation costs and environmental impacts, while complying with regulations. They cannot, however, predict the quality of returned products, and as a result, the planning of recycling activities is not reliable. Moreover, the components of products all have different ages and lifetimes. Thus, there may be a number of components that can be recycled more than once. If the life history of these components is not available, though, recyclable components may be disposed of after being recycled once. In this paper, we propose an integrated data system that uses radio frequency identification technology to provide useful information that can make remanufacturing more efficient.	radio-frequency identification	Young-Woo Kim;Jinwoo Park	2012		10.1007/978-3-642-40361-3_21	systems engineering;engineering;operations management;manufacturing engineering	DB	-63.33450332982117	9.595452270170965	82606
5aae894572331fbbdd13ee33050f55cd3b3585ea	an empirical study of the business value of the u.s. airlines' computerized reservations systems	empirical study;development economics;information technology;marketing strategy;empirical evidence;strategic information systems;airlines;information system;business value;computerized reservations systems;competitive advantage	The U.S. airlines’ computerized reservations systems (CRSs) have frequently been cited as examples of the successful use of information technologies (TTs) for strategic purposes. Recent literature contains logical arguments and anecdotal evidence which suggest that the carriers that have invested in the systems have been able to use them, in concert with other operating and marketing strategies, to achieve competitive advantage. However, very little rigorous, model‐based empirical evidence has been brought to bear on this issue. The purpose of this article is to develop economic models for examining the business value of the CRSs and to provide empirical evidence regarding that value during the early 1980s. The modelling builds upon previous research aimed at estimating the business value of ITs and is gener‐alizable to estimating the value of ITs in other settings. We find that increases in the use of the CRSs were associated with more highly significant increases in their owners’ shares of air passenger...		Rajiv D. Banker;Holly H. Johnston	1995	J. Org. Computing	10.1080/10919399509540253	empirical evidence;economics;marketing;operations management;business value;marketing strategy;empirical research;strategic information system;management;information technology;information system;competitive advantage;commerce	ECom	-81.67196463456798	6.851002929291905	82684
bb4710f9ef81a50039c3adb7fa9d06317d4d00b8	a software system for the discovery of situations involving drivers in storms		We present an environmental software system that obtains, inte- grates, and reasons over situational knowledge about natural phenomena and human activity. We focus on storms and driver directions. Radar data for rain- fall intensity and Google Directions are used to extract situational knowledge about storms and driver locations along directions, respectively. Situational knowledge about the environment and about human activity is integrated in or- der to infer situations in which drivers are potentially at higher risk. Awareness of such situations is of obvious interest. We present a prototype user interface that supports adding scheduled driver directions and the visualization of situa- tions in space-time, in particular also those in which drivers are potentially at higher risk. We think that the system supports the claim that the concept of situ- ation is useful for the modelling of information about the environment, including human activity, obtained in environmental monitoring systems. Fur- thermore, the presented work shows that situational knowledge, represented by heterogeneous systems that share the concept of situation, is relatively straightforward to integrate.	software system	Markus Stocker;Okko Kauhanen;Mikko Hiirsalmi;Janne Saarela;Pekka Rossi;Mauno Rönkkö;Harri Hytönen;Ville Kotovirta;Mikko Kolehmainen	2015		10.1007/978-3-319-15994-2_22	simulation;engineering;knowledge management;operations management	NLP	-63.0773336014978	17.411447473368288	82718
4ed14681f9a3168bb34a07eb3efbcf6ca990ddc1	a resource based view of the information systems sourcing mode	developpement de logiciels;outsourcing;information systems;resource based view;resource based theory;theorie fondee sur les competences;impartition;resource use;gestion de projets de developpement de systemes;strategic value resource based view information systems sourcing mode outsourcing systems development case studies;business data processing;software development;system development;information system;information systems management information systems outsourcing predictive models project management resource management availability information technology instruments companies;outsourcing information systems business data processing;management of software development projects	This paper studies the relationships between the choice of a sourcing mode for information systems, the value of the resources used in systems development activities and the presence of those resources at sufficient level within the firm. The objective is to better understand the factors underlying the decision to keep the development of an information system inside the firm or to entrust it to an outside partner. A sourcing model is proposed using the resource-based theory. Two case studies drawn from a larger study are used to illustrate the concepts used in the research model. Data from these two projects illustrate how the model could be used to predict the sourcing mode retained by the managers for each project, given the availability of the necessary resources and the strategic value of the future system.	information system;software development process	Vital Roy;Benoit Aubert	2000		10.1109/HICSS.2000.926962	knowledge management;operations management;strategic sourcing;management;information system	HCI	-69.19523715911666	6.984592702745965	82808
e51be67cca2a95292f5b2fc09bb02b50726722a6	extracting software product lines: a cost estimation perspective	investment analysis;risk assessment;extractive approach;cost estimation;software product line	Companies are often forced to customize their software products. Thus, a common practice is to clone and adapt existing systems to new customer requirements. With the extractive approach, those derived variants can be migrated into a software product line. However, changing to a new development process is risky and may result in unnecessary costs. Therefore, companies apply cost estimations to predict whether another development approach is beneficial. Existing cost models for software-product-line engineering focus on development from scratch. Contrarily, the extractive approach is more common in practice but specialized models are missing. Thus, in this work we focus on product-line extraction from a set of legacy systems. We i) describe according cost factors, ii) put them in context with the development process and cost curves, and iii) identify open challenges in product-line economics. This way, our work supports cost estimations for the extractive approach and provides a basis for further research.	analysis of algorithms;legacy system;requirement;semiconductor industry;software product line	Jacob Krüger;Wolfram Fenske;Jens Meinicke;Thomas Leich;Gunter Saake	2016		10.1145/2934466.2962731	risk assessment;implicit cost;systems engineering;engineering;operations management;management science;management;cost estimate	SE	-66.80514262028791	21.841239315724465	82896
a6b61bd5b50886ab2d12e1da1cf0e868217b6869	banking it consolidation in time and on budget	software;corporate acquisitions;software analysis;hypercube;olap technology it consolidation banking german banks atm fiducia itp panorama hypercube multidimensional analysis;banking;legacy software;service provider;it consolidation banking;software measurement;cobol;software analysis legacy software modernization cobol pl i hypercube olap it consolidation;olap;itp panorama;hypercube networks banking data mining;data mining;banking system;olap technology;banking hypercubes software maintenance documentation corporate acquisitions investments software systems personal communication networks multidimensional systems scalability;it consolidation;german banks;fiducia;hypercubes;legacy software modernization;hypercube multidimensional analysis;atm;quadrature amplitude modulation;documentation;pl i;hypercube networks	The IT services provider of a major group of German banks, FIDUCIA AG, services 15 000 branch offices with almost 100 000 PCs and 10 000 ATMs. FIDUCIAAG was facing the challenge to consolidate three different banking systems into one, after a merger of previously independent IT services organizations.To support this task, FIDUCIA chose ITPPANORAMA,whose key element is a multidimensional analysis HyperCube on OLAP technology with extreme scalability. We report on the ongoing project and give a brief overview of the product features and underlying technology.	multidimensional analysis;online analytical processing;personal computer;scalability;semiconductor consolidation	Peter Meinen;Jürgen Overhoff	2009	2009 13th European Conference on Software Maintenance and Reengineering	10.1109/CSMR.2009.11	computer science;operating system;software engineering;data mining;database;programming language;hypercube	HPC	-74.48030267076845	18.2244268401571	82933
396a8c81c133dc867aa3defe6374e29bc0a791b9	sarbanes-oxley: achieving compliance by starting with iso 17799	protection information;legislation;sarbanes oxley act;observancia;norme iso;north america;america del norte;amerique du nord;securite;amerique;information technology;securite informatique;norma iso;datos financieros;technologie information;donnee financiere;etats unis;controle;estados unidos;iso standard;aspecto juridico;financial data;computer security;legislacion;proteccion informacion;legal aspect;information protection;seguridad informatica;safety;international standards organization;compliance;control;aspect juridique;observance;america;tecnologia informacion;seguridad;check	Compliance with the Sarbanes–Oxley Act of 2002 (SOX) has been hampered by the lack of implementation details. This article argues that IT departments that have implemented ten categories of IT controls provided by the International Standards Organization (ISO 17799) will be well on their way toward SOX compliance. A side-by-side comparison of the 124 control components of the ISO Standard and the published SOX implementation guidelines is provided.	iso/iec 27002	Dwight A. Haworth;Leah Rose Pietron	2006	IS Management	10.1201/1078.10580530/45769.23.1.20061201/91775.9	check;computer science;internal control;management;law;information technology;computer security;information protection policy;scientific control	Visualization	-67.6417718283842	5.646473210652834	83028
898fad32e0b213809a24b18dc073940572174060	the role of architecture in drm vendor economics		The advent of digital rights management (DRM) has lead to the creation of two generations of DRM technologies. First generation technologies largely focused on copy protection, and because of this, many erroneously equate copy protection and DRM. Second generation technologies, however, have begun to address a much broader scope of possibilities associated with the myriad of business opportunities that can be built around the more general idea of managing rights. Nevertheless, to date, DRM vendors have experienced very little success in the marketplace. This in spite of the fact that respected analysts had predicted the growth of a multi-billion dollar DRM market by the year 2004. Experts have postulated a number of reasons for the failure of this market to appear, some related to the business models, others to the technologies themselves, and still others related to the lack of standards in this area. In this paper we consider the role that the definition of a well-defined DRM architecture will have in addressing these problems. In particular, we consider how the specification of a minimal architectural framework can help to guide the development of technologies in DRM, as well as help to identify the role that standards should play and where in the architecture they should be defined. It is important to note that a similar architectural framework was instrumental in the development of Internet technologies and standards. Furthermore, the issues faced by the telecommunications industry in the early eighties are quite similar to those currently faced by the DRM industry. Various networking technologies were available, and what was needed was a technological framework that could integrate them and still allow different kinds of services to the applications that involve communication. It was the layered architecture of the OSI model and the hourglass shape of the TCP/IP protocol suite that provided the necessary framework. It is also important to recognize that in general layered architectures act as a buffer against rapidly changing technologies. The hourglass-shaped structure of TCP/IP has helped to make the Internet universal. Subsequently, the integration of the Internet into the telecommunications industry was supported by a standardization process. Furthermore, the development of standards promoted the integration of independently developed solutions. This paper makes use of the Internet analogy in order to consider how a layered architectural framework can also promote and guide the development of the DRM industry.	copy protection;digital rights management;internet protocol suite;osi model;protocol stack;requirement	Pramod A. Jamkhedkar;Gregory L. Heileman	2005			systems engineering;architecture;the internet;business;standardization;business model;analogy;spite;marketing;copy protection;digital rights management	Web+IR	-74.68889226336938	6.656058529045943	83062
bda1b97a395a03ce7c9e9805a70af6a6067393be	improving data quality of health information systems: a holistic design- oriented approach	cost saving;market dynamics;conceptual model;data exchange;structural change;business engineering;data quality in healthcare;ehealth;system design;information management;health information system;data quality;health services;health information systems	Structural changes and increasing market dynamics in the healthcare sector intensify the health providers’ need for cost-savings and process optimisation. To address actual drawbacks the adoption of eHealth is currently seen as opportunity to improve not only effectiveness and efficiency but also quality of health services. Data quality aspects will therefore gain in importance. As the actual use of the data is outside of the systems designers’ control and in contrast to empiricaland practitionerbased research approaches it is the goal of this contribution to present a first design-oriented approach that helps systems designers to understand the reality of the different stakeholders of healthcare. For this purpose a conceptual model with 44 design elements is presented. For the analysis of the domain two different perspectives are identified. An inter-organisational view defines all elements needed to depict the boundaries of healthcare organisations in order to enable exchange, sharing and integration of data, while the intra-organisational view helps to analyse the inner organisational reality. As not only technical but mainly interand intra-organisational issues are actually restricting data exchange, the proposed conceptual model provides a holistic framework for the improvement of data quality which is the prerequisite of high quality health services.	data quality;display resolution;holism;information system;mathematical optimization	Tobias Mettler;Peter Rohner;Lars Baacke	2008			data exchange;health informatics;data quality;computer science;knowledge management;conceptual model;marketing;structural change;ehealth;management science;information quality;management;systems design	DB	-71.59388720446063	11.101304106707765	83079
4682709c8f7bdde7b9eefcc4f7af7c96837a0c2a	opportunities for innovation in a system of systems framework	technological innovation;airports;system of systems;data mining;companies;system performance;technological innovation control systems pressing systems engineering and theory transportation environmental management cybernetics usa councils system performance power system dynamics;pressing;systems analysis;innovation theory;complex problems;cities and towns;complex problems system of systems innovation theory	This paper examines various theories of innovation and sees how they apply to a system of systems framework. Systems of systems present an unusual challenge for innovation theory because some of the basic rules of the theory are broken by the system of systems paradigm. In particular, systems of systems are often deployed to solve complex problems that have a pressing need. However, pressing problems that require top system performance typically require an integrated solution — a solution where one entity controls all the components. By definition, a system of systems administrator does not control all the components and, indeed, may have control of none of them. This paper reviews various theories of innovation and applies them to an example that has previously been proposed for a system of systems.	apple sos;principle of good enough;programming paradigm;system administrator;system of systems;theory	Gary T. Anderson	2009	2009 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2009.5346883	systems analysis;system of systems;system of systems engineering;computer science;artificial intelligence;computer performance;pressing	Robotics	-66.1553282523848	8.808349518246663	83111
6d8486a46a92fa66a8b992c258173977c02f79aa	enterprise integration and interoperability in manufacturing systems: trends and issues	gestion integrada;manufacturing systems;gestion integree;technologie communication;web based applications;empresa virtual;red www;empresa numerica;entreprise virtuelle;interoperabilite;interoperabilidad;cooperation;information technology;best practice;production system;manufacturing enterprise;reseau web;systeme production;integrated management;technologie information;cooperacion;sistema produccion;communication technologies;transferencia conocimiento;digital enterprise;information and communication technology;transfert des connaissances;internet;enterprise integration;logiciel libre;security key;knowledge sharing;virtual enterprise;knowledge transfer;world wide web;software libre;communication technology;interoperability;entreprise numerique;cle securite;tecnologia informacion;manufacturing system;tecnologia comunicacion;open source software;open source;llave seguridad	Recent advances in information and communication technologies have allowed manufacturing enterprise to move from highly data-driven environments to a more cooperative information/knowledge-driven environment. Enterprise knowledge sharing (know-how), common best practices use, and open source/web based applications are enabling to achieve the concept of integrated enterprise and hence the implementation and interoperability of networked enterprises. Enterprise Integration and Interoperability in Manufacturing Systems is a key concept to face the challenges of these new environments. This paper describes challenges, trends and issues that must be addressed in order to support the generation of new technological solutions.	best practice;enterprise integration;interoperability;open-source software	Hervé Panetto;Arturo Molina	2008	Computers in Industry	10.1016/j.compind.2007.12.010	manufacturing execution system;information and communications technology;enterprise system;simulation;enterprise application integration;enterprise systems engineering;enterprise software;computer science;systems engineering;engineering;architecture domain;operations management;integrated enterprise modeling;cimosa;enterprise architecture management;computer-integrated manufacturing;enterprise architecture;enterprise integration;law;information technology;cross-domain interoperability;enterprise planning system;enterprise information security architecture;enterprise information system;enterprise life cycle	HPC	-67.6312732991215	7.018610176030842	83186
882ed315ca691b80ea410ec9777f4d23ec9e0b08	collaboration environments for small and medium-sized architecture, engineering and construction enterprises: success factors in implementation		Collaboration in the architecture, engineering and construction (AEC) sector increases productivity, improves quality, competitiveness and profitability. This poses the problem of what are the most appropriate collaboration methods available. With building information modelling (BIM) being imposed by the UK Government on AEC projects, the development of new technology is one solution, but ensuring the whole-sector adoption is fundamental. It is commonly observed that SMEs have fewer chances to get it right and so they need guidance. A qualitative study of 14 cases was developed to map the current practice of implementations and their success level. The case studies revealed a number of factors affecting the success at the organisation level. This also led to further quantitative investigation of the factors affecting the collaboration at the end-user level. The results suggested that the success of collaborative technologies depends on 23 factors. Recommendations from these findings are used to suggest a path for SMEs to implement BIM.		Aisha Abuelmaatti;Vian Ahmed	2018	IJITM	10.1504/IJITM.2018.10013506	process management;knowledge management;engineering;implementation;building information modeling;architecture;profitability index;information technology;small and medium-sized enterprises;government;qualitative research	DB	-77.0053787315389	9.14577954744959	83194
82c3321762b0a4092461e21d0c9a8524d8e2a654	a conceptual framework for delivering cost effective business intelligence solutions as a service	cloud bi;bi as a service;business intelligence;data warehouses;cloud computing;saas	Smart use of business intelligence (BI) can allow organizations to leverage the huge amounts of transactional data at their disposal and turn it into a powerful decision support mechanism that gives them competitive advantage. Despite the potential benefits of an effective BI system, the adoption and use of BI systems within the enterprise remains low, especially among smaller companies with resource constraints. This can partly be explained by the predominant deployment approach available today in which a firm needs to procure, install, configure and operate a BI system in-house. Barriers of high cost, complexity and lack of in-house expertise discourage many firms from adopting BI systems. This paper argues that adopting a cloud computing model, where BI is offered as a service over the Internet can lower these barriers and accelerate the pace of BI adoption. However, migrating BI systems from traditional on-premise environments to the cloud presents huge challenges. There are technical, economic, organizational and regulatory hurdles to overcome. Further, BI systems are multi-component (ETL, Data warehouse, data marts, OLAP, reporting, data mining etc.) and deciding which component(s) to move to the cloud, and which ones to leave on-premise needs careful consideration. In addition, the fact that cloud computing is still in its infancy means there is a general lack of conceptual and architectural frameworks to guide companies considering migrating enterprise systems to the cloud. This paper takes a closer look into traditional BI and proposes a conceptual framework that companies can use to chart an adoption path for cloud BI. The framework combines attributes of IT outsourcing, traditional BI, cloud computing as well as decision theory to present a consolidated view of cloud BI. The domain of South African Higher Education was chosen as the target in which the framework will be tested.	cloud computing;complexity;data mining;decision support system;decision theory;dynamic data;enterprise system;on-premises software;online analytical processing;outsourcing;procurement;software deployment	G. M. Muriithi;J. E. Kotzé	2013		10.1145/2513456.2513502	marketing;operations management;data mining;business intelligence;business	DB	-72.24120437414928	11.887079305455549	83218
adf2427a119278ecdc620e3f7be119ff311f21db	ngoss-based convergent oss framework using bpm for converged e-business environment	neoss;bpm and eai.;web service;ngoss;paradigm shift;operations management;value added;operations support systems	Recently, most wired telecom service carriers are faced to decreasing of the number of subscriber because of the expansion of wireless market and blocking of net earnings. To overcome that business environment, telecom service carriers try to change their service infrastructure from network-focused service to value-added and customer-focused one that can create new value. Such changes will ultimately bring forth a service and network convergence. This also entails paradigm shift in operational management and now most service carriers are rushing to build a new converged Operational Supporting System (OSS) to efficiently accommodate the network evolution. To cope with this new business environment, KT had driven NeOSS (New OSS) [1] project, which total expense is about 1.5 billion dollar, to build convergent OSS for past 3 years. This paper proposes convergent OSS framework, NeOSS framework, complied with NGOSS (New Generation Operations Systems and Software) [2] and e-TOM (Enhanced Telecom Operation Map) [3]. After that, we describe three architectural core principles considered to build NeOSS, which are business process integration by using BPM [4] (Business Process Management) technology, distributed application integration by using EAI [5] technology and web service. Lastly, we make a conclusion.	beam propagation method;blocking (computing);business process;distributed computing;electronic business;enterprise application integration;network convergence;open sound system;programming paradigm;tom;web service	Cheol-Seong Kim;Byung-Soo Chang;Sung-il Kim	2006			web service;electronic business;operations support system;engineering;systems engineering	Web+IR	-72.61917155334888	8.12737151786513	83283
e4a9c11abb16f1ed41cd8f82a5e5b1b4498978f8	complaining customers as innovation contributors: stimulating service innovation through multichannel complaint management	customer experiences service innovation customer complaint management customer integration;unique customer experience information access customer integration service innovation processes service innovation management multichannel customer complaint management;service innovation;customer experiences;customer complaint management;technological innovation companies process control context educational institutions computers;customer integration;innovation management customer satisfaction customer services	This manuscript reviews the literature on an under researched field of customer integration into service innovation processes. By combining knowledge from two substantive research areas - customer complaint management and service innovation management -, we develop and discuss a conceptual framework for a promising form of (indirect) customer integration to stimulate service innovations. Specifically, we suggest companies to make use of multichannel complaint management since it bears potential to access unique customer experience information. We further explain which multichannel complaint barriers need to be addressed particularly to stimulate service innovation.	service innovation	Julia Meik;Christian Brock;Markus Blut	2014	2014 Annual SRII Global Conference	10.1109/SRII.2014.27	service level requirement;enterprise relationship management;customer to customer;voice of the customer;service level objective;customer;innovation management;knowledge management;marketing;customer reference program;customer intelligence;business;customer satisfaction;customer service assurance;customer retention;service quality;conversion marketing;customer advocacy	HPC	-75.97585369227409	5.402939142637905	83317
bf435afb8d56c8cb62aabbfceab357055a381d51	using data envelopment analysis in software development productivity measurement	function points;bank;development effort;data envelopment analysis dea;software development;productivity;data envelope analysis	The ever-increasing size and complexity of software systems make the cost of developing and maintaining software important. Unfortunately, the process of software production has not been particularly well understood. This article helps clarify the relationship between postimplementation function points (FP) and the corresponding development effort for software development projects in a large Canadian bank. Knowledge of this relationship enables evaluations of the productivity of completed projects and, in particular, provides a predictive tool for future projects. The empirical analysis employs a combination of traditional regression models and Data Envelopment Analysis (DEA). The regression analyses show a log-linear relationship between project size and development effort, which is subsequently used in the DEA models. The DEA models identify best performers and use these as benchmarks, but are not limited to the constant returns to scale assumption of the regression analyses and are capable of including the delivery time as a nondiscretionary input. Finally, by including data from the International Software Benchmarking Standards Group (ISBSG) repository in the DEA models, the bank’s projects are benchmarked not only against its own best performers but also against what is globally feasible. Copyright  2006 John Wiley & Sons, Ltd.	benchmark (computing);data envelopment analysis;function point;function type;john d. wiley;linear algebra;linear logic;linear model;log-linear model;outsourcing;software development;software system;statistical model;time complexity	Mette Asmild;Joseph C. Paradi;Atin Kulkarni	2006	Software Process: Improvement and Practice	10.1002/spip.298	productivity;bank;economics;engineering;operations management;software development;function point;software engineering;data mining;data envelopment analysis;management;operations research	SE	-85.40678283441707	9.398555050592014	83320
050a747814b746d173d62ad75242525ff018c166	driven profiling of open standards for office applications	software;document handling;generators;standards;iso standards;vocabulary;standards document handling open systems;iec standards;xml;open systems;iso committees electronic documents open standards interoperability office application documents oasis commitee;iso standards xml iec standards software generators vocabulary	The exchange of electronic documents between organizations, administrations and citizens is of growing relevance. Connected to this, the interoperability of office application documents is of essential interest. Even though the introduction of open standards in the field of office application fundamentally increased the degree of compatibility, interoperability issues still exist. As a result of this, the idea of the utilization of interoperability profiles emerged in relating OASIS and ISO committees. This paper proposes a feature driven but widely format independent approach for the profiling of office document standards that can magnificently simplify the definition of domain specific profiles.	interoperability;profiling (computer programming);relevance	Bjoern Kirchhoff	2011	2011 7th International Conference on Standardization and Innovation in Information Technology (SIIT)	10.1109/SIIT.2011.6083612	xml;iso 29110;computer science;software engineering;database;open system;world wide web	DB	-72.31333202663193	14.214622142938273	83341
0f20ca01f1e6b419ff0dbc96b212321ba498cbcf	optimizing dual-shore sqa resource and activities in offshore outsourced software projects	optimisation;layered knowledge sharing platform;dual shore development;outsourcing;software quality outsourcing remuneration quality of service cost function lattices europe delay frequency educational institutions;project management;offshore outsourcing;cost function;lattices;life cycle;remuneration;software quality assurance;resource allocation;dual shore software quality assurance resource optimization;satisfiability;domain knowledge;user requirement dual shore software quality assurance resource optimization offshore outsourced software project user satisfaction lattice project;distributed environment;user requirement;offshore outsourced software project;knowledge sharing;user requirements;lattice project;software reengineering;europe;quality of service;frequency;software quality optimisation outsourcing project management resource allocation software development management;user satisfaction;software quality;software development management;dual shore development software quality assurance software reengineering layered knowledge sharing platform	Although offshore outsourcing software projects from USA and Europe to India, China or Russia is prevalent currently, software quality assurance activities cost more and last longer due to the communication and coordination issues in distributed environment. Having part of SQA team on-site with user is essential and the trade-off must be made between the cost and quality of service. This paper introduces our research work on how to reduce the SQA resource cost and improve the user satisfaction with optimized SQA resource and activities. Distributed groups shared the layered domain knowledge, deliverables and user requirements were arranged in layers. On-site SQA activities were prioritized to be performed. Since large amount of SQA effort was performed by offshore SQA group, only 10% SQA persons were on-site with user in Lattice project. While user requirement was satisfied in time and deliverables were of high quality, the entire cost of SQA resource was reduced due to the shortened life cycle and minimized on-site SQA group	customer relationship management;display resolution;dynamic dispatch;optimizing compiler;outsourcing;quality of service;requirement;software quality analyst;software quality assurance;user requirements document;wrapping (graphics)	Bin Xu;Xiao-Ping Pan	2006	2006 Canadian Conference on Electrical and Computer Engineering	10.1109/CCECE.2006.277486	project management;computer science;systems engineering;user requirements document	SE	-67.01692921087123	20.845626619099693	83454
9104cf1915a6dedcfa9e7611ed3a54d9f26a3dd8	a conceptual framework and a suite of tools to support crisis management		This article aims at describing an approach to support crisis management. The main idea is to use an original vision of Big-Data to manage the question of collaboration issues in crisis response. On the one hand, this article introduces a general framework that structures the methodology applied in our approach. This framework includes several technical and business dimensions and embeds scientific results that are presented in this article or have been described in previous articles. On the other hand, the resulting implemented suite of tools is also presented with regards to the conceptual framework. Finally, in order to emphasize all the main features described in this article, both the framework and the suite of tools are illustrated and put into action through a scenario extracted from a real exercise.	big data;business continuity;conceptual schema	Frédérick Bénaben;Aurélie Montarnal;Sébastien Truptil;Matthieu Lauras;Audrey Fertier;Nicolas Salatgé;Sebastien Rebiere	2017			model-driven architecture;knowledge management;environmental resource management;management;information system;collaboration	Visualization	-69.34429269784091	9.461147858561766	83455
25831b14ff3201ba595ee47c9347b576a34b15cd	knowledge-sharing issues in experimental software engineering	experimental design;knowledge management;empirical software engineering;software engineering;experimental software engineering;knowledge sharing;experimental replication;software reading techniques	Recently the awareness of the importance of replicating studies has been growing in the empirical software engineering community. The results of any one study cannot simply be extrapolated to all environments because there are many uncontrollable sources of variation between different environments. In our work, we have reasoned that the availability of laboratory packages for experiments can encourage better replications and complementary studies. However, even with effectively specified laboratory packages, transfer of experimental know-how can still be difficult. In this paper, we discuss the collaboration structures we have been using in the Readers’ Project, a bilateral project supported by the Brazilian and American national science agencies that is investigating replications and transfer of experimental know-how issues. In particular, we discuss how these structures map to the Nonaka–Takeuchi knowledge sharing model, a well-known paradigm used in the knowledge management literature. We describe an instantiation of the Nonaka–Takeuchi Model for software engineering experimentation, establishing a framework for discussing knowledge sharing issues related to experimental software engineering. We use two replications to illustrate some of the knowledge sharing issues we have faced and discuss the mechanisms we are using to tackle those issues in Readers’ Project.	bilateral filter;experiment;experimental software engineering;extrapolation;knowledge management;programming paradigm;universal instantiation	Forrest Shull;Manoel G. Mendonça;Victor R. Basili;Jeffrey C. Carver;José Carlos Maldonado;Sandra Camargo Pinto Ferraz Fabbri;Guilherme Horta Travassos;Maria Cristina Ferreira de Oliveira	2004	Empirical Software Engineering	10.1023/B:EMSE.0000013516.80487.33	personal software process;verification and validation;software engineering process group;software mining;computer science;systems engineering;engineering;knowledge management;package development process;social software engineering;software framework;software development;software engineering;software construction;management science;software walkthrough;resource-oriented architecture;design of experiments;management;software requirements	SE	-66.16768714475769	25.002101130139174	83460
3a95439ddda2e4976f7948639c3305304f101d66	business model dynamics: a case survey	different stage;case survey;business model;business models;business model design;market-related force;business model dynamics;new business model;business model dynamic;market-related driver;external driver;external factor;early stage;start-ups;life cycle;e commerce	In the turbulent world of e-commerce, companies can only survive by continuously reinventing their business models. However, because most studies look at business models as snapshots in time, there is little insight into how changing market-related, technological and regulatory conditions generally drive revisions in business models. In this paper, we examine which types of external drivers are strongest in forcing business models to change throughout their life cycle. To do so, we study 45 longitudinal case descriptions on business model dynamics of (networks of) organizations in various industries. The results of this survey indicate that technological and market-related forces are the most important drivers of business model dynamics, while regulation plays only a minor role. In particular for start-ups, the effect of technological and market-related drivers is the strongest in the early stages of a new business model, while the effects are moderate over time for established, large companies. Our results provide clues to practitioners on what external factors to take into account in different stages of business model design and redesign.	conceptualization (information science);device driver;disk sector;e-commerce;enterprise life cycle;exception handling;external validity;interpro;international standard serial number;napster;turbulence	Mark de Reuver;Harry Bouwman;Ian MacInnes	2009	JTAER		e-commerce;business model;business analysis;business transformation;product-service system;business process reengineering;computer science;artifact-centric business process model;marketing;operations management;business case;electronic business;business relationship management;management;new business development;line of business	Web+IR	-79.35216069649306	5.730726661797284	83608
774175c0b2b24865dc72f14e7aa11737d18e2b85	the innovative application of cloud computing on auditing	auditing;caat;coa;mobile communication;eams;computer assisted audit techniques;continuous online auditing;embedded audit modules;cloud computing	Facing an increasingly complex business environment, companies implement information systems to improve their efficiency and effectiveness. Using computers as an auditing tool has become an important issue. Furthermore, tools of continuous auditing have been applied to improve it but because of high cost, information security or interest conflict, most of them are not adopted by enterprises. This paper constructs the concept of auditing from the innovative service SaaS of cloud computing and explores benefits of the application of cloud computing on auditing in practice. We also hope to provide a reference for auditors and enterprises planning to adopt it. This paper also uses data from a purchase to pay cycle to set up a simulated company and conducts experiments on students utilising a company simulation case study.	cloud computing	Shaio Yan Huang;Ching-Wen Lin;Yi-Feng Jian	2014	IJMC	10.1504/IJMC.2014.061463	information security audit;mobile telephony;cloud computing;computer science;knowledge management;marketing;operating system;audit;world wide web;computer security	HPC	-79.6204972593942	12.746947575522562	83618
bcb0dba49f06ad2a2ae83ce627b3c899c2ac3e13	simulation software engineering: experiences and challenges	software engineering;simulation software	Using software for large-scale simulations has become an important research method in many disciplines. With increasingly complex simulations, simulation software becomes a valuable assest. Yet, the quality of many simulation codes is worrying. In this paper, we want to collect and structure the challenges for a systematic simulation software engineering as a reference and the basis for further research. We describe our own experiences with developing simulation software and collaborating with non-computer-scientists. We complement our experienced challenges with a brief literature review. We structured the challenges for simulation software engineering into six areas: motivation and recognition; education and training; developer turnover; software length of life; verification, validation and debugging; and efficiency vs. maintainability. Overcoming these challenges needs efforts from research agencies, scientific computing researchers as well as software engineering researchers.	code;computational science;debugging;simulation software;software engineering	Stefan Wagner;Dirk Pflüger;Miriam Mehl	2015		10.1145/2830168.2830171	personal software process;verification and validation;software engineering process group;simulation software;software verification;search-based software engineering;computer science;package development process;social software engineering;component-based software engineering;software development;software construction;software walkthrough;software analytics;resource-oriented architecture;software deployment;software requirements;software quality;software system;software peer review	SE	-65.49302938469678	25.437212772206617	83623
4ef0635800fe49f8d624fdb53270a239a220c361	product intelligence in industrial control: theory and practice		This paper explores the evolving industrial control paradigm of product intelligence. The approach seeks to give a customer greater control over the processing of an order – by integrating technologies which allow for greater tracking of the order and methodologies which allow the customer [via the order] to dynamically influence the way the order is produced, stored or transported. The paper examines developments from four distinct perspectives: conceptual developments, theoretical issues, practical deployment and business opportunities. In each area, existing work is reviewed and open challenges for research are identified. The paper concludes by identifying four key obstacles to be overcome in order to successfully deploy product intelligence in an industrial application.		Duncan C. McFarlane;Vaggelis Giannikas;Alex C. Y. Wong;Mark Harrison	2013	Annual Reviews in Control	10.1016/j.arcontrol.2013.03.003	computer science;engineering;artificial intelligence;management science;operations research	Logic	-65.30795427057808	9.5652635508473	83721
f860e4966b88de018b32d4cdae87a7ba7efc2561	a model for inbound supply risk analysis	gestion integrada;modelizacion;analyse risque;gestion integree;factor riesgo;analytic hierarchy process;logistique;processus hierarchie analytique;risk factor;fournisseur;risk analysis;hierarchized structure;gestion risque;risk management;integrated management;structure hierarchisee;supplier;vulnerability;facteur risque;limit set;modelisation;analisis riesgo;risk factors;vulnerabilite;hierarchical classification;vulnerabilidad;logistics;analytical hierarchy process ahp;retard;literature review;proceso jerarquia analitico;classification hierarchique;supply chain;prototype implementation;gestion riesgo;inbound supply risk analysis;supply chain risk management;retraso;modeling;clasificacion jerarquizada;estructura jerarquizada;supply chain management;proveedor;logistica	Managing risk has become a critical component of supply chain management. The implications of supply chain failures can be costly and lead to significant customer delivery delays. Though, different types of supply chain vulnerability management methodologies have been proposed for managing supply risk, most offer only point-based solutions that deal with a limited set of risks. This research aims to reinforce inbound supply chain risk management by proposing an integrated methodology to classify, manage and assess inbound supply risks. The contributions of this paper are four-fold: (1) inbound supply risk factors are identified through both an extensive academic literature review on supply risk literature review as well as a series of industry interviews; (2) from these factors, a hierarchical risk factor classification structure is created; (3) an analytical hierarchy processing (AHP) method with enhanced consistency to rank risk factor for suppliers is created; and (4) a prototype computer implementation system is developed and tested on an industry example. # 2006 Elsevier B.V. All rights reserved.	analytical hierarchy;it risk management;inbound marketing;inventory;microsoft forefront;multitier architecture;prototype;scalability;supply chain attack;usability;vulnerability management	Teresa Wu;Jennifer Blackhurst;Vellayappan Chidambaram	2006	Computers in Industry	10.1016/j.compind.2005.11.001	supply chain risk management;supply chain management;risk management;service management;engineering;marketing;operations management;operations research;risk factor	AI	-68.11126259889	5.546563010015049	83736
5c217673c721b4f82f6d7e3b07a1e66a1f3b1950	glass ceilings in portugal?: an analysis of the gender wage gap using a quantile regression approach	wage gap;quantile regression;wage discrimination;gender;gender wage gap;glass ceilings;glass ceiling	Despite the evidence of female progress with regard to women’s role in the labor market, gender inequality remains. Women are still less likely to be employed than men, occupational gender segregation continues, and females continue to earn less than males. The gender wage gap remains wide in several occupational sectors, among which is the information technology (IT) sector. This paper focuses the determinants of gender wage inequality. More precisely, it investigates for statistical evidence of a glass ceiling effect on women’s wages. Based on the quantile regression framework, the empirical analysis extends the decomposition of the average gender wage gap to other parts of the earnings distribution. The main objective is to empirically test whether gender-based wage discrimination is greater among high paid employees, in line with glass ceiling hypothesis. Larger unexplained gaps at the top of the wage distribution indicate the existence of a glass ceiling effect in Portugal. DOI: 10.4018/978-1-4666-0924-2.ch004	glass;social inequality	Raquel Mendes	2010	IJHCITP	10.4018/jhcitp.2010040101	quantile regression;economics;computer science;economy;efficiency wage;welfare economics;labour economics	Web+IR	-86.31435633750307	6.8118736586388025	83741
32e7d71001cd9cb4cdd1bd68317cb289b0622a61	a multi-method for defining the organizational change	change process modeling;computacion informatica;change management;grupo de excelencia;ciencias basicas y experimentales;organizational change;guidance;enterprise modeling;change process;intention driven modeling	The assumption of the work presented in this paper is the situatedness of the change process. The Enterprise Knowledge Development Change Management Method (EKD-CMM) provides multiple and dynamically constructed ways of working to organize and to guide the change management. The method is built on the notion of labeled graph of intentions and strategies called a road map and the associated guidelines. The EKD-CMM road map is a navigational structure that supports the dynamic selection of the intention to be achieved next and the appropriate strategy to achieve it whereas guidelines help in the operationalization of the selected intention following the selected strategy. This paper presents the EKD-CMM road map and guidelines and exemplifies their use with a real case study.	bus mastering;business process;capability maturity model;change management (engineering);emoticon;graph labeling;knowledge modeling;organizational behavior;plan;process modeling;quality of results;situated;software deployment;top-down and bottom-up design	Selmin Nurcan;Colette Rolland	2003	Information & Software Technology	10.1016/S0950-5849(02)00162-3	enterprise modelling;engineering;knowledge management;change management;management science;management	HCI	-63.42673685470048	14.143137552440386	83854
fda6b25089208e4b6d355fb4c9ffe51937047cf5	data-driven optimization and knowledge discovery for an enterprise information system		This book provides a comprehensive set of optimization and prediction techniques for an enterprise information system. Readers with a background in operations research, system engineering, statistics, or data analytics can use this book as a reference to derive insight from data and use this knowledge as guidance for production management. The authors identify the key challenges in enterprise information management and present results that have emerged from leading-edge research in this domain. Coverage includes topics ranging from task scheduling and resource allocation, to workflow optimization, process time and status prediction, order admission policies optimization, and enterprise service-level performance analysis and prediction. With its emphasis on the above topics, this book provides an in-depth look at enterprise information management solutions that are needed for greater automation and reconfigurability-based fault tolerance, as well as to obtain data-driven recommendations for effective decision-making.	enterprise information system;program optimization	Qing Duan;Krishnendu Chakrabarty;Jun Zeng	2015		10.1007/978-3-319-18738-9	enterprise system;enterprise systems engineering;enterprise software;knowledge management;architecture domain;data science;integrated enterprise modeling;enterprise architecture management;data mining;enterprise architecture;knowledge extraction;enterprise integration;enterprise information security architecture;enterprise information system;enterprise life cycle	ML	-66.97259145280285	12.371271863175002	83870
dd327b099a98ea7c1e8edc9244e1a19889d7323b	product innovation is practical, important, and possible	software;technological innovation;team learning;experience;satisfiability;software engineering;experience report;customer satisfaction;software packages customer satisfaction innovation management software development management;innovation management;high priority;product designer;user experience;business;schedules;software product design;technological innovation schedules product design user interfaces software business software engineering;user experience product innovation product designer experience user satisfaction;customer satisfaction software product innovation software product design;product design;user satisfaction;user interfaces;software development management;software product innovation;software packages;product innovation	This experience report describes how an agile delivery team and product designer become progressively more sophisticated with product design and product innovation over the course of three separate software products. The team learned that despite having on-time product deployments and loads of high-priority features, customers were satisfied but not thrilled unless there was product design and innovation. This report introduces the techniques the team used to inject innovation into its software products and tells the story how even a small amount of effort spent on product design and innovation can make a big difference in customer satisfaction.	agile software development	Theresa Smith	2008	Agile 2008 Conference	10.1109/Agile.2008.85	product innovation;innovation management;systems engineering;knowledge management;marketing;design review;scrum;business;product management;product design;new product development;product engineering	Metrics	-67.9063467250448	25.738655876022957	84040
c1850ba10412968c22f83de37d42a0235e144881	point: software lives too long	programming language;software maintenance;automatic programming software maintenance application software computer languages magnetic heads operating systems buildings hardware software reliability aging;utility function;hardware systems software engineering methodology old programs requirements traceability program maintainability utility functions databases operating systems design constructs programming language features;calendar year;software engineering;operating system;requirements traceability	For many years, I looked longingly at old Victorian mansions and thought about how nice it would be to live in one of those beautiful houses. Fortunately, I have never had that experience. I have, however, had the opportunity to talk to many people who have occupied them, and most tell me that an old house is a maintenance nightmare. It was built to yesterday’s technological standards. There is no insulation in the outer walls. Its limited electrical wiring is clearly inadequate for today’s power needs, and the decaying wires in the walls create a severe electrical hazard. The plumbing is limited and often needs complete replacement. To live in an old house is to return to the primitive past with all of its limitations in comfort and convenience. The people who lived in these old houses when they were built were accustomed to privations that modern folks are not. THIS OLD APPLICATION	hazard (computer architecture);windows me;wiring	John C. Munson	1998	IEEE Software	10.1109/52.687938	verification and validation;computing;system requirements;computer science;systems engineering;engineering;software design;social software engineering;software framework;component-based software engineering;software development;software engineering;software construction;programming language;software maintenance;software requirements;requirements traceability;software system	Arch	-68.90668626254576	27.751433729281423	84086
e17631c1ab40be7fab9fc376cc244ae618ab4e6b	a new network epsilon-based dea model for supply chain performance evaluation	network data envelopment analysis;supply chain performance evaluation;epsilon based measure	Supply chain performance evaluation problems are inherently complex problems with multilayered internal linking activities and multiple entities. Data Envelopment Analysis (DEA) has been used to evaluate the relative performance of organizational units called Decision Making Units (DMUs). However, the conventional DEA models cannot take into consideration the complex nature of supply chains with internal linking activities. Network DEA models using radial measures of efficiency are used for supply chain performance evaluation problems. However, these models are not suitable for problems where radial and non-radial inputs and outputs must be considered simultaneously. DEA models using Epsilon-Based Measures (EBMs) of efficiency are proposed for a simultaneous consideration of radial and non-radial inputs and outputs. We extend the EBM model and propose a new Network EBM (NEBM) model. The proposed NEBM model combines the radial and non-radial measures of efficiency into a unified framework for solving network DEA problems. A case study is presented to exhibit the efficacy of the procedures and to demonstrate the applicability of the proposed method to a supply chain performance evaluation problem in the semiconductor industry. 2013 Elsevier Ltd. All rights reserved.	data envelopment analysis;ebxml;entity;performance evaluation;radial (radio);semiconductor industry;subroutine;supply chain attack;unified framework	Madjid Tavana;Hadi Mirzagoltabar;Seyed Mostafa Mirhedayatian;Reza Farzipoor Saen;Majid Azadi	2013	Computers & Industrial Engineering	10.1016/j.cie.2013.07.016	reliability engineering;engineering;operations management;data envelopment analysis;operations research	AI	-81.8550699847424	9.9930560820063	84171
ff751d21c99907e58eb6c7c943a0cead24aaccb6	the challenges of case design integration in the telecommunication application domain	telecommunication software;case environment;software process model;software reusability;software product line	The magnitude of the problems facing the telecommunication software industry is presently at a point at which software engineers should become deeply involved. This paper presents a research project on advanced telecommunication technology carried out in Europe, called BOOST (Broadband Object-Oriented Service Technology). The project involved cooperative work among telecommunication companies, research centres and universities from several countries. The challenges to integrate CASE tools to support software development within the telecommunication application domain are discussed. A software process model that encourages component reusability, named the X model, is described as part of a software life cycle model for the telecommunication software industry.	application domain;boost;computer-aided software engineering;process modeling;software development process;software engineer;software industry;software release life cycle	Luiz Fernando Capretz	2003	Transactions of the SDPS		personal software process;long-term support;verification and validation;software sizing;software project management;systems engineering;engineering;package development process;backporting;social software engineering;component-based software engineering;software development;software design description;software engineering;software construction;software walkthrough;resource-oriented architecture;software deployment;software development process;software requirements;software system;computer engineering;software peer review	SE	-65.96627773364972	20.97880961658052	84185
22de8e121c8fabcfb64d45afd6f39e1bd5b801cc	analysing firm performance in chinese it industry: dea malmquist productivity measure	convergence;efficiency;information technology;dea;technical diffusion;efficiency convergence;data envelopment analysis;malmquist productivity;malmquist productivity index;it industry;performance measurement;china;firm performance	Chinese IT industry has become more important and maturity after development for tens of years and come up quickly in global IT market. They may have huge influence on Chinese IT market or even the world. This paper is concerned with the study on exploring the performance of Chinese IT industry, including the managerial, technical and scale efficiencies and their changes over time. We employ data envelopment analysis (DEA)-based Malmquist method to measure the performance of listed IT firms in China, in the period of 2005 to 2007 and reveal the detailed technology and efficiency changes over time by analysing decomposed components of Malmquist index. Furthermore, the technical diffusion of Chinese IT industry is tested by efficiency convergence analysis. Accordingly, the IT companies can make decisions on the functions and strategies shifts that are beneficial to the performance improvement and achieving competitive advantages.	capability maturity model;data envelopment analysis	Xiaohong Chen;Xiaoding Wang;Desheng Dash Wu;Zejing Zhang	2011	IJITM	10.1504/IJITM.2011.037759	performance measurement;convergence;economics;computer science;operations management;data envelopment analysis;efficiency;management;law;information technology;china;commerce	AI	-83.52049721503847	6.827649603624879	84250
236e8fa49f49b2878b87d2132fb4b7efd7267a0d	architectures, coordination, and distance: conway's law and beyond	collaborative work documentation application software scheduling process design real time systems embedded system time to market programming stress;project management;coordination development teams project communication project schedules modular design distributed development teams communication;modular design;project scheduling;software development management;geographic distribution;project management software development management	"""Software engineering researchers have long argued that the architecture of a system plays a pivotal role in coordinating development work. Over 30 years ago, Melvin Conway proposed what has since become known as Conway's Law—that the structure of the system mirrors the structure of the organization that designed it. 1 This relation, Conway argued, is a necessary consequence of the communication needs of the people doing the work. David Parnas, in fact, defined a software module as """" a responsibility assignment rather than a subprogram, """" 2 driving home the idea that modular design enables decisions about the internals of each module to be made independently. Of course, the computer that runs the software doesn't care. The point of structure is to support coordination of the development work. Architecture, however, addresses only one of the several dimensions on which we must coordinate development. To support efficient use of resources, projects require plans that specify when milestones must be completed and who will do the Architectures, Coordination, and Distance: Conway's Law and Beyond Geographically distributed development teams face ex traordinary communication and coordination problems. The authors' case study clearly demonstrates how common but unanticipated events can stretch project communication to the breaking point. Project schedules can fall apar t, par ticularly during integration. Modular design is necessary, but not sufficient to avoid this fate."""	conway's game of life;conway's law;modular design;software engineering;subroutine	James D. Herbsleb;Rebecca E. Grinter	1999	IEEE Software	10.1109/52.795103	project management;extreme project management;real-time computing;simulation;project;software project management;computer science;systems engineering;engineering;software engineering;project management 2.0;project management triangle;management;modular design;schedule;software development process;project planning;project portfolio management	SE	-67.06062402748618	16.637376295933365	84428
98e05b36445d04ce3c5f149561fc2adecc6c9bf7	understanding the role of requirements artifacts in kanban	requirements artifacts;collaboration;user stories;kanban	User stories are a well-established way to record requirements in agile projects. They can be used as such to guide the daily work of developers or be split further into tasks, which usually represent more technical requirements. User stories and tasks guide communication and collaboration in software projects. However, there are several challenges with writing and using user stories in practice that are not well documented yet. Learning about these challenges could raise awareness for potential problems. Understanding how requirements artifacts are used for daily work could lead to better guidelines on writing stories that support daily work tasks. Moreover, user stories may not be appropriate to capture all kinds of requirements that are relevant for a project. We explore how to utilize requirements artifacts effectively, what their benefits and challenges are, and how their scope granularity affects their utility. For this, we studied a software project carried out in the Software Factory at the Department of Computer Science, University of Helsinki. We investigated the requirements artifacts and then interviewed the developers and the customer about their experiences. Story and task cards have helped the participants throughout the project. However, despite having a Kanban board and rich communication within the team, some requirements were still too implicit, which also led to misunderstandings. This and other challenges revealed by the study can guide future in-depth research.	agile software development;computer science;experience;requirement;software factory;software project management;user story	Olga Liskin;Kurt Schneider;Fabian Fagerholm;Jürgen Münch	2014		10.1145/2593702.2593707	requirements analysis;requirements management;simulation;systems engineering;engineering;knowledge management;requirement;software engineering;management;user story;kanban;collaboration	SE	-67.67945453593634	22.75477579742953	84454
a3ee876f8fdf6c9dc743fc8414cac95c8ec8b717	mapping of soa and rup: doa as case study	rational unified process;multi user;software engineering;profitability;service oriented architecture	SOA (Service Oriented Architecture) is a new trend towards increasing the profit margins in an organization due to incorporating business services to business practices. Rational Unified Process (RUP) is a unified method planning form for large business applications that provides a language for describing method content and processes. The well defined mapping of SOA and RUP leads to successful completion of RUP software projects to provide services to their users. DOA (Digital Office Assistant) is a multi user SOA type application that provides appropriate viewer for each user to assist him through services. In this paper authors proposed the mapping strategy of SOA with RUP by considering DOA as case study.	direction of arrival;multi-user;office assistant;rational unified process;service-oriented architecture;whole earth 'lectronic link	Shahid Hussain;Sheikh Muhammad Saqib;Bashir Ahmad;Shakeel Ahmad	2010	CoRR		systems engineering;engineering;software engineering;service-oriented architecture;rational unified process;profitability index	SE	-64.53221577817014	21.466300274718048	84480
1ac3f833387e73c4b33f54c9b0071a2b451c2c01	structural analysis on team internal soft factors to project success	eigenvalues and eigenfunctions;cultural difference;pragmatics;project management;fuzzy dematel technique;human psychological concern;project manager;internal service;loading;exploratory factor analysis;fuzzy decision making trial and evaluation laboratory;fuzzy set theory;team working;customer satisfaction;fuzzy logic;decision making trial and evaluation laboratory dematel;team working customer satisfaction decision making fuzzy set theory project management software development management;project success structural analysis failure rate project management customer satisfaction conducive environment high quality product project team internal soft factor exploratory factor analysis fuzzy decision making trial and evaluation laboratory fuzzy dematel technique human psychological concern;project success;high quality product;failure rate;conducive environment;cultural differences decision making pragmatics loading project management correlation eigenvalues and eigenfunctions;correlation;structural analysis;software development management;internal service exploratory factor analysis decision making trial and evaluation laboratory dematel fuzzy logic project management;structure analysis;project team internal soft factor;cultural differences	Project has witnessed a high rate of failure. The project management is concerned with disciplines that better manage the project inherent strengths and weaknesses. In contrast to the hard factors, soft factors are very often ignored during project management. Such factors, involved with the internal services within the team, are concerned with working with people, ensuring customer satisfaction and creating a conducive environment for the project team to deliver high quality products which meeting stakeholder expectations. This research aims to investigate the intertwined effects of project team internal soft factors. Combining the exploratory factor analysis and fuzzy decision making trial and evaluation laboratory (DEMATEL) technique, an empirical case is demonstrated to show the effectiveness of the proposed method. The exploratory factor analysis is applied for extracting the dimension and criteria structure. The fuzzy DEMATEL technique is then used to analyze the intertwined effect. The proposed method has proven to be an effective one for analyzing the complex interrelation of human psychological concerns.	customer relationship management;display resolution;exploratory factor analysis;structural analysis	Don Jyh-Fu Jeng	2011	2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011)	10.1109/FUZZY.2011.6007335	project management;computer science;knowledge management;artificial intelligence;management science;structural analysis;project management triangle;pragmatics	SE	-81.65376226188707	13.428544386062491	84600
67717a7b0212867c1c3e974b14e9485084a22073	sei capability maturity model's impact on contractors	assessment and evaluation;software cost estimation;software process improvement;dp industry;contracts;software engineering;capability maturity model coordinate measuring machines costs software engineering programming government total quality management software measurement software quality job shop scheduling;software houses;total quality management;capability maturity model;department of defence sponsorship software engineering institute s capability maturity model contractors software development companies quality software government agencies evaluations assessments voluntary internal efforts improvement independent assessor s examination maturity level costs savings efficient software engineering practices reduced software production costs;software development;government policies;economics;production cost;government policies software engineering dp industry economics software development management software houses software cost estimation cost benefit analysis contracts software quality;cost benefit analysis;software quality;software development management;software engineering practices	The Software Engineering Institute's Capability Maturity Model measures software development companies' ability to produce quality software within budget and on schedule. The CMM rates software development companies and government agencies at one of five maturity levels. Two key features are assessments and evaluations. Assessments represent a company's or government agency's voluntary, internal efforts to identify its current CMM maturity level and areas needing improvement. Evaluations represent an independent assessor's examination of a company's maturity level. Industry responses to the model vary. Some companies have reported that savings significantly offset the costs of achieving and maintaining a maturity level. Others have attacked the CMM on many fronts, such as its failure to incorporate Total Quality Management principles. Nevertheless, either enthusiastically or reluctantly companies are attempting to achieve higher CMM levels. The authors present both favorable and unfavorable reactions to the model as well as insight gained through discussions with various companies. They conclude that since no approach that enforces improvements will be universally acceptable in all aspects to all concerned, the CMM, on balance, can be considered a very successful model, particularly when combined with TQM principles. There may be uncertainty as to whether the costs of attaining and maintaining a CMM level will be recouped through reduced software production costs and more efficient software engineering practices (as published studies report). But with continued strong DoD sponsorship, there will likely be an increasing number of companies that base their software process improvement efforts on the CMM. >	capability maturity model;software engineering institute	Hossein Saiedian;Richard Kuzara	1995	IEEE Computer	10.1109/2.362633	public policy;total quality management;cost–benefit analysis;software development;software engineering;management;computer security;capability maturity model;people capability maturity model;software quality	Vision	-70.12013932435843	20.540738393862227	84616
893a38bb054614aabafaba131504a9fb3fee7d11	knowledge for translating management innovation into firm performance		This paper examines the role of tacit and explicit knowledge in translating innovation measures into firm performance in Japanese companies. While innovation has been found to be a source of higher firm performance, this research is considering whether innovation measures adopted by the firm trans‐ late directly into higher firm performance or whether these innovation meas‐ ures generate tacit and/or explicit knowledge which themselves produce higher corporate performance. Using a questionnaire survey and conditional process analysis, this paper found that there was no direct effect of innovation measures onto firm perform‐ ance, and that instead, both tacit and explicit knowledge fully mediated the rela‐ tionship between innovation measures and firm performance. Previous research did not consider the role of knowledge as interface to translate management inno‐ vation into firm performance. This paper uncovers the mediating role of knowl‐ edge, potentially elucidating past inconclusive results.		Rémy Magnier-Watanabe;Caroline Benton	2017		10.1007/978-3-319-62698-7_14	explicit knowledge;business;tacit knowledge;innovation management;knowledge management	HPC	-81.52905132504284	4.257970995468623	84694
10185568843d2edf5630b635ff3fc362117d3161	reliability improvement of major defense acquisition program cost estimates - mapping dodaf to cosysmo	defense;cosysmo;as02 government defense and security;systems engineering metrics;and security;dodaf;measurement reliability;cost estimation;as02 government	Major defense acquisition programs (MDAPs) are notoriously prone to substantial cost overruns, and engineering and design issues are often partially to blame. Accordingly, as the Department of Defense (DoD) develops increasingly complex systems, additional emphasis is being placed on systems engineering to ensure that programs remain on schedule and within budget. Notably, in 2013 the DoD mandated the submission of DoD Architecture Framework (DoDAF)models during a program’s Materiel Solution Analysis Phase—early in the system life cycle—a period historically characterized by a lack of system specification and parametric estimation. Capitalizing on this change, this paper establishes an explicit connection between the DoDAF and the Constructive Systems Engineering Cost Model (COSYSMO) with the goal of improving the measurement reliability of MDAP cost estimates. In particular, we employ techniques from text mining and social network analysis to produce a mapping between DoDAF’s 52 models and COSYSMO’s 18 drivers, yielding an untapped source of information for determining functional system size and, ultimately, estimating systems engineering effort. C⃝ 2015 Wiley Periodicals, Inc. Syst Engin 18: 530–	algorithm;artificial intelligence;cosysmo;complex systems;data mining;department of defense architecture framework;information source;john d. wiley;molecular systems biology;monte carlo method;parametric polymorphism;scott continuity;simulation;social network analysis;soft systems methodology;system lifecycle;systems engineering;text mining	Ricardo Valerdi;Matthew Dabkowski;Indrajeet Dixit	2015	Systems Engineering	10.1002/sys.21327	reliability engineering;cosysmo;economics;systems engineering;engineering;management;computer security;cost estimate	SE	-64.78142503491765	20.468575850322505	84719
24f7b6eabcce6e0d933ec6485b15e2c6edb3bf80	ecology of technology: a perspective	processus innovation;factor humano;developpement industriel;science and technology;technology;innovation process;hombre;techno giants;eurocentrism;proceso innovacion;ecology;demilitarization;estructura social;human factor;cambio social;technologie;human;social change;social structure;facteur humain;changement social;structure sociale;human centered;homme;industrial development;tecnologia	Science and technology are on trial due to the rapid changes — neither university nor science lead developments in technology, the most advanced military technology has lost linkages with industries, the widened North-South gaps — they are all sources of crisis in the global ecological balance. The Euro-centric universalism is useless to solve the global technology problems.	ecology	Takeshi Hayashi	1993	AI & SOCIETY	10.1007/BF01908608	social science;human factors and ergonomics;social structure;social change;sociology;technology	EDA	-86.58346961900546	4.576436588677829	84731
9c021dccf86e4ccf027d513423f7317790c491ed	measuring flexibility, descriptive complexity, and rework potential in generic system architectures	team;layered hierarchy;grid networks;descriptive complexity;tree hierarchy;rework;architecture metrics;rework potential	A systemu0027s architecture defines its flexibility-the ease with which changes to the systemu0027s structure may be made. Systems may possess an internal structure that is relatively complex to describe-especially after many changes. In addition, some changes may require several iterations before stakeholders converge on a final design choice. There is currently no unified mathematical theory that captures these factors. We developed metrics for flexibility, descriptive complexity, and rework potential defined on graph-based representations of a systemu0027s structure. We applied these metrics to four idealized generic system architectures: Tree-structured hierarchies are easy to describe and require minimal design iteration. However, they permit relatively few changes, unless the underlying architecture itself is changed. Furthermore, each such change adds significant complexity. Grid networks are more flexible than trees, but also more descriptively complex. Grids also require some degree of design iteration between stakeholders. Teams are extremely flexible but the requirement for consensus e.g., due to cognitive limits among humans restricts their size. Layered hierarchies possess moderate flexibility and require fewer iterations than corresponding grids and teams, although they can be difficult to describe succinctly. Our findings suggest that no architecture is ideal under all circumstances; rather, each has strengths and weaknesses that can be exploited in different environments.	descriptive complexity theory;rework (electronics)	David A. Broniatowski;Joel Moses	2016	Systems Engineering	10.1002/sys.21351	reliability engineering;computer science;systems engineering;engineering;operations management;descriptive complexity theory	SE	-78.99521789514279	14.392107934122869	84766
afb88f301d46807ff253ff67d3107b94bc93f40e	analyzing the impact of social attributes on commit integration success		As the software development community makes it easier to contribute to open source projects, the number of commits and pull requests keep increasing. However, this exciting growthrenders it more difficult to only accept quality contributions. Recent research has found that both technical and social factors predictthe success of project contributions on GitHub. We take this question a step further, focusing on predicting continuousintegration build success based on technical andsocial factors involved in a commit. Specifically, we investigated if social factors (such as being acore member of the development team, having a large number of followers, or contributing a large number of commits) improve predictions of buildsuccess. We found that social factors cause a noticeable increasein predictive power (12%), core team members are more likely to pass the buildtests (10%), and users with 1000 or more followers are more likely topass the build tests (10%).	commitment scheme;continuous integration;decimation (signal processing);display resolution;open-source software;proxy server;software development	Mauricio Soto;Zack Coker;Claire Le Goues	2017	2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)		data mining;software development;computer science;decision tree;software;commit;social network	SE	-71.2989689276759	25.68229416703488	84800
b73fdb2852c693f3843cbfe81919bb8820204073	certics assessment methodology for software technological development and innovation	spice assessment reference model assessment method iso iec 15504;assessment method;iso iec 15504;certics assessment methodology iso iec 15504 spice standard methodological references software assessment methodology public policy instrument brazilian government software development organizations software technological innovation software technological development;software engineering government iec standards iso standards;assessment reference model;spice;software technological innovation organizations standards organizations iso standards iec standards	Technological development and innovation are key drives for software development organizations. Furthermore they are strategic for the growth of a region or a Country. Therefore, the Brazilian government established a public policy instrument to identify and stimulate software resulting from technological development and innovation in the Country. Hence a software assessment methodology, named as CERTICS, has been created and established in Brazil. Its construction has been based on the reality of software development organizations and guided by methodological references including the ISO/IEC 15504 (SPICE) Standard. CERTICS includes a reference model, an assessment method and an arrangement for its application, monitoring and continuous improvement. A software organization can also benefit from CERTICS as good practices reference model on technology development and innovation. This article presents an overview of the rationality, design, major components and early practical results of CERTICS Methodology version 1.1.	iso/iec 15504;rationality;reference model;spice;software development;usb flash drive	Angela Maria Alves;Clenio F. Salviano;Giancarlo Nuti Stefanuto;Sonia T. Maintinguer;Carolina V. Mattos;Camila Zeitoum;Márcia Regina Martins Martinez;Giancarlo Reuss	2014	2014 9th International Conference on the Quality of Information and Communications Technology	10.1109/QUATIC.2014.32	iso/iec 9126;reliability engineering;verification and validation;iso/iec 12207;iso 29110;iso 9000;systems engineering;engineering;software engineering;iso/iec 15504;iec 62304;software peer review	SE	-70.31601721105794	15.19230884694981	84824
e0ba3d35bec539b5d4cff258be05c5fbd4b67725	analysis for supply hub in industrial cluster: classic vs. new perspective	big data;conferences	An Industrial Cluster (IC) is a group of similar and interconnected companies in a particular field located in a geographic concentration to share common resources. The relationship of supply chain management (SCM) and ICs are not sufficiently investigated mathematically so far. In this research, Supply Hub in Industrial Cluster (SHIC) is proposed as a common provider of warehousing and logistics services to optimize the interactions of enterprises located within ICs. The experimental results show that the proposed model dominates the classic supply chain model through merging storage space and consolidation shipment.	interaction;logistics;semiconductor consolidation;social history and industrial classification;usb hub	Vahid Kayvanfar;S. M. Moattar Husseini;Behrooz Karimi;Mohsen S. Sajadieh;Tan Wen Jun	2016	2016 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2016.7841043	engineering;marketing;operations management;operations research	Robotics	-72.30714319297184	5.913342215655287	84826
dc21fc1ea22f659b0a219ea8f53c290ac162ee9a	an approach to solve contradiction problems for the safety integration in innovative design process	production performance;modelizacion;safety requirement;proceso concepcion;concepcion ingenieria;maintenabilite;fiabilidad;reliability;engineering design;design process;productivite;behavioral analysis;securite;conception ingenierie;systemic;developpement produit;sistemico;integration;productividad;preparacion serie fabricacion;working situation model;modelisation;innovation;mantenimientabilidad;integracion;contradictions;fiabilite;analyse comportementale;adaptive method;safety;analisis conductual;exigence securite;systemique;process planning;productivity;innovacion;seguridad;preparation gamme fabrication;modeling;innovative design;exigencia seguridad;desarrollo producto;system safety;maintainability;processus conception;product development	Improving product behavioral performance in design process needs an innovative approach to integrate simultaneously most of the trades (safety, reliability, maintainability, etc.). Currently, safety integration is done in the latest phase of design process, to respect safety directives. This late integration causes some contradictions between productivity and safety. These contradictions could be related to technical and organizational problems. This paper proposes an innovative approach that aims to eliminate these contradictions in order to improve product performance in the use situations. This approach is based on 4 steps: systemic safety integration using Working situation model, taking into account the requirements of safety directives and standards; identifying the contradiction resulting from designer’s choices and finally resolving these contradictions using adapted methods like TRIZ. An application case is outlined in off-set industry, to show the applicability and usefulness of our approach. 2010 Elsevier B.V. All rights reserved.	computer-aided design;emoticon;printing;production system (computer science);requirement;sensor;software framework	Rémy Houssin;Amadou Coulibaly	2011	Computers in Industry	10.1016/j.compind.2010.12.009	innovation;productivity;systems modeling;design process;systems engineering;engineering;operations management;reliability;system safety;engineering design process;new product development;maintainability	AI	-68.24603372549831	7.341397310638915	84839
85884b3dac32bbb17e607a3dcb7149a28f908803	examining whether highly e-innovative firms are more e-effective	e business;resource based theory;innovation	The resource-based view (RBV) ascribes superior firm performance to firm resources and capabilities. In recent years, much debate about the value of e-business has been raised because of the costly investments required. Although studies have found positive relationships between e-business and firm performance, there is a need to further investigate into these topics. Since innovation has become a key factor for increasing the competitiveness of firms and e-business has been proposed as complement to innovation, this paper analyses, based on the RBV perspective, whether companies with high level of Internet resources and with high e-innovation are more effective electronically. The methodology involved a large sample firms and data collected by the European e-Business Market Watch, an established e-business observatory sponsored by the European Commission. Results indicated that differences of e-sales effectiveness of firms with high and low Internet resources were not statistically significant, while on the contrary firms with a high level of e-innovation outperformed on e-sales effectiveness.	electronic business;high-level programming language	Pedro Soto-Acosta;Ricardo Colomo Palacios;Daniel Perez González	2011	Informatica (Slovenia)		innovation;knowledge management;electronic business	Metrics	-82.99457878837184	5.810336310813603	84844
9517ff47e3cccc515d6ba387eed6c9a4e04e1eb9	dynamic surface imaging for the space shuttle experiments			experiment	J. V. Candy	1998	Int. J. Imaging Systems and Technology	10.1002/(SICI)1098-1098(1998)9:6%3C463::AID-IMA8%3E3.0.CO;2-X	computer vision;artificial intelligence;computer science;space shuttle	Robotics	-89.99909022894514	24.48633618270356	84879
858b4e18cd6310428be2c124c282f1b3e41becfa	measurement of information system project success in organizations - what researchers can learn from practice		The adequate measurement of information system project success is yet an unsolved problem. By means of a qualitative empirical study, we analyze the way companies in Germany measure the success of their information system projects. We conducted 9 semi-structured expert interviews with project managers and other decision-makers to gather information about (1) their subjective views on and (2) their companies’ actual measurement of information system project success. Although the participants consider further success dimensions as important, the companies almost exclusively rely on the traditional criteria of time, cost, and quality. Furthermore, we provide insights into reasons for this behaviour. Practitioners assume that a project is efficient when it is completed on-time and inbudget as long as plans are not exceeded. Our study provides a new perspective on information system project success and shows that it is mainly a matter of measurability to use further success dimensions.	information system;semiconductor industry	Dominik Joosten;Dirk Basten;Werner Mellis	2011			knowledge management;information system;management science;computer science	SE	-79.72067828976533	9.54281163480918	84922
2406eb2e25c4563c0ee05e6bc32201895e66f6c9	international examples of large-scale systems - theory and practice iii: competition and strategy in electronic marketplaces		Electronic marketplaces are evolving in both business to business, and business to consumer contexts. Although the initial hype surrounding all types of marketplaces appears to overstate their short-term impact, established companies across all types of industrial sectors are entering into collaborative, industry-wide initiatives to agree on common technical and trading standards to improve the effectiveness of the interactions between buyers and sellers on a global scale. An overview of contemporary developments is presented, and common patterns across different sectors are identified. Three case studies are presented in the areas of automotive, banking, and consumer markets. It is shown that product-market characteristics affect the formation of business relationships and market structures, and the design of information flows and shared systems is a reflection of typically strong business relationships and hierarchical market structures.	industrial pc;interaction;systems theory	Christopher Patrick Holland	2003	CAIS		economics;marketing;management;commerce	HCI	-75.78977284683283	5.060252935736985	84935
0acf0ba9e28558bcd1180ec5446071e86765a12b	key considerations for starting or maintaining your simulation department		Organizations interested in applying simulation to support informed decision-making need to make a key decision whether to contract the work or develop an internal capability to perform the work. Both alternatives have its pros and cons and the ideal approach varies across organizations. This presentation reviews some of the key factors worth consideration, including the caliber of staff, costs, risks, and necessary and perceived internal focus. Furthermore, managers and developers have different responsibilities and challenges in ensuring a successful project, very much influenced by the structure of the simulation work being performed. The presenters will share experiences working both in an internal simulation role and working as external service providers.	simulation	Amy B. Greer;Martin M. Franklin	2017	2017 Winter Simulation Conference (WSC)	10.1109/WSC.2017.8248149	service provider;systems engineering;computer science;cons	HCI	-76.7920531065574	10.839684088343219	84958
2abaf3f3f77b323ad319b5de95e4b9f6e16b72ad	evaluating a taxonomy of handover activities in one swedish company	deployment;in house handover context;handover activity;software engineering process;software engineering dp industry;software system;dp industry;training;software systems;transition;maintenance engineering;swedish software company;software engineering;software community;taxonomy;software engineering process taxonomy evaluation handover activity software system software community swedish software company in house handover context;maintainability transition training deployment predelivery maintenance;programvaruteknik;maintenance engineering taxonomy software systems training hardware documentation;predelivery maintenance;documentation;maintainability;taxonomy evaluation;hardware	Handing over a software system from development to maintenance is still an under-researched domain. The software community has a hazy insight into its constellation and inherent activities. In this paper, we have evaluated a preliminary version of a taxonomy of handover activities within one Swedish software company. The evaluation is conducted in an in-house handover context only. Despite this, our results provide evidence of its enormous complexity, variability and strong dependency on many other software engineering processes.	software engineering;software system;spatial variability	Mira Kajko-Mattsson;Ahmad Salman Khan;Tommy Tyrberg	2010	2010 36th EUROMICRO Conference on Software Engineering and Advanced Applications	10.1109/SEAA.2010.41	maintenance engineering;reliability engineering;systems engineering;engineering;software engineering;taxonomy;software system	SE	-64.50486099110243	28.67519790198913	84984
2e00730a6e6413653cf50dc1ef75828f6aa7b6c3	continuous vs step change production process improvement as enablers for product redesign and new market opportunities		Firms competing in global markets have to rapidly improve and innovate their products, processes, value chains and business models. Innovations can originate from a multitude of sources, where market needs and technology push are about the most common. However, company internal efforts towards incremental improvements of production processes can sum up to achieve breakthrough product innovations. This study focuses on the dynamic between process and product development, and bring about new knowledge on how systematic improvements of technological and organizational aspects related to manufacturing affects product innovation. We hypothesize that in global and mature markets and dispersed value chains the effect of mutual understanding and close collaboration between process- and product development can lead to breakthrough innovations at least as fast as by focusing on step change and disruptive process innovations. To explore this hypothesis we have conducted a case study, exploring two companies according to what we categorize as; the continuous improvement approach and the disruptive approach. Findings demonstrate that neither of the approaches necessarily respond to the ever-increasing requirement to reduce time-to-market, but a set of barriers and enablers that together with contextual issues, supports step changes on products and processes.		Geir Ringen;Kjersti Øverbø Schulte	2017		10.1007/978-3-319-66926-7_7	scheduling (production processes);product innovation;new product development;multitude;process management;systems engineering;technology push;business model;engineering	NLP	-78.56084250902748	4.735395424483871	85036
81225abf7b9203683d25a818c9de25b1d3604b17	learning from failures	learning from failure;software development failure;failure causes;despatch systems	Abstract#R##N##R##N#This paper addresses the topic of failures in computer-based systems. It does so in three stages: first it describes failure cases in a single domain, that of computer-based ambulance despatch systems; then it discusses some of the features of those cases; finally it considers the importance of reporting and analysing failures, in order to discover their causes and to learn lessons that will help eliminate those causes of failure in the future. Failure, it is argued, provides a valuable learning opportunity that may lead, when recognized, to better practice and future success. Copyright © 2003 John Wiley & Sons, Ltd.		Darren Dalcher;Colin Tully	2002	Software Process: Improvement and Practice	10.1002/spip.156	failure reporting, analysis, and corrective action system;failure causes;engineering;artificial intelligence;operations management;management;operations research	SE	-74.69572047792738	19.063353113052536	85273
129c3ae3cb897412890eb4bccbac33d85b85b73a	preliminary evaluation of business to business electronic commerce by using qualitative simulation and scenario generation	structural model;business to business;electronic commerce;corporate modelling;e commerce;business communication;investment;corporate modelling electronic commerce investment business communication;electronic commerce business companies costs modeling information systems investments systems engineering and theory productivity supply chains;scenario generation;e commerce business to business electronic commerce qualitative simulation scenario generation structural model	A scenario generator for qualitative simulation systems has been developed for explaining the influence flow to the concerned node in the structural model. The paper presents the method through which companies evaluate business to business electronic commerce (e-commerce) systems by using the scenario generator. The method consists of the following processes: the adjustment of the structural model to specific companies; the qualitative simulation and the scenario generation of the new systems; and the evaluation of the scenario. This method will be useful for companies to evaluate e-commerce systems.		Fumio Komiya;Tebuo Kusuzaki;Shuji Soga;Kentaro Ohtani;Isao Tsushima;Ayako Hiramatsu	1998		10.1109/ICSMC.1998.727605	e-commerce;business analysis;investment;computer science;artifact-centric business process model;electronic business;business process model and notation;business rule;new business development;business process modeling;business activity monitoring;consumer-to-business	AI	-70.78786245343161	4.813870726560198	85285
e7bd93c1c1726a1a831861ce390a2bc32ca933fa	domain-specific languages (dagstuhl seminar 15062)	004;internal dsls external dsls domain specific modeling extensible languages language workbenches textual graph based visual languages;language design language implementation techniques	This report documents the program and outcomes of Dagstuhl Seminar 15062 “Domain-Specific Languages”, which took place February 1-6, 2015. The seminar was motivated on the one hand by the high interest in domain-specific languages in academia and industry and on the other hand by the observation that the community is divided into largely disconnected subdisciplines (e.g., internal, external, visual, model-driven). The seminar included participants across these subdisciplines and included overview talks, technical talks, demos, discussion groups, and an industrial panel. This report collects the abstracts of talks and other activities at the seminar and summarizes the outcomes of the seminar.	domain-specific language	Sebastian Erdweg;Martin Erwig;Richard F. Paige;Eelco Visser	2015	Dagstuhl Reports	10.4230/DagRep.5.2.26	natural language processing;speech recognition;computer science	NLP	-64.44805893595736	23.81272683919951	85294
c64d54acb324921f06b2fb68065bb7dc498128af	e-commerce business models in the context of web3.0 paradigm		Web 3.0 promises to have a significant effect in users and businesses. It will change how people work and play, how companies use information to market and sell their products, as well as operate their businesses. The basic shift occurring in Web 3.0 is from information-centric to knowledge-centric patterns of computing. Web 3.0 will enable people and machines to connect, evolve, share and use knowledge on an unprecedented scale and in new ways that make our experience of the Internet better. Additionally, semantic technologies have the potential to drive significant improvements in capabilities and life cycle economics through cost reductions, improved efficiencies, enhanced effectiveness, and new functionalities that were not possible or economically feasible before. In this paper we look to the semantic web and Web 3.0 technologies as enablers for the creation of value and appearance of new business models. For that, we analyze the role and impact of Web 3.0 in business and we identify nine potential business models, based in direct and undirected revenue sources, which have emerged with the appearance of semantic web	capability maturity model;collaborative consumption;e-commerce payment system;electronic funds transfer;graph (discrete mathematics);location-based service;mobile commerce;seamless3d;semantic web;sharing economy;strategic management;user experience;world wide web	Fernando Almeida;José D. Santos;José A. Monteiro	2013	CoRR	10.5121/ijait.2013.3601	web accessibility initiative;knowledge management;social semantic web;web intelligence;web engineering	Web+IR	-74.85089658139829	5.644943740632789	85295
5d343d75d3b129502b8821734dbf8a9912f2100a	trade-off's in web application development - how to assess a trade-off opportunity		This paper discusses a trade-off strategy for small projects and presents a preliminary guidelines for assessing the appropriateness of a trade-off. The motivation of this work is to make the development team aware of the performed trade-offs and to see both the associated opportunities and risks, to assess the appropriateness of a trade-off and to gather information for the future development of the system. The development environment of a web application is changing over time and it is important to know what the success criteria are at any time.	norm (social);web application development	Sven Ziemer;Tor Stålhane	2007			computer science;management science;web application;data mining;web application development;development environment;trade-off	SE	-71.47013977788673	19.42268271619589	85296
cb764f3a1212c5d2679789c16baac7c96090f416	the waterfall model in large-scale development	software development process;empirical evidence;large scale;software development;datavetenskap datalogi;computer science;empirical research	Waterfall development is still a widely used way of working in software development companies. Many problems have been reported related to the model. Commonly accepted problems are for example to cope with change and that defects all too often are detected too late in the software development process. However, many of the problems mentioned in literature are based on beliefs and experiences, and not on empirical evidence. To address this research gap, we compare the problems in literature with the results of a case study at Ericsson AB in Sweden, investigating issues in the waterfall model. The case study aims at validating or contradicting the beliefs of what the problems are in waterfall development through empirical research.	agile software development;experience;requirement;software development process;waterfall model	Kai Petersen;Claes Wohlin;Dejan Baca	2009		10.1007/978-3-642-02152-7_29	empirical evidence;systems engineering;engineering;operations management;software development;software engineering;management science;empirical process;empirical research;management;software development process	SE	-68.92636419903	22.645514430603377	85306
59bbee2f215c09eddc1e16a85304008bc3b536a9	management science methodologies in environmental management and sustainability: discourses and applications	forecasting;reliability;system approach;project management;complexity theory;information systems;maintenance;system dynamics;soft or;information technology;packing;operations research;location;investment;journal;journal of the operational research society;inventory;purchasing;history of or;critical system;logistics;marketing;scheduling;production;communications technology;computer science;operational research;viable system model;environmental management;system management;applications of operational research;or society;jors;management science;infrastructure	This paper investigates and discusses the use of systemic methodologies (SMs) developed in management science/operational research (MS/OR), in particular, those SM that have been informing the complexity inherent in environmental management and sustainable (EM/S) practices. By surveying a sample of the top MS/OR and systems journals, we assess the extent to which systemic management science methodologies developed recently have been used in tackling EM/S problems. Titles and abstracts of EM/S applications published in MS/OR and systems journals between 1989 and 2009 were queried for the occurrence of typical keywords associated with a set of SMs (eg, complexity theory, systems dynamics, soft systems, critical systems, viable systems model). The survey identifies a set of articles representing the practice of either a particular methodology or of a mixture of various SMs in EM/S setting. By assembling and critically reviewing a sample of applications in EM/S the paper hopes to raise awareness among environmentalists, operational researchers and management scientists of the benefits of using systemic approaches developed in MS/OR and, in this way, to encourage further exchange and conversation between these fields of management.	environmental resource management;management science	Alberto Paucar-Caceres;A. Espinosa	2011	JORS	10.1057/jors.2010.110	project management;logistics;inventory;economics;forecasting;investment;computer science;marketing;operations management;reliability;management science;system dynamics;location;management;operations research;information technology;scheduling;viable system model	EDA	-65.87385994748564	8.052846654756769	85385
c1a16f5548a764c2979951c3f933cfad91c9bd83	energy, entropy and exergy in communication networks	energy;communication networks;exergy;lifecycle analysis;entropy	The information and communication technology (ICT) sector is continuously growing, mainly due to the fast penetration of ICT into many areas of business and society. Growth is particularly high in the area of technologies and applications for communication networks, which can be used, among others, to optimize systems and processes. The ubiquitous application of ICT opens new perspectives and emphasizes the importance of understanding the complex interactions between ICT and other sectors. Complex and interacting heterogeneous systems can only properly be addressed by a holistic framework. Thermodynamic theory, and, in particular, the second law of thermodynamics, is a universally applicable tool to analyze flows of energy. Communication systems and their processes can be seen, similar to many other natural processes and systems, as dissipative transformations that level differences in energy density between participating subsystems and their surroundings. This paper shows how to apply thermodynamics to analyze energy flows through communication networks. Application of the second law of thermodynamics in the context of the Carnot heat engine is emphasized. The use of exergy-based lifecycle analysis to assess the sustainability of ICT systems is shown on an example of a radio access network.	dissipative system;embodied cognition;holism;interaction;mathematical optimization;optical amplifier;radio access network;telecommunications network	Slavisa Aleksic	2013	Entropy	10.3390/e15104484	entropy;simulation;energy;thermodynamics;exergy;physics;quantum mechanics	HPC	-79.39868271011446	16.99433078550792	85406
1fa48de7914c55fdf4999be959dc3a1d90339cf2	attributes required for profiting from open innovation in networks	innovation networks;open innovation;structural equation modelling;innovativeness;organisational attributes;task fulfilment;individual attributes;research networks;radiation dosimetry;cost reductions	When individuals act in open innovation networks, their attributes are specifically crucial in respect of their success or failure to profit from the network. The empirical background of this research was a 16-month investigation of EURADOS, the successful European network for research on radiation dosimetry. EURADOS presently consists of almost 200 members from 52 European institutions in 31 countries. Not all EURADOS members profit equally from its open innovation network. Structural equation modelling provided an answer regarding the personal and organisational attributes that are required to profit from the network in terms of an increase in innovativeness, a reduction in costs and a better fulfilment of tasks in the home organisation. The attributes openness and the possibility to contribute influence the value that individuals derive from open innovation networks equally.	open innovation	Ellen Enkel	2010	IJTM	10.1504/IJTM.2010.035980	structural equation modeling;economics;knowledge management;marketing;management science;open innovation;management;commerce	Vision	-83.58486152900687	4.660352831245421	85418
1fafd6696ea15e189394e3b1b616392170b77c37	preliminary analysis of the effects of confirmation bias on software defect density	cognitive psychology;unit testing;defect density;software defect density;software development;confirmation bias	In cognitive psychology, confirmation bias is defined as the tendency of people to verify hypotheses rather than refuting them. During unit testing software developers should aim to fail their code. However, due to confirmation bias, most defects might be overlooked leading to an increase in software defect density. In this research, we empirically analyze the effect of confirmation bias of software developers on software defect density.	software bug;software developer;unit testing	Gül Çalikli;Ayse Basar Bener	2010		10.1145/1852786.1852870	reliability engineering;confirmation bias;computer science;software development;unit testing;forensic engineering;programming language	SE	-65.50840684157806	32.25899361843523	85647
ff01229c102c57f5b5964393e8e29bf0d7324606	supply/demand chain modeling utilizing logistical-based costing	cost function;market model;marketing models;logistic model;statistical analysis;model development;supply chain management;cost model;design methodology	Purpose – The purpose of this research is to describe how total cost concept with logistical based costing (LBC) is developed in detail and then used to build logistical models on the Microsoft Excel™ platform that are integrated from the customer's factory to the supplier's door.Design/methodology/approach – The models developed in this project are deterministic, event‐based algorithms to compare logistical conduits for bulk and containerized commodities. The demand chain approach is used to derive the pathways in reverse order from the customer to the supplier. The methodology is necessary to find all possible conduits from origin to destination, including points where product may cross over between various logistics systems. The approach is applied to the bulk and container system with disconnects (elevators, ports) serving as the demarcation points. The pathways from supplier to end‐user must be identified prior to application of classification and costing techniques. A goal of this research was to co...	logistics	Jake M. Kosior;Doug Strong	2006	J. Enterprise Inf. Management	10.1108/17410390610658513	supply chain management;economics;design methods;computer science;systems engineering;engineering;marketing;operations management;logistic regression;statistics	DB	-63.598251589979625	5.72715343650491	85684
4d8269752874ca759212e7b5af6b48bb3b4beacb	speed and innovation through architecture	software systems;innovation process;software engineering;software architecture;innovation;complex system;software development;new world;innovation experiment systems	The nature of software system development is changing. Rather than building systems according to specification, innovation processes and customer intimacy are at the heart of software development, requiring unprecedented levels of agility and speed. In addition, software is increasingly built in the context of a software ecosystem where other companies and independent developers add value as well. As these trends require small teams to work efficiently and rapidly in the context of large complex systems, the role of software architecture (and that of a software architect in particular) is more important in this new world, but there is significant evolution in its implementation. This keynote starts by characterizing the new approach to software engineering and the role of compositionality. It then explores the implications for software architecture and the role of the software architect. The talk will present examples from several industries to illustrate specific focus areas.	complex systems;software architect;software architecture;software development;software ecosystem;software engineering;software system	Jan Bosch	2012		10.1145/2304736.2304738	innovation;personal software process;complex systems;verification and validation;software engineering process group;software sizing;systems engineering;engineering;package development process;backporting;social software engineering;component-based software engineering;software development;software design description;software engineering;software construction;software walkthrough;software analytics;resource-oriented architecture;software deployment;software development process;software quality;software system;computer engineering;software peer review	SE	-65.81314124296996	22.723644037104815	85750
75193ecadd75ec937f00a764236f6f6af8183eed	a software technology plan for national information infrastructure	socio cultural domains;software technology life cycle software technology plan national information infrastructure long term software technology plan korean national information infrastructure application service requirements socio cultural domains softech 2015 technical specifications;project management;national information infrastructure;life cycle;application software;uncertainty;software development management internetworking internet socio economic effects management of change;government;softech 2015;gravity;application service requirements;computer industry;systems engineering and theory;korean national information infrastructure;technology planning;software technology life cycle;internet;long term software technology plan;environmental economics;internetworking;management of change;information society;software technology plan;process planning;socio economic effects;software development management;technology planning systems engineering and theory government process planning application software environmental economics uncertainty gravity computer industry project management;technical specifications	A model for the long-term software technology plan that we applied for Korean National Information Infrastructure is reviewed an this paper. Analysis and assumption on technology trends for twenty-year time frame are attempted, and application service requirements for public, economic and socio-cultural domains are identified. Based on the service requirements and technological environments of the future information society, we have categorized core sets of software technology, and have structured those technology sets into a technology tree. In the actual plan of SOFTECH 2015, each component of the technology tree was broken down into technical specifications t o be utilized as inputs for the national plan of software R & D. We recognize that software technology life-cycle is becoming shorter, and many say that the life-cycle is less than one year. However, under many uncertainties, long-term plans are still required in many instances.	categorization;national information infrastructure;requirement	Dan-Hyung Lee;Moon-Hyun Kim	1995		10.1109/FTDCS.1995.524989	information technology architecture;information technology management;systems engineering;engineering;knowledge management;software asset management;software as a service;management science	SE	-72.50248146321351	15.132684724217937	85810
f1b2f941120b9a62bdd4bc22e5cf9711b30b4fa5	itil capacity management: more than charts over coffee	capacity management	Many organizations are embracing ITIL as a model for best practices. This paper provides a brief background of ITIL and the ITSM processes, with particular attention paid to Capacity Management. The author also considers two different organizations and discusses Capacity Management as practiced in those environments, comparing and contrasting those practices with ITIL Capacity Management. Finally, recommendations are made for implementing ITIL Capacity Management in any environment, with additional focus given to the interfaces between ITIL Capacity Management and the other ITSM processes.	align (company);best practice;chart;gap analysis;itil	Rich Fronheiser	2006			capacity management;information technology infrastructure library;business;systems engineering	ML	-73.52487359621556	9.933580392185906	85822
87330ac3f407ebb6bf740b8a0c5677b76b43217e	cloud computing for big data entrepreneurship in the supply chain: using sap hana for pharmaceutical track-and-trace analytics	entrepreneurship;in memory computing cloud computing big data entrepreneurship sap hana pharmaceutical track and trace analytics cloud based analytics monitoring safety pharmaceutical supply chain medication track and trace system xqs service gmbh;big data analytics;pharmaceutical supply chain;supply chains big data cloud computing pharmaceutical industry pharmaceutical technology supply chain management;sap hana;start up;rfid;track and trace;track and trace big data analytics cloud computing entrepreneurship pharmaceutical supply chain rfid sap hana start up;supply chains big data cloud computing clouds innovation management drugs;cloud computing	This paper focuses on the real-world use of cloud-based analytics for supporting medication track-and-trace and monitoring safety and quality in the pharmaceutical supply chain. The paper describes a track-and-trace system developed by German start-up XQS-Service GmbH and adopted by manufacturers, wholesalers, pharmacies and clinics in Germany. The paper also discusses the benefits of using in-memory computing to process large amounts of big data to support efficient and effective medication track-and-trace.	big data;cloud computing;in-memory database;in-memory processing;sap hana;track and trace	Alina M. Chircu;Eldar Sultanow;Flavius C. Chircu	2014	2014 IEEE World Congress on Services	10.1109/SERVICES.2014.84	marketing;operations management;advertising;business	DB	-71.01165948137013	6.390618458619047	85861
54b916ab258a7c0eb1f2b2f5515253e4998f114d	is amazon becoming the new cool software company for developers?	paas amazon software company middleware platform as a service;middleware cloud computing dp industry;cloud tidbits;cloud;computer architecture;paas cloud cloud tidbits devops platform as a service;platform as a service;business;software development;paas;devops;cloud computing software development business computer architecture;cloud computing	"""Amazon has clearly decided that being the leader in infrastructure as a service (IaaS) isn't enough. Amazon has decided to become a """"software company"""" and bring forward its own portfolio of middleware (and more importantly DevOps tools) to address the platform as a service (PaaS) and tooling innovation gap, even if at the expense of commonality/portability with anyone else."""	cloud computing;devops;middleware;platform as a service;software portability	David Bernstein	2015	IEEE Cloud Computing	10.1109/MCC.2015.10	cloud computing;computer science;operating system;world wide web	Embedded	-69.21925095442782	26.872454518524	85955
187185a944c62d46fb1ef0b68c2bd61ef7606f33	dark innovation: an interview with jerry michalski	process innovation;business model;new products	As computing technologists, we tend to think of innovations in terms of new products or services supported by, or made of, computing technologies. But there are other types of innovation besides products. There are process innovations, such as McDonald's method of making hamburgers fast; social innovations, such as Mothers Against Drunk Driving; and business model innovations, such as Starbucks replacing a coffee shop with an Internet cafe. In all these categories, we tend to think of innovations as new ways of doing things that positively impact many people.		Peter J. Denning	2012	Ubiquity	10.1145/2160598.2160599	business model;management	Web+IR	-74.56336296499306	4.750046882173417	85960
151139738e8aedfef2ddd2197f0d96181057a2c2	assisting engineers in switching artifacts by using task semantic and interaction history	trust;empirical study;context aware;software engineering;software development;recommendation	Recent empirical studies show that software engineers use 5 tools and 14 artifacts on average for a single task. As development work is frequently interrupted and several simultaneous tasks are performed in parallel, engineers need to switch many times between these tools and artifacts. A lot of time gets wasted in repeatedly locating, reopening or selecting the right artifacts needed next. To address this problem we introduce Switch!, a context-aware artifact recommendation and switching tool. Switch! assists engineers in switching artifacts based on the type of the development task and the interaction history.	interrupt;software engineer	Walid Maalej;Alexander Sahm	2010		10.1145/1808920.1808935	simulation;computer science;artificial intelligence;world wide web	SE	-68.06874502736261	29.180223541114813	85963
694c422cb6ee2ef2b6cbb5956dfd51a6dbc413a3	software security is software reliability	software security;software reliability	Enlist hacker expertise, but stay with academic fault naming conventions, when defending against the risk of exploitation of vulnerabilities and intrusions.	application security;software quality;software reliability testing;vulnerability (computing)	Felix FX Lindner	2006	Commun. ACM	10.1145/1132469.1132502	software security assurance;personal software process;long-term support;verification and validation;security bug;computer science;package development process;backporting;social software engineering;software reliability testing;software development;software construction;software testing;software walkthrough;secure coding;software deployment;computer security;software quality;avionics software;software peer review	Security	-67.02679056371262	27.739079839371986	86034
4e11162428e441499a6b65ec2d83dccdd2091e91	automated technology for verification and analysis		s of Invited Talks Consistency Made Easy: Towards Building Correct by Design Cloud Applications	design by contract	Josef Kittler;Friedemann Mattern;John C. Mitchell	2017		10.1007/978-3-319-68167-2		Embedded	-91.92949737696937	25.565618721379096	86250
7b14dbe564ab04333c142ee87c7afeb5c7aa1845	pioneering process improvement experiment in hungary	bootstrap;oop;quality management;project planning	The PASS (Pay-roll Accounting and Settlement System) project is the first Central and Eastern European ESSI PIE (European Systems and Software Initiative Process Improvement Experiment) project directly supported by the European Commission. The PASS project was carried out under the ESSI initiative with the financial support of the Commission of the European Communities under the ESPRIT Programme EP21223. The PASS (Pay-roll Accounting and Settlement System) project started as a new business project of MemoLuX. Its business purpose is to develop a modular, platform independent, integrated networked software system satisfying functional requirements of EU standards in public accountancy and applicable for the Hungarian as well as for the international market. The system provides direct service among employers, employees and banks. The PASS project is the baseline project for the process improvement experiment. In the PIE, the quality of MemoLuXu0027s development process was enhanced to become well defined and predictable with the professional support of MTA SZTAKI (Computer and Automation Institute of the Hungarian Academy of Sciences). During the dissemination supported by ISCN, this PIE is used as a master example to adapt Eastern European processes to the high quality norms of Western Europe, this way facilitating the integration of Eastern Europe into a joint EU in the long term. Objectives and expected results are improving the control of the development process (QA Unit, structured system analysis, improved testing process, efficient project planning, ISO 9001 documentation), raising the maturity level (CMM) to 3 and achieving compliance with ISO 9001 requirements at this high level of maturity. Copyright © 2000 John Wiley u0026 Sons Ltd		Miklós Biró;János Ivanyos;Richard Messnarz	2000	Software Process: Improvement and Practice	10.1002/1099-1670(200012)5:4%3C213::AID-SPIP129%3E3.0.CO;2-0	quality management;systems engineering;engineering;operations management;software engineering;object-oriented programming;management;project charter;project planning	SE	-69.80170995064844	17.77172027092334	86287
0686e217a0095b23e860c4451cfe30af1c5d3640	success factors in a reward and equity based crowdfunding campaign		As the world changes, financial sector changes with it, and alternative finance emerge. Crowdfunding is part of this evolution and its use is becoming more widespread day by day. As more entrepreneurs and organizations seek crowdfunding to fund their ventures, and to solve the most important obstacle to achieve a successful crowdfunding campaign, which is not the information available about how to create a successful campaign, but rather its dispersion, it becomes of uttermost importance to create a framework to help them to succeed raising finance. It is, therefore, the main goal of this paper, to provide a valuable guide to help entrepreneurs in the hard task of raising finance through crowdfunding, mainly those using equity and reward crowdfunding. To accomplish the purposed goal, it is important to understand the underlying motivations of those investing in crowdfunding, in order to offer them what they seek and what motivates them. It is also essential to identify which kind of crowdfunding fits better the entrepreneur's goals and which one increases the likelihood of achieving them. Finally, is crucial to understand the specific success factors for a successful crowdfunding campaign. One of the main findings of this work, is that the motivations for investors in equity and reward crowdfunding are similar. This is very important, since it solves the problem of the scarcity of data for the equity model. With similar motivations it is possible to apply the data available for reward crowdfunding to equity crowdfunding, since both campaign can be design to answer the same needs and motivations.	equity crowdfunding;fits	Francisco Ferreira;Leandro Pereira	2018	2018 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)	10.1109/ICE.2018.8436308	equity crowdfunding;scarcity;marketing;task analysis;obstacle;business;equity (finance)	Robotics	-80.690398581689	7.4717254749973145	86299
8ae09e1ccfe4f267f24a3e8ff30df6abec4b213d	seteli: the strategy expert for telecommunication investments	expert systems;telecommunication network management expert systems;subscriber preferences strategy expert telecommunication investments seteli system technical knowledge network structure;investments expert systems decision support systems object oriented modeling knowledge management telecommunication services spread spectrum communication costs artificial intelligence large scale systems;network structure;subjective assessment;telecommunication network management	A description is given of the Seteli system, which combines technical knowledge about network structure with managers' subjective assessments. Managers can examine how the network would evolve subject to different assumptions regarding demand and subscriber preferences. The nature of the telecommunication planning problem in Finland (where Seteli was developed) is described. The development of Seteli is briefly recounted, its structure is examined, and the use of the system is discussed. Seteli's current status and its utility are assessed.<<ETX>>		Ahti Salo;Raimo P. Hämäläinen	1990	IEEE Expert	10.1109/64.60707	legal expert system;computer science;knowledge management;artificial intelligence;management science;subject-matter expert;expert system	Robotics	-83.73150922946499	13.122592806533225	86311
29b742f8cd8bebcf5a6e73ea3b5418cd6377f71d	analyzing of cpfr success factors using fuzzy cognitive maps in retail industry	decision synchronization;incentive alignment;collaborative planning forecasting and replenishment cpfr;information sharing;fuzzy cognitive map fcm	The retail sector environment is characterized by intense pressure of competition, ever-changing portfolio of products, hundreds of different products, ever-changing customer requirements and be able to stand in a mass market. When considering that the giant retailers work together with their suppliers, each independent operation is seen as a comprehensive structure, consisting of thousands of sub-processes. In short, the retail industry dynamism and work in cooperation with the competitiveness of the sector is one of a rare combination. Of course in such a sector businesses of all sizes in many aspects of creating an efficient and low cost structure is in the effort. Collaborative planning, forecasting and replenishment (CPFR) model which is a scheme integrating trading partners' internal and external information systems is proposed to assist establishing a more effective supply chain structure in retail industry. Although CPFR can provide many benefits, there have been many failed implementations. The aim of this study is to determine the factors that will support better implementation of CPFR strategy in retail industry and analyze them using fuzzy cognitive map (FCM) approach. FCMs have proven particularly useful for solving problems in which a number of decision variable and uncontrollable variables are causality interrelated. A CPFR model made up of three sub-systems, namely information sharing, decision synchronization and incentive alignment, is proposed and ''what-if'' scenarios for proposed model are developed and interpreted. To our knowledge, this is the first study that uses FCMs for CPFR success factors assessment.	causality;commitment scheme;competitive analysis (online algorithm);control table;edi;electronic data interchange;expert system;fuzzy cognitive map;itil;information quality;information retrieval;openness;requirement;spectral leakage;type system	Gülçin Büyüközkan;Zeynep Vardaloglu	2012	Expert Syst. Appl.	10.1016/j.eswa.2012.02.014	artificial intelligence;machine learning	HCI	-72.95957946543673	5.831284124098887	86314
9341813637749a40bef6fe7f55da332812ef8c5f	internet predictions	social network services;personal data vault;ubiquitous data capture;technology forecasting;cloud service;technological innovation;multinationals;services computing;climate;wireless;internet evolution;green;socio technical systems;distributed computing;interactive entertainment;broadband;data processing;global networks;profit center;personal data vault component participatory sensing mobile phones cloud service online social networking ubiquitous data capture component leveraged data processing component;ubiquitous data capture component;open machine translation;software engineering;procedural content generation;internet of things;online social networking;data analysis;knowledge worker;creation net;internet;leveraged data processing component;clouds;participatory sensing;social networking online;mobile handsets;intercultural collaboration;technological forecasting mobile computing social networking online;prognosticators;calculo repartido;information society;language grid;bottom of the pyramid;carbon emissions;space technology;mobile phones;quant revolution;mobile computing;data protection;vision;calcul reparti;space technology participatory sensing internet personal data vault ubiquitous data capture data processing data protection tussle socio technical systems broadband global networks interactive entertainment cloud computing procedural content generation climate carbon emissions green internet evolution wireless technology forecasting prognosticators vision tussle socio technical systems broadband global networks information society future ict for sustainable growth internet of things open machine translation intercultural collaboration services computing language grid cloud computing bottom of the pyramid quant revolution multinationals knowledge worker profit center creation net software engineering telecommunications;tussle;personal data vault component;telecommunications;cloud computing	14 www.computer.org/internet/ IEEE INTERNET COMPUTING cellular phone infrastructure and consumer devices that incorporate location services, digital imagers, accelerometers, Bluetooth access to off-board sensors, and easy programmability. These systems can be leveraged by individuals and communities to address a range of civic concerns, from safety and sustainability to personal and public health. At the same time, they will push even further on our societies’ concepts of privacy and private space.	bluetooth;digital camera;internet;mobile phone;privacy;sensor	Deborah Estrin;K. Mani Chandy;Robert Michael Young;Larry Smarr;Andrew M. Odlyzko;David D. Clark;Viviane Reding;Toru Ishida;Sharad K Sharma;Vinton G. Cerf;Urs Hölzle;Luiz André Barroso;Geoff Mulligan;Adrian Hooke;Chip Elliott	2010	IEEE Internet Computing	10.1109/MIC.2010.12	technology forecasting;simulation;cloud computing;computer science;artificial intelligence;operating system;database;distributed computing;multimedia;world wide web;computer security;computer network	Metrics	-63.86229459826919	4.761780475700266	86408
022fdd0aaf1c786b4ac7d80c2c1b08a526d1c6c4	management through vision: a case study towards requirements of bizviz	databases;application development;mathematics;information sources;information security;business visualization;prototypes;decision support systems case study bizviz business visualization information visualization business processes social security institution databases simulation;simulation;government;information visualization;production data security visual databases mathematics computer science data visualization management information systems prototypes information security government;data visualisation;business data processing;decision support systems;data visualization;production;management information systems;social security institution;computer science;social security;bizviz;business process;decision support systems data visualisation business data processing public administration;business processes;visual databases;data security;public administration	BizViz (Business Visualization) is a rapidly growing phenomenon in the realm of visualization and information visualization. The main goal of BizViz is to support managers in their daily work of understanding forces within business processes and making decisions to control these forces. The case study presented in this paper illustrates the added value of visualization at a social security institution (Gak) in the Netherlands. The prototype built, integrates both databases and simulation as information sources. This way managers can study the past and present as well as experiment with possible future situations. The intended contribution of this paper is twofold. First, it presents how visualization can (successfully) be applied in practice. It illustrates the added value of BizViz in the social security domain. As a second contribution, the paper derives and discusses requirements that play an important role in visualization application development intended for decision-making support.	requirement	Bastiaan Schönhage;Anton Eliëns	2000		10.1109/IV.2000.859786	computer science;knowledge management;data mining;management science	Robotics	-70.490739257941	4.424760314260879	86520
4c25d19dc94b41bbbffae834ea3e877f93e8806c	scale-free model in software engineering: a new design method		Along with software systems become larger and more complex, engineers need better ways to design and maintain these systems. We apply complex network theory into software engineering for this purpose. Recently researchers have found most of complex systems comply with a scale-free network model. This paper presents an initial research into the small-world and scale-free phenomenon within the development of a three-dimensional visualization platform, Total Discovery of Space, which has applied to practical aerospace engineering successfully. It is expected that this model will be practical for software engineering.	software engineering	Zhengxu Zhao;Yang Guo	2013		10.1007/978-3-642-41908-9_35	architecture tradeoff analysis method;verification and validation;software sizing;software verification;search-based software engineering;software design;social software engineering;component-based software engineering;software development;feature-oriented domain analysis;software construction;computer-aided engineering;resource-oriented architecture;software measurement;computer-aided software engineering;software requirements;software system;computer engineering	SE	-63.49135304021694	24.00501323850344	86601
3a1e17b44fbbb3d436faad4726f324bfcc15ba51	a classification system for testing, part 2	system testing software testing permission life testing books libraries software quality software systems heart web server;software testing;integration testing;goal driven testing;unit testing;080399;faculty of science environment engineering and technology;software engineering;software life cycle software testing classification system 2d testing classification matrix goal driven approach requirements driven testing structure driven testing statistics driven testing risk driven testing;test case classification;program testing;phase driven testing;software development;classification system;computer software;software testing software engineering software development test case classification goal driven testing phase driven testing	This article looks at what happens when you combine the four goal-driven approaches to testing classification (requirements-driven, statistics-driven, and risk-driven) with the three phase-driven approaches (unit testing, integration testing, and system testing).	integration testing;requirement;system testing;unit testing	Robert L. Glass	2009	IEEE Software	10.1109/MS.2009.1	test strategy;keyword-driven testing;reliability engineering;white-box testing;integration testing;computer science;systems engineering;engineering;software development;software engineering;session-based testing;software testing;unit testing	SE	-64.11393143003758	27.937838062328964	86712
9849f32d59772a533e3ef40945bdc87a1a904d76	global software engineering and agile practices: a systematic review	distributed software development;systematic review;elektroteknik och elektronik;agile practices;electrical engineering electronic engineering information engineering;computer and information science;software engineering;natural sciences;engineering and technology;global software engineering;programvaruteknik	Agile practices have received attention from industry as an alternative to plan-driven software development approaches. Agile encourages e.g. small self-organized collocated teams, whilst global software engineering (GSE) implies distribution across cultural, temporal and geographical boundaries. Hence, combining them is a challenge. A systematic review was conducted to capture the status of combining agility with GSE. The results were limited to peer-reviewed conference papers or journal articles, published between 1999 and 2009. The synthesis was made through classifying the papers into different categories (e.g. publication year, contribution type, research method). At the end 81 were judged as primary for further analysis. The distribution of papers over the years indicated that GSE and Agile in combination has received more attention in the last five years. However, the majority of the existing research is industrial experience reports, in which Agile practices were modified with respect to the context and situational requirements. The emergent need in this research area is suggested to be developing a framework that considers various factors from different perspectives when incorporating Agile in GSE. Practitioners may use it as a decision-making basis in early phases of software development. KEYWORDS: Agile Practices; Global Software Engineering; Distributed Software Development; Systematic Review 1. INTRODUCTION Distributed teams consisting of stakeholders from different national and organizational cultures, different geographic locations and potentially different time zones characterize global software engineering. These characteristics have significant effects on communication, coordination, and control, and mitigating the effects is a challenge [27]. In comparison with plan-driven software development approaches, Agile methods are more flexible when it comes to taking requirements’ changes into consideration in all phases of software development [11]. They emphasize extensive collaboration between customers and developers, and encourage small self-organized collocated teams [20]. Although mitigating the GSE challenges by themselves is not a straightforward task, combining Agile practices with a global or distributed context complicates things even further. Frequent face-to-face communication among collocated team members improves a feeling of “teamness” and builds trust [7], whilst distance in GSE implies a different way of working, organizational standards, organizational cultures and policies, which may decrease the team’s cohesion. However, (globally) distributed Agile has attracted attention due to its potential associated benefits such as shorter time to market, reduced development cost, and managing late requirements’ changes. This indicates the need for investigating the experiences reported in the current research literature to determine how Agile practices can be efficiently applied in (globally) distributed projects. Although several studies have reported successful integration of Agile and GSE (e.g. [26][9]), a thorough analysis of the studies to reveal the applicability of the reported experiences and best practices in different organizational settings and project demands is yet unexplored. This research primarily aims at extending the systematic mapping study conducted by Jalali and Wohlin [13] into a systematic review. The previous study provided a classification and a visual summary of the type of research reports and results that have been published. The study methods and classification approaches in a systematic mapping and a systematic review differ in terms of goals, breadth and depth [17]. Therefore, we have used both methods complementary. In this paper, the list of databases is expanded and the analysis is extended to research method, contribution, and the results of the included papers. Further, the discussions are enriched and detailed in order to better explain the current status of using Agile practices in GSE based on Please    cite    as:    Jalali,    S.    and    Wohlin,    C.    (2011),    Global    software    engineering    and    agile    practices:    a    systematic    review. J.    Softw.    Maint.    Evol.:    Res.    Pract.    doi:    10.1002/smr.561 findings from the literature. Hence, the objective of this study is to first summarize the current research literature, and then to investigate which Agile practices have been used effectively in which GSE contexts. The remainder of the paper is organized as follows. Section 2 gives a brief background and summarizes related work. Section 3 discusses the research methodology and explains different steps of conducting this systematic review. The results of the study are presented in Section 4, and discussion and observations around them are provided in Section 5. Finally, conclusions and future research directions are presented in Section 6. 2. BACKGROUND AND RELATED WORK The Agile practices and GSE alternatives are shortly presented in this section before putting Agile practices in the context of GSE. Moreover, related research work regarding Agile practices and GSE is summarized, and finally the motivations and objectives of this study are explained. 2.1. Agile Practices Agile methods consist of a set of practices for software development that have been created by experienced practitioners [28], aiming at overcoming the limitations of plan-based approaches through considering changes of the system’s requirements [11]. Agility is defined as “flexibility” and “leanness” [6], and mentioned to be about “feedback and change” in a way to “embrace, rather than reject, higher rates of change” [24]. Agile approaches focus on establishing close collaboration between customers and developers, and delivering software within time and budget constraints. Since they rely on frequent informal face-to-face communication rather than providing lengthy documentation, the process is repetitive, adaptive, and minimally defined [3]. The key features of Agile methods are continuous requirements gathering; frequent face-to-face communication; Pair Programming; refactoring; continuous integration; early expert customer’s feedback; and minimal documentation [4]. The most widely used methodologies based on the Agile principals are Extreme Programming (XP) and Scrum. However, other methods such as Feature-driven Development, Dynamic Systems Development Method, Crystal Clear method, and Lean development have been also used [1][10]. 2.2. Global Software Engineering Geographically distributed software development teams characterize distributed software development, whilst globally distributed teams characterize global software development [19]. In this study, we have considered both as GSE. The description of different terms related to GSE is inspired by [19], and the authors have only made minor changes and generalization presented as follows. Outsourcing (offshore/onshore outsourcing): an external company is responsible for providing software development services or products for the client company. When both subcontracting and client companies are located in the same country, it is known as onshore outsourcing. Offshoring (offshore insourcing): a company creates its own software development centers located in different countries to handle the internal demand. Distributed team: team members are spread in different locations and work remotely on different parts of the project (independent tasks) with or without any face-to-face interactions. The difference between a virtual and a distributed team is that virtual team members work jointly on the same tasks. 2.3. Agile Practices in Global Software Engineering Although Agile methods are well suited when customers and developers are collocated and there is frequent interaction among them [3], several software organizations have reported their successful experience of incorporating Agile in distributed software development (e.g. [26][9]). Please    cite    as:    Jalali,    S.    and    Wohlin,    C.    (2011),    Global    software    engineering    and    agile    practices:    a    systematic    review. J.    Softw.    Maint.    Evol.:    Res.    Pract.    doi:    10.1002/smr.561 However, there are challenges associated with this combination, and to get it to work effectively considerable effort is needed. The major difficulties are summarized as related to communication, personnel, culture, different time zones, trust, and knowledge management [4]. Nevertheless, various tactics and solutions are also reported by different software organizations to mitigate these challenges. 2.4. Related Work Here, a summary of the previous relevant research is presented. Systematic review studies on Agile methods or/and global software engineering are briefly presented. In addition, studies that have partially explored the combination of any Agile method in any GSE context are introduced even though if they are not a systematic review study. Dybå and Dingsøyr [10] conducted a systematic review of empirical studies of Agile software development up to 2005 resulted in identifying 36 relevant empirical studies. Besides the comprehensive analysis of the papers, the need to increase both the number and the quality of studies and to establish a common research agenda in the area of study is pinpointed. In a systematic review study by Smite et al. [22] the empirical evidence in GSE-related research literature has been investigated. The amount of empirical studies in the area was found to be relatively small, hence it is concluded that the GSE field is still immature. Hence, they have shed light on paths for future work for both researchers and practitioners. Taylor et al. [23] conducted a study in 2006 to evaluate the usefulness for practitioners of the existing research on Agile global software development. The study included articles published between 2001 and 2005. They concluded that the published research is of min	agile software development;best practice;code refactoring;continuous integration;database;distributed computing;documentation;dynamic systems development method;emergence;experience;extreme programming;feature-driven development;feedback;generic stream encapsulation;interaction;knowledge management;maxima and minima;offshore custom software development;outsourcing;pair programming;requirement;requirements elicitation;scientific literature;scrum (software development);self-organization;software engineering;statistical classification;systematic review;while	Samireh Jalali;Claes Wohlin	2012	Journal of Software: Evolution and Process	10.1002/smr.561	feature-driven development;requirements analysis;personal software process;systematic review;agile unified process;extreme programming practices;agile usability engineering;systems engineering;engineering;knowledge management;social software engineering;software development;requirement;software engineering;agile software development;empirical process;management;lean software development;software development process	SE	-69.15073490757693	21.733755159392555	86738
68a3bce1b38053afea7c638ecc2372ca7e8c0253	introduction: queueing systems special issue on queueing systems with abandonments		The investigation of queueing systems with impatient customers has a long history in queueing theory, with the now standard Erlang-A model going back at least to Palm’s classic 1957 paper. Despite this long record of work, focus on systems with abandonments has been rekindled due to recent interest in analyzing complex service systems, most notably large call centers. Interestingly, this new surge of work has been motivated both by applied and theoretical advances. In the applied realm, very large call centers are now an important component in many industrial sectors and advanced call management software enables the use of sophisticated algorithms in operating these centers. On the more theoretical side, there have been great advances in approximating large queueing systems with abandonments via many-server asymptotics and related diffusion approximations. Furthermore, there have been numerous advances in the understanding of flexible server models with abandonments, which serve as models for systems with “cross-trained” agents. Techniques from revenue management, game theory, and stochastic optimization have also been applied to the operation of such systems. In addition, queueing systems with impatient customers also have a rich connection with perishable inventory and organ transplantation systems. Thus the requirements of practitioners, and the myriad new tools of theoreticians, have motivated this focused issue on stochastic systems with abandonments. This special issue actually consists of nine papers, eight of which appear together here. Due to a publishing error, one of the papers, by Baris Ata and Mustafa H.		John J. Hasenbein;David Perry	2013	Queueing Syst.	10.1007/s11134-013-9376-4	call management;game theory;software;management science;real-time computing;mathematics;stochastic optimization;queueing theory;revenue management	Metrics	-64.68137861671369	4.808668188401345	86809
48a959167bdbee9730b54c86e5d70710eada6dfb	system lifecycle processes for cyber security in a research reactor facility		The digitalization of nuclear facilities has brought many benefits, including high performance and convenient maintainability, in terms of facility operation. However, cyber accidents accompanied by the use of digital technologies have increased, and cyber security has been one of the most important issues in the nuclear industry area. Several guidelines have been published for nuclear power plants, but it is difficult to apply all requirements within the guidelines to research reactor facilities because the characteristics in terms of facility scale, purpose, and system design, are different from those of power plants. To address this emerging topic, this paper introduces system lifecycle processes for cyber security in a research reactor facility. It addresses the integration of activities for securing systems and guarding a facility safely using the practices at a research reactor facility.	computer security;reactor (software);requirement;system lifecycle;systems design	Jaekwan Park;Jeyun Park;Youngki Kim	2013	Science China Information Sciences	10.1007/s11432-013-4792-y	computer security	AI	-63.83386526657275	8.027764442335325	86812
49d46a9c00a1ea355abe0a18aa1defde63e48243	object-oriented methods: current practices and attitudes	object oriented methods;analysis and design;object oriented;system development	A total of 160 experienced object-oriented (OO) developers and OO trainees responded to a survey covering current practices and attitudes toward various OO methods, in particular and some aspects of OO systems development, in general. The survey probed into developers' training in OO analysis and design (OOAD) methods; familiarity, preference and use of OOAD methods; the use of OO tools; and attitudes toward OOAD methods. The purpose of this paper is to report the findings in these areas. Overall, (1) trainees are relying more on universities and less on self-instruction compared to existing developers; (2) the Booch, OMT and Jacobson methods are known by, preferred by, and used by developers more than other methods; (3) automated tools are used regularly by less than 50% of developers; and (4) attitudes toward OOAD methods by developers and trainees are favorable, although some differences do exist. The results are important for understanding the current state of, and attitudes toward, OOAD methods as interest in this area continues to grow and mature.		Richard A. Johnson;Bill C. Hardgrave	1999	Journal of Systems and Software	10.1016/S0164-1212(99)00041-2	simulation;computer science;systems engineering;software engineering;management science;programming language;object-oriented programming;management	OS	-71.59010623960806	23.912124592632996	86829
cfd6f801d3ef0c251d349ad8ed2e36beb610cc58	an ism approach for modelling the enablers in the implementation of total productive maintenance (tpm)	tpm;total productive maintenance;enablers;interpretive structural modelling	Total Productive maintenance (TPM) is increasingly implemented by many organizations to improve their equipment efficiency and to obtain the competitive advantage in the global market in terms of cost and quality. But, implementation of TPM is not an easy task. There are certain enablers, which help in the implementation of TPM. The utmost need is to analyse the behaviour of these enablers for their effective utilization in the implementation of TPM. The main objective of this paper is to understand the mutual interaction of these enablers and identify the ‘driving enablers’ (i.e. which influence the other enablers) and the ‘dependent enablers’ (i.e. which are influenced by others). In the present work, these enablers have been identified through the literature, their ranking is done by a questionnaire-based survey and interpretive structural modelling (ISM) approach has been utilized in analysing their mutual interaction. An ISM model has been prepared to identify some key enablers and their managerial implications in the implementation of TPM.	autonomous robot;class diagram;computer cluster;implicit shape model;interaction;linkage (software);reliability block diagram;resultant;sfiaplus;trusted platform module	Rajesh Attri;Sandeep Grover;Nikhil Dev;Deepak Kumar	2013	Int. J. Systems Assurance Engineering and Management	10.1007/s13198-012-0088-7	systems engineering;engineering;knowledge management;operations management	SE	-78.69247988751984	9.755260898351413	86858
81680c4060da52591b9e6f0de5b731fa28169ffe	the uneven diffusion of collaborative technology in a large organization	oil and gas	This paper investigates the large-scale diffusion of a collaborative technology in a range of different business contexts. The empirical data used in the article were obtained from a longitudinal (2007–2009) case study of a global oil and gas company (OGC). Our study reports on ongoing efforts to deploy an integrated collaborative system that uses Microsoft SharePoint (MSP) technology. We assess MSP as a configurational technology and analyze the diffusion of a metadata standard developed in-house, which forms an embedded component of MSP. We focus on two different organizational contexts, namely research and development (R&D) and oil and gas production (OGP), and illustrate the key differences between the ways in which configurational technology is managed and used in these contexts, which results in an uneven diffusion. In contrast with previous studies, we unravel the organizational and technological complexity involved, and thus empirically illustrate the flexibility of large-scale technology and show how the trajectories of the various components are influenced by multiple modes of ordering.	embedded system;facebook platform;max;sharepoint	Gasparas Jarulaitis	2010			simulation;engineering;operations management;operations research	HCI	-78.23327059936581	5.864303544199989	86941
b05767c1c1740136d3a25452f1f47108a8c6c325	the effects of software processes on meeting targets and quality	meeting targets;project management;competition;requirements management;project management software engineering software development management systems re engineering configuration management;software processes;software engineering;customer satisfaction;cross functional teams software processes meeting targets product quality productivity time to market customer satisfaction project size project complexity competition reengineering requirements management project planning defect tracking configuration management design inspections code inspections survey senior practitioners project outcomes;code inspections;senior practitioners;defect tracking;cross functional teams;software quality software engineering software performance computer industry customer satisfaction project management coordinate measuring machines scheduling productivity time to market;time to market;productivity;product quality;project size;project complexity;survey;reengineering;design inspections;configuration management;project outcomes;software development management;software process;project planning;systems re engineering	Firms developing sojtware face increasing pressures to improve product quality, productivity, time to market, and customer satisfaction. As projects increase in size and complexity, and competition grows, firms are reengineering their sofnyare processes. They are adopting more intensive procedures for requirements management, project planning, defect tracking, configuration management, and design and code inspections, and so forth. To explore the potential eflectiveness of these efforts, we conducted a survey of senior practitioners at the 1993 Software Engineering Process Group National Meeting. The survey asked participants about the processes followed on, and the outcome of a specific software project. Certain practices, notably project planning and cross functional teams, were consistently associated with favorable outcomes. Based on the survey results, other practices may have little impact on project outcomes.	code refactoring;configuration management;defect tracking;requirement;requirements management;software engineering process group;software bug;software project management	Christopher Deephouse;Dennis Goldenson;Marc I. Kellner;Tridas Mukhopadhyay	1995		10.1109/HICSS.1995.375677	project management;software review;team software process;requirements management;productivity;competition;software quality management;software engineering process group;software project management;business process reengineering;knowledge management;software engineering;configuration management;customer satisfaction;management;software quality control;software development process;project planning	SE	-65.91413199941046	28.40457894226441	86987
0ed1fe96f14f58c24e3ad08da75618eb572146e8	component-based process modelling in health care	cost saving;component;market dynamics;based;structural change;information management;care;process modelling;process optimization;health;health care	Structural changes and increasing market dynamics in the health care sector intensify the hospitals’ need for cost-savings and process optimization. A first step is the documentation of processes in order to clarify the actual needs. As in health care processes are rather complex and often different players with divergent demands are involved, a disciplined approach to effectively and efficiently model processes is required. For this purpose, in this contribution a component-based modelling approach is presented and applied.	component-based software engineering;documentation;mathematical optimization;process modeling;process optimization	Lars Baacke;Tobias Mettler;Peter Rohner	2009			actuarial science;economics;operations management;process optimization;structural change;component;health;management science;information management;health care	SE	-64.01484143294367	11.253620677788446	87017
de6331ed06e51394a21de5b1a9a9a7db2e1e709f	information flow in reverse logistics: an industrial information integration study		With the coming of low-carbon society, the reverse logistics of used batteries for lowering the carbon emission becomes an important research topic; in which, information integration of reverse logistics is the key for implementing reverse logistics systems. Currently there are not many enterprises that are capable of using enterprise systems or e-business systems to manage reverse logistics. In the framework of industrial information integration engineering, this research investigates the process of reverse logistics of used batteries, with an emphasis on the information integration of reverse logistics of used batteries.	electronic business;enterprise system;informatics;information flow;information management;logistics	Xianliang Shi;Ling Xia Li;Lili Yang;Zhihua Li;Jung Y. Choi	2012	Information Technology and Management	10.1007/s10799-012-0116-y	logistics;humanitarian logistics;operations management;integrated logistics support	DB	-63.776169728464154	8.958742632328466	87074
56181ea70ab9ecde18fe0c2c3076a00bff10b75b	understanding software processes through system dynamics simulation: a case study	simulation planning software development system dynamics system dynamics simulation project management simulation model;digital simulation software engineering software development management project management planning;project management;communication systems;system dynamics;project manager;data collection;software systems;software development process;testing;transportteknik och logistik;software engineering;lead time;process management;computer aided software engineering;computational modeling;logistics;feedback loop;software development;system dynamics simulation;datavetenskap datalogi;software systems computer aided software engineering testing computational modeling computer simulation differential equations feedback loop logistics communication systems conferences;planning;differential equations;simulation tool;simulation model;computer simulation;software development management;software process;digital simulation;conferences;simulation planning	This paper presents a study with the intent to examine the opportunities provided by creating and using simulation models of software development processes. A model of one software development project was created through means of system dynamics, with data collected from documents, interviews and observations. The model was simulated in a commercial simulation tool. The simulation runs indicate that increasing the effort spent on the requirements phase, to a certain extent, will decrease the lead time and increase the quality in similar projects. The simulation model visualizes relations in the software process, and can be used by project managers when planning future projects. The study indicates that this type of simulation is a feasible way of modelling the process dynamically although the study calls for further investigations as to how project or process managers can benefit the most from using system dynamics simulations.	simulation;system dynamics	Carina Andersson;Lena Karlsson;Josef Nedstam;Martin Höst;Bertil I. Nilsson	2002		10.1109/ECBS.2002.999821	computer simulation;project management;team software process;simulation;computer science;systems engineering;engineering;software engineering;goal-driven software development process;software development process;computer engineering	Logic	-67.344714701037	17.98841912106669	87083
0ad0b2a91bb922b5bda759686baf18a34d94e0c2	information systems security risk assessment: harmonization with international accounting standards	databases;trust;information systems security;information systems;assets assessment;threat identification;information security;information system security risk assessment;time dimension information security information risk quantitative assessment data source assets assessment;threat acceptance information system security risk assessment international accounting standard trust threat identification threat reduction;risk management;limit of quantitation;threat acceptance;companies;international accounting standard;information systems information security risk management risk analysis monitoring management information systems communication system control terminology iso standards nist;servers;international accounting standards;accounts data processing;risk assessment;quantitative assessment;time dimension;security of data accounts data processing information systems;data source;security;information risk;security of data;threat reduction	This paper emerges from research by (Alter, S. et al., 2004), (Dillard, K. et al., 2004), (Landoll, D.J., 2006) and (Soliman, K., 2006), and it draws on real-world examples so as to underline some limits of quantitative risk assessment. The paper is a case study and emphasized that theoretical formulas used in information security risk assessments do not contain the time dimension of the analysis. The article further develops findings published in our article Information Security Risk Assessment: The Qualitative versus Quantitative Dilemma (Soliman, K., 2006) as we agree that the risk of information system security may only be assessed or estimated, but in practice, it cannot be measured accurately. A degree of trust should be associated with the assessment made by the security analyst. There are other elements that must be evaluated: average time for threat identification, average time for releasing technical procedures to reduce or accept threat and average time necessary until the system becomes operational and the threat is eliminated. The value of loss is different in any of the three moments and should be estimate for any of them.	information security;information system;risk assessment	Adrian Munteanu;Doina Fotache;Octavian Dospinescu	2008	2008 International Conference on Computational Intelligence for Modelling Control & Automation	10.1109/CIMCA.2008.26	risk assessment;multiple time dimensions;detection limit;quantitative research;computer science;information security;data mining;risk analysis;trustworthy computing;computer security;information system;server;factor analysis of information risk	SE	-81.88726131053279	16.803354868516138	87137
fe3eb3bf0cfe77a4597e774eff13ac29dbbb97c0	a step by step approach for adopting plse	project management;top down;software engineering;product line software engineering software product line;costs investments manufacturing systems programmable control manufacturing automation software engineering product development delay estimation embedded system microprogramming;software product line;project management software engineering;high risk	Software product line concept (P. Clements, 2001) must be one of the solutions to increase the efficiency of the development for our various products in general. However, under a lot of pressure to shortening the development period, developers tend to avoid high initial investment of PLSE (including initial cost, training efforts and high risk) by contraries. There is an example that top-down approach is effective in overcoming this difficulty (P. Clements, 2001). But since we cannot stop our business, we consider the step-by-step approach which respected an actual result and certainty.	software product line;top-down and bottom-up design	Shigeru Hitomi;Hitoshi Yamane	2004	11th Asia-Pacific Software Engineering Conference	10.1109/APSEC.2004.10	project management;reliability engineering;personal software process;long-term support;verification and validation;software quality management;software engineering process group;software sizing;software project management;systems engineering;engineering;package development process;social software engineering;software development;software design description;software engineering;top-down and bottom-up design;software construction;systems development life cycle;software walkthrough;software maintenance;management;software deployment;software development process;software system;product engineering	SE	-64.95813763618786	27.68193590235294	87144
cb31627d2a3ff3b1216ec90e32edd4bfe0990867	strategic management for product development	strategic management;software engineering;value added;product development	ProjectLEADER® PETERS &company Project Management and Engineering, Inc. ProjectLEADER® Training h ProjectMAGIC® Consulting h ProjectMAN® Simulations 317.873.0086 h 1.888.873.0086 h Fax 317.873.0052 h www.projectLEADER.com Organizations demand valid product delivery dates. First, leverage this mandate to develop valid product delivery schedules. Use this to increase the project management skill set of the Office of Product Development staff and to teach project management methodology to the business units. Use this as a time to assess the challenges and needs of the business units. Second, get clarity on the purpose, mission, roles, and responsibilities of the three players (steering committee, OPD, and business units) involved in product development. Third, plan the strategy and tactics of the Product Development Office – write its business plan. Finally, instill project discipline in the product development teams using product launches and progress reviews as the tools to motivate adoption.		Anne-Maria Aho;Lorna Uden	2013	Business Proc. Manag. Journal	10.1108/BPMJ-09-2012-0098	personal software process;software engineering process group;economics;systems engineering;engineering;knowledge management;value added;marketing;social software engineering;operations management;software development;requirement;management;software development process;new product development;strategic management;product engineering	DB	-69.21411072718935	18.04023186939582	87241
b98ba00b2250b1c61b9c8de5a5fd4afac79d1735	the business of software: the ontology of paper	software engineering;next generation	The next generation of software engineering will involve designing systems without using paper-based formats, instead using software to develop software.	software engineering	Phillip G. Armour	2009	Commun. ACM	10.1145/1435417.1435427	personal software process;verification and validation;software engineering process group;software sizing;software verification;computer science;package development process;backporting;social software engineering;component-based software engineering;software development;software construction;software walkthrough;software analytics;resource-oriented architecture;software deployment;software requirements;software quality;software system;software peer review	SE	-63.150050885870634	25.421475408230698	87262
dcbb5ae112f5b15b8432f4c19fa72cf24090d3b1	a critical to quality factors choice: an integrated ahp-qfd model for information quality		Quality of information is critical to corporate decision support. However, ensuring quality of information is not a straight forward task, due to the intertwined nature of the information quality dimensions. Particularly, measuring the impact of each dimension on other dimensions has proved to be extremely difficult. This paper presents a case study of measuring information quality in a manufacturing organization. It applies six-sigma approach to the product perceptive of information. In doing so, the paper utilizes analytical hierarchy process to find the correlation between information quality dimensions. The paper then applies the quality function deployment model to determine critical to quality factors for managing information quality.	critical to quality;information quality;quality function deployment	Abrar Haider;Sang Hyun Lee	2012		10.1007/978-3-642-34228-8_13	management science;knowledge management;analytic hierarchy process;information quality;critical to quality;decision support system;computer science;quality function deployment	Logic	-77.97926103070539	8.897114336088288	87420
608f6e1d890fc638e78d606e7b59685e55dc0a95	quality management in open source projects - experiences from the open ecard project		Open Source Software (OSS) has immensely increased in popularity over the years and it is well known, that software with public access to the sources is on average less error prone than closed source software, especially if the project is supported by a large community which peer reviews the sources [Kua02]. For new and smaller projects however there is no large community yet and hence achieving and maintaining sufficient product quality is challenging. Against this background the present paper discusses aspects of product quality management for OSS in general and shares the experiences gathered in the Open eCard project, which has developed an ISO/IEC 24727 based eID client.	broadcast signal intrusion;cognitive dimensions of notations;content management system;iso/iec 42010;indiegogo;open sound system;open-source software;test automation;usability	Daniel Nemmert;Hans-Martin Haase;Detlef Hühnlein;Tobias Wich	2015			systems engineering;engineering;database;world wide web	SE	-71.63737146946845	29.176662538198567	87466
f999d2209bf2c10055567ac2eb85d552675df1ec	editorial: the first international workshop on verification and validation of enterprise information systems (vveis 2003)	enterprise information system;verification and validation	One of the basic problems in the history of computer science, and more recently of software engineering, is to ensure reliability and correctness of the systems produced. Hence, verification and validation issues have been, are, and will be a great concern for the community. In particular, the area of ‘enterprise information systems’ (EISs) is continuously pushing ahead the complexity of the systems involved, uncovering fresh challenges as new application domains are considered and new technologies are combined or created. Developing systems in the area of EISs involves dealing with considerable difficulties such as distributed systems, interaction via synchronous or asynchronous communication, consistency of data, as well as security and performance issues. After several decades of sustained effort, verification and validation techniques are now available to industry and business related developers, not just in theory, but also through real tools that can be incorporated into the development cycle. The problems are still numerous though, as systems grow and new technologies are considered. Having in mind that EISs are a continuous source of interesting challenges for the verification and validation community, we established a workshop that is to be organized annually as one of the satellite events of the International Conference on Enterprise Information Systems (ICEIS). It is the aim of this workshop to stimulate the exchange of ideas/experiences of practitioners, researchers and engineers working in the area of validating/verifying software for EISs. Since one of the aims of the workshop is to stimulate dialogue between people working in the area from different perspectives, a wide range of contributions are welcomed, both practical and theoretical papers, including case studies, from all areas related to increasing confidence in the correctness of EIS software. This covers a diverse range of topics, including: verification, for example through model checking, as well as system validation and testing. The first edition of the workshop was held in April 2003 at Angers, France, where a variety of presentations covered theoretical and practical issues. We selected three articles that reflect that variety.	application domain;correctness (computer science);distributed computing;enterprise information system;history of computer science;history of computing hardware;knowledge spillover;model checking;software engineering;verification and validation	Juan Carlos Augusto;Marc Roper	2003	Softw. Test., Verif. Reliab.	10.1002/stvr.282	verification and validation;verification;software verification;computer science;systems engineering;software engineering;data mining;enterprise integration	SE	-65.41850787192033	24.697596473985723	87571
dc255aa112ae8bd9dddb7ec5225ba0c42f82d8e6	research of b2b e-business application and development technology based on soa	web service;service oriented architecture	Today, the B2B e-business systems in most enterprises usually have multiple heterogeneous and independent systems which are based on different platforms and operate in different functional departments. To deal with the increased services in future, an enterprise needs to expand its system continuously. This, however, will cause great inconvenience to the future systemmaintenance. To implement e-business successfully, a unified internal e-business integration environment must be established to integrate the internal system and thus realize a unified internal mechanism within the enterprise e-business system. The SOA (service-oriented architecture), however, can well meet the above requirements. The integration of SOA-based applications can reduce the dependency of different types of IT systems, reduce the cost of system maintenance and the complexity of the IT system operation, increase the flexibility of the system deployment, and at the same time exclude the barrier of service innovation. Research and application of SOA-based enterprise application systems has become a very important research project at present. Based on SOA, this document designs an enterprise e-business application model and realizes a flexible and expandable e-business platform. © Springer Science+Business Media, LLC 2011.	electronic business;service-oriented architecture	Li Liang Xian	2009		10.1007/978-1-4419-7355-9_31	service delivery framework;service-oriented architecture;service design;service;data as a service;oasis soa reference model	EDA	-70.2332767856436	11.631211290413265	87578
f6ce863ca85dc08b78b91f979015aa1999e72dfb	asm.br: a template for specifying indicators		Measurement is a crucial practice for software organizations to monitor projects and improve processes. It defines indicators and provides information to support decision-making. Defining indicators is not a trivial task. Although the literature proposes several indicators, there is not a consensus about how to specify them. This paper presents ASM.br (Assistance for Software Measurement based on relationships), a template for specifying indicators by using a one-page form in which textual and graphical information is recorded and the relationships between indicators and between them and goals are put explicitly. By using ASM.br, indicators are recorded in a standardized way, easing understanding and reuse. ASM.br was applied in a software organization and the results suggests its feasibility and utility.	graphical user interface;software measurement	Sylvio Bonelli;Gleison Santos;Monalessa Perini Barcellos	2017		10.1145/3084226.3084280	systems engineering;software;data mining;reuse;computer science;software measurement	SE	-66.32278875866042	16.743657493112238	87644
7cb866fe97c5f90c9c3e00fbdd62a26895baede7	a data quality management and control framework and model for health decision support	data quality control;inspection data models quality control process control quality assessment complexity theory;quality assessment;quality assessment health decision support data quality control quality inspection;quality inspection;health care data analysis decision support systems;health decision support;problematic data processing method data quality management framework data quality control framework health decision support systems key data terms nonuniform data standards health data complexity health data diversity data quality inspection method data quality processing data quality assessment	Health data quality issues are important influencing factors on the validity and scientific nature of health decisions. However, a series of health data quality issues have emerged, such as serious lack of key data terms and non-uniform data standards because of the complexity and diversity of health data. Based on the current state of data quality for health decision support, the paper proposes a framework and model for health data quality management and control. And a method of data quality inspection, processing and assessment applicable for the health data with complex types is designed based on the proposed model. On the one hand, it defines the inspection model of the semi-structured health data, designs the quick calling strategy of the inspection rules, and makes clear the data quality inspection method and problematic data processing method. On the other hand, it proposes a mathematic model for objective and quantitative evaluation of the health data. This paper provides future reference for addressing the data quality management of health decision support systems and to protect for the practical value thereof.	data model;data quality;decision support system;semiconductor industry;traceability	Tao Dai;Hongpu Hu;Yanli Wan;Quan Chen;Yan Wang	2015	2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2015.7382218	data governance;quality policy;data quality;data management;data mining;management science;information quality	DB	-80.78310992270815	15.528747001415887	87665
b6acd8457d6d435165ed531171df2f825d1bb0ce	the implications of india's special characteristics for fdi spillovers		A number of recent studies examine technology transfer from foreign direct investment (FDI) to India's domestic industrial enterprises. This study goes further by investigating the implications of institutions for the nature of technology spillovers during 2004 - 2013. The author examines three institutional features that comprise aspects of India's â€œspecial characteristicsâ€ : (1) the different sources of FDI, where FDI is nearly evenly divided between mostly Organization for Economic Co-operation and Development (OECD) countries and Mauritius; (2) India's heterogeneous ownership structure, involving state- (SOEs) and non-state owned (non-SOEs) enterprises; and (3) industrial promotion via tariffs or through tax holidays to foreign direct investment. The author found robust positive and significant spillovers (as measured by total factor productivity) to domestic firms via backward linkages (the contacts between foreign buyers and local suppliers). The results suggest varied success with industrial promotion policies. Final goods tariffs as well as input tariffs are negatively associated with firm-level productivity. However, the author found statistically significant evidence of stronger productivity spillovers associated with firms that received tax breaks, suggesting that tax holidays were more successful than tariffs as an instrument to promote productivity growth in India.	fault detection and isolation	Manoj Kumar	2016	IJABIM	10.4018/IJABIM.2016070101	economics;international trade;macroeconomics;economy;management;economic growth	HCI	-84.61408126588776	6.255466461137348	87750
e0a0ab02c82f3d122e3451a17ee66c28225be7fb	social network of software development at github	social network services;collaboration;data mining;monitoring;open source software;java	This paper looks at organization of software development teams and project communities at GitHub. Using social network analysis several open-source projects are analyzed and social networks of users with ties to a project are shown to have some scale-free properties. We further show how to find core development group and a network metric is introduced to measure collaboration among core members, corresponding to if a project is healthy and more likely to be successful.	open-source software;social network analysis;software development;telecommunications network	William Leibzon	2016	2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)	10.1109/ASONAM.2016.7752419	software project management;computer science;knowledge management;social software engineering;data mining;java;world wide web;collaboration	SE	-75.49314284635082	22.00888488913123	87769
1329f06d2dd3697dd7620c4e466da7f52f25e796	e-business maturity and information technology	e business;information technology;stages of growth;maturity;conferenceobject	Maturity models describe the maturing of the use of information systems in organizations. They are a useful framework to describe an organization’s current position as well as a range of possible position in the future in terms of their e-business maturity. The relationship between Information Technology (IT) and e-business maturity is examined. We used a model of e-business maturity, Stages Of Growth for e-business (SOGe) model, to put an organization in a maturity stage. In our survey we presented a set of technologies and we asked to enterprises which of them are implemented, in development, planned or inexistent. We concluded that there is a strongly correlation between IT implementation and the e-business maturity.	capability maturity model;electronic business;information system	Elisabete Morais;José Adriano Pires;Ramiro Moreira Gonçalves	2009			maturity;maturity;information technology;maturity	HCI	-79.83860295789553	4.514445751353715	87801
cd3d159ab1c18d52d5b02a62c6c4265ee644c727	a knowledge-based framework for software development outsourcing decision	decision support system;fahp;software development outsourcing;system dynamics modeling;software development;knowledge base		outsourcing;software development	Minghui Wu;Hareton K. N. Leung	2005			software peer review;goal-driven software development process;software mining;package development process;software engineering process group;knowledge process outsourcing;outsourcing;social software engineering;business;knowledge management	SE	-64.53905512917358	21.38124724971635	87821
5cdff890e7fd30425bede09f8a786aac95964ee9	an exploratory study to identify complementary resources to the implementation of web-based applications in a paint supply chain	web based applications;resource based theory;complementary resources;supply chain;exploratory study;supply chain management;competitive advantage	This study focuses on identifying (a) the complementary resources that influence the successful implementation of Web-based applications for supply chain management, and (b) the degree to which certain types of complementary resources function to support the successful implementation of Web-based applications. An exploratory case study is employed to identify potential complementary resources that are required for the successful implementation of Web-based applications that support various portions of the supply chain management task. The utility of a potential complementary resource to each Webbased application for a supply chain task is evaluated to determine the relative value of the resource to the Web-based application. A matrix is then developed to show the degree to which a particular complementary resource is utilized to implement Web-based applications. The contribution of this study is to enhance our understanding of how Web-based applications and complementary resources can work together to create competitive advantages in supply chains.	emoticon;exploratory testing;value (ethics);web application;world wide web	Yootaek Lee;Jay Kim;Jeffery Miller	2008	IJISSCM	10.4018/jisscm.2008040103	web application;supply chain management;economics;knowledge management;marketing;operations management;supply chain;exploratory research;competitive advantage	HCI	-78.0764343410862	6.305795389505575	87872
8246b18c628113aeab1a1a1e69b17e44927d1c23	kpi-based decision evaluation system to enhance qmss for higher educational institutes		Innovation has always been the engine of economic growth. Nowadays, billions are disbursed on research and universities are working increasingly closely with industries to help them developing new products and processes. In order, to maintain this important role and be more competitive, higher education institutions (HEIs) are trying to assure a high level of quality by following standards and using a sophisticated quality management system (QMS). The use of decisions support systems as enabler for QMS is an approved concept. However, it is not well introduced in the field of education despite its proven results within other domains. Here, we aimed to enhance the decision-making act in the application HEIs through the presentation of a novel concept called decision evaluation system. This system enables stakeholders to track, evaluate, recommend and comment decisions. It meets the recommendations of ISO 9001:2015 and specially the quality management principle 6: “make decisions based on evidence.” In this work the global process and the reference architecture will be presented. In conclusion, our work shows that HEIs take advantages from historical decisions and increases the ability to review, challenge and change opinions based on experience and knowledge sharing. KEywoRDS Business Intelligence, Decision Evaluation System, Decision Support Systems, Quality Management Systems, ISO9000: 2015 KPI-Based Decision Evaluation System to Enhance QMS for Higher Education Institutes		Abdelkerim Rezgui;Jorge Marx Gómez;Raji Ben Maaouia	2017	IJDSST	10.4018/IJDSST.2017040103	knowledge management;artificial intelligence;marketing;data mining;management science;management	HCI	-70.63829487137328	10.294463904167445	87899
4bd5aecc9e4fe52729f04f7441057be3ed209d09	research on key nodes appraising method for adaptive change management of knowledge-based enterprise based on the grey relational analysis	change management;management of change commerce grey systems learning artificial intelligence;neural immune learning networks;grey relational analysis;appraisal knowledge management environmental economics research and development management intelligent networks intelligent structures decision making functional analysis nonhomogeneous media kernel;commerce;learning networks;adaptive change management;grey systems;management of change;adaptive change management knowledge based enterprise grey relational analysis neural immune learning networks;learning artificial intelligence;knowledge based enterprise;knowledge base	Neural-immune-learning networks that structural embedded in the networks of knowledge-based enterprise is the kernel sub-networks, and it sustains the creation of the abilities of sense-respond-decision, immune-adjusting and learning. The key nodes and relationships are the key points of the adaptive organic change for manager. In this paper, the grey relational analysis method of recognizing key nodes for NIL-networks is given.	embedded system;grey relational analysis	Wenping Wang;Qiuying Shen	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4413740	statistical relational learning;computer science;knowledge management;artificial intelligence;grey relational analysis;change management;data mining	Robotics	-74.61163342633321	8.289774071151726	87906
25e53f9f008ce348a7af727158ffd43c4a0f78c4	experiences with planning techniques for assisting software design activities	quality attributes;object oriented design;mixed initiative;design space;design technique;software design	Any software design process can be seen as a workflow of design tasks, in which the developer makes different decisions regarding both functionality and quality-attribute properties of the intended system. However, ensuring the right compromises among design solutions is always a problematic and error-prone activity for the developer. Furthermore, he/she may get overwhelmed by the diversity of design techniques and technologies nowadays available. Along this line, we believe that it is possible to provide a declarative representation of this knowledge, so as to apply AI techniques when searching for solutions in the design space. Specifically, mixed-initiative planning algorithms are an interesting approach to automate some aspects of that search. In this paper, we report on three successful experiences with planning to assist the developer in decision-making for architectural and object-oriented design contexts. We also analyze the perspectives of a planning-based approach in the development of software design tools.	automated planning and scheduling;bayesian network;biconnected component;cognitive dimensions of notations;computer-aided software engineering;database schema;declarative programming;deterministic algorithm;hierarchical task network;instance (computer science);intelligent agent;interpretation (logic);nonlinear gameplay;overhead (computing);precondition;smartbook;software design	Jorge Andrés Díaz Pace;Marcelo R. Campo	2007	Applied Intelligence	10.1007/s10489-007-0081-z	iterative design;environmental design and planning;experience design;computer science;knowledge management;software design;object-oriented design;interaction design pattern;environmental graphic design;management science;design education;design technology;design brief;generative design	SE	-63.62081353060822	18.15759027709246	88059
8f80289eb88158856b83a35840717d76c0416e0e	competitiveness, flexibility and management controls: mobilising and managing flexibility in four smes	constructivism;management control;strategy;management accounting;flexibility	"""Flexibility often stands for competitiveness. Through a multiple case study of four SMEs, this paper analyses how flexibility is drawn into organisational situations and how it is managed through certain management control inscriptions. All four firms claim to be flexible, but they do so in very different ways, and the supporting management control mechanism differs widely among the situations. Flexibility is only """"something"""" under specific conditions and it may even mean completely opposing things in different firms, e.g. both 'more components"""" and """"fewer components"""". This paper analyses flexibility and the associated management control inscriptions which translate it into competitiveness."""		Allan Hansen;Carsten Orts Hansen;Jan Mouritsen	2000	IJMTM	10.1504/IJMTM.2000.001375	management control system;economics;strategy;operations management;management;management accounting;constructivism	HCI	-76.41109596939009	8.312351711736586	88091
16cb2555559eedafa3905f7dbb542e8d641b43b7	a new approach to integrate internet-of-things and software-as-a-service model for logistic systems: a case study	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;internet of things;logistic cloud;uk phd theses thesis;life sciences;cloud manufacturing;software as a service;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	Cloud computing is changing the ways software is developed and managed in enterprises, which is changing the way of doing business in that dynamically scalable and virtualized resources are regarded as services over the Internet. Traditional manufacturing systems such as supply chain management (SCM), customer relationship management (CRM), and enterprise resource planning (ERP) are often developed case by case. However, effective collaboration between different systems, platforms, programming languages, and interfaces has been suggested by researchers. In cloud-computing-based systems, distributed resources are encapsulated into cloud services and centrally managed, which allows high automation, flexibility, fast provision, and ease of integration at low cost. The integration between physical resources and cloud services can be improved by combining Internet of things (IoT) technology and Software-as-a-Service (SaaS) technology. This study proposes a new approach for developing cloud-based manufacturing systems based on a four-layer SaaS model. There are three main contributions of this paper: (1) enterprises can develop their own cloud-based logistic management information systems based on the approach proposed in this paper; (2) a case study based on literature reviews with experimental results is proposed to verify that the system performance is remarkable; (3) challenges encountered and feedback collected from T Company in the case study are discussed in this paper for the purpose of enterprise deployment.	business process;cloud computing;computation (action);customer relationship management;deploy;erp;enterprise resource planning;internet of things;management information systems;management information system;programming languages;programming language;review [publication type];scalability;software as a service	Shang-Liang Chen;Yun-Yao Chen;Chiang Hsu	2014		10.3390/s140406144	computer science;bioinformatics;engineering;knowledge management;electrical engineering;software as a service;data mining;internet of things	AI	-69.66196722691492	11.392172936591006	88093
2746974ab2369409498f2fac827ce658df881374	open source migration in greek public sector: a feasibility study		Open Source software has been recently recognized by governments as a viable and cost effective solution. However, transition to open source is not a plug-and-play process but one that requires deep knowledge of open source dynamics and of organization's operations, budgetary constraints, capacities, ethics and political agenda. As with any IT transition, there are uncertainties and risks that need to be handled in order to maximize the gains for the organization and for the society through the provided services. In this paper we present a feasibility study conducted in 15 Greek public sector organizations with the aim to discover the value this transition brings to a typical public sector organization.	monte carlo method;open sound system;open-source software;plug and play;process migration;ps (unix);resource-oriented architecture;simulation;value (ethics);volatility	Androklis Mavridis;Dimitrios Fotakidis;Ioannis Stamelos	2012		10.1007/978-3-642-33442-9_15	environmental resource management;knowledge management;computer science;software;public sector;cash flow;call option;political agenda;strike price;budget constraint	SE	-73.648396875321	8.49036959585167	88100
1db0f9e4ef446e28d8c8fcc1f9817e2dc195aada	split - workshop on software product line testing	product line;software development;product line engineering;software product line;weed management	 Product line engineering (PLE) has become a major topic in industrial software development, and many organizations have started to consider PLE as state of the practice. One topic that needs greater emphasis is testing of product lines. Product line testing is crucial to the successful establishment of PLE technology in an organization. The workshop aims at addressing some of the open fundamental challenges of testing in a PLE setting. How can we manage the complexity of the test space? Can we leverage our established testing tools and procedures? A particularly hard challenge for test groups in a PLE setting is keeping pace with development productivity gains. If software developers can create unique product instances 10 times faster using PLE techniques, how does the test organization keep pace without having to hire 10 times as many test engineers? Are there PLE techniques that can provide efficiency gains for testing similar to those for development? These are questions that we have to face when transitioning to PLE, and, without adequate answers, testing becomes the bottleneck in PLE.	software product line	Birgit Geppert;Charles W. Krueger;J. Jenny Li	2004		10.1007/978-3-540-28630-1_37	systems engineering;engineering;product lifecycle;software development;weed control;software engineering;manufacturing engineering	SE	-67.4138955414287	23.142788586364766	88111
4354125f383ede6be4c37f12c5c79673e57d61da	personality, emotional intelligence and work preferences in software engineering: an empirical study	empirical study;personality;computer and information science;five factor model;software engineering;natural sciences;self compassion;programvaruteknik;teique	Objective: Various studies on personality in SE have found large, small or no effects and there is no consensus on the importance of psychometric measurements in SE. There is also a lack of studies employing other psychometric instruments or using larger datasets. We aim to evaluate our results in a larger sample, with software engineers in an earlier state of their career, using advanced statistics.	cluster analysis;emoticon;intellect;numerical analysis;psychometric software;replication (computing);software development process;software engineer	Makrina Viola Kosti;Robert Feldt;Lefteris Angelis	2014	Information & Software Technology	10.1016/j.infsof.2014.03.004	knowledge management;personality;big five personality traits;empirical research;management	SE	-72.31291688416617	22.20003682967783	88116
6cda2a8f80c427b794cf62860d302fdcef2d3597	modeling e-commerce website quality with quality function deployment	quality assurance;reliability;electronic commerce;quality function deployment;qfd;e commerce;quality function deployment quality management total quality management aerospace industry conference management engineering management electronic commerce quality assurance technological innovation testing;user survey;kj tool;user quality requirement;customer satisfaction;web sites customer satisfaction electronic commerce quality function deployment;questionnaire survey;total quality management;quality requirement;kj tool e commerce web site quality modeling quality function deployment quality management quality assurance user quality requirement customer satisfaction;qfd quality management e commerce quality requirement;web sites;literature review;interviews;e commerce web site quality modeling;site quality;transmission line matrix methods;correlation;quality management	The quality of a website is a crucial determinant of whether visitors are likely to return to the site. The research of E-commerce quality, such as Loiacono’s WEBQUALTM, neither saw the E-commerce website as a product, nor did it analyze E-commerce quality from the quality management and quality assurance. The main contribution of this paper is to construct an E-commerce website quality model from the users’ quality requirement, based on Total Quality Management’s theory and tools. The E-commerce was seen as the product of an activity process in this research. By Quality Function Deployment, the users’ demands were transformed into the quality requirement of the E-commerce website, so the Customer Satisfaction was met. First, 98 image words about E-commerce quality were collected through user survey and literature review. Using KJ tool, the words were reduced to 15. From them, the most frequent 5 quality words were chosen using user questionnaire survey. Surround the 5 words, the users’ quality requirements were acquired and showed as a level table, based on 5 sample websites and user interview. In the next phase, the quality characters were sorted and analyzed quantificationally. In the end, a hierarchical E-commerce website quality model was built.	e-commerce payment system;quality function deployment;requirement;requirements analysis;software deployment;web development	Chang Jinling;Song Tong;Li Chuncan;Song Tao	2009	2009 IEEE International Conference on e-Business Engineering	10.1109/ICEBE.2009.65	e-commerce;quality function deployment;data quality;marketing	DB	-78.43999483083599	18.764888202324016	88133
5ceb8e7b3d172c0d022f8437a1803118e87188ad	evaluating the performance of swedish savings banks according to service efficiency	evaluation performance;analisis envolvimiento datos;banking;performance evaluation;savings banks;service orientation;evaluacion prestacion;performance;estimation non parametrique;shareholder;secteur bancaire;dea;customer service;commercial banks;service utilisateur;profit;non parametric estimation;actionnaire;service industries;beneficio;data envelopment analysis;industrie service;benefice;profitability;estimacion no parametrica;servicio usuario;user service;data envelope analysis;service efficiency;analyse enveloppement donnee	This article develops principles for an evaluation of the efficiency of a savings bank. It starts out from the observation that such a bank is less profit oriented than a commercial bank. The customer is a vital stakeholder to the savings bank implying a greater emphasis on customer service provision. We are using data envelopment analysis (DEA) as a method to consider the service orientation of savings banks. We thereby demonstrate how an evaluation of the performance of savings banks according to ‘‘service efficiency’’ differs from an evaluation based on the traditional ‘‘profit’’ or shareholder concept. We determine the number of Swedish savings banks being ‘‘service efficient’’ as well as the average degree of service efficiency in this industry. 2006 Elsevier B.V. All rights reserved.	data envelopment analysis	Göran Bergendahl;Ted Lindblom	2008	European Journal of Operational Research	10.1016/j.ejor.2006.08.027	economics;marketing;operations management;data envelopment analysis;commerce	AI	-83.9508214797242	8.511910924814647	88166
a8fe4e79854a385ef8ff9169b6d6e9c4886a8078	technical components and requirements model for supporting collaboration in the product technology transfer process		During the product technology transfer process, the high level of communication, coordination and cooperation among the members involved in this activity is necessary. Making use of a collaborative environment is essential for managing this process and the project. In this sense, the paper presents the technical components and requirements model to support developing this environment and assist the collaboration and product technology transfer. The Enterprise Knowledge Development (EKD) methodology was used to develop the model. The proposed model was based on literature and in seven case studies conducted in small and medium-sized high-technology firms. As a result, this paper shows a group of functional and non-functional requirements as well as the goals that the information system should have to allow the communication and the exchange of information in transfer projects involving collaboration. Furthermore, this research contributes to the information system development that supports these projects.	requirement	Juliana Sayuri Kurumoto;Fábio Müller Guerrini	2013		10.1007/978-3-642-40543-3_25	systems engineering;knowledge management;process management	SE	-63.35817386991252	14.68086645482435	88253
4a88e4f1fac05c6a4953bdfb0243d1ef7606cd3c	business value of it-enabled call centers: an empirical analysis	empirical analysis;call center;business value	Corporate information technology (IT) investments in customer support and service such as CRM systems have been on a steady rise. Of late, the primary interest has shifted toward assessment of returns on these investments. This research attempts to assess the value of IT investments in a customer support setting using a process-level analysis. Given the lack of academic IS research in the area of customer support and value of IT in the service context, this study aims to bridge this gap by building on prior business value of IT literature. In order to identify the contribution of IT in the context of our study, we explicitly control for personnel-specific factors and customer-specific factors. Our findings indicate that IT enabled call centers significantly improve the performance of the customer support process. Further, we also find that the benefits from IT enabled call centers may be higher when the customer-reported problems are complex and difficult to resolve. In addition, we find that both personnel-specific factors and customer-specific factors significantly influence the business benefits from IT in call center and customer support applications.	email;fax;uniform resource identifier	Ramanath Subramanyam;Mayuram S. Krishnan	2001			public relations;customer to customer;knowledge management;marketing;business value;customer retention;management;customer advocacy	HCI	-81.85169066632507	5.144151785625451	88277
22ee2341c78df98b4c7caccefd9a33b752ae9e63	agile game: a project management game for agile methods		Since mid-1990s, companies have adopted agile methods and incorporated them in their development methodologies. For this reason, future project managers and developers need to have a full understanding of these methods. At present, the university’s approach to agile methods is theoretical and is not reflected during the development of a product and their practical use. The purpose of this project is the creation of a software system in the form of a game, named Agile Game, which simulates their use. The system is designed for use as supplementary material in lectures, to help students understand agile methods, to present their use within a project, and to demonstrate how they differ from traditional project management methodologies. The final system, which is web based, was implemented using PHP, MySQL and JavaScript. It was fully tested against the requirements and evaluated by peer students. The evaluation showed that the majority of users were satisfied with the system but they thought that it should contain more detailed information at every step of the game. For this reason, some parts of the design and the content were reviewed to meet user requirements.	agile software development	Aikaterini Gkritsi	2010			simulation;agile unified process;agile usability engineering;systems engineering;engineering;knowledge management;requirement;agile software development;game developer	ECom	-73.28990173231853	23.88287190656121	88322
e030fe37d47d47362317dbfe22d5e364ed6e77b2	how do humans inspect bpmn models: an exploratory study	empirical research;human-centered support;process model maintainability;process model quality	Even though considerable progress regarding the technical perspective on modeling and supporting business processes has been achieved, it appears that the human perspective is still often left aside. In particular, we do not have an in-depth understanding of how process models are inspected by humans, what strategies are taken, what challenges arise, and what cognitive processes are involved. This paper contributes toward such an understanding and reports an exploratory study investigating how humans identify and classify quality issues in BPMN process models. Providing preliminary answers to initial research questions, we also indicate other research questions that can be investigated using this approach. Our qualitative analysis shows that humans adapt different strategies on how to identify quality issues. In addition, we observed several challenges appearing when humans inspect process models. Finally, we present different manners in which classification of quality issues was addressed.	business process model and notation;cognition	Cornelia Haisjackl;Pnina Soffer;Shao Yi Lim;Barbara Weber	2016		10.1007/s10270-016-0563-8	engineering;knowledge management;data mining;management science	AI	-75.8394999678436	12.493289159693987	88371
0b9e71bf98b2198024af7934db56c230f12b3adb	reporting empirical evidence in distributed software development: an extended taxonomy	context industries taxonomy interviews terminology instruments organizations;distributed software development systematization of knowledge expert opinion survey taxonomy empirical evidence;software development management distributed processing pattern classification;distributed software development;instruments;industries;systematization of knowledge;empirical evidence;knowledge synthesis distributed software development empirical dsd evidence classification preliminary taxonomy evaluation;taxonomy;interviews;terminology;organizations;expert opinion survey;context	Distributed Software Development (DSD) has been discussed by industry and academia for almost two decades now, and, as consequence, there is a large number of empirical scientific papers and industrial reports on it. However, the description of the context in which the empirical study was conducted is not always clear or complete, making the process of searching for empirical evidence burdensome. It becomes difficult to understand or to judge the relevance of study given that DSD scenarios are diverse. What works in one context might not apply to another. To reduce such difficulty, we need, as a research community, to have means to standardize how we report empirical studies and their findings aiming to make them more readily available to practitioners and researchers. In this paper we present an extended taxonomy to classify empirical DSD evidence. We conducted an expert opinion survey with researchers and practitioners to identify elements to compose the taxonomy. Preliminary evaluation of the proposed taxonomy suggests that it can be used to synthesize existing knowledge, to identify gaps in literature, to identify related work and to help researchers who will publish or review further empirical work, as well as practitioners who are interested in published empirical studies.	data structure diagram;distributed computing;document structure description;relevance;scientific literature;software development;taxonomy (general)	Antonio Rafael Da Rosa Techio;Rafael Prikladnicki;Sabrina Marczak	2015	2015 IEEE 10th International Conference on Global Software Engineering	10.1109/ICGSE.2015.23	empirical evidence;interview;computer science;organization;knowledge management;data mining;management science;terminology;empirical research;management;taxonomy	SE	-70.97753263769009	22.664675227577593	88387
8117997f25b2eb8b5c68b321dffe697aa52d5f04	dtk - a metaplatform for scientific software development			software development	Julien Wintz;Thibaud Kloczko;Nicolas Niclausse;David Rey	2012	ERCIM News		data mining;software engineering;computer science;software development	SE	-64.61744619875016	23.902808576745535	88480
673406461a41df88b579fc0f540dd06eacebde23	theoretical considerations for make-or-buy decisions during ‘product design and engineering’: three indian case studies	transaction cost economics case studies core competencies product design new product development outsourcing resource based view;outsourcing;case studies;production engineering decision making outsourcing product design product development;core competencies;resource based view;transaction cost economics;outsourcing product design decision making product development companies manufacturing economics;india make or buy decisions product design and engineering manufacturing function decision making product development transaction cost economics resource based view strategic intents in house capabilities outsourcing;product design;new product development	Make-or-buy decisions taken during `product design and engineering' often happen based on incomplete and inaccurate information but have tremendous effects on the manufacturing function; this is an under-researched topic. Based on three cases in India, the study examines the processes and decision-making for make-or-buy during new product development from three theoretical perspectives: Transaction Cost Economics, Resource-Based View and notion of core competencies (in chronological order of being established). Driven by both strategic intents and experiences with suppliers, these companies resorted to developing in-house capabilities rather than continuation of supply. That finding leads to reflection on the validity of current theoretical propositions and can be traced back to literature on outsourcing that dates back decades.	continuation;experience;new product development;outsourcing	Rob Dekkers	2014	2014 IEEE International Conference on Industrial Engineering and Engineering Management	10.1109/IEEM.2014.7058637	transaction cost;economics;systems engineering;marketing;operations management;product lifecycle;knowledge process outsourcing;product management;product design;core competency;management;new product development;commerce;outsourcing;product engineering	DB	-79.2881290764119	5.409632101431679	88591
795c02e1de4cdc9b4c4d12a2570428d8e4682f98	edi adoption and implementation: an examination of perceived operational and strategic benefits, and controls	node encoding;association rules;data mining;xml indexing;xml tree	In recent years, Electronic Data Interchange (EDI) has revolutionized the way in which businesses conduct their trading activities. Even though the popularity and potential attached to EDI is growing rapidly, knowledge regarding the nature of EDI benefits and EDI control practices is very limited. This paper reports the results of a survey of EDI users that explores these key implementation issues. This study focuses on organizational factors that are associated with EDI adoption and implementation. Findings indicate that organizations experience both operational and strategic benefits from EDI. Customer‐initiated EDI users recognized slightly greater EDI strategic benefits than did other users. Also, long‐time users recognized both strategic and operational benefits in greater proportions than did more recent users, and smaller firms more often cited better customer service and convenience (as strategic and operational benefits, respectively) from implementing EDI. An examination of control practices rev...	electronic data interchange	Ram S. Sriram;Vairam Arunachalam;Daniel M. Ivancevich	2000	J. Information Systems	10.2308/jis.2000.14.1.37	public relations;association rule learning;computer science;knowledge management;marketing;data mining;management	HCI	-81.66429654053627	5.9093003759575184	88616
cfa9379d4bbf22f7c303164fccd0825b35df4d1c	conceptualizing resource orchestration - the role of service platforms in facilitating service systems		The digitization, as the profound digital transformation of business activities, processes, competencies, and models, leads in many companies to the implementation of digital service platforms as a nucleus for rapid development and the implementation of reusable digital innovations and solutions. Based on service platforms, companies strive to gain superior resource density that enables interaction with customers and partners in real-time and foster service innovation. This paper focusses on the role and purpose of service platforms in value creation and in particular, the management of resources. For this, a longitudinal single-case study method is conducted that comprises three embedded case studies as units of analysis. In this context, the meta-theoretical foundations of S-D logic are reflected and, as a result, resource orchestration as a management capability of service platforms is introduced and discussed. Moreover, the role of service platforms in managing resources in the development of innovative service systems is examined.	embedded system;real-time transcription;service innovation	Zolnowski Andreas;Warg Markus	2018			orchestration (computing);computer science;knowledge management;digital transformation	Mobile	-76.99910325724976	6.006473430594311	88718
296cbb14cdb329f34eefa5fc33e895428645a465	software development worldwide: the state of the practice	indian software industry;software;programming europe software performance hardware productivity outsourcing iterative methods internet application software workstations;outsourcing;project management;software productivity;socio economic effects software engineering project management outsourcing;software defects;iterative development;waterfall model;software development process;software engineering;software process models;research and development;daily builds;software development;japanese software industry;practice;international software surveys;competence development;outsourcing software development worldwide india japan europe us quantitative data software development practices competing development models;software quality;socio economic effects	"""Research motivations We decided to conduct a new study for three reasons. First, we were interested in how practices and performance levels vary around the world. In particular, one issue that remains unresolved is the accuracy of anecdotal reports of high levels of quality, productivity, and on-time performance in Japanese and, more recently, Indian projects. Many people have speculated that Indian software companies emphasize formal practices and processes (such as the Software Engineering Institute's Capability Maturity Model Level 5 certification) for use as a signal of quality when bidding on outsourcing contracts. However, previous studies of this topic used only small sample sizes 3 or were based mainly on older data from a few companies. 4–6 Second, we wanted to ascertain the degree to which a large sample of projects, regardless of location, used different types of development practices associated with particular development """" models. """" We wanted to assess the penetration of a range of practices—from those associated with more traditional waterfall approaches, which tend to emphasize control and discipline in development, 7 to those focus Software Development Worldwide: The State of the Practice O ver the past decade, researchers have studied the wide range of practices available to software developers 1,2 and the differences in practices and performance levels around the world. 3 This article reports early descriptive results from a new global survey of completed software projects that attempts to shed light on international differences in the adoption of different development practices. Our findings are particularly relevant to firms that are considering the potential advantages or a greater use of outsourcing."""	capability maturity model;focus;outsourcing;software engineering institute;software developer;software development;software industry;ver (command);waterfall model	Michael A. Cusumano;Alan MacCormack;Chris F. Kemerer;Bill Crandall	2003	IEEE Software	10.1109/MS.2003.1241363	project management;engineering management;waterfall model;systems engineering;engineering;software development;software engineering;iterative and incremental development;management;software development process;software quality;outsourcing	SE	-66.54390410541991	27.583296521154523	88739
8964cb3d8ee19a2c41de34a68b5841910fccd44f	modeling a cmm implementation method with spem			capability maturity model;meta-process modeling	Javier Elizondo;Rubens Sales;Jorge Luis Risco Becerra	2004			computer science	EDA	-63.011948536700146	23.367176572945933	88885
f4d9fe50780576010eaac854ad39a2d9e9ac7271	a descriptive structure to assess the maturity of a standard: application to the unl system	costs turning ip networks web and internet services software standards educational technology search engines power generation economics dictionaries;turning;web and internet services;search engines;descriptive structure;information technology;universal networking language;software engineering;international standards descriptive structure unl system universal networking language multilingual services internet;international standards;internet;internet software engineering software standards information technology;dictionaries;ip networks;software standards;educational technology;multilingual services;unl system;power generation economics;internal standard	The authors are actively involved in the task of turning out the UNL (Universal Networking Language) technology into a standard to support multilingual services in Internet. For this purpose, the first steps have been the creation of a supporting organisation. However, this is not enough to ensure the success of a standard, maturity aspects of the technology are to be referred and considered as well. This work is the outcome of the analysis of different international standards that were innovative in Internet and the area of software. Consequently, some criteria have been extracted, considered important for the assessment of the state of maturity of the UNL technology as standard.	capability maturity model;universal networking language	Edmundo Tovar;Jesús Cardeñosa	2001		10.1109/SIIT.2001.968560	educational technology;the internet;universal networking language;computer science;marketing;software engineering;internal standard;multimedia;law;information technology;world wide web	NLP	-72.89813357810563	14.598079817811788	88910
04df45f4ae121b1f301586ddb486d091d2a2e498	electronic intermediaries managing and orchestrating organizational networks using e-services	e services;intermediaries;consumer electronics;lead time;supply chains;demand and supply;organizational networks;value creation;management;requirement specification;orchestration	Organizations increasingly cooperate in organizational networks. Electronic intermediaries can provide all kinds of e-services to support the creation and management of such networks. While there has been substantial discussion on intermediaries matching demand and supply, there has been little analysis in relation to the management and orchestration of organizational networks. In this article we analyze an intermediary that uses e-services for orchestrating a network in the consumer electronics industry. The empirical results show that the coordination and management of networks requires specific expertise and skills which result in the rise of intermediary specialized in orchestrating such organizational networks. The primary value creation activity of the intermediary is leveraging the products, activities and knowledge of the specialized companies and providing e-services for orchestrating the organizational network to create short lead times, improve customer responsiveness and ensure adaptability.	e-services;responsiveness	Marijn Janssen	2009	IJESMA	10.4018/jesma.2009010104	economics;marketing;operations management;intermediary;supply and demand;supply chain;orchestration;management;world wide web;commerce	HCI	-76.88778054628698	5.7160476660509465	89113
5807f73fabcdb856075165e32192e9a3b6308c30	enterprise architectures for addressing sustainability silos		A need exists for behaviour change and transparency in modern organisations where the focus needs to shift towards sustainability thinking rather than just sustainability reporting for compliance reasons. The number of organisations which are undertaking Green Initiatives and reporting on sustainability are increasing. However many of these organisations are not viewing these initiatives strategically. The effect on information requirements and business processes is often not considered and the available tools and technologies are not used to their full potential. As a result, whilst sustainability reports are produced, the underlying infrastructure consists of “sustainability silos” comprising of a lack of integrated systems, inconsistent data and information where the integrity is not reliable. In order to address these issues this study investigates the extent to which organisations consider environmental information requirements and processes when planning their information systems and Enterprise Architecture (EA). The inclusion of Green Initiative strategies into the design of an organisation’s enterprise systems and EA is proposed. This will ensure alignment between environmental management and IT planning and result in integrated systems, an improved sustainability reporting process and more effective decision making regarding the environmental impact of organisations.	business process;enterprise architecture;enterprise system;environmental resource management;information system;integrated development environment;requirement;transparency (graphic);while	Brenda Scholtz;Anthea Connolley;André P. Calitz	2013		10.1007/978-3-642-36011-4_12	construction engineering;systems engineering;engineering;environmental resource management	SE	-70.16106112131821	9.486820697834654	89138
ad72fdec6adc75cf9ed76ad28116f78525db31b0	digital library repository service planning and development	digital library	At its most basic, a repository is a digital resource panelists, all of whom have experience in the management and delivery system. In practical implementation of a repository system, will present issues implementation, a repository may contain diverse content in establishing and building varying types of repository characterized by heterogeneity of format and in the level services within a research library setting. of detail and format of descriptive metadata, be based The following issues will be discussed: upon wildly varying architectures, provide multiple levels of resource preservation, support a wide-ranging scope of discovery and delivery services in support of an institution's mission, and potentially provide the tools that allow effective use of its contents. The development of digital asset management tools, content workflows, and discovery interface with authentication and access controls is an expensive and time-consuming process that requires detailed planning and effective project management. Furthermore, as the distributed and local digital repository environment coalesces and its requirements become clearer, institutions will also undergo organizational change in significant ways to take on these management and delivery functions. The 0 0	access control;authentication;digital asset;digital library;organizational behavior;requirement	Leslie Johnston;Margret Branschofsky;Michael R. Leach	2004		10.1002/meet.1450410180	digital library;computer science;world wide web	HCI	-72.05494976094808	13.68783649674839	89309
6d885263fbea0d3bd4960641965843543b6eee96	analysis of knowledge-intensive processes focused on the communication perspective		Knowledge-intensive Processes (KiPs) are unstructured processes that demand an understanding beyond control flow and data. Being knowledge-centric and varying at each instance, KiPs demand new perspectives for proper process analysis. Most KiPs have strong collaboration characteristics, where interactions among participants are crucial to achieve process goals. Process participants perform activities and collaborate with each other, driven by their Beliefs, Desires and Intentions; therefore, the analysis of these elements is vital to the correct understanding, modeling and execution of a KiP. This research proposes a method based on Speech Act Theory and Process Mining to discover the flow of speech acts related to Beliefs, Desires and Intentions from event logs, and shows how this relation fosters process performance analysis. The approach was evaluated through a case study in a real life scenario, and results showed that relevant insights in forms of speech acts flow patterns were discovered and related to performance issues of the KiP.	cloud computing;control flow;count data;interaction;logical connective;real life;software development;software system	Pedro H. Piccoli Richetti;João Carlos de A. R. Gonçalves;Fernanda Araujo Baião;Flávia Maria Santoro	2017		10.1007/978-3-319-65000-5_16	management science;systems engineering;flow (psychology);process mining;control flow;computer science	SE	-66.61369151483387	14.557192935759078	89360
602aa4cec8a143a4c1fb96c368bbcb693909883b	model-driven analytics: connecting data, domain knowledge, and learning		Gaining profound insights from collected data of today’s application domains like IoT, cyber-physical systems, health care, or the financial sector is businesscritical and can create the next multi-billion dollar market. However, analyzing these data and turning it into valuable insights is a huge challenge. This is often not alone due to the large volume of data but due to an incredibly high domain complexity, which makes it necessary to combine various extrapolation and prediction methods to understand the collected data. Model-driven analytics is a refinement process of raw data driven by a model reflecting deep domain understanding, connecting data, domain knowledge, and learning.	cyber-physical system;extrapolation;model-driven engineering;model-driven integration;refinement (computing)	Thomas Hartmann;Assaad Moawad;François Fouquet;Grégory Nain;Jacques Klein;Yves Le Traon;Jean-Marc Jézéquel	2017	CoRR		analytics;computer science;knowledge management;data science;data mining;software analytics;semantic analytics	ML	-63.58643884807589	16.12139177911936	89372
11fc22b3bc09bec5cf8773ac7b0d504b2d140fa7	an evolutionary technique to approximate multiple optimal alignments		The alignment of observed and modeled behavior is an essential aid for organizations, since it opens the door for root-cause analysis and enhancement of processes. The state-of-the-art technique for computing alignments has exponential time and space complexity, hindering its applicability for medium and large instances. Moreover, the fact that there may be multiple optimal alignments is perceived as a negative situation, while in reality it may provide a more comprehensive picture of the model’s explanation of observed behavior, from which other techniques may benefit. This paper presents a novel evolutionary technique for approximating multiple optimal alignments. Remarkably, the memory footprint of the proposed technique is bounded, representing an unprecedented guarantee with respect to the state-of-the-art methods for the same task. The technique is implemented into a tool, and experiments on several benchmarks are provided.		Farbod Taymouri;Josep Carmona	2018		10.1007/978-3-319-98648-7_13	spacetime;systems engineering;memory footprint;mathematical optimization;bounded function;computer science;exponential function	EDA	-77.08987695504169	18.735822081766088	89468
8671065a5ec38679edb62f1674c45890b08fb1d4	evaluating an early software engineering course with projects and tools from open source software	qualitative method;maintenance;software maintenance;program comprehension;software engineering;survey data;student learning;open source software;reverse engineering;open source	We developed a software engineering course that emphasizes code maintenance and evolution by having students reverse engineer and modify open-source projects. To evaluate whether this course had the desired effects on student learning, we analyze pre- and post-course survey data using qualitative methods. This analysis, in combination with other data, suggests that the students gained an appreciation and understanding of software maintenance, documentation, and tool use.	documentation;open-source software;reverse engineering;software engineering;software maintenance	Robert McCartney;Swapna S. Gokhale;Thérèse Smith	2012		10.1145/2361276.2361279	personal software process;long-term support;verification and validation;software engineering process group;software project management;computer science;qualitative research;social software engineering;software development;software engineering;survey data collection;software construction;software walkthrough;software analytics;software maintenance;software deployment;software requirements;reverse engineering;software system;software peer review	SE	-65.86296596334662	26.54136180244327	89518
0300349e06134de27ff049e800296c1a74e4f625	the potential impact of the iec61499 standard on the progress of distributed intelligent automation	distributed system;iec61499;systeme reparti;distributed component based automation systems;process measurement;componente logicial;control inteligente;composant logiciel;recherche developpement;intelligent control;schema fonctionnel;esquema funcional;research and development;sistema repartido;investigacion desarrollo;function blocks;software component;process control;distributed component based automation;function block diagram;commande intelligente;industrial automation	This paper provides an overview of the newly emerging standard IEC61499. In particular, it discusses the motivation for the standard's development, basic technical features, implementation problems and business perspectives. Key research and development activities relevant to the standard's development and implementation are also discussed. Thus, the aim of this paper is to provide up-to-date information on the standard in a concise form for automation engineers, researchers and for the management of companies involved in industrial automation business.		Valeriy Vyatkin	2006	IJMTM	10.1504/IJMTM.2006.008801	computer science;systems engineering;engineering;operations management;component-based software engineering;automation;process control;function block diagram;isa100.11a;intelligent control	Robotics	-67.16211838677886	7.063091853622996	89550
0b384d090decd244e4385af7437824dfa5e0a5de	the effect of social network structures at the business/it interface on it application change effectiveness	information management system;information systems security;mis systems;information systems research;journal of it;jit;teaching cases;information security;case studies;information science;information security systems;information technology;business information technology;security information systems;it journals;information systems management;it teaching cases;operational research society;business model;journal of information technology teaching cases;computer information systems;jit journal;geographic information systems;information technology journal;information management;information systems journals;information systems technology;managing information systems;accounting information systems;information and management;management information systems;define information systems;strategic information systems;business information management;soft system methodology;information system;health information systems;computer information technology;journal of information technology;business information systems;business systems analyst;journal information technology;it journal;management science;journal of information systems;information technology journals	The challenge of managing the relationship between a firm’s business and IT in order to derive business value from IT is an important topic on researchers’ and practitioners’ agendas. The focus of most related research and management actions has been on the top management or project management levels. However, conflicts frequently arise within the line organization when applications are extended, enhanced, maintained, or otherwise changed operationally outside software development projects. This study focuses on the impact of relationships at the application-change level and strives to identify and explain favorable social structures for effective business/IT dialog at the operational level. We collected data in seven comprehensive case studies, including 88 interviews and corresponding surveys, and applied social network analysis to show that three social structures at the implementation level influence the degree to which IT applications are maintained and enhanced in line with business requirements: (1) interface actors connecting business and IT, (2) the relationships between interface actors and the corresponding unit, and (3) the relationships between interface actors and other employees in their unit. In three cases, less favorable structures are revealed that correspond to low application change effectiveness and software applications that do not meet business requirements. The other cases benefit from favorable social structures and thus enhance fulfillment of business requirements and result in higher IT business value. This paper contributes to IS research by helping to explain why companies may not provide favorable IT services despite favorable relationships at the top management level and successful application development projects.	social network	Katja Zolper;Daniel Beimborn;Tim Weitzel	2014	JIT	10.1057/jit.2014.6	business analysis;business requirements;computer science;systems engineering;engineering;knowledge management;artifact-centric business process model;business process management;electrical engineering;business case;management information systems;management science;business relationship management;business rule;new business development;information technology;information system	HCI	-78.17462054058802	7.859016529559558	89711
666e909496e2930e57e89d6a69839c1af9a5edda	model-based tests of truisms	formal specification;software process improvement;life cycle;software engineering life testing software testing costs software quality object oriented modeling debugging cities and towns life estimation runtime;software engineering;software quality software engineering truisms software construction software process level software lifecycle cocomo ii cost estimation model discrete event software process model software project life cycle tar2 treatment learner;software process model;systems analysis;model based testing;process improvement;cost estimation;software quality formal specification systems analysis software process improvement;software quality;discrete event;software process	Software engineering (SE) truisms capture broadlyapplicable principles of software construction. The trouble with truisms is that such general principles may not apply in specific cases. This paper tests the specificity of two SE truisms: (a) increasing software process level is a desirable goal; and (b) it is best to remove errors during the early parts of a software lifecycle. Our tests are based on two well-established SE models: (1) Boehm et.al.’s COCOMO II cost estimation model; and (2) Raffo’s discrete event software process model of a software project life cycle. After extensive simulations of these models, the TAR2 treatment learner was applied to find the model parameters that most improved the potential performance of the real-world systems being modelled. The case studies presented here showed that these truisms are clearly sub-optimal for certain projects since other factors proved to be far more critical. Hence, we advise against truism-based process improvement. This paper offers a general alternative framework for model-based assessment of methods to improve software quality: modelling + validation + simulation + sensitivity. That is, after recording what is known in a model, that model should be validated, explored using simulations, then summarized to find the key factors that most improve model behavior.	apply;cocomo;emoticon;process modeling;sensitivity and specificity;simulation;software construction;software development process;software engineering;software project management;software quality;world-system	Tim Menzies;David Raffo;Siri-on Setamanit;Ying Hu;Sina Tootoonia	2002		10.1109/ASE.2002.1115012	reliability engineering;biological life cycle;systems analysis;personal software process;long-term support;verification and validation;model-based testing;software engineering process group;software sizing;software verification;computer science;systems engineering;package development process;social software engineering;software reliability testing;software development;software engineering;software construction;formal specification;software walkthrough;software measurement;software deployment;goal-driven software development process;software development process;software quality;cost estimate;software metric;software quality analyst;software peer review	SE	-63.113569035651615	31.61467516277878	89801
a6960a78e16700c24510f7f0f188b0acb35276ad	novice code understanding strategies during a software maintenance assignment		Existing efforts on teaching software maintenance have focussed on constructing adequate codebases that students with limited knowledge could maintain, with little focus on the learning outcomes of such exercises and of the approaches that students employ while performing maintenance. An analysis of the code understanding strategies employed by novice students as they perform software maintenance exercises is fundamental for the effective teaching of software maintenance. In this paper, we analyze the strategies employed by second year students in a maintenance exercise over a large codebase. We analyze student reflections on their code understanding, maintenance process and the use of tools. We show that students are generally capable of working with large codebases. Our study also finds that the majority of students follow a systematic approach to code understanding, but that their approach can be significantly improved through the use of tools and a better understanding of reverse engineering approaches.	amiga reflections;reverse engineering;software maintenance	Claudia Szabo	2015	2015 IEEE/ACM 37th IEEE International Conference on Software Engineering		personal software process;long-term support;verification and validation;software engineering process group;strategy;systems engineering;engineering;knowledge management;package development process;social software engineering;software development;software engineering;software construction;software walkthrough;software analytics;software maintenance;software deployment;software requirements;collaboration;software peer review	SE	-66.02884755334468	26.72755944993119	89808
676977fa9efb19c6995cc6d025517ef574d7c11e	enhancing international knowledge transfer through information technology: the intervention of communication culture	mars;outsourcing;communication culture;information technology;suppliers;it;taiwan;knowledge transfer;multivariate adaptive regression splines	Nowadays, outsourcing and offshore-partnering activities have become popular business models for leading multinational enterprises. Usually, outsourcing not only helps leading multinational enterprises increase their competitive advantage, but also offers suppliers a great opportunity to learn from their international customers. Hence, it raises the issue of what factors are important for suppliers to successfully manage knowledge transfer in the international context. In the previous studies, little attention has been paid to how information technology (IT) influences cross-border knowledge transfer. In this present study, taken from the supplier’s perspective, we propose that IT can improve the quality and quantity of information exchange, and have a positive influence on cross-border knowledge transfer. Based on a sample site of 206 Taiwanese suppliers and tested by the multivariate adaptive regression splines (MARS) statistical approach, the empirical results reveal that a knowledge management integrated database has a positive impact on technical knowledge transfer. Additionally, when in a low-context communication culture, video conferencing has a positive impact on technical knowledge transfer and instant messaging also has a positive impact on marketing knowledge transfer. Importantly, inter-organisational IT does not have a significant impact on knowledge transfer when in a high-context communication culture.	information exchange;instant messaging;knowledge management;multivariate adaptive regression splines;outsourcing;smoothing spline	Wei-Li Wu;Yi-Chih Lee	2012	IJICT	10.1504/IJICT.2012.045744	mars exploration program;simulation;multivariate adaptive regression splines;computer science;knowledge management;machine learning;knowledge value chain;information technology;outsourcing	AI	-82.11122492950314	5.514221508337501	89817
8f7b124a8dbb6052af840567f11668cf9837dcd6	data recovery function testing for digital forensic tools	digital forensics;functional testing;data collection;validation and verification	Many digital forensic tools used by investigators were not originally designed for forensic applications. Even in the case of tools created with the forensic process in mind, there is the issue of assuring their reliability and dependability. Given the nature of investigations and the fact that the data collected and analyzed by the tools must be presented as evidence, it is important that digital forensic tools be validated and verified before they are deployed. This paper engages a systematic description of the digital forensic discipline that is obtained by mapping its fundamental functions. The function mapping is used to construct a detailed function-oriented validation and verification framework for digital forensic tools. This paper focuses on the data recovery function. The data recovery requirements are specified and a reference set is presented to test forensic tools that implement the data recovery function.	computer forensics;data recovery;dependability;encase;list of digital forensics tools;loss function;programming paradigm;requirement;software requirements specification;test case;verification and validation	Yinghua Guo;Jill Slay	2010		10.1007/978-3-642-15506-2_21	computer science;data mining;database;computer security	EDA	-63.53482311104916	30.12554516334052	89851
4a0518d6a5aeb7bcc7de267431bc02e87e8df78a	incentives between firms (and within)	incentives;relational contracts;economic theory;supply transactions;agency theory;incentive contracts	This paper reviews the significant progress in “agency theory” (i.e., the economic theory of incentives) during the 1990s, with an eye towards applications to supply transactions. I emphasize six recent models, in three pairs: (1) new foundations for the theory of incentive contracts, (2) new directions in incentive theory, and (3) new applications to supply transactions. By reviewing these six models, I hope to establish three things. First, the theory of incentive contracts needed but has received new foundations. Second, new directions in incentive theory teach us that incentive contracts are not the only source of incentives. Finally (and especially relevant to supply transactions), the integration decision is an instrument in the incentive problem.	flaw hypothesis methodology;new foundations;relevance;risk aversion	Robert Gibbons	2005	Management Science	10.1287/mnsc.1040.0229	principal–agent problem;economics;incentive;public economics;finance;microeconomics;commerce	Theory	-81.98670166683313	6.153418319033453	89879
af81d00218ada25e624d8102f60b0ff083682fc1	unlocking supply chain disruption risk within the thai beverage industry	structural equation modelling sem;beverage industry;disruption risk;thailand;logistic risk	Purpose – A growing need for global sourcing of business has subjected firms to higher levels of uncertainty and increased risk of supply disruption. Differences in industry and infrastructure make it more difficult for firms to manage supply disruption risks effectively. The purpose of this paper is to extend developing research in this area by addressing gaps within existing literature related to environmental turbulence and uncertainties. Design/methodology/approach – The authors test the model using data collected from 253 senior managers and directors in the Thai beverage industry using advanced statistical techniques to explore the relationship between representations of supply disruption risk and uncertainty. Findings – The results show that both magnitude and probability of risk impact on the disruption risk, but the probability of loss is a dominant determinant. The authors also find that demand uncertainty and quality uncertainty affect the risk perception of purchasing managers, and are related to the magnitude of disruption risk, rather than the frequency of occurrence. Interestingly, the results show that quality uncertainty negatively impacts on the severity of disruption risk. Research limitations/implications – The construct validity of demand uncertainty was under the required threshold, intimating the need for further construct development. Practical implications – The framework provides managers with direction on how to formulate and target their disruption risk management strategies. The work also allows practitioners to critical reflect on implicit risk management strategies they may already employ and their effectiveness. Originality/value – The paper identifies key antecedents of supply disruption risk and tests them within a novel industrial context of the beverage industry and a novel national context of Thailand.	denial-of-service attack;purchasing;risk management;turbulence	Ying Kei Tse;Rupert L. Matthews;Kim Hua Tan;Yuji Sato;Chaipong Pongpanich	2016	Industrial Management and Data Systems	10.1108/IMDS-03-2015-0108	marketing;operations management;management;commerce;financial risk management	HCI	-84.08038090139807	4.3480414332133295	90099
50ac393f31e940b7c017b6920f038dbb2bde5398	spi - a guarantee for success? - a reality story from industry	mejoramiento procedimiento;developpement logiciel;level 2;software process improvement;computer software maintenance;software development process;ingenieria logiciel;software engineering;maintenance logiciel;amelioration procede;desarrollo logicial;software development;genie logiciel;process improvement;fiabilite logiciel;fiabilidad logicial;software reliability;qualite logiciel;software quality	The Tokheim development centre in Bladel, The Netherlands, has a 10-year experience in applying Software Process Improvement methods (SPI). Several methods were used to improve and adapt the software development process and successes have been booked regularly during those years. In 1992 the Bladel site was ISO9001 certified and achieved CMM Level 2. Now several years later the software development centre is still ISO9001/TickIT certified, however recent BOOTSTRAP Assessments pointed out that several sub-processes of the software development process can be rated in the lowest capability ranges. It is our observation that the reason for this is that the SPI program has been heavily influenced by several factors not clearly identified in most methods. The presence and extent of these influencing factors is a major prerequisite for the successful implementation of a SPI method. This paper presents an overview of the SPI methods applied in the Tokheim organisation and how the progress of the SPI program has been influenced.		Erik Rodenbach;Frank van Latum;Rini van Solingen	2000		10.1007/978-3-540-45051-1_21	systems engineering;engineering;operations management;software engineering;software quality	Vision	-69.46371412432389	19.88874470790901	90132
