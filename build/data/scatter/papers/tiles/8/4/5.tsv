id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
363bf8fdb49807e1924aa26f8fd20b65bce7a7ed	the questions of systems implementation with large-scale integration	lsi;reduction;logic;cost reduction;memory part number costs;cost performance economics logic lsi memory part number costs reduction;large scale integration;system design;cost performance;economics	Large-scale integration (LSI) is four to five years old for many of us, yet we must continue to ask some very fundamental questions. Can we reach the substantial hardware cost reductions projected? How much will this impact system cost? Where is the market and how will it evolve? Will implementation problems force radical system designs? Does the key rest with automation, e.g., with simulation and testing, or rather will it depend upon our ability to design with repetitive structures? And not the least of our considerations, would a low-cost LSI provide the economic base for the solution to the many highly complex problems? This paper will attempt to focus attention upon some of the saJient issues involved in the fore-going questions.	reduced cost;simulation;very-large-scale integration	Merlin G. Smith;William A. Notz;Erwin Schischa	1969	IEEE Transactions on Computers	10.1109/T-C.1969.222750	parallel computing;simulation;reduction;electrical engineering;logic;algebra	Arch	10.14219997237339	56.81325514115122	14534
34d6c5731432faa741ada57330ed4661f5a61587	computational structures for application specific vlsi processors	digital signal processing;thesis or dissertation;kb thesis scanning project 2015		central processing unit;computation	C. H. Lau	1989			computer science;electrical engineering;computer engineering	EDA	5.80952886244607	49.80430853713605	14544
6b882026c17bc2856a1595f8995dc20871987843	dfa-based and simd nfa-based regular expression matching on cell be for fast network traffic filtering	simd;network security;deterministic finite automaton;data processing;cell broadband engine;regular expressions;network traffic;dfa;nfa;regular expression;nondeterministic finite automata;software implementation	Regular expression matching is the heart of many data processing routines, such as string search, network traffic filtering, etc. The traditional way of regexp matching is building and execution of a deterministic finite automaton (DFA), that provides O(1) processing time per 1 input symbol for any regular expression. But this technique almost always forces many modern SIMD-processors to perform regexp search in scalar mode, thus it doesn't use the most part of their computational power.  This paper represents traditional straightforward DFA along with another regexp implementation, based on nondeterministic finite automata (NFA) SIMD-simulation on Cell Broadband Engine processor. Software implementation of NFA-based SIMD algorithm achieves as much as 10 Gbit/s per one Cell BE processor with 512 NFA states, thus it is feasible for preliminary network traffic filtering of suspicious objects, while DFA-based scalar one gains up to 60 Gbit/s with 60-state automaton. One Cell BE processor can maintain NFA of 6000..7000 overall states simultaneously, so if one wants to use signatures with more that 512 states, it's possible with linear performance-to-signatures tradeoff.	alphabet (formal languages);antivirus software;automata theory;cell (microprocessor);central processing unit;computation;deterministic finite automaton;electronic signature;finite-state machine;gigabit;network packet;network traffic control;nondeterministic finite automaton;regular expression;simd;simulation;string searching algorithm	Feodor Kulishov	2009		10.1145/1626195.1626228	powerset construction;nondeterministic finite automaton with ε-moves;parallel computing;nondeterministic finite automaton;data processing;computer science;theoretical computer science;network security;dfa minimization;generalized nondeterministic finite automaton;regular expression;algorithm	HPC	6.55128206170437	44.16717069717014	14552
eb5eaed8516c344383d5dadc828671a43b2d7da9	cosy: an energy-efficient hardware architecture for deep convolutional neural networks based on systolic array	deep convolutional neural network (cnn);energy-efficiency;hardware architecture;systolic array;zero-skipping	Deep convolutional neural networks (CNNs) show extraordinary abilities in artificial intelligence applications, but their large scale of computation usually limits their uses on resource-constrained devices. For CNN’s acceleration, exploiting the data reuse of CNNs is an effective way to reduce bandwidth and energy consumption. Row-stationary (RS) dataflow of Eyeriss is one of the most energy-efficient state-of-the-art hardware architectures, but has redundant storage usage and data access, so the data reuse has not been fully exploited. It also requires complex control and is intrinsically unable to skip over zero-valued inputs in timing. In this paper, we present COSY (CNN on Systolic Array), an energy-efficient hardware architecture based on the systolic array for CNNs. COSY adopts the method of systolic array to achieve the storage sharing between processing elements (PEs) in RS dataflow at the RF level, which reduces low-level energy consumption and on-chip storage. Multiple COSY arrays sharing the same storage can execute multiple 2-D convolutions in parallel, further increasing the data reuse in the low-level storage and improving throughput. To compare the energy consumption of COSY and Eyeriss running actual CNN models, we build a process-based energy consumption evaluation system according to the hardware storage hierarchy. The result shows that COSY can achieve an over 15% reduction in energy consumption under the same constraints, improving the theoretical Energy-Delay Product (EDP) and Energy-Delay Squared Product (ED^2P) by 1.33 x on average. In addition, we prove that COSY has the intrinsic ability for zero-skipping, which can further increase the improvements to 2.25× and 3.83× respectively.	algorithm;applications of artificial intelligence;artificial neural network;computation;convolution;convolutional neural network;data access;dataflow;electronic data processing;high- and low-level;memory hierarchy;microprocessor;neural network software;radio frequency;stationary process;systolic array;throughput	Chen Xin;Qiang Chen;Miren Tian;Mohan Ji;Chenglong Zou;Xin'an Wang;Bo Wang	2017	2017 IEEE 23rd International Conference on Parallel and Distributed Systems (ICPADS)	10.1109/ICPADS.2017.00034	throughput;convolutional neural network;systolic array;real-time computing;dataflow;reuse;efficient energy use;hardware architecture;energy consumption;computer science	HPC	3.5975443392905095	43.28872986989454	14600
4ed9d9d97e3652dd82abfd4a6bf2884e8ad6103f	hardware assisted pre-emptive control flow checking for embedded processors to improve reliability	control flow error detection;embedded processor reliability;hardware software codesign;reliable processors control flow checking embedded processor reliability hardware software technique micro instruction routines preemptive fault detection;hardware software technique;software transformation;real embedded systems;hardware assisted preemptive control flow checking;fault detection technique;embedded system;embedded systems;control flow checking;micro instruction routines;cfe handling;fault detection;logic testing;control flow;integrated circuit testing;code size;cost effectiveness;cfe handling hardware assisted preemptive control flow checking embedded processor reliability real embedded systems fault detection technique software transformation hardware software technique control flow error detection;preemptive fault detection;error detection;integrated circuit reliability;hardware application software computer errors australia software performance runtime computer crashes monitoring embedded software embedded system;microprocessor chips embedded systems fault diagnosis hardware software codesign integrated circuit reliability integrated circuit testing logic testing;embedded processor;reliable processors;fault diagnosis;microprocessor chips	Reliability in embedded processors can be improved by control flow checking and such checking can be conducted using software or hardware. Proposed software-only approaches suffer from significant code size penalties, resulting in poor performance. Proposed hardware-assisted approaches are not scalable and therefore cannot be implemented in real embedded systems. This paper presents a scalable, cost effective and novel fault detection technique, to ensure proper control flow of a program. This technique includes architectural changes to the processor and software modifications. While architectural refinement incorporates additional instructions, the software transformation utilizes these instructions into the program flow. Applications from an embedded systems benchmark suite are used for testing and evaluation. The overheads are compared with the state of the art approach that performs the same error coverage using software-only techniques. Our method has greatly reduced overheads compared to the state of the art. Our approach increased code size by between 3.85-11.2% and reduced performance by just 0.24-1.47% for eight different industry standard applications. The additional hardware (gates) overhead in this approach was just 3.59%. In contrast, the state of the art software-only approach required 50-150% additional code, and reduced performance by 53.5-99.5% when error detection was inserted.	benchmark (computing);central processing unit;control flow;embedded system;error detection and correction;fault detection and isolation;graphic art software;overhead (computing);refinement (computing);scalability;technical standard	Roshan G. Ragel;Sri Parameswaran	2006	Proceedings of the 4th International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS '06)	10.1145/1176254.1176280	embedded system;parallel computing;real-time computing;computer science	EDA	7.220358137005046	58.988590271061916	14610
18a756b95d0f8951d569410bebd58ffd54d3ea8a	stdf memory fail datalog standard	standard fail data log format;object recognition;memory management;failure;integrated circuit yield;built in self test communication standards electronic design automation and methodology memory management failure analysis software standards software testing system testing very large scale integration graphics;yield learning;standard;data mining;chip;arrays;indexes;structural testing;datalog;built in self test;memory structural tests;integrated circuit testing;memory structural tests yield learning standard fail data log format scan tests;integrated circuit yield integrated circuit testing;stdf;standard memory datalog stdf failure;memory;scan tests;data models;yield management	Yield learning in modern technologies requires fail data logging from the scan and memory structural tests to gain insight into the failing location inside a chip.Currently there is no standard format to store the fail data in an efficient way. A group of more than 20companies from ATE, EDA, Semiconductor and Yield Management companies has been working to enhance the Standard Fail Data log Format (STDF) V4 to enable efficient fail data log for scan and memory fails. This paper describes the proposed memory fail datalog format.	data logger;datalog;failure;overhead (computing);requirement;semiconductor	Ajay Khoche;Jay Katz;Sauro Landini;Kochen Liao;Neetu Agrawal;Glenn Plowman;Song-lin Zuo;Liyang Lai;John Rowe;Thomas Zanon	2009	2009 27th IEEE VLSI Test Symposium	10.1109/VTS.2009.29	chip;database index;data modeling;yield management;parallel computing;real-time computing;computer science;cognitive neuroscience of visual object recognition;database;datalog;memory;memory management	DB	19.365327523120175	52.27677157797206	14613
103481e242041a1375710a6958880f92ccfb3aea	an efficient fpga-based parallel phase unwrapping hardware architecture	computer architecture discrete cosine transforms wrapping shift registers imaging laplace equations field programmable gate arrays;digital holographic microscopy fpga phase unwrapping network on chip	This paper presents a novel phase unwrapping hardware architecture for imaging applications. The architecture is a hardware implementation of a path-independent noniterative discrete cosine transform (DCT) based minimum mean square algorithm for accurate and fast phase unwrapping. The implementation is based on field programmable gate array. The architecture is able to exploit the parallelism among different stages of the algorithm for maximizing the throughput of the computation. A network-on-chip platform is built for the computation time measurement. As compared with other implementations for fast phase unwrapping, the proposed architecture has the advantages of high throughput, high accuracy, and low power consumption.	algorithm;computation;discrete cosine transform;field-programmable gate array;instantaneous phase;mean squared error;network on a chip;parallel computing;throughput;time complexity	Huan-Yuan Chen;Shu-Hao Hsu;Wen-Jyi Hwang;Chau-Jern Cheng	2017	IEEE Transactions on Computational Imaging	10.1109/TCI.2017.2663767	throughput;architecture;parallel computing;field-programmable gate array;hardware architecture;discrete cosine transform;computation;shift register;exploit;computer science	Arch	9.321866308809552	41.34736719139105	14635
2a42bffd5f022ecc43018b853a41eb40d59337f1	efficient implementation of the aes algorithm for security applications	encryption;ciphers;field programmable gate arrays;power demand;algorithm design and analysis;pipeline processing;throughput	Throughput, area and power optimized designs for the advanced encryption standard algorithm are proposed in this paper. The presented designs are suitable for the encrypt-only AES-128 algorithm. Both designs integrate pipelining and iterative architectures in one design. This is achieved through applying the concept of partial loop unrolling where iterations and multistage pipelining are used to optimize area, throughput and dynamic power consumption. The first design achieves a throughput of 34.08 Gbps, an operating frequency of 266.29 MHz with 521 slice registers. The second design achieves a throughput of 34.09 Gbps, 674 slice registers and a frequency of 266.33 MHz on a Xilinx 14.2 Virtex-5 XC5VLX50-3 FPGA device. Both designs consumes an approximate value of 455 mW of dynamic power using Vivado 2014.4 on Zynq-7000 XC7Z010clq225-3 FPGA device.	approximation algorithm;clock rate;data rate units;encryption;field-programmable gate array;iteration;loop unrolling;multistage amplifier;pipeline (computing);throughput	Shady Mohamed Soliman;Baher Magdy;Mohamed A. Abd El-Ghany	2016	2016 29th IEEE International System-on-Chip Conference (SOCC)	10.1109/SOCC.2016.7905466	embedded system;algorithm design;throughput;electronic engineering;parallel computing;real-time computing;computer science;encryption;field-programmable gate array	EDA	9.93641842464062	45.433842742778815	14674
98dec3cd378c9f49e7fb747a006038c684752ba9	implementation of the 65nm cell broadband engine	microprocessor chips cmos digital integrated circuits;cell broadband engine;cmos digital integrated circuits;size 65 nm cell broadband engine processor cell architecture cmos soi technology;microprocessor chips;engines cmos technology silicon on insulator technology voltage random access memory cmos process circuits computer architecture clocks frequency	The first generation cell broadband engine processor introduced the cell architecture that consists of nine processor cores fabricated in the 90 nm CMOS SOI technology. This paper describes the advances made by moving the cell broadband engine design from 90 nm CMOS SOI to 65 nm CMOS SOI.	cmos;cell (microprocessor);cell signaling;physical design (electronics);power supply;requirement;silicon on insulator;veritas cluster server	Mack W. Riley;Brian K. Flachs;Sang H. Dhong;Gilles Gervais;Steve Weitzel;Michael Chuansan Wang;David W. Boerstler;Mark Bolliger;John M. Keaty;Juergen Pille;R. Berry;Osamu Takahashi;Y. Nishino;T. Uchino	2007	2007 IEEE Custom Integrated Circuits Conference	10.1109/CICC.2007.4405831	embedded system;electronic engineering;computer hardware;engineering	EDA	14.447705836495306	58.21487238902032	14816
73dc89ef3e1a2a1744336b9bf6c46e5105c07277	intrinsic evolvable hardware platform for digital circuit design and repair using genetic algorithms	evolvable hardware;direct bitstream manipulation;partial crossover operators;intrinsic fitness evaluation;autonomous fault recovery	A hardware/software platform for intrinsic evolvable hardware is designed and evaluated for digital circuit design and repair on Xilinx Virtex II Pro Field Programmable Gate Arrays (FPGAs). Dynamic bitstream compilation for mutation and crossover operators is achieved by directly manipulating the bitstream using a layered framework. Experimental results on a case study have shown that benchmark circuit evolution from an unseeded initial population, as well as a complete recovery of a stuck-at fault is achievable using this platform. An average of 0.47 microseconds is required to perform the genetic mutation, 4.2 microseconds to perform the single point conventional crossover, 3.1 microseconds to perform Partial Match Crossover (PMX) as well as Order Crossover (OX), 2.8 microseconds to perform Cycle Crossover (CX), and 1.1 milliseconds for one input pattern intrinsic evaluation. These represent a performance advantage of three orders of magnitude over the JBITS software framework and more than seven orders of magnitude over the Xilinx design tool driven flow for realizing intrinsic genetic operators on a Virtex II Pro device.	benchmark (computing);bitstream;compiler;design tool;digital electronics;evolvable hardware;field-programmable gate array;genetic algorithm;genetic operator;integrated circuit design;natural language processing;nikon cx format;pmx (technology);software framework;stuck-at fault;virtex (fpga)	Rashad S. Oreifej;Ronald F. DeMara	2012	Appl. Soft Comput.	10.1016/j.asoc.2012.03.032	embedded system;real-time computing	EDA	10.730126501457223	49.07899917981992	14849
fd05137203bbad8425cf290a150a610f0f845214	dual-rail asynchronous logic multi-level implementation	multi level implementation;decomposition;boolean network;asynchronous logic;node	A synthesis flow oriented on producing the delay-insensitive dual-rail asynchronous logic is proposed. Within this flow, the existing synchronous logic synthesis tools are exploited to design technology independent single-rail synchronous Boolean network of complex (AND-OR) nodes. Next, the transformation into a dual-rail Boolean network is done. Each node is minimized under the formulated constraint to ensure hazard-free implementation. Then the technology dependent mapping procedure is applied. The MCNC and ISCAS benchmark sets are processed and the area overhead with respect to the synchronous implementation is evaluated. The implementations of the asynchronous logic obtained using the proposed (with AND-OR nodes) and the state-of-the-art (nodes are designed based on DIMS, direct logic, NCL) network structures are compared. A method, where nodes are designed as simple (NAND, NOR, etc.) gates is chosen for a detailed comparison. In our approach, the number of completion detection logic inputs is reduced significantly, since the number of nodes that should be supplied with the completion detection is less than in the case of the network structure that is based on simple gates. As a result, the improvement in sense of the total complexity and performance is obtained.	asynchronous circuit;benchmark (computing);boolean network;delay insensitive circuit;logic synthesis;nand gate;nested context language;overhead (computing);synchronous circuit	Igor Lemberski;Petr Fiser	2014	Integration	10.1016/j.vlsi.2013.02.002	boolean circuit;and-inverter graph;logic synthesis;real-time computing;boolean network;logic optimization;logic level;asynchronous circuit;computer science;theoretical computer science;pass transistor logic;sequential logic;node;decomposition;digital electronics;algorithm	EDA	16.81357370076899	50.070081927134645	14852
94433f27ae60530be3d7dbfde84f4931a92c2865	solving resource-constrained network problems by implicit enumeration - preemptive case				Linus Schrage	1972	Operations Research	10.1287/opre.20.3.668		Theory	7.044864709854218	32.50900980389066	14856
2ab59d4912700e3da05f9288844a03bef4957c03	mixed arithmetic architecture: a solution to the iteration bound for resource efficient fpga and cpld recursive digital filters	programmable logic devices;signal flow graph;multiplying circuits;look ahead;signal flow graphs;recursive filters;digital filter;iterative methods;programmable logic array;circuit feedback;arithmetic pipeline processing delay programmable logic arrays digital filters flow graphs feedback loop scattering logic design prototypes;feedback loop;field programmable gate arrays iterative methods programmable logic devices recursive filters signal flow graphs pipeline arithmetic delays circuit feedback multiplying circuits;11 bit mixed arithmetic architecture iteration bound resource efficient filters cpld recursive digital filters fpga recursive digital filters equivalence transforms recursive section signal flow graph maximum allowable pipeline delay feedback loop bit parallel arithmetic delay limits resource efficiency bit parallel multiplier prototypes biquadratic filter xilinx xc4010 sample processing rate 14 bit;field programmable gate arrays;equivalent transformation;pipeline arithmetic;delays	This paper describes a new approach for negating the iteration bound of recursive digital filters. The approach is based on first applying equivalence transforms to the recursive section signal flow graph to determine the maximum allowable pipeline delay for each feedback loop and then selecting bit-parallel arithmetic where pipelined digit-serial computation does not meet these delay limits. Scattered look-ahead pipelining is considered in combination with the proposed method. The resultant structures remain predominantly digit-serial in operation, making the approach ideally suited to designs for programmable logic arrays since high resource efficiency is achieved. Using new digit-serial and bit-parallel multiplier prototypes offering reduced pipeline delay, a 14-bit data path 1 1-bit coefficient biquadratic filter for the Xilinx XC4010 achieves 36 Msamplessec-l sample processing rate, up to 5 times higher than previously reported results.	1-bit architecture;coefficient;complex programmable logic device;computation;digital filter;feedback;field-programmable gate array;iteration;pipeline (computing);quartic function;recursion (computer science);resultant;signal-flow graph;turing completeness	J. Living;Bashir M. Al-Hashimi	1999		10.1109/ISCAS.1999.777930	embedded system;electronic engineering;real-time computing;digital filter;signal-flow graph;programmable logic array;computer science;theoretical computer science;programmable logic device;feedback loop;control theory;iterative method;field-programmable gate array	EDA	12.892906670269756	45.3795220388576	14867
d47107a64f1073c6587b3fef6cd5a75be7e6e63b	circuit area-latency optimization technique for high-precision elementary functions	look up table;optimization technique;arithmetic module design;soc design;engineering all;integrated circuit design;system on chip;system on chip circuit optimisation digital arithmetic integrated circuit design modules;ieee 754 double precision;elementary functions;digital arithmetic;arithmetic delay table lookup taylor series very large scale integration computer science hardware logic gates logic circuits wire;arithmetic module design elementary functions ieee 754 double precision;circuit optimisation;logic gate;hardware implementation;modules;circuit area latency optimization;soc design circuit area latency optimization elementary functions ieee 754 double precision arithmetic module design	Hardware implementation of IEEE-754 double-precision elementary functions, such as square root, reciprocal, etc, requires search for a tradeoff between the logic gate/wire area and the look-up table area. In this paper, we propose a new technique to reduce both the look-up table size and access latency of IEEE-754 double-precision elementary functions. Simulations show that the optimization reduces the total memory requirements for five elementary functions by 1/50 while almost the same computation delay. The technique is simple yet suitable for the SoC design of arithmetic modules	computation;double-precision floating-point format;elementary function;logic gate;lookup table;mathematical optimization;requirement	Koji Hashimoto;Vasily G. Moshnyaga;Kazuaki Murakami	2006	APCCAS 2006 - 2006 IEEE Asia Pacific Conference on Circuits and Systems	10.1109/APCCAS.2006.342464	system on a chip;computer architecture;electronic engineering;lookup table;logic gate;computer science;elementary function;theoretical computer science;modular programming;integrated circuit design	EDA	12.890193368409811	46.81309200234283	15008
935c70874abef5985ac0ab196f8ced95bf9931d3	an interactive testability analysis program - ittap	magnitude improvement;testability measure;test length;testability calculation;itt-lsi technology center;testability analysis program;run time;interactive testability analysis program;selective trace concept;design for testability;cad;automatic control;controllability;length measurement;logic;user interface	ITTAP is a testability analysis program developed at the ITT-LSI technology center. In this paper we describe a testability measure for the test length and discuss the use of the selective trace concept for testability calculations. It is shown that ITTAP provides an order of magnitude improvement in run time over existing programs like SCOAP.	run time (program lifecycle phase)	Deepak K. Goel;Robert M. McDermott	1982	19th Design Automation Conference	10.1145/800263.809262	control engineering;reliability engineering;electronic engineering;controllability;length measurement;computer science;automatic control;cad;user interface;logic	EDA	8.688092864094417	52.55319943962586	15020
a54f04f3256b7e8f7922f589f24b85e45c8ead38	environment for integration of distributed heterogeneous computing systems	distributed computing;heterogeneous computing;middleware	Connecting multiple and heterogeneous hardware devices to solve problems raises some challenges, especially in terms of interoperability and communications management. A distributed solution may offer many advantages, like easy use of dispersed resources in a network and potential increase in processing power and data transfer speed. However, integrating devices from different architectures might not be an easy task. This work deals with the synchronization of heterogeneous and distributed hardware devices. For this purpose, a loosely coupled computing platform named Virtual Bus is presented as main contribution of this work. In order to provide interoperability with legacy systems, the IEEE 1516 standard (denoted HLA - High Level Architecture) is used. As proof of concept, Virtual Bus was used to integrate three different computing architectures, a multi-core CPU, a GPU and a board with an Altera FPGA and an ARM processor, which execute a remote image processing application that requires a communication between the devices. All components are managed by Virtual Bus. This proposal simplify the coding efforts to integrate heterogeneous distributed devices and results demonstrated the successful data exchanging and synchronization among all devices, proving its feasibility.	central processing unit;field-programmable gate array;graphics processing unit;heterogeneous computing;high-level architecture;image processing;interoperability;legacy system;list of device bit rates;loose coupling;multi-core processor	Thiago Werlley B. Silva;Daniel C. Morais;Halamo G. R. Andrade;Antonio Marcus Nogueira de Lima;Elmar U. K. Melcher;Alisson V. Brito	2017	Journal of Internet Services and Applications	10.1186/s13174-017-0072-1	symmetric multiprocessor system;field-programmable gate array;computer science;real-time computing;distributed computing;high-level architecture;interoperability;legacy system;arm architecture;communications management;middleware	HPC	-1.106939538158608	44.68764071200697	15029
e959627c38c00b84db424b10acb65012f47dbe6f	development of a high performance tspc library for implementation of large digital building blocks	logical standard cells;cmos technology;software libraries circuit testing cmos technology inverters frequency latches clocks optimization methods vhf circuits circuit synthesis;software libraries;high performance tspc library;clocks;building block;true single phase clocked circuits;flip flops;1 ghz;inverters;semi automatic method;cmos logic;digital building blocks;cellular arrays;vhf circuits;software libraries cmos logic circuits cellular arrays logic cad clocks flip flops;cmos logic circuits;0 35 micron;split output latches;true single phase clock;circuit testing;latches;ad hoc design;frequency;logic cad;very high frequency;high performance;heuristic study;0 35 micron high performance tspc library digital building blocks heuristic study split output latches true single phase clocked circuits ad hoc design semi automatic method logical standard cells cmos logic 1 ghz;circuit synthesis;optimization methods	In this paper, we present a heuristic study of split-output latches built according to the true single-phase clocked circuits (TSPC) design style. Previous studies have shown that TSPC circuits can operate at very high frequencies. Our goal is to implement large digital building blocks operating at 1 GHz or more (in 0.35 pm CMOS technology). Our methodology started with the development of a first set of cells based on ad-hoc design. A fast semi automatic method was then developed to reduce the number of simulations. Based on the experience acquired with TSPC optimization, we proposed an automatic algorithm to optimize TSPC cells and a method to realize libraries of logical standard cells. This paper describes the different steps followed to elaborate the TSPC library. The method and the cell library are validated by characterizing small digital building blocks.	algorithm;cmos;cell (microprocessor);clock rate;heuristic;hoc (programming language);library (computing);mathematical optimization;semiconductor industry;simulation	B. Le Chapelain;A. Mechain;Yvon Savaria;Guy Bois	1999		10.1109/ISCAS.1999.777908	embedded system;electronic engineering;real-time computing;computer science;cmos;quantum mechanics	EDA	17.844450997868982	50.708324490774274	15030
2894e25eb7dda5285c13f0c4c6e4aeb8421e3f51	unified static scheduling on various models	concurrent computing;iterative algorithms;processor scheduling;optical computing;data flow graph;scheduling algorithm;model integration;polynomial time;processor scheduling algorithm design and analysis scheduling algorithm timing delay iterative algorithms optical computing parallel processing concurrent computing grid computing;grid computing;algorithm design and analysis;parallel processing;timing	Given a behavioral description of an algorithm represented by a data-flow graph, we show how to obtain a rote-optimal static schedule with the minimum unfolding factor under two timing models, integral grid model and fractional grid model, and two design styles for each model, pipelined design and non-piplined design. We present a simple and unified approach to deal with the four possible combinations. A unified polynomial-time scheduling algorithm is presented, which works on the original data-flow graphs without really unfolding. The values of the minimum rate-optimal unfolding factors for all the four combinations are also derived.	algorithm;data-flow analysis;dataflow;polynomial;scheduling (computing);time complexity;unfolding (dsp implementation)	Liang-Fang Chao;Edwin Hsing-Mean Sha	1993	1993 International Conference on Parallel Processing - ICPP'93	10.1109/ICPP.1993.183	fair-share scheduling;time complexity;parallel processing;algorithm design;parallel computing;flow shop scheduling;concurrent computing;dynamic priority scheduling;computer science;rate-monotonic scheduling;theoretical computer science;operating system;data-flow analysis;two-level scheduling;distributed computing;optical computing;programming language;round-robin scheduling;scheduling;grid computing	EDA	4.02832605729755	38.49459399895746	15072
9675a9f061de26e799bda35c6193ea63f7ee3365	ipdps 2014 selected papers on numerical and combinatorial algorithms		The IEEE International Parallel and Distributed Processing Symposium (IPDPS) is an international forum for engineers and scientists fromaround theworld to present their latest research findings in all aspects of parallel computation. Papers are placed into one of four tracks: algorithms, applications, architecture and software. In 2014, the IPDPS program committee accepted 32 papers out of 159 submissions in the algorithms track. Selected papers on the topics of numerical and combinatorial algorithms were invited to send extended versions of their conference papers to this special issue, and after a reviewing according to JPDC’s rigorous standards, six papers were accepted for publication in JPDC. The first three articles discuss numerical algorithms. The paper ‘‘Reconstructing householder vectors from tallskinny QR’’ by Grey Ballard, James Demmel, Laura Grigori, Mathias Jacquelin, Nicholas Knight and Hong Diep Nguyen shows how to perform a tall-skinny QR and then reconstruct the Householder vector representation with the same asymptotic communication efficiency and little extra computational cost. The high performance and numerical stability of this algorithm is shown both theoretically and empirically. This leads to efficient parallel QR algorithms. Experiments on supercomputers demonstrate the benefits of the communication cost improvements: in particular, there are substantial improvements over tuned library implementations for tall-and-skinny matrices. The paper ‘‘Mixing LU and QR factorization algorithms to design high-performance dense linear algebra solvers’’ by Mathieu Faverge, Julien Herrmann, Julien Langou, Bradley Lowery, Yves Robert and Jack Dongarra introduces hybrid LU–QR algorithms for solving dense linear systems of the form Ax = b. Throughout amatrix factorization, these algorithms dynamically alternate LU with local pivoting and QR elimination steps, based upon a robustness criterion. LU elimination steps can be very efficiently parallelized, and are twice as cheap in terms of floating-point operations as QR steps. However, LU steps are not necessarily stable, while QR steps are always stable. The hybrid algorithms execute a QR step when a robustness criterion detects some risk for instability, and they execute an LU step otherwise. A comprehensive set of experiments shows that hybrid LU–QR algorithms provide a continuous range of trade-offs between stability and performance. The paper ‘‘A framework for general sparse matrix–matrix multiplication onGPUs andheterogeneous processors’’ byWeifeng Liu and Brian Vinter proposes a framework for general sparse matrix–matrix multiplication (SpGEMM) on GPUs and emerging CPU–GPU heterogeneous processors. Memory pre-allocation for the resulting matrix is organized by a hybrid method that saves	algorithm;applications architecture;brian;central processing unit;computation;computational complexity theory;computer architecture;experiment;graphics processing unit;instability;international parallel and distributed processing symposium;lu decomposition;linear algebra;linear system;matrix multiplication;numerical analysis;numerical stability;parallel computing;qr decomposition;sparse matrix;supercomputer	Anne Benoit	2015	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2015.09.001	distributed computing;theoretical computer science;computer science	HPC	-1.0812173364202216	38.10846393117337	15133
8251b44cf9e1963b483afbec21726c34cbeb2e11	fast interconnect and gate timing analysis for performance optimization	lumped parameter circuit;microprocessor;evaluation performance;optimisation;interconnection;capacitancia;algorithm performance;performance evaluation;optimizacion;integrated circuit;timing performance analysis capacitance wire partitioning algorithms integrated circuit interconnections delay effects design optimization very large scale integration filtering algorithms;efficient algorithm;concepcion optimal;evaluacion prestacion;conception optimale;circuit vlsi;gate delay calculation;low complexity;circuito integrado;physical design;vlsi design;effective capacitance;circuit complexity;conception circuit integre;algorithme;interconexion;algorithm;complexite circuit;temps calcul;integrated circuit design;circuit parametre localise;vlsi circuit;vlsi circuit optimisation delay estimation integrated circuit design integrated circuit interconnections reduced order systems timing;resultado algoritmo;wire delay calculation;integrated circuit interconnections;interconnexion;performance algorithme;physical design optimization;vlsi;optimal design;circuito parametro localizado;timing analysis;sign off delay calculators asymptotic waveform elmore delay gate delay calculation interconnect delay calculation static timing analysis physical design optimization vlsi designs model order reduction wire delay calculation;static timing analysis;elmore delay;optimization;microprocesseur;capacitance;temps retard;delay time;circuito vlsi;tiempo computacion;computation time;circuit optimisation;model order reduction;tiempo retardo;vlsi designs;microprocesador;performance optimization;asymptotic waveform;reduced order systems;delay estimation;interconnect delay calculation;circuit integre;sign off delay calculators;static timing analysis asymptotic waveform effective capacitance elmore delay gate delay calculation interconnect delay calculation;capacite electrique;algoritmo;timing	Static timing analysis is a key step in the physical design optimization of VLSI designs. The lumped capacitance model for gate delay and the Elmore model for wire delay have been shown to be inadequate for wire-dominated designs. Using the effective capacitance model for the gate delay calculation and model-order reduction techniques for wire delay calculation is prohibitively expensive. In this paper, we present sufficiently accurate and highly efficient filtering algorithms for interconnect timing as well as gate timing analysis. The key idea is to partition the circuit into low and high complexity circuits, whereby low complexity circuits are handled with efficient algorithms such as total capacitance algorithm for gate delay and the Elmore metric for wire delay and high complexity circuits are handled with sign-off algorithms. Experimental results on microprocessor designs show accuracies that are quite comparable with sign-off delay calculators with more than of 65% reduction in the computation times	and gate;algorithm;computation;delay calculation;elmore delay;lumped element model;mathematical optimization;microprocessor;model order reduction;physical design (electronics);propagation delay;static timing analysis;very-large-scale integration	Soroush Abbaspour;Massoud Pedram;Amir H. Ajami;Chandramouli V. Kashyap	2006	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2006.887834	embedded system;electronic engineering;real-time computing;delay calculation;computer science;electrical engineering;elmore delay;very-large-scale integration;static timing analysis;algorithm	EDA	18.546945956604112	53.80010488012142	15146
590f62a61cbeb79915e3676229b1a8786d504db2	designing new ternary reversible subtractor circuits		Abstract The reducing of the width of quantum reversible circuits makes multiple-valued reversible logic a very promising research area. Ternary logic is one of the most popular types of multiple-valued reversible logic, along with the Subtractor, which is among the major components of the ALU of a classical computer and complex hardware. In this paper the authors will be presenting an improved design of a ternary reversible half subtractor circuit. The authors shall compare the improved design with the existing designs and shall highlight the improvements made after which the authors will propose a new ternary reversible full subtractor circuit. Ternary Shift gates and ternary Muthukrishnan–Stroud gates were used to build such newly designed complex circuits and it is believed that the proposed designs can be used in ternary quantum computers. The minimization of the number of constant inputs and garbage outputs, hardware complexity, quantum cost and delay time is an important issue in reversible logic design. In this study a significant improvement as compared to the existing designs has been achieved in as such that with the reduction in the number of ternary shift and Muthukrishnan-Stroud gates used the authors have produced ternary subtractor circuits.	subtractor	Asma Taheri Monfared;Majid Haghparast	2017	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2017.06.022	subtractor;theoretical computer science;logic synthesis;ternary operation;garbage;electronic circuit;computer science;quantum computer;quantum	EDA	16.08733407946668	45.34191764612134	15257
b9d3ed19dacfcf8bfcb60c44511d84afe05e8f54	cs-based secured big data processing on fpga	matching pursuit algorithms;kernel;compressed sensing;encryption;image reconstruction big data field programmable gate arrays compressed sensing matching pursuit algorithms kernel encryption;big data processing virtex 7 fpga omp algorithm orthogonal matching pursuit artix 7 fpga drm deterministic random matrix cs approach compressive sensing communication bandwidth processing time reduction real time systems veracity data set variety data set velocity data set volume data set;big data;image reconstruction;time frequency analysis big data compressed sensing field programmable gate arrays real time systems;encryption compressive sensing big data;field programmable gate arrays	The four V's in Big data sets, Volume, Velocity, Variety, and Veracity, provides challenges in many different aspects of real-time systems. Out of these areas securing big data sets, reduction in processing time and communication bandwidth are of utmost importance. In this paper we adopt Compressive Sensing (CS) based framework to address all three issues. We implement compressive Sensing using Deterministic Random Matrix (DRM) on Artix-7 FPGA, and CS reconstruction using Orthogonal Matching Pursuit (OMP) algorithm on Virtex-7 FPGA. The results show that our implementations for CS sampling and reconstruction are 183x and 2.7x respectively faster when compared to previously published work. We also perform case study of two different applications i.e. multi-channel Seizure Detection and Image processing to demonstrate the efficiency of our proposed CS-based framework. CS-based framework allows us to reduce communication transfers up to 75% while achieving satisfactory range of quality. The results show that our proposed framework is 290x faster and has 7.9x less resource utilization as compared to previously published AES based encryption.	algorithm;big data;compressed sensing;encryption;field-programmable gate array;image processing;matching pursuit;openmp;real-time computing;real-time locating system;sampling (signal processing);velocity;veracity	Amey M. Kulkarni;Ali Jafari;Colin Shea;Tinoosh Mohsenin	2016	2016 IEEE 24th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)	10.1109/FCCM.2016.59	iterative reconstruction;embedded system;parallel computing;kernel;real-time computing;big data;computer science;theoretical computer science;operating system;data mining;compressed sensing;encryption;field-programmable gate array	Arch	7.044643423676218	41.00934315564538	15264
b280b7a12c7939f2b9035298037de95f3f28ca06	sequential delay budgeting with interconnect prediction	circuito combinatorio;field programmable gate array;design automation;interconnection;integrated circuit;circuit retard;circuito secuencial;placement;sequential circuits;circuit vlsi;circuit sequentiel;circuito integrado;red puerta programable;reseau porte programmable;upper bound;interconexion;combinatory circuit;vlsi circuit;interconnect prediction;programacion lineal;very large scale integration vlsi;delay budgeting;circuit combinatoire;interconnexion;linear programming;programmation lineaire;horloge;temps retard;delay time;circuito vlsi;circuito retardo;borne superieure;tiempo retardo;delay circuit;clock;reloj;circuit integre;field programmable gate arrays fpgas;cota superior;sequential circuit	In this paper we describe sequential retiming with delay budgeting and interconnect lengths prediction. The quality of sequential retiming depends upon the estimated interconnect delays. We derive the interconnect delays from the average net-lengths predictions and the newly proposed mutual contraction measure [9]. We transform the predicted net lengths into net weights and use them in our sequential budgeting. The results show that sequential retiming with the new net weighting and budgeting to estimate the net delays can improve circuit speeds by 12.29% on the average, compared to timing-driven placement. The new net weighting method is better than the uniform weighting. Retiming with estimated net delays is better than retiming at the logic level only.	logic level;retiming	Chao-Yang Yeh;Malgorzata Marek-Sadowska	2003	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1145/639929.639935	embedded system;electronic engineering;real-time computing;computer science;linear programming;electrical engineering;sequential logic	EDA	18.12707067024385	53.700991500083056	15289
148f6034797d0fd70bca6eb670e30971ec488d0e	design guidelines for reconfigurable multiplier blocks	school of no longer in use;electronics and computer science;multiplying circuits;logic design;digital filter;integrated circuit design;integrated circuit design multiplying circuits digital arithmetic field programmable gate arrays logic design digital filters fir filters vlsi;design guideline;custom vlsi implementations reconfigurable multiplier blocks design guidelines time multiplexed digital filters adder subtractor partial products area utilization multiplier block algorithm complexity fpga implementations;digital filters;vlsi;digital arithmetic;fir filters;guidelines multiplexing table lookup finite impulse response filter adders costs very large scale integration digital filters algorithm design and analysis filter bank;field programmable gate arrays	The newly proposed reconfigurable multiplier blocks offer significant savings in area over the traditional multiplier blocks for time-multiplexed digital filters or any other system where only a subset of the coefficients that can be produced by the multiplier block is needed in a given time. The basic structure comprises a multiplexer connected to at least one input of an adderhbtractor that can generate several partial products, leading to better area utilization. The multiplier block algorithm complexity of a design increases logarithmically as the number of the multiplexers is increased. Design guidelines for the maximum utilization of the reconfigurable multiplier block structures are also presented.	algorithm;brute-force search;coefficient;digital filter;field-programmable gate array;multiplexer;multiplexing;very-large-scale integration	Süleyman Sirri Demirsoy;Andrew G. Dempster;Izzet Kale	2003		10.1109/ISCAS.2003.1205831	electronic engineering;digital filter;computer hardware;computer science;engineering;electrical engineering;theoretical computer science	EDA	13.079562555029144	44.92883335748205	15338
016f924b55962e3036ac054344121fb79860a8c1	early-stage definition of lpx: a low power issue-execute processor	circuit design;low power	We present the high-level microarchitecture of LPX: a low-power issue-execute processor prototype that is being designed by a joint industry-academia research team. LPX implements a very small subset of a RISC architecture, with a primary focus on a vector (SIMD) multimedia extension. The objective of this project is to validate some key new ideas in power-aware microarchitecture techniques, supported by recent advances in circuit design and clocking.	high- and low-level;lpx (form factor);low-power broadcasting;prototype	Pradip Bose;David M. Brooks;Alper Buyuktosunoglu;Peter W. Cook;K. Das;Philip G. Emma;Michael Gschwind;Hans M. Jacobson;Tejas Karkhanis;Prabhakar Kudva;Stanley Schuster;James E. Smith;Vijayalakshmi Srinivasan;Victor V. Zyuban;David H. Albonesi;Sandhya Dwarkadas	2002		10.1007/3-540-36612-1_1	computer architecture	EDA	3.8974927243692696	54.51248235155007	15352
f133c500ce7197b7b26a5e537f1dc2852b81fa48	high efficiency data access system architecture for deblocking filter supporting multiple video coding standards	cmos integrated circuits;filtering;standards;optical filters;multiple video standards high efficiency deblocking filter;system buses;video coding;deblocking filter;vlsi cmos integrated circuits filtering theory system buses video coding;filtering algorithms;multiple video standards;vlsi;static var compensators;encoding;high efficiency;article;standards filtering algorithms filtering optical filters static var compensators encoding video coding;filtering theory;qfhd high efficiency data access system architecture deblocking filter multiple video coding standards vlsi architecture in loop deblocking filter ilf h 264 bp mp hp svc mvc avs vc 1 macro block adaptive frame field mbaff coding efficiency pdm input prediction data order extended output frame buffer module system bus architecture mb based scan order cmos technology real time performance requirement maximum operating frequency real time operating frequency power consumption voltage scaling	This paper presents an efficient VLSI architecture of in-loop deblocking filter (ILF) with high efficiency data access system for supporting multiple video coding standards including H.264 BP/MP/HP, SVC, MVC, AVS, and VC-1. Advanced standards, such as H.264 MP/HP, SVC, and MVC, adopt Macro Block Adaptive Frame Field (MBAFF) to enhance coding efficiency which results in the performance bottleneck of deblocking filter due to complex data access requirement. This design challenge has not been discussed in previous works according to our best knowledge. Therefore, we develop a Prediction Data Management (PDM) to manage the input prediction data order of deblocking filter for different coding types (like frame/field) and multiple standards. We also design an extended output frame buffer module to solve the system bus architecture restriction (like 1K boundary and burst length) and achieve high efficiency data access by using MB-based scan order. By using these techniques, we can solve the data accessing design challenge and reduce 67% bus latency. After being implemented by using 90 nm CMOS technology, the proposed work can achieve real-time performance requirement of QFHD (3840×2160@30fps) when operated at 156MHz at the cost of 50.6K gates and 2.4K bytes local memory. The maximum operating frequency of the proposed design, i.e. 370MHz, is higher than the required real-time operating frequency so that voltage scaling may be adopted to reduce power consumption.	algorithm;algorithmic efficiency;baseline (configuration management);byte;cmos;clock rate;data access;data compression;deblocking filter;dynamic voltage scaling;framebuffer;h.264/mpeg-4 avc;hp 3000;image scaling;interlaced video;model–view–controller;real-time clock;reduction (complexity);scalable video coding;system bus;systems architecture;very-large-scale integration;video coding format	Cheng-An Chien;Guo-An Jian;Hsiu-Cheng Chang;Kuan-Hung Chen;Jiun-In Guo	2012	IEEE Transactions on Consumer Electronics	10.1109/TCE.2012.6227475	filter;embedded system;electronic engineering;real-time computing;computer science;electrical engineering;deblocking filter;optical filter;very-large-scale integration;cmos;encoding	EDA	12.620946435798814	40.23093179877182	15399
36bcaa6943c89315269edf666ec12ccaa8734aa9	efficient equivalence checking with partitions and hierarchical cut-points	verification;pins;logic design;tellurium;permission;impedance matching permission tellurium pins logic design;impedance matching;equivalence checking;combinational equivalence checking	Previous results show that both flat and hierarchical methodologies present obstacles to effectively completing combinational equivalence checking. A new approach that combines the benefits while effectively dealing with the pitfalls of both styles of equivalence checking is presented.	combinational logic;formal equivalence checking;turing completeness	Demos Anastasakis;Lisa McIlwain;Slawomir Pilarski	2004	Proceedings. 41st Design Automation Conference, 2004.	10.1145/996566.996714	model checking;embedded system;impedance matching;combinatorics;discrete mathematics;logic synthesis;verification;computer science;electrical engineering;formal equivalence checking;tellurium;mathematics;abstraction model checking;algorithm	EDA	19.091496267348777	47.701267425687675	15400
d9dcab6db949103f12ae5791e4ace063e5d440a4	parallel implementation of image coding usingwavelet transform: syndex software environment application	image resolution;wavelet transforms;convolutional codes;convolution	This work is a contribution to Adequation between Algorithm and Architecture. It presents an example of application made with SynDEx, a software environment to implement signal processing or automatic algorithms on multiprocessor network. This communication shows that a Conditioned Data Flow Graph used for modelling an algorithm, is enough to do an implementation on multi-processeur network.	algorithm;algorithmic trading;dataflow architecture;multiprocessing;signal processing	Christophe Cudel;Bertrand Vigouroux	1996	1996 8th European Signal Processing Conference (EUSIPCO 1996)		computer vision;parallel computing;computer science;theoretical computer science	HPC	8.693116894130481	39.546408513973894	15421
4185ede6b44a5a9c559ccc302261f305f1c13c9f	accelerating ltv based homomorphic encryption in reconfigurable hardware	tk7885 7895 computer engineering computer hardware;qa075 electronic computers computer science	After being introduced in 2009, the first fully homomorphic encryption (FHE) scheme has created significant excitement in academia and industry. Despite rapid advances in the last 6 years, FHE schemes are still not ready for deployment due to an efficiency bottleneck. Here we introduce a custom hardware accelerator optimized for a class of reconfigurable logic to bring LTV based somewhat homomorphic encryption (SWHE) schemes one step closer to deployment in real-life applications. The accelerator we present is connected via a fast PCIe interface to a CPU platform to provide homomorphic evaluation services to any application that needs to support blinded computations. Specifically we introduce a number theoretical transform based multiplier architecture capable of efficiently handling very large polynomials. When synthesized for the Xilinx Virtex 7 family the presented architecture can compute the product of large polynomials in under 6.25 msec making it the fastest multiplier design of its kind currently available in the literature and is more than 102 times faster than a software implementation. Using this multiplier we can compute a relinearization operation in 526 msec. When used as an accelerator, for instance, to evaluate the AES block cipher, we estimate a per block homomorphic evaluation performance of 442 msec yielding performance gains of 28.5 and 17 times over similar CPU and GPU implementations, respectively.	aes instruction set;block cipher;cathode ray tube;central processing unit;coefficient;computation;fastest;field-programmable gate array;graphics processing unit;hardware acceleration;homomorphic encryption;pci express;polynomial;real life;reconfigurable computing;software deployment;speedup;virtex (fpga)	Yarkin Doröz;Erdinç Öztürk;Erkay Savas;Berk Sunar	2015		10.1007/978-3-662-48324-4_10	parallel computing;real-time computing;computer science;theoretical computer science;operating system;computer security;algorithm	Arch	8.07633159650639	44.267667217074816	15423
037c405481fc32efb9f5def427d3403a13320abe	effect of bias temperature instability on soft error rate	aging;positive bias temperature instability pbti;negative bias temperature instability nbti;soft error rate ser;critical charge	Aging and soft errors have become the two most critical reliability issues for nano-scaled CMOS designs. With the decreasing of device sizes the aging effect cannot be ignored during soft error rate SER estimation. In this paper, firstly the aging effect due to bias temperature instability BTI is analyzed on circuits using 32-nm CMOS technology for soft errors. Secondly, we derive an accurate SER estimation model which can incorporate BTI impact, including the negative BTI impact on PMOS and the positive BTI impact on NMOS. This model computes the failures in time FIT rate of sequential circuits. Experiments are carried on ISCAS89 circuits, and two findings are discovered: 1 for ten years simulation operating time, the maximum SER difference can go up to 12.5i¾ź% caused by BTI impact; 2 the BTI-aware SER grows quickly during the early operating time, and grows slowly in the later years.	instability;soft error	Zhen Wang;Jianhui Jiang	2015		10.1007/978-3-319-27161-3_67	ageing;real-time computing	HCI	19.995870136732126	59.484011008152514	15438
fe088faa82e926f8f4494fcf902e4b0d149e92fe	low-complexity technique for secure storage and sharing of biomedical images	image storage;complexity theory;image communication;real time;telemedicine;image sharing secure storage image communication healthcare system telemedicine;swinburne;biomedical imaging;low complexity;secure storage;cryptographic techniques;medical computing;power engineering and energy;medical services;biomedical engineering;medical image;healthcare system;computational complexity;cryptography;image reconstruction;medical image processing;pixel;image storage secure storage biomedical imaging cryptography image reconstruction medical diagnostic imaging pixel biomedical engineering power engineering and energy computational complexity;biomedical image processing;image sharing;deciphering;telemedicine computational complexity cryptography health care medical computing medical image processing;image sharing biomedical images secure storage cryptographic techniques computational complexity deciphering image encrypted blocks image communication healthcare system telemedicine;image encrypted blocks;biomedical images;biomedical computing;medical diagnostic imaging;health care	In this paper, we present an apparently trivial but powerful technique for secure storage and sharing of medical images using simple bit-level scrambling. Unlike the conventional cryptographic techniques, it involves very low computational complexity. Unless some one knows the keys for deciphering, cannot extract the original image from the encrypted blocks of image by any kind of attack. The proposed technique is useful for secure storage and sharing of large volume of medical images for real-time healthcare applications	bit-level parallelism;computational complexity theory;cryptography;encryption;real-time locating system	Pramod Kumar Meher;Jagdish Chandra Patra;M. R. Meher	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1693705	medical imaging;computer vision;computer science;cryptography;theoretical computer science;computer security;pixel;health care	Embedded	7.080450598518323	34.98015646357874	15440
b7919c1b0daac0c390ba6f831d313f8939790057	implementation of space-efficient voltage-insensitive capacitances in integrated circuits	integrated circuit;thin dielectric layers;mosfets;voltage capacitance circuits switches mosfets dielectric devices fabrication space technology cmos technology capacitors;d a converter;integrated circuit design;space efficient capacitances;capacitors;power dissipation;voltage biasing;mos transistors;voltage insensitive capacitances;device geometric sizing;dielectric materials;capacitive elements voltage insensitive capacitances integrated circuits thin dielectric layers mos transistors device geometric sizing voltage biasing d a converter space efficient capacitances;mosfet;capacitive elements;integrated circuits;mosfet capacitors dielectric materials integrated circuit design	In this paper, an approach of implementing voltage-insensitive and space-efficient capacitances has been proposed. It tackles capacitive elements related to the thin dielectric layers available in fabrication technologies of integrated circuits. It makes the use of the voltage-independent ones, such as the overlap capacitances of MOS transistors, and the voltage-dependency of the capacitances attached to the same circuit nodes are minimized by device geometric sizing and voltage biasing. In the case of using the MOS transistor gate dielectric layer, the capacitance is scaled by the width of the gate area of the transistor employed. As the transistor feature size is of sub-micron, the unit capacitance can be very small and the capacitances can be scaled easily and precisely, which facilitates a significant down-scale of the circuit geometric dimension, operation delay and, at the same time, power dissipation. As an example of application of the proposed approach, a charge-mode parallel D/A converter is proposed in this paper	biasing;gate dielectric;integrated circuit;transistor	Chunyan Wang	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1692975	electronic engineering;hybrid-pi model;computer science;electrical engineering;integrated circuit	EDA	13.91093341275913	56.01893105090047	15442
53cc8bd830c45d1e51a30d5bd0e9c907cbde9883	hidden markov modeling and fuzzy controllers in fpgas	fuzzy controller;hardware software codesign;performance evaluation;computerised control;application software;hidden markov model;hidden markov models fuzzy control field programmable gate arrays application software hardware acceleration software algorithms software tools pattern recognition speech recognition;fuzzy control;software systems;fpga;acceleration;fixed point;hidden markov models;complex system;pattern recognition hidden markov modeling fuzzy controllers fpga algorithms performance evaluation hardware software codesign tools field programmable gate arrays speech recognition;fuzzy controllers;pattern recognition;software algorithms;speech recognition;algorithms;hardware software codesign tools;software tools;hidden markov modeling;field programmable gate arrays;speech recognition hidden markov models fuzzy control field programmable gate arrays performance evaluation pattern recognition computerised control;software implementation;hardware	This paper compares software and FPGA-based har ware implementations of two applications. The first application uses hidden Markov models, and the second application is a fuzzy contr oller. Both applications ar e accelerated when implemented in FPGA-based har w e, but this acceleration is obtained by using different algorithms then those used in software implementations. These different algorithms produce slightly different outputs; therefore both solution quality and performance must be evaluated to compare har dware and software implementations. The experience of designing these applications has implications for har dware/softwar e codesign tools and for the migration of existing software applications to FPGA-	algorithm;application domain;computation;diff utility;field-programmable gate array;formal specification;fuzzy control system;hidden markov model;markov chain;warez	Herman Schmit;Donald E. Thomas	1995		10.1109/FPGA.1995.477428	embedded system;real-time computing;computer science;theoretical computer science;machine learning;hardware architecture;hidden markov model;field-programmable gate array	SE	4.509795461861265	51.753190039499344	15462
3e0f06d4c52efd7d3a6f56ee2e5a8de87ec2eda5	low power methodology for an asic design flow based on high-level synthesis	common power format low power methodology asic design flow high level synthesis power management system on chip design modern nanometric technologies power optimization system level maximum power savings clock gating power gating leakage power reduction inverse discrete cosine transform design register transfer level;power gating;clock gating;system on chip discrete cosine transforms high level synthesis inverse transforms low power electronics;high level synthesis;low power;common power format clock gating power gating soc high level synthesis low power;clocks optimization switches logic gates hardware standards power demand;soc;common power format	Power management in system-on-chip (SoC) design has become very important in modern nanometric technologies. It is desirable to consider power optimization at the system-level for maximum power savings due to its higher level of abstraction. Clock gating and power gating are two well-known techniques for dynamic and leakage power reduction respectively. They can even be integrated to get maximum power reduction by using the same signal to control both. This work presents a methodology using both these techniques to save power of an inverse discrete cosine transform (IDCT) design when the register transfer level (RTL) is generated automatically by high-level synthesis (HLS). Power gating is implemented by capturing the power intent using common power format (CPF). This work mainly highlights the prospects of integrating CPF with automatically generated RTL using HLS flow. Saving in dynamic power by a factor of around 10× is obtained through clock gating while more than 50% saving in static power is obtained through power gating. Power gating also results in some area overhead.	application-specific integrated circuit;clock gating;coalition for patent fairness;design flow (eda);discrete cosine transform;high- and low-level;high-level synthesis;mathematical optimization;maximum power transfer theorem;overhead (computing);power gating;power management;power optimization (eda);register-transfer level;spectral leakage;system on a chip	Fahad Bin Muslim;Affaq Qamar;Luciano Lavagno	2015	2015 23rd International Conference on Software, Telecommunications and Computer Networks (SoftCOM)	10.1109/SOFTCOM.2015.7314103	electronic engineering;real-time computing;computer hardware;engineering;switched-mode power supply;clock gating;power optimization;low-power electronics	EDA	15.687671861464267	55.93676588346905	15464
24cebbab0c0523010a68cd598fd99e5abb1ebf9f	reduction calculator in an fpga based switching hub for high performance clusters	frequency 150 mhz reduction calculator fpga logic field programmable gate array switching hub scientific computation accelerator argot accelerated radiative transfer oct tree radiative transfer equation astronomical objects peach2 pci express adaptive communication hub ver2 gpu graphics processor unit adders dma direct memory access cpu;calculators field programmable gate arrays switches graphics processing units ports computers adders mathematical model;logic circuits adders field programmable gate arrays graphics processing units	Unused logic in the field-programmable gate array (FPGA) for the switching hub is one potential resource to accelerate the computation of data exchanged through the hub. However, for large scale scientific computation, it is difficult to implement such an accelerator on the FPGA used in high performance computers. Here, a reduction calculator for executing ARGOT (accelerated radiative transfer on grids using oct-tree) to solve the radiative transfer equation used for simulation of astronomical objects is implemented on the FPGA of PEACH2 (PCI Express Adaptive Communication Hub ver2), a low latency switching hub for high performance GPU (graphics processor unit) clusters. The implemented reduction calculator uses a pipelined tree of adders and works with a 150-MHz clock without affecting the switching hub functions. Use of the DMA (direct memory access) transfer with descriptors made it possible to improve the performance of CPU excution by a maximum of about 45 times in a real system.	central processing unit;computation;computational science;computer;direct memory access;field-programmability;field-programmable gate array;graphics processing unit;network switch;octal;octree;pci express;simulation;supercomputer;the hub (forum);usb hub	Takuya Kuhara;Chiharu Tsuruta;Toshihiro Hanawa;Hideharu Amano	2015	2015 25th International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2015.7293985	embedded system;parallel computing;computer hardware;computer science;operating system	HPC	-0.6157503467778042	46.26942854046022	15485
758071dc77971a52455862710fedafd3e7a0e18d	design of a digital ip for 3d-ic die-to-die clock synchronization		In this paper the design of a novel IP for 3D IC die-to-die clock synchronization is presented. The proposed design offers notable benefits over the conventional dual DLL based architectures for 3D IC clock synchronization. Simulation results of the IP are presented with GLOBALFOUNDRIES 14nm finFET library, and Through-Silicon Via (TSV) technology.	clock synchronization;die (integrated circuit);dynamic-link library;simulation;three-dimensional integrated circuit;through-silicon via	Mehdi Sadi;Sukeshwar Kannan;Luke England;Mark Mohammad Tehranipoor	2017	2017 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2017.8050431	clock synchronization;electronic engineering;three-dimensional integrated circuit;computer science;synchronization;self-clocking signal	Arch	13.561515489445725	56.83001047361498	15489
4b8bf22c32de15724125a7f71fa008a46c6fe22d	gpu-acceleration on a low-latency binary-coalescence gravitational wave search pipeline		Abstract Low-latency detections of gravitational waves (GWs) from compact stellar binary coalescences are crucial to enable prompt follow-up observations to astrophysical transients by conventional telescopes, as demonstrated by the first joint GW and electromagnetic observations on July 17, 2017. Searching over the GW parameter space with the requirement of low-latency presents a computational challenge. This will become more severe considering denser sampling of the source space due to improving GW detector sensitivities. In our previous work, a low-latency matched filtering search method was developed, called Summed Parallel Infinite Impulse Response (SPIIR) filtering, which is suitable for parallelization, and an over 50x speedup of this method was achieved using Fermi -generation GPUs. In this paper, a multi-rate scheme for filtering, which reduces the computation time by a factor of several, is presented. The recent features in NVIDIA GPUs, namely the read-only data cache, warp-shuffle, and cross-warp atomic techniques, were exploited to improve the performance of filtering over previous GPU acceleration by a factor of 1.5 ∼ 11x on a Maxwell -generation GPU, whereas on a Pascal -generation GPU, the hardware upgrade can bring along 7 ∼ 11x speedup, and a further 1.5 ∼ 2.5x speedup can be gained by employing these new techniques. This leads to an over 100x speedup for the multi-rate scheme using the Maxwell GPU, and an over 260x speedup using the Pascal GPU. This GPU-accelerated multi-rate scheme was incorporated into a low-latency search pipeline – the SPIIR pipeline – and an overall near-limit CPU usage reduction is expected. This IIR technique in general and its GPU acceleration technique here hold the potential to find applications in other signal processing fields, such as in image and radio astronomy data processing.	coalescing (computer science);graphics processing unit	Xiangyu Guo;Qi Chu;Shin Kee Chung;Zhihui Du;LinQing Wen;Yanqi Gu	2018	Computer Physics Communications	10.1016/j.cpc.2018.05.002	cpu time;acceleration;mathematical optimization;parallel computing;cache;signal processing;filter (signal processing);mathematics;speedup;gravitational wave;infinite impulse response	HCI	-4.505860974980695	38.46401342851927	15699
339d420e04a4afa919d60cc88a741056b8f58dce	multi-gate mosfet design	mosfet circuits;cmos technology;leakage current;design for manufacture;heating;performance improvement;electrodes;fets;threshold voltage;semiconductor device modeling;short channel effect;field effect transistor;mosfet circuits cmos technology fets circuit synthesis design for manufacture threshold voltage semiconductor device modeling heating digital circuits electrodes;digital circuits;circuit synthesis	In this paper, circuit design issues of emerging multi-gate field effect transistors (MuGFET) are discussed with special emphasis on the link between circuit design and technology. The influence of novel midgap gate electrode materials on digital circuits is presented and examples of the first basic analog building blocks realized with these advanced devices are shown. Furthermore the influence of new device specific effects on analog circuits, like self heating or output conductance improvement due to undoped body are discussed and RF and ESD issues are covered	analogue electronics;circuit design;conductance (graph);digital electronics;field effect (semiconductor);radio frequency;transistor	Gerhard Knoblinger	2006	2006 European Solid-State Device Research Conference	10.1109/ISQED.2007.106	short-channel effect;field-effect transistor;electronic engineering;semiconductor device modeling;engineering;electrical engineering;electrode;leakage;threshold voltage;design for manufacturability;cmos;digital electronics;computer engineering	EDA	13.13911009572837	56.421879033611795	15721
c52b28d12a1dafe34f4d345b5696dc551d4d1364	pursuit and evasion on a ring: an infinite hierarchy for parallel real--time systems	wdm;real time;complexity class;graph;parallel computer;cycle covering;survivability;real time computing;real time systems	We show that, for any positive integer n , there exists at least one timed ω -language Ln which is accepted by a 2n -processor real-time algorithm using arbitrarily slow processors, but cannot be accepted by a (2n-1) -processor real-time algorithm. It follows therefore that real-time algorithms form an infinite hierarchy with respect to the number of processors used. Furthermore, such a result holds for any model of parallel computation.	evasion (network security)	Stefan D. Bruda;Selim G. Akl	2001		10.1145/378580.378721	complexity class;mathematical optimization;combinatorics;discrete mathematics;parallel computing;computer science;theoretical computer science;operating system;mathematics;distributed computing;graph;algorithm;wavelength-division multiplexing	Logic	14.857155743446839	32.716855284907425	15736
99157cae51cb34b99acf02268793edf45e702b12	mozart: a concurrent multilevel simulator	value set;active fault machines;description systeme;circuit faults switches circuit simulation computational modeling fault detection logic testing delay discrete event simulation failure analysis logic design;concurrent multilevel simulator;system description;syntax;fault simulator;puerta logica;large circuits;circuit faults;fault simulation;integrated circuit;two pass simulation;logic design;serial algorithm;switch levels;register transfer;serial algorithm circuit analysis computing digital ic logic circuit register transfer level functional level gate level mozart concurrent multilevel simulator fault simulator large circuits switch levels syntax value set delay model two pass simulation list event scheduling active fault machines performance metric;circuito integrado;sistema n niveles;circuito logico;tecnologia mos complementario;simulator;gate level;performance metric;porte logique;failure analysis;circuit simulation;computational modeling;simulador;circuit logique;digital integrated circuits;systeme n niveaux;fault detection;logic testing;multilevel system;simulateur;mos transistor;descripcion sistema;integrated logic circuits;logic cad circuit analysis computing digital integrated circuits integrated logic circuits;technologie mos complementaire;switches;circuit analysis computing;logic cad;register transfer level;logic circuit;logic gate;list event scheduling;functional level;digital ic;active fault;transistor mos;mozart;circuit integre;complementary mos technology;delay model;discrete event simulation	MOZART is a concurrent fault simulator for large circuits described at the RT, functional, gate, and switch levels. The requirements of multilevel simulation have guided the definition of MOZART’S syntax, value set, delay model, and algorithms. Performance is improved by reducing unnecessary activity. Two such techniques are levelized two-pass simulation, which minimizes the number of events and evaluations, and list event scheduling, which allows optimized processing of simultaneous (fraternal) events for concurrent machines. Moreover, efficient handling of abnormally large or active faulty machines can improve fault simulator performance by several orders of magnitude. These and related issues are discussed in this paper: both analytical and experimental evidence is provided for the effectiveness of the solutions adopted in MOZART. A new performance metric is introduced for fault simulation. This is based on comparison with the serial algorithm and is more accurate than those used	boolean algebra;fault simulator;requirement;scheduling (computing);sequential algorithm;simulation	Silvano Gai;Pier Luca Montessoro;Fabio Somenzi	1988	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.7799	embedded system;electronic engineering;real-time computing;logic gate;computer science;electrical engineering;operating system;algorithm	EDA	20.21109960273179	49.73324163741059	15745
a184568a2b928671e4b93388454095e72b35980c	instruction set extensions for secure applications		The main goal of this paper is to expose the community to past achievements and future possible uses of Instruction Set Extension (ISE) in security applications. Processor customization has proven to be an effective way for achieving high performance with limited area and energy overhead for several applications, ranging from signal processing to graphical computation. Concerning cryptographic algorithms, a large body of work exists on speeding up block ciphers and asymmetric cryptography with specific ISEs. These algorithms often mix non-standard operations with regular ones, thus representing an ideal target for being accelerated with dedicated instructions. Tools supporting automatic generations of ISEs demonstrated to be useful for algorithm exploration, while secure instructions can increase the robustness against side channels attacks of software routines. In this paper, we discuss how processor customization and the relative tool chains can be used by designers to address security problems and we highlight possible research directions.	algorithm;american and british english spelling differences;block cipher;computation;graphical user interface;overhead (computing);public-key cryptography;signal processing	Francesco Regazzoni;Paolo Ienne	2016	2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;electronic engineering;parallel computing;real-time computing;computer science;theoretical computer science;operating system;processor register;programming language;algorithm	EDA	7.958101811824134	45.435344156131045	15751
c51512d10f066ac46d6a6e14fe4bf38f218cc5e8	high performance motion estimation architecture using efficient adder-compressors	diamond search;frames per second;sum of absolute difference;real time;motion estimation;internal structure;spectrum;adder compressor;fast algorithm;high performance	This paper presents a high performance architecture for motion estimation (ME) using efficient adder compressors. The architecture is based on the Quarter Sub-sampled Diamond Search algorithm (QSDS) with Dynamic Iteration Control (DIC) algorithm. The main characteristic of the proposed architecture is the large amount of processing units (PUs) that are used to calculate the SAD (Sum of Absolute Differences) metric. The internal structures of the PUs are composed by a large number of addition operations to calculate the SADs. In this paper, efficient 4-2 and 8-2 adder compressors are used in the PUs architecture to achieve a higher performance. These adder compressors enable the simultaneous addition of 4 and 8 operands respectively. The PUs, using adder compressors, were applied to the ME architecture. The implemented architectures were described in VHDL and syntethized with Leonardo Spectrum tool to the TSMC 0.18μm CMOS standard cell technology. Synthesis results indicate that the new QSDS-DIC architectures reach the best performance result and enable gains of 12% in terms of processing rate. The architectures can reach real time for HDTV (1920x1080 pixels) in the worst case processing 65 frames per second, and they can process 269 HDTV frames per second in the average case.	adder (electronics);best, worst and average case;cmos;central processing unit;digital differential analyzer;iteration;motion estimation;operand;pixel;schedule for affective disorders and schizophrenia;search algorithm;standard cell;sum of absolute differences;vhdl	André Souza E Silva;Eduardo A. C. da Costa;Sérgio J. M. de Almeida;Marcelo Schiavon Porto;Sergio Bampi	2009		10.1145/1601896.1601912	embedded system;spectrum;computer vision;electronic engineering;real-time computing;computer hardware;telecommunications;computer science;electrical engineering;operating system;motion estimation;serial binary adder;carry-save adder;frame rate;algorithm;adder	HPC	12.862795711055016	41.84786852687709	15806
4abd44f7a5e4f1489b2c6c62f7f8e917727be1dc	quantized decoder adaptively predicting both optimum clock frequency and optimum supply voltage for a dynamic voltage and frequency scaling controlled multimedia processor			dynamic frequency scaling	Nobuaki Kobayashi;Tadayoshi Enomoto	2018	IEICE Transactions		theoretical computer science;mathematics;electronic engineering;frequency scaling;voltage;clock rate;quantization (physics)	OS	12.064478648283565	48.20840825066339	15836
6c6f25cd4e58dcf53285534136050427cbc1ce84	a practical algorithm for retiming level-clocked circuits	level clocked;timing optimization;circuit optimisation;circuits latches clocks delay effects timing equations constraint theory;clock skew;retiming;cad level clocked circuit retiming clock skew clock retiming optimal skew solution clock skew optimization latches gates problem size minimum period retiming iscas89 circuits	A new approach for fast retiming of level-clocked circuits is presented here. The method relies on the relation between clock skew and retiming, and computes the optimal skew solution to translate it to a retiming. Since clock skew optimization operates on the latches (rather than the gates as in conventional retiming), it is much faster because of a smaller problem size; the translationto the retiming solution is computationally cheap. The minimum period retiming for each of the ISCAS89 circuits was obtained within minutes by this algorithm.	algorithm;analysis of algorithms;benchmark (computing);clock rate;clock skew;computation;critical path method;mathematical optimization;retiming;software propagation;turing completeness	Naresh Maheshwari;Sachin S. Sapatnekar	1996		10.1109/ICCD.1996.563591	electronic engineering;parallel computing;real-time computing;clock skew;computer science;retiming;digital clock manager	EDA	17.35196642583852	51.969371790311705	15848
5166d664b9dd61b6cc844c71aa843ff8ebb09898	critical area extraction of extra material soft faults	reliability;circuit faults;telecommunication network reliability;integrated circuit;integrated circuit layout;application software;routing;design for quality;mask layout;telephony;critical area extraction;routing network;circuit faults circuit testing integrated circuit reliability integrated circuit layout computer network reliability telecommunication network reliability routing application software design for quality telephony;circuit testing;integrated circuit reliability;defect sensitivity;extra material soft faults;reliability critical area extraction extra material soft faults integrated circuit layout eye tool mask layout defect sensitivity routing network;fault diagnosis;fault diagnosis integrated circuit layout integrated circuit reliability;eye tool;computer network reliability	A method of extracting the extra material critical area of soft faults from an integrated circuit layout is presented. This has been implemented in the EYE tool allowing eficient extraction of the critical area from arbitrary mask layout. Results comparing defect sensitivity of a routing network modified to reduce defect sensitivity are reported. The application to defect related reliability is explored.	circuit diagram;integrated circuit layout;routing;software bug	Gerard A. Allan;Anthony J. Walton	1995		10.1109/DFTVS.1995.476937	reliability engineering;routing;electronic engineering;application software;real-time computing;computer science;engineering;electrical engineering;integrated circuit;reliability;integrated circuit layout;telephony	EDA	23.529752796213188	53.66043325523657	15864
06dfd25710d2252859cd37118edeb0afad286603	memory-scalable gpu spatial hierarchy construction	memory bound;memory scalable gpu;nvidia geforce gtx 280;breadth first search construction order;paper;memory management;memory consumption;storage management;bounding volume hierarchy;computer graphic equipment;layout;kd trees algorithm;coprocessors;cuda;interactive application;shape;ray tracing;gpu spatial hierarchy construction;kd tree;nvidia;graphic processing unit;bounding volume hierarchy memory bound kd tree;computer science;memory allocation;memory allocation strategy;tree searching;breadth first search;graphics processing unit;graphics processing unit memory management algorithm design and analysis parallel processing ray tracing layout shape;bounding volume hierarchy construction;algorithm design;algorithm design and analysis;parallel processing;data transfer;memory;tree searching computer graphic equipment coprocessors storage management;memory scalable gpu gpu spatial hierarchy construction graphics processing unit breadth first search construction order memory consumption kd trees algorithm memory allocation strategy bounding volume hierarchy construction	Recent GPU algorithms for constructing spatial hierarchies have achieved promising performance for moderately complex models by using the breadth-first search (BFS) construction order. While being able to exploit the massive parallelism on the GPU, the BFS order also consumes excessive GPU memory, which becomes a serious issue for interactive applications involving very complex models with more than a few million triangles. In this paper, we propose to use the partial breadth-first search (PBFS) construction order to control memory consumption while maximizing performance. We apply the PBFS order to two hierarchy construction algorithms. The first algorithm is for kd-trees that automatically balances between the level of parallelism and intermediate memory usage. With PBFS, peak memory consumption during construction can be efficiently controlled without costly CPU-GPU data transfer. We also develop memory allocation strategies to effectively limit memory fragmentation. The resulting algorithm scales well with GPU memory and constructs kd-trees of models with millions of triangles at interactive rates on GPUs with 1 GB memory. Compared with existing algorithms, our algorithm is an order of magnitude more scalable for a given GPU memory bound. The second algorithm is for out-of-core bounding volume hierarchy (BVH) construction for very large scenes based on the PBFS construction order. At each iteration, all constructed nodes are dumped to the CPU memory, and the GPU memory is freed for the next iteration's use. In this way, the algorithm is able to build trees that are too large to be stored in the GPU memory. Experiments show that our algorithm can construct BVHs for scenes with up to 20 M triangles, several times larger than previous GPU algorithms.	allocation;bounding volume hierarchy;breadth-first search;cpu (central processing unit of computer system);central processing unit;core dump;fragmentation (computing);graphics processing unit;ip fragmentation;iteration;large;memory disorders;memory bound function;moderate response;out-of-core algorithm;parallel computing;scalability;trees (plant)	Qiming Hou;Xin Sun;Kun Zhou;Christian Lauterbach;Dinesh Manocha	2011	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2010.88	uniform memory access;parallel processing;algorithm design;parallel computing;out-of-core algorithm;computer science;theoretical computer science;flat memory model;memory management;computer graphics (images)	Visualization	-2.626112299530451	42.054941324814536	15893
a716466177d37dbb36f1712650ba8c230a32d090	design and integration of parallel hough-transform chips for high-speed line detection	process design image edge detection computer vision robustness circuits cmos technology embedded system application software cmos process clocks;cmos integrated circuits;cmos technology;application software;clocks;parallel processor parallel hough transform chip design high speed line detection computer vision hough transform processing array processor design parallel peak extraction circuit tsmc cmos technology soc design;system on chip cmos integrated circuits computer vision hough transforms integrated circuit design parallel processing;cmos process;line detection;parallel hough transform chip design;high speed line detection;embedded system;process design;computer vision;chip;hough transform processing;soc design;integrated circuit design;array processor design;image edge detection;system on chip;parallel peak extraction circuit;hough transforms;robustness;circuits;hough transform;tsmc cmos technology;high speed;parallel processing;parallel processor	Line detection is often needed in computer vision applications. The Hough transform processing of image data for line detection is robust but time-consuming. With the use of multiple processors, the processing time for Hough transform can be much reduced. In our research, we design an array processor for line-detection based on Hough transform that performs the line-parameter calculation and accumulation for different angles in parallel. Such an array processor together with its parallel peak extraction circuits have been implemented on a single chip. Based on the TSMC 0.35mum CMOS technology, the fabricated chip (with 10 processors) can be run successfully up to the clock rate of 50MHz. This paper presents the SOC design that can be extended to the integration of multiple chips to form a faster system with more parallel processors	array processing;cmos;central processing unit;clock rate;computer vision;edge detection;hough transform;robustness (computer science);tree accumulation;vector processor	Ming-Yang Chern;Yi-Hsiang Lu	2005	11th International Conference on Parallel and Distributed Systems (ICPADS'05)	10.1109/ICPADS.2005.126	embedded system;parallel processing;parallel computing;computer science;cmos	EDA	12.785906275651156	47.399003919449264	15921
e40da0ead8e97dd9e5b91926ebc12ecc7fdd7a56	a high performance systolic architecture for k-nn classification	random access memory;clocks;training;software implementations high performance systolic architecture k nn classification maximum performance category memocode design euclidean distance mahalanobis distance fpga based platform;runtime;software architecture field programmable gate arrays parallel processing pattern classification;vectors;vectors clocks runtime table lookup field programmable gate arrays random access memory training;field programmable gate arrays;table lookup	This paper describes the architecture of the winning entry to the 2014 Memocode Design Contest, in the maximum performance category. This year's Memocode design contest asks contestants to find the 10 nearest neighbors between 1,000 testing points and 10,000,000 training points. Instead of using Euclidean distance, the contest uses Mahalanobis distance. The contest has 2 awards: the maximum performance award and the cost adjusted performance award. Our implementation uses a brute force approach that calculates the distance between every testing point to every training point. We use the Convey HC-2ex, a FPGA-based platform. However, the theory applies to software implementations as well. At the time of publication, our runtime is 0.54 seconds.	brute-force search;euclidean distance;flops;field-programmable gate array;k-nearest neighbors algorithm	Kevin Townsend;Phillip H. Jones;Joseph Zambreno	2014	2014 Twelfth ACM/IEEE Conference on Formal Methods and Models for Codesign (MEMOCODE)	10.1109/MEMCOD.2014.6961862	embedded system;parallel computing;real-time computing;computer science;theoretical computer science;operating system;field-programmable gate array	HPC	8.197641331850138	46.288344286852045	15928
07b40b07bd70e8d0265fc2995e3356fb0c1a115f	a configurable system-on-chip architecture for embedded devices	modelizacion;traitement signal;digital signal processing;diseno circuito;adaptability;adaptabilite;haute performance;calculateur embarque;real time;circuit design;circuit vlsi;tecnologia mos complementario;system on a chip;adaptabilidad;circuit a la demande;processing time;chip;programming model;modelisation;temps calcul;computer architecture;custom circuit;vlsi circuit;circuito integrato personalizado;architecture ordinateur;sistema sobre pastilla;system on chip;senal numerica;signal processing;tratamiento digital;temps reel;boarded computer;signal numerique;alto rendimiento;tiempo real;temps traitement;digital processing;conception circuit;arquitectura ordenador;systeme sur puce;digital signal;circuito vlsi;tiempo computacion;computation time;technologie mos complementaire;procesamiento senal;modeling;high performance;tiempo proceso;calculador embarque;traitement numerique;complementary mos technology;embedded device	This paper describes a novel Configurable System-on-Chip (CSoC) architecture for stream-based computations and real-time signal processing. It offers high computational performance and a high degree of flexibility and adaptability by employing a micro Task Controller (mTC) unit in conjunction with programmable and configurable hardware. The hierarchically organized architecture provides a programming model, allows an efficient mapping of applications and is shown to be easy scalable to future VLSI technologies where over a hundred processing cells on a single chip will be feasible to deal with the inherent dynamics of future application domains and system requirements. Several mappings of commonly used digital signal processing algorithms and implementation results are given for a standard-cell ASIC design realization in 0.18 micron 6-layer UMC CMOS technology.	embedded system;system on a chip	Sebastian Wallner	2004		10.1007/978-3-540-30102-8_6	system on a chip;embedded system;real-time computing;computer science;signal processing	EDA	3.11938438768127	47.36933456052109	15977
62786dd7b45dacde79cd92d8606f4317def176c1	a 512-kb level-2 cache design in 45-nm for low power ia processor silverthorne	level 2;cache storage;memory size 512 kbyte;microprocessor chips cache storage cmos digital integrated circuits logic design;sense amplifier evaluation;sram array;design support;logic design;clocks;fuses;tristate write driver;sense amplifier precharge;size 45 nm;power gating;latch pulse width;arrays;voltage 0 75 v;self timed cache;low power;leakage power;logic gates;voltage 1 v;cmos digital integrated circuits;low power ia processor silverthorne;technical activities guide tag;high volume manufacturing;driver circuits;technical activities guide tag clocks voltage cmos process pulse amplifiers latches sleep random access memory error correction circuits;bit line floating;level 2 cache design;latches;voltage 0 75 v level 2 cache design low power ia processor silverthorne self timed cache sense amplifier evaluation sense amplifier precharge latch pulse width high volume manufacturing sram array bit line floating tristate write driver cache leakage power reduction memory size 512 kbyte size 45 nm voltage 1 v;microprocessor chips;cache leakage power reduction	A self-timed, phase based 512KB L2 cache design in 45 nm CMOS process for a low power IA core is presented. The design supports back to back access every core clock by performing sense amplifier (SA) evaluation and SA precharge (SAPCH) in one phase. Dynamic latch is used in the data-out path to enable the use of narrowest SA and latch pulse (LATCK) width during high volume manufacturing (HVM). Power gated (PG) WL driver, sleep and deep sleep for the SRAM array along with floating the bit lines and tri-state write driver schemes are implemented to reduce cache leakage power. The design meets the performance requirement of 2.0 GHz and 1.0 GHz at 1.0 V and 0.75 V respectively at 90C.	cmos;cpu cache;hardware-assisted virtualization;sense amplifier;spectral leakage;static random-access memory;three-state logic;throughput;triangular function	Mohammed H. Taufique;Alex Okpisz;Haseeb N. Ahmed;John R. Riley;Mohammad M. Hasan;Gianfranco Gerosa	2008	2008 IEEE Custom Integrated Circuits Conference	10.1109/CICC.2008.4672105	fuse;electronic engineering;parallel computing;logic synthesis;logic gate;computer hardware;computer science;engineering	EDA	16.653885317284256	58.75146205313772	16012
7de3f13ec5fc4e3b66d6df9af6dff9be2921aab4	on compacting test sets by addition and removal of test vectors	test vector removal;circuit testing circuit faults compaction fault detection computational modeling physics computing cities and towns very large scale integration flip flops combinational circuits;test compaction;circuit faults;test compaction test set size reduction test vector addition test vector removal stuck at faults combinational circuits;combinatorial circuits;very large scale integration;flip flops;physics computing;computational modeling;compaction;stuck at faults;fault detection;logic testing;combinatorial circuits logic testing integrated logic circuits integrated circuit testing;integrated circuit testing;test set size reduction;cities and towns;cost effectiveness;test vector addition;circuit testing;integrated logic circuits;combinational circuit;combinational circuits	This paper presents a method of test contyaction for stuck-at faults in conlbinationnl circuits, thnt conlplenients previously proposed ntethoh and allowsfurther reduction in test set size in a cost-eflective q v . Agiven test set & con yacted hygencrating additionnl test vectors. Each test vector added allows the removnl of two or more test vectors front the existing test set, thus reducing its size. Experimental results for benchmark circuits demonstrate the effectiveness of the method.	benchmark (computing);naruto shippuden: clash of ninja revolution 3;test set;test vector;test-and-set	Seiji Kajihara;Irith Pomeranz;Kozo Kinoshita;Sudhakar M. Reddy	1994		10.1109/VTEST.1994.292312	electronic engineering;parallel computing;computer science;automatic test pattern generation;test compression;mathematics;combinational logic;algorithm	EDA	20.389574518140627	50.04170380413135	16024
c8c6bb53b93b45c1ab45a7dc24461358138c62ca	message-passing two steps least square algorithms for simultaneous equations models	message passing algorithms;two stage least squares;least square;message passing;simultaneous equation model;high performance	The solution of Simultaneous Equations Models in high performance systems is analyzed. Message-passing algorithms for the Twostage Least Squares method are developed. Algorithms are studied theoretically and experimentally. The algorithms make extensive use of basic libraries like BLAS, LAPACK, and ScaLAPACK to obtain efficient and portable versions.		Jose-Juan López-Espín;Domingo Giménez	2007		10.1007/978-3-540-68111-3_14	parallel computing;message passing;instrumental variable;computer science;theoretical computer science;distributed computing;programming language;least squares	Theory	-1.7851737432195434	36.57711250999059	16097
f11288f5f9a253a15929cb3af02fa193e09713dc	implementation of a cellular automaton with globally switchable rules		Cellular automata represent a discrete model of a computational machine with the inherent concept of totally distributed state transitional function. Previous studies have indicated that well-devised type of a global influence turns out to be an important factor in terms of improving the overall efficiency of a computation process within automata. In this context, polymorphic electronics is an approach that introduces a specific way of a global control to the circuit, not by means of using a dedicated global signal but through employing an inherent environmental variable. In our case the global information is uniformly propagated through the existing voltage supply rail, which is naturally available to all individual cells of a given automaton. It seems that the suggested approach may be very useful for the implementation of enhanced cellular automata. In this paper, the real hardware implementation of a cellular automaton using polymorphic chip and the obtained experimental results are presented together with a subsequent discussion.	cellular automaton	Vaclav Simek;Richard Ruzicka;Adam Crha;Radek Tesar	2014		10.1007/978-3-319-11520-7_39	block cellular automaton;discrete mathematics;theoretical computer science;distributed computing;mobile automaton	Logic	18.30243292289462	42.200741111683094	16105
e48e39e30671225855b10f57171724b77a3737f5	yield and cost modeling for 3d chip stack technologies	integrated circuit yield;integrated circuit yield integrated circuit design integrated circuit interconnections;stacking wire packaging bonding system on a chip routing cost function integrated circuit interconnections cmos process production systems;system performance;chip;integrated circuit design;integrated circuit interconnections;cost model;3d chip stack technologies yield modeling cost modeling	It has been shown that stacking a set of known good dice into a 3D chip array may be beneficial in terms of system performance and footprint area. This paper demonstrates that, in the general sense, it is also beneficial to arrange chips into a 3D stack from yield and cost perspectives. It is shown that an optimal point occurs where cost is minimized by stacking an appropriate amount of dice into a single system	computer-aided design;disk partitioning;focus stacking;plausibility structure	Patrick P. Mercier;S. R. Singh;K. Iniewski;B. Moore;P. O'Shea	2006	IEEE Custom Integrated Circuits Conference 2006	10.1109/CICC.2006.320948	chip;system on a chip;physical design;embedded system;electronic engineering;telecommunications;integrated circuit packaging;computer science;engineering;electrical engineering;circuit design;computer performance;circuit extraction;integrated circuit design	EDA	13.49882636312245	56.075413269613904	16139
cc0ce1c95cf86b3a1b71ea511cadd200de7caf0d	evaluating root parallelization in go	multiprocessing programs;tree parallelization;computer go;games decision trees program processors synchronization parallel algorithms;computer go program;monte carlo tree search;monte carlo tree search mcts;tree parallelization computer go majority voting monte carlo tree search mcts root parallelization;synchronization;root parallelization method;games;tree parallelization monte carlo tree search computer go program root parallelization method fuego cpu central processing unit;multiprocessing systems;fuego;root parallelization;tree searching;majority voting;cpu;computer games;decision trees;program processors;tree searching computer games monte carlo methods multiprocessing programs multiprocessing systems parallel processing;parallel processing;monte carlo methods;central processing unit;parallel algorithms	Parallelizing Monte Carlo tree search (MCTS) has been considered to be a way to improve the strength of Computer Go programs. In this paper, we analyze the performance of two root parallelization methods: the standard strategy based on average selection and our new strategy based on majority voting. As a starting code base, we used Fuego, which is one of the best programs available. Our experimental results with 64 central processing unit (CPU) cores show that majority voting outperforms average selection. Additionally, we show through an extensive analysis that root parallelization has limitations.	automatic parallelization;central processing unit;computer go;experiment;monte carlo method;monte carlo tree search;non-blocking algorithm;overhead (computing);parallel computing;performance;root certificate;root-finding algorithm;scalability;section 508 amendment to the rehabilitation act of 1973;selection algorithm	Yusuke Soejima;Akihiro Kishimoto;Osamu Watanabe	2010	IEEE Transactions on Computational Intelligence and AI in Games	10.1109/TCIAIG.2010.2096427	parallel processing;computer architecture;parallel computing;computer science;theoretical computer science;central processing unit	DB	-0.09802604605328621	41.25701574748799	16163
ae1e5b02979d7a1dff38d50c40e5c50407802ef1	parametric failures in cmos ics - a defect-based analysis	power supplies;cmos integrated circuits;failure analysis circuit testing crosstalk integrated circuit testing cmos integrated circuits temperature system testing switching circuits power supplies frequency;crosstalk;switching circuits;multiparameter test strategies;failure analysis;electronic properties;parametric failures;nanoelectronics;integrated circuit testing;system testing;defect based analysis;circuit testing;complex test problem;integrated circuit reliability;temperature;multiparameter test strategies parametric failures cmos ics defect based analysis nanoelectronics electronic properties complex test problem;integrated circuit reliability cmos integrated circuits failure analysis integrated circuit testing nanoelectronics;frequency;cmos ics	Defect-based test studies have thoroughly characterized CMOS IC hard bridge and open defects while less is known about a third class called parametric failures. These are more difficult to detect, and their presence is growing in CMOS IC nanoelectronics. The objective of this work is to present data that encompass the electronic properties of parametric failures that affect our ability to test present and future CMOS ICs. While parametric failures are widely reported, we seek to classify these failures with supporting data. Solutions to this complex test problem require that we structure and formalize their behaviors. Data indicate that multiparameter test strategies have the best match to some of the failures while good test strategies do not exist for others.	cmos;software bug	Jaume Segura;Ali Keshavarzi;Jerry M. Soden;Charles F. Hawkins	2002		10.1109/TEST.2002.1041749	nanoelectronics;failure analysis;electronic engineering;crosstalk;temperature;computer science;engineering;electrical engineering;frequency;cmos;system testing;computer engineering	EDA	22.53879980977503	54.624931689393094	16202
00759ecf57e26327840f776e8e3e4903b38a37c7	thermal via allocation for 3-d ics considering temporally and spatially variant thermal power	sqp optimization;quadratic programming;thermal management packaging;thermal analysis;3d integrated circuits;quadratic program;parameterized model reduction;thermal via density;circuit design;order reduction;journal article;optimization problem;thermal management packaging circuit optimisation integrated circuit design integrated circuit modelling quadratic programming thermal analysis;integrated circuit design;temperature profile;model reduction;nonlinear optimization problem;integrated circuit modelling;steady state thermal analysis;drntu engineering electrical and electronic engineering integrated circuits;temperature sensitive;structured model reduction;thermal power;algorithms;design;structured and parameterized macromodel;quadratic programming thermal via allocation 3d integrated circuits thermal power steady state thermal analysis transient temperature structured model reduction parameterized model reduction temperature sensitivity thermal via density thermal violation integral nonlinear optimization problem lagrangian relaxation;thermal violation integral;thermal management and simulation;circuit optimisation;nonlinear optimization;model order reduction;simulation model;thermal management;algorithm design;thermal via allocation;sqp optimization algorithms design thermal management and simulation model order reduction;temperature sensitivity;temperature sensors steady state thermal resistance dielectric substrates three dimensional integrated circuits thermal conductivity thermal engineering reduced order systems lagrangian functions quadratic programming;lagrangian relaxation;transient temperature;steady state	All existing methods for thermal-via allocation are based on a steady-state thermal analysis and may lead to excessive number of thermal vias. This paper develops an accurate and efficient thermal-via allocation considering temporally and spatially variant thermal-power. The transient temperature is calculated using macromodel by a structured and parameterized model reduction, which generates temperature sensitivity with respect to thermal-via density. By defining a thermal-violation integral based on the transient temperature, a nonlinear optimization problem is formulated to allocate thermal-vias and minimize thermal violation integral. This optimization problem is transformed into a sequence of subproblems by Lagrangian relaxation, and each subproblem is solved by quadratic programming using sensitives from the macromodel. Experiments show that compared to the existing method using steady-state thermal analysis, our method is 126X faster to obtain the temperature profile, and reduces the number of thermal vias by 2.04X under the same temperature bound.	lagrangian relaxation;linear programming relaxation;macromodel;mathematical optimization;nonlinear programming;nonlinear system;optimization problem;quadratic programming;steady state;thermal grease;via (electronics)	Hao Yu;Yiyu Shi;Lei He;Tanay Karnik	2006	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1145/1165573.1165611	control engineering;optimization problem;algorithm design;design;mathematical optimization;electronic engineering;lagrangian relaxation;computer science;circuit design;steady state;quadratic programming;thermal analysis;integrated circuit design	EDA	23.95378549786653	59.26687784643306	16205
0edd95ea2e9ade1e3ad32d52d663751d2b17164e	the von neumann architecture is due for retirement	von neumann model;promising candidate;easy-touse parallel model;von neumann architecture;parallel hardware;unusual model;parallel dataflow model;selfmodifying dataflow graph;processor industry;global memory;local graph transformation	The processor industry has reached the point where sequential improvements have plateaued and we are being flooded with parallel hardware we don’t know how to utilise. An efficient, general-purpose and easy-touse parallel model is urgently needed to replace the von Neumann model. We introduce and discuss the selfmodifying dataflow graph, an unusual model of computation which combines the naturally parallel dataflow model with local graph transformations to eliminate the need for a global memory. We justify why it is a promising candidate.	computer;data structure;data-flow analysis;dataflow;dynamic data;dynamization;general-purpose modeling;graph rewriting;manycore processor;mathematical optimization;model of computation;parallel computing;programmer;programming language;single-core;the australian;von neumann architecture	Aleksander Budzynowski;Gernot Heiser	2013			dataflow architecture;computer science;theoretical computer science;algorithm	Arch	-3.6012734250923586	47.181588249689554	16233
7b8c2a5a04d1fc189914cbfbe09d1cd3087a2c5e	autonomous design in vlsi: growing and learning on silicon	microprocessors;vlsi integrated circuit design;iron;junctions;system on a chip;autonomous design;computer architecture;integrated circuit design;dynamic environment;artificial neural networks;static environment;vlsi;junctions computer architecture microprocessors field programmable gate arrays hardware system on a chip artificial neural networks;dynamic environment autonomous design vlsi static environment;field programmable gate arrays;hardware	To-day no good solutions are known for the problem of a performing machine immersed within static or dynamic environments and meeting unexpected, unpredictable, unknown, new situations. Perhaps because no method can be found for the direct design of solutions that would meet this requirement, furthermore now in great demand. This paper presents the work we are developing to address that particular problem, of which abundant solutions ironically appear second nature to myriads of biological creatures.		Ludovic A. Krundel;David J. Mulvaney;Vassilios A. Chouliaras	2010	2010 IEEE Computer Society Annual Symposium on VLSI	10.1109/ISVLSI.2010.109	embedded system;electronic engineering;engineering;computer engineering	Theory	10.025614066432803	57.11494055682436	16371
36d431561c189e47025271c6e8beb4371af28cc3	a hardware oriented fuzzification algorithm and its vlsi implementation	cost accuracy trade off;vlsi;fuzzification algorithm;fuzzy hardware	An efficient fuzzification algorithm named as Dynamic Precision Fuzzification (DPF) is introduced in this paper which is mainly developed for hardware implementation. The DPF which might be generally used with any piecewise linear membership function, exploits an inherent capacity of the normal fuzzification algorithm to improve its efficiency when realized in a finite-precision implementation bed such as digital VLSI. The accuracy simulation results of the DPF and normal fuzzification method are presented and compared to show the superiority of the DPF. As the word-length is the most important parameter in a finite-precision implementation environment which determines the system cost-precision trade-off, the simulation results show that DPF provides suitable precision improvements with respect to traditional fuzzification without increasing the system word-length. The VLSI synthesis results of both methods are also presented to show that this considerable accuracy improvement is achieved by an acceptable increase in its VLSI implementation costs in terms of area, delay, and power consumption with respect to traditional methods.	algorithm;fuzzy set	Mohammad Haji Seyed Javadi;Hamid Reza Mahdiani;Esmaeil Zeinali Kh.	2013	Soft Comput.	10.1007/s00500-012-0940-3	simulation;computer science;machine learning;data mining;very-large-scale integration	Robotics	10.8552777902783	47.003813586163965	16385
8856d4c235d84363f90a50f142347d33fd5e64b2	aeris: area/energy-efficient 1t2r reram based processing-in-memory neural network system-on-a-chip		ReRAM-based processing-in-memory (PIM) architecture is a promising solution for deep neural networks (NN), due to its high energy efficiency and small footprint. However, traditional PIM architecture has to use a separate crossbar array to store either positive or negative (P/N) weights, which limits both energy efficiency and area efficiency. Even worse, imbalance running time of different layers and idle ADCs/DACs even lower down the whole system efficiency. This paper proposes AERIS, an <u>A</u>rea/<u>E</u>nergy-efficient 1T2R <u>R</u>eRAM based processing-<u>I</u>n-memory NN <u>S</u>ystem-on-a-chip to enhance both energy and area efficiency. We propose an area-efficient 1T2R ReRAM structure to represent both P/N weights in a single array, and a reference current cancelling scheme (RCS) is also presented for better accuracy. Moreover, a layer-balance scheduling strategy, as well as the power gating technique for interface circuits, such as ADCs/DACs, is adopted for higher energy efficiency. Experiment results show that compared with state-of-the-art ReRAM-based architectures, AERIS achieves 8.5x/1.3x peak energy/area efficiency improvements in total, due to layer-balance scheduling for different layers, power gating of interface circuits, and 1T2R ReRAM circuits. Furthermore, we demonstrate that the proposed RCS compensates the non-ideal factors of ReRAM and improves NN accuracy by 5.2% in the XNOR net on CIFAR-10 dataset.		Jinshan Yue;Yongpan Liu;Fang Su;Shuangchen Li;Zhe Yuan;Zhibo Wang;Wenyu Sun;Xueqing Li;Huazhong Yang	2019		10.1145/3287624.3287635	system on a chip;electronic engineering;scheduling (computing);electronic circuit;efficient energy use;crossbar switch;computer science;power gating;resistive random-access memory;xnor gate	EDA	4.090332887690461	43.03274048465039	16420
0e303a715e22b2fc40c92f6ae986e1a157411bf1	automated integration and communication synthesis of reconfigurable mpsoc platform	reconflgurable mpsoc design;hardware software codesign;reconfigurable computing;reconfigurable architectures;functional description;hardware computer architecture multiprocessing systems application software costs bandwidth space exploration field programmable gate arrays system on a chip adaptive systems;fpga;system buses;fpga reconflgurable mpsoc design multiprocessor system on chip hardware software communication synthesis high level architecture functional description custom bus architecture;system on chip;custom bus architecture;multiprocessor system on chip;multiprocessing systems;field programmable gate arrays;high level architecture;hardware software communication synthesis;system on chip field programmable gate arrays hardware software codesign multiprocessing systems reconfigurable architectures system buses;communication synthesis	The communication synthesis is the main problematic in the multiprocessor system-on-chip (MPSoC). To resolve this problem, several methodologies can be used. These methodologies require automated methods to specify, generate and optimize the hardware, software, and the architectural interfaces between them. In this paper, we present a methodology flow for hardware-software communication synthesis for multiprocessor system-on-chip platform which are dedicated to streaming applications. Our methodology consists of high level architecture communication synthesis from functional description of the MPSoC design. The solution that we propose consists in synthesizing a custom bus architecture for the reconfigurable computing applications, which therefore allows minimizing hardware cost in the FPGA.	central processing unit;crossbar switch;esa;emoticon;field-programmable gate array;high-level architecture;image processing;level design;mpsoc;multiprocessing;network on a chip;performance;reconfigurable computing;test bench;wishbone (computer bus)	AbdelHalim Samahi;El-Bay Bourennane	2007	Second NASA/ESA Conference on Adaptive Hardware and Systems (AHS 2007)	10.1109/AHS.2007.35	embedded system;computer architecture;parallel computing;computer science;field-programmable gate array	EDA	2.828151053248332	52.21154539544459	16445
db983e9bc1b5c09e6005745cbfcd021e81ca8a4f	constructions of sparse asymmetric connectors: extended abstract.	graph theory;distributed system;reseau communication;teoria grafo;systeme reparti;discrete mathematics;theorie graphe;sistema repartido;informatique theorique;diskret matematik;red de comunicacion;communication network;computer theory;informatica teorica	We consider the problem of connecting a set I of n inputs to a set O of N outputs (n   -e  for some e > 0. Moreover, we give explicit constructions based on a new number theoretic approach that need O(Nn   + N  n 3/2 ) edges.		Andreas Baltz;Gerold Jäger;Anand Srivastav	2003		10.1007/978-3-540-24597-1_2	combinatorics;artificial intelligence;graph theory;mathematics;algorithm;telecommunications network	Theory	22.712106451066937	32.47097463436686	16536
4de82011a0e03f30b1a64f2a917fbc4bbdfb5a78	k-neighborhood broadcasting	networks.;communications;broadcasting;2 dimensional	Broadcasting refers to the task whereby a node in a network, knowing a piece of information, must transmit it to all the other nodes. In this paper, we consider a generalized form of broadcasting, that we call k-neighborhood broadcasting. It consists in the following : a node u in the network has to send its information to all the nodes which are at distance less than or equal to k from u. We study k-neighborhood broadcasting (or k-NB for short) in paths, trees, cycles, 2-dimensional grids and 2-dimensional tori under the store and forward, 1-port, unit cost model. For most of these families, we give the optimal k-NB time ; if not, the optimal k-NB time is given within an additive constant never exceeding 2.	analysis of algorithms;isogonal figure;naive bayes classifier;node (computer science);store and forward;triangulated irregular network;utility functions on indivisible goods;vertex-transitive graph	Guillaume Fertin;André Raspaud	2001			computer network;telecommunications;broadcasting;computer science	Theory	20.83592224996929	34.42743421887756	16538
3940faf90e6d33e4b7b5f5f1a6b302688970cf16	a$^{\mbox{\huge\bf 2}}$bc: adaptive address bus coding for low power deep sub-micron designs	coupling;energy efficient;performance;a priori knowledge;total power;low power;leakage power;power consumption;deep sub micron;energy saving	Due to larger buses (length, width) and deep sub-micron effects where coupling capacitances between bus lines are in the same order of magnitude as base capacitances, power consumption of interconnects starts to have a significant impact on a system's total power consumption. We present novel address bus encoding schemes that take coupling effects into consideration. The basis is a physical bus model that quantifies coupling capacitances. As a result, we report power/energy savings on the address buses of up to 56% compared to the best known ordinary power/energy efficient encoding schemes. Thereby, we exceed the only to-date approach that also takes coupling effects into consideration. Moreover, our encoding schemes do not assume any a priori knowledge that is particular to a specific application.	address bus;bus encoding;coupling (computer programming);electrical connection	Jörg Henkel;Haris Lekatsas	2001		10.1145/378239.379058	electronic engineering;real-time computing;a priori and a posteriori;performance;engineering;electrical engineering;efficient energy use;coupling	EDA	15.600492295886907	56.83808770548265	16596
57e356b973cb23c97d9d9272f2ea18748ab50d9a	design of digit-serial fir filters: algorithms, architectures, and a cad tool	multiple constant multiplications 0 1 integer linear programming ilp digit serial arithmetic finite impulse response fir filters gate level area optimization;0 1 integer linear programming ilp;gate level area optimization;logic gates algorithm design and analysis finite impulse response filter optimization hardware approximation algorithms complexity theory;fir filters circuit cad circuit optimisation;finite impulse response filters digit serial fir filter design cad tool low complexity bit parallel multiple constant multiplication design digital signal processing systems digit serial mcm design gate level area high level synthesis algorithms design architectures computer aided design tool optimization algorithms;circuit cad;fir filters;circuit optimisation;multiple constant multiplications;finite impulse response fir filters;digit serial arithmetic	In the last two decades, many efficient algorithms and architectures have been introduced for the design of low-complexity bit-parallel multiple constant multiplications (MCM) operation which dominates the complexity of many digital signal processing systems. On the other hand, little attention has been given to the digit-serial MCM design that offers alternative low-complexity MCM operations albeit at the cost of an increased delay. In this paper, we address the problem of optimizing the gate-level area in digit-serial MCM designs and introduce high-level synthesis algorithms, design architectures, and a computer-aided design tool. Experimental results show the efficiency of the proposed optimization algorithms and of the digit-serial MCM architectures in the design of digit-serial MCM operations and finite impulse response filters.	algorithm;computer-aided design;design tool;digital signal processing;finite impulse response;high- and low-level;high-level synthesis;mathematical optimization;multi-chip module	Levent Aksoy;Cristiano Lazzari;Eduardo Costa;Paulo F. Flores;José C. Monteiro	2013	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2012.2188917	electronic engineering;real-time computing;computer science;theoretical computer science;finite impulse response	EDA	14.106013715263643	46.919180308975214	16610
46d571de72fb4b0820428ea5579022304f8a73b7	characterizing the impact of intermittent hardware faults on programs	fault propagation;cmos integrated circuits;hardware trap;extreme complimentary metal oxide semiconductor;microarchitecture;circuit faults;computer crashes;nondeterministic bursts;transient analysis;intermittent hardware faults;fault injection campaigns;failure analysis;circuit faults hardware microarchitecture computer crashes transient analysis benchmark testing fault tolerance;software based diagnosis intermittent hardware faults cmos technology scaling extreme complimentary metal oxide semiconductor computer system reliability nondeterministic bursts processor failures real world machines fault tolerance fault injection campaigns microarchitectural processor simulator nonbenign intermittent hardware errors hardware trap silent data corruptions;nonbenign intermittent hardware errors;silent data corruptions;processor failures;intermittent hardware faults fault diagnosis fault injection fault model fault propagation;fault tolerance;real world machines;integrated circuit reliability;fault model;fault injection;software based diagnosis;microarchitectural processor simulator;computer system reliability;benchmark testing;fault diagnosis;microprocessor chips;microprocessor chips cmos integrated circuits failure analysis fault tolerance integrated circuit reliability;hardware;cmos technology scaling	Extreme complimentary metal-oxide-semiconductor (CMOS) technology scaling is causing significant concerns in the reliability of computer systems. Intermittent hardware errors are non-deterministic bursts of errors that occur in the same physical location. Recent studies have found that 40% of the processor failures in real-world machines are due to intermittent hardware errors. A study of the effects of intermittent faults on programs is a critical step in building fault-tolerance techniques of reasonable accuracy and cost. In this work, we characterize the impact of intermittent hardware faults in programs using fault-injection campaigns in a microarchitectural processor simulator. We find that 80% of the non-benign intermittent hardware errors activate a hardware trap in the processor, and the remaining 20% cause silent data corruptions. We have also investigated the possibility of using the program state at failure time in software-based diagnosis techniques, and found that much of the erroneous data are intact and can be used to identify the source of the error.	algorithm;burst error;cmos;crash (computing);dynamic data;experiment;fault injection;fault tolerance;image scaling;memory footprint;microarchitecture;processor register;propagation of uncertainty;register file;reliability engineering;semiconductor;software propagation;state (computer science);terminate (software)	Layali Rashid;Karthik Pattabiraman;Sathish Gopalakrishnan	2015	IEEE Transactions on Reliability	10.1109/TR.2014.2363152	reliability engineering;embedded system;benchmark;failure analysis;fault tolerance;parallel computing;real-time computing;microarchitecture;engineering;fault model;cmos	Arch	8.388516098961585	60.43804926416949	16674
154e9c1315832f5fd288525b56b97420bafcfcd3	all-to-all broadcast problems on cartesian product graphs	all to all broadcast;hypercube;broadcasting set;cartesian product;all to all broadcasting number;cycle;complete graph	All-to-all communication occurs in many important applications in parallel processing. In this paper, we study the all-to-all broadcast number (the shortest time needed to complete the all-to-all broadcast) of Cartesian product of graphs under the assumption that: each vertex can use all of its links at the same time, and each communication link is half duplex and can carry only one message at a unit of time. We give upper and lower bounds for the all-to-all broadcast number of Cartesian product of graphs and give formulas for the all-to-all broadcast numbers of some classes of graphs, such as the Cartesian product of two cycles, the Cartesian product of a cycle with a complete graph of odd order, the Cartesian product of two complete graphs of odd order, and the hypercube Q 2 n under this model.	cartesian closed category	Fei-Huang Chang;Ma-Lian Chia;David Kuo;Sheng-Chyang Liaw;Jen-Chun Ling	2016	Theor. Comput. Sci.	10.1016/j.tcs.2015.10.002	box topology;combinatorics;discrete mathematics;graph product;cartesian product;mathematics;distributed computing;complete graph;hypercube	Theory	22.486125574173546	34.5754986897538	16684
f1f04b60e3953cae84e689f30317447c1886796e	a computer-aided-design system for segmented-folded pla macro-cells	achievable result;design symbolic layout generation;pla structure;layout synthesis;segmented-folded pla macro-cells;compact area;brief outline;low design time;computer-aided-design system;vlsi application;folding feature;practical design algorithm;computer aided design;application software;logic design;very large scale integration;computer science;algorithm design and analysis	A PLA structure incorporating segmenting and folding features intended for use as a macro-cell for VLSI applications is described. A brief outline of a computer-aided-design system for the layout synthesis of such PLAs is given. The objectives are the attainment of compact area, low design time, and ease of design Symbolic layout generation for the PLA is treated in detail and practical design algorithms for partitioning and folding are discussed. An example is used to illustrate currently achievable results.	algorithm;computer-aided design;data compaction;data structure;hoare logic;programmable logic array;very-large-scale integration	I. Suwa;William J. Kubitz	1981	18th Design Automation Conference		embedded system;algorithm design;computer architecture;electronic engineering;application software;logic synthesis;logic optimization;logic family;programmable logic array;computer science;theoretical computer science;operating system;computer aided design;programmable logic device;complex programmable logic device;very-large-scale integration;simple programmable logic device;digital electronics;programmable array logic;register-transfer level;algorithm	EDA	12.074603441924987	50.71408876247176	16706
81730293816daabb5a82e4f3651f5d9346f6e677	signature analysis with modified linear feedback shift registers (m-lfsrs)	signature analysis;shift registers feedback logic testing;circuit under test;circuit faults;aliasing probability;m lfsrs;clocks;linear feedback shift registers;hardware requirements;linear feedback shift register;logic circuits;modified linear feedback shift registers;feedback;built in self test;circuit under test responses;vectors;compaction;shift registers;logic testing;analytic expressions;circuit testing;linear feedback shift registers circuit faults compaction built in self test hardware circuit testing clocks steady state logic circuits vectors;circuit under test responses signature analysis analytic expressions modified linear feedback shift registers m lfsrs aliasing probability hardware requirements average tet time overhead;average tet time overhead;hardware;steady state	A signature analysis technique that uses modified linear feedback shift registers (LFSRs) is presented. It is demonstrated that the modified-LFSR-based analyzers can be designed with significantly lower aliasing probability compared to conventional LFSRs of the same size. The methodology for their design is described. Analytic expressions for their aliasing probability, hardware requirements, and the average tet-time overhead are obtained and experimentally verified. Studies using several circuit-under-test responses corroborate the improvement in the aliasing probability for the same hardware (or reduction in hardware for the same aliasing probability) when the modified LFSRs are used. The extra test-time required by the modified LFSRs is found to be nominal ( >		R. Raina;Peter N. Marinos	1991		10.1109/FTCS.1991.146639	electronic engineering;real-time computing;computer science;algorithm	Crypto	20.752349672907602	51.46090083381303	16758
83d52da79ffc062260430bba9891855dbeca824b	fault simulation and test algorithm generation for random accessmemories	random access memory;software tool;fault simulation;integrated circuit;random access storage automatic test pattern generation fault simulation integrated circuit testing;memory testing;automatic test pattern generation;pattern generation;indexing terms;chip;multiport memory fault simulation test algorithm generation random access memory semiconductor memory ramses software tool tags word oriented memory bit oriented memory;memory architecture;integrated circuit testing;random access storage;circuit faults random access memory circuit testing read write memory analytical models semiconductor memory semiconductor device testing automatic testing algorithm design and analysis technical activities guide tag;fault model	The size and density of semiconductor memories is rapidly growing, making them increasingly harder to test. New fault models and test algorithms have been continuously proposed to cover defects and failures of modern memory chips and cores. However, software tool support for automating the memory test development procedure is still insufficient. For this purpose, we have developed a fault simulator (called RAMSES) and a test algorithm generator (called TAGS) for random-access memories (RAMs). In this paper, we present the algorithms and other details of RAMSES and TAGS and the experimental results of these tools on various memory architectures and configurations. We show that efficient test algorithms can be generated automatically for bit-oriented memories, word-oriented memories, and multiport memories, with 100% coverage of the given typical RAM faults.	algorithm;fault simulator;fault model;memory tester;programming tool;random access;random-access memory;semiconductor memory;simulation	Chi-Feng Wu;Chih-Tsun Huang;Kuo-Liang Cheng;Cheng-Wen Wu	2002	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.992771	chip;parallel computing;index term;fault coverage;computer hardware;computer science;theoretical computer science;automatic test pattern generation;integrated circuit;fault model	EDA	21.427523226329157	52.63680809560494	16759
142c956d8792b413064bbe486a748f5558d16ede	a new triple-layer otc channel router	circuit multicouche;concepcion asistida;surface minimale;computer aided design;double layer;routeur;routing circuits nonhomogeneous media rails large scale integration logic synthesizers algorithm design and analysis;implantation topometrie;algorithm performance;integrated circuit;integrated circuit layout;superficie minima;routing;channel area triple layer otc channel router over the cell routing irregular cell area standard cells custom cells double layer routing track to metal3 transformation segment to metal3 transformation segment to metal1 transformation triple layer routing physical circuit layouts fully customized cells mcnc benchmark circuits deutsch s difficult problem;circuito integrado;layout;cellular arrays;circuit a la demande;network routing;experimental result;custom circuit;circuito integrato personalizado;multilayered circuit;resultado algoritmo;application specific integrated circuits;linear time;performance algorithme;resultado experimental;logic cad network routing cellular arrays application specific integrated circuits integrated circuit layout;conception assistee;router;encaminamiento;minimal surface;resultat experimental;logic cad;circuito multicapa;circuit integre;acheminement;implantacion topometria	We propose a new algorithm for a triple-layer over-the-cell (OTC) routing in an irregular cell area. While other OTC routers assume that the cell structure is restricted to a certain type such as uniform height, our OTC router accepts both standard cells and custom cells. Our OTC channel routing consists of five steps: (i) double-layer routing, (ii) track-to-metal3 transformation, (iii) segment-to-metal3 transformation, (iv) segment-to-metal1 transformation, and (v) triple-layer routing. We prove that the transformation steps can be executed in linear time. The effectiveness of our algorithm is demonstrated through physical circuit layouts with fully customized cells. Our algorithm is applied to route MCNC benchmark circuits and Deutsch's difficult problem resulting in a significant reduction in channel area.	channel router;over-the-counter (finance);router (computing)	Jaewon Kim;Sung-Mo Kang	1996	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.536712	embedded system;routing;electronic engineering;static routing;telecommunications;computer science;engineering;electrical engineering;operating system;computer aided design;algorithm;computer network	EDA	15.248816612854162	50.89489418496891	16778
f3bf5fbd25aad2e4cc8f0b324f4320ab390a19dc	testing for missing-gate faults in reversible circuits	design for testability;circuit testing circuit faults quantum computing design for testability electrical fault detection fault detection computer architecture laboratories application software circuit simulation;fault models;reversible circuits;missing gate faults;design for test method fault testing missing gate faults reversible circuits logical reversibility quantum circuits k cnot gates controllable not gates k cnot circuits stuck at model n gate k cnot circuit test vectors;failure mode;quantum gates;circuit simulation;built in self test;low power;quantum circuits;logic testing;integrated circuit testing;design for test;circuit simulation logic testing quantum gates integrated circuit testing design for testability fault location built in self test;fault model;design for test reversible circuits quantum circuits fault models missing gate faults;fault location	Logical reversibility occurs in low-power applications and is an essential feature of quantum circuits. Of special interest are reversible circuits constructed from a class of reversible elements called k-CNOT (controllable NOT) gates. We review the characteristics of k-CNOT circuits and observe that traditional fault models like the stuck-at model may not accurately represent their faulty behavior or test requirements. A new fault model, the missing gate fault (MGF) model, is proposed to better represent the physical failure modes of quantum technologies. It is shown that MGFs are highly testable, and that all MGFs in an N-gate k-CNOT circuit can be detected with from one to [N/2] test vectors. A design-for-test (DFT) method to make an arbitrary circuit fully testable for MGFs using a single test vector is described. Finally, we present simulation results to determine (near) optimal test sets and DFT configurations for some benchmark circuits.	benchmark (computing);controlled not gate;design for testing;fault model;integrated circuit;low-power broadcasting;quantum circuit;requirement;reversible computing;simulation;test vector	John P. Hayes;Ilia Polian;Bernd Becker	2004	13th Asian Test Symposium	10.1109/ATS.2004.84	reliability engineering;electronic engineering;engineering;stuck-at fault;theoretical computer science;design for testing;algorithm	EDA	23.038210552770586	52.12590381271509	16853
270c5561089241310d2240673141f824d65820c3	techniques for formal transformations of binary decision diagrams	boolean operations formal transformations binary decision diagrams discrete function representation multi level logic networks network complexity nonterminal nodes;boolean functions;logic circuits;data structures boolean functions binary decision diagrams upper bound input variables graphics logic functions polynomials autocorrelation optimization methods;circuit complexity;boolean functions binary decision diagrams circuit complexity logic circuits;boolean operation;binary decision diagrams;technology mapping;binary decision diagram	Binary decision diagrams (BDDs), when used for the representation of discrete functions, permit the direct technology mapping into multi-level logic networks. Complexity of a network derived from a BDD is expressed by its number of non-terminal nodes. The paper discusses the problem of reducing the BDDs. It makes two main contributions: (a) the bounds of the potential complexity of the BDD are determined and proven; (b) a formal technique is presented for simplification of Boolean operations on a set of BDDs.	binary decision diagram;complexity;emoticon;level of detail	Giora Kolotov;Ilya Levin;Vladimir Ostrovsky	2004	Proceedings of the 2004 11th IEEE International Conference on Electronics, Circuits and Systems, 2004. ICECS 2004.	10.1109/ICECS.2004.1399730	circuit complexity;boolean algebra;boolean circuit;and-inverter graph;circuit minimization for boolean functions;discrete mathematics;reed–muller expansion;boolean network;boolean domain;binary expression tree;majority function;decision tree model;boolean expression;product term;standard boolean model;theoretical computer science;complexity index;mathematics;boolean function;binary decision diagram;algorithm;two-element boolean algebra	EDA	18.839670900914044	45.779119229721054	16871
09294e107b9bf070a506960624b0c732d2916bca	designing an exascale interconnect using multi-objective optimization		Exascale performance will be delivered by systems composed of millions of interconnected computing cores. The way these computing elements are connected with each other (network topology) has a strong impact on many performance characteristics. In this work we propose a multi-objective optimization-based framework to explore possible network topologies to be implemented in the EU-funded ExaNeSt project. The modular design of this system's interconnect provides great flexibility to design topologies optimized for specific performance targets such as communications locality, fault tolerance or energy-consumption. The generation procedure of the topologies is formulated as a three-objective optimization problem (minimizing some topological characteristics) where solutions are searched using evolutionary techniques. The analysis of the results, carried out using simulation, shows that the topologies meet the required performance objectives. In addition, a comparison with a well-known topology reveals that the generated solutions can provide better topological characteristics and also higher performance for parallel applications.	algorithm;baseline (configuration management);crossover (genetic algorithm);experiment;fault tolerance;locality of reference;mathematical optimization;modular design;multi-objective optimization;multipath routing;mutation (genetic algorithm);network topology;optimization problem;parallel computing;population;procedural generation;router (computing);simulation	Jose Antonio Pascual;Joshua Lant;Andrew Attwood;Caroline Concatto;Javier Navaridas;Mikel Luján;John Goodacre	2017	2017 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2017.7969572	computer science;network topology;interconnection;modular design;fault tolerance;multi-objective optimization;topology optimization;distributed computing;optimization problem;logical topology	HPC	0.3328082698714893	60.067759359882615	16873
27e39ff0d20f564c038b6254e9897e0854d63f7d	mapping time-critical safety-critical cyber physical systems to hybrid fpgas	software;safety critical software distributed processing field programmable gate arrays multiprocessing systems performance evaluation reconfigurable architectures;hardware field programmable gate arrays vehicles software actuators process control computer architecture;bandwidth reduction time critical safety critical cyber physical system mapping hybrid fpga cps safety critical functions electronic control units communication demands computational demands software ecu multicore approach reconfigurable fabric parallel hardware implementation complex sensor processing software flexibility distributed processing;actuators;computer architecture;process control;vehicles;field programmable gate arrays;hardware	Cyber Physical Systems (CPSs), such as those found in modern vehicles, include a number of important time and safety-critical functions. Traditionally, applications are mapped to several dedicated electronic control units (ECUs), and hence, as new functions are added, compute weight and cost increase considerably.%ECU consolidation, where multiple functions are combined on fewer ECUs is an important area, but traditional software ECUs fail to offer the required performance, parallelism, and isolation to support this. With increasing computational and communication demands, traditional software ECUs fail to offer the required performance to provide determinism and predictability, while multi-core approaches fail to provide sufficient isolation between tasks. Hybrid FPGAs, combining a processor and reconfigurable fabric on a single die, allow for parallel hardware implementation of complex sensor processing tightly coupled with the flexibility of software on a processor. We demonstrate the advantages of such architectures in consolidating distributed processing with predictability, determinism and isolation, enabling ECU consolidation and bandwidth reduction.	central processing unit;cyber-physical system;distributed computing;embedded system;engine control unit;field-programmable gate array;multi-core processor;parallel computing;semiconductor consolidation;sensor	Kizheppatt Vipin;Shanker Shreejith;Suhaib A. Fahmy;Arvind Easwaran	2014	2014 IEEE International Conference on Cyber-Physical Systems, Networks, and Applications	10.1109/CPSNA.2014.14	embedded system;real-time computing;reconfigurable computing;computer science;engineering;electrical engineering;operating system;process control;control theory;field-programmable gate array;computer engineering;actuator	EDA	2.3289666286138218	54.39758259033188	16976
fbef7cc66cdbaf0c4b4e258c596e07b2e9744f3c	low-cost dc bist for analog circuits: a case study	logic gates matlab reliability correlation testing;low power electronics analogue integrated circuits built in self test integrated circuit testing;analog testing dft bist;analogue integrated circuits;built in self test;low power electronics;integrated circuit testing;size 65 nm analog circuits low cost dc bist dc analog testing sensitivity to faults node structural testing fault driven testing low voltage transconductor fault coverage catastrophic fault parametric fault	This paper presents a DC analog testing technique based on a simple voltage comparison of the highest sensitivity-to-faults node, which is found by simulation. The technique is a structural, fault driven testing approach and can be applied to any analog circuit with very few extra added circuitry. A proof of concept has been implemented in a 65nm low-voltage transconductor, showing good fault coverage for both catastrophic and parametric faults.	analogue electronics;built-in self-test;electronic circuit;fault coverage;simulation	Pablo A. Petrashin;Carlos Dualibe;Walter J. Lancioni;Luis E. Toledo	2013	2013 14th Latin American Test Workshop - LATW	10.1109/LATW.2013.6562668	mixed-signal integrated circuit;embedded system;electronic engineering;real-time computing;engineering;stuck-at fault	EDA	23.606835117317857	52.869234765590384	17035
751e352fe52946ca3d0f51956706313ce521b658	hierarchical power management for asymmetric multi-core in dark silicon era	feedback controller;multiprocessing systems linux;power management;linux;multiprocessing systems;asymmetric multi core;quality of service heart rate multicore processing benchmark testing linux frequency measurement;arm big little asymmetric multicore platform hierarchical power management asymmetric multicore architecture silicon era diverse power performance characteristics thermal limit hierarchical power management framework control theory coordinate multiple controller thermal design power budget linux;feedback controller asymmetric multi core power management	Asymmetric multi-core architectures integrating cores with diverse power-performance characteristics is emerging as a promising alternative in the dark silicon era where only a fraction of the cores on chip can be powered on due to thermal limits. We introduce a hierarchical power management framework for asymmetric multi-cores that builds on control theory and coordinates multiple controllers in a synergistic manner to achieve optimal power-performance efficiency while respecting the thermal design power budget. We integrate our framework within Linux and implement/evaluate it on real ARM big.LITTLE asymmetric multi-core platform.	control theory;dark silicon;linux;multi-core processor;power management;synergy;thermal design power	Thannirmalai Somu Muthukaruppan;Mihai Pricopi;Vanchinathan Venkataramani;Tulika Mitra;Sanjay Vishin	2013	2013 50th ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2463209.2488949	embedded system;real-time computing;computer science;engineering;operating system;linux kernel	EDA	-4.331762224014378	55.45854704201571	17043
79650f6d9ae0d07f32ac312e597c04a61cdf26be	tinyspice: a parallel spice simulator on gpu for massively repeated small circuit simulations	gpu computing;variation aware analysis;spice simulation;newton raphson method;integrated circuit design;circuit simulation;newton raphson iterations tinyspice parallel spice simulator gpu massively repeated small circuit simulations variation aware ic designs cell characterizations sram memory yield analysis nonlinear circuits lut data structures;gpu computing variation aware analysis spice simulation;data structures;graphics processing units;sram chips circuit simulation data structures graphics processing units integrated circuit design newton raphson method spice;integrated circuit modeling central processing unit vectors graphics processing units educational institutions abstracts mobile communication;spice;sram chips	In nowadays variation-aware IC designs, cell characterizations and SRAM memory yield analysis require many thousands or even millions of repeated SPICE simulations for relatively small nonlinear circuits. In this work, we present a massively parallel SPICE simulator on GPU, TinySPICE, for efficiently analyzing small nonlinear circuits, such as standard cell designs, SRAMs, etc. In order to gain high accuracy and efficiency, we present GPU-based parametric three-dimensional (3D) LUTs for fast device evaluations. A series of GPU-friendly data structures and algorithm flows have been proposed in TinySPICE to fully utilize the GPU hardware resources, and minimize data communications between the GPU and CPU. Our GPU implementation allows for a large number of small circuit simulations in GPU's shared memory that involves novel circuit linearization and matrix solution techniques, and eliminates most of the GPU device memory accesses during the Newton-Raphson (NR) iterations, which enables extremely high-throughput SPICE simulations on GPU. Compared with CPU-based TinySPICE simulator, GPU-based TinySPICE achieves up to 138X speedups for parametric SRAM yield analysis without loss of accuracy.	algorithm;central processing unit;data structure;graphics processing unit;high-throughput computing;iteration;newton's method;noise reduction;nonlinear system;spice;shared memory;simulation;standard cell;static random-access memory;throughput	Lengfei Han;Xueqian Zhao;Zhuo Feng	2013	2013 50th ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2463209.2488843	embedded system;computer architecture;electronic engineering;parallel computing;data structure;computer hardware;computer science;operating system;newton's method;programming language;general-purpose computing on graphics processing units;integrated circuit design	EDA	12.787188811263368	49.29262887162313	17081
2a171705dd9d8d1f14f9b9806d4c8256a3f14ace	a genetic programming approach to reconfigure a morphological image processing architecture	digital image processing;genetic programming approach;morphological image processing architecture;automatic image;morphological instruction;original reconfigurable architecture;developed architecture;morphological operator;genetic programming;color image practical application;low-level image analysis	Mathematical morphology supplies powerful tools for low-level image analysis. Many applications in computer vision require dedicated hardware for real-time execution. The design of morphological operators for a given application is not a trivial one. Genetic programming is a branch of evolutionary computing, and it is consolidating as a promising method for applications of digital image processing. The main objective of genetic programming is to discover how computers can learn to solve problems without being programmed for that. In this paper, the development of an original reconfigurable architecture using logical, arithmetic, and morphological instructions generated automatically by a genetic programming approach is presented. The developed architecture is based on FPGAs and has among the possible applications, automatic image filtering, pattern recognition and emulation of unknown filter. Binary, gray, and color image practical applications using the developed architecture are presented and the results are compared with similar techniques found in the literature.		Emerson Carlos Pedrino;José Hiroki Saito;Valentin Obac Roda	2011	Int. J. Reconfig. Comp.	10.1155/2011/712494	embedded system;computer vision;parallel computing;computer science;theoretical computer science;operating system;algorithm	Robotics	8.421168573506131	48.362283376899505	17145
8fc4ee14b0430f96b980b334adeb6be917748b57	leave the wires to last-functional evaluation of the ieee std 1149.5 module test and maintenance bus	functional evaluation;automotive engineering;mtm bus standard;ieee standards;cost function;maintenance engineering ieee standards automatic testing system buses modules;application software;automatic testing;backplanes;cost of implementation;module test and maintenance bus;wires;maintenance engineering;system buses;live insertion;standards development;conformant devices;fault tolerance;aerospace electronics;high level model;live insertion functional evaluation ieee std 1149 5 module test and maintenance bus mtm bus standard high level model cost of implementation conformant devices;system testing;very high speed integrated circuits;ieee std 1149 5;wires system testing cost function standards development aerospace electronics automotive engineering application software backplanes very high speed integrated circuits fault tolerance;modules	"""The IEEE Std 1149.5 Module Test and Maintenance (MTM) Bus Standard is described using a high level model of system testing. Constituent elements of the Standard are examined as logical consequences of and choices based upon, this model. Functionality of the bus is outlined; and its cost of implementation, discussed. The MTM-Bus is briefly compared to IEEE Std 1149.1 with regard to the number of conformant devices necessary in order for the MTM-Bus to be usable. Standardized support of """"live insertion"""" of modules is presented as an example of non-test application of the MTM-Bus. Current status of IEEE Std 1149.5 is provided."""		Rodham E. Tulloss	1995		10.1109/TEST.1995.529911	maintenance engineering;std bus;embedded system;dnp3;fault tolerance;backplane;electronic engineering;application software;computer science;engineering;electrical engineering;operating system;modular programming;system testing;ieee floating point;computer engineering	Vision	9.641493128305557	53.40053761772279	17173
6f4456fe5e6855e72e539572d19977a15fae0757	quantum-dot cellular automata serial decimal processing-in-wire: run-time reconfigurable wiring approach	quantum dot cellular automata;reconfigurable computing;decimal adder subtractor;johnson mobius code	The quantum-dot cellular automata (QCA) technology is promising to overcome the limits of CMOS technology for perspective computers. A chain of QCA is used as a wire. Logic gates are simply implemented by a cross pattern of QCA. Because the leading role of QCA wires, the serial data transfer/processing is preferable. The growing market of financial, Internet-based, and automatic control computer applications requires a binary-coded decimal data encoding for direct processing of decimal information without representation and conversion errors. The 5-bit decimal Johnson–Mobius encoding and radical departure from Boolean logic allow using a delay element, implemented by short length of QCA wire, as a function element. The previous published by author serial decimal QCA arithmetic designs demonstrate hardware simplification in comparison with traditional designs. The paper presents novel serial decimal adder and adder/subtractor designs used the run-time reconfigurable wiring approach, which results in further significant QCA hardware simplification.	automata theory;quantum dot cellular automaton;serial decimal;wiring	Michael Gladshtein	2016	Microelectronics Journal	10.1016/j.mejo.2016.07.009	reconfigurable computing;computer science;theoretical computer science;physics;algorithm	Logic	15.61460303412298	45.72528929553163	17219
44074a5f9839ffc22799a674183d3be98a37893c	analysis of reachable sensitisable paths in sequential circuits with sat and craig interpolation	interpolation;mc;justification;reachability;sensitisable path;sequential circuits interpolation;sequential circuits;longest path;model checking;logic gates delay sequential circuits circuit faults integrated circuit modeling interpolation transfer functions;normal operator;craig;itc 99 circuits sequential circuits reachable sensitisable path analysis craig interpolation scanning strategies arbitrary circuit states high adaptable sat based path enumeration algorithm test sequences;small delay fault;test pattern generator;mc sensitisable path longest path sequential circuit atpg small delay fault justification reachability craig bmc;bmc;sequential circuit;atpg	Test pattern generation for sequential circuits benefits from scanning strategies as these allow the justification of arbitrary circuit states. However, some of these states may be unreachable during normal operation. This results in non-functional operation which may lead to abnormal circuit behaviour and result in over-testing. In this work, we present a versatile approach that combines a highly adaptable SAT-based path-enumeration algorithm with a model-checking solver for invariant properties that relies on the theory of Craig interpolants to prove the unreachability of circuit states. The method enumerates a set of longest sensitisable paths and yields test sequences of minimal length able to sensitise the found paths starting from a given circuit state. We present detailed experimental results on the reach ability of sensitisable paths in ITC 99 circuits.	algorithm;boolean satisfiability problem;computation;craig's theorem;formal proof;interpolation;model checking;reachability;solver;unreachable memory	Matthias Sauer;Stefan Kupferschmid;Alexander Czutro;Sudhakar M. Reddy;Bernd Becker	2012	2012 25th International Conference on VLSI Design	10.1109/VLSID.2012.101	electronic engineering;discrete mathematics;theoretical computer science;mathematics;sequential logic;algorithm	EDA	21.2090818500309	50.27484533102126	17288
0ce50e11a95864d1d595ffa2e63d1d59a65cb596	fault modeling and test effectiveness evaluation for vlsi circuits	fault model			Peter S. Bottorff	1982			computer science;electronic engineering;automatic test pattern generation;fault coverage;fault model;very-large-scale integration;fault (power engineering)	EDA	22.682568931712346	51.847534710974365	17310
7edd248d12a0507b9eb8e20962c7b71e088097fb	optimization and implementation of h.264 encoder on symmetric multi-processor platform	quantization;video sequence;optimisation;video coding code standards embedded systems image sequences multiprocessing systems optimisation storage management chips video codecs;memory management;h 264 encoder;optimal method;embedded symmetric multiprocessor architecture;code standards;data mining;embedded system;video coding;embedded systems;h 264 video coding standard;coarse grained functional partitioning method;computational complexity;synchronization;memory management optimization method;multiprocessor architecture;symmetric multi processor;load balancing;h 264;embedded system h 264 video coding symmetric multi processor;storage management chips;video codecs;optimization;multiprocessing systems;coarse grained;magnetic cores;encoding;load balancing h 264 video coding standard h 264 encoder embedded symmetric multiprocessor architecture coarse grained functional partitioning method synchronization memory management optimization method video sequence;image sequences;video coding computer architecture parallel processing computer science computational complexity memory management encoding embedded computing educational institutions optimization methods	The H.264 video coding standard has achieved a significant improvement in coding efficiency over previous standards. However, the computational complexity of the H.264 encoder is increased drastically, which results practical difficulties in its implementation on the embedded platform. This paper presents two implementation techniques to optimize the H.264 encoder on the embedded symmetric multiprocessor architecture. We propose a coarse-grained functional partitioning method to balance the load of the encoder among the cores with small overhead of synchronization. On the other hand, we present a memory management optimization method to exploit the memory subsystem on the embedded platform effectively for the H.264 encoder. The experimental results demonstrate that, for the video sequences with VGA format, the performance of the optimized H.264 encoder is greatly improved.	algorithmic efficiency;computational complexity theory;data compression;embedded system;encoder;h.264/mpeg-4 avc;mathematical optimization;memory management;overhead (computing);symmetric multiprocessing;video graphics array;video coding format	Ouyang Kun;Ouyang Qing;Zhengda Zhou	2009	2009 WRI World Congress on Computer Science and Information Engineering	10.1109/CSIE.2009.740	embedded system;synchronization;parallel computing;real-time computing;quantization;computer science;load balancing;operating system;computational complexity theory;encoding;memory management	Embedded	11.677058683352614	40.078129779609306	17366
966c8c085df7009a03948f88e4cb3ee134e34529	design of a wsi scale parallel processor for intelligent robot control based on a dynamic reconfiguration of multi-operand arithmetic units	robot control	A restructurable (reconfigurable) parallel VLSI processor designed to minimize the operation delay time which can be generally used for various operations neces sary for controlling an intelligent robot was proposed previously by the authors. This processor is constructed by connecting a number of processor elements (PEs) in paral lel under the assumption that one PE is integrated with one VLSI chip. In contrast to this, a method for constructing a highly integrated processor by integrating several tens to 100 PEs on a single WSI is investigated in this paper. First, a method for constructing multiple buses efficiently which move a multiple number of integrated PEs effectively is proposed. In addition, a method for constructing a defectsalvaging system utilizing the restructurable parallel archi tecture by incorporating a defect-salvaging configuration as a measure against the yield decrease of WSI is proposed. As a result of designing a restructurable parallel processor based on the methods proposed, it has been clarified that a WSI scale highly integrated processor with 102 PEs piled up can be constructed with only 35% of the chip surface increase with the incorporation of the defect-salvaging configuration. © 2000 Scripta Technica, Syst Comp Jpn, 31(12): 3342, 2000	archi;bus (computing);central processing unit;cognitive robotics;digi-comp i;multiplexing;operand;overhead (computing);parallel computing;reconfigurable computing;robot control;software bug;usability;very-large-scale integration;wafer-scale integration	Yoshichika Fujioka;Nobuhiro Tomabechi	2000	Systems and Computers in Japan	10.1002/1520-684X(20001115)31:12%3C33::AID-SCJ4%3E3.0.CO;2-3	embedded system;real-time computing;computer science;artificial intelligence;operating system;robot control	Robotics	8.814180857923976	49.60822079754581	17396
cdd598b68a1ca7b2f4a3b48ee24e7030f9d1fa08	current path analysis for electrostatic discharge protection	esd protection;esd connected component;decompose esd connected component;circuit design;esd current path;current path;esd current path analysis;esd analysis;esd path;electrostatic discharge protection;ic pad;path analysis;integrated circuit design;nanoelectronics;graph theory;high voltage;network analysis;connected component;current density;breadth first search;performance;vlsi;electrostatic discharge;manufacturability;chip	The electrostatic discharge (ESD) problem has become a challenging reliability issue in nanometer circuit design. High voltages resulted from ESD might cause high current densities in a small device and burn it out, so on-chip protection circuits for IC pads are required. To reduce the design cost, the protection circuit should be added only for the IC pads with an ESD current path, which arises the ESD current path analysis problem. In this paper, we first introduce the analysis problem for ESD protection in circuit design. We then model the circuit as a constrained graph, decompose ESD connected components linked with the pads, and apply the breadth-first search (BFS) to identify the ESD connected components in each constrained graph and thus the current paths. Experimental results show that our algorithm can detect all ESD paths very efficiently and economically. To our best knowledge, our algorithm is the first point tool available to the public for the ESD analysis.	discharger;path analysis (statistics)	Hung-Yi Liu;Chung-Wei Lin;Szu-Jui Chou;Wei-Ting Tu;Chih-Hung Liu;Yao-Wen Chang;Sy-Yen Kuo	2006		10.1109/ICCAD.2006.320166	nanoelectronics;chip;path analysis;embedded system;electronic engineering;connected component;electrostatic discharge;breadth-first search;network analysis;performance;computer science;engineering;high voltage;electrical engineering;graph theory;circuit design;mathematics;very-large-scale integration;design for manufacturability;current density;integrated circuit design	EDA	17.944210127267517	52.928351563807745	17485
47e689a9a33b7ffef537a656311c28ad2c674dac	a power-aware code-compression design for risc/vliw architecture	lempel ziv;instruction level parallel;vliw processor;reduced instruction set computer;compression algorithm;vliw processors;lzw compression;che wei lin chang hong lin wei jhih wang a power aware code compression design for risc vliw architecture;very long instruction word;memory systems;cost effectiveness;power consumption;static analysis;embedded processor;hardware implementation;embedded computing;code compression;cell based libraries;instruction level parallelism ilp	We studied the architecture of embedded computing systems from the viewpoint of power consumption in memory systems and used a selective-code-compression (SCC) approach to realize our design. Based on the LZW (Lempel-Ziv-Welch) compression algorithm, we propose a novel cost effective compression and decompression method. The goal of our study was to develop a new SCC approach with an extended decision policy based on the prediction of power consumption. Our decompression method had to be easily implemented in hardware and to collaborate with the embedded processor. The hardware implementation of our decompression engine uses the TSMC 0.18 μm-2p6m model and its cell-based libraries. To calculate power consumption more accurately, we used a static analysis method to estimate the power overhead of the decompression engine. We also used variable sized branch blocks and considered several features of very long instruction word (VLIW) processors for our compression, including the instruction level parallelism (ILP) technique and the scheduling of instructions. Our code-compression methods are not limited to VLIW machines, and can be applied to other kinds of reduced instruction set computer (RISC) architecture.	algorithm;central processing unit;code::blocks;compiler;data compression ratio;embedded system;instruction-level parallelism;lempel–ziv–stac;lempel–ziv–welch;library (computing);opcode;overhead (computing);parallel computing;scheduling (computing);scheme;simulation;static program analysis;verilog;very long instruction word;welch's method	Che-Wei Lin;Wei Jhih Wang	2011	Journal of Zhejiang University SCIENCE C	10.1631/jzus.C1000321	data compression;computer architecture;parallel computing;real-time computing;cost-effectiveness analysis;computer science;very long instruction word;static analysis	EDA	-0.10876292382700657	51.30758382930933	17537
065815c9ae506aaa0957177cb5a1c98cb532b542	modeling trans-threshold correlations for reducing functional test time in ultra-low power systems		This paper presents a methodology for reducing functional test time in subthreshold SoCs targeting ultra-low power (ULP) internet-of-things (IoT) devices. Due to their low operating speed and voltage, subthreshold SoCs require significantly longer time to test than traditional SoCs. The proposed method models trans-threshold correlations to allow high voltage, high speed testing while accurately predicting delay and power at the low, subthreshold operational voltage. This approach is orthogonal to other traditional testing methodologies and can significantly reduce the test time of digital and memory blocks on subthreshold SoCs. Using this process results in 5.4 χ savings in test time for sequential and combinational test circuits, and over 2 χ savings in test time for memory circuits with no overhead to area or prefabrication design time.	combinational logic;functional testing;ibm power systems;overhead (computing);source-to-source compiler;system on a chip	Christopher J. Lukas;Farah B. Yahya;Benton H. Calhoun	2017	2017 IEEE International Test Conference (ITC)	10.1109/TEST.2017.8242039	electronic engineering;computer science;operating speed;electric power system;threshold voltage;high voltage;voltage;electronic circuit;low voltage;subthreshold conduction	EDA	20.233089472692225	54.77523975739711	17592
1fc51d43c8acb70f1217d01feb0603207a87fe75	seu impact in processor's control-unit: preliminary results obtained for leon3 soft-core	reliability;circuit faults;flip flops;micromechanical devices;registers;field programmable gate arrays;program processors	The miniaturization issues from the advanced integrated circuit manufacturing technologies lead to increase the probabilities of single node upset and multiple upsets errors of neighbor nodes. The study of such conjecture is mandatory to specify the protection requirements. This paper deals with the study of such single and multiple errors due to the impact of a single particle in the control unit of complex devices such as processors. Because the layout of the studied device cannot be anticipated, the node's neighborhood is thus unknown. To deal with this issue, this work presents the results of both exhaustive and random fault-injection experiments performed at register transfer level (RTL) and targeting the control bits of LEON3 processor. Fault injection is achieved by means an automatic netlist fault injection tool called NETFI-2.	central processing unit;computer program;control unit;experiment;fault (technology);fault injection;fault tolerance;field-programmable gate array;index register;integrated circuit;leon;netlist;register-transfer level;requirement;single event upset	Thierry Bonnoit;Alexandre Coelho;Nacer-Eddine Zergainoh;Raoul Velazco	2017	2017 18th IEEE Latin American Test Symposium (LATS)	10.1109/LATW.2017.7906763	embedded system;electronic engineering;real-time computing;engineering	EDA	8.45708145169933	59.22725826425134	17641
ca95ffe0b312cce07b3a5f5b1323b76e54973253	vector coding techniques for high speed digital simulation	scalar processor;vector coding;digital simulation;software design;high speed;scientific processing;use vector;execution speed;computer simulation;computational modeling;connection;arithmetic;hardware;registers;data structures;discrete event simulation	Execution speed of event driven simulators can be enhanced by software designs that use vector coding techniques developed for scientific processing. The power of vector coding can be utilized by logic simulators running on scalar processors.	central processing unit;data compression;logic simulation;scalar processor	Howard E. Krohn	1981	18th Design Automation Conference		computer simulation;computer architecture;electronic engineering;parallel computing;connection;computer science;software design;theoretical computer science;discrete event simulation;processor register;computational model	EDA	7.657597227719123	47.449034995456145	17688
8d9e2ec122a3c35bfa3b329f122545e678e1d440	computing the two-dimensional discrete fourier transform on the aspen parallel computer architecture	discrete fourier transform		computer architecture;discrete fourier transform	Allen L. Gorin;Allan J. Silberger;Louis Auslander	1987			parallel computing;discrete hartley transform;architecture;discrete fourier transform (general);discrete cosine transform;discrete fourier transform;computer science;harmonic wavelet transform	Arch	8.343623343248492	39.461132664131846	17709
2bfdc78dd7f5087dc7875345dc34b0dbe0987919	spiral: joint runtime and energy optimization of linear transforms	microprocessors;digital signal processing;software engineering discrete fourier transforms program compilers signal processing;application software;intel pentium m microprocessor;intel pentium m microprocessor linear transforms energy optimization signal processing algorithms digital signal processing dsp software implementations voltage frequency scaling hardware technique spiral code generation system discrete fourier transform;linear transforms;code generation;spiral;runtime;software engineering;automatic generation;code generation system;sensor network;first order;voltage frequency scaling;energy optimization;spirals runtime signal processing algorithms voltage hardware digital signal processing discrete fourier transforms microprocessors application software embedded computing;signal processing;voltage;spirals;software development;discrete fourier transform;mobile communication;linear transformation;software implementations;hardware technique;signal processing algorithms;program compilers;discrete fourier transforms;embedded computing;software implementation;dsp;hardware	There is much interest into joint runtime and energy optimization of implementations of signal processing algorithms. Applications in domains such as embedded computing, sensor networks, and mobile communications often require processing of signals under simultaneous runtime, energy and/or power constraints. Hence, in addition to runtime, power and energy are first-order design considerations for both hardware and software developers in those domains. This paper studies the automatic generation of software implementations of digital signal processing (DSP) transforms that are optimized with respect to both runtime and energy. We explore the impact of algorithm selection (a software technique) and voltage-frequency scaling (a hardware technique) on the runtime and energy of computing fast linear transforms. We use SPIRAL, a code generation system, to enumerate automatically many alternative algorithms for the discrete Fourier transform. We measure the runtime and energy of these algorithms at different voltage-frequency settings of an Intel Pentium M microprocessor. We report experimental results supporting that algorithm selection and voltage-frequency scaling do achieve the following: (1) have large impact on the runtime and energy of computing the discrete Fourier transform on a microprocessor; and (2) enable the optimization of important joint runtime-energy objectives	algorithm selection;code generation (compiler);digital signal processing;discrete fourier transform;embedded system;enumerated type;first-order predicate;frequency scaling;image scaling;mathematical optimization;microprocessor;software developer;spiral model	Marek R. Telgarsky;James C. Hoe;José M. F. Moura	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1660837	computer vision;computer architecture;parallel computing;real-time computing;computer science;electrical engineering;digital signal processing;signal processing;runtime verification;spiral	EDA	2.9633712240613654	53.63072036554195	17723
c474d99c9aaedd59aa2263d33693f04f3b629a3c	structural cdc analysis methods		This paper analyzes a problem of Clock Domain Crossings (CDC) verification in modern System-on-Chips. We suggest a set of topological methods that automatically discover CDCs and detect typical structural mistakes frequently occurring at the border of independent clock domains in many designs. Detecting structural design rule violations is a critical part of a complex CDC verification flow implemented in our commercial product ALINT-PRO-CDC™ [1].	clock signal;network planning and design;system on a chip	Sergiy Zaychenko;Pavlo Leshtaev;Bogdan Gureev;Maksym Shliakhtun	2016	2016 IEEE East-West Design & Test Symposium (EWDTS)	10.1109/EWDTS.2016.7807671	embedded system;real-time computing;engineering;distributed computing	Embedded	18.5433342466478	52.869487395340215	17773
00aea851b012a90a22891e0d81b402a3b0555e6f	data acquisition system based on the t800 transputer for the pc-bus	data acquisition system	This article describes an extensible transputer-based data acquisition system mounted on a PC card format. Comparisons of this large card format are made with the transputer module standard (TRAM) formats with reference to their application in engineering systems, in particular for real-time control, instrumentation and signal processing. Two examples of the use of a multi-processor transputer system are given: the simulation of a broad loom weaving machine for production testing of electronic jacquards, and as a high performance vision system.	bus (computing);data acquisition;transputer	Charles R. Allen;Roy Booth;Stephen McConnell	1993	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/S0141-9331(05)80004-3	embedded system;real-time computing;computer hardware;computer science;data acquisition	EDA	4.76232754291816	47.853837519329176	17827
b0807338cbac801b4812ec5bb6307b183e50bb00	metric-based transformations for self testable vlsi designs with high test concurrency	vlsi;built-in self test;design for testability;high level synthesis;logic testing;bist methodology;behavioral restructuring;high test concurrency;metric-based transformations;self testable vlsi designs	We propose an approach for improving the testability of a design under BIST methodology through behavioral restructuring. Our results show that the proposed transformations help reduce the number of required test sessions.	built-in self-test;concurrency (computer science);high-level programming language;high-level synthesis;overhead (computing);software testability;traverse	Mahsa Vahidi;Alex Orailoglu	1995			computer architecture;electronic engineering;parallel computing;concurrent computing;computer science;electrical engineering;very-large-scale integration;programming language	EDA	19.70800579342846	50.11136630837431	17837
30f9f9d3e00b152a52f1a71367e8f38ee2aafddc	correct synthesis and integration of compiler-generated function units		Computer architectures can use custom logic in addition to general purpose processors to improve performance for a variety of applications. The use of custom logic allows greater parallelism for some algorithms. While conventional CPUs typically operate on words, fine-grained custom logic can improve efficiency for many bit level operations. The commodification of field programmable devices, particularly FPGAs, has improved the viability of using custom logic in an architecture. This thesis introduces an approach to reasoning about the correctness of compilers that generate custom logic that can be synthesized to provide hardware acceleration for a given application. Compiler intermediate representations (IRs) and transformations that are relevant to generation of custom logic are presented. Architectures may vary in the way that custom logic is incorporated, and suitable abstractions are used in order that the results apply to compilation for a variety of the design parameters that are introduced by the use of custom logic.	algorithm;bit-level parallelism;central processing unit;compiler;computer architecture;correctness (computer science);field-programmable gate array;hardware acceleration;intermediate representation;parallel computing	Martin Andrew Ellis	2008			compiler;computer architecture;computer science	EDA	1.6860616193393223	49.86618616081042	17839
bf842f76ae93abfcf5e946fbfebfbe1fcf1f72ec	loop parallelization techniques for fpga accelerator synthesis	altera opencl;vivado hls;vectorization;loop coarsening;loop tiling	Current tools for High-Level Synthesis (HLS) excel at exploiting Instruction-Level Parallelism (ILP). The support for Data-Level Parallelism (DLP), one of the key advantages of Field Programmable Gate Arrays (FPGAs), is in contrast very limited. This work examines the exploitation of DLP on FPGAs using code generation for C-based HLS of image filters and streaming pipelines. In addition to well-known loop tiling techniques, we propose loop coarsening, which delivers superior performance and scalability. Loop tiling corresponds to splitting an image into separate regions, which are then processed in parallel by replicated accelerators. For data streaming, this also requires the generation of glue logic for the distribution of image data. Conversely, loop coarsening allows processing multiple pixels in parallel, whereby only the kernel operator is replicated within a single accelerator. We present concrete implementations of tiling and coarsening for Vivado HLS and Altera OpenCL. Furthermore, we present a comparison of our implementations to the keyword-driven parallelization support provided by the Altera Offline Compiler. We augment the FPGA back end of the heterogeneous Domain-Specific Language (DSL) framework Hipacc to generate loop coarsening implementations for Vivado HLS and Altera OpenCL. Moreover, we compare the resulting FPGA accelerators to highly optimized software implementations for Graphics Processing Units (GPUs), all generated from exactly the same code base. Oliver Reiche · M. Akif Özkan · Frank Hannig · Jürgen Teich Hardware/Software Co-Design, Department of Computer Science, Friedrich-Alexander University Erlangen-Nürnberg (FAU), Germany E-mail: {oliver.reiche, akif.oezkan, hannig, teich}@fau.de Moritz Schmid Siemens Healthcare GmbH, Germany E-mail: moritz.schmid@siemens.com	age of empires ii: the conquerors;algorithm;application-specific integrated circuit;automatic parallelization;blue (queue management algorithm);cuda;central processing unit;code generation (compiler);compiler;computer science;dec alpha;data parallelism;design space exploration;digital light processing;digital signal processor;digital subscriber line;domain-specific language;dual-ported ram;electronic design automation;embedded system;fifo (computing and electronics);field-programmable gate array;graphics processing unit;hardware description language;high-level synthesis;image processing;input/output;instruction-level parallelism;integrated development environment;interrupt latency;online and offline;opencl api;parallel computing;pipeline (software);pixel;place and route;random access;random-access memory;register-transfer level;simd;spmd;scalability;software development kit;speedup;stream (computing);throughput;tiling window manager	Oliver Reiche;M. Akif Ozkan;Frank Hannig;Jürgen Teich;Moritz Schmid	2018	Signal Processing Systems	10.1007/s11265-017-1229-7	parallel computing;operator (computer programming);compiler;field-programmable gate array;scalability;computer architecture;real-time computing;software;computer science;glue logic;code generation;loop tiling	Arch	-0.5860208591574675	46.7584573921101	17844
38a0774be428b0a9f87fb89875734cb00f99606f	an algorithmic approach to error localization and partial recomputation for low-overhead fault tolerance	sparse linear algebra algorithmic error correction partial recomputation error localization numerical methods;mathematics computing;probability;circuit faults;software fault tolerance;checkpointing;error analysis;vectors;fault tolerant systems;computational complexity;fault tolerance;numerical methods;partial recomputation;algorithmic error correction;matrix multiplication;vectors circuit faults fault tolerance fault tolerant systems sparse matrices context error analysis;vectors checkpointing computational complexity conjugate gradient methods mathematics computing matrix multiplication parallel processing probability software fault tolerance;conjugate gradient methods;sparse matrices;context;parallel processing;error localization probability conjugate gradient solver performance improvement mvm iterative linear solvers matrix vector multiplication linear algebra operations faulty application outputs algorithmic correction checkpoint rollback based scheme massively parallel system size massively parallel system complexity low overhead fault tolerance partial recomputation;error localization;sparse linear algebra	The increasing size and complexity of massively parallel systems (e.g. HPC systems) is making it increasingly likely that individual circuits will produce erroneous results. For this reason, novel fault tolerance approaches are increasingly needed. Prior fault tolerance approaches often rely on checkpoint-rollback based schemes. Unfortunately, such schemes are primarily limited to rare error event scenarios as the overheads of such schemes become prohibitive if faults are common. In this paper, we propose a novel approach for algorithmic correction of faulty application outputs. The key insight for this approach is that even under high error scenarios, even if the result of an algorithm is erroneous, most of it is correct. Instead of simply rolling back to the most recent checkpoint and repeating the entire segment of computation, our novel resilience approach uses algorithmic error localization and partial recomputation to efficiently correct the corrupted results. We evaluate our approach in the specific algorithmic scenario of linear algebra operations, focusing on matrix-vector multiplication (MVM) and iterative linear solvers. We develop a novel technique for localizing errors in MVM and show how to achieve partial recomputation within this algorithm, and demonstrate that this approach both improves the performance of the Conjugate Gradient solver in high error scenarios by 3x-4x and increases the probability that it completes successfully by up to 60% with parallel experiments up to 100 nodes.	algorithm;computation;conjugate gradient method;experiment;fault tolerance;iterative method;linear algebra;matrix multiplication;overhead (computing);rollback (data management);solver;transaction processing system	Joseph Sloan;Rakesh Kumar;Greg Bronevetsky	2013	2013 43rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)	10.1109/DSN.2013.6575309	parallel processing;fault tolerance;parallel computing;sparse matrix;numerical analysis;matrix multiplication;computer science;theoretical computer science;probability;distributed computing;computational complexity theory;software fault tolerance	HPC	0.06290065209805647	38.14306474268446	17875
7be7550ecef660918afc7f590b4bcc7799666949	an on-chip communication mechanism design in the embedded heterogeneous multi-core architecture	architectural design;multi core processor;embedded heterogeneous multicore architecture;computer architecture communication system control costs computer science system on a chip embedded system microprocessors reduced instruction set computing digital signal processing communication system traffic control;main cooperation model;chip multiprocessor;satisfiability;chip;multiprocessing systems computer architecture;computer architecture;multiprocessing systems;on chip communication;mechanism design;chip multiprocessor on chip communication embedded heterogeneous multicore architecture main cooperation model	Much attention is now placed on the CMP (chip multiprocessor) architecture design while one important issue in this domain is the on-chip communication mechanism. The classic design of communication mechanism in embedded heterogeneous multi-core processor only satisfies its basic communication requirement and will cost a lot of additional communication. This paper proposes a novel communication mechanism called 'Main-Cooperation' model whose kernel component controls all the on-chip communication. The experimentation result shows that our model is 23% better than the classic one in the domain.	embedded system;intel core (microarchitecture);multi-core processor;multiprocessing	Like Yan;Qingsong Shi;Tianzhou Chen;Guobing Chen	2008	2008 IEEE International Conference on Networking, Sensing and Control	10.1109/ICNSC.2008.4525524	chip;multi-core processor;mechanism design;embedded system;computer architecture;parallel computing;real-time computing;telecommunications;computer science;operating system;satisfiability	Robotics	2.1077988357103163	54.42632206748911	17887
fde7eec2b0d0d8d33c766e370631fd996ef808d3	hot chips 2014 nvidia's denver processor	energy efficiency;pipeline microarchitecture nvidia denver processor system design system architecture processing capabilities superscalar architecture;androids;prefetching;pipeline processing multiprocessing systems parallel architectures;humanoid robots;ecosystems;graphics processing units;prefetching androids humanoid robots energy efficiency graphics processing units program processors ecosystems;program processors	This article consists of a collection of slides from the author's conference presentation on the special features, system design and architectures, processing capabilities, and targeted markets for NVIDIA's Denver family of processors.	central processing unit;hot chips;systems design	Darrell Boggs;Gary Brown;Bill Rozas;Nathan Tuck;K. S. Venkatraman	2014	2014 IEEE Hot Chips 26 Symposium (HCS)	10.1109/HOTCHIPS.2014.7478811	computer architecture;parallel computing;computer hardware;computer science	EDA	4.2628419253794245	49.026702455941255	17890
e0035aac3974a9c57d294a06ffcc9bf72ff2ef1d	cache modeling and optimization for portable devices running mpeg-4 video decoder	cache optimization;portable devices;consumer demand;cache memory;multimedia application;satisfiability;system performance;video decoder;mpeg 4;memory systems;digital signal processor;cache modeling;simulation tool;memory bandwidth	There are increasing demands on portable communication devices to run multimedia applications. ISO (an International Organization for Standardization) standard MPEG-4 is an important and demanding multimedia application. To satisfy the growing consumer demands, more functions are added to support MPEG-4 video applications. With improved CPU speed, memory sub-system deficiency is the major barrier to improving the system performance. Studies show that there is sufficient reuse of values for caching that significantly reduce the memory bandwidth requirement for video data. Software decoding of MPEG-4 video data generates much more cache-memory traffic than required. Proper understanding of the decoding algorithm and the composition of its data set is obvious to improve the performance of such a system. The focus of this paper is cache modeling and optimization for portable communication devices running MPEG-4 video decoding algorithm. The architecture we simulate includes a digital signal processor (DSP) for running the MPEG-4 decoding algorithm and a memory system with two levels of caches. We use VisualSim and Cachegrind simulation tools to optimize cache sizes, levels of associativity, and cache levels for a portable device decoding MPEG-4 video.	armulator;angular defect;cpu cache;cache (computing);central processing unit;cooley–tukey fft algorithm;digital signal processor;locus;lock (computer science);mathematical optimization;memory bandwidth;mobile device;personal digital assistant;signal processing;simulation;tracing (software);video decoder	Abu Asaduzzaman;Imad Mahgoub	2006	Multimedia Tools and Applications	10.1007/s11042-006-6145-y	bus sniffing;pipeline burst cache;digital signal processor;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;page cache;cpu cache;computer hardware;cache;computer science;write-once;cache invalidation;operating system;computer performance;smart cache;cache algorithms;cache pollution;mpeg-4;memory bandwidth;cache-only memory architecture;non-uniform memory access;satisfiability	Metrics	9.274462719269915	40.336567822340704	17895
885111f1e7faa374ca22e998be5a5a4c25905887	a hardware architecture proposal for the enhanced karnik-mendel algorithm based on sequential arithmetic operators	enhanced karnik mendel algorithm;field programmable gate arrays digital arithmetic;uncertainty;cordic;fuzzy sets;hardware architectural proposals;hardware architecture;fuzzy logic;fpga technology;sequential arithmetic operators;digital arithmetic;sequential online arithmetic;field programmable gate arrays;fpga technology enhanced karnik mendel algorithm sequential arithmetic operators hardware architectural proposals sequential online arithmetic cordic;proposals;algorithm design and analysis;proposals hardware fuzzy sets fuzzy logic algorithm design and analysis uncertainty field programmable gate arrays;hardware	This article presents two hardware architectural proposals for the Enhanced Karnik-Mendel algorithm. The first proposal takes advantage of sequential online arithmetic, while the latter includes operators based on the CORDIC algorithm. Computing performance is estimated for both proposals over FPGA technology. Results show that the CORDIC based approach could achieve the best indices in both simplicity and speed.	algorithm;cordic;field-programmable gate array	Miguel A. Melgarejo;L. Karina Duran	2010	International Conference on Fuzzy Systems	10.1109/FUZZY.2010.5584370	fuzzy logic;algorithm design;computer architecture;uncertainty;computer science;artificial intelligence;theoretical computer science;hardware architecture;fuzzy set;algorithm;field-programmable gate array;cordic	EDA	10.638568846853023	46.921516305727565	17918
55f48e55aa655d0d800518ac75710f2acf1a3ebb	improving the accuracy of defect diagnosis with multiple sets of candidate faults	circuit faults;transition faults candidate faults defect diagnosis failure analysis testing;candidate faults;testing;transition faults;defect diagnosis;failure analysis;accuracy;computational modeling;circuit faults accuracy failure analysis fault diagnosis benchmark testing computational modeling integrated circuit modeling;set theory benchmark testing fault diagnosis;integrated circuit modeling;defect diagnosis procedure candidate faults confidence levels faulty output response;benchmark testing;fault diagnosis	Given a chip that produced a faulty output response to a test set, a defect diagnosis procedure produces a set of candidate faults that is expected to identify the defects that are present in the chip. The accuracy of the set of candidate faults is higher when the set is smaller or when its overlap with the defects that are present in the chip is larger. To increase the accuracy of a set of candidate faults, this paper describes an approach where several sets of candidate faults are computed based on different subsets of the test set. The subsets are obtained by removing small numbers of tests from the complete test set. The result is sets of candidate faults that are similar but not identical. The number of sets where a fault appears yields a confidence level that the fault actually belongs in a set of candidate faults. New sets of candidate faults are defined based on the confidence levels obtained. The smallest set of candidate faults can be used as the final result of defect diagnosis, or the sets can be used for ranking the candidates. Experimental results for benchmark circuits demonstrate the effectiveness of this approach.	benchmark (computing);software bug;test set	Irith Pomeranz	2016	IEEE Transactions on Computers	10.1109/TC.2015.2468234	benchmark;failure analysis;real-time computing;computer science;accuracy and precision;software testing;computational model;algorithm;statistics	SE	22.267804551791407	51.39827162952103	17920
0704bb38aaeff2a1ccde7bcfc754c56fd6b7e9b4	efficient ddd-based term generation algorithm for analog circuit behavioral modeling	analog circuits heuristic algorithms dynamic programming system on a chip circuit testing transfer functions character generation system analysis and design performance gain circuit stability;shortest path;decision diagrams;incremental placement;generic algorithm;transfer functions;behavior modeling;dynamic program;graphs;analog circuits;circuit simulation;analogue integrated circuits;integrated circuit modelling;transfer function;graphs integrated circuit modelling circuit simulation analogue integrated circuits decision diagrams determinants transfer functions;congestion;standard cell;symbolic term generation symbolic analysis ddd based term generation algorithm analog circuit behavioral modeling symbolic product terms linear analog circuits compact determinant decision diagram representation transfer functions analog circuit characteristics ddd graph dominant term shortest path search graph vertices;determinant decision diagram;determinants	An efficient approach to generating symbolic product terms for behavioral modeling of large linear analog circuits is presented. The approach is based on a compact determinant decision diagram (DDD) representation of transfer functions and characteristics of analog circuits. The new algorithm is based on the concept that a dominant term in a DDD graph can be found by searching the shortest path in the graph. But instead of traversing a whole DDD graph each time, we show that a shortest path can be found by just updating a small number of the newly added vertices after the first shortest path is found. Experimental results indicate that the new symbolic term generation algorithm outperforms both pure shortest path based algorithm and dynamic programming based algorithm, which is the fastest symbolic term generation algorithm published so far.	algorithm;analogue electronics;behavioral modeling;dynamic programming;fastest;influence diagram;shortest path problem;transfer function	Sheldon X.-D. Tan;C.-J. Richard Shi	2003		10.1145/1119772.1119947	suurballe's algorithm;constrained shortest path first;computer science;pathfinding;theoretical computer science;machine learning;yen's algorithm;mathematics;transfer function;shortest path problem;k shortest path routing;shortest path faster algorithm;algorithm	EDA	16.826223566812644	49.1148608824867	17970
eeaebc0c9ff72a1f2cfd6598b7026f3b606db011	fully distributed state estimation of smart grids	distributed system;smart power grids matrix algebra message passing power system state estimation;matrix algebra;state estimation;smart power grid;sparse matrices fully distributed state estimation intelligent electronic devices smart power grids hierarchical multilevel state estimation fully distributed computational network architecture message passing algorithms electric power system state estimation algebraic operations;smart power grids;sensors vectors computer architecture communication networks smart grids software particle separators;power system state estimation;message passing;distributed system state estimation smart power grid	A large number of sensors, meters and intelligent electronic devices (IEDs) with the capabilities of sensing, actuation, computation and communication will be deployed in future smart power grids for the purpose of measurement, monitoring, protection, diagnosis, control, optimization and other transactions. The conventional centralized and hierarchical multi-level state estimation is not scalable enough to process the huge amount of data generated all over the grid. In this paper we propose a fully distributed computational network architecture and the associated Message-Passing (MP) algorithms for electric power system state estimation. Unlike the conventional state estimation schemes that centered around algebraic operations on sparse matrices, our approach is based on MP and information fusing on graphs in a totally distributed fashion. The optimality, scalability and other advantages will be demonstrated.	algorithm;artificial neural network;centralized computing;computation;encapsulated postscript;internet;jt (visualization format);linear algebra;mathematical optimization;message passing;network architecture;portable document format;scalability;sensor;sparse matrix;telecommunications network	Yunxin Li	2012	2012 IEEE International Conference on Communications (ICC)	10.1109/ICC.2012.6364888	embedded system;message passing;computer science;theoretical computer science;distributed computing	Robotics	6.099278591928337	35.61967726978816	17993
c1eb91c5945f3d23096680a104551eced3872540	parallelization of path planning algorithms for auvs concepts, opportunities, and program-technical implementation	power consumption autonomous underwater vehicles battery powered vehicles computerised instrumentation energy measurement microprocessor chips multiprocessing systems parallel algorithms parallel programming path planning performance evaluation power aware computing;path planning message passing sea surface vehicles parallel processing planning;performance evaluation;teledyne webb research slocum glider path planning algorithm parallelization program technical implementation autonomous underwater vehicles advanced sensing capabilities data storage post mission inspection energy aware many core computing architecture on board path planning battery operated auv 48 core single chip system energy consumption power consumption performance measurement system parameters computation intensive tasks scc system;path planning;single chip cloud computer;auv slocum glider;parallel programming;power aware computing;energy measurement;autonomous underwater vehicles;graph methods;time varying environment;multiprocessing systems;computerised instrumentation;power consumption;time varying environment graph methods auv slocum glider parallel programming single chip cloud computer path planning;battery powered vehicles;microprocessor chips;parallel algorithms	Modern autonomous underwater vehicles (AUVs) have advanced sensing capabilities including sonar, cameras, acoustic communication, and diverse bio-sensors. Instead of just sensing its environment and storing the data for post-mission inspection, an AUV could use the collected information to gain an understanding of its environment, and based on this understanding autonomously adapt its behavior to enhance the overall effectiveness of its mission. Many such tasks are highly computation intensive. This paper presents the results of a case study that illustrates the effectiveness of an energy-aware, many-core computing architecture to perform on-board path planning within a battery-operated AUV. A previously published path planning algorithm was ported onto the SCC, an experimental 48 core single-chip system developed by Intel. The performance, power, and energy consumption of the application were measured for different numbers of cores and other system parameters. This case study shows that computation intensive tasks can be executed within an AUV that relies mainly on battery power. Future plans include the deployment and testing of an SCC system within a Teledyne Webb Research Slocum glider.	acoustic cryptanalysis;algorithm;automated planning and scheduling;automatic parallelization;autonomous robot;british informatics olympiad;computation;computer architecture;general slocum;glider (conway's life);konami scc;manycore processor;motion planning;on-board data handling;parallel computing;sonar (symantec);sensor;software deployment;source code control system	Mike Eichhorn;Hans C. Woithe;Ulrich Kremer	2012	2012 Oceans - Yeosu	10.1109/OCEANS-Yeosu.2012.6263557	embedded system;real-time computing;simulation;engineering	Robotics	0.5241607718712282	46.376933716561595	18065
52476dba24636d8aa87854e518c0e169277a98fe	designing configurable, modifiable and reusable components for simulation of multicore systems	cache storage;reconfigurable architectures;software engineering cache storage multiprocessing systems parallel architectures quality control reconfigurable architectures;software engineering;coherence cache model reusable components modifiable components configurable components multicore architecture simulation system component models software design considerations parallel simulation framework mcp cache;multicore;parallel architectures;modifiable components;multiprocessing systems;software design;quality control;software reuse parallel simulation multicore modifiable components software design;software reuse;parallel simulation	A simulation system for modern multicore architectures is composed of various component models. For such a system to be useful for research purposes, modifiability is a key quality attribute. Users, when building a simulation model, need to have the capability to adjust various aspects of a component, or even replace a component with another of the same type. Software design considerations can determine whether or not a simulation system is successful in providing such capabilities. This paper presents a few design tactics that we adopt in creating configurable, modifiable, and reusable components for Manifold, our parallel simulation framework for multicore systems. The main example component is MCP-cache, a coherence cache model. The ideas behind the tactics are general enough and should be useful to designers of similar systems.	multi-core processor;simulation;software design	Jesse G. Beu;Sudhakar Yalamanchili;Tom Conte	2012	2012 SC Companion: High Performance Computing, Networking Storage and Analysis	10.1109/SC.Companion.2012.67	multi-core processor;quality control;computer architecture;parallel computing;real-time computing;computer science;software design;component-based software engineering;operating system	HPC	-3.15526494383995	49.48922834827662	18083
1877a239b623249a57adaaabf2085bc97f19aed6	energy-efficient soft real-time cpu scheduling for mobile multimedia systems	gestion energia;cpu scheduling;sistema operativo;performance guarantee;evaluation performance;mobile multimedia;informatique mobile;calculateur embarque;multimedia;mobile device;performance evaluation;energy efficient;real time;evaluacion prestacion;dynamic voltage scaling;mobile computer;multimedia application;soft real time;qualite service;gestion energie;operating system;scheduling;probability distribution;temps reel;power management;boarded computer;tiempo real;systeme exploitation;stochastic scheduling;quality of service;mobile computing;voltage scaling;calculador embarque;service quality;ordonnancement;reglamento;energy management;calidad servicio	This paper presents GRACE-OS, an energy-efficient soft real-time CPU scheduler for mobile devices that primarily run multimedia applications. The major goal of GRACE-OS is to support application quality of service and save energy. To achieve this goal, GRACE-OS integrates dynamic voltage scaling into soft real-time scheduling and decides how fast to execute applications in addition to when and how long to execute them. GRACE-OS makes such scheduling decisions based on the probability distribution of application cycle demands, and obtains the demand distribution via online profiling and estimation. We have implemented GRACE-OS in the Linux kernel and evaluated it on an HP laptop with a variable-speed CPU and multimedia codecs. Our experimental results show that (1) the demand distribution of the studied codecs is stable or changes smoothly. This stability implies that it is feasible to perform stochastic scheduling and voltage scaling with low overhead; (2) GRACE-OS delivers soft performance guarantees by bounding the deadline miss ratio under application-specific requirements; and (3) GRACE-OS reduces CPU idle time and spends more busy time in lower-power speeds. Our measurement indicates that compared to deterministic scheduling and voltage scaling, GRACE-OS saves energy by 7% to 72% while delivering statistical performance guarantees.	central processing unit;codec;dynamic voltage scaling;image scaling;laptop;linux;mobile device;operating system;overhead (computing);quality of service;real-time clock;real-time computing;requirement;scheduling (computing);smoothing	Wanghong Yuan;Klara Nahrstedt	2003		10.1145/945445.945460	bogomips;fair-share scheduling;embedded system;real-time computing;dynamic priority scheduling;computer science;operating system;mobile computing;scheduling;computer security;cpu shielding	OS	-4.243232010432741	59.90591764436197	18172
456445cf3301b9064d86a4834ca346a5354bc831	hybrid soft error detection by means of infrastructure ip cores [soc implementation]	intellectual property;soft error	High integration levels, coupled with the increased sensitivity to soft errors even at ground level, make the task of guaranteeing adequate dependability levels more difficult then ever. In this paper, we propose to adopt low-cost infrastructure-intellectual-property (I-IP) cores in conjunction with software-based techniques to perform soft error detection. Experimental results are reported that show the effectiveness of the proposed approach.	algorithm;benchmark (computing);dependability;error detection and correction;experiment;fault detection and isolation;fault injection;height above ground level;intel mcs-51;overhead (computing);prototype;semiconductor intellectual property core;soft error;system on a chip	Letícia Maria Veiras Bolzani;Maurizio Rebaudengo;Matteo Sonza Reorda;Fabian Vargas;Massimo Violante	2004	Proceedings. 10th IEEE International On-Line Testing Symposium	10.1109/IOLTS.2004.25	reliability engineering;embedded system;real-time computing;soft error;computer science;engineering;intellectual property	Embedded	7.031543204200612	59.06285013796077	18276
060afdfef6f56897e50d8659a28e76197d2bcc4d	universal strong encryption fpga core implementation	cryptography;field programmable gate arrays;logic design;2.8 mbit/s;fpga core implementation;idea algorithm;round unicorn architecture;round module;xilinx fpga;data security;design cycle reduction;single core module;symmetric block cipher;system design methodology;universal strong encryption	"""IDEA is a symmetric block cipher with a 128-bit key proposed to replace DES where a strong encryption is required. Many applications need speed of a hardware encryption implementation while trying to preserve flexibility and low cost of a software implementation. In this paper we have presented one solution of this problem. Our system architecture uses single core module named Round to implement IDEA algorithm. Using the core we were able to implement and test example application in only three days. This """"of the shelf"""" solution for designing cryptographic application using IDEA algorithm significantly reduced design cycle, thus greatly reducing time-to-market and cost of such designs. By increasing the number of the round modules system designer can linearly increase speed of the design. This system design methodology makes it possible to achieve necessary performance, or to preserve area (and reduce costs) when needed unlike other known approaches. We have implemented one round UNICORN architecture in Xilinx FPGA. After implementation the chip has been tested using the standard test vectors and it was capable of performing 2.8Mbps encryption in both ECB and CBC mode."""	128-bit;algorithm;block cipher mode of operation;computer-aided design;encryption;europe card bus;field-programmable gate array;strong cryptography;systems architecture;systems design	Davor Runje;Mario Kovac	1998			embedded system;parallel computing;40-bit encryption;computer science;theoretical computer science;field-programmable gate array;systems architecture	Arch	8.665213771617696	44.941276124527896	18287
4bf9a81729c79d12f00bbd5b3d73d38a36f0d863	very fast and compact fixed template cnn realizations for b/w processing	electronic circuits;cell time constant;detectors;time constant;building block;very large scale integration;connected component detector;cellular neural nets;charge coupled devices;digital building blocks;application specific integrated circuits cellular neural nets vlsi neural chips integrated circuit design;integrated circuit design;neural chips;indium tin oxide;guidelines;cell time constant fixed template cnn realizations b w processing digital building blocks connected component detector hole filler operation speed;hole filler;application specific integrated circuits;b w processing;fixed template cnn realizations;vlsi;operation speed;cellular neural networks charge coupled devices electronic circuits laboratories hardware detectors very large scale integration application specific integrated circuits indium tin oxide guidelines;cellular neural networks;connected component;hardware	In this paper a new approach is given for realizing fixed template CNN hardware. The ]proposed building blocks are digital and therefore no accuracy considerations are needed. This method is suitable for a certain type of CNN operation only. The limitations are discussed in the text. Two design examples are given, namely for the Connected Componenet Detector and for the Hole Filler. In the simulations the circuits show very fast operation :speed with the cell time constant well below Ins.	simulation;star filler	Ari Paasio;Asko Kananen;Kari Halonen	1999		10.1109/ISCAS.1999.777642	embedded system;electronic engineering;computer science;engineering;electrical engineering;theoretical computer science;very-large-scale integration	EDA	13.21597963591325	47.892490467927686	18364
0e27fff4d83820c986116d35ac91d4832698e2f2	design and test of energy-efficient, high-performance, and secure computing technologies via accelerators		This issue of IEEE Designu0026Test is focused on accelerators tailored to the specific requirements of data centers. The guest editors, Mustafa Ozdal, Gi-Joon Nam, and Debbie Marr, argue that existing CPU and GPU platforms may not be the most efficient options for certain compute patterns, which led to a new breed of emerging accelerator platforms that are introduced in this special issue. My thanks to the guest editors and authors for presenting the newest trends. Accompanied in this special issue is a survey by Mustafa Ozdal, who presents us with a comprehensive view with a focus on commercial platforms.		Jörg Henkel	2018	IEEE Design & Test	10.1109/MDAT.2017.2785268	computer engineering;efficient energy use;computer science	EDA	-2.0935115299147404	46.21059909695891	18373
836cfce77ab6bc33e013ed985a66303bfa7432b4	8.8 irazor: 3-transistor current-based error detection and correction in an arm cortex-r4 processor	registers clocks delays transistors latches;clocks;registers;transistors;edac approach irazor 3 transistor current based error detection and correction arm cortex r4 processor technology scaling process voltage temperature aging margins socs large timing margins register designs;latches;transistor circuits error correction error detection microcontrollers system on chip;delays	It is well known that technology scaling has led to increasing process/voltage/temperature/aging margins that substantially degrade performance and power in modern processors and SoCs. One approach to address these large timing margins is the use of specialized registers on critical paths that perform error detection and correction (EDAC) [1-5]. While promising, the previously proposed implementations have been limited in several ways. Most notably, they often incur large overheads beyond conventional register designs (e.g., 8-to-44 additional transistors per register). This becomes an obstacle for commercial designs and, hence, there have been no reported implementations of EDAC approaches within substantial commercial processors. Finally, the performance gain from EDAC approaches has not been thoroughly quantified in relation to competing, lower overhead approaches such as frequency binning and canary circuits/critical path monitors [6].	arm architecture;central processing unit;critical path method;error detection and correction;image scaling;integrated circuit;overhead (computing);product binning;system on a chip;transistor	Yiqun Zhang;Mahmood Khayatzadeh;Kaiyuan Yang;Mehdi Saligane;Nathaniel Ross Pinckney;Massimo Alioto;David Blaauw;Dennis Sylvester	2016	2016 IEEE International Solid-State Circuits Conference (ISSCC)	10.1109/ISSCC.2016.7417956	embedded system;electronic engineering;real-time computing;computer science;operating system;processor register;transistor	Arch	6.728666196411695	59.70301743474431	18374
457d1412a0d0a96b51703a21b042c1eacca27164	testing of non-feedback bridging faults	circuito combinatorio;probability;deteccion;circuit vlsi;detection;calculo automatico;computing;algorithme;calcul automatique;combinatory circuit;algorithm;feedback;vlsi circuit;bridging;entretoise;circuit combinatoire;probabilidad;probabilite;circuito vlsi;boucle reaction;arriostramiento;retroalimentacion;algoritmo	Deterministic and random testing of non-feedback bridging faults is considered. Deterministic testing involves the computation of test vectors. Test generation algorithms for non-feedback bridging faults, involving only two lines, is discussed in [1]. We show that the algorithms in [9,11,12,15] for computing test vectors for stuck-at faults can be used for computing test vectors for faults which are a combination of stuck-at and non-feedback bridging faults. The non-feedback faults may consist of a short among more than just two lines. In order to determine the random test length, the detectability profile of the circuit with respect to a given fault list is required [10,21]. This motivates the need for computing the detection probability of faults. Algorithms for computing the detection probability of non-feedback faults has not been discussed. We present an algorithm for computing the detection probability of non-feedback bridging faults. It is shown that this algorithm can also be used for computing the detection probability of faults which are a combination of stuck-at and non-feedback bridging faults.	algorithm;bridging (networking);computation;feedback;random testing;software testing	Sreejit Chakravarty	1990	Integration	10.1016/0167-9260(90)90031-U	computing;real-time computing;bridging;computer science;theoretical computer science;operating system;probability;feedback;algorithm	SE	21.282261696755988	49.344907895079075	18403
8e91a1327679ec0b992ac23aa752dca3e538c9af	a pure-cmos nonvolatile multi-context configuration memory for dynamically reconfigurable fpgas	nonvolatile memory transistors field programmable gate arrays channel hot electron injection arrays context programming;size 0 18 mum cmos nonvolatile multi context configuration memory fpga computational resource channel hot electron injection context switching sram word length 16 bit;sram chips cmos memory circuits field programmable gate arrays logic design random access storage	Multi-context configuration memory stores multiple sets of configuration data and changes the entire configuration of FPGA quickly, enabling enhancement of hardware utilization with dynamic reconfiguration architectures. The memory area for one set of configuration data should be much smaller than the computational resource it controls. In this paper, we propose a pure-CMOS, nonvolatile, and small-footprint multi-context configuration memory. The multi-context memory includes multiple 2Tr nonvolatile memory elements, which are programmed by channel hot-electron injection, and allows context switching in a single clock cycle. A primitive dynamically reconfigurable device having a lookup table and minimum interconnect backed by 16-bit 8-context configuration memory was fabricated by a 0.18 um CMOS process and its functionality was demonstrated. The 2Tr nonvolatile memory element is more than 4 times denser than 6Tr SRAM, enabling achievement of greater logic density. The pure-CMOS and nonvolatile features would enhance the attractiveness of the technology in many applications.	16-bit;cmos;clock signal;computation;computational resource;context switch;electron;embedded system;experiment;field-programmable gate array;hot-carrier injection;lookup table;non-volatile memory;nonvolatile bios memory;reconfigurability;static random-access memory;system on a chip;unified model	Kosuke Tatsumura;Masato Oda;Shinichi Yasuda	2014	2014 International Conference on Field-Programmable Technology (FPT)	10.1109/FPT.2014.7082778	nonvolatile bios memory;uniform memory access;interleaved memory;semiconductor memory;parallel computing;dynamic random-access memory;nano-ram;sense amplifier;non-volatile memory;memory refresh;computer hardware;computer science;bubble memory;computer memory;non-volatile random-access memory;conventional memory;extended memory;registered memory;computing with memory	EDA	15.353610211976568	59.27579697684702	18430
7822d3e06b1e7aa4cbbfe2a43444b3bbd77b39eb	parallelized benchmark-driven performance evaluation of smps and tiled multi-core architectures for embedded systems	performance evaluation multi core embedded systems;performance evaluation;floating point computation parallelized benchmark driven performance evaluation smp tiled multicore architecture embedded system moore law single core architecture distributed system realtime system reliability constrained system embedded domain information fusion gaussian elimination execution time symmetric multiprocessors tilepro64 tma intel based smp runtime metric speedup metric efficiency metric cost metric scalability metric performance per watt metric;embedded systems;multiprocessing systems;sensor fusion;parallel processing;multicore processing benchmark testing tiles embedded systems measurement scalability;sensor fusion embedded systems multiprocessing systems parallel processing performance evaluation	With Moore's law supplying billions of transistors on-chip, embedded systems are undergoing a transition from single-core to multi-core to exploit this high transistor density for high performance. However, there exists a plethora of multi-core architectures and the suitability of these multi-core architectures for different embedded domains (e.g., distributed, real-time, reliability-constrained) requires investigation. Despite the diversity of embedded domains, one of the critical applications in many embedded domains (especially distributed embedded domains) is information fusion. Furthermore, many other applications consist of various kernels, such as Gaussian elimination (used in network coding), that dominate the execution time. In this paper, we evaluate two embedded systems multi-core architectural paradigms: symmetric multiprocessors (SMPs) and tiled multi-core architectures (TMAs). We base our evaluation on a parallelized information fusion application and benchmarks that are used as building blocks in applications for SMPs and TMAs. We compare and analyze the performance of an Intel-based SMP and Tilera's TILEPro64 TMA based on our parallelized benchmarks for the following performance metrics: runtime, speedup, efficiency, cost, scalability, and performance per watt. Results reveal that TMAs are more suitable for applications requiring integer manipulation of data with little communication between the parallelized tasks (e.g., information fusion) whereas SMPs are more suitable for applications with floating point computations and a large amount of communication between processor cores.	benchmark (computing);computation;convolution;data compression;embarrassingly parallel;embedded system;expectation propagation;fast fourier transform;field-programmability;field-programmable gate array;floating-point unit;gaussian elimination;image processing;information privacy;linear network coding;locality of reference;moore's law;multi-core processor;parallel computing;performance evaluation;performance per watt;real-time clock;run time (program lifecycle phase);scalability;signal processing;single-core;speedup;symmetric multiprocessing;tilepro64;tower mounted amplifier;transistor	Arslan Munir;Ann Gordon-Ross;Sanjay Ranka	2012	2012 IEEE 31st International Performance Computing and Communications Conference (IPCCC)	10.1109/PCCC.2012.6407785	embedded system;parallel processing;parallel computing;real-time computing;computer science;operating system;sensor fusion	HPC	-2.25368008480657	47.82810964766383	18452
334b5bd58f2a03bdbd94863d857ec6c08f6c58cd	a reconfigurable architecture with sequential logic-based stochastic computing		Computations based on stochastic bit streams have several advantages compared to deterministic binary radix computations, including low power consumption, low hardware cost, high fault tolerance, and skew tolerance. To take advantage of this computing technique, previous work proposed a combinational logic-based reconfigurable architecture to perform complex arithmetic operations on stochastic streams of bits. The long execution time and the cost of converting between binary and stochastic representations, however, make the stochastic architectures less energy efficient than the deterministic binary implementations. This article introduces a methodology for synthesizing a given target function stochastically using finite-state machines (FSMs), and enhances and extends the reconfigurable architecture using sequential logic. Compared to the previous approach, the proposed reconfigurable architecture can save hardware area and energy consumption by up to 30% and 40%, respectively, while achieving a higher processing speed. Both stochastic reconfigurable architectures are much more tolerant of soft errors (bit flips) than the deterministic binary radix implementations, and their fault tolerance scales gracefully to very large numbers of errors.	approximation error;bitwise operation;combinational logic;computation;diagram;fault tolerance;finite-state machine;run time (program lifecycle phase);sequential logic;state transition table;stochastic computing	M. Hassan Najafi;Peng Li;David. J. Lilja;Weikang Qian;Kia Bazargan;Marc D. Riedel	2017	JETC	10.1145/3060537	architecture;fault tolerance;stochastic computing;computer science;streams;parallel computing;real-time computing;finite-state machine;sequential logic;binary number;combinational logic	Arch	8.943921903533928	41.26305848638332	18468
fd643c19ff89d9d6fa65b591528836e3020ec377	on the detectability of hardware trojans embedded in parallel multipliers		This paper presents a new method for analyzing the detectability of a hardware Trojan (HT) that has infected parallel multipliers. The proposed method handles the Rare Path Trojan (RPT) that exploits the difficulty in activating a specific path of a parallel multiplier. In this paper we analyze some typical multipliers from the viewpoints of RPT characteristics and insertion/detection possibility. The validity of our criteria is evaluated by multiple regression analysis. Our experimental result indicates that a multiplier based on a redundant binary tree has a larger detectability than other ones.	algorithm;binary tree;entry sequenced data set;hardware trojan;rp (complexity);redundant binary representation;trojan horse (computing)	Akira Ito;Rei Ueno;Naofumi Homma;Takafumi Aoki	2018	2018 IEEE 48th International Symposium on Multiple-Valued Logic (ISMVL)	10.1109/ISMVL.2018.00019	trojan;theoretical computer science;regression analysis;binary tree;computer science;discrete mathematics;hardware trojan;controllability;logic gate;exploit;monte carlo method	Embedded	22.6576762836746	49.639888237229265	18524
92a5331f7572ec7d661a5c22b71322c77a75c3b8	a scalable, serially-equivalent, high-quality parallel placement methodology suitable for modern multicore and gpu architectures	wire length quality scalable parallel placement methodology serially equivalent parallel placement methodology high quality parallel placement methodology modern multicore gpu architectures routing run times automated fpga design flow fpga architectures parallel tools simulated annealing parallel work net list nonconflicting swaps common parallel computing primitives stream compaction category reduction sort nvidia cuda platform post routing critical path delay;annealing;computer architecture;indexes;computational modeling;field programmable gate arrays annealing computational modeling instruction sets computer architecture educational institutions indexes;field programmable gate arrays;sorting field programmable gate arrays graphics processing units logic design multiprocessing systems network routing parallel architectures simulated annealing;instruction sets	Placement and routing run-times continue to dominate the automated FPGA design flow. As the size of FPGA architectures continue to grow exponentially, it remains critical to develop parallel tools for FPGA design where the amount of exposed concurrent work scales with the size of the designs to be synthesized. In this paper, we propose a novel algorithm for parallel placement, based on simulated annealing, where the amount of parallel work directly scales with the size of the net-list to be placed. Our approach concurrently evaluates and conditionally applies very large sets of non-conflicting swaps using common parallel computing primitives, including stream compaction, category reduction, and sort. While our design is suitable for targeting all modern parallel computing platforms, we present results from our implementation which targets NVIDIA's CUDA platform, where we achieve a mean speed-up of 19x over VPR with post-routing critical-path-delay and wire-length quality that matches or exceeds VPR. We believe that this work is an important step towards the development of a scalable, high-quality placement tool.	algorithm;analysis of algorithms;cuda;cilk plus;data compaction;field-programmable gate array;graphics processing unit;iteration;multi-core processor;paging;palette swap;parallel computing;place and route;problem domain;routing;scalability;simulated annealing;throughput;thrust	Christian Fobel;Gary William Grewal;Deborah Stacey	2014	2014 24th International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2014.6927481	database index;embedded system;computer architecture;parallel computing;annealing;computer science;theoretical computer science;operating system;instruction set;computational model;field-programmable gate array	EDA	0.6656771243824068	41.626520556110684	18548
677defdf3bd5fa8eaf8c8e10e2f48a3251eee314	source-to-source compilation of loop programs for manycore processors	thesis or dissertation	It is widely accepted today that the end of microprocessor performance growth based on increasing clock speeds and instruction-level parallelism (ILP) demands new ways of exploiting transistor densities. Manycore processors (most commonly known as GPGPUs or simply GPUs) provide a viable solution to this performance scaling bottleneck through large numbers of lightweight compute cores and memory hierarchies that rely primarily on software for their efficient utilization. The widespread proliferation of this class of architectures today is a clear indication that exposing and managing parallelism on a large scale as well as efficiently orchestrating on-chip data movement is becoming an increasingly critical concern for highperformance software development. In such a computing landscape performance portability – the ability to exploit the power of a variety of manycore chips while minimizing the impact on software development and productivity – is perhaps one of the most important and challenging objectives for our research community. This thesis is about performance portability for manycore processors and how source-to-source compilation can help us achieve it. In particular, we show that for an important set of loop-programs, performance portability is attainable at low cost through compile-time polyhedral analysis and optimization and parametric tiling for run-time performance tuning. In other words, we propose and evaluate a source-to-source compilation path that takes affine loop-programs as input and produces parametrically tiled parallel code amenable to run-time tuning across different manycore platforms and devices – a very useful and powerful property if we seek performance portability because it decouples the compiler from the performance tuning process. The produced code relies on a platform-independent run-time environment, called Avelas, that allows us to formulate a robust and portable code generation algorithm. Our experimental evaluation shows that Avelas induces low run-time overhead and even substantial speed-ups for wavefront-parallel programs compared to a state-of-the-art compile-time scheme with no run-time support. We also claim that the low overhead of Avelas is a strong indication that it can also be effective as a general-purpose programming model for manycore processors as we demonstrate for a set of ParBoil benchmarks.	algorithm;central processing unit;code generation (compiler);compile time;general-purpose computing on graphics processing units;general-purpose modeling;graphics processing unit;image scaling;instruction-level parallelism;manycore processor;mathematical optimization;memory hierarchy;microprocessor;overhead (computing);parallel computing;performance tuning;polyhedron;programming model;runtime system;software development;software portability;source-to-source compiler;tiling window manager;transistor	Athanasios Konstantinidis	2013			computer architecture;parallel computing;real-time computing;computer science	Arch	-4.455391885794759	48.904434745001396	18580
8c4647b0321da5f7808ba8e0f03151482d0fc934	multi-objective autotuning of mobilenets across the full software/hardware stack		We present a customizable Collective Knowledge workflow to study the execution time vs. accuracy trade-offs for the MobileNets CNN family. We use this workflow to evaluate MobileNets on Arm Cortex CPUs using TensorFlow and Arm Mali GPUs using several versions of the Arm Compute Library. Our optimizations for the Arm Bifrost GPU architecture reduce the execution time by 2--3 times, while lying on a Pareto-optimal frontier. We also highlight the challenge of maintaining the accuracy when deploying CNN models across diverse platforms. We make all the workflow components (models, programs, scripts, etc.) publicly available to encourage further exploration by the community.	arm architecture;auto-tune;central processing unit;graphics processing unit;mali (gpu);pareto efficiency;run time (program lifecycle phase);tensorflow	Anton Lokhmotov;Nikolay Chunosov;Flavio Vella;Grigori Fursin	2018		10.1145/3229762.3229767	parallel computing;architecture;arm architecture;software;workflow;computer science;scripting language	HPC	-2.4528629998963525	46.03089832738326	18596
0d91ddf192a32db6b21393f7a88e5e232d3c08d6	frequency and yield optimization using power gates in power-constrained designs	process variation;optimal method;power gating;satisfiability;yield;technology scaling;total power;leakage power;optimization;multicore processors;power gate;frequency	Manufactured dies exhibit a large spread of maximum frequency and leakage power due to process variations, which have been increasing with technology scaling. Reducing the spread is very important for maximizing the frequency and the yield of power-constrained designs, because otherwise many dies that do not satisfy frequency or power constraints would be discarded. In this paper, we propose two optimization methods to improve the maximum operating frequency and the yield using power gates that already exist in many power-constrained designs. In the first method, we consider the designs of multiple cores, where each of them can be independently power-gated. When each core shows different frequencies due to within-die variations, the strength of a power gate in each core is adjusted to make their maximum operating frequencies even. This allows faster cores to consume less active leakage power, reducing the total power consumption well below a power constraint in a globally-clocked design. We subsequently increase global supply voltage for higher overall frequency until the power constraint is satisfied. In our experiments assuming multicore processors with 2--16 cores, the maximum operating frequency was improved by 4-23%. In the second method, we take leaky-but-fast dies (which otherwise would be discarded) and adjust the strength of the power gates such that they can operate in an acceptable power and frequency region. The problem is extended to designs employing a frequency binning strategy, where we have an additional objective of maximizing the number of dies for higher frequency bins. In our experiments with ISCAS benchmark circuits, most discarded fast-but leaky dies were recovered using the second method.	active directory;benchmark (computing);central processing unit;clock rate;die (integrated circuit);experiment;image scaling;mathematical optimization;multi-core processor;product binning;spectral leakage	Nam Sung Kim;Jun Seomun;Abhishek A. Sinkar;Jungseob Lee;Tae Hee Han;Ken Choi;Youngsoo Shin	2009		10.1145/1594233.1594263	power gain;multi-core processor;yield;frequency scaling;electronic engineering;real-time computing;computer science;engineering;frequency;process variation;engineering drawing;quantum mechanics;satisfiability	EDA	17.049758796420896	55.40530307139	18617
4a2ed29d897b45dec824c6f587392499836ca693	applying tmr in hardware accelerators generated by high-level synthesis design flow for mitigating multiple bit upsets in sram-based fpgas		This paper investigates the use of Triple Modular Redundancy (TMR) in hardware accelerators designs described in C programming language and synthesized by High Level Synthesis (HLS). A setup composed of a soft-core processor and a matrix multiplication design protected by TMR and embedded into an SRAM-based FPGA was analyzed under accumulated bit-flips in its configuration memory bits. Different configurations using single and multiple input and output workload data streams were tested. Results show that by using a coarse grain TMR with triplicated inputs, voters, and outputs, it is possible to reach 95% of reliability by accumulating up to 61 bit-flips and 99% of reliability by accumulating up to 17 bit-flips in the configuration memory bits. These numbers imply in a Mean Time Between Failure (MTBF) of the coarse grain TMR at ground level from 50% to 70% higher than the MTBF of the unhardened version for the same reliability confidence.	design flow (eda);field-programmable gate array;high-level synthesis;static random-access memory;triple modular redundancy	André Flores dos Santos;Lucas Antunes Tambara;Fabio Benevenuti;Jorge Tonfat;Fernanda Gusmão de Lima Kastensmidt	2017		10.1007/978-3-319-56258-2_18	embedded system;parallel computing;real-time computing;computer science	EDA	7.170141291933601	58.613109512142735	18671
5f53fd7fb05b38f3adff397fb07c0806e8492071	low power aging-aware on-chip memory structure design by duty cycle balancing	negative bias temperature instability;cache;register file	The degradation of CMOS devices over the lifetime can cause severe threat to the system performance and reliability at deep submicron semiconductor technologies. The negative bias temperature instability (NBTI) is among the most important sources of the aging mechanisms. Applying the traditional guardbanding technique to address the decreased speed of devices is too costly. On-chip memory structures, such as register ̄les and on-chip caches, su®er a very high NBTI stress. In this paper, we propose the aging-aware design to combat the NBTIinduced aging in integer register ̄les, data caches and instruction caches in high-performance microprocessors. The proposed aging-aware design can mitigate the negative aging e®ects by balancing the duty cycle ratio of the internal bits in on-chip memory structures. Besides the aging problem, the power consumption is also one of the most prominent issues in microprocessor design. Therefore, we further propose to apply the low power schemes to di®erent memory structures under aging-aware design. The proposed low power aging-aware design can also achieve a signi ̄cant power reduction, which will further reduce the temperature and NBTI degradation of the on-chip memory structures. Our experimental results show that our aging-aware design can e®ectively reduce the NBTI stress with 30.8%, 64.5% and 72.0% power saving for the integer register ̄le, data cache and instruction cache, respectively.	cmos;cpu cache;duty cycle;elegant degradation;microprocessor;negative-bias temperature instability;processor design;semiconductor;very-large-scale integration	Shuai Wang;Tao Jin;Chuanlei Zheng;Guangshan Duan	2016	Journal of Circuits, Systems, and Computers	10.1142/S0218126616501152	embedded system;negative-bias temperature instability;parallel computing;real-time computing;computer hardware;cache;computer science;engineering;operating system;register file	EDA	6.766945090352001	60.19523995393244	18686
21c9522ff2e7d4669c516f46aa914e06c22fd8d2	a fully-pipelined hardware design for gaussian mixture models		Gaussian Mixture Models (GMMs) are widely used in many applications such as data mining, signal processing and computer vision, for probability density modeling and soft clustering. However, the parameters of a GMM need to be estimated from data by, for example, the Expectation-Maximization algorithm for Gaussian Mixture Models (EM-GMM), which is computationally demanding. This paper presents a novel design for the EM-GMM algorithm targeting reconfigurable platforms, with five main contributions. First, a pipeline-friendly EM-GMM with diagonal covariance matrices that can easily be mapped to hardware architectures. Second, a function evaluation unit for Gaussian probability density based on fixed-point arithmetic. Third, our approach is extended to support a wide range of dimensions or/and components by fitting multiple pieces of smaller dimensions onto an FPGA chip. Fourth, we derive a cost and performance model that estimates logic resources. Fifth, our dataflow design targeting the Maxeler MPC-X2000 with a Stratix-5SGSD8 FPGA can run over 200 times faster than a 6-core Xeon E5645 processor, and over 39 times faster than a Pascal TITAN-X GPU. Our design provides a practical solution to applications for training and explores better parameters for GMMs with hundreds of millions of high dimensional input instances, for low-latency and high-performance applications.	cluster analysis;computer vision;data mining;dataflow;expectation–maximization algorithm;field-programmable gate array;fixed-point arithmetic;google map maker;graphics processing unit;mixture model;signal processing	Conghui He;Haohuan Fu;Ce Guo;Wayne Luk;Guangwen Yang	2017	IEEE Transactions on Computers	10.1109/TC.2017.2712152	field-programmable gate array;mixture model;parallel computing;computer hardware;computer science;algorithm design;theoretical computer science;reconfigurable computing;expectation–maximization algorithm;signal processing;dataflow;gaussian	ML	0.3860172521920942	44.230639886562955	18689
00c720f1d81bbb456d42fe4ac9ff6dbb49a86d44	effective low power bist for datapaths	switching activity;carry lookahead adder;circuit under test;power saving;fault clustering;primitive polynomial;defect clustering;reject ration;low power;power dissipation;fault coverage;power reduction;defect level	Power in processing cores (microprocessors, DSPs) is primarily consumed in the datapath part. Among the datapath functional modules, multipliers consume the largest amount of power due to their size and complexity. We propose a low power BIST scheme for datapaths built around multiplieraccumulator pairs. The target is low average power dissipation between successive test vectors. This is achieved by taking advantage of the regularity of multiplier modules and achieving very high fault coverage by a linearsized test set with as small as possible input switching activity. The proposed BIST scheme is more efficient than pseudorandom BIST for the same high fault coverage target. Up to 77.25% power saving is achieved in the set of experimental results provided in the paper.	built-in self-test;cpu power dissipation;datapath;fault coverage;microprocessor;pseudorandomness;test set	Dimitris Gizopoulos;Nektarios Kranitis;Mihalis Psarakis;Antonis M. Paschalis;Yervant Zorian	2000		10.1145/343647.344345	electronic engineering;parallel computing;real-time computing;fault coverage;engineering;dissipation;primitive polynomial	EDA	19.53039723404025	53.074023556200785	18774
63c5c6b80a3dbc9f2c76b61958b09bbfefbcac72	combining instruction coding and scheduling to optimize energy in system-on-fpga	automatic control;system on fpga;processor architecture;instruction reordering;software measurement;embedded processor control;processor scheduling;dynamic power optimization;processor scheduling embedded systems field programmable gate arrays;design optimization;instruction coding;computer architecture;embedded systems;energy optimization;dynamic power optimization instruction coding scheduling energy optimization system on fpga instruction reordering embedded processor control hardware implementation;scheduling;power optimization;process control;field programmable gate arrays power generation computer architecture power measurement processor scheduling automatic control process control hardware software measurement design optimization;power generation;power reduction;field programmable gate arrays;high performance;embedded processor;hardware implementation;power measurement;hardware	In this paper, we investigate a combination of two techniques n struction coding and instruction re-ordering - for optimizing energy in embedded processor control. We present the first practical, hardware implementation incorporating both approaches as part of a novel flow for automatic power-optimization of an FPGA soft processor. Our infrastructure generates customized processors and associated software, to enable power optimizations to be evaluated on multiple architectures and FPGA platforms. We evaluate using both software estimates of power and actual measurements from both low-cost and high-performance FPGAs. We generate over 150 optimized processor designs for two FPGA platforms, two processor architectures and six different benchmarks at four different clock rates and achieve consistent measured dynamic power reduction of up to 74%, without performance cost. Our results are applicable beyond processor optimization, quantifying the benefits of practical switching reduction and highlighting non-obvious pitfalls and complexities in dynamic power optimization	ace;benchmark (computing);central processing unit;clock rate;compiler;computer hardware;cost estimation in software engineering;datapath;embedded system;experiment;field-programmable gate array;instruction scheduling;mathematical optimization;overhead (computing);power optimization (eda);scheduling (computing)	Robert G. Dimond;Oskar Mencer;Wayne Luk	2006	2006 14th Annual IEEE Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2006.31	electricity generation;embedded system;computer architecture;parallel computing;real-time computing;multidisciplinary design optimization;application-specific instruction-set processor;microarchitecture;computer science;automatic control;process control;software measurement;scheduling;power optimization;field-programmable gate array	Arch	0.7176796068139288	53.73502361783735	18883
a15be2d9ce798e4414f7eb7a43ab4931f6409724	an optimization mechanism for mid-bond testing of tsv-based 3d socs			optimization mechanism;program optimization;system on a chip;through-silicon via	Kele Shen;Zhigang Yu;Zhou Jiang	2016	IEICE Transactions		electronic engineering;engineering;bond	Robotics	12.457355214756333	54.37819268036865	18904
23f3a802af8493d6fd72d3034ab3bfa7345e9db1	practical aggregation of semantical program properties for machine learning based optimization	compiler construction;design optimization;statistical model;limit set;program optimization;optimization problem;feature vector;lines of code;general methods;machine learning;embedded system design;compilation;performance prediction	Iterative search combined with machine learning is a promising approach to design optimizing compilers harnessing the complexity of modern computing systems. While traversing a program optimization space, we collect characteristic feature vectors of the program, and use them to discover correlations across programs, target architectures, data sets, and performance. Predictive models can be derived from such correlations, effectively hiding the time-consuming feedback-directed optimization process from the application programmer.  One key task of this approach, naturally assigned to compiler experts, is to design relevant features and implement scalable feature extractors, including statistical models that filter the most relevant information from millions of lines of code. This new task turns out to be a very challenging and tedious one from a compiler construction perspective. So far, only a limited set of ad-hoc, largely syntactical features have been devised. Yet machine learning is only able to discover correlations from information it is fed with: it is critical to select topical program features for a given optimization problem in order for this approach to succeed.  We propose a general method for systematically generating numerical features from a program. This method puts no restrictions on how to logically and algebraically aggregate semantical properties into numerical features. We illustrate our method on the difficult problem of selecting the best possible combination of 88 available optimizations in GCC. We achieve 74% of the potential speedup obtained through iterative compilation on a wide range of benchmarks and four different general-purpose and embedded architectures. Our work is particularly relevant to embedded system designers willing to quickly adapt the optimization heuristics of a mainstream compiler to their custom ISA, microarchitecture, benchmark suite and workload. Our method has been integrated with the publicly released MILEPOST GCC [14].	aggregate data;aggregate function;benchmark (computing);embedded system;feature extraction;gnu compiler collection;general-purpose modeling;heuristic (computer science);hoc (programming language);iterative method;milepost gcc;machine learning;mathematical optimization;microarchitecture;numerical analysis;optimization problem;optimizing compiler;predictive modelling;profile-guided optimization;program optimization;programmer;scalability;source lines of code;speedup;statistical model	Mircea Namolaru;Albert Cohen;Grigori Fursin;Ayal Zaks;Ari Freund	2010		10.1145/1878921.1878951	limit set;optimization problem;statistical model;embedded system;single compilation unit;parallel computing;real-time computing;multidisciplinary design optimization;feature vector;profile-guided optimization;computer science;theoretical computer science;operating system;machine learning;compiler construction;program optimization;optimizing compiler;programming language;source lines of code	PL	-1.5229673312516088	50.95493080645597	18909
0498cc99f82c9d0542dd7109f0d55cda61fdebf2	exploiting ilp in page-based intelligent memory	paged storage;parallel architectures;reconfigurable architectures;active pages;vliw processor;instruction-level parallelism;intelligent memory system;reconfigurable logic	This study compares the speed, area, and power of different implementations of Active Pages [OCS98], an intelligent memory system which helps bridge the growing gap between processor and memory performance by associating simple functions with each page of data. Previous investigations have shown up to 1000X speedups using a block of reconfigurable logic to implement these functions next to each sub-array on a DRAM chip. In this study, we show that instruction-level parallelism, not hardware specialization, is the key to the previous success with reconfigurable logic. In order to demonstrate this fact, an Active Page implementation based upon a simplified VLIW processor was developed. Unlike conventional VLIW processors, power and area constraints lead to a design which has a small number of pipeline stages. Our results demonstrate that a four-wide VLIW processor attains comparable performance to that of pure FPGA logic but requires significantly less area and power.	central processing unit;dynamic random-access memory;field-programmable gate array;instruction-level parallelism;parallel computing;partial template specialization;reconfigurable computing;very long instruction word	Mark Oskin;Justin Hensley;Diana Franklin;Frederic T. Chong;Matthew K. Farrens;Aneet Chopra	1999			chip;computer architecture;parallel computing;real-time computing;telecommunications;computer science	Arch	0.17007368528197153	48.60774480390587	18964
9160d7ed198c4ac359fec22e470f99bf1e7b38b6	logic-dram co-design to exploit the efficient repair technique for stacked dram	random access memory;logic dram co design repair technique stacked dram three dimensional integration 3d logic dram integrated computing system energy efficiency defective dram cells die stacking small architectural modification redundant address remapping logic die memory repair potential yield loss power consumption overhead;power consumption overhead logic dram co design repair technique stacked dram three dimensional integration 3d logic dram integrated computing system energy efficiency defective dram cells die stacking small architectural modification redundant address remapping logic die memory repair potential yield loss;fuses;maintenance engineering;three dimensional integrated circuits dram chips integrated circuit design integrated logic circuits logic design;arrays;redundancy;random access memory maintenance engineering redundancy arrays fuses three dimensional displays stacking;stacking;three dimensional displays;yield memory repair redundancy 3d dram 3d integration;memory repair redundancy 3d dram 3d integration yield;dram chips integrated circuit design integrated logic circuits logic design three dimensional integrated circuits	Three-dimensional (3D) integration is promising to provide dramatic performance and energy efficiency improvement to 3D logic-DRAM integrated computing system, but also poses significant challenge to the yield. To address this challenge, this paper explores a way to leverage logic-DRAM co-design to reactivate unused spares and thereby enable the cost-efficient technique to repair 3D integration-induced defective DRAM cells after die stacking. In particular, we propose to make the DRAM array open its spares to off-chip access by a small architectural modification and further design the defective address comparison and redundant address remapping with an efficient architecture on logic die to achieve equivalent memory repair. Simulation results demonstrate that the proposed repair technique for stacked DRAM can significantly alleviate potential yield loss, with minimal area and power consumption overhead and negligible timing penalty.	cost efficiency;dynamic random-access memory;memory controller;overhead (computing);performance;simulation;stacking	Minjie Lv;Hongbin Sun;Qiwei Ren;Bing Yu;Jingmin Xin;Nanning Zheng	2015	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2015.2403031	fuse;maintenance engineering;electronic engineering;parallel computing;real-time computing;memory rank;cas latency;engineering;stacking;universal memory;redundancy	Arch	7.5572861055158596	60.400340039812704	18995
b2ce4d9ab514e5afc8808ba72d5f4120b3dd454d	robot control computation in microprocessor systems with multiple arithmetic processors using a modified df/ihs scheduling algorithm	computers;electronic circuits;microprocessors;eficacia sistema;multiprocessor scheduling;general and miscellaneous mathematics computing and information science;planificacion integral;sintesis control;multiprocessor;computerised control;multiprocessor systems;microelectronic circuits 990200 mathematics computers;efficient algorithm;simulation;integrated planning;performance systeme;mc68881 apus computerised control np complete problem microprocessor systems multiple arithmetic processors high performance robot controller off the shelf components depth first initial heuristic search;simulacion;robotics;mathematical logic;system performance;methode calcul;trajectories;algorithme;metodo calculo;calculating method;heuristic search;algorithm;scheduling algorithm;robot control;dynamics;synthese commande;array processors;robots;mechanics;dynamique inverse;computer codes;robotica;search problems computerised control multiprocessing systems robots;algorithms;optimization;search problems;robotique;multiprocessing systems;multiprocesador;high performance;off the shelf;control synthesis;ordonnancement;np complete problem;robot control microprocessors arithmetic modis scheduling algorithm processor scheduling kinematics job shop scheduling service robots feedback control;algoritmo;multiprocesseur	Abstruct-The problem of designing a high performance robot controller with multiple arithmetic processing units (APU’s) is addressed. One attractive feature about this controller is that a minimum number of special purpose hardware components are needed, and in fact off-the-shelf components can be used. In the controller described one main processor (MPU) schedules a number of APU’s to produce the computational throughput. In this design an efficient scheduling algorithm plays the most important role in the system performance. The depth first/initial heuristic search (DF/IHS) algorithm is an efficient algorithm that solves difficult nonpolynomial (NP)-complete problems of scheduling a set of particularly ordered computational tasks onto a multiprocessor system. When interprocessor communication overheads are appreciable, it is not very effective in providing a practical near-optimum schedule. It fails to consider the problem of contention for shared resources. A new multiprocessor scheduling algorithm, which minimizes the effects of overhead and by doing so it reduces the effect of contention, is presented. This scheduling algorithm is used to derive the operational instructions of the APU’s and the MPU for our multiple APU-based robot controller. Simulations show six Motorola MC 68881 APU’s can be used to generate the robotic control computations in approximately 2.5 ms. The control computations involve inverse dynamic calculations, forward kinematics, inverse kinematics, and trajectory computations.	algorithm;apu nahasapeemapetilon;central processing unit;computation;computer simulation;direction finding;forward kinematics;heuristic;inter-process communication;inverse kinematics;mpu-401;microprocessor;motorola 68881;multiprocessing;multiprocessor scheduling;overhead (computing);robot control;scheduling (computing);throughput	Shaheen Ahmad;Bo Li	1989	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.44032	robot;mathematical optimization;dynamics;electronic circuit;mathematical logic;parallel computing;real-time computing;multiprocessing;np-complete;computer science;artificial intelligence;trajectory;theoretical computer science;machine learning;control theory;mathematics;robot control;robotics;integrated business planning;scheduling;multiprocessor scheduling	Robotics	3.1511372512115146	38.225030143106785	19041
b04eb8ced0751b5a7af5307c1ad81430eb404603	entropy-based optimum test points selection for analog fault dictionary techniques	large scale analog systems;test point;circuit faults;minimum entropy methods integrated circuit testing fault diagnosis rough set theory;cost function;rough set theory;rough set entropy based optimum test points selection analog fault dictionary techniques analog fault diagnosis two dimensional integer coded dictionary ambiguity set minimum entropy index statistical experiments large scale analog systems;minimum entropy methods;indexing terms;polynomials;statistical experiments;fault dictionary;circuit simulation;large scale;analog fault diagnosis;analog fault dictionary techniques;two dimensional integer coded dictionary;indexation;dictionaries;integrated circuit testing;system testing;entropy based optimum test points selection;circuit testing;dictionaries circuit faults fault diagnosis circuit testing computational efficiency entropy circuit simulation polynomials cost function system testing;entropy;rough set;minimum entropy index;computational efficiency;ambiguity set;fault diagnosis	An efficient method to select an optimum set of test points for dictionary techniques in analog fault diagnosis is proposed. This is done by searching for the minimum of the entropy index based on the available test points. First, the two-dimensional integer-coded dictionary is constructed whose entries are measurements associated with faults and test points. The problem of optimum test points selection is, thus, transformed to the selection of the columns that isolate the rows of the dictionary. Then, the likelihood for a column to be chosen based on the size of its ambiguity set is evaluated using the minimum entropy index of test points. Finally, the test point with the minimum entropy index is selected to construct the optimum set of test points. The proposed entropy-based method to select a local minimum set of test points is polynomial bounded in computational cost. The comparison between the proposed method and other reported test points selection methods is carried out by statistical experiments. The results indicate that the proposed method more efficiently and more accurately finds the locally optimum set of test points and is practical for large scale analog systems.	algorithmic efficiency;brute-force search;column (database);computation;dictionary;experiment;maxima and minima;np-hardness;polynomial;simulation;statistical model;test point;test set	Janusz A. Starzyk;Dong Liu;Zhihong Liu;Dale E. Nelson;Jerzy Rutkowski	2004	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2004.827085	rough set;computer science;theoretical computer science;machine learning;mathematics;statistics	EDA	21.043537327730903	49.022516314479255	19048
bb087346edbd89897c114f634d65969eb31d93cf	a task parallel algorithm for computing the costs of all-pairs shortest paths on the cuda-compatible gpu	threaded code;graph theory;kernel;multi threading;paper;all pairs distance;parallel algorithm;approximation algorithms;parallel algorithms computational complexity computer graphic equipment graph theory iterative methods multi threading;all pairs shortest path;computer graphic equipment;gpu;scattering;compute unified device architecture;iterative algorithm;all pairs shortest path cost computing;acceleration;harish iterative algorithm;chip;cuda;arrays;iterative methods;computer architecture;development environment;apsp;path problems;computational complexity;synchronization;nvidia geforce 8800 gts;threaded code task parallel algorithm all pairs shortest path cost computing graphics processing unit gpu compute unified device architecture cuda harish iterative algorithm on chip memory;parallel algorithms concurrent computing costs iterative algorithms graphics parallel processing hardware distributed computing computer architecture field programmable gate arrays;single source shortest path;nvidia;task parallel algorithm;algorithms;computer science;graphics processing unit;on chip memory;graphics;acceleration all pairs shortest path cuda gpu;parallel algorithms	This paper proposes a fast method for computing the costs of all-pairs shortest paths (APSPs) on the graphics processing unit (GPU). The proposed method is implemented using compute unified device architecture (CUDA), which offers us a development environment for performing general-purpose computation on the GPU. Our method is based on Harish's iterative algorithm that computes the cost of the single-source shortest path (SSSP) for every source vertex. We present that exploiting task parallelism in the APSP problem allows us to efficiently use on-chip memory in the GPU, reducing the amount of data being transferred from relatively slower off-chip memory. Furthermore, our task parallel scheme is useful to exploit a higher parallelism, increasing the efficiency with highly threaded code. As a result, our method is 3.4--15 times faster than the prior method. Using on-chip memory, our method eliminates approximately 20% of data loads from off-chip memory.	cuda;computation;computer graphics;computer memory;general-purpose markup language;graphics processing unit;iterative method;multithreading (computer architecture);parallel algorithm;parallel computing;shared memory;shortest path problem;task parallelism;thread (computing);threaded code;vertex (geometry)	Tomohiro Okuyama;Fumihiko Ino;Kenichi Hagihara	2008	2008 IEEE International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2008.40	parallel computing;computer science;graph theory;theoretical computer science;distributed computing;parallel algorithm;iterative method	HPC	-2.9526943310942024	41.629498384949756	19074
4d70c3b7ff0ec716e100d16e279da15c99ac24a1	the future of flexible hw platform architectures panel discussion	on chip bus;cache;intellectual property;system on a chip;low power;estimation	Flexible hardware platform architectures have emerged as a competitor and alternative to specialized SOC designs. Important applications are mobile systems or multimedia devices. Design cost and design time reduction are the main benefits of such platforms while cost and power efficiency are the main concerns. Today, hardware platforms own a small but growing share of the embedded systems market in wireless and multimedia terminals, and telecommunication and automotive devices.	embedded system;flexible-fuel vehicle;performance per watt;system on a chip	Rolf Ernst;Grant Martin;Oz Levia;Pierre G. Paulin;Stamatis Vassiliadis;Kees A. Vissers	2000		10.1145/343647.343874	system on a chip;embedded system;estimation;parallel computing;cache;computer science;engineering;operating system;network on a chip;intellectual property;statistics	EDA	9.504843085458557	55.86902868444625	19111
3acd8042a8a9ec453b05a2884b31c15f50b682d0	a framework for testing core-based systems-on-a-chip	computer testing;finite state machines;logic testing;core-based;finite-state automata;low-overhead compact test solutions;single symbolic test;systems-on-a-chip;test architectures;testability analysis	Available techniques for testing core-based systems-on-a-chip (SOCs) do not provide a systematic means for synthesising low-overhead test architectures and compact test solutions. In this paper, we provide a comprehensive framework that generates low-overhead compact test solutions for SOCs. First, we develop a common ground for addressing issues such as core test requirements, core access and test hardware additions. For this purpose, we introduce finite-state automata for modeling tests, transparency modes and test hardware behavior. In many cases, the tests repeat a basic set of test actions for different test data which can again be modeled using finite-state automata. While earlier work can derive a single symbolic test for a module in a register-transfer level (RTL) circuit as a finite-state automation, this work extends the methodology to the system level, and, additionally contributes a satisfiability-based solution to the problem of applying a sequence of tests phased in time. This problem is known to be a bottleneck in testability analysis not only at the system level, but also at the RTL. Experimental results show that the system-level average area overhead for making SOCs testable with our method is only 4.4%, while achieving an average test application time reduction of 78.5% over recent approaches. At the same time, it provides 100% test coverage of the precomputed test sets/sequences of the embedded cores.	automata theory;embedded system;fault coverage;finite-state machine;overhead (computing);precomputation;register-transfer level;requirement;system on a chip;test data;transparency (graphic)	Srivaths Ravi;Ganesh Lakshminarayana;Niraj K. Jha	1999	1999 IEEE/ACM International Conference on Computer-Aided Design. Digest of Technical Papers (Cat. No.99CH37051)		system on a chip;embedded system;electronic engineering;real-time computing;computer science;krylov subspace;automatic test pattern generation;test compression;lyapunov equation;code coverage;finite-state machine;register-transfer level;algorithm;satisfiability	EDA	8.028898872435793	52.834181230263006	19112
647487629b3a15c31b171343fdb294f3c576b704	low-power 32bit×32bit multiplier design with pipelined block-wise shutdown	block design;economies d energie;multiplier;haute performance;power line;leakage current;ahorros energia;reseau electrique;electrical network;power efficiency;red electrica;distributed computing;turn off;shutdown;clock gating;function block;plan bloc;power supply;plan bloque;parada;low power;multiplicateur;alimentation electrique;energy consumption;puissance faible;consommation energie;alto rendimiento;calculo repartido;energy savings;horloge;procesador oleoducto;low power design;power consumption;consommation energie electrique;processeur pipeline;alimentacion electrica;arret;high performance;clock;calcul reparti;reloj;multiplicador;pipeline processor;consumo energia;potencia debil	This paper proposes a novel low-power 32bit × 32bit multiplier with pipelined block-wise shutdown scheme. When it idles, it turns off supply voltage to reduce both dynamic and static power. Restoring whole large-size functional block often causes severe power line noise, so the proposed multiplier shutdowns and wakes up sequentially along with pipeline stage. It is described in Verilog HDL and fabricated in standard cell library. Total gate count is about 22400 gates. Power simulation was performed in deep submicron technologies where static power due to leakage current increases significantly. In idle mode, the proposed multiplier consumes 0.013mW and 0.006mW in 0.13μm and 0.09μm technologies, respectively, and it reduces power consumption to 0.07%~0.08% of active mode. As fabrication technology becomes small, power efficiency degrades in the conventional clock gating scheme, but the proposed multiplier does not. The low-power design methodology in this paper can be easily adopted in most functional blocks with pipeline architecture.	32-bit;active directory;clock gating;gate count;hardware description language;low-power broadcasting;noise (electronics);performance per watt;pipeline (computing);shutdown (computing);simulation;spectral leakage;standard cell;verilog;very-large-scale integration	Yong-Ju Jang;Yoan Shin;Min-Cheol Hong;Jae-Kyung Wee;Seongsoo Lee	2005		10.1007/11602569_42	clock;electrical network;block design;electrical efficiency;telecommunications;computer science;leakage;multiplier;clock gating	EDA	18.239162387107612	55.42219008142966	19159
6216ec83bbf1561516e9a86979856084cd50a677	system-level design technologies for heterogeneous distributed systems	quality of life;ultra low power;new technology;integrated circuits design;building block;heterogeneous data;optimization problem;integrated circuit design;heterogeneous distributed system;system level design;sensors and actuators;environmental management;design methodology	The ongoing scaling and hybridization of manufacturing technologies enables us to attain unprecedented levels performance as well as to integrate electronic and fluidic circuits with sensors and actuators. Smart micro/nano systems will be the building blocks of wearable and ambient systems, that gather and integrate heterogeneous data in real time and operate and communicate in a wireless and ultra low power mode. These systems will foster a revolution in health and environmental management, with the final objective of improving security and quality of life. At the same time, they will create a large market of components and systems, and a renewed perspective for electronic design and manufacturing companies.  To accomplish such an ambitious goal, new technologies and architectures must be matched and tailored to the operational environment by solving novel an challenging design and optimization problems, through the creation of novel design methodologies and tools.	as-interface;distributed computing;electronic design automation;electronic system-level design and verification;environmental resource management;gnu nano;image scaling;level design;mathematical optimization;wearable computer	Giovanni De Micheli	2008		10.1145/1404371.1404378	optimization problem;embedded system;electronic engineering;simulation;quality of life;design methods;computer science;systems engineering;engineering;electrical engineering;electronic system-level design and verification;integrated circuit design	EDA	9.304961881653776	56.73206116151599	19166
f9a08f80443e1deca60478b8ecfac1510278ed83	multilevel timing-constrained full-chip routing in hierarchical quad-grid model	multilevel timing constrained full chip routing;pins;routing;very large scale integration;wires;trees mathematics;timing driven routing trees;trees mathematics integrated circuit interconnections network routing;network routing;chip;hierarchical quad grid model multilevel timing constrained full chip routing timing driven routing trees;trees graphs;integrated circuit interconnections;routing circuit testing wires computer science integrated circuit interconnections benchmark testing timing pins central processing unit very large scale integration;circuit testing;computer science;benchmark testing;central processing unit;hierarchical quad grid model;timing	In this paper, given a set of timing-driven routing trees for all the interconnection nets, a new multilevel timing-constrained full-chip routing (MTFR) in a dynamic hierarchical quad-grid model is proposed to complete full-chip routing in reasonable time. The experimental results show that the proposed MTFR approach uses less CPU time to obtain 100% timing-constrained routing results for all the tested benchmark circuits	benchmark (computing);central processing unit;interconnection;routing	Jin-Tai Yan;Yen-Hsiang Chen;Chia-Fang Lee;Ming-Ching Huang	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1693864	routing table;routing;electronic engineering;static routing;parallel computing;real-time computing;equal-cost multi-path routing;computer science;dynamic source routing;multipath routing;destination-sequenced distance vector routing;distance-vector routing protocol;link-state routing protocol	EDA	15.307542189607958	51.90304046163999	19249
bffd519c738022b66e03973d013fe32329795abf	design of configurable sequential circuits in quantum-dot cellular automata		Abstract Quantum-dot cellular automata (QCA) is a likely candidate for future low power nano-scale electronic devices. Sequential circuits in QCA attract more attention due to its numerous application in digital industry. On the other hand, configurable devices provide low device cost and efficient utilization of device area. Since the fundamental building block of any sequential logic circuit is flip flop, hence constructing configurable, multi-purpose QCA flip-flops are one of the prime importance of current research. This work proposes a design of configurable flip-flop (CFF) which is the first of its kind in QCA domain. The proposed flip-flop can be configured to D, T and JK flip-flop by configuring its control inputs. In addition, to make more efficient configurable flip-flop, a clock pulse generator (CPG) is designed which can trigger all types of edges (falling, rising and dual) of a clock. The same CFF design is used to realize an edge configurable (dual/rising/falling) flipflop with the help of CPG. The biggest advantage of using edge configurable (dual/rising/falling) flip-flop is that it can be used in 9 different ways using the same single circuit. All the proposed designs are verified using QCADesigner simulator.	1-bit architecture;automata theory;bitwise operation;central pattern generator;clock signal;flops;flip-flop (electronics);gnu nano;logic gate;multi-purpose viewer;pulse generator;qualitative comparative analysis;quantum cellular automaton;quantum dot cellular automaton;random-access memory;requirement;sequential logic;shift register;simulation	Mrinal Goswami;Mayukh Roy Chowdhury;Bibhash Sen	2017	CoRR		flops;electronics;theoretical computer science;flip-flop;prime (order theory);computer science;cellular automaton;sequential logic;quantum dot cellular automaton;clock signal	EDA	15.941094269069527	45.583102980225114	19284
13c7961506ab9157ef3fef039d5c824222cb296d	functional level power analysis: an efficient approach for modeling the power consumption of complex processors	estimation methodology;dsp application;efficient approach;power consumption;embedded software;assembly code;different processor;average error;complex processors;functional level power analysis;ti c62;associated tool;high-level consumption estimation methodology;codesign;code generation;place and route;functional model;reconfigurable computing;parametric model;system on chip;embedded systems;system on a chip;low power electronics;power analysis	A high-level consumption estimation methodology and its associated tool, SoftExplorer, are presented. The estimation methodology uses a functional modeling of the processor combined with a parametric model to allow the designer to estimate the power consumption when the embedded software is executed on the target. SoftExplorer uses as input the assembly code generated by the compiler; its efficiency is compared to SimplePower's approach. Results for different processors (TI C62, C67, C55 and ARM7) and for several DSP applications provide an average error less than 5%.	arm7;assembly language;central processing unit;compiler;digital signal processor;embedded software;high- and low-level;parametric model;ti-nspire series	Johann Laurent;Nathalie Julien;Eric Senn;Eric Martin	2004	Proceedings Design, Automation and Test in Europe Conference and Exhibition		system on a chip;embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;statistics	EDA	2.750415089354476	52.93260369353795	19300
1cd92e67de02eca8d8269c97ecbd175f39e2c05c	a high speed fpga implementation of the rijndael algorithm	rijndael algorithm;new aes algorithm;national institute;fpga implementation;aes algorithm submission;draft fips;xilinx fpga;original rijndael algorithm;aes proposal;non-pipelined fpga implementation;high speed;block ram;field programmable gate arrays;logic design;cryptography	This paper presents a high speed, non-pipelined FPGA implementation of the Rijndael algorithm (Daemen, 1999), which has been selected as the new AES algorithm by the National Institute of Standards and Technology (NIST). In this study, we have implemented both the encryption and the decryption algorithms of Rijndael on the same FPGA. All the key and data length combinations of the original Rijndael algorithm are supported. This implementation, which uses 8378 slices and 4 block RAMs of the Xilinx FPGA, has a worst case operating frequency of 65 MHz, yielding a maximum throughput of 1.19 Gb/s.	algorithm;best, worst and average case;clock rate;encryption;field-programmable gate array;gigabyte;maximum throughput scheduling;pipeline (computing)	Refik Sever;A. Neslin Ismailoglu;Yusuf Çagatay Tekmen;Murat Askar;Burak Okcan	2004	Euromicro Symposium on Digital System Design, 2004. DSD 2004.	10.1109/DSD.2004.1333297	embedded system;parallel computing;programmable logic array;computer science;cryptography	EDA	9.254943106310526	45.22598793051201	19332
1622db9af5a561297c153efa0156b0840b1703bd	novel algorithm combining temporal partitioning and sharing of functional units	temporal partitioning;attractive solution;functional unit;enabling sharing;novel algorithm combining temporal;temporal partition;functional units;novel algorithm;fpga device;resource virtualization;resource sharing;proposed algorithm attempt;switches;field programmable gate arrays;hardware;circuits;resource management;silicon;job shop scheduling	Resource virtualization on FPGA devices, achievable due to its dynamic reconfiguration capabilities, provides an attractive solution to save silicon area. Architectural synthesis for dynamically reconfigurable FPGA-based digital systems needs to consider the case of reducing the number of temporal partitions (reconfigurations), by enabling sharing of some functional units in the same temporal partition. This paper proposes a novel algorithm for automated datapath design, from behavioral input descriptions (represented by a dataflow graph), which simultaneously performs temporal partitioning and sharing of functional units. The proposed algorithm attempts to minimize both the number of temporal partitions and the execution latency of the generated solution. Temporal partitioning, resource sharing, scheduling, and a simple form of allocation and binding are all integrated in a single task. The algorithm is based on heuristics and on a new concept of construction by gradually enlarging timing slots. Results show the efficiency and effectiveness of the algorithm when compared to existent approaches.	algorithm;data-flow analysis;dataflow;datapath;digital electronics;field-programmable gate array;heuristic (computer science);reconfigurability;scheduling (computing);x86 virtualization	João M. P. Cardoso	2001	The 9th Annual IEEE Symposium on Field-Programmable Custom Computing Machines (FCCM'01)		shared resource;embedded system;job shop scheduling;electronic circuit;parallel computing;real-time computing;network switch;computer science;resource management;theoretical computer science;operating system;distributed computing;silicon;field-programmable gate array	Embedded	-0.041930727343108966	51.841311780458355	19375
70eecc9efcd7ca2a6b5d4f78c532a2c65ce523c0	an integrated design for testability and automatic test pattern generation system: an overview	idas system;automatic test pattern generator;integrated design;heuristic controllability;general overview;automatic test pattern generation;major component;future research activity;concurrent fault simulator;brief description;displays;design for testability;fault detection;pattern analysis;observability;logic;controllability	A general overview on an Integrated Design for Testability and Automatic Test Pattern Generation System (IDAS) is given. The major components of IDAS include: heuristic controllability/observability (C/O) analysis, prediction of testing costs, tools for evaluation, display and improvement of testability, and C/O guided automatic test pattern generator. The IDAS system includes also the logic and concurrent fault simulator CADAT. A brief description of major components with a scenario how to use IDAS is given. Future research activities are discussed.	design for testing;fault simulator;heuristic;software testability;test card	Erwin Trischler	1984	21st Design Automation Conference Proceedings		reliability engineering;electronic engineering;observability;controllability;computer science;automatic test pattern generation;logic;algorithm	EDA	9.153300052005386	52.872575645213125	19384
66aa2f0e7924ddb0467518fa2cccded9737e4321	time constrained allocation and assignment techniques for high throughput signal processing	application specific integrated circuits;circuit layout cad;computational complexity;optimisation;signal processing;application specific architectures;assignment techniques;complex application specific datapaths;condition blocks;global optimization process;hierarchical compositions;high throughput signal processing;nested loops;real life examples;time constrained allocation	In this paper, a technique for the allocation of complex application specific datapaihs will be presented. The technique is especially suited for the synthesis of application specijic architectures for high-throughput signal processing applications. Such applications comprise hierarchical compositions of nested loops and condition blocks. A minimum area set of datapaths is allocated and the available cycle budget is automatically distributed over the diflerent blocks of the hierarchy in one global optimization process. Results for a number of real life examples are presented.	control flow;datapath;global optimization;high-throughput computing;mathematical optimization;real life;signal processing;throughput	Werner Geurts;Francky Catthoor;Hugo De Man	1992			network synthesis filters;high-throughput screening;throughput;electronic engineering;parallel computing;real-time computing;nested loop join;computer science;signal processing;application-specific integrated circuit;computational complexity theory;algorithm;global optimization	EDA	0.10956278873039998	52.037474347930136	19394
cdec03794464bba10da2a0740d81f3861ee8eb64	on-line error detection in a carry-free adder	index terms--carry-free addition;on-line error detection;indexing terms;cost effectiveness;error detection	This work presents an improved design for a carry - free adder featuring on-line error detection. The salient contribution of this work is an extremely quick and cost- effective method of conversion from either two's complement or signed magnitude format into the internal 1-out-of-3 code used within this adder.	adder (electronics);error detection and correction	Whitney J. Townsend;Mitchell A. Thornton;Parag K. Lala	2002			error detection and correction;theoretical computer science;computer science;adder	ML	22.09536552997401	47.748366994913695	19437
5caf773badf2c28690b949536ff2f01cb606c1f7	harnessing fabrication process signature for predicting yield across designs	fabrication;training;fabrication semiconductor device modeling predictive models training correlation data models radio frequency;size 65 nm high volume manufacturing time to market quality requirements yield estimation hvm silicon wafers fabrication facility bmf technique bayesian model fusion technique sizable industrial data rf devices fabrication process signature;radio frequency;semiconductor device modeling;time to market bayes methods elemental semiconductors nanotechnology product quality production facilities semiconductor technology silicon;predictive models;correlation;data models	Yield estimation is an indispensable piece of information at the onset of high-volume manufacturing (HVM) of a device. The increasing demand for faster time-to-market and for designs with growing quality requirements and complexity, requires a quick and successful yield estimation prior to HVM. Prior to commencing HVM, a few early silicon wafers are typically produced and subjected to thorough characterization. One of the objectives of such characterization is yield estimation with better accuracy than what pre-silicon Monte Carlo simulation may offer. In this work, we propose predicting yield of a device using information from a similar previous-generation device, which is manufactured in the same technology node and in the same fabrication facility. For this purpose, we rely on the Bayesian Model Fusion (BMF) technique. The effectiveness of the proposed methodology is evaluated using sizable industrial data from two RF devices in a 65nm technology.	bayesian network;hardware-assisted virtualization;monte carlo method;onset (audio);radio frequency;requirement;semiconductor device fabrication;simulation;wafer (electronics)	Ali Ahmadi;Haralampos-G. D. Stratigopoulos;Amit Nahar;Bob Orr;Michael Pas;Yiorgos Makris	2016	2016 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2016.7527386	embedded system;data modeling;electronic engineering;semiconductor device modeling;simulation;computer science;engineering;electrical engineering;predictive modelling;fabrication;correlation;radio frequency	EDA	23.178189290774544	55.868937898780864	19500
c46386cb053f5878643c6a6693e8e330628a3d0e	architectural tradeoff in implementing rsa processors	chinese remainder theorem;left to right;modular arithmetic	An investigation of a suite of RSA processors using different exponentiation and modular arithmetic algorithms is the main theme of this paper. The execution time and the amount of hardware required of different algorithms used to implement the RSA processor are compared. The modular algorithms examined in this paper are classical modular algorithm, Barrett's modular algorithm, Hensel's odd division and Montgomery's modular algorithm. The exponentiation algorithms implemented are the left-to-right binary method, the right-to-left binary method, the Chinese remainder theorem. This work finds that the fast RSA processor is the one using the Chinese remainder theorem with right to left scan for exponentiation operations and Barrett's algorithm for modular arithmetic operations. The RSA processor using least amount of hardware is the one using the left-o-right binary method for exponentiation operations and Montgomery's algorithm for modular operations.	algorithm;barrett reduction;central processing unit;montgomery modular multiplication;polynomial remainder theorem;right-to-left;run time (program lifecycle phase)	Fu-Chi Chang;Chia-Jiu Wang	2002	SIGARCH Computer Architecture News	10.1145/511120.511123	exponentiation by squaring;barrett reduction;modular arithmetic;parallel computing;kochanski multiplication;theoretical computer science;chinese remainder theorem;modular exponentiation	Arch	9.197049119689733	43.892548042323035	19633
828ead42b34678413351e7cd4233e8b516a2a28e	robust vlsi architecture for system-on-chip design and its implementation in viterbi decoder	resource utilization;cmos integrated circuits;integrated circuit yield;parallel pipeline architectures robust vlsi architecture system on chip design viterbi decoder broken computing modules data flows resource utilization circuit area cmos technology parallel concurrent processing;integrated circuit design;parallel architectures;system on chip;viterbi decoder;vlsi;robustness very large scale integration system on a chip viterbi algorithm decoding computer architecture switches cmos technology data flow computing resource management;parallel architectures vlsi system on chip integrated circuit design viterbi decoding integrated circuit yield cmos integrated circuits;data flow;viterbi decoding;vlsi architecture	The paper presents a robust VLSI architecture which avoids most of the malfunctions and makes the system work correctly. The proposed architecture realizes robustness only by using small switches. The switches avoid broken computing modules and reconfigure data flows between the normal modules. This architecture has advantages compared to conventional duplicated systems in terms of resource utilization and circuit area, and improves yield rate. We designed a Viterbi decoder based on the proposed robust architecture and evaluated its effectiveness in CMOS technology.	cmos;network switch;system on a chip;systems architecture;very-large-scale integration;viterbi decoder	Yasuyuki Hatakawa;Shingo Yoshizawa;Yoshikazu Miyanaga	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1464515	computer architecture;electronic engineering;parallel computing;computer science;viterbi decoder;statistics	Arch	8.055896012668335	56.97883858461585	19659
0c29131ad16631f06af85d99b59f38bbc32e0e21	a novel recursive algorithm for bit-efficient realization of arbitrary length inverse modified cosine transforms	recursive estimation;inverse modified cosine transform;reconfigurable architectures;tpm;fpga;dsp domain recursive algorithm bit efficient realization inverse modified cosine transform imdct vlsi implementation reconfigurable architectures;dsp domain;trusted computing;embedded systems;reconfigurable architecture;transforms;bit efficient realization;partial dynamic reconfiguration;vlsi digital signal processing chips reconfigurable architectures recursive estimation transforms;imdct;vlsi;digital signal processing chips;recursive algorithm;vlsi implementation;cosine transform;reconfigurable hardware;codecs reconfigurable architectures computer architecture digital signal processing frequency domain analysis application specific processors hardware very large scale integration compression algorithms discrete cosine transforms	In this paper a novel approach for Inverse Modified Cosine Transform (IMDCT) computation is presented, based on a recursive algorithm. Due to its nature, this IMDCT calculation can be performed on a reduced bit width datapath without loss of accuracy, compared to alternative recursive architectures. Combined with the regular structure, the approach allows for a much more area efficient VLSI implementation compared to existing systems. Due to its bit efficiency this approach is attractive to be implemented on reconfigurable architectures of the DSP domain as well.	algorithm;computation;datapath;modified discrete cosine transform;recursion (computer science);very-large-scale integration	Ralf König;Timo Stripf;Jürgen Becker	2008	2008 Design, Automation and Test in Europe	10.1145/1403375.1403522	embedded system;electronic engineering;parallel computing;reconfigurable computing;computer science;theoretical computer science;discrete cosine transform;very-large-scale integration;trustworthy computing;field-programmable gate array;recursion	EDA	11.296419038932736	44.04247960136858	19664
29542eb16aafa0657956574f03b4aadc60f3fa0e	new front-end and line justification algorithm for automatic test generation	front end		algorithm	Ruey-Sing Wei;Alberto L. Sangiovanni-Vincentelli	1986			real-time computing;computer science;theoretical computer science;engineering drawing;front and back ends	EDA	9.720889035372045	53.17841712482413	19683
d55e6020548358e602a6d6515227dd42b4867ac1	asynchronous multi-channel adc and dsp processor interface	analog devices ad9219;digital signal processing;clock domains asynchronous multichannel adc dsp processor interface system on chip smooth interfacing analog to digital converter serial interface high speed dsp processor analog devices ad9219 low power consumption vsli design asynchronous fifo gray code synchronization;vsli design;gray code;dsp processor interface;vlsi analogue digital conversion asynchronous circuits clocks digital signal processing chips gray codes low power electronics synchronisation system on chip;clocks;clock domains;radiation detectors;serial interface;synchronization digital signal processing clocks radiation detectors reflective binary codes system on a chip usa councils;usa councils;serial to parallel converter adc interface asynchronous fifo;asynchronous multichannel adc;reflective binary codes;interface design;system on a chip;ease of use;serial to parallel converter;synchronisation;system on chip;smooth interfacing;synchronization;analogue digital conversion;asynchronous fifo;low power electronics;gray code synchronization;vlsi;analog to digital converter;adc interface;digital signal processing chips;asynchronous circuits;low power consumption;high speed;high speed dsp processor;gray codes	System-on-chip technology requires smooth interfacing between its various peripherals and processors. Multi-channel Analog-to-Digital Converter with high speed and serial interface even raises more challenges to the designer in order to interface the ADC with processor. In this paper, we present the interface design between a multi-channel ADC and high speed DSP processor for system-on-chip design. The Analog Devices' AD9219 is selected because it is designed for low cost, low power consumption, small area size and ease of use. We have implemented the VSLI design of the interface to pass data from ADC to DSP processor. The interface uses asynchronous FIFOs with Gray code synchronization technique to handle synchronization issue created from two clock domains. We have explained the application of the design to show the connectivity between the ADC, the ADC interface and the DSP processor.	analog-to-digital converter;central processing unit;clock signal;computer memory;counter (digital);data buffer;digital signal processor;fifo (computing and electronics);integrated circuit;interrupt;least significant bit;mpsoc;parallel port;peripheral;serial communication;system on a chip;usability	Nennie Farina Mahat;Lam Kien Sieng;Muhamad Khairol Ab Rani	2010	2010 IEEE Asia Pacific Conference on Circuits and Systems	10.1109/APCCAS.2010.5775085	system on a chip;gray code;embedded system;synchronization;electronic engineering;computer hardware;telecommunications;computer science;operating system;algorithm	EDA	7.749240543032807	48.76259586457339	19729
897ab810ce7ec6d5c0a719a6e141b69e750a2b5d	switch complexity in systems with hybrid redundancy	replacement repair schemes;standby sparing;system reliability;restoring organs;computer reliability;computer reliability disagreement detector hybrid redundancy replacement repair schemes restoring organs standby sparing switching strategies;fault tolerant system;n modular redundancy;switching strategies;hybrid redundancy;disagreement detector	The combination of N-modular redundancy (NMR) and standby sparing has resulted in a promising redundancy technique for protecting those portions of a fault-tolerant system whose continuous real-time operation is essential. This technique, known as hybrid redundancy, uses N + Sp identical modules. N of these are connected to a majority voter to form an NMR core. The remaining Sp are used as standby spares. Disagreement detectors instruct a switch to replace with a standby spare any of the N modules that disagrees with the majority consensus. Since the switch and disagreement detector, as well as the modules, must function properly for the system to perform its designed task, the overall system reliability depends on the disagreement detector, switch, and hybrid reliabilities. Hence the overall reliability is a function of switch reliability. A highly reliable, thus simple, switch is desirable. First, strategies where every spare can be switched into every voter position (totally assigned) are considered. An optimal strategy is developed where the number of states in the switch is the criterion used for optimality. Formulas for the switch state count for the various strategies are derived and the strategies compared. Next, partially assigned switching strategies, strategies where every spare need not be capable of occupying every voter position, are examined. It is shown that designs where all spares are assigned to the same t + 2 of the 2t + 1 voter positions have as good reliability as the more complex totally assigned switching strategy.	fault tolerance;hot spare;real-time clock;sensor;switch	Daniel P. Siewiorek;Edward J. McCluskey	1973	IEEE Transactions on Computers	10.1109/T-C.1973.223707	fault tolerance;computer science;redundancy	Embedded	10.34161235301499	59.88683034492619	19749
1fda43f556c08c4cbac4e8bc738ec512e3e6e366	an embedded wavelet image coder with parallel encoding and sequential decoding of bit-planes	sequential bit planes;wavelet based coders;image coding;data compression;decoding;sequential decoding;bit plane encoding;video compression;multiple bit planes embedded wavelet image coder parallel encoding sequential decoding wavelet based coders image compression video compression data structure zerotree structure wavelet coefficient reordering bit plane partition run length coding bit plane encoder nonzerotree algorithms sequential bit planes bit plane encoding parallel processing;tree data structures;wavelet transforms;embedded systems;nonzerotree algorithms;image compression;streaming media;run length coding;data structures;wavelet coefficient reordering;embedded wavelet image coder;binary sequences;embedded systems image coding sequential decoding wavelet transforms data compression tree data structures;multiple bit planes;parallel encoding;encoding;bit plane partition;bit plane encoder;data structure;zerotree structure;wavelet coefficients;parallel processing;image coding decoding encoding parallel processing binary sequences wavelet coefficients partitioning algorithms video compression data structures streaming media;partitioning algorithms	Wavelet based coders are widely used in image and video compression. Many popular embedded wavelet coders are based on a data structure known as zerotree. However, there exists a category of embedded wavelet coders that are fast and efficient even without zerotrees. These coders are based on three key concepts: (1) wavelet coefficient reordering; (2) bit-plane partition; and (3) encoding of bit-planes with efficient run-length coding. In this paper, we propose a bit-plane encoder that can be used in these non-zerotree algorithms. Instead of encoding the bit-planes sequentially, the bit-plane encoding process can be completed in one pass when multiple bit-plane encoders are used simultaneously. This bit-plane encoder is inherently suitable for parallel processing architecture. The decoding process is treated sequentially since each bit-plane stream can only be synchronized upon the correct decoding of higher bit-planes. To the best of our knowledge, this paper is the first to realize parallelization through encoding multiple bit-planes simultaneously.	embedded system;sequential decoding;wavelet	Yufei Yuan;Mrinal Kanti Mandal	2004		10.1109/ISCAS.2004.1328878	data compression;parallel computing;real-time computing;data structure;computer science;theoretical computer science	EDA	12.006367091148638	39.11884363747068	19760
66d88116f50139cb90682c60a333ee767df2abf1	driver assistance system design and its optimization for fpga based mpsoc	random access memory;logic design;design optimization field programmable gate arrays application software software performance target tracking automotive engineering safety hardware software systems multiprocessing systems;radar tracking;multiprocessor systems;traffic engineering computing field programmable gate arrays logic design multiprocessing systems system on chip;fpga;system on a chip;multiple target tracking;logic gates;multiple target tracking driver assistance system fpga mpsoc;system on chip;traffic engineering computing;multiprocessing systems;field programmable gate arrays;target tracking;processing speed;program processors;mpsoc;driver assistance system	This paper discusses the design and optimization of an FPGA based MPSoC dedicated to Multiple Target Tracking (MTT) application. MTT has long been used in various military and civilian applications but its use in automotive safety has been little investigated. The system attains a desired processing speed and uses minimum FPGA resources. We profile the software to identify performance bottlenecks and then gradually tune the hardware and software to meet system constraints. The result is a complete embedded MTT application running on a multiprocessor system that fits in a contemporary medium sized FPGA device.	embedded system;experiment;fits;field-programmable gate array;mpsoc;mathematical optimization;meaning–text theory;multiprocessing;program optimization;radar;systems design;usb hub	Jehangir Khan;Smaïl Niar;Mazen A. R. Saghir;Yassin Elhillali;Atika Rivenq	2009	2009 IEEE 7th Symposium on Application Specific Processors	10.1109/SASP.2009.5226338	system on a chip;embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;fpga prototype;field-programmable gate array	Embedded	2.867291125697023	52.76855894384724	19783
172d515e5475679e51c583b2f7f4babc98a416b1	three-dimensional dynamic programming for homology search	parallelisme;dynamic programming;field programmable gate array;serveur documentaire;diseno circuito;programacion dinamica;base donnee;haute performance;alignement sequence;calculateur embarque;modelo 3 dimensiones;homology search;reconfigurable architectures;modele 3 dimensions;circuit design;database;base dato;bioinformatique;three dimensional model;dynamic program;alineacion secuencia;red puerta programable;elemento matricial;reseau porte programmable;three dimensional;ordinateur hote;parallelism;host computer;paralelismo;modelo 2 dimensiones;homology;documentary server;boarded computer;programmation dynamique;servidor documental;modele 2 dimensions;alto rendimiento;computador huesped;matrix element;conception circuit;sequence alignment;bioinformatica;computational biology;homologia;high performance;off the shelf;architecture reconfigurable;calculador embarque;element matriciel;two dimensional model;bioinformatics;homologie	"""Alignment problems in computational biology have been fo- cused recently because of the rapid growth of sequence databases. Many systems for alignment have been proposed to date, but most of them are designed for two-dimensional alignment (alignment between two se- quences), because huge amount of memory and very long computational time are required by alignment among three or more sequences. In this paper, we describe a compact system with an off-the-shelf FPGA board and a host computer for three-dimensional alignment using Dynamic Programming. Through our approach, high performance are attained by """"two phase search"""" with reconfigurations of an FPGA and co-processing the FPGA and software. Furthermore, in order to achieve higher par- allelism in the FPGA, we use a payoff matrix for matching elements in sequences and the matrix is divided into sub-matrices which are mini- mized. In comparison to a single Intel Pentium4 2.53GHz processor, our system with a single XC2V6000 enables more than 250-fold speedup."""	dynamic programming;homology modeling	Yoshiki Yamaguchi;Tsutomu Maruyama;Akihiko Konagaya	2004		10.1007/978-3-540-30117-2_52	embedded system;three-dimensional space;homology;simulation;computer science;theoretical computer science;operating system;dynamic programming;circuit design;sequence alignment;host;algorithm;field-programmable gate array	ML	0.572082217714958	42.45024556599917	19820
0362dd1949943537ff997e873ed93cc21c3967c7	a distributed fifo scheme for on chip communication	interconnect delays;signal propagation;parasitic effects;degradation;distributed fifo scheme;interconnect on chip communication buffer wave pipelining self timed;integrated circuit design integrated circuit interconnections delays nanoelectronics repeaters buffer circuits;clocks;integrable system;interconnect;first in first out;delay effects;system on a chip;buffer circuits;interconnection network;chip;soc design;multiple clock cycles;integrated circuit design;chip interconnects;nanometer regime;system on chip;environmental variation;integrated circuit interconnections;voltage;nanoelectronics;power management;clock synchronization;repeaters;self timed;buffer;temperature;on chip communication;communication system control;frequency;wave pipelining;buffer control circuitry;1 67 ghz;first in first out buffers;0 25 micron;data transfer;integrated circuit interconnections clocks communication system control delay effects degradation repeaters system on a chip voltage temperature frequency;delays;repeater insertion;0 25 micron distributed fifo scheme on chip communication interconnect delays nanometer regime parasitic effects multiple clock cycles signal propagation repeater insertion first in first out buffers buffer control circuitry chip interconnects soc design 1 67 ghz	Interconnect delays are increasingly becoming the dominant source of performance degradation in the nanometer regime, largely because of disturbances that result from parasitic effects. On chip communication now requires multiple clock cycles for signal propagation between communicating modules/components. Repeater insertion is widely used to improve global interconnect delays. We propose having distributed first-in-first-out buffers to facilitate communication between components/modules of highly integrated systems, such as system-on-chip. This stateful scheme has very good tolerance for voltage and temperature variations. The buffer control circuitry is self-timed and allows for ease of interfacing in multiple domain clock designs. We present the buffer and its associated control circuits that allow data transfers at a maximum frequency of 1.67 GHz in a 0.25 /spl mu/m technology.	clock signal;electronic circuit;elegant degradation;fifo (computing and electronics);mumps;repeater insertion;software propagation;state (computer science);system on a chip	Ray Robert Rydberg;Jabulani Nyathi;José G. Delgado-Frias	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1464971	system on a chip;embedded system;electronic engineering;real-time computing;telecommunications;computer science;engineering	EDA	14.91227059589254	57.45149166728249	19821
22b77ce84863e87102a84c35a93e66f04eaa9b06	network-on-chip for a partially reconfigurable fpga system	fault tolerant;partial reconfiguration;reconfigurable computing;network on chip;fpga;readback	A custom FPGA-based, 1U CubeSat form-factor reconfigurable computing development platform has been designed and built for the purpose of implementing and testing multicore and multiprocessor systems. The platform was designed to leverage the active partial reconfiguration and configuration readback capabilities of the Xilinx Virtex-6 device. This enables myriad research opportunities in parallel processing, high-performance reconfigurable computing, and multicore/multiprocessor system design. An example application features a nine-processor radiation tolerant computer system which motivates further research into network-on-chip solutions for reducing multicore system routing complexity.	computer;field-programmable gate array;form factor (design);multi-core processor;network on a chip;parallel computing;reconfigurable computing;routing;symmetric multiprocessing;systems design	Justin A. Hogan;Raymond J. Weber;Brock J. LaMeres;Todd Kaiser	2013		10.1145/2464996.2467285	embedded system;fault tolerance;parallel computing;real-time computing;reconfigurable computing;computer science;network on a chip;field-programmable gate array	EDA	0.8092784643792532	49.64637189907256	19832
f1a694f08b95ae863aacf2641e068a9f140d7edd	complexity of partially defined combinational switching functions	threshold gate networks;complexity bounds;limited fan in;complexity bounds limited fan in partially defined switching functions threshold gate networks;partially defined switching functions	The complexity of the switching networks necessary to realize arbitrary combinational functions is studied. Asymptotic upper and lower bounds for fully defined functions are well known, while lower bounds also exist for partially defined combinational functions. The present paper supplements these results with the upper bounds for the partially defined functions. The results have possible relevance to pattern recognition.	combinational logic;pattern recognition;relevance	David R. Smith	1971	IEEE Transactions on Computers	10.1109/T-C.1971.223212	combinatorics;discrete mathematics;mathematics;algorithm	Theory	23.25756363066804	40.86806495669518	19849
c1936e34726e30ddc5d3b0c22c86a7cdd2b73db7	minority gate oriented logic design with quantum-dot cellular automata	diseno circuito;design principle;quantum dot cellular automata;puerta logica;logica cuantica;adder;echelle nanometrique;logic design;building block;circuit design;adicionador;tecnologia mos complementario;vlsi design;nano computing;boolean expression;porte logique;quantum logic;conception logique;automate cellulaire;expresion booleana;expression booleenne;nanometer scale;minority gate;conception circuit;additionneur;technologie mos complementaire;concepcion logica;logic gate;cellular automaton;logique quantique;complementary mos technology;automata celular	This paper presents novel combinational logic designs with plus-shaped quantum-dot cellular automata (QCA) using minority gate as the fundamental building block Present CMOS technology of VLSI design is fast approaching its fundamental limit, and researchers are looking for a nano-scale technology for future ICs in order to continue the pace of circuit miniaturization predicted by Moore's law even beyond 2016 QCA is considered to be a promising technology in this regard This paper provides the fundamentals of QCA followed by the proposed QCA structure realizing a minority gate, given by the Boolean expression m (x1, x2, x3) = x1′.x2′+x2′.x3′+x3′.x1′ Universality of minority gate is established, and minority gate oriented design principles are provided Minority gate oriented designs for XOR and full adder are presented Simulation results show the effectiveness of the proposed designs.	logic synthesis;quantum dot cellular automaton	Samir Roy;Biswajit Saha	2006		10.1007/11861201_75	cellular automaton;nanocomputer;quantum logic;logic synthesis;boolean expression;logic gate;computer science;circuit design;mathematics;very-large-scale integration;programming language;algorithm;adder	EDA	16.571805301670622	45.58419233480752	19850
4f6322139aaf30f473e5ac5cfc99eaf8711a5118	automatic c compiler generation from architecture description language isac.	004;isac architecture compiler generation	This paper deals with retargetable compiler generation. After an introduction to applicationspecific instruction set processor design and a review of code generation in compiler backends, ISAC architecture description language is introduced. Automatic approach to instruction semantics extraction from ISAC models which result is usable for backend generation is presented. This approach was successfully tested on three models of MIPS, ARM and TI MSP430 architectures. Further backend generation process that uses extracted instruction is semantics presented. This process was currently tested on the MIPS architecture and some preliminary results are shown. Digital Object Identifier 10.4230/OASIcs.MEMICS.2010.47	arm architecture;architecture description language;code generation (compiler);codec;compiler;identifier;information sharing and analysis center;processor design;retargeting;ti msp430	Adam Husár;Miloslav Trmac;Jan Hranac;Tomás Hruska;Karel Masarík	2010		10.4230/OASIcs.MEMICS.2010.47	computer architecture;parallel computing;compiler correctness;computer science;compiler construction;programming language	Arch	3.3035725799518607	50.62069683695888	19871
43eedc751dd6de49594f5a797fd01910119c0bbe	tool capabilities needed for designing 100 mhz interconnects	printed circuit design;simulations tool capabilities 100 mhz interconnects printed circuit board design complexity;delay timing integrated circuit interconnections circuit simulation clocks system buses equations printed circuits computational modeling electronic mail;integrated circuit interconnections;circuit cad printed circuit design integrated circuit interconnections printed circuit testing;circuit cad;printed circuit testing;printed circuit board	Printed circuit board design complexity increases greatly as bus speeds exceed 100 MHz. This increased complexity is due more to the large number of simulations a designer must complete rather than simulation or modeling accuracy. This paper presents the case for these increased numbers of simulations, and presents techniques for managing this complexity.	batch processing;human-readable medium;printed circuit board;simulation;speaker wire	Tim A. Schreyer	1998		10.1109/ASPDAC.1998.669506	physical design;embedded system;circuit diagram;electronic engineering;boundary scan;diagnostic board;tape-out;integrated circuit packaging;computer science;engineering;electrical engineering;circuit design;flying probe;diode-or circuit;design layout record;circuit extraction;printed circuit board;electronic circuit simulation;routing;discrete circuit	EDA	12.389496744172725	51.430610370590266	19933
fc50f40536f72d6121bce2cf63c3711dcb07638f	testing of vega2, a chip multi-processor with spare processors.	atpg patterns;automatic test pattern generation;chip multiprocessor;chip;spare processor cores;memory macros;logic testing;atpg patterns vega2 testing chip multiprocessor memory macros spare processor cores column redundancy defective processors;logic testing eprom built in self test costs system testing automatic test pattern generation packaging java home appliances sun;electronic engineering computing;chip multi processor;multiprocessing systems;column redundancy;vega2 testing;multiprocessing systems automatic test pattern generation electronic engineering computing logic testing microprocessor chips;microprocessor chips;defective processors	Vega2 is a CMP (chip multi-processor) with 48 processor cores, and several spare cores to improve yield. The chip also contains about 1000 memory macros both inside and outside the processor cores. The larger memories have column redundancy as well. The main DFT challenge for Vega2 is to produce an architecture that makes it easy to identify defective processors, thoroughly test the memories and efficiently apply ATPG patterns.	central processing unit;multiprocessing	Samy Makar;Tony Altinis;Niteen Patkar;Janet Wu	2007	2007 IEEE International Test Conference	10.1109/TEST.2007.4437584	chip;embedded system;computer architecture;parallel computing;computer hardware;telecommunications;computer science;engineering;automatic test pattern generation	EDA	10.756075233831705	54.51201809129964	19940
aec6769bb200fe7dcfe734544437d6b94d836bd9	the optimistic update theorem for path delay testing in sequential circuits	fault simulation;sequential circuits;adverse effect;timing analysis;test generation;path delay fault;fault coverage;path delay faults;flip flop	For sequential circuit path delay testing, we propose a new update rule for state variables whereby flipflops are updated with their correct values provided they are destinations of at least one robustly activated path delay fault. Existing algorithms in the literature, for robust fault simulation and test generation, assign unknown values to off-path latches that have non-steady signals at their inputs in the previous vector. Such procedures are pessimistic and predict low fault coverages. They also have an adverse effect on the execution time of fault simulation especially if the circuit has a large number of active paths. The proposed update rule avoids these problems and yet guarantees robustness.	algorithm;fault coverage;fault injection;fault tolerance;robustness (computer science);run time (program lifecycle phase);sequential logic;simulation;software propagation	Soumitra Bose;Prathima Agrawal;Vishwani D. Agrawal	1993	J. Electronic Testing	10.1007/BF00971977	real-time computing;fault coverage;adverse effect;computer science;engineering;stuck-at fault;fault model;distributed computing;sequential logic;static timing analysis;algorithm	EDA	21.487439081393184	51.12101832196532	19985
c361d315d0c6cbb7e86fb8524ac79c411aa68dec	soc design of ogg vorbis decoder using embedded processor	hardware acceleration;field programmable gate array;authentication;wlan;interface design;cost analysis;function block;secure remote password protocol;computational complexity;modular exponentiation;modified discrete cosine transform;embedded processor;reconfigurable hardware	This paper presents an ARM-based SoC architecture for the Ogg Vorbis audio decoder. A trivial software-based implementation incurs high computational cost and requires high operation frequency. In order to achieve realtime processing and efficient bus interface design for our target system, the load of an embedded processor is reduced through the use of specific hardware for a functional block that has higher computational complexity than other blocks of Ogg Vorbis decoding process. Based on computational cost analysis of whole decoding process, IMDCT (Inverse Modified Discrete Cosine Transform) is detected as the most computation-intensive functional block. As a result of FPGA (Field Programmable Gate Array) implementation, a 48% improvement in execution cycle is achieved by the specific hardware with 3,749 slices.	algorithm;algorithmic efficiency;computation;computational complexity theory;embedded system;field-programmable gate array;modified discrete cosine transform;libvorbis	Atsushi Kosaka;Satoshi Yamaguchi;Hiroyuki Okuhata;Takao Onoye;Isao Shirakawa	2004		10.1145/977091.977158	embedded system;parallel computing;real-time computing;computer science	EDA	9.372021401272232	45.17027110917922	19997
9a7e30f172c338409a8b0753adf05713d0fc7b5e	a sub-mw h.264 baseline-profile motion estimation processor core with a vlsi-oriented block partitioning strategy and simd/systolic-array architecture	simd;tecnologia electronica telecomunicaciones;systolic array;reference frame;motion estimation;low power;h 264;tecnologias;grupo a	We propose a sub-mW H.264 baseline-profile motion estimation processor for portable video applications. It features a VLSIoriented block partitioning strategy and low-power SIMD/systolic-array datapath architecture, where the datapath can be switched between an SIMD and systolic array depending on processing flow. The processor supports all the seven kinds of block modes, and can handle three reference frames for a CIF (352 × 288) 30-fps to QCIF (176 × 144) 15-fps sequences with a quarter-pixel accuracy. It integrates 3.3 million transistors, and occupies 2.8×3.1 mm2 in a 130-nm CMOS technology. The proposed processor achieves a power of 800 μW in a QCIF 15-fps sequence with one reference picture. key words: low power, motion estimation, H.264, SIMD, systolic array	baseline (configuration management);cmos;datapath;h.264/mpeg-4 avc;low-power broadcasting;motion estimation;pixel;simd;server message block;systolic array;transistor;very-large-scale integration	Junichi Miyakoshi;Yuichiro Murachi;Tetsuro Matsuno;Masaki Hamamoto;Takahiro Iinuma;Tomokazu Ishihara;Hiroshi Kawaguchi;Masayuki Miyama;Masahiko Yoshimoto	2006	IEICE Transactions	10.1093/ietfec/e89-a.12.3623	reference frame;embedded system;parallel computing;real-time computing;simd;systolic array;computer science;motion estimation	Arch	12.376356692923878	40.51410608901496	20036
924234d9ea895fa55cc4d29b40abe10b29541ded	energy-efficient, noninvasive water flow sensor		We are interested in hot and cold water flow detection in domestic kitchen and bathroom taps for smart home environments. Water flow monitoring is particularly valuable for long-term behavioural monitoring systems for health-related applications, as it enables the collection of long-term data on the hydration levels of the house residents, and it is associated with several activities of daily life, such as cooking and cleaning. This paper presents a water flow sensing device that is based on sensing the vibrations on the pipe when water is flowing through them. The proposed solution is noninvasive and energy efficient, as it does not require cutting the water pipes or altering the plumbing system, and consumes less then 2 uA in continuous operation. The proposed water flow sensor has been integrated to SPHERE, a sensing platform of non-medical sensors for healthcare monitoring and behavioural analytics in a home environment, and deployed to more than 15 residential properties.	algorithm;analog signal;clamping (graphics);continuous operation;expectation propagation;home automation;internet relay chat;kitchen sink regression;machine learning;named pipe;piezoelectricity;population;rectifier (neural networks);sensor;server (computing);spectral leakage;sputter cleaning	Antonis Vafeas;Atis Elsts;James Pope;Xenofon Fafoutis;George Oikonomou;Robert J. Piechocki;Ian Craddock	2018	2018 IEEE International Conference on Smart Computing (SMARTCOMP)	10.1109/SMARTCOMP.2018.00084	automotive engineering;flow (psychology);smart meter;home automation;efficient energy use;logic gate;water resources;continuous operation;computer science;analytics	Mobile	2.065516057956481	32.8995106291584	20039
ab9c78612c47397269d5b696ad4a141d47ee419e	first- and second-level packaging for the ibm eserver z900	comprehensive design methodology;glass-ceramic substrate;mcm electrical design;electrical property;ibm eserver z900;system design;electrical verification;design methodology;different glass-ceramic technology;second-level packaging;aggressive wiring ground rule;z900 mcm	This paper describes the system packaging of the processor cage for the IBM eServer z900. This server contains the world’s most complex multichip module (MCM), with a wiring length of 1 km and a maximum power of 1300 W on a glass-ceramic substrate. The z900 MCM contains 35 chips comprising the heart of the central electronic complex (CEC) of this server. This MCM was implemented using two different glass-ceramic technologies: one an MCM-D technology (using thin film and glass-ceramic) and the other a pure MCM-C technology (using glass-ceramic) with more aggressive wiring ground rules. In this paper we compare these two technologies and describe their impact on the MCM electrical design. Similarly, two different board technologies for the housing of the CEC are discussed, and the impact of their electrical properties on the system design is described. The high-frequency requirements of this design due to operating frequencies of 918 MHz for on-chip and 459 MHz for off-chip interconnects make a comprehensive design methodology and post-routing electrical verification necessary. The design methodology, including the wiring strategy needed for its success, is described in detail in the paper.	cmos;cpu cache;characteristic impedance;clock rate;coupling (computer programming);crosstalk;data rate units;design tool;enterprise system;ibm system z;ibm eserver;impedance matching;imperative programming;input/output;interconnection;internet backbone;mainframe computer;maximum power transfer theorem;multi-chip module;multiprocessing;noise (electronics);physical design (electronics);power supply;printed circuit board;ps (unix);requirement;routing;server (computing);signal integrity;simulation;speaker wire;systems design;terabit;tracing (software);via (electronics);wiring	Hubert Harrer;Harald Pross;Thomas-Michael Winkel;Wiren D. Becker;Herb I. Stoller;Masakazu Yamamoto;Shinji Abe;Bruce J. Chamberlin;George A. Katopis	2002	IBM Journal of Research and Development	10.1147/rd.464.0397	embedded system;electronic engineering;engineering;electrical engineering;operating system	EDA	9.69294269753051	51.901924376152905	20138
850826c9d15277b12be89286705fcf21d67f88ef	iddq defect detection in deep submicron cmos ics	electric current measurement cmos digital integrated circuits leakage currents integrated circuit testing;process variation;power supply;electric current measurement;cmos digital integrated circuits;leakage currents;integrated circuit testing;die individual cutoff point iddq defect detection deep submicron cmos ics standby electrical current observation iddq testing bridging faults transistor stuck on faults;circuit testing cmos technology logic testing electrical fault detection cmos logic circuits logic circuits current mos devices power supplies bipolar transistor circuits;defect detection	Traditional testing is based on applying signals to circuit inpiits and observing logic state of outputs. Howevet; with the iniroclitctiorr of CMOS technology a new test techniqite emerged, thut is based on applying signal inputs and observing standby electrical current. This technique is knonn as IDDQ testing. In most CMOS circuits, when all inputs are frozen in place and circuit transient activities settle down, [he current drops by 3-5 orders of magnitude below normal levels. During testing, if there is a deviation porn this behavior a defect is detected. While parametric test of electric current has rilrrays been popular (including in bipolar and NMOS reclrnologies) to detect short between power supply lines, the promise of IDDQ test is that it will also allow detection of bridging between signal nets and transistor stuck-on faults in CMOS circuits. However; in reality the resolution of the size of the defect is limited by the magnitude of stand-h? current. Since normal process variation can cause shift in stand-by current by two orders of magnitude. choosing a single threshold for IDDQ defect detection is impractical. Many ideas have been proposed to overcome this problem, most of which are based on choosing an adaptive cut-offpoint for a wafec or a lot. In this paper; rve suggest a technique, that in concept, is akin to setting an individiial cutoff point for a die. The concept has been validated based 011 actual data.	bridging (networking);cmos;die (integrated circuit);iddq testing;nmos logic;power supply;software bug;transistor;very-large-scale integration	Sandip Kundu	1998		10.1109/ATS.1998.741606	embedded system;electronic engineering;engineering;electrical engineering;iddq testing;integrated injection logic;process variation	EDA	22.62175877240169	54.20511109415582	20190
d597016f1beb6754739699e80fb9eea40292c0fb	high-speed low-power multiplexer-based selector for priority policy	high-speed low-power multiplexer-based selector;lowest path;priority encoder;priority policy;balanced propagation path;proposed priority scheme;propagation path;critical path;unbalanced propagation path;lowest weight;proposed design	Article history: Received 8 December 2011 Received in revised form 3 December 2012 Accepted 3 December 2012 Available online 9 January 2013 The nature of priority policy causes inefficient throughput in synchronous clock systems because of an unbalanced propagation path. To improve speed, the proposed priority scheme improves extremely unbalanced delays between the highest and lowest weight by integrating the multiplexer-based date selector with the priority encoder. Balanced propagation paths are analyzed based on the gate-level evaluation and demonstrated by post-layout simulation. In terms of scalability, this design is suitable for extending width and has a latency of only O(logm) for m requests. The proposed design also improves the critical path by using delayed-precharge technology for dynamic logic and transmission gate at transistor level. The simulation results show that, for 8–128 requests cases, this approach achieves balanced propagation paths from fastest to lowest path. The proposed design achieves a 4.5 speedup and a 57.2% decrease in power dissipation. Crown Copyright 2012 Published by Elsevier Ltd. All rights reserved.	128-bit;boolean algebra;cmos;clock network;critical path method;crown group;fastest;full custom;low-power broadcasting;multiplexer;priority encoder;scalability;simulation;software propagation;speedup;throughput;transistor;transmission gate;unbalanced circuit	Jih-Ching Chiu;Kai-Ming Yang	2013	Computers & Electrical Engineering	10.1016/j.compeleceng.2012.12.002	electronic engineering;real-time computing;telecommunications;computer science;distributed computing;computer network	EDA	12.322031121068733	47.702705777910325	20254
cb6b564d5de725e38c8a02feb883a5a4ae6e48f0	high-performance vlsi processor for robot inverse dynamics computation	cmos integrated circuits;inverse dynamics;vlsi cmos integrated circuits delays inverse problems matrix algebra microprocessor chips parallel architectures robots;linear array;matrix algebra;chip;vlsi technology matrix multiply addition processor minimum delay time inverse dynamics linear array structure odd even alternative computation mmp architecture data dependence graphs;parallel architectures;data dependence;robots;vlsi;delay time;high performance;very large scale integration robots delay effects equations computer architecture manipulator dynamics lagrangian functions high performance computing servomechanisms motion control;delays;microprocessor chips;inverse problems	A VLSI-oriented matrix multiply-addition processor (MMP) is proposed for minimum-delay-time inverse dynamics computation on a linear array structure. It is shown that the delay time of the inverse dynamics computation becomes minimum based on the concept of the odd-even alternative computation. The MMP architecture is systematically designed by using two types of data-dependence graphs of the odd-even alternative computation. It is demonstrated by the layout evaluation that the MMP can be easily implemented in a single chip using the current VLSI technology (e.g. 1 mu m CMOS). The performance with regard to the delay time is higher than for previously reported architectures. >	computation;inverse dynamics;robot;very-large-scale integration	Somchai Kittichaikoonkit;Michitaka Kameyama;Tatsuo Higuchi	1991		10.1109/ICCD.1991.139984	chip;robot;embedded system;electronic engineering;parallel computing;telecommunications;computer science;inverse problem;theoretical computer science;very-large-scale integration;inverse dynamics;cmos	Robotics	5.7943313727366625	44.8693399716788	20380
4849e14f6ab98cf1a0d0335c48793c14b010b84c	a binary representation for decimal numbers				Peter M. Fenwick	1972	Australian Computer Journal		data mining;decimal representation;decimal128 floating-point format;repeating decimal;decimal32 floating-point format;arithmetic;computer science;binary integer decimal;decimal;decimal data type;decimal floating point	Vision	20.900767902448347	41.72735074333002	20385
4df68de5935cf34f4106c38cb4f464e4a213863f	novel designs of nanometric parity preserving reversible compressor	nanometric circuits;parity preservation;quantum circuits;reversible compressor;reversible logic;theoretical physics	Reversible logic is a new field of study that has applications in optical information processing, low power CMOS design, DNA computing, bioinformatics, and nanotechnology. Low power consumption is a basic issue in VLSI circuits today. To prevent the distribution of errors in the quantum circuit, the reversible logic gates must be converted into fault-tolerant quantum operations. Parity preserving is used to realize fault tolerant in this circuits. This paper proposes a new parity preserving reversible gate. We named it NPPG gate. The most significant aspect of the NPPG gate is that it can be used to produce parity preserving reversible full adder circuit. The proposed parity preserving reversible full adder using NPPG gate is more efficient than the existing designs in term of quantum cost and it is optimized in terms of number of constant inputs and garbage outputs. Compressors are of importance in VLSI and digital signal processing applications. Effective VLSI compressors reduce the impact of carry propagation of arithmetic operations. They are built from the full adder blocks. We also proposed three new approaches of parity preservation reversible 4:2 compressor circuits. The third design is better than the previous two in terms of evaluation parameters. The important contributions have been made in the literature toward the design of reversible 4:2 compressor circuits; however, there are not efforts toward the design of parity preservation reversible 4:2 compressor circuits. All the scales are in the nanometric criteria.	adder (electronics);arithmetic logic unit;bioinformatics;cmos;central processing unit;dna computing;digital signal processing;fault tolerance;garbage collection (computer science);information processing;lazy evaluation;logic gate;quantum circuit;reversible computing;software propagation;very-large-scale integration	Soghra Shoaei;Majid Haghparast	2014	Quantum Information Processing	10.1007/s11128-014-0762-6	toffoli gate;theoretical computer science;mathematics;algorithm	EDA	16.862391712688478	45.14170684229355	20388
f01769309a9a174e9ea6f734784ade54846c4691	speeding up scalar multiplication using a new signed binary representation for integers	information security;efficient algorithm;wireless network;public key cryptosystem;secure communication;elliptic curve cryptosystems ecc;signed binary representation sbr;scalar multiplication	Scalar multiplication dP and gP+hQ are important in encryption, decryption and signature in information security and wireless network. The speed of computation of scalar multiplication is significant for related applications. In this paper, a new signed binary representation (SBR) for integers called complementary code method (CC) is proposed, which has minimum weight and needs less memory. An efficient algorithm using CC method for computing dP is shown also. According to analyzing and comparing to the other methods, this algorithm is the better one in window methods and is the simplest for applying in software and hardware. By applying joint representation in computing gP+hQ, new algorithm using CC method has the least joint weight compared to other methods mentioned in this paper. So, the new SBR can efficiently speed up the computation of scalar multiplication dP and gP+hQ and can be widely used in secure communication for improving the speed of encryption and signature.		Bangju Wang;Huanguo Zhang;Zhang-yi Wang;Yuhua Wang	2007		10.1007/978-3-540-73417-8_35	secure communication;computer science;information security;theoretical computer science;wireless network;scalar multiplication	NLP	9.220931223566115	43.004661061988195	20462
227b38df3640f5e05b3246b0e842b7c2bcc5f97e	dynamic fault detection in digital systems using dynamic voltage scaling and multi-temperature schemes	bist techniques dynamic fault detection digital systems dynamic voltage scaling multitemperature schemes transient faults nanometer products variable power supply voltage clock frequency semiempirical analytical model logic level low cost fault simulation delay variation on chip availability dvs;multitemperature schemes;fault detection digital systems dynamic voltage scaling power supplies temperature built in self test testing clocks frequency analytical models;fault simulation;variable power supply voltage;low cost fault simulation;dynamic voltage scaling;dynamic fault detection;delay variation;self testing;power supply;chip;integrated circuit testing built in self test digital systems fault simulation;built in self test;logic level;nanometer products;transient faults;digital systems;fault detection;integrated circuit testing;transient fault;semiempirical analytical model;on chip availability;clock frequency;analytical model;digital modulation;bist techniques;dvs	Detection of physical defects (or transient faults) in nanometer products is very challenging. Parametric test, using variable power supply voltage, clock frequency and temperature can be rewarding. However, their impact on digital system performance needs to be evaluated. In this paper, a novel semi-empirical analytical model to compute, at logic level, the impact of power supply voltage variations (DeltaVDD) and/or of temperature variations (DeltaT) on speed response of a digital module is proposed. The model allows low-cost fault simulation. Moreover, it is shown that delay variation can be emulated either by a DeltaVDDi or a DeltaTj variation. The on-chip availability of multiple VDD values in products with DVS (dynamic voltage scaling) opens opportunities for novel BIST techniques. A new DVS-based BIST approach is proposed and its ability to detect and diagnose resistive open defects is ascertained	built-in self-test;clock rate;digital electronics;dynamic voltage scaling;emulator;fault detection and isolation;image scaling;logic level;power supply;semiconductor industry;simulation;value-driven design	Marcial Jesús Rodríguez-Irago;Juan J. Rodríguez-Andina;Fabian Vargas;Jorge Semião;Isabel C. Teixeira;João Paulo Teixeira	2006	12th IEEE International On-Line Testing Symposium (IOLTS'06)	10.1109/IOLTS.2006.25	chip;embedded system;electronic engineering;real-time computing;logic level;telecommunications;computer science;engineering;clock rate;fault detection and isolation;algorithm	EDA	21.41873920937411	56.02672275695805	20495
604b23c3dc92f014e8052e69bd155ace501c464b	2t1m-based double memristive crossbar architecture for in-memory computing		The recent discovery of the memristor has renewed the interest for fast arithmetic operations via high-radix numeric systems. In this direction, a conceptual solution for high-radix memristive arithmetic logic units (ALUs) was recently published. The latter combines CMOS circuitry for data processing and a reconfigurable “segmented” crossbar memory block. In this paper we build upon such a conceptual design and propose a 3D extension of the classic crossbar topology via 2T1M cross-points which still permits the parallel creation of partial products for faster multiplication with lower circuit complexity. Furthermore, we present a binary to high-radix data conversion circuit to complement the stateprogramming module of the previous work. A simulation-based validation of read/write multi-level memory operations from/to the 2T1M 3D memristive crossbar was performed using SPICE and a thresholdtype switching model of a bipolar voltage-controlled memristor. Such realization of in-memory computations could lead to faster arithmetic algorithms in future memristive ALUs.	algorithm;amplifier;arithmetic logic unit;cmos;circuit complexity;computation;crossbar switch;electronic circuit;in-memory database;in-memory processing;memristor;spice;simulation	Ioannis Vourkas;Georgios Papandroulidakis;Georgios Ch. Sirakoulis;Angel Abusleme	2016	IJUC		architecture;parallel computing;crossbar switch;computer science;in-memory processing	EDA	12.298447249884616	47.71565242146219	20496
b0424b8976cb0e094418f3354c2862460861b430	system utilization of large-scale integration	fabrication;control systems;logic arrays;large arrays;data processing;logic circuits;computer organization;size control;large scale integration;logic gates;registers;logic partitioning computer organization functional partitioning large arrays large scale integration;large scale integration logic arrays logic gates control systems logic circuits military computing size control fabrication data processing registers;logic partitioning;functional partitioning;military computing	A new approach to a computer organization promises that will illustrate the effective use of LSI arrays will very effective utilization of the LSI technology. Functional partitionemploy functional partitioning of both the data path ing of both the data path and control is employed. A dramatic reduction in array pin requirements by a factor of two control structures Functi pritioin ofche or more is achieved. Arrays as small as a few dozen gates can be control structure is unique with this computer's archieffectively utilized. The total system is exceedingly flexible in both tecture and is the key to a very favorable utilization performance and instruction set. of arrays from 30 gates upward in size. ... The circuits for array use are emitter coupled logic	control flow;emitter-coupled logic;lambert's cosine law;microarchitecture;requirement;tiling array;very-large-scale integration	Saul Y. Levy;Robert J. Linhardt;Henry S. Müller;Robert D. Sidnam	1967	IEEE Trans. Electronic Computers	10.1109/PGEC.1967.264742	computer architecture;electronic engineering;data processing;logic gate;computer science;engineering;theoretical computer science	Arch	8.61479069907857	49.21736921904753	20515
033eb46b4910e0db20012f578aa9e96731721f11	design of pop-11 (pdp-11 on programmable chip)	ide interface;unix v6;fpga base-board;ansi c development tool;compatible processor;programmable chip;logic design;cyclones;logic gates;field programmable gate arrays;chip;operating systems	We developed POP-11/40 which is PDP-11/40 compatible processor, implemented it on an FPGA. We also designed ANSI C development tools an FPGA base-board which has SRAM, Serial and IDE interface, and run UNIX V6 on this system. In this paper, we describe a feature of a design of POP-11/40, base-board and UNIX V6.	ansi c;field-programmable gate array;pdp-11;pop-11;static random-access memory;version 6 unix	Yoshihiro Iida;Naohiko Shimizu	2004	ASP-DAC 2004: Asia and South Pacific Design Automation Conference 2004 (IEEE Cat. No.04EX753)	10.1145/1015090.1015244	chip;embedded system;electronic engineering;parallel computing;logic synthesis;logic gate;programmable logic array;computer science;electrical engineering;operating system;programmable logic device;simple programmable logic device;programming language;programmable array logic;register-transfer level;cyclone;field-programmable gate array;computer engineering	EDA	6.449747988847873	49.42119479456681	20529
eeba8d877e03489b1935b862329835adaaca1b71	perspectives of using oscillators for computing and signal processing		It is an intriguing concept to use oscillators as fundamental building blocks of electronic computers. The idea is not new, but is currently subject to intense research as a part of the quest for ’beyond Moore’ electronic devices. In this paper we give an engineering-minded survey of oscillator-based computing architectures, with the goal of understanding their promise and limitations for next-generation computing. We will mostly discuss non-Boolean, neurally-inspired computing concepts and put the emphasis on hardware and on circuits where the oscillators are realized from emerging, nanoscale building blocks. Despite all the promise that oscillatory computing holds, existing literature gives very few clear-cut arguments about the possible benefits of using oscillators in place of other analog nonlinear circuit elements. In this survey we will argue for finding the rationale of using oscillatory building blocks and call for benchmarking studies that compare oscillatory computing circuits to level-based (analog) implementations.	benchmark (computing);boolean algebra;british informatics olympiad;computation;computer;darpa grand challenge;design rationale;ibm notes;interaction;linear circuit;low-power broadcasting;microsoft outlook for mac;neuromorphic engineering;nonlinear system;oscillator (cellular automaton);out there;quantum computing;signal processing	György Csaba;Wolfgang Porod	2018	CoRR		implementation;theoretical computer science;electrical element;electronic circuit;signal processing;benchmarking;electronics;nonlinear system;oscillation;computer science	EDA	5.4510474946969625	40.527202166830584	20552
080e63677a7de661d97adf1ea5b101b54f0b3090	an improved rom compression technique for direct digital frequency synthesizers	read only storage;interpolation;frequency synthesizers;data compression;neck;very large scale integration;taylor series linear interpolation method;direct digital synthesis;nonlinear distortion;counting circuits;taylor series linear interpolation method rom compression techniques rom based dds direct digital frequency synthesizers;gold;rom compression techniques;energy consumption;linear interpolation;series mathematics;direct digital frequency synthesizers;direct digital frequency synthesizer;read only memory frequency synthesizers counting circuits table lookup nonlinear distortion very large scale integration interpolation gold neck energy consumption;table lookup;rom based dds;read only memory;data compression direct digital synthesis read only storage interpolation series mathematics;taylor series	In this paper a comparison between the famous ROM compression techniques, for ROM-based direct digital frequency synthesizers, was performed. Inaccuracies of previous comparisons are pointed out. An improved ROM compression technique based on the Taylor-series linear interpolation method, developed by Bellaouar et al. (see IEEE J Solid-State Circuits, vol. 35, no. 3, p. 385-390, 2000), is presented. Simulation results show that compared to previously reported compression techniques, this improvement can achieve considerable reductions in the ROM size.	normalized frequency (unit);read-only memory	Mostafa M. El Said;M. L. Elmasry	2002		10.1109/ISCAS.2002.1010734	data compression;gold;nonlinear distortion;electronic engineering;direct digital synthesizer;computer hardware;interpolation;computer science;electrical engineering;taylor series;mathematics;very-large-scale integration;linear interpolation;series;read-only memory	HCI	14.539916538815925	43.82027157586546	20575
db7f816ca23baf1ad698d362983aa4da2831b8ff	efficient analysis of systems with multiple states	computational complexity systems analysis multiple states multiple performance levels communication networks computer systems multistate multivalued decision diagrams multistate systems multistate components;decision diagrams;multiple performance levels;object oriented methods;communication networks;probability;decision diagram;computer systems;multistate multivalued decision diagrams;systems analysis;community networks;computational complexity;probability computational complexity decision diagrams object oriented methods;efficiency analysis;multistate components;multiple states;multistate systems;data structures boolean functions failure analysis computational complexity application software communication networks computer networks fault trees binary decision diagrams boolean algebra	A multistate system is a system in which both the system and its components may exhibit multiple performance levels (or states) varying from perfect operation to complete failure. Examples abound in real applications such as communication networks and computer systems. Analyzing the probability of the system being in each state is essential to the design and tuning of dependable multistate systems. The difficulty in analysis arises from the non-binary state property of the system and its components as well as dependence among those multiple states. This paper proposes a new model called multistate multivalued decision diagrams (MMDD) for the analysis of multistate systems with multistate components. The computational complexity of the MMDD-based approach is low due to the nature of the decision diagrams. An example is analyzed to illustrate the application and advantages of the approach.	algorithmic efficiency;computational complexity theory;computer;decision problem;dependability;diagram;telecommunications network	Liudong Xing	2007	21st International Conference on Advanced Information Networking and Applications (AINA '07)	10.1109/AINA.2007.62	systems analysis;influence diagram;computer science;theoretical computer science;machine learning;probability;computational complexity theory;algorithm;statistics	EDA	24.058247875252984	45.31381067489809	20602
5bf37616aafdd30c60f2b0256665ff6687dc0900	a two-level interleaving architecture for serial convolvers	tolerancia falta;traitement signal;correlacion;numerical sequence;sequences;arquitectura red;fonction filtre;fault tolerant;convolution;suite numerique;estudio comparativo;reseau pipeline;sucesion numerica;pipeline arithmetic convolution correlators vlsi digital signal processing chips sequences fault tolerance cmos digital integrated circuits parallel architectures;circuit vlsi;convolucion;indexing terms;pipeline network;architecture reseau;correlators;feasibility;etude comparative;vlsi circuit;parallel architectures;cmos digital integrated circuits;filter function;signal processing;fault tolerance;comparative study;pipelining two level interleaving architecture serial convolvers bit serial architecture correlation long numerical sequences long filter functions input samples throughput fault tolerance computing performance vlsi technology cascadability cmos serial parallel multiplier;vlsi;digital signal processing chips;interleaved codes convolvers signal processing tomography diffraction computer architecture fourier transforms scattering shape measurement performance analysis;network architecture;correlation;circuito vlsi;red oleoductos;procesamiento senal;pipeline arithmetic;tolerance faute;practicabilidad;faisabilite	In this correspondence, we present a bit-serial architecture for convolving/correlating long numerical sequences by long filter functions. Because of its two-level interleaving structure, the proposed device does not require “wait cycles” between consecutive input samples. As a result, it achieves the highest possible throughput. Cascadability, fault tolerance, feasibility in VLSI technology, and computing performances are discussed and analyzed.	fault tolerance;forward error correction;numerical analysis;performance;serial communication;throughput;very-large-scale integration;wait state	Francescomaria Marino	1999	IEEE Trans. Signal Processing	10.1109/78.757248	embedded system;fault tolerance;parallel computing;computer science;signal processing;mathematics	HPC	13.610073247672503	37.569263222223576	20605
a67acb426ba930d53913d2a173dd845a94d937eb	the application kernel approach - a novel approach for adding smp support to uniprocessor operating systems		The current trend of using multiprocessor computers for server applications requires operating system adaptations to take advantage of more powerful hardware. However, modifying large bodies of software is very costly and time consuming, and the cost of porting an operating system to a multiprocessor might not be motivated by the potential performance benefits. In this paper we present a novel method, the application kernel approach, for adaption of an existing uniprocessor kernel to multiprocessor hardware. Our approach considers the existing uniprocessor kernel as a ‘black box’, to which no or very small changes are made. Instead, the original kernel runs operating system services unmodified on one processor whereas the other processors execute applications on top of a small custom kernel. We have implemented the application kernel for the Linux operating system, which illustrates that the approach can be realized with fairly small resources. We also present an evaluation of the performance and complexity of our approach, where we show that it is possible to achieve good performance while at the same time keeping the implementation complexity low. Copyright c © 2006 John Wiley & Sons, Ltd.	bootstrapping (statistics);central processing unit;computer;cyclomatic complexity;daemon (computing);distributed computing;feedback;free software license;gnu;john d. wiley;kernel (operating system);linux;mosix;operating system;prototype;scalability;server (computing);symmetric multiprocessing;system call;uniprocessor system	Simon Kågström;Håkan Grahn;Lars Lundberg	2006	Softw., Pract. Exper.	10.1002/spe.732		OS	-4.252937825907049	45.98014013089793	20630
604dcd85d3ac105ef1cc583f9de72715f9c2f3b3	a full-parallel digital implementation for pre-trained nns	digital signal processing;csd encoding;optimization algorithm;canonic signed digit encoding;neural networks;bit serial distributed arithmetic;high performance computing;parallel many multiplier structure complexity;cost saving;bit serial parallel neural network implementation;canonic signed digit;digital filter;computer architecture;neural chips;parallel digital hardware implementation;trained neural networks;parallel architectures;vectors;efficient implementation;design environment;application specific integrated circuits;hardware cost savings;digital filters;neural networks hardware parallel architectures costs digital filters digital arithmetic signal processing algorithms encoding computer architecture high performance computing;circuit optimisation neural chips parallel processing distributed arithmetic matrix multiplication vectors pipeline arithmetic field programmable gate arrays application specific integrated circuits;distributed arithmetic;pre trained nn;asic realization;full parallel digital implementation;digital arithmetic;matrix multiplication;high speed bit level pipeline operation;fpga realization;parallel architecture;field programmable gate arrays;bit level pattern coincidences;circuit optimisation;signal processing algorithms;optimal algorithm;automatic neural network design environments full parallel digital implementation pre trained nn trained neural networks parallel architectures parallel digital hardware implementation parallel many multiplier structure complexity bit serial parallel neural network implementation hardware cost savings digital filters bit serial distributed arithmetic matrix vector multiplier optimization algorithm csd encoding canonic signed digit encoding bit level pattern coincidences high speed bit level pipeline operation fpga realization asic realization;encoding;automatic neural network design environments;high speed;hardware implementation;pipeline arithmetic;matrix vector multiplier;parallel processing	I n m a n y applications the most significant advantages of neural networks come mainly f r o m their parallel architectures ensuring rather high operation speed. T h e dif icult ies of parallel digital hardware implementat ion arise mostly f rom the high complexity of the parallel many-multiplier structure. This paper suggests a n e w bit-serial/parallel neural network implementation method f o r pre-trained networks. The method makes possible significant hardware cost savings. The proposed approach which is based on the results of a previously suggested method for e f i c i en t implementation of digital filters uses bit-serial distribvted aritlimetic. T h e ef ic ient implementation of a matrix-vector multiplier is based o n a n optimization algorithm which utilizes the advantages of C S D (Canonic Signed Digit) encoding and bit-level pattern coincidences. T h e resulting architecture performs full-precision computation and allows high-speed bit-level pipe-line operation. T h e proposed approach seems to be a promising one f o r FPGA and ASIC realization of pre-trained neural networks and can be integrated into automatic nevral network design environments. However: these irnplerrientation methods can be useful in m a n y other fields of digital signal processing.	algorithm;application-specific integrated circuit;artificial neural network;bit-level parallelism;computation;digital electronics;digital filter;digital signal processing;field-programmable gate array;mathematical optimization;network planning and design;serial communication;serial computer	Tamás Szabó;Lörinc Antoni;Gábor Horváth;Béla Fehér	2000		10.1109/IJCNN.2000.857873	computer architecture;parallel computing;digital filter;computer science;theoretical computer science;machine learning;artificial neural network	Robotics	12.166312530435821	44.364105418996175	20652
0a126e92be5ac938329074043416bd2d82a5a6a7	design automation methodology and tools for superconductive electronics		Josephson junction-based superconducting logic families have been proposed to implement analog and digital signals, which can achieve low energy dissipation and ultra-fast switching speed. There are two representative technologies: DC-biased RSFQ (rapid single flux quantum) technology and its variants that achieve a verified speed of 370 Ghz, and AC-biased AQFP (adiabatic quantum-flux-parametron) that achieves an energy dissipation near quantum limits. Despite extraordinary characteristics of the superconducting logic families, many technical challenges remain, including the choice of circuit fabrics and architectures that utilize the SFQ technology and the development of effective design automation methodologies and tools. This paper presents our work on developing design flows and tools for DC- and AC-biased SFQ circuits, leveraging unique characteristics and design requirements of the SFQ logic families. More precisely, physical design algorithms, including placement, clock tree routing, and signal routing algorithms targeting RSFQ circuits are presented first. Next, a majority/minority gate-based automatic synthesis framework targeting AQFP logic circuits is described. Finally, experimental results to demonstrate the efficacy of the proposed framework and tools are presented.		Massoud Pedram;Yanzhi Wang	2018	2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)	10.1145/3240765.3243470	electronic engineering;computer science;electronics;digital signal;physical design;logic family;rapid single flux quantum;design flow;logic gate;electronic design automation	EDA	14.6981469387411	56.771030164285534	20654
468f408e5eddcbef52ca2e075e9a6dba653efeff	scale in chip interconnect requires network technology	cmos technology;thin film;clocks;wires capacitance scalability delay cmos technology thin film transistors timing joining processes clocks network on a chip;wires;chip;joining processes;thin film transistors;capacitance;scalability;network on a chip;timing	"""Continued scaling of CMOS has lead to a problem of scale as gates are faster than light travelling across a chip. Scalability used to be the hallmark of CMOS. Half the size, double the speed, half the power etc.. Today, transistors can leak as much current as they drive, and wires are no longer """"thin film technology"""" approximated by plate capacitance over ground. Today wires are much thicker than wide and have significantly more capacitance (-coupling) with their neighbors than over ground. A """"short"""" wire (from one gate to a neighboring one) can be a stub of a few 100nm, while a long wire can connect an IP block with a processor one centimeter away. That is a factor of 100000, which represents a problem of scale and requires fundamentally different solutions. Scalability can be addressed by scaling existing techniques, while problems of scale require new approaches. We discuss problems of scale in the context of chip interconnect."""	3d film;approximation algorithm;cmos;ip address blocking;image scaling;scalability;transistor	Enno Wein	2006	2006 International Conference on Computer Design	10.1109/ICCD.2006.4380813	chip;embedded system;electronic engineering;scalability;thin-film transistor;telecommunications;computer science;engineering;electrical engineering;capacitance;network on a chip;cmos;thin film	EDA	13.770081340487158	56.70901374246923	20702
66291370bfe24a5b4614e08b25ae9fedc76b00b6	online fault tolerance for fpga logic blocks	tolerancia falta;field programmable gate arrays fpga;logic faults;on line systems;reliability adaptive computing fault tolerance field programmable gate arrays fpga reconfigurable computing reconfigurable systems;field programmable gate array;adaptive computing systems;reconfigurable systems;fiabilidad;reliability;durabilite;reseau logique;reconfigurable system;environnement hostile;fault tolerant;execution time;fault tolerance field programmable gate arrays reconfigurable logic logic arrays hardware logic testing fault diagnosis adaptive systems adaptive arrays programmable logic arrays;reconfigurable computing;fpga logic blocks;reconfigurable architectures;reutilizacion;implementation;orca 2c series fpga;fault tolerant techniques;durabilidad;logic circuits;sistema n niveles;red puerta programable;systeme adaptatif;reseau porte programmable;reconfigurable architectures adaptive systems fault diagnosis field programmable gate arrays logic circuits logic testing;reuse;medio ambiente hostil;fault tolerant system;detection defaut;online fault tolerance;adaptive systems;durability;systeme n niveaux;systeme en ligne;fiabilite;fault tolerance;logic testing;multilevel system;adaptive system;sistema tolerando faltas;adaptive computing;sistema adaptativo;temps execution;systeme tolerant les pannes;reconfigurable systems online fault tolerance fpga logic blocks adaptive computing systems reconfigurable hardware field programmable gate arrays fault tolerant techniques logic faults defective logic blocks fault diagnosis orca 2c series fpga reconfigurable computing;field programmable gate arrays;hostile environment;tiempo ejecucion;implementacion;online testing;deteccion imperfeccion;architecture reconfigurable;reconfigurable hardware;tolerance faute;defective logic blocks;reutilisation;red logica;fault diagnosis;defect detection;logic array	Most adaptive computing systems use reconfigurable hardware in the form of field programmable gate arrays (FPGAs). For these systems to be fielded in harsh environments where high reliability and availability are a must, the applications running on the FPGAs must tolerate hardware faults that may occur during the lifetime of the system. In this paper, we present new fault-tolerant techniques for FPGA logic blocks, developed as part of the roving self-test areas (STARs) approach to online testing, diagnosis, and reconfiguration . Our techniques can handle large numbers of faults (we show tolerance of over 100 logic faults via actual implementation on an FPGA consisting of a 20 times 20 array of logic blocks). A key novel feature is the reuse of defective logic blocks to increase the number of effective spares and extend the mission life. To increase fault tolerance, we not only use nonfaulty parts of defective or partially faulty logic blocks, but we also use faulty parts of defective logic blocks in nonfaulty modes. By using and reusing faulty resources, our multilevel approach extends the number of tolerable faults beyond the number of currently available spare logic resources. Unlike many column, row, or tile-based methods, our multilevel approach can tolerate not only faults that are evenly distributed over the logic area, but also clusters of faults in the same local area. Furthermore, system operation is not interrupted for fault diagnosis or for computing fault-bypassing configurations. Our fault tolerance techniques have been implemented using ORCA 2C series FPGAs which feature incremental dynamic runtime reconfiguration	built-in self-test;clock rate;computer-aided design;downtime;dummy variable (statistics);fastest;fault tolerance;field-programmable gate array;interrupt;minimax;orca;online and offline;real-time recovery;reconfigurable computing;routing;stellar classification;system on a chip;tiling window manager	John M. Emmert;Charles E. Stroud;Miron Abramovici	2007	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2007.891102	embedded system;fault tolerance;electronic engineering;real-time computing;logic optimization;computer science;engineering;adaptive system	EDA	19.649244624404165	51.095427782988395	20892
1a6b08677c8e2db0fba3d0e31ad6352723e6a4ba	spatial and temporal granularity limits of body biasing in utbb-fdsoi	emerging memory technology;drp design temporal granularity utbb fdsoi soi technology performance characteristics electrical task substrate potential dynamic voltage scaling finer island sizes body bias islands body bias combinations energy efficiency timing constraints combination based analysis tool optimized body bias island partitions body biasing levels optimized body bias assignments dynamic body biasing dynamically switching body biases power consumption additional circuitry switching overheads application specific switching strategies frequency scaling scenario forward body biasing dynamic reconfigurable processor;physical unclonable functions pufs;silicon on insulator circuit analysis computing reconfigurable architectures;mrams;switches layout clocks optimization delays power demand	Advances in SOI technology such as STMicro's 28nm UTBB-FDSOI enabled a renaissance of body biasing. Body biasing is a fast and efficient technique to change power and performance characteristics. As the electrical task to change the substrate potential is small compared to Dynamic Voltage Scaling, much finer island sizes are conceivable. This however creates new challenges in regard to design partitioning into body bias islands and body bias combinations across such designs. These combinations should be chosen so that energy efficiency improves while maintaining timing constraints. We introduce a combination based analysis tool to find optimized body bias island partitions and body biasing levels. For such partitions, optimized body bias assignments for static, programmable and dynamic body biasing can be computed. The overheads incurred by dynamically switching body biases are estimated to yield actual improvements and to give an upper bound for the power consumption of required additional circuitry. Based on these partitionings and the switching overheads, optimized application specific switching strategies are computed. The effectiveness of this method is demonstrated in a frequency scaling scenario using forward body biasing on a Dynamic Reconfigurable Processor (DRP) design. We show that leakage can be greatly reduced using the proposed methods and that dynamic body biasing can be beneficial even at small time periods.	2.5d;biasing;disaster recovery plan;dynamic voltage scaling;electronic circuit;frequency scaling;image scaling;reconfigurable computing;renaissance;silicon on insulator;spectral leakage	Johannes Maximilian Kühn;Dustin Peterson;Hideharu Amano;Oliver Bringmann;Wolfgang Rosenstiel	2015	2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)	10.7873/DATE.2015.1051	electronic engineering;real-time computing;computer hardware;engineering	EDA	16.205870155077594	57.00213208007687	20909
33999389894e092789ceba10335232f41cfbf22a	hybrid nanoelectronics: future of computer technology	moores law;logic design;computer systems;hybrid;carbon nanotube;single electron transistor;fault tolerance;nanoelectronics;resonant tunneling devices;quantum cellular automata;circuit;circuits;microelectronics;device;architecture;scientific knowledge;noise;王慰 刘明 andrew hsu 纳米电子学 混种 断层差 微电子学 回路 计算机系统 hybrid nanoelectronics future of computer technology	Nanotechnology may well prove to be the 21st century’s new wave of scientific knowledge that transforms people’s lives. Nanotechnology research activities are booming around the globe. This article reviews the recent progresses made on nanoelectronic research in US and China, and introduces several novel hybrid solutions specifically useful for future computer technology. These exciting new directions will lead to many future inventions, and have a huge impact to research communities and industries.	computer	Wei Wang;Andrew Hsu	2006	Journal of Computer Science and Technology	10.1007/s11390-006-0871-5	nanoelectronics;fault tolerance;carbon nanotube;hybrid;computer science;noise;architecture;microelectronics	Theory	12.103500360798716	58.45905242234868	20926
9d04853df94cba8c2d0695223f123835b27357ad	broadcasting secure messages via optimal independent spanning trees in folded hypercubes	distributed protocol;fault tolerant;independent spanning trees;fault tolerant broadcasting;folded hypercubes;interconnection network;hamming distance;interconnection networks;spanning tree;edge disjoint spanning trees	"""Fault-tolerant broadcasting and secure message distribution are important issues for network applications. It is a common idea to design multiple spanning trees with a specific property in the underlying graph of a network to serve as a broadcasting scheme or a distribution protocol for receiving high levels of fault-tolerance and security. An n-dimensional folded hypercube, denoted by FQ""""n, is a strengthening variation of hypercube by adding additional links between nodes that have the furthest Hamming distance. In, [12], Ho(1990) proposed an algorithm for constructing n+1 edge-disjoint spanning trees each with a height twice the diameter of FQ""""n. Yang et al. (2009), [29] recently proved that Ho's spanning trees are indeed independent, i.e., any two spanning trees have the same root, say r, and for any other node v r, the two different paths from v to r, one path in each tree, are internally node-disjoint. In this paper, we provide another construction scheme to produce n+1 independent spanning trees of FQ""""n, where the height of each tree is equal to the diameter of FQ""""n plus one. As a result, the heights of independent spanning trees constructed in this paper are shown to be optimal."""	file spanning	Jinn-Shyong Yang;Hung-Chang Chan;Jou-Ming Chang	2011	Discrete Applied Mathematics	10.1016/j.dam.2011.04.014	fault tolerance;combinatorics;discrete mathematics;hamming distance;minimum degree spanning tree;spanning tree;minimum spanning tree;mathematics;distributed computing;distributed minimum spanning tree;shortest-path tree	Theory	22.434094658870343	34.81708044400202	20932
1335b4e25e4b9b8df8d5b1d8d8d2b145bdc6f5fc	memory reduction of i/sub ddq/ test compaction for internal and external bridging faults	i ddq test compaction;weighted random sequences memory reduction i sub ddq test compaction internal bridging faults external bridging faults cmos circuits test application time reduction iddq test sequence reassignment method sequential circuits;automatic testing cmos logic circuits sequential circuits logic testing integrated circuit testing fault simulation;iddq test sequence;circuit faults;fault simulation;time measurement;random sequences;automatic testing;sequential circuits;cmos circuits;sequential analysis;memory reduction;reassignment method;current measurement;test application time reduction;cmos memory circuits;compaction;cmos logic circuits;random sequence;weighted random sequences;logic testing;circuit testing circuit faults sequential analysis compaction sequential circuits cmos memory circuits current measurement time measurement benchmark testing random sequences;integrated circuit testing;external bridging faults;circuit testing;internal bridging faults;benchmark testing	I/sub DDQ/ testing is an effective method for bridging faults of CMOS circuits. Since the measurement of current in I/sub DDQ/ testing takes a long time, a short test sequence is strongly desirable for reducing test application time. In this paper we present a test compaction method for an I/sub DDQ/ test sequence using a reassignment method for all bridging faults in sequential circuits. Since a large memory space is required to treat all bridging faults, an effective fault list is required. We propose the test compaction method using an assignment list of signal values. The proposed method is realized with small size memory and runs fast for many large sequential circuits. Experimental results on the benchmark circuits show that it is effective in reducing test length for given weighted random sequences.	bridging (networking);data compaction	Toshiyuki Maeda;Kozo Kinoshita	2000		10.1109/ATS.2000.893648	compaction;benchmark;electronic engineering;parallel computing;real-time computing;engineering;sequential analysis;random sequence;sequential logic;statistics;time	EDA	21.204367361876713	51.99238533442386	20933
98e48ba519f321766ce59acbe0003e6cbf4092d9	multiple fault testing using minimal single fault test set for fanout-free circuits	multiple stuck at faults;methode essai;nor gates;circuit faults;minimal single fault test set;integrated circuit;detection panne;time complexity;very large scale integration;failure detection;vlsi logic circuits;circuit vlsi;circuit testing circuit faults electrical fault detection fault detection very large scale integration combinational circuits computer science test pattern generators circuit simulation laboratories;circuito integrado;circuito logico;multiple fault testing;or gates;algorithme;nand gates;single stuck at fault test;algorithm;fanout free circuits;circuit simulation;complexite temps;vlsi circuit;test pattern generators;pseudo tree circuits;circuit logique;parity checkers;fault detection;logic testing;integrated circuit testing;vlsi;circuit testing;computer science;integrated logic circuits;circuito vlsi;test method;vlsi fault location integrated circuit testing integrated logic circuits logic testing;complejidad tiempo;time complexity and gates or gates not gates nand gates vlsi logic circuits multiple fault testing minimal single fault test set fanout free circuits single stuck at fault test multiple stuck at faults nor gates pseudo tree circuits parity checkers;deteccion falla;logic circuit;not gates;circuit integre;electrical fault detection;and gates;algoritmo;combinational circuits;fault location;metodo ensayo	The authors examine the properties of fanout-free circuits, and develop an algorithm to generate single stuck-at fault test experiments that also detect all multiple stuck-at faults. These experiments are shown to be minimal in size. Results demonstrate that elaborate selection of nonsensitizing test pattern guarantees the detection of all multiple stuck-at faults using single stuck-at test experiments. The algorithm is deterministic, and will produce test sets for tree circuits containing any mixture of AND, OR, NOT, NAND, and NOR gates. The results can be extensively applied to multiple stuck-at fault detection for pseudo tree circuits such as parity checkers. The time complexity of the algorithm is determined to the O(n/sup 2/), where n is the number of gates in the circuit. >	fan-out;test set	Wen-Ben Jone;Patrick H. Madden	1993	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.184851	electronic engineering;real-time computing;fault coverage;computer science;stuck-at fault;automatic test pattern generation;mathematics;very-large-scale integration;algorithm	EDA	21.24268869657697	48.995490093896144	20964
fec0a9e12706b2c744bd92e3d8a8726b443e1c25	a fast mpeg video encryption algorithm	des;mpeg video;mpeg codec;multimedia data;multimedia data security;mpeg video encryption	Multimedia data secmity is important for multimedia commerce. Previous cryptography studies have focused on text data. The encryption algorithms devdoped to secure text data may not be suitable to multimedia applications becattse of large data sizes and real time constraint. For multimedia applications, light weight encryption algorithms are attractive. We present a novel MPEG Video Encryption Algorithm, called VEA The basic idea of VEA is to use a secret key randomly changing the sign bits of all of the DCT coefficients of MPEG video. VEA’S encryption effects are achieved by the IDCT during MPEG video decompression processing. VEA adds minimum overhead to MPEG codecj one Mm&e XOR operation to each none zero DCT coefficient. A software implementation of VEA is fast enough to meet the real time requirement of MPEG video applications. Our experimental results show that VEA achieves satisfying results. We believe that it can be used to secure video-on-demand, tideo conferencing and video email applications.	algorithm;coefficient;cryptography;data compression;discrete cosine transform;encryption;exclusive or;key (cryptography);moving picture experts group;overhead (computing);randomness;text corpus;video email	Changgui Shi;Bharat K. Bhargava	1998		10.1145/290747.290758	codec;multiview video coding	Security	8.564479468307573	36.63309906760563	20974
b131f164b14cdfea62272331408bff04831dd28a	parallel implementation of linear algebra problems on dawning-1000	linear algebra;parallel algorithm;symmetric matrix;cholesky factorization;complex data;matrix multiplication;parallel implementation;numerical linear algebra;lu factorization	In this paper, some parallel algorithms are described for solving numerical linear algebra problems on Dwning-1000. They include matrix multiplication,LU factorization of a dense matrix, Cholesky factorization of a symmetric matrix, and eigendecomposition of symmetric matrix for real and complex data types. These programs are constructed based on fast BLAS library of Dawning-1000 under NX environment. Some comparison results under different parallel environments and implementing methods are also given for Cholesky factorization. The execution time, measured performance and speedup for each problem on Dawning-1000 are shown. For matrix multiplication andLU factorization, 1.86GFLOPS and 1.53GFLOPS are reached.	blas;cholesky decomposition;lu decomposition;matrix multiplication;nx bit;numerical linear algebra;parallel algorithm;qr decomposition;run time (program lifecycle phase);sparse matrix;speedup	Xuebin Chi	1998	Journal of Computer Science and Technology	10.1007/BF02946602	square root of a matrix;eigendecomposition of a matrix;incomplete cholesky factorization;quadratic sieve;sparse matrix;incomplete lu factorization;lu decomposition;matrix pencil;nonnegative matrix;matrix multiplication;centrosymmetric matrix;theoretical computer science;linear algebra;invertible matrix;parallel algorithm;positive-definite matrix;factorization of polynomials;numerical linear algebra;minimum degree algorithm;euler's factorization method;matrix decomposition;cholesky decomposition;block matrix;diagonal matrix;transpose;symmetric matrix;complex data type	HPC	-2.2862673366708806	38.535146832016295	20975
fa91c5a0b420cb3abf409ea111bde4bc0d2f1654	behavioral simulation for analog system design verification	analog system design verification;archsim;analysis modes;frequency domain analysis;space exploration;archsim behavioral simulation analog system design verification verification process performance specifications design space exploration time domain analysis frequency domain analysis sensitivity analysis distortion analysis design specifications analysis modes;circuit simulation circuit synthesis analog circuits application specific integrated circuits design methodology integrated circuit synthesis space exploration design optimization frequency domain analysis sensitivity analysis;design optimization;time domain analysis;design specifications;analog circuits;circuit simulation;analogue integrated circuits;verification process;application specific integrated circuits;sensitivity analysis;system design;electric distortion;mixed analogue digital integrated circuits;integrated circuit synthesis;behavioral simulation;circuit cad;design space exploration;circuit analysis computing;distortion analysis;circuit synthesis;electric distortion circuit analysis computing analogue integrated circuits circuit cad mixed analogue digital integrated circuits frequency domain analysis time domain analysis sensitivity analysis;performance specifications;design methodology	Absl”Synthesis of analog circuits is an emergent field, with efforts focused at the cell level. With the growing trend of mixed ASIC designs that contain significant portions of analog sections, compatible design methodologies in the analog domain are necessary to complement those in the digital domain. The synthesis process requires an associated verification process to ensure that the designs meet performance specifications at the onset. In this paper we present a behavioral simulation methodology for analog system design veri,lfication and design space exploration. The verification task integrates with analog systemlevel synthesis for an integrated synthesis-verification process that avoids expensive post synthesis simulation by invoking external simulators. Thus rapid redesign at the architectural level can be undertaken for design parameter variation and during optimization. The verification suite is composed of a repertoire of analysis modes that include time and frequency domain analysis, sensitivity analysis and distortion analysis. Besides verification of design specifications, these analysis modes are also used to generate metrics for comparison of various architectural choices that could realize a given set of specifications. The implementation is in the form of a behavioral simulator, ARCHSIM.	analogue electronics;application-specific integrated circuit;design space exploration;distortion;domain analysis;emergence;mathematical optimization;onset (audio);simulation;systems design	Brian A. A. Antao;Arthur J. Brodersen	1995	IEEE Trans. VLSI Syst.	10.1109/92.406999	control engineering;electronic engineering;real-time computing;multidisciplinary design optimization;design methods;analogue electronics;computer science;space exploration;high-level verification;runtime verification;application-specific integrated circuit;sensitivity analysis;frequency domain;functional verification;systems design	EDA	10.726006327491602	50.09031196987626	21053
39890345ce10e6b740e3ff0544af0479bcdc8f09	a study on ram requirements of various sha-3 candidates on low-cost 8-bit cpus		In this paper, we compare the implementation costs of various SHA-3 candidates on low-cost 8-bit CPUs by estimating RAM/ROM requirements of them. As a first step toward this kind of research, in our comparison, we make reasonable estimations of RAM/ROM requirements of them which can be done without implementation.	8-bit;central processing unit;random-access memory;requirement;sha-3	Kota Ideguchi;Toru Owada;Hirotaka Yoshida	2009	IACR Cryptology ePrint Archive		parallel computing;computer architecture;sha-3;8-bit;computer science	Crypto	7.983206956879184	45.02023456725159	21119
b98a25e396f4d14cfe8700d01f4367060861a42a	multiple voltage synthesis scheme for low power design under timing and resource constraints	computers power supply;bepress selected works;computer algorithms computer scheduling computers power supply energy consumption;computer scheduling;energy consumption;computer algorithms	In this paper, a tabu-search-based behavior level synthesis scheme is proposed to minimize chip power consumption with resources operating at multiple voltages under the timing and resource constraints. Unlike the conventional methods where only scheduling is considered, our synthesis scheme considers both scheduling and partitioning simultaneously to reduce power consumption due to the functional units as well as the interconnects among them. More importantly, our approach tends to efficiently address a few practical layout problems inherent to multiple voltage designs. In particular, we have configured our solutions as a three-tuple vector to account for both the schedule and the partition. Cycling of the same solutions is prevented by applying a tabu list with an update mechanism enhanced with an aspiration function. In this way, the algorithm can search a large solution space with modest computation effort and fast convergence rate. Experiments with a number of DSP benchmarks show that the proposed algorithms achieve an average power reduction by 49.6%		Ling Wang;Yingtao Jiang;Henry Selvaraj	2005	Integrated Computer-Aided Engineering	10.3233/ica-2005-12405	fair-share scheduling;parallel computing;real-time computing;computer science;theoretical computer science	EDA	15.366613716700959	54.53177279249869	21122
bbfdf374b25d758d9b80e3df9b40b18793e35e06	capture and shift toggle reduction (castr) atpg to minimize peak power supply noise	flip flop toggles;optimisation;peak power supply noise;automatic test pattern generation;flip flops;dynamic test compaction;boolean algebra;optimisation automatic test pattern generation boolean algebra flip flops integrated circuit noise;flip flop toggles capture and shift toggle reduction castr automatic test pattern generation peak power supply noise power reduction dynamic test compaction pseudo boolean optimization;yield loss;castr;pseudo boolean optimization;capture and shift toggle reduction;power reduction;integrated circuit noise;automatic test pattern generation power supplies noise reduction electronic equipment testing performance evaluation compaction benchmark testing logic power engineering and energy power generation;peak power;flip flop	Excessive peak power supply noise (PPSN) causes yield loss problem during test. To reduce PPSN, we proposed a new technique called Capture and Shift Toggle Reduction (CASTR). CASTR performs power reduction during dynamic test compaction so the test length overhead is very small. It also includes pseudo Boolean optimization (PBO) and random-based techniques to improve the results. Experimental results show that we can reduce flip-flop toggles, which is highly correlated with PPSN, by 33.4% during shift mode and 41.2% in capture operation simultaneously for the large ISCAS89 benchmarks.	data compaction;flops;flip-flop (electronics);mathematical optimization;overhead (computing);plate boundary observatory;power supply;switch	Hsiu-Ting Lin;Jen-Yang Wen;James Li;Ming-Tung Chang;Min-Hsiu Tsai;Sheng-Chih Huang;Chili-Mou Tseng	2008	2008 IEEE International Test Conference	10.1109/TEST.2008.4700701	boolean algebra;embedded system;electronic engineering;real-time computing;engineering;automatic test pattern generation	EDA	17.367874358588217	55.207840925057795	21128
241cba8746def6fff4310ea364ab72c883cffa46	linearly scaling 3d fragment method for large-scale electronic structure calculations	cray xt4 processor core;ls3df algorithm;fragment method;p system;ls3df method;electronic structure calculation;linearly scaling;ls3df program yield;hundred thousand processor;direct density;large-scale electronic structure calculation;direct dft calculation;824-atom znteo alloy calculation;three dimensional;electronic structure;atoms;accuracy;parallel processing;materials;code optimization;data mining;ab initio calculations;performance;materials science;metals;density function theory;divide and conquer;algorithms;parallel computer	We present a new linearly scaling three-dimensional fragment (LS3DF) method for large scale ab initio electronic structure calculations. LS3DF is based on a divide-and-conquer approach, which incorporates a novel patching scheme that effectively cancels out the artificial boundary effects due to the subdivision of the system. As a consequence, the LS3DF program yields essentially the same results as direct density functional theory (DFT) calculations. The fragments of the LS3DF algorithm can be calculated separately with different groups of processors. This leads to almost perfect parallelization on over one hundred thousand processors. After code optimization, we were able to achieve 60.3 Tflop/s, which is 23.4% of the theoretical peak speed on 30,720 Cray XT4 processor cores. In a separate run on a BlueGene/P system, we achieved 107.5 Tflop/s on 131,072 cores, or 24.2% of peak. Our 13,824-atom ZnTeO alloy calculation runs 400 times faster than a direct DFT calculation, even presuming that the direct DFT calculation can scale well up to 17,280 processor cores. These results demonstrate the applicability of the LS3DF method to material simulations, the advantage of using linearly scaling algorithms over conventional O(N3) methods, and the potential for petascale computation using the LS3DF method.	64-bit computing;ab initio quantum chemistry methods;algorithm;blue gene;central processing unit;cluster analysis;code;computation;computational complexity theory;computational science;computer simulation;cray xt4;density functional theory;double-precision floating-point format;electronic structure;flops;functional theories of grammar;gordon bell prize;haar wavelet;image scaling;iteration;jaguar;mathematical optimization;p system;parallel computing;petascale computing;program optimization;simulation;solar cell;subdivision surface	Lin-Wang Wang;Byounghak Lee;Hongzhang Shan;Zhengji Zhao;Juan C. Meza;Erich Strohmaier;David H. Bailey	2008	2008 SC - International Conference for High Performance Computing, Networking, Storage and Analysis	10.1145/1413370.1413437	three-dimensional space;parallel processing;particle-in-cell;productive nanosystems;parallel computing;divide and conquer algorithms;atom;inertial confinement fusion;performance;computer science;theoretical computer science;operating system;central processing unit;program optimization;accuracy and precision;velocity;density functional theory;ab initio quantum chemistry methods;electronic structure;quantum mechanics;flops;memory management;p system	HPC	-4.312216696043847	37.538566378617155	21133
1e64494f2459382c4d0c1a6d6bb38898933d801e	methods and apparatuses for drying electronic devices	vaporisation consumer electronics drying electronic products electronics packaging;ocean temperature;water heating;wireless communication;liquids;humidity;liquids ocean temperature wireless communication water heating humidity conferences;drying electronic devices smart devices;conferences	The proliferation of smart devices and portable, wireless, wearable technology has exploded in the last decade. With the advent of social media and the internet of things, these devices have now intertwined themselves with consumers on a daily basis such that people are connected more with their device than ever before. With the simultaneous miniaturization of these devices, consumers are more apt to subject them to unintended water perils. Methods and apparatuses are described herein to remove water completely from electronic devices in a fast yet convenient manner.	digital camera;game controller;internet of things;mobile phone;smart device;smartphone;social media;usb flash drive;wearable technology	Reuben Q. Zielinski;Joel C. Trusty	2016	2016 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2016.7430508	sea surface temperature;electronic engineering;telecommunications;computer science;electrical engineering;humidity;wireless	Robotics	4.457450070981106	33.259172826654996	21171
e6004e546340eae6b184456b36d287fc134bdce3	timing characterization and constraining tool	design automation;reconfigurable logic;digital integrated circuits;integrated circuit synthesis;design methodology	This paper presents Timing Characterization and constraining Tool (TCT) that facilitates designing of modular reconfigurable Integrated Circuits (ICs) by supporting early constraint-based design space exploration and timing constraining. These steps of the design methodology are crucial from the perspective of quality of results and are not directly addressed by the synthesis tools used nowadays. Although the idea of TCT is presented here using one of the currently available logic synthesis tools as an example, it can be easily adapted for other ones. Such flexibility increase usability of TCT and makes it very helpful for scientists who look for new integrated architectures that utilize dynamically reconfigurable resources.		Piotr Amrozik;Andrzej Napieralski	2014	Microelectronics Journal	10.1016/j.mejo.2013.11.014	electronic engineering;real-time computing;design methods;electronic design automation;engineering;computer engineering	Theory	4.844009583104717	53.757005243125114	21212
0d7b6ea40e200fb12f1d936c7a50d9dc1fe362d7	line-broadcasting in complete k-trees		A l ine-broadcasting model in a connected graph G = (V,E), |V | = n, is a model in which one vertex, called the originator of the broadcast holds a message that has to be transmitted to all vertices of the graph through placement of a series of calls over the graph. In this model, an informed vertex can transmit a message through a path of any length in a single time unit, as long as two transmissions do not use the same edge at the same time. Farley [6] has shown that the process is completed within at most ⌈log 2 n⌉ time units from any originator in a tree (and thus in any connected undirected graph) and that the cost of broadcasting one message from any vertex, i.e. the total number of edge used, is at most (n− 1)⌈log 2 n⌉. In this paper, we present lower and upper bounds for the cost to broadcast one message in a complete k−tree, k ≥ 2, from any vertex using the line-broadcasting model. We prove that if B(u) is the minimum cost to broadcast in a graphG = (V,E) from a vertex u ∈ V using the line-broadcasting model, then (2 − o(1))n ≤ B(u) ≤ (2 + o(1))n, where u is any vertex in a complete k-tree. Furthermore, for certain conditions, B(u) ≤ (2 − o(1))n.	connectivity (graph theory);graph (discrete mathematics);k-tree;vertex (graph theory)	R. Hollander Shabtai;Yehuda Roditty	2015	CoRR		combinatorics;telecommunications;computer science;theoretical computer science;mathematics;distributed computing;broadcasting;neighbourhood	Theory	20.52579040887667	33.587697098257266	21229
8321d158814039aafb581e061593ff5e91c2e1fb	iterative timing analysis considering interdependency of setup and hold times	iterative timing analysis;interdependency	The interdependency of setup and hold times of flipflops in digital circuits needs to be considered in order to obtain more accurate results of timing analysis. In this paper, an iterative STA method is developed based on a new modeling of flipflop timing behavior. Two basic problems are solved: whether a circuit can work at a given clock period, and how the minimal clock period is determined. Experimental results show that a reduction of the clock period by 3.3% can be achieved compared to traditional STA method.		Ning Chen;Bing Li;Ulf Schlichtmann	2011		10.1007/978-3-642-24154-3_8	real-time computing;interdependence;computer science;timing failure;operations research;static timing analysis	EDA	17.424166446496535	52.55997574239054	21230
3cafd1d425701e95fa2dd488c3631c140eb836b8	parallel algorithms for index-permutation graphs. an extension of cayley graphs for multiple chip-multiprocessors (mcmp)	parallel algorithm;efficient algorithm;hypercube networks parallel algorithms;chip multiprocessor;cayley graph;natural extension;bisection bandwidth parallel algorithms index permutation graphs cayley graphs multiple chip multiprocessors embeddings intercluster diameter average intercluster distance;permutation graph;indexation;parallel computer;hypercube networks;parallel algorithms bandwidth network topology routing hypercubes costs packet switching clustering algorithms computer networks concurrent computing;parallel algorithms	The index-permutation graph (IPG) model is a natural extension of the Cayley graph model, and super-IPGs form an efficient class of IPGs that contain a wide variety of networks as subclasses. In this paper, we derive a number of efficient algorithms and embeddings for super-IPGs, proving their versatility. We show that a multitude of important networks can also be emulated in super-IPGs with optimal slowdown. Also, the intercluster diameter average intercluster distance, and bisection bandwidth of suitably constructed super-IPGs are optimal within small constant factors. Finally we show that when parallel computers, built as multiple chip-multiprocessors (MCMP), are based on super-IPGs, they can significantly outperform those based on hypercubes, k-ary n-cubes, and other networks in carrying out communication-intensive tasks.	bisection bandwidth;computer;conformance testing;emulator;magma;network topology;olap cube;parallel algorithm;parallel computing;scalability;super-recursive algorithm	Chi-Hsiang Yeh;Behrooz Parhami	2001	International Conference on Parallel Processing, 2001.	10.1109/ICPP.2001.952041	parallel computing;computer science;distributed computing;parallel algorithm	DB	21.710733996589873	36.107771885210965	21235
4a22c3f92095d35d37b55d5ca0c04270d806f456	self-reconfigurable pervasive platform for cryptographic application	cryptography coprocessors field programmable gate arrays hardware digital systems application software logic arrays prototypes bluetooth runtime;dynamic reconfiguration;reconfigurable architectures;cryptographic coprocessor;pervasive system;coprocessors;customer electronics;self reconfigurable platform;customer electronics reconfigurable coprocessors self reconfigurable platform cryptographic coprocessor;run time reconfigurable;reconfigurable architectures coprocessors cryptography;cryptography;high performance;reconfigurable coprocessors;software implementation	The complexity exhibited by pervasive systems is constantly increasing. Customer electronics devices provide day to day a larger amount of functionalities. A common approach for guaranteeing high performance is to include specialized coprocessor units. However, these systems lack flexibility, since one must define, in advance, the coprocessor functionality. A solution to this problem is to use run-time reconfigurable coprocessors, exploiting the advantages of hardware while keeping a flexible platform. In this paper, we describe a self-reconfigurable pervasive platform containing a dynamically reconfigurable cryptographic coprocessor. As case-study, we consider three ciphering algorithms and we compare the performance of the coprocessor against a full-software implementation. The number of ciphering algorithms can be infinitely extended using a remote server	algorithm;cipher;complexity;coprocessor;cryptography;reconfigurability;run time (program lifecycle phase);self-reconfiguring modular robot;server (computing);ubiquitous computing	Arnaud Lagger;Andres Upegui;Eduardo Sanchez;Ivan Gonzalez	2006	2006 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2006.311312	embedded system;parallel computing;real-time computing;computer science;cryptography;operating system	EDA	7.059001575223196	46.08906993664168	21261
44966e14acf6dfd6dfafcf6702c9f053ada4d890	symmetric transparent on-line bist of word-organized memories with binary adders	random access memory;random access memory adders built in self test europe prediction algorithms logic gates;prediction algorithms;random access storage adders built in self test;built in self test;logic gates;adders;binary addition symmetric transparent bist schemes ram modules memory contents periodic testing signature prediction phase accumulator modules binary adders;europe	Symmetric Transparent BIST schemes for RAM modules assure the preservation of the memory contents during periodic testing while at the same time skipping the signature prediction phase required in transparent BIST schemes, achieving considerable reduction in test time. In this work the utilization of accumulator modules comprising adders implementing binary addition is proposed.	accumulator (computing);binary number;built-in self-test;online and offline;random-access memory	Ioannis Voyiatzis	2015	2015 20th IEEE European Test Symposium (ETS)	10.1109/ETS.2015.7138744	electronic engineering;parallel computing;prediction;logic gate;computer science;engineering;theoretical computer science;adder;statistics	Embedded	19.944768086128818	51.48075700373623	21263
0f605355801b0115382f9ab37a71a206b023b430	a two-level on-chip bus system based on multiplexers	internet protocol;on chip bus;distributed system;sistema bus;bus system;diseno circuito;tratamiento transaccion;systeme reparti;protocolo internet;canal bus;reutilizacion;circuit design;protocole internet;canal colector;simultaneidad informatica;on chip buses;buffer system;system on a chip;reuse;sistema amortiguador;chip;computer architecture;concurrency;sistema repartido;architecture ordinateur;sistema sobre pastilla;system integration;multiplexeur;bus channel;multiplexer;conception circuit;arquitectura ordenador;systeme sur puce;systeme bus;multiplexor;transaction processing;systeme tampon;simultaneite informatique;traitement transaction;reutilisation	The SoC (System on a Chip) design paradigm becomes a promising way of system integration as the level of design complexity is getting higher. There may be many IP modules to be integrated on a single chip in the modern SoC design. On-chip buses are usually used to interconnect the modules on a chip. Many bus architectures have been proposed for the interconnection of modules on a chip. We propose a two-level on-chip bus system that provides inter-bus transactions with multiplexers rather than with tri-state buffers or MOS switches such as in the segmented bus approaches. Our bus system can maximize the system throughput with concurrent inter-bus transactions as well as intra-bus transactions while preserving the already developed on-chip bus protocols for IP reuse. We present the performance simulation results of our approach with several different configurations compared with the existing segmented bus structures in terms of the total number of bus transactions executed in a given time.	multiplexer	Kyoung-Sun Jhang;Kang Yi;Soo Yun Hwang	2004		10.1007/978-3-540-30102-8_30	multiplexer;bus;std bus;embedded system;bus error;memory bus;real-time computing;three-state logic;iebus;computer science;local bus;operating system;conventional pci;system bus;control bus;back-side bus;bus network;address bus	EDA	0.5272189802641132	58.25270849919897	21311
9b157fc13a6c8d3ab569d46909de095cb0e3df7c	new design of 2 $\times$ vdd-tolerant power-rail esd clamp circuit for mixed-voltage i/o buffers in 65-nm cmos technology	cmos integrated circuits;voltage 300 v;cmos technology;thyristors;leakage current;electrostatic discharge esd;operant conditioning;holding voltage;thin gate oxide devices;temperature 293 k to 298 k;buffer circuits;human body model;voltage 1 8 v;electrostatic discharge;mixed voltage i o buffers;logic gates;voltage 1 v;leakage currents;electrostatic discharges;vdd tolerant power rail esd clamp circuit;power rail esd clamp circuit;silicon controlled rectifier;voltage difference reduction;voltage 6 5 kv;room temperature;human body model esd level;machine model;logic gate;article;clamps;size 65 nm;electrostatic devices;esd detection circuit;machine model esd level;low standby leakage current	A new 2 VDD-tolerant power-rail electrostatic discharge (ESD) clamp circuit realized with only thin gate oxide 1-V (1 VDD) devices and a silicon-controlled rectifier (SCR) as the main ESD clamp device has been proposed and verified in a 65-nm CMOS process. This new design has a low standby leakage current by reducing the voltage difference across the gate oxide of the devices in the ESD detection circuit. The proposed design with an SCR width of 50 can achieve a 6.5-kV human-body-model ESD level, a 300-V machine-model ESD level, and a low standby leakage current of only 103.7 nA at room temperature under the normal circuit operating condition with 1.8 V bias.	cmos;clamper (electronics);clamping (graphics);discharger;electrostatic loudspeaker;gate oxide;input/output;silicon controlled rectifier;spectral leakage;value-driven design	Chih-Ting Yeh;Ming-Dou Ker	2012	IEEE Transactions on Circuits and Systems II: Express Briefs	10.1109/TCSII.2012.2184372	embedded system;electronic engineering;logic gate;engineering;electrical engineering;cmos	EDA	17.505648065063852	58.67447473332549	21334
1b05c601a7a7bce5585fc4e228fe05372ae311ac	using synchronizers for refining synchronous communication onto hardware/software architectures	object oriented methods;hardware software codesign;elektroteknik och elektronik;synchronous communication refininement;electrical engineering electronic engineering information engineering;sw sw component;hardware software architectures;hardware software architecture synchronization computer architecture prototypes clocks object oriented modeling asynchronous communication field programmable gate arrays communication system control;satisfiability;soc design;synchronisation;computer architecture;software architecture;hw sw component;system on chip;asynchronous communication;synchronizers;ip core hardware software codesign architecture synchronous communication refining;ip core;synchronisation computer architecture hardware software codesign;industrial property;synchronization components;hw hw component;synchronous communication;hw sw codesign architectures	We have presented a formal set of synchronization components called synchronizers for refining synchronous communication onto HW/SW codesign architectures. Such an architecture imposes asynchronous communication between HW-HW, SW-SW and HW-SW components. The synchronizers enable local synchronization, thus satisfy the synchronization requirement of a typical IP core. In this paper, we present their implementations in HW, SW and HW/SW, as well as their application. To validate our concepts, we conduct a case study on a Nios FPGA that comprises a processor, memory and custom logic. The final HW/SW implementation achieves equivalent performance to pure HW implementation. Our prototyping experience suggests that the synchronizers can be standardized as library modules and effectively separate the design of computation from that of communication.	algorithm;computation;embedded system;equalization (communications);field-programmable gate array;heuristic;mathematical optimization;nios embedded processor;overhead (computing);prototype;semiconductor intellectual property core;shattered world;synchronization (computer science);synchronizer (algorithm)	Zhonghai Lu;Jonas Sicking;Ingo Sander;Axel Jantsch	2007	18th IEEE/IFIP International Workshop on Rapid System Prototyping (RSP '07)	10.1109/RSP.2007.38	embedded system;computer architecture;real-time computing;computer science;operating system;software engineering;asynchronous communication	EDA	5.423808356623056	52.70553828127153	21349
0fb3e6662241aa6e7160f3f637ddca36b4a197c3	rematerialization-based register allocation through reverse computing	instruction level parallel;reverse computing;thread level parallelism;register allocation;spill code;quantum chromodynamics;rematerialization;register pressure	Reversible computing aims at keeping all information on input and intermediate values available at any step of the computation. Rematerialization in register allocation is an alternate solution to spilling where values are recomputed from available data instead of held in registers. In this paper we present the basic ideas of our algorithm for rematerialization with reverse computing. We use the memory demanding LQCD (Lattice Quantum ChromoDynamics) application to demonstrate that important gains of up to 33% on register pressure can be obtained. This in turn enables an increase in Instruction-Level Parallelism or in Thread-Level Parallelism. We demonstrate a 16.8% (statically timed) gain over a basic LQCD computation.	algorithm;computation;instruction-level parallelism;lattice qcd;parallel computing;quantum;register allocation;rematerialization;reversible computing;task parallelism	Mouad Bahi;Christine Eisenbeis	2011		10.1145/2016604.2016632	parallel computing;real-time computing;computer science;theoretical computer science	HPC	-2.200078693514813	51.71028386868321	21405
1281c75944d1d8d58efc688ff8b4d2d97cbb84de	high-level synthesis for low-power design	hardware acceleration;algorithm;high level synthesis;compiler optimization;low power design	Power and energy efficiency have emerged as first-order design constraints across the computing spectrum from handheld devices to warehouse-sized datacenters. As the number of transistors continues to scale, effectively managing design complexity under stringent power constraints has become an imminent challenge of the IC industry. The manual process of power optimization in RTL design has been increasingly difficult, if not already unsustainable. Complexity scaling dictates that this process must be automated with robust analysis and synthesis algorithms at a higher level of abstraction. Along this line, high-level synthesis (HLS) is a promising technology to improve design productivity and enable new opportunities for power optimization for higher design quality. By allowing early access to the system architecture, high-level decisions during HLS can have a significant impact on the power and energy efficiency of the synthesized design. In this paper, we will discuss the recent research development of using HLS to effectively explore a multi-dimensional design space and derive low-power implementations. We provide an in-depth coverage of HLS low-power optimization techniques and synthesis algorithms proposed in the last decade. We will also describe the key power optimization challenges facing HLS today and outline potential opportunities in tackling these challenges.	algorithm;early access;first-order predicate;high- and low-level;high-level synthesis;holism;image scaling;low-power broadcasting;mathematical optimization;mobile device;power optimization (eda);quality of results;systems architecture;throughput;transistor	Zhiru Zhang;Deming Chen;Steve Dai;Keith A. Campbell	2015	IPSJ Trans. System LSI Design Methodology	10.2197/ipsjtsldm.8.12	computer architecture;parallel computing;real-time computing;hardware acceleration;profile-guided optimization;computer science;optimizing compiler;high-level synthesis;algorithm	EDA	3.2704002754752963	55.02148012652158	21411
200e4dc2785bce1fc4b765736d49081119ff07d8	synthesizing a representative critical path for post-silicon delay prediction	replica method;prediction error;critical path;representative critical path;post silicon optimization	Several approaches to post-silicon adaptation require feedback from a replica of the nominal critical path, whose variations are intended to reflect those of the entire circuit after manufacturing. For realistic circuits, where the number of critical paths can be large, the notion of using a single critical path is too simplistic. This paper overcomes this problem by introducing the idea of synthesizing a representative critical path (RCP), which captures these complexities of the variations. We first prove that the requirement on the RCP is that it should be highly correlated with the circuit delay. Next, we present two novel algorithms to automatically build the RCP. Our experimental results demonstrate that over a number of samples of manufactured circuits, the delay of the RCP captures the worst case delay of the manufactured circuit. The average prediction error of all circuits is shown to be below 2.8% for both approaches. For both our approach and the critical path replica method, it is essential to guard-band the prediction to ensure pessimism: our approach requires a guard band 30% smaller than for the critical path replica method.	algorithm;best, worst and average case;critical path method	Qunzeng Liu;Sachin S. Sapatnekar	2009		10.1145/1514932.1514973	real-time computing;fast path;computer science;theoretical computer science;critical path method;mean squared prediction error;mathematics;distributed computing;statistics	EDA	22.443532773835297	56.80486035912688	21422
c54d730bf7380c25599aeaefd889a3ef2fd9a332	practical fast clock-schedule design algorithms	shortest path;tecnologia electronica telecomunicaciones;negative cycle detection;semi synchronous circuits;clock schedule;tecnologias;grupo a;semisynchronous circuits	In this paper, a practical clock-scheduling engine is introduced. The minimum feasible clock-period is obtained by using a modified Bellman-Ford shortest path algorithm. Then an optimum cost clockschedule is obtained by using a bipartite matching algorithm. It also provides useful information to circuit synthesis tools. The experiment to a circuit with about 10000 registers and 100000 signal paths shows that a result is obtained within a few minutes. The computation time is almost linear to the circuit size in practice. key words: clock-schedule, shortest path, negative cycle detection, semisynchronous circuits	bellman–ford algorithm;computation;cycle detection;dijkstra's algorithm;matching (graph theory);scheduling (computing);shortest path problem;time complexity	Atsushi Takahashi	2006	IEICE Transactions	10.1093/ietfec/e89-a.4.1005	real-time computing;telecommunications;computer science;mathematics;distributed computing;shortest path problem;algorithm	EDA	20.393685351204027	40.49602925631376	21457
87857884ab1a5e689618a830fee5684dfea62d29	over/undershooting effects in accurate buffer delay model for sub-threshold domain	sub threshold cmos analytical model over undershoot propagation delay;size 45 nm over undershooting effects buffer delay model sub threshold domain transistors threshold voltage ultra low power circuits sub threshold circuit super threshold domain mathematical model sub threshold cmos inverter transistor on current transient variation gate switching input to output coupling capacitance closed form expressions inverter delay cascading inverter delay clock buffers process voltage temperature variations cmos process;buffer circuits;delays inverters semiconductor device modeling cmos integrated circuits capacitance predictive models transistors;logic gates;cmos logic circuits;low power electronics;low power electronics buffer circuits cmos logic circuits delays logic gates;delays	Scaling down the supply voltage (Vdd) below the transistors threshold voltage (Vth) has become a very popular technique in designing Ultra-Low-Power circuits whose demand has dramatically increased in the last few years. Designing these kinds of circuit is still a challenge, especially when the latest advanced process technologies are employed. The well-known design methodology used in the typical super-threshold domain (Vdd > Vth) cannot be applied to the design of a sub-threshold circuit due to the different transistor current-voltage relationships that hold when Vdd <; Vth. For this reason, designers need supports suitable for the sub-threshold domain. This paper proposes a complete mathematical model able to predict the output behavior of a sub-threshold CMOS inverter. The model proposed here takes into account the effects of the transient variation of the transistor on-current during the gate switching. Moreover, for the first time, over/undershoot effects due to the input-to-output coupling capacitance are taken into account. The proposed model is formed by closed-form expressions able to predict the over/undershoot position, its amplitude and the inverter delay with great accuracy. Furthermore, it can be easily exploited in predicting the delay of cascading inverters, usually used to realize clock buffers. Under Process-Voltage-Temperature variations, the delay of a single inverter realized using a commercial CMOS 45 nm process technology is predicted with a maximum error lower than 16%. Even better results are obtained when the model is applied to inverter chains.	cmos;low-power broadcasting;markov chain;mathematical model;power inverter;transistor;whole earth 'lectronic link	Pasquale Corsonello;Fabio Frustaci;Marco Lanuzza;Stefania Perri	2014	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2013.2285691	electronic engineering;real-time computing;logic gate;computer science;engineering;electrical engineering;fo4;pass transistor logic;low-power electronics	EDA	21.448597493270604	58.62554493320688	21468
636c2f4f5837620be7e6f0163752abeacde95047	variation-aware low-power synthesis methodology for fixed-point fir filters	predictive technology model;passband;process variation;power saving;degradation;finite impulse response filter voltage adders degradation energy consumption delay computer architecture predictive models passband design methodology;band pass filters;normalized passband stopband ripple;variation aware design;size 70 nm;finite impulse response filter;fir filters band pass filters;voltage 0 8 v variation aware low power synthesis methodology fixed point fir filters finite impulse response aggressive voltage scaling level constrained common subexpression elimination algorithm filter quality low power dissipation predictive technology model normalized passband stopband ripple size 70 nm;variation aware low power synthesis methodology;fixed point;voltage 0 8 v;finite impulse response;low power;filtering algorithms;process parameters;low power dissipation;adders;critical path;fir filter;graceful degradation;aggressive voltage scaling;common subexpression elimination;filter quality;fir filters;fixed point fir filters;level constrained common subexpression elimination algorithm;voltage scaling;low power methodology;finite impulse response fir filter synthesis;algorithm design and analysis;variation aware design finite impulse response fir filter synthesis low power methodology	In this paper, we present a novel finite-impulse response (FIR) filter synthesis technique that allows for aggressive voltage scaling by exploiting the fact that all filter coefficients are not equally important to obtain a ldquoreasonably accuraterdquo filter response. Our technique implements a level-constrained common-subexpression-elimination algorithm, where we can constrain the number of adder levels (ALs) required to compute each of the coefficient outputs. By specifying a tighter constraint (in terms of the number of adders in the critical path) on the important coefficients, we ensure that the later computational steps compute only the less important coefficient outputs. In case of delay variations due to voltage scaling and/or process variations, only the less important outputs are affected, resulting in graceful degradation of filter quality. The proposed architecture, therefore, lends itself to aggressive voltage scaling for low-power dissipation even under process parameter variations. Under extreme process variation and supply voltage scaling (0.8 V), filters implemented in the predictive technology model (PTM) 70 nm technology show an average power savings of 25%-30% with minor degradation in filter response in terms of normalized passband/stopband ripple (0.02 at a scaled voltage of 0.8 V compared with 0.005 at a nominal supply).	adder (electronics);algorithm;calculus of variations;coefficient;common subexpression elimination;computation;critical path method;dynamic voltage scaling;elegant degradation;fault tolerance;finite impulse response;image scaling;low-power broadcasting;polynomial texture mapping;quality of service;requirement;ripple effect;signal processing	Jung Hwan Choi;Nilanjan Banerjee;Kaushik Roy	2009	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2008.2009135	control engineering;electronic engineering;computer science;engineering;finite impulse response;control theory;programming language;algorithm	EDA	21.29561597862405	60.26196791926626	21476
c7986df3423e15b4a1b107f9817311783d8ebc5a	on minimizing the number of l-shaped channels	usu;building block;routing;very large scale integration;distributed computing;routing permission very large scale integration algorithm design and analysis shape distributed computing machinery wiring;shape;permission;fast algorithm;wiring;machinery;algorithm design and analysis	We study in this paper the problem of minimizing the number of L-shaped channels in the routing region definition and ordering for building-block layouts. Given a building-block layout of rectangular modules, the routing region is to be decomposed into straight and L-shaped channels and routed in a certain order. Since channel routers usually produce superior results, it is desirable to minimize the number of L-shaped channels used in such a decomposition. We present a fast algorithm which can produce provably better results than the previously known algorithm proposed in [7]. Our algorithm is based on a careful study of the structure of such layouts, and a transformation of the original problem to a graph theoretical problem. For examples of up to 136 channels, our algorithm took less than one tenth of a second to finish the computation, and it obtained up to 29% reduction in the number of L-shaped channels over the results produced by the algorithm in [71.	algorithm;computation;graph theory;routing	Yang Cai;Martin D. F. Wong	1991		10.1145/127601.127689	embedded system;algorithm design;routing;electronic engineering;machine;shape;computer science;electrical engineering;theoretical computer science;distributed computing;very-large-scale integration;engineering drawing	EDA	15.354671705733825	50.97195213799224	21481
60faa1422d9bef7d9f82a1d2b0d0abfb36837387	a structured parallel periodic arnoldi shooting algorithm for rf-pss analysis based on gpu platforms	time-consuming analysis;periodic steady state;computer graphic equipment;cyclic-block-structured shooting-newton method;structured parallel periodic arnoldi;gmres method;newton method;radiofrequency integrated circuits;parallel computing platform;radiofrequency-millimeter-wave integrated circuit;parallel algorithms;state-of-the-art implicit gmres method;ideal parallel computing platform;rf-pss analysis;periodic arnoldi;parallel implementation detail;periodic structure;periodic structures;recent gpu platform;coprocessors;structured parallel shooting-newton method;multicore cpu;gpu platform;parallel cyclic-block-structured shooting-newton algorithm;many-core cpu;periodic steady state analysis;millimetre wave integrated circuits;structured parallel periodic arnoldi shooting algorithm;integrated circuit;parallel processing;shooting method;steady state;accuracy;radio frequency;instruction sets;sparse matrices;millimeter wave;parallel computer	The recent multi/many-core CPUs or GPUs have provided an ideal parallel computing platform to accelerate the time-consuming analysis of radio-frequency/millimeter-wave (RF/MM) integrated circuit (IC). This paper develops a structured shooting algorithm that can fully take advantage of parallelism in periodic steady state (PSS) analysis. Utilizing periodic structure of the state matrix of RF/MM-IC simulation, a cyclic-block-structured shooting-Newton method has been parallelized and mapped onto recent GPU platforms. We first present the formulation of the parallel cyclic-block-structured shooting-Newton algorithm, called periodic Arnoldi shooting method. Then we will present its parallel implementation details on GPU. Results from several industrial examples show that the structured parallel shooting-Newton method on Tesla's GPU can lead to speedups of more than 20x compared to the state-of-the-art implicit GMRES methods under the same accuracy on the CPU.	algorithm;arnoldi iteration;central processing unit;generalized minimal residual method;graphics processing unit;integrated circuit;manycore processor;newton;newton's method;parallel computing;physical symbol system;radio frequency;shooting method;simulation;steady state	Xuexin Liu;Hao Yu;Jacob Relles;Sheldon X.-D. Tan	2011	16th Asia and South Pacific Design Automation Conference (ASP-DAC 2011)		shooting method;embedded system;parallel processing;parallel computing;sparse matrix;computer hardware;computer science;theoretical computer science;integrated circuit;instruction set;accuracy and precision;extremely high frequency;parallel algorithm;newton's method;steady state;radio frequency;coprocessor	EDA	-2.9848297784107998	39.33094898675937	21492
03aa9781a335536a1c82a74150bb32999f1e70e1	an area-efficient design of reconfigurable s-box for parallel implementation of block ciphers	reconfigurable s box;block cipher;area cost;parallel implementation	Abstract: A LUT with Hierarchical Structure (HS-LUT) is proposed in this paper to realize the unique nonlinear component, Substitution Box (S-box), of the block ciphers. Different types of S-boxes are analyzed and four important features of them are summarized. Then, custom 4R/1W memory is proposed as the storage unit of the reconfigurable S-box, and an example set of block ciphers is put forward to describe how to achieve a satisfactory structure of reconfigurable S-box. The proposed HS-LUT is applicable for different sets of ciphers and it is implemented under TSMC 40nm CMOS technology to compare with similar work. The comparison result shows that the proposed HS-LUT gains 6.88% to 51.76% area efficiency improvement.	block cipher;cmos;central processing unit;cryptography;nonlinear system;s-box	Jinjiang Yang;Ge Wei;Peng Cao;Jun Yang	2016	IEICE Electronic Express	10.1587/elex.13.20160138	block cipher;computer architecture;parallel computing;computer science;distributed computing	EDA	9.110766477096004	44.68924665876133	21527
3d90fa088aa0eda2fb333ded268cc3ea7dbff53e	an efficient lossless compression method for internet search data in hardware accelerators	machine learning algorithms;search engine;image coding;nonblocking streaming data;data compression;search engines;lossless compression;hardware accelerator;bit mapping decompression method;hardware accelerators;data decompression technology;large scale;internet;data compression technology;statistical analysis;machine learning;bit mapping compression method;nonblocking streaming data lossless compression method internet search data hardware accelerators machine learning algorithms search engines software implementations commodity computers fpga based accelerators data compression technology data decompression technology statistics bit mapping compression method bit mapping decompression method;statistics;commodity computers;statistical analysis data compression field programmable gate arrays internet learning artificial intelligence search engines;internet search data;software implementations;power consumption;field programmable gate arrays;learning artificial intelligence;internet hardware acceleration data compression machine learning algorithms search engines energy consumption large scale systems statistical analysis data analysis;lossless compression method;encoding;algorithm design and analysis;fpga based accelerators;software implementation;hardware	Nowadays machine learning algorithms are intensively used to improve the relevance of search engines by training on Internet search data, while their software implementations on commodity computers are not efficient (in terms of computation speed, power consumption, etc). Therefore FPGA-based accelerators have been proposed to process these large scale data. Data compression/decompression technology plays an important role for it could significantly increase those accelerators’ performance. Based on the statistics on datasets and the analysis of conventional data compression algorithms, we propose a bit mapping compression/decompression method to provide non-blocking streaming data to accelerators. Experiments indicate that the performance of hardware accelerator is increased up to 140% after adding the bit mapping modules. This method could also be extended to other data-intensive hardware accelerators.	blocking (computing);computation;computer;data compression;data-intensive computing;field-programmable gate array;hardware acceleration;lossless compression;machine learning;non-blocking algorithm;reconfigurable computing;relevance;streaming media;web search engine	Jing Yan;Rong Luo;Rui Gao;Ning-Yi Xu	2009	2009 WRI World Congress on Computer Science and Information Engineering	10.1109/CSIE.2009.340	parallel computing;real-time computing;computer science;theoretical computer science;operating system;database;search engine;statistics	HPC	2.2134235794191035	41.79187066064074	21529
0a97e566a1618effeb7b64215d2f6c805a2031a5	test point insertion based on path tracing	vlsi test point insertion path tracing circuit under test fault coverage probabilistic techniques primary inputs insertion methods bist logic testing;bist;circuit under test;insertion methods;probabilistic techniques;probability;primary inputs;automatic testing;circuit testing circuit faults built in self test system testing test pattern generators control systems observability logic circuits automatic testing hardware;automatic testing vlsi fault diagnosis logic testing integrated circuit testing probability built in self test timing;path tracing;built in self test;logic testing;integrated circuit testing;vlsi;fault coverage;test point insertion;fault diagnosis;timing	The set of test patterns applied to a circuit during built-in self-test (BIST) may not provide sufficiently high fault coverage due to the presence of hard-to-detect faults. This paper presents an innovative method for inserting test points in a way that complete (100%) single stuck-at fault coverage is obtained for a specified set of test patterns. A very different approach is taken compared with previous test point insertion methods. Instead of using probabilistic techniques for test point placement, a path tracing procedure is used to placeboth control and observation points. Instead of adding extra scan elements to drive the control points, a few of the existing primary inputs to the circuit are ANDed together to form signals that drive the control points. By selecting which patterns the control point is activated for, the effectiveness of each control point is maximized. A comparison is made with the best previously published results for other test point insertion methods, and it is shown that the path tracing method requires fewer test points and less overhead to achieve the same or better fault coverage. It is also shown that the path racing method can be used for timing-driven test point insertion that achieves zero performance degradation. Because the path tracing method is not based on randomness properties of the test patterns, it can be used for any set of test patterns. The set of test patterns can be pseudo-random (e.g., generated by a linear feedback register), quasi-random (e.g., generated by a multiple input signature analyzer), or not be random at all.	built-in self-test;control point (mathematics);elegant degradation;fault coverage;linear-feedback shift register;low-discrepancy sequence;overhead (computing);path tracing;pseudorandomness;randomness;stuck-at fault;test card;test point	Nur A. Touba;Edward J. McCluskey	1996		10.1109/VTEST.1996.510828	reliability engineering;path tracing;electronic engineering;real-time computing;fault coverage;computer science;automatic test pattern generation;probability;very-large-scale integration;statistics	EDA	21.18781218049954	51.19916059548913	21530
a179ed7f8febca9e664824709eb5f150faf5d0e4	fault-free cycles passing through prescribed paths in hypercubes with faulty edges	embedding;hypercube;matematicas aplicadas;prescribed path;fault tolerant;mathematiques appliquees;cycle embedding;parallel computation;interconnection network;calculo paralelo;edge fault tolerant;plongement;parallel computer;interconnection networks;hypercubes;inmersion;applied mathematics;calcul parallele;red interconexion;reseau interconnexion;hipercubo	Abstract   An   n  -dimensional hypercube, or   n  -cube, denoted by     Q    n    , is well known as bipartite and one of the most efficient networks for parallel computation. In this work, we consider the problem of cycles passing through prescribed paths in an   n  -dimensional hypercube with faulty edges. We obtain the following result: For   n  ≥  3   and   2  ≤  h    n  , let   F   be a subset of   E   (    Q    n    )    with    |  F  |     n  −  h  . Then, every fault-free path   P   with length   h   lies on a fault-free cycle in     Q    n    −  F   of every even length from   d   to     2    n     inclusive where   d  =  2  h   if   h  >   |  F  |   +  1   and   d  =  2  h  +  2   otherwise. The result is optimal.		Chang-Hsiung Tsai	2009	Appl. Math. Lett.	10.1016/j.aml.2008.08.021	combinatorics;discrete mathematics;mathematics;hypercube	Theory	24.190567112176158	34.790000973851384	21595
1dbda559f74c229b0e4bd803aafffb8cba889041	a main frame semiconductor memory for fourth generation computers	spectrum;large scale integration	It has been obvious for several years that Large Scale Integration could be applied to memories. Memories offer several advantages in that a large volume of one type of device can be manufactured, and that the design can be optimized for one application. There exists a wide spectrum of memory product areas with varying size, costs, speed and enviromental performance. Most of these application areas are presently serviced by various forms of magnetic storage.	magnetic storage;mainframe computer;memory module;semiconductor memory;very-large-scale integration	Thomas W. Hart;Durrell W. Hillis;John Marley;Robert C. Lutz;Charles R. Hoffman	1969		10.1145/1478559.1478616	electronic engineering;computer hardware;engineering;electrical engineering	AI	12.717691756479317	58.90066429909958	21604
3795e6b63fd472454dada760f67ae634242df481	mixed-signal soc testing: is mixed-signal design-for-test on its way		The world market for electronic systems will reach $1 Trillion within a year and with further exponential growth over the next five years. The growth in areas such as telecommunications has increased the demand for creating single chip solutions to system. This has been achieved by integrating a number of complex sub-systems, including standard interface blocks (e.g., analog/digital converters), reused design cores (e.g., memory or microprocessors), embedded software, and new, innovative, custom designed “user blocks”, into a single chip. Today, system-on-a-chip (SoC) has become a reality. However, the complexity of SoC makes it very difficult to achieve the desire test coverage without affecting the design schedule. In the last decade, testing complex digital ICs has dramatically improved. Fully automated test solutions are commercially available. DFT (design-for-test) and BIST (Builtin-self-test) methodologies have been well-developed and available for today’s high-complexity and/or high performance designs. However, mixed-signal DFT is far behind. This is simply due to the lack of standard fault model for analog circuits, standard mixed-signal DFT methodology, and commercially available ATPG for mixed-signal circuits. A number of DFT and BIST design methods of analog modules, such as ADC, DAC, PLL, analog front end circuits, and etc., have been developed by university researchers. Due to high testing cost, many companies are still not willing to make efforts to perform testing on analog modules like embedded PLLs. In order to improve SoC design productivity, efficient SoC test methods must be developed and commercially available. This panel will address the following interesting issues: * Is toady’s mixed-signal DFT methodology on its way to resolve the challenges of today’s industrial market demanding? * Is BIST the mixed-signal DFT solution? * Is embedded test the mixed-signal test solution? 15 1081-7735/00 $10.00020001EEE Proceedings of the 9th Asian Test Symposium (ATS00) 1081-7735/00 $10.00 © 2000 IEEE	analog-to-digital converter;analogue electronics;built-in self-test;design for testing;digital-to-analog converter;embedded software;embedded system;fault coverage;fault model;microprocessor;mixed-signal integrated circuit;phase-locked loop;system on a chip;test automation;time complexity	Chin-Long Wey;Adam Osseiran;José Luis Huertas;Yeon-Chen Nieu	2000		10.1109/ATS.2000.10013		EDA	9.9431210037902	55.02320449331383	21627
b921a8947a63cf1e8a29e313e796fd65ec3d8d6b	a connectivity based clustering algorithm with application to vlsi circuit partitioning	cluster algorithm;clustering algorithms partitioning algorithms very large scale integration iterative algorithms clustering methods large scale integration design automation integrated circuit interconnections large scale systems joining processes;vlsi integrated circuit design;very large scale integrated;partitioning clustering iterative improvement;integrated circuit design;vlsi physical design;large scale partitioning problems connectivity based clustering algorithm vlsi circuit partitioning very large scale integration vlsi physical design automation;large scale;clustering method;vlsi	Circuit partitioning is a fundamental problem in very large-scale integration (VLSI) physical design automation. In this brief, we present a new connectivity-based clustering algorithm for VLSI circuit partitioning. The proposed clustering method focuses on capturing natural clusters in a circuit, i.e., the groups of cells that are highly interconnected in a circuit. Therefore, the proposed clustering method can reduce the size of large-scale partitioning problems without losing partitioning solution qualities. The performance of the proposed clustering algorithm is evaluated on a standard set of partitioning benchmarks-ISPD98 benchmark suite. The experimental results show that by applying the proposed clustering algorithm, the previously reported best partitioning solutions from state-of-the-art partitioners are further improved.	algorithm;benchmark (computing);cluster analysis;computer cluster;integrated circuit;physical design (electronics);very-large-scale integration	Jianhua Li;Laleh Behjat	2006	IEEE Transactions on Circuits and Systems II: Express Briefs	10.1109/TCSII.2005.862174	parallel computing;computer science;engineering;electrical engineering;theoretical computer science;machine learning;very-large-scale integration;integrated circuit design	EDA	15.026289577645402	52.29493755893888	21648
76c335e1ae163fdaab3f7a6666ed987d46e12aa6	hr* graph conditions between counting monadic second-order and second-order graph formulas		Graph conditions are a means to express structural properties for graph transformation systems and graph programs in a large variety of application areas. With HR∗ graph conditions, non-local graph properties like “there exists a path of arbitrary length” or “the graph is circle-free” can be expressed. We show, by induction over the structure of formulas and conditions, that (1) any node-counting monadic second-order formula can be expressed by an HR∗ condition and (2) any HR∗ condition can be expressed by a second-order graph formula.	graph property;graph rewriting;mathematical induction	Hendrik Radke	2013	ECEASST	10.14279/tuj.eceasst.61.831	graph power;edge-transitive graph;factor-critical graph;null graph;graph property;regular graph;clique-width;distance-regular graph;simplex graph;cubic graph;voltage graph;distance-hereditary graph;butterfly graph;quartic graph;complement graph;line graph;string graph;strength of a graph;circulant graph;coxeter graph	OS	21.53563020400288	32.51322799208604	21656
5c37f186471bf473a35732226757ecf9b8bbeced	accelerated waveform methods for parallel transient simulation of semiconductor devices	accelerated waveform method;semiconductor device;parallel transient simulation;semiconductor devices	In this paper we compare accelerated waveform relaxation algorithms to pointwise direct and iterative methods for the parallel transient simulation of semiconductor devices on parallel machines. Experimental results are presented for simulations on single (serial) workstations, clusters of workstations, and an IBM SP-2. The results show that accelerated waveform methods are competitive with standard pointwise methods on serial machines, but that accelerated waveform methods are significantly faster in loosely coupled MIMD parallel environments. In particular, parallel accelerated waveform methods achieve significant speedup on workstation clusters and achieve nearly linear speed-up (with respect to the number of processors) on the IBM SP-2. Experiments with parallel versions of standard pointwise methods exhibited limited or no parallel speedup. The strong implication of the results is that, as MIMD machines and cluster-based computing become more prevalent, accelerated waveform methods will gain in importance for all areas of simulation requiring the solution of initial value problems.	algorithm;central processing unit;iterative method;lagrangian relaxation;linear programming relaxation;loose coupling;mimd;semiconductor device;simulation;speedup;waveform;workstation	Mark W. Reichelt;Andrew Lumsdaine;Jacob K. White	1993		10.1145/259794.259840	embedded system;parallel processing;mathematical optimization;electronic engineering;parallel computing;semiconductor device;computer science;electrical engineering;theoretical computer science;operating system;iterative method;algorithm	HPC	-3.4611031229454903	38.346769796284356	21682
1b236f0627926ed3be8708eee6b065f054418060	streamorph: a case for synthesizing energy-efficient adaptive programs using high-level abstractions	interfilter channels;high-level abstractions;synchronous dataflow programming model;stream program properties;energy-efficient adaptive program synthesis;program state copying processes;environmental changes;high-level abstraction;consistent program state maintenance;streamorph technique;energy-efficient adaptive program;energy reduction;computation structures;operating frequency reduction;energy saving;streamorph method;sdf programming model;runtime stream graph transformation;communication structure;buffer size reduction;computing resource saving;runtime transformation;demand changes;adaptive program;memory usage optimization;data flow analysis;demand change;graph theory;program state copying process;off-idle core turning;intel xeon e5450;single program;stream program;streamit compiler;consistent program state;communication structures;stream program optimization;program compilers;adaptive systems;computer programming;optimization;data processing	This paper presents the concept of adaptive programs, whose computation and communication structures can morph to adapt to environmental and demand changes to save energy and computing resources. In this approach, programmers write one single program using a language at a higher level of abstraction. The compiler will exploit the properties of the abstractions to generate an adaptive program that is able to adjust computation and communication structures to environmental and demand changes.  We develop a technique, called StreaMorph, that exploits the properties of stream programs' Synchronous Dataflow (SDF) programming model to enable runtime stream graph transformation. The StreaMorph technique can be used to optimize memory usage and to adjust core utilization leading to energy reduction by turning off idle cores or reducing operating frequencies. The main challenge for such a runtime transformation is to maintain consistent program states by copying states between different stream graph structures, because a stream program optimized for different numbers of cores often has different sets of filters and inter-filter channels. We propose an analysis that helps simplify program state copying processes by minimizing copying of states based on the properties of the SDF model.  Finally, we implement the StreaMorph method in the StreamIt compiler. Our experiments on the Intel Xeon E5450 show that using StreaMorph to minimize the number of cores used from eight cores to one core, e.g. when streaming rates become lower, can reduce energy consumption by 76.33% on average. Using StreaMorph to spread workload from four cores to six or seven cores, e.g. when more cores become available, to reduce operating frequencies, can lead to 10% energy reduction. In addition, StreaMorph can lead to a buffer size reduction of 82.58% in comparison with a straight-forward inter-core filter migration technique when switching from using eight cores to one core.	compiler;computation;dataflow programming;experiment;graph rewriting;high- and low-level;programmer;programming model;state (computer science)	Dai N. Bui;Edward A. Lee	2013	2013 Proceedings of the International Conference on Embedded Software (EMSOFT)		embedded system;parallel computing;real-time computing;data processing;computer science;graph theory;operating system;computer programming;distributed computing;programming language	Embedded	-3.982860148901601	52.84391896613239	21687
0e1590a0e21e9486d999cf254044214393320089	mapping semigroup array operations onto multicomputer with torus topology	hypercube;semigroup operations;multiprocessor systems;distributed multiprocessor systems;distributed computing system;torus;mapping	In this paper, an algorithm for mapping semigroup array operations onto distributed computer systems with toroidal topology is analyzed. This algorithm is based on the well-known butterfly scheme and mapping this scheme onto hypercube with subsequent XOR -embedding of the hypercube onto torus. We show that the hypercube-onto-torus mapping algorithm provides the time of the semigroup operation implementation on torus less than the time provided by an algorithm using a sequence of cyclic data shifts.	algorithm;exclusive or;parallel computing;toroidal graph;whole earth 'lectronic link	Mikhail S. Tarkov	2011		10.1145/1968613.1968768	torus;distributed computing;hypercube	HPC	13.82670018750799	35.53217475926759	21757
0b3e74a4f87242395e0b3473730dbba48219bfbc	fpga area reduction by multi-output function based sequential resynthesis	combinational resynthesis;sat based boolean matching;optimal logic depth;pins;network synthesis;logic design;boolean functions;computability;fpga area reduction;sequential circuits;resynthesis;boolean function;logic circuits;table lookup boolean functions combinational circuits computability field programmable gate arrays logic design mimo systems network synthesis sequential circuits;fpga;lut based fpga fpga area reduction multioutput function sequential resynthesis algorithm sat based boolean matching retiming optimal logic depth combinational resynthesis;sat logic synthesis fpga resynthesis;sat;mimo systems;logic synthesis;registers;field programmable gate arrays table lookup boolean functions logic circuits space technology simultaneous localization and mapping integrated circuit synthesis algorithm design and analysis computer science logic functions;simultaneous localization and mapping;logic functions;integrated circuit synthesis;lut based fpga;space technology;multioutput function;computer science;field programmable gate arrays;mimo;table lookup;sequential resynthesis algorithm;algorithm design and analysis;retiming;combinational circuits	We propose a new resynthesis algorithm for FPGA area reduction. In contrast to existing resynthesis techniques, which consider only single-output Boolean functions and the combinational portion of a circuit, we consider multi-output functions and retiming, and develop effective algorithms that incorporate recent improvements to SAT-based Boolean matching. Our experimental results show that with the optimal logic depth, the resynthesis considering multi-output functions reduces area by up to 0.4% compared to the one considering single-output functions, and the sequential resynthesis reduces area by up to 10% compared to combinational resynthesis when both consider multi-output functions. Furthermore, our proposed resynthesis algorithm reduces area by up to 16% compared to the best existing academic technology mapper, Berkeley ABC.	algorithm;boolean satisfiability problem;combinational logic;field-programmable gate array;retiming	Yu Hu;Victor Shih;Rupak Majumdar;Lei He	2008	2008 45th ACM/IEEE Design Automation Conference	10.1145/1391469.1391478	electronic engineering;logic synthesis;computer science;theoretical computer science;mathematics;boolean function;algorithm;field-programmable gate array	EDA	16.520397687131986	48.0811441413178	21833
68c5a16a2db4845a463d55e75842f23de66811ee	increasing test throughput through the implementation of parallel test on a 16-bit multimedia audio codec	codecs;digital logic;automatic test equipment;audio coding;telecommunication equipment testing;automatic test software;analog integrated circuits;multimedia communication;multimedia communication codecs telecommunication equipment testing automatic test equipment automatic test software audio coding;throughput circuit testing costs logic testing semiconductor device testing pressing electronics industry integrated circuit testing logic circuits analog integrated circuits;16 bit test throughput parallel test multimedia audio codec cost of test mixed signal industry high performance stereo codec test time improvement hp9490 mixed signal test system tester hardware dut board hardware test software;high performance	The cost of test is one of the most pressing issues faced by the semiconductor industry today. One important method for reducing the cost of testing is parallel test. Parallel test has been used successfully for many years for digital logic, memory, and analog integrated circuits, but is only now gaining popularity in the mixed signal industry. This paper explores the issues involved when deciding to use parallel test and describes the implementation of parallel test for a high performance, stereo, multimedia audio codec. Test time improvement and performance results are presented.	16-bit;codec;throughput	Harold Bogard;Celeste Repasky	1995		10.1109/TEST.1995.529862	boolean algebra;embedded system;automatic test equipment;electronic engineering;codec;real-time computing;computer science;test harness	HPC	10.844547831192545	41.791201360769804	21871
21a8e971519ff17665d203b0b45663b3963a5a1c	spirit: a highly robust combinational test generation algorithm	single path oriented propagation;circuit faults;redundancy identification spirit test pattern generation algorithm combinational circuit static learning dynamic learning boolean satisfiability implication graph data structure single cone processing single path oriented propagation backward justification;computability;automatic test pattern generation;automatic testing;robustness circuit testing test pattern generators automatic test pattern generation circuit faults combinational circuits data structures redundancy automatic testing system testing;indexing terms;test pattern generation algorithm;boolean satisfiability;implication graph;backward justification;test pattern generators;redundancy;data structures;logic testing;system testing;test generation;robustness;circuit testing;combinational circuit;single cone processing;redundancy identification;data structure;dynamic learning;static learning;spirit;redundancy computability automatic test pattern generation combinational circuits logic testing;combinational circuits	1 This work was supported by JSPS under grant P99747 2 Currently visiting at Nara Institute of Science and Technology Abstract In this paper, we present a robust test generation algorithm for combinational circuits based on the Boolean satisfiability method called SPIRIT. We elaborate some well-known techniques as well as present new techniques that improve the performance and robustness of test generation algorithms. As a result, SPIRIT achieves 100% fault efficiency for a full scan version of the ITC'99 benchmark circuits in a reasonable amount of time.	algorithm;benchmark (computing);boolean satisfiability problem;combinational logic;javaserver pages	Emil Gizdarski;Hideo Fujiwara	2002	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/TCAD.2002.804387	data structure;computer science;theoretical computer science;machine learning;mathematics;combinational logic;programming language;algorithm	EDA	19.478951551020852	48.276711353240245	21904
819faec9ff40118fc0dcb826fdf06be8877aeb19	two-dimensional multirate systolic array design for artificial neural networks	biology computing;neural networks;neural nets;clocks;very large scale integration;systolic arrays;2 dimensional pipelining;systolic array;multirate clocking;systolic arrays neural nets pipeline processing;2 dimensional;computer vision;artificial neural networks;neural net;weight passing scheme;propagation delay;hopfield neural net;hopfield neural net weight passing scheme multirate systolic array design artificial neural networks 2 dimensional pipelining 2d systolic implementation multirate clocking;systolic arrays artificial neural networks signal processing algorithms very large scale integration biological neural networks computer vision clocks pipeline processing biology computing neural networks;2d systolic implementation;signal processing algorithms;biological neural networks;pipeline processing;artificial neural network;multirate systolic array design;neural network	In this paper a novel design of neural networks using 2-dimensional systolic array is proposed. Two techniques are applied in the design, namely, 2-dimensional pipelining and multirate processing (2 level clocking). 2-dimensional pipelining operation gives significant improvement in computation time compared to the currently known 1D and 2D systolic implementation schemes. Besides, multirate clocking is used so that weights (synapses) can be transmitted and passed systolically in a rate much higher than activation voltages, to achieve maximum array throughput and to eliminate global interconnections present in many array (including systolic) designs (thus reducing synchronization and propagation delay problems). This scheme of passing weights also saves area significantly since local storage area for the weights can be reduced. The design is applied to the implementation of a Hopfield neural net. >	artificial neural network;systolic array	E. R. Khan;Nam Ling	1991		10.1109/GLSV.1991.143964	propagation delay;electronic engineering;two-dimensional space;parallel computing;systolic array;computer science;theoretical computer science;very-large-scale integration;artificial neural network	EDA	13.146651678039307	43.43319289818635	21914
06acc5642563505d4217be427e967c9265d10ad7	gpu parallel implementation of the approximate k-svd algorithm using opencl	paper;ati;signal processing;algorithms;opencl;ati firepro v8800	Training dictionaries for sparse representations is a time consuming task, due to the large size of the data involved and to the complexity of the training algorithms. We investigate a parallel version of the approximate K-SVD algorithm, where multiple atoms are updated simultaneously, and implement it using OpenCL, for execution on graphics processing units (GPU). This not only allows reducing the execution time with respect to the standard sequential version, but also gives dictionaries with which the training data are better approximated. We present numerical evidence supporting this somewhat surprising conclusion and discuss in detail several implementation choices and difficulties.	approximation algorithm;computer graphics;dictionary;gauss–seidel method;graphics processing unit;in the beginning... was the command line;iteration;jacobi method;k-svd;numerical analysis;opencl api;parallel algorithm;refinement (computing);run time (program lifecycle phase);singular value decomposition;sparse matrix	Paul Irofti;Bogdan Dumitrescu	2014	2014 22nd European Signal Processing Conference (EUSIPCO)		parallel computing;theoretical computer science;distributed computing;general-purpose computing on graphics processing units	HPC	-1.9815199391189793	39.11179451461945	22071
9983756829a54c23b0d625b49fc6015f56433c1a	analysis of error masking and restoring properties of sequential circuits	error masking;sequential circuits integrated circuit reliability data structures boolean functions integrated circuit modeling probabilistic logic;boolean functions;sequential circuits;binary decision diagrams bdds;soft errors finite state machines fsms error masking sequential circuits state transition matrices stms transition probability matrices binary decision diagrams bdds;transition probability matrices;matrix algebra;bdd approach error masking property error restoring property sequential circuit cmos technology scaling complimentary metal oxide semiconductor logic circuit feedback signal soft error restoring input switching theory state transition matrix stm binary decision diagram finite state machine fsm model stm analysis;sequential circuits binary decision diagrams finite state machines matrix algebra;finite state machines;binary decision diagrams;data structures;state transition matrices stms;integrated circuit modeling;soft errors;probabilistic logic;integrated circuit reliability;finite state machines fsms	Scaling of CMOS technology into nanometric feature sizes has raised concerns for the reliable operation of logic circuits, such as in the presence of soft errors. This paper deals with the analysis of the operation of sequential circuits. As the feedback signals in a sequential circuit can be logically masked by specific combinations of primary inputs, the cumulative effects of soft errors can be eliminated. This phenomenon, referred to as error masking, is related to the presence of so-called restoring inputs and/or the consecutive presence of specific inputs in multiple clock cycles (equivalent to a synchronizing sequence in switching theory). In this paper, error masking is extensively analyzed using the operations of state transition matrices (STMs) and binary decision diagrams (BDDs) of a finite state machine (FSM) model. The characteristics of state transitions with respect to correlations between the restoring inputs and time sequence are mathematically established using STMs; although the applicability of the STM analysis is restricted due to its complexity, the BDD approach is more efficient and scalable for use in the analysis of large circuits. These results are supported by simulations of benchmark circuits and may provide a basis for further devising efficient and robust implementations when designing FSMs.	benchmark (computing);binary decision diagram;cmos;clock signal;finite-state machine;logic gate;scalability;sequential logic;simulation;soft error;software transactional memory;state transition table;switching circuit theory;synchronizing word;time series	Jinghang Liang;Jie Han;Fabrizio Lombardi	2013	IEEE Transactions on Computers	10.1109/TC.2012.147	parallel computing;data structure;computer science;theoretical computer science;sequential logic;probabilistic logic;finite-state machine;boolean function;algorithm	EDA	22.62063789896286	49.96874657458569	22081
678d1b3b501a8883975e8a7a146563cc17489941	mitigating nbti in the physical register file through stress prediction	stress;negative bias temperature instability;degradation;reliability;resource management;failure analysis;registers;transistors;sram chips failure analysis integrated circuit noise integrated circuit reliability negative bias temperature instability;integrated circuit reliability;registers stress transistors degradation reliability resource management;integrated circuit noise;size 22 nm stress prediction transistor parameter value degradation negative bias temperature instability reliability problem transistor generation nbti aging sram cell noise margin failure rate system failure instruction granularity physical register file design size 45 nm size 32 nm;sram chips	Degradation of transistor parameter values due to Negative Bias Temperature Instability (NBTI) has emerged as a major reliability problem in current and future transistor generations. NBTI Aging of SRAM cell leads to a lower noise margin, thereby increasing the failure rate. The physical register file, which consists of an array of SRAM cells, can suffer from data loss, leading to system failure. In this paper, we explore a novel approach by investigating NBTI stress and mitigation at the instruction granularity. While a wide range of NBTI stress exists in different registers, the stress induced by specific instructions is highly predictable. Using such a prediction mechanism, we propose an NBTI tolerant power efficient physical register file design. Our approach improves the noise margin in a register file by 20%, 32%, and 125% for the 45nm, 32nm, and 22nm technology nodes, respectively. Overall, we observe 14.8% power saving and a 19.8% area penalty in the register file.	cell (microprocessor);elegant degradation;failure rate;negative-bias temperature instability;noise margin;register file;semiconductor industry;sensor;static random-access memory;transistor	Saurabh Kothawade;Dean Michael Ancajas;Koushik Chakraborty;Sanghamitra Roy	2012	2012 IEEE 30th International Conference on Computer Design (ICCD)	10.1109/ICCD.2012.6378662	negative-bias temperature instability;failure analysis;electronic engineering;real-time computing;degradation;engineering;electrical engineering;resource management;operating system;reliability;processor register;stress;transistor	EDA	19.915071562497392	58.16795245771383	22084
14fc828b3e686df94e274c9be0ab7f4b6b69f288	dynamic reconfigurable architectures and transparent optimization techniques - automatic acceleration of software execution				Antonio Carlos Schneider Beck;Luigi Carro	2010		10.1007/978-90-481-3913-2		EDA	4.049307525168354	49.19996307838975	22085
11018f336c897ce49a359b83613656f450baf3f3	a code width built-in-self test circuit for 8-bit pipelined adc	bist;generators;word length 8 bit pipelined adc code width built in self test circuit bist scheme analog to digital converter analog preprocessing input signal range residue signal amplification switched capacitor circuits arbitrary faults histogram based analysis techniques differential nonlinearity integral nonlinearity nonmonotonic behavior missing code fault;linearity;circuit faults;radiation detectors;radiation detector;integral non linearity;missing code fault;built in self test;built in self test analogue digital conversion;registers;analogue digital conversion;analog to digital converter;dnl;fault coverage;code width;differential non linearity;circuit faults built in self test radiation detectors generators linearity registers;missing code fault bist code width fault coverage dnl inl;inl;switched capacitor	This paper presents a novel built-in-self-test (BIST) scheme based on code-width and sample difference. The proposed BIST scheme is applied on 8-bit pipelined ADC (Analog to Digital Converter). An 8-bit pipelined ADC is designed. This pipelined ADC uses analog preprocessing to divide the input signal range into sub-intervals and amplification of a residue signal for further processing in the subsequent stages. The realization of the preprocessing stages has been implemented using switched-capacitor circuits. The proposed BIST scheme is verified by simulation of 8 bit pipelined ADC with arbitrary faults. The proposed method is alternative to histogram based analysis techniques to provide test time improvements. In addition to the measurement of DNL (Differential Non Linearity) and INL (Integral Non Linearity), non monotonic behavior and missing code fault have been detected.	8-bit;analog-to-digital converter;built-in self-test;noise reduction;preprocessor;simulation;switched capacitor	Alok Barua;Md. Tausiff	2011	2011 21st International Conference on Systems Engineering	10.1109/ICSEng.2011.58	embedded system;electronic engineering;real-time computing;mathematics;particle detector	EDA	23.585886903264484	50.686835955191526	22096
ea366417c957507bfb004a6f54594d4e7c4479f5	increasing the locality of iterative methods and its application to the simulation of semiconductor devices	semiconductor device simulation;scientific application;finite element simulation;data locality;irregular codes;semiconductor devices;iterative methods;data access;reordering techniques;sparse matrix;linear equations;iteration method;semiconductor devices simulation	Irregular codes are present in many scientific applications , such as finite element simulations. In these simulations the solutio n of large sparse linear equation systems is required, which are often solved using i terat ve methods. The main kernel of the iterative methods is the sparse matrix–ve ctor multiplication which frequently demands irregular data accesses. Therefo r , techniques that increase the performance of this operation will have a great im pact on the global performance of the iterative method and, as a consequence, o the simulations. In this paper a technique for improving the locality of spars e matrix codes is presented. The technique consists of reorganizing the data guided by a locality model instead of restructuring the code or changing the spar se matrix storage format. We have applied our proposal to different iterative m thods provided by two standard numerical libraries. Results show an impact on the overall performance of the considered iterative method due to the increase in th locality of the sparse matrix–vector product. Noticeable reductions in th e execution time have been achieved both in sequential and in parallel executions . This positive behavior allows the reordering technique to b e successfully applied to real problems. We have focused on the simulation of semico nductor devices and in particular on theBIPS3D simulator. The technique was integrated into the simulator. Both sequential and parallel executions have be en extensively analyzed in this paper. Noticeable reductions in the execution time r equired by the simulations are observed when using our reordered matrices in co mparison with the original simulator.	code;constructor (object-oriented programming);finite element method;iterative method;library (computing);linear equation;locality of reference;numerical analysis;overhead (computing);run time (program lifecycle phase);semiconductor device;simulation;sparse matrix;system of linear equations;the matrix	Juan Carlos Pichel;Dora Blanco Heras;José Carlos Cabaleiro;Antonio J. García-Loureiro;Francisco F. Rivera	2010	IJHPCA	10.1177/1094342009106416	mathematical optimization;parallel computing;computer science;theoretical computer science;operating system;iterative method;matrix-free methods;algorithm;algebra	HPC	-3.298875949985588	38.543693714544524	22120
14eafcb8fda826a6df15620340c3efcfd48960ad	exploiting non-inherent parallelism in an internal mergesort	internal mergesort;non-inherent parallelism	Sorting and searching large data sets are two of the most widely re­ searched topics in the field of Com­ puter Science. Many sorting algorithms have been discovered, each with its own advantages and disadvantages (see [3] and [4] for examples). We present an algorithm which effectively utilizes a shared-memory, multi-processor to merge two large sorted lists. This algorithm exploits more parallelism than that inherent in the typical mergesort al­ gorithm and gets excellent 3peedups. We report the results of sorting a vec­ tor of 50,000 initially randomly or­ dered integers on each of four ma­ chines: a Sequent Balance 21000, a Sequent Symmetry, an Encore Multimax, and an Alliant FX/8.	alliant computer systems;merge sort;multiprocessing;parallel computing;randomness;sequent calculus;shared memory;sorting algorithm;tor messenger	Susan R. Wallace	1990		10.1145/98949.99044	data parallelism;instruction-level parallelism;task parallelism	Theory	0.10837870977034145	37.09198186399662	22123
c98dded7b8e9e84865dcc8e3dd40a3f886857628	multi-objective optimization of multimedia embedded systems using genetic algorithms and stochastic simulation		To meet the ever shrinking time-to-market for multimedia embedded systems, designers need effective system-level optimization techniques to support their design decisions. Despite multimedia embedded systems’ highly variable execution times and soft real-time constraints, most previous work has adopted a constant execution time (worst-case) approach to evaluate if a candidate architecture satisfies the timing constraints. Such an approach is too pessimistic and might result in unnecessary costly architectures. In this work, we propose a new method for design space exploration of multimedia embedded systems. Given a system specification, the proposed method automatically explores the design space to quickly identify Pareto-optimal solutions (or an approximation) that optimize conflicting design metrics, such as price and power consumption. Our approach combines (i) a fast and formal strategy for performance evaluation that captures the varying runtime behavior of multimedia systems and (ii) a new multi-objective genetic algorithm for architecture exploration. The experiments on well-known benchmarks show the efficiency of our method in comparison to similar ones.	embedded system;genetic algorithm;mathematical optimization;multi-objective optimization;simulation;stochastic gradient descent	Bruno Costa e Silva Nogueira;Paulo Romero Martins Maciel;Eduardo Antonio Guimarães Tavares;Ricardo Martins;Ermeson Carneiro de Andrade	2017	Soft Comput.	10.1007/s00500-016-2061-x	genetic algorithm;theoretical computer science;mathematical optimization;computer science;embedded system;multicube;architecture;real-time computing;multimedia;stochastic simulation;design space exploration;multi-objective optimization;system requirements specification	EDA	0.798148840955764	55.8514904592342	22149
6325f83f7cb5c599f59a995bd60572ba0c53a7e4	fasolt: a program for feedback-driven data-path optimization	langage description donnee;hardware design languages;cycle time;bus topology;topology;equation differentielle;optimisation;synthese circuit;implantation topometrie;microprocesseur mos 6502;network synthesis;fasolt;data path optimization;register level data path design;cad;flot donnee;circuit vlsi;delay effects;design optimization;average delay cad fasolt data path optimization bus topology register level data path design layout model feedback drive system scheduling allocation levels topology synthesis wiring automatic synthesis system cycle time;network topology;feedback;feedback drive system;vlsi circuit cad feedback logic cad network topology optimisation;adders;scheduling;sonde muonique;topology synthesis;vlsi;layout model;resolution equation;average delay;circuit synthesis design optimization topology network synthesis signal synthesis delay effects hardware design languages adders feedback costs;circuit cad;graphe fluence;signal synthesis;reconnaissance forme;automatic synthesis system;allocation levels;wiring;application;logic cad;circuit synthesis;circuit integre;programme ordinateur;acheminement	The author describes Fasolt, a system that automatically optimizes the bus topology of a register-level data-path design. The unique aspect of Fasolt is that it uses information taken from a detailed layout model to choose optimizing transformations of the bus topology and the schedule of operations and transfers of an existing design. Such a system is called a feedback-drive system because it uses information derived at a low level (in this case, that of physical layout) to drive the selection of optimizing transformations at a higher level or levels (in this case, the scheduling and allocation levels). This allows the scheduling and topology synthesis steps to take wiring considerations into account in a way that has hitherto not been demonstrated in an automatic synthesis system. Experiments have shown that improvements in area, cycle time, and overall average delay can often be achieved in the same design using this approach. >	mathematical optimization	David W. Knapp	1992	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.137515	control engineering;network synthesis filters;embedded system;electronic engineering;real-time computing;multidisciplinary design optimization;cycle time variation;computer science;cad;fundamental resolution equation;feedback;very-large-scale integration;scheduling;network topology;bus network;adder;computer network	EDA	13.758697844229223	51.097450501053146	22168
43c86ae25d8d7b84d9c802e829b81e891f1f29a4	a class of an almost-optimal size-independent parallel prefix circuits	computers;delay adders computational modeling integrated circuit modeling latches computers educational institutions;size independent parallel algorithms prefix circuist;fast adder almost optimal size independent parallel prefix circuit prefix computation;computational modeling;prefix circuist;adders;integrated circuit modeling;latches;size independent;combinational circuits;parallel algorithms	Prefix computation is one of the fundamental problems that can be used in many applications such as fast adders. Most proposed parallel prefix circuits assume that the circuit is of the same width as the input size. In this paper, we present a class of parallel prefix circuits that perform well when the input size, n, is more than the width of the circuit, m. That is, the proposed circuit is an almost optimal in speed when n >; m. Specifically, we derive a lower bound for the depth of the circuit and prove that the circuit requires one time step more than the optimal number of time steps needed to generate its first output. We also show that the size of the circuit is optimal within one. The input is divided into subsets each of width m-1 and presented to the circuit in subsequent time steps. The circuit is compared to other circuits to show its outperforming speed. The circuit is faster than any other circuit of the same width and fan-out.	computation;fan-out;information;pulse-width modulation	Hatem M. El-Boghdadi	2012	2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum	10.1109/IPDPSW.2012.225	equivalent circuit;parallel computing;computer science;theoretical computer science;parallel algorithm;combinational logic;circuit extraction;computational model;algorithm;adder	EDA	19.015715146576596	50.02883402458293	22211
6fda3e62b99634b588ad5c2b3fecaa247c69ff27	single-layer global routing	fabrication;concepcion asistida;computer aided design;homotopic transformation;integrated circuit;etude theorique;clocks;routing;very large scale integration;optimal postprocessing algorithm;wire length;circuit vlsi;routing sequence;circuit layout cad network routing vlsi;circuito integrado;single layer global routing;routed net maximization;network routing;tile size;routing path;wire;algorithme;algorithm;vlsi circuit;slgrp;congestion map;large scale integration;integrated circuit interconnections;global routing;routing very large scale integration tiles wire partitioning algorithms large scale integration integrated circuit interconnections fabrication clocks wiring;estudio teorico;rubber band equivalent routing;vlsi;conception assistee;homotopic routing;tile sequence;circuit layout cad;density algorithm;encaminamiento;tiles;vlsi layout;circuito vlsi;theoretical study;wiring;density algorithm single layer global routing slgrp homotopic routing rubber band equivalent routing routing sequence congestion map bounding box length routing path tile sequence algorithm routed net maximization tile size optimal postprocessing algorithm wire length homotopic transformation vlsi layout;bounding box length;circuit integre;acheminement;partitioning algorithms;algoritmo	Absmcr-We introduce the single-layer global routing problem (SLGRP), also called homotopic routing or rubber-bandequivalent routing, and propose a technique for solving it. Given a set of nets, the proposed technique first determines the routing sequence based on the estimated congestion, the bounding-box length and priority. Then, it finds a routing path, being a sequence of tiles, for each net (one net at a time), avoiding “congested” areas. The overall goal of the algorithm is to maximize the number of routed nets. The proposed global router is the first true singlelayer global router ever reported in the literature. The size of tiles, w x w, is an input parameter in our algorithm. For w = 1, the proposed global router serves as an effective detailed router. An optimal postprocessing algorithm, minimizing wire length and number of bends, under homotopic transformation, is presented. The technique has been implemented and tried out for randomly generated data. The algorithm is very efficient and produces good results.	algorithm;bend minimization;minimum bounding box;network congestion;parameter (computer programming);procedural generation;router (computing);routing	Majid Sarrafzadeh;Kuo-Feng Liao;Chak-Kuen Wong	1994	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.273751	routing table;routing;enhanced interior gateway routing protocol;electronic engineering;static routing;equal-cost multi-path routing;computer science;engineering;dynamic source routing;electrical engineering;theoretical computer science;multipath routing;destination-sequenced distance vector routing;computer aided design;very-large-scale integration;routing protocol;link-state routing protocol;engineering drawing;metrics;geographic routing;routing information protocol;computer network	EDA	15.34147287737404	51.133066596942	22260
3be618c1c2ddb88473c2f7b11afdc46a915b3790	square-rooting algorithms for high-speed digital circuits	carry propagation elimination;parallel reduction of summands;redundant number notation binary square rooting carry propagation elimination parallel reduction of summands;redundant number notation;digital circuits;high speed;binary square rooting	Two binary algorithms for the square rooting of a number or of a sum of two numbers are presented. They are based on the classical nonrestoring method. The main difference lies in the replacement of subtractions and additions by a parallel reduction f three summands, which may be positive and negative, to two summands to eliminate the carry propagation. Two of three summands form the successive partial remainder. Their most significant bit triples, sometimes together with a sign bit of the earlier partial remainder, are used to determine digits -1,0, +1 of a redundant square-root notation. These digits are transformed during the square-rooting process into the conventional notation square-root bits which are next used in further square-rooting steps to form the third reduced summands.	adder (electronics);algorithm;like button;most significant bit;sign bit;software propagation	Stanislaw Majerski	1985	IEEE Transactions on Computers	10.1109/TC.1985.1676618	arithmetic;discrete mathematics;electrical engineering;mathematics;digital electronics;algorithm	Theory	15.972843560051007	44.00211965163271	22262
fdecba06f9c86fec764e1915bb7f87807e6c9104	exploiting the expressiveness of cyclo-static dataflow to model multimedia implementations	signal image and speech processing;description systeme;circuit codeur;system description;coding circuit;concepcion sistema;video signal processing;debit information;information transmission;implementation;compresion senal;flux donnee;analyse temporelle;flujo datos;modele statique;buffer system;analisis temporal;indice informacion;compression signal;time analysis;sistema amortiguador;video coding;quantum information technology spintronics;codage video;system design;circuito codificacion;modelo estatico;signal compression;multimedia communication;economic aspect;traitement signal video;information rate;static model;aspect economique;descripcion sistema;transmision informacion;transmission information;data flow;implementacion;systeme tampon;communication multimedia;conception systeme;aspecto economico	• A submitted manuscript is the author's version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. • The final author version and the galley proof are versions of the publication after peer review. • The final published version features the final layout of the paper including the volume, issue and page numbers.	assignment zero;best, worst and average case;circular buffer;dataflow;electrical engineering;embedded system;encoder;iteration;model of computation;parallel computing;shared memory;throughput;video decoder	Kristof Denolf;Marco Bekooij;Johan Cockx;Diederik Verkest;Henk Corporaal	2007	EURASIP J. Adv. Sig. Proc.	10.1155/2007/84078	data flow diagram;simulation;telecommunications;computer science;bicarbonate buffering system;theoretical computer science;implementation;systems design	Crypto	15.754289802462107	40.161063725488255	22368
16ee25a4abab4952cf04abef222dd5b1db69e2bf	a fast error correction technique for matrix multiplication algorithms	fast error correction technique;cycle time;radiation induced soft errors;cmos technology;fault tolerant;matrix multiplication algorithms;software fault tolerance;temporal redundancy techniques;data mining;recomputation time;transient analysis;error correction cmos technology redundancy fault tolerance voltage pulse circuits space vector pulse width modulation costs cmos logic circuits logic devices;embedded systems;pulse circuits;fault tolerant computing;redundancy;fault tolerant systems;fault tolerant software execution;error correction;cmos logic circuits;fault tolerant algorithms;fault tolerance;voltage;software fault tolerance recomputation time fault tolerant software execution fault tolerant algorithms;single event transients fast error correction technique matrix multiplication algorithms temporal redundancy techniques radiation induced soft errors spatial redundancy techniques;matrix multiplication;societies;space vector pulse width modulation;computational efficiency;spatial redundancy techniques;high power;soft error;logic devices;matrix multiplication embedded systems error correction fault tolerant computing;single event transients	Temporal redundancy techniques will no longer be able to cope with radiation induced soft errors in technologies beyond the 45 nm node, because transients will last longer than the cycle time of circuits. The use of spatial redundancy techniques will also be precluded, due to their intrinsic high power and area overheads. The use of algorithm level techniques to detect and correct errors with low cost has been proposed in previous works, using a matrix multiplication algorithm as the case study. In this paper, a new approach to deal with this problem is proposed, in which the time required to recompute the erroneous element when an error is detected is minimized.	error detection and correction;matrix multiplication algorithm	Costas Argyrides;Carlos Arthur Lang Lisbôa;Dhiraj K. Pradhan;Luigi Carro	2009	2009 15th IEEE International On-Line Testing Symposium	10.1109/IOLTS.2009.5195995	reliability engineering;fault tolerance;electronic engineering;parallel computing;real-time computing;computer science	Arch	8.429670983889931	59.14096301951569	22384
bde08c21b2c6b8f0777fd4aeac72160d596f91d1	a systolic design-rule checker	verification;spacing;fabrication;red sistolica;concepcion asistida;systolic systems;computer aided design;wire routing geometry algorithm design and analysis computer architecture design automation logic chip scale packaging circuits complexity theory;errors;interconnection;automatic monitoring;integrated circuit;design rule checking;poligono regular;circuito integrado;polygone regulier;regular polygon;chip;design rules;computer architecture;câblage;systolic network;polygons;controle automatique;chips electronics;interconnexion;colocacion cablesl 579170;distancia;reseau systolique;conception assistee;arquitectura;largeur;algorithms;control automatico;design rule checks;width;verificacion;wiring;edges;ancho;systolic systems design rule checks feature width spacing rectilinear geometries;architecture;circuit integre;distance;interconeccion;rectilinear geometries;feature width	We develop a systolic design rule checker (SDRC) for rectilinear geometries. This SDRC reports all width and spacing violations. It is expected to result in a significant speed up of the design rule check phase of chip design.	euclidean distance;global serializability;regular grid;speedup;transistor;von luschan's chromatic scale	Rajiv Kane;Sartaj Sahni	1984	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.1987.1270242	embedded system;electronic engineering;computer science;design rule checking;architecture;operating system;computer aided design;mathematics;engineering drawing;algorithm	EDA	17.93092589964334	38.74915265774074	22392
019976fae6eac7ee3913bd59c45c767f5415d825	synthesis of customized loop caches for core-based embedded systems	cache storage;circuit cad;embedded systems;low-power electronics;memory architecture;microprocessor chips;program control structures;core-based embedded systems;customized loop cache synthesis;embedded microprocessor based systems;embedded system program loops;fast estimation-based approach;instruction fetch energy;instruction memory hierarchy;loop cache configuration;loop cache exploration tool;loop cache size;power savings;program profile;simulation-based approach	Embedded system programs tend to spend much time in small loops. Introducing a very small loop cache into the instruction memory hierarchy has thus been shown to substantially reduce instruction fetch energy. However, loop caches come in many sizes and variations-using the configuration best on the average may actually result in worsened energy for a specific program. We therefore introduce a loop cache exploration tool that analyzes a particular program's profile, rapidly explores the possible configurations, and generates the configuration with the greatest power savings. We introduce a simulation-based approach and show the good energy savings that a customized loop cache yields. We also introduce a fast estimation-based approach that obtains nearly the same results in seconds rather than tens of minutes or hours.	embedded system	Susan Cotterell;Frank Vahid	2002		10.1109/ICCAD.2002.1167602	loop tiling;embedded system;loop inversion;estimation;computer architecture;parallel computing;real-time computing;loop fission;loop interchange;computer science;loop nest optimization;operating system;low-power electronics;inner loop	EDA	-1.74775952546333	54.11932644891986	22430
e87311a9e10b4a75dfc38ee9a06630850a9dd8b8	ilp-based alleviation of dense meander segments with prioritized shifting and progressive fixing in pcb routing	printed circuit board pcb routing;crosstalk;routing;delay lines;nickel;ilp model;wires;printed circuit design;speedup effect;dense meander segments;integrated circuit modeling;ilp model pcb routing speedup effect dense meander segments;article;wires routing delays crosstalk nickel integrated circuit modeling delay lines;pcb designs ilp based alleviation dense meander segments prioritized shifting progressive fixing pcb routing length matching bus signals printed circuit board wire length compensation;delays	Length-matching is an important technique to balance delays of bus signals in high-performance printed circuit board (PCB) routing. Existing routers, however, may generate very dense meander segments. Signals propagating along these meander segments exhibit a speedup effect due to crosstalk between the segments of the same wire, thus leading to mismatch of arrival times even under the same physical wire length. In this paper, we present a post-processing method to enlarge the width and the distance of meander segments and hence distribute them more evenly on the board so that crosstalk can be reduced. In the proposed framework, we model the sharing of available routing areas after removing dense meander segments from the initial routing, as well as the generation of relaxed meander segments and their groups for wire length compensation. This model is transformed into an ILP problem and solved for a balanced distribution of wire patterns. In addition, we adjust the locations of long wire segments according to wire priorities to swap free spaces toward critical wires that need much length compensation. To reduce the problem space of the ILP model, we also introduce a progressive fixing technique so that wire patterns are grown gradually from the edge of the routing toward the center area. Experimental results show that the proposed method can expand meander segments significantly even under very tight area constraints, so that the speedup effect can be alleviated effectively in high-performance PCB designs.	crosstalk;paging;printed circuit board;printing;problem domain;progressive scan;router (computing);routing;solver;speedup;video post-processing	Tsun-Ming Tseng;Bing Li;Tsung-Yi Ho;Ulf Schlichtmann	2015	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2015.2402657	nickel;routing;electronic engineering;crosstalk;telecommunications;computer science;engineering;engineering drawing;computer network	EDA	15.37545836577123	52.81131407126732	22434
8290cbb4fb4511c6081dbe9be84c439774dbc680	jpeg, mpeg-4, and h.264 codec ip development	code development;optimisation;image codec;image coding;mpeg 4 standard computer architecture algorithm design and analysis design methodology hardware video codecs design optimization discrete cosine transforms partitioning algorithms bandwidth;optimisation integrated circuit design video codecs video coding image coding application specific integrated circuits design engineering;architecture exploration;design engineering;tool framework;design optimization;h 264 codec ip development;faulty behavior;video codec;defect simulation;video coding;computer architecture;integrated circuit design;mpeg 4 standard;application specific integrated circuits;discrete cosine transforms;mpeg 4 codec ip development;system analysis;test generation;bandwidth;video codecs;algorithm optimization;code development jpeg codec ip development mpeg 4 codec ip development h 264 codec ip development image codec custom video codecs system analysis algorithm optimization architecture exploration;jpeg codec ip development;dram testing;algorithm design and analysis;custom video codecs;partitioning algorithms;hardware;design methodology	This paper summarizes our design experiences of various image and video codec IPs. The design issues and methodology of custom video codecs are discussed. The design methodology can be summarized as four stages, system analysis, algorithm optimization, architecture exploration, and code development. Based on these guidelines, several design cases are presented, including the proposed JPEG, MPEG-4, and H.264 architectures.	algorithm;codec;h.264/mpeg-4 avc;jpeg;mathematical optimization;system analysis	Chung-Jr Lian;Yu-Wen Huang;Hung-Chi Fang;Yung-Chi Chang;Liang-Gee Chen	2005	Design, Automation and Test in Europe	10.1109/DATE.2005.193	embedded system;algorithm design;electronic engineering;real-time computing;multidisciplinary design optimization;design methods;computer science;system analysis;application-specific integrated circuit;bandwidth;integrated circuit design	EDA	10.927811549323064	41.0579489507263	22481
2ffcabeded1da3191b0c06d96e5fec8b710041da	designing of test pattern generators for stimulation of crosstalk faults in bus-type connections	circuit faults;crosstalk;crosstalk circuit faults integrated circuit interconnections system on chip built in self test radiation detectors hardware;radiation detectors;system on a chip integrated circuit interconnections crosstalk test pattern generator built in self test;built in self test;system on chip;integrated circuit interconnections;system on chip built in self test crosstalk design for testability ieee standards integrated circuit design integrated circuit modelling integrated circuit reliability integrated circuit testing;test pattern generator design ip cores built in self test circuits ieee 1500 standard scan path design for testability solutions tpg structure output test sequence hardware overhead tpg structure feature vhdl language digital bus crosstalk faults extended maximum aggressor fault model xmafm mafm test vector sequence tpg design soc system on chip crosstalk fault detection bus type connections crosstalk fault stimulation;hardware	The paper reveals a completely new and original structure of a Test Pattern Generator (TPG) dedicated for detection of crosstalk faults that may happen to bus-type connections between individual cores of a System on a Chip (SoC). The TPG is designed to generate either the MAFM (Maximum Aggressor Fault Model) sequence of test vectors or the XMAFM (eXtended Maximum Aggressor Fault Model) one, which guarantees that all possible crosstalk faults of the capacitive nature that may occur between individual lines of a digital bus are detectable. The study presents the synthesizable and parameterized (scalable) model of the mentioned TPG developed in the VHDL language. The proposed TPG structures feature with high working frequency and good scalability in terms of both the hardware overhead and length of the output test sequence. In addition, the TPG structure enables easy integration of the solution with Design for Testability solutions, such as a scan path, a wrapper designed in compliance with the IEEE 1500 standard or with the Built-in Self-Test circuits that might be already implemented in IP cores embedded in a SoC. In such a case the area overhead of the proposed structure never exceeds several dozens of equivalent gates, which is a negligible amount.	built-in self-test;bus (computing);crosstalk;design for testing;embedded system;fault model;overhead (computing);scalability;semiconductor intellectual property core;system on a chip;transmission line;vhdl	Tomasz Garbolino	2014	17th International Symposium on Design and Diagnostics of Electronic Circuits & Systems	10.1109/DDECS.2014.6868807	system on a chip;embedded system;electronic engineering;real-time computing;crosstalk;computer science;engineering;automatic test pattern generation;particle detector	EDA	20.73513299298503	52.10365174018342	22494
bd8ed22fec34a66a0abb4da641e0cdde64ca9f4f	a unified design methodology for a hybrid wireless 2-d noc	wireless communication transceivers integrated circuit interconnections design methodology antennas substrates hybrid power systems;design engineering;network on chip;wireless communication;hybrid power systems;integrated circuit interconnections;antennas;design space exploration unified design methodology hybrid wireless 2d noc communication throughput flat mesh based noc multifaceted design design complexity path loss model wireless channel;transceivers;substrates;network on chip design engineering;design methodology	Hybrid wireless NoCs are proposed to improve the communication throughput and energy compared to flat mesh-based NoCs. However, the multi-faceted design of a hybrid wireless NoC presents a conundrum amongst the different design paradigms. In this work, a methodology with minimal design complexity is presented for the design of hybrid wireless NoCs encompassing the different paradigms. To illustrate one use of the methodology, path loss models for the wireless channel implemented with various operating frequencies and substrate types are presented which are to be used in the design space exploration of the NoC.	design space exploration;exemplification;faceted classification;interdependence;network on a chip;overhead (computing);throughput	Ankit More;Baris Taskin	2012	2012 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2012.6272113	embedded system;electronic engineering;design methods;telecommunications;computer science;engineering;antenna;network on a chip;wi-fi array;wireless;computer network;transceiver	Embedded	3.5733093440970856	59.60568086116736	22554
650fc9c2af9cd3b19eb4994868b366f69a0b9ca4	track placement: orchestrating routing structures to maximize routability	performance estimation;fft;fpga;automatic generation;fpga architecture;matrix multiplicaiton;energy efficient design techniques;optimal algorithm	The design of a routing channel for an FPGA is a complex process, requiring the careful balance of flexibility with silicon efficiency. With the growing move towards embedding FPGAs into SoC designs, and the opportunity to automatically generate FPGA architectures, this problem becomes even more critical. The design of a routing channel requires determining the number of routing tracks, the length of the wires in those tracks, and the positioning of the breaks on the tracks. This paper focuses on the last of these problems, the placement of breaks in tracks to maximize overall flexibility. We have developed both an optimal algorithm and a number of heuristics to solve the track placement problem. The optimal algorithm finds a best solution provided the problem meets a number of restrictions. Most of the heuristics are without restrictions, and the most promising of these find solutions on average within 1.13% of optimal. Full text available as UWEE Technical Report #UWEETR-2002-0013 at http://www.ee.washington.edu/.	algorithm;field-programmable gate array;heuristic (computer science);routing;system on a chip	Katherine Compton;Scott Hauck	2003		10.1145/611817.611864	embedded system;fast fourier transform;parallel computing;real-time computing;simulation;matrix multiplication;computer science;theoretical computer science;destination-sequenced distance vector routing;operating system;field-programmable gate array	EDA	14.083442901017825	52.61771350445367	22564
debf5e3da9075909947eca04305f51bc7e1bed2d	obdd minimization based on two-level representation of boolean functions	minimisation of switching nets binary decision diagrams boolean functions;minimization boolean functions data structures combinational circuits algorithm design and analysis central processing unit formal verification logic design polynomials input variables;boolean functions;minimisation of switching nets;cad;exact results;boolean function;low complexity;indexing terms;optimum variable ordering;formal verification;binary decision diagrams;polynomial time;algorithm design;thin boolean functions obdd minimization two level representation boolean functions boolean function classes low complexity series parallel functions evaluation machine;series parallel;binary decision diagram	ÐIn this paper, we analyze the basic properties of some Boolean function classes and propose a low complexity OBDD variable ordering algorithm, which is exact (optimum) to some classes of functions and very effective to general two-level form functions. We show that the class of series-parallel functions, which can be expressed by a factored form where each variable appears exactly once, can yield exact OBDD variable orderings in polynomial time. We also study the thin Boolean functions whose corresponding OBDDs can be represented by the form of thin OBDDs in which the number of nonterminal nodes is equal to the number of input variables. We show that a thin Boolean function always has an essential prime cube cover and the class of series-parallel functions is a proper subset of thin Boolean functions. We propose a heuristic viewing OBDDs as evaluation machines with function cube covers as their inputs and apply a queuing principle in the algorithm design. Our heuristic, the augmented Dynamic Shortest Cube First algorithm, is proven to be optimum for the seriesparallel functions and also be very effective for general two-level form functions. Experimental results on a large number of two-level form benchmark circuits show that the algorithm yields an OBDD total size reduction of over 51 percent with only 7 percent CPU time compared to the well-known network-based Fan-In Heuristic implemented in the SIS package. Comparing to the known exact results, ours is only 49 percent larger in size while only uses 0.001 percent CPU time. Index TermsÐBinary Decision Diagram, optimum variable ordering, CAD, formal	algorithm design;benchmark (computing);binary decision diagram;boolean algebra;central processing unit;computable function;computer-aided design;cube;heuristic;heuristic (computer science);integrated circuit;interpreter (computing);np-completeness;polynomial;programmable logic array;series-parallel graph;terminal and nonterminal symbols;time complexity;whole earth 'lectronic link	Yu-Liang Wu;Hongbing Fan;Malgorzata Marek-Sadowska;Chak-Kuen Wong	2000	IEEE Trans. Computers	10.1109/12.895868	combinatorics;discrete mathematics;boolean network;boolean expression;computer science;mathematics;boolean function;algorithm;algebra	EDA	18.396831227187675	45.92117149520545	22617
fa3cd8a2ff273c98a715eb90574b7380ef321d6f	a cell current compensation scheme for 3d nand flash memory	ctat 3d nand cell current temperature compensate ats;three dimensional displays temperature sensors temperature distribution flash memories temperature control computer architecture;temperature control;temperature sensors;silicon compensation elemental semiconductors flash memories nand circuits;computer architecture;three dimensional displays;si cell current compensation scheme 3d nand flash memory cell threshold voltage degradation temperature compensation scheme ats analog temp sensor ctat circuit bit line level cell threshold voltage distribution mlc nand silicon wafer;temperature distribution;flash memories	The 3D NAND, so-called vertical NAND has cell Vt degradation especially in low temperature, and it affects cell Vt distribution and shift when NAND operates. To solve this problem, the temperature compensation scheme by ATS(Analog Temp Sensor) using CTAT circuit has proposed. The temperature compensated bias controls the bit line level to generate temperature compensated cell Vt distribution. The proposed scheme has been implemented in 36 stacks, 256Gb MLC NAND and proved its effect in silicon wafer.	elegant degradation;flash memory;line level;multi-level cell;nand gate;wafer (electronics)	Sungwook Choi;KyuTae Park;Marco Passerini;HeeJoung Park;DoYoung Kim;ChiHyun Kim;KunWoo Park;Jinwoong Kim	2015	2015 IEEE Asian Solid-State Circuits Conference (A-SSCC)	10.1109/ASSCC.2015.7387432	embedded system;electronic engineering;computer hardware;computer science	EDA	17.519525034295626	59.873814906733095	22621
21dc8b9d7b3ee82a0f7ed71cb6b422a35518e1b6	ditto processor	performance evaluation;logic design;fault tolerant computing;redundancy;virtual machines;microprocessor chips	Concentration of design effort for current single-chip Commercial-Off-The-Shelf (COTS) microprocessors has been directed towards performance. Reliability has not been the primary focus. As supply voltage scales to accommodate technology scaling and to lower power consumption, transient errors are more likely to be introduced. The basic idea behind any error tolerance scheme involves some type of redundancy. Redundancy techniques can be categorized in three general categories: (1) hardware redundancy, (2) information redundancy, and (3) time redundancy. Existing time redundant techniques for improving reliability of a superscalar processor utilize the otherwise unused hardware resources as much as possible to hide the overhead of program re-execution and verification. However, our study reveals that re-executing of long latency operations contributes to performance loss. We suggest a method to handle short and long latency instructions in slightly different ways to reduce the performance degradation. Our goal is to minimize the hardware overhead and performance degradation while maximizing the fault detection coverage. Experimental studies through microarchitecture simulation are used to compare performance lost due to the proposed scheme with non-fault tolerant design and different existing time redundant fault tolerant schemes. Fourteen integer and floating-point benchmarks are simulated with 1.8~13.3% performance loss when compared with non-fault-tolerant superscalar processor.	benchmark (computing);categorization;elegant degradation;error-tolerant design;fault detection and isolation;fault tolerance;image scaling;microarchitecture simulation;microprocessor;overhead (computing);redundancy (engineering);redundancy (information theory);superscalar processor	Shih-Chang Lai;Shih-Lien Lu;Jih-Kwon Peir	2002		10.1109/DSN.2002.1028947	triple modular redundancy;reliability engineering;embedded system;parallel computing;logic synthesis;real-time computing;computer science;virtual machine;operating system;distributed computing;redundancy	Arch	6.845430078069163	60.07359221833182	22628
dab4bcf8472dcb4822704d6e7d4a6f5126e22ffe	allocation of fifo structures in rtl data paths	clocks merging automatic control high level synthesis shift registers cost function terminology;hls storage fifo structure rtl data path cost cdfg interconnect multicycle functional unit queue synthesis;queueing theory;cost reduction;high level synthesis;queueing theory high level synthesis;functional unit	"""Along with the functional units, storage and interconnects also contribute significantly to the data path cost. This paper addresses the issue of reducing the storage and interconnect cost by allocating queues for storing variables. In contrast to the earlier works, we support """"regular"""" cdfgs and multi-cycle functional units for queue synthesis. Initial results on HLS benchmark examples have been encouraging and show the potential of data path cost reduction using queue synthesis. A novel feature of our work is the formulation of the problem for a variety of FIFO structures with their own """"queueing"""" criteria."""	fifo (computing and electronics)	Heman Khanna;M. Balakrishnan	1997		10.1109/ICVD.1997.568064	embedded system;parallel computing;real-time computing;computer science;high-level synthesis;queueing theory;statistics	EDA	-1.0569770158081861	52.83573193678416	22632
0b404303e2f2b079ee160253fd929bdbcc049ea5	liming-constrained congestion-driven global routing		It is well known that congestion control becomes more and more important in modern grid-based routing process. In this paper, a timing-constrained congestion-driven global routing approach is proposed to obtain initial congestion-driven global routing result without destroying the timing constraint of any routing net, and a post-processing simulated-annealing-based rip-up-and-reroute improvement is proposed to release the congestion of the grid edges. As mentioned in experimental results, the proposed TCGR+STRR algorithm can obtain near 100% global routability in congestion control for the tested benchmark circuits.	algorithm;benchmark (computing);network congestion;routing;simulated annealing;video post-processing	Jin-Tai Yan;Shun-Hua Lin	2004	ASP-DAC 2004: Asia and South Pacific Design Automation Conference 2004 (IEEE Cat. No.04EX753)	10.1145/1015090.1015275	routing table;benchmark;routing;electronic engineering;static routing;real-time computing;simulation;process simulation;dsrflow;zone routing protocol;equal-cost multi-path routing;computer science;engineering;dynamic source routing;probabilistic method;multipath routing;destination-sequenced distance vector routing;routing protocol;link-state routing protocol;upper and lower bounds;triangular routing;network congestion;path vector protocol;geographic routing;routing information protocol;field-programmable gate array;computer network	EDA	15.459055659036439	52.3269735831824	22642
8377b2116bbf66abe11295c5115eec4b1f78c2be	statistical data stability and leakage evaluation of finfet sram cells with dynamic threshold voltage tuning under process parameter fluctuations	standard tied-gate;statistical distributions;6t sram cell topology;finfet technology;ig-finfet sram circuit;tied-gate finfet sram circuits;robust operation;dynamic threshold voltage tuning;standby power distribution;finfet sram cell;access transistor;sram chips;sram cell topology;static noise margin distribution;active power;circuit tuning;finfet sram circuit;leakage evaluation;double gate mosfet;process parameter fluctuations;size 32 nm;six transistor finfet static memory cell;cell area;cache memory;mosfet circuits;ig finfet sram cell;process variations;circuit stability;finfet static memory cell;independent-gate finfet sram cell;double-gate data access transistors;statistical data stability;sram cell;statistical power distributions;dynamic access transistor;finfet sram cells;statistical power;data access;process variation;fluctuations;cmos technology;threshold voltage;leakage current;dielectrics	A new six transistor (6T) FinFET static memory cell with dynamic access transistor threshold voltage tuning is evaluated in this paper for statistical power and stability distributions under process parameter variations. The independent-gate (IG) FinFET SRAM cell activates only one gate of the double-gate data access transistors during a read operation. The disturbance caused by the direct data access mechanism of the standard 6T SRAM cell topology is significantly reduced by dynamically increasing the threshold voltage of the access transistors. All the transistors in the presented SRAM cell are sized minimum without producing any data stability concerns. The average read static-noise-margin of the statistical samples with the independent gate bias technique is 82% higher as compared to the standard tied-gate FinFET SRAM cells under process variations. Furthermore, the IG-FinFET SRAM circuit reduces the average leakage power and the cell area by up to 53.3% and 17.5%, respectively, as compared to the standard tied-gate FinFET SRAM circuits sized for comparable data stability in a 32 nm FinFET technology.	cell (microprocessor);computer data storage;data access;memory cell (binary);spectral leakage;static random-access memory;transistor	Zhiyu Liu;Sherif A. Tawfik;Volkan Kursun	2008	9th International Symposium on Quality Electronic Design (isqed 2008)	10.1109/ISQED.2008.97	probability distribution;data access;electronic engineering;real-time computing;cpu cache;computer science;engineering;electrical engineering;statistical power;ac power;threshold voltage;process variation;statistics	Arch	18.835580361409814	59.15610861697779	22648
6b964a94f655beb40a465d6f1ce88ca69dc338ed	fpga-based reconfigurable computing ii	fpga-based reconfigurable	reconfigurable computing	field-programmable gate array;reconfigurable computing	J. Morris Chang;C. Dan Lo	2007	Microprocessors and Microsystems	10.1016/j.micpro.2007.01.003	computer architecture;parallel computing;reconfigurable computing;fpga prototype	EDA	4.546362056581437	49.283321798270876	22792
4220a2629348d534130eddab8146fec82058aa5f	impact of large scale integration on aerospace computers	pin minimization;size weight and power;self repair;integrated circuit;logic design;application software;multiprocessor systems;parity checking;aerospace computers digital;technology management;functional system organization;design simulation;computational modeling;large scale integration;energy consumption;large scale integration lsi;computer aided manufacturing;space technology;large scale integration costs energy consumption logic design computer aided manufacturing application software technology management space technology computational modeling computer simulation;computer simulation;standardization;standardization aerospace computers digital design simulation functional system organization large scale integration lsi parity checking pin minimization self repair	The continued development of Large Scale Integration DESIGN CONSIDERATIONS (LSI) presages the advent of a fourth generation of computers, and is causing an upheaval at all levels of computer technology-both As far as the computer designer is concerned, LSI technical and managerial. Today's computers use integrated circuit stands for the large scale integration of system design components containing at most ten gates per component; however, with component and circuit design. Further, the segreLSI is introducing hundreds of gates per component and will evengation of these disciplines is no longer possible. The tually evolve into thousands of gates per component. To the aero. . . space planner, this technological breakthrough of LSI means ystem designer must become familiar with the field of tremendous reductions in cost, size, weight, and power consumption the component designer and vice versa in order to effecof logic components, together with increased speed and reliability. tively utilize and gain from LSI. The new systems comHowever, for the aerospace planner to successfully implement ponent designer who emerges will then need to cope aerospace computers with LSI, the computer designers and managers with a batch of new design rules and problems. Since must reorient their methodology and goals. A multitude of new the design parameters of computer circuits, through design and cost considerations must be carefully scrutinized, and out of this must come the new techniques that will permit effective LSI, are undergoing revolutionary changes together incorporation of LSI in aerospace computers. with growing application requirements, it follows that Higher speed, a smaller system, greater reliability, and lower cost new system design techniques will be required. stem from the physical structure (more gates per component with no Some of the design considerations are increase in component size) of the LSI component. These inherent features of LSI, together with multiprocessor system organization, higher speed and smaller systems, point to future aerospace computers with capabilities equal to today's pin minimization, best ground base systems. greater system reliability,	circuit design;computer engineering;integrated circuit;multiprocessing;requirement;systems design;windows aero	Earl C. Joseph	1967	IEEE Trans. Electronic Computers	10.1109/PGEC.1967.264741	computer simulation;electronic engineering;application software;logic synthesis;simulation;computer science;engineering;electrical engineering;technology management;parity bit;operating system;integrated circuit;space technology;computational model;standardization;algorithm;computer engineering	EDA	9.939565978563214	56.73097740268622	22800
be56c52efcfc9ddaf400e4fb26d298389c3af249	four-bit transient-to-digital converter with a single rc-based detection circuit for system-level esd protection	system level esd protection;rc based detection;transient to digital converter;electrostatic discharge esd	A novel on-chip four-bit transient-to-digital converter with a single RC-based detection for system-level electrostatic discharge (ESD) protection design is proposed in this paper. The proposed transient to digital converter is designed to detect ESD-induced transient disturbances and transfer different ESD voltages into digital codes under system-level ESD tests. This work is simulated in a 65-nm CMOS process using a 2.5 V supply voltage. The simulation results show the correctness of the circuit function and effectively distinguish different voltage disturbances.	4-bit;cmos;code;correctness (computer science);discharger;electrostatic loudspeaker;simulation	Nan Han;Yuan Wang;Guangyi Lu;Jian Cao;Xing Zhang	2015	2015 IEEE 11th International Conference on ASIC (ASICON)	10.1109/ASICON.2015.7517133	embedded system;electronic engineering;engineering;electrical engineering	EDA	20.43451463675173	56.20398895647135	22812
be2e0aa27b0c1d7f3965edd53eabca132a448d7f	optimal strategies for maintaining a chain of relays between an explorer and a base camp	robot movil;relative position;reseau communication;plane;estrategia optima;mobile robot;distributed networks;performance;plan;chain;speed independent;68wxx;algorithme;optimal strategy;algorithm;robot mobile;informatique theorique;chaine;plano;decision;distancia;cadena;algorithms;distributed;rendimiento;communication;red de comunicacion;comunicacion;network communications;communication network;strategie optimale;moving robot;distance;computer theory;algoritmo;informatica teorica	We envision a scenario with robots moving on a terrain represented by a plane. A mobile robot, called explorer is connected by a communication chain to a stationary base camp. The chain is expected to pass communication messages between the explorer and the base camp. It is composed of simple, mobile robots, called relays. We are investigating strategies for organizing and maintaining the chain, so that the number of relays employed is minimized and nevertheless the distance between neighbored relays in the chain remains bounded. We are looking for local and distributed strategies employed by restricted relays that have to base their decision (“Where should I go?”) solely on the relative positions of its neighbors in the chain. We present the Manhattan–Hopper and the Hopper strategy which improve the performance of all known solutions to this problem significantly. They are the first such strategies that are optimal in this setting, i.e., that allow the explorer to move with constant speed, independent of the length of the chain, and keep this length minimum up to a constant factor. © 2008 Elsevier B.V. All rights reserved. In this paper we will deal with the question on how to organize a chain of relays connecting two points on a plane. One of the points, the base camp, is stationary while the other is mobile and is called explorer. We have no influence on the movement of the explorer; the strategies we design control movement of the relays. We design strategies for the relays that keep the chain short and maintain connectivity despite movement of the explorer. The main challenge lies in the restriction that the relays have to decide locally, basing their decisions only on very limited information. Up to now we have informally spoken of a chain of relays. Hereby we mean an ordered sequence of relays, denoted by v1, . . . , vn. For technical reasons we include the explorer and the base camp into the chain, as respectively its first and last element; v0 describes the explorer and vn+1 the base camp. Elements of the chain will be referred to as stations, meaning either a relay, the explorer or the base camp. The main restriction on the chain is that we require the distance between stations neighbored in the chain not to exceed the transmission distance of 1. Thanks to this, the chain can forward communication messages between the base camp and the explorer. We want to design strategies for the relays which guide their movement on the plane. Each of the relays is assumed to execute its own copy of this strategy and is able to sense the environment. We will go deeper into detail on the computational and sensing abilities of relays when describing the relay model later. The greatest challenge for this problem is that we are looking for strategies which operate locally and with very simple logic. Therefore, we have to design a solution in which each relay computes its position on its own, where communication is not used at all, and relays possess no memory. We are interested to see whether one can design efficient strategies solving this problem within this simple model. ∗ Corresponding author. Tel.: +49 5251 606480. E-mail addresses: jaroslaw@kutylowski.de (J. Kutyłowski), fmadh@uni-paderborn.de (F. Meyer auf der Heide). 0304-3975/$ – see front matter © 2008 Elsevier B.V. All rights reserved. doi:10.1016/j.tcs.2008.04.010 3392 J. Kutyłowski, F. Meyer auf der Heide / Theoretical Computer Science 410 (2009) 3391–3405 Fig. 1. A long winding chain.	converge;hopper;mobile robot;organizing (structure);relay;stationary process;steiner tree problem;theoretical computer science	Jaroslaw Kutylowski;Friedhelm Meyer auf der Heide	2009	Theor. Comput. Sci.	10.1016/j.tcs.2008.04.010	mobile robot;simulation;performance;telecommunications;computer science;artificial intelligence;plane;mathematics;chain;plan;distance;algorithm;telecommunications network	Theory	18.83399722111675	32.46940943750086	22819
16cc5beb2984e8baf6bd7e283715ef9182866fb1	low voltage flash memory design based on floating gate soffet	voltage control;silicon on insulator;logic gates;threshold voltage;nonvolatile memory;capacitance;flash memories	Flash memory is widely used, especially for mobile applications, as nonvolatile memory storage. In this paper, we present a Flash memory design based on Silicon on Ferroelectric-Insulator FET (SOFFET) device. This device has shown tremendous potential for various ultra-low-power (ULP) applications. SOFFET has the potential to provide high performance, multi-VT design, strong threshold voltage control, low voltage operation, and below 60mV/decade subthreshold swing. The proposed approach utilizes conventional floating gate on SOI architecture with an additional ferroelectric insulator layer. The threshold voltage of the device is influenced by the charge accumulation on the floating gate as well as a control voltage. Due to the ferroelectric layer which allows the formation of negative capacitance effect inside a transistor, the presented Flash memory cell can be turned at a lower electric field. As a result, both program and erase operations can be performed at a lower voltage, making it ideal for low-power application. Moreover, the proposed Flash memory is compatible with conventional SOI fabrication process with minor adjustment.	ferroelectric ram;flash memory;low-power broadcasting;memory cell (binary);mobile app;non-volatile memory;semiconductor device fabrication;silicon on insulator;topological insulator;transistor;tree accumulation	Emeshaw Ashenafi;Azzedin D. Es-Sakhi;Masud H. Chowdhury	2016	2016 29th IEEE International System-on-Chip Conference (SOCC)	10.1109/SOCC.2016.7905452	ferroelectric ram;electronic engineering;sense amplifier;non-volatile memory;programmable metallization cell;logic gate;computer hardware;engineering;electrical engineering;silicon on insulator;capacitance;charge trap flash;non-volatile random-access memory;threshold voltage;overdrive voltage	EDA	16.994313574707977	59.60046454076398	22856
eb63ccf5322791b66d512ac492e62c442445b75c	a power- and area-efficient sram core architecture with segmentation-free and horizontal/vertical accessibility for super-parallel video processing	tratamiento paralelo;circuit decodeur;traitement signal;random access memory;tecnologia electronica telecomunicaciones;estimation mouvement;static random access memory;memoria acceso directo;image processing;traitement parallele;integrated circuit;video signal processing;estimacion movimiento;procesamiento imagen;video processing;motion estimation;circuito integrado;memoire acces direct statique;segmentation;circuito desciframiento;traitement image;decoding circuit;low power;parallel architectures;architecture parallele;signal processing;memoire acces direct;low power electronics;h 264;traitement signal video;sram;mpeg;procesador;image signal processing;tecnologias;processeur;grupo a;procesamiento senal;electronique faible puissance;segmentacion;parallel processing;processor;circuit integre	For super-parallel video processing, we proposed a powerand area-efficient SRAM core architecture with a segmentation-free access, which means accessibility to arbitrary consecutive pixels, and horizontal/vertical access. To achieve these flexible accesses, a spirallyconnected local-wordline select signal and multi-selection scheme in wordlines are proposed, so that extra X-decoders in the conventional multidivision SRAM can be eliminated. Consequently, the proposed SRAM reduces a power and area by 57–60% and 60%, respectively, when it is applied to a 128 parallel architecture. The proposed 160-kbit SRAM with 16-read ports (2-read port SRAM with eight-parallel architecture) is implemented to a search window buffer for an H.264 motion estimation processor core which dissipates 800 μW for QCIF 15-fps in a 130-nm technology. key words: SRAM, low power, parallel processing, image signal processing, H.264, MPEG	accessibility;h.264/mpeg-4 avc;intel core (microarchitecture);motion estimation;moving picture experts group;multi-core processor;parallel computing;pixel;signal processing;static random-access memory;video processing	Junichi Miyakoshi;Yuichiro Murachi;Tomokazu Ishihara;Hiroshi Kawaguchi;Masahiko Yoshimoto	2006	IEICE Transactions	10.1093/ietele/e89-c.11.1629	embedded system;parallel processing;electronic engineering;static random-access memory;telecommunications;image processing;computer science;electrical engineering;operating system;signal processing	Arch	13.854301377334323	40.58930666946616	22866
670f07398f57eed1cfac9afddf72491c61c3d4ce	scalable fine-grained metric-based remeshing algorithm for manycore/numa architectures		In this paper, we present a fine-grained multi-stage metricbased triangular remeshing algorithm on manycore and NUMA architectures. It is motivated by the dynamically evolving data dependencies and workload of such irregular algorithms, often resulting in poor performance and data locality at high number of cores. In this context, we devise a multi-stage algorithm in which a task graph is built for each kernel. Parallelism is then extracted through fine-grained independent set, maximal cardinality matching and graph coloring heuristics. In addition to index ranges precalculation, a dual-step atomic-based synchronization scheme is used for nodal data updates. Despite its intractable latencyboundness, a good overall scalability is achieved on a NUMA dual-socket Intel Haswell and a dual-memory Intel KNL computing nodes (64 cores). The relevance of our synchronization scheme is highlighted through a comparison with the state-of-the-art.	algorithm;automatic vectorization;bridging (networking);bridging model;bulk synchronous parallel;c++;clock rate;computer graphics (computer science);data dependency;distributed memory;explicit parallelism;flops;fork–join model;graph coloring;haswell (microarchitecture);heuristic (computer science);independent set (graph theory);job shop scheduling;locality of reference;mcdram;makespan;manycore processor;matching (graph theory);maximal set;memory hierarchy;multi-core processor;non-uniform memory access;parallel computing;polygon mesh;relevance;scalability;smoothing;work stealing	Hoby Rakotoarivelo;Franck Ledoux;Franck Pommereau;Nicolas Le Goff	2017		10.1007/978-3-319-64203-1_43	scalability;kernel (linear algebra);workload;cardinality;parallel computing;distributed computing;independent set;heuristics;synchronization;algorithm;graph coloring;computer science	HPC	-3.4471555787680535	42.57890313343355	22963
bed56b75a3e4cb247022092d8a287339b1525827	parallel pagerank computation on a gigabit pc cluster	concurrent computing acceleration clustering algorithms computer networks computer architecture personal communication networks ethernet networks parallel algorithms communication standards network synthesis;web graph;pagerank scores;parallel algorithm;concurrent computing;web pages;personal communication networks;network synthesis;information retrieval;gigabit pc cluster;parallel algorithms workstation clusters internet information retrieval;communication model;acceleration;computer networks;computer architecture;internet;gigabit ethernet;web ir community;communication standards;pc cluster;clustering algorithms;workstation clusters;opteron pc;ethernet networks;parallel algorithm parallel pagerank computation gigabit pc cluster web graph web ir community opteron pc gigabit ethernet pagerank scores;parallel pagerank computation;parallel algorithms	Efficient computing the PageRank scores for a large Web graph is actually one of the hot issues in Web-IR community. Recent research projects have been proposed to accelerate the computation, both in algorithmic and architectural ways. We focus on a parallel PageRank computational architecture on a cluster of Opteron PCs networked via a gigabit Ethernet. We propose both an efficient parallel algorithm of the standard PageRank computation, and a simple pairwise communication model needed to synchronize local PageRank scores between processors. Our experimental results conducted on a large Web graph, over 1.5 billion links, synthesized from the real set of crawled Web pages in the TH domain, are quite promising. The current implementation takes less than 15 seconds for an iteration run.	central processing unit;computation;computer cluster;gigabit;iteration;pagerank;parallel algorithm;web page;webgraph	Bundit Manaskasemsak;Arnon Rungsawang	2004	18th International Conference on Advanced Information Networking and Applications, 2004. AINA 2004.	10.1109/AINA.2004.1283923	parallel computing;concurrent computing;computer science;theoretical computer science;operating system;database;distributed computing;parallel algorithm;computer network	HPC	-2.040865221001334	42.49161205243847	23020
79adc2125f4b1a3d9ccfb423f6f7740f0df4de54	reliability evaluation for single event transients on digital circuits	crosstalk;digital ic single event transient digital integrated circuit set signal probability universal generating function technique reliability block diagram logic masking effect gate error attenuation crosstalk effect interconnect wire iscas85 circuit simulation;wires electric;wires electric circuit simulation crosstalk integrated circuit interconnections integrated circuit reliability integrated logic circuits logic gates;circuit simulation;integrated circuit reliability logic gates crosstalk transient analysis integrated circuit modeling digital circuits;logic gates;integrated circuit interconnections;single event transient crosstalk effects masking reliability evaluation;integrated logic circuits;integrated circuit reliability	The effect of single event transient (SET) on reliability has become a significant concern for digital circuits. This paper proposed an algorithm for evaluating the reliability for SET on digital circuits, based on signal probability, universal generating function technique, and generalized reliability block diagrams. The algorithm provides an expression for the reliability of SET under consideration for the effects of logic masking, error attenuation of gates, and crosstalk effects among interconnect wires. We perform simulations of ISCAS85 circuits. The results indicate that the proposed algorithm can effectively evaluate the reliability for SET on circuits. The error attenuation of gates can increase the reliability by more than 41.6%, and the masking and crosstalk effects will improve the reliability by more than 43%.	algorithm;crosstalk;diagram;digital electronics;simulation	Baojun Liu;Li Cai	2012	IEEE Transactions on Reliability	10.1109/TR.2012.2209249	mixed-signal integrated circuit;electronic circuit;electronic engineering;real-time computing;crosstalk;asynchronous circuit;logic gate;computer science;engineering;pass transistor logic;integrated injection logic;circuit extraction;digital electronics;register-transfer level;computer engineering	EDA	24.530162463271658	52.613278170301164	23057
d3921fe57cd246546b5bb3eead5fc8ee69652082	a parametric dfm solution for analog circuits: electrical driven hot spot detection, analysis and correction flow	e dfm;stress effects;design for manufacture;parametric yield;layout engines stress spice voltage controlled oscillators lithography;electrical design for manufacturing;dfm;design for manufacturing;integrated circuit design design for manufacture;integrated circuit design;process variations;electrical design for manufacturability;size 65 nm parametric dfm solution analog circuits electrical driven hot spot detection correction flow semiconductor industry design for manufacturing electrical aware design industrial voltage control oscillator;design for manufacturability;lithography variations;e hotspot process variations design for manufacturability design for manufacturing dfm lithography variations stress effects electrical design for manufacturability electrical design for manufacturing e dfm parametric yield;e hotspot	As VLSI technology pushes into advanced nodes, designers and foundries have exposed a hitherto insignificant set of yield problems. To combat yield failures, the semiconductor industry has deployed new tools and methodologies commonly referred to as design for manufacturing (DFM). Most of the early efforts concentrated on catastrophic failures, or physical DFM problems. Recently, there has been an increased emphasis on parametric yield issues, referred to as electrical-DFM (e-DFM). In this paper, we present a complete electrical-aware design for manufacturing solution that detects, analyzes, and fixes electrical hotspots (e-hotspots) caused by different process variations within the analog circuit design. Novel algorithms are proposed to implement the engines that are used to develop this solution. Our proposed flow is examined on a 65nm industrial voltage control oscillator (VCO). E-hotspot devices with 5.5% variation in DC current are identified. After fixing the e-hotspots, the DC current variation in these devices is reduced to 0.9%, while saving the original VCO specifications.	algorithm;analogue electronics;chemical-mechanical planarization;circuit design;computer-aided design;debian;design for manufacturability;resolution enhancement technologies;resolution enhancement technology;semiconductor industry;test case;very-large-scale integration;voltage-controlled oscillator	Rami F. Salem;Ahmed Arafa;Sherif Hany;Abdelrahman ElMously;Haitham Eissa;Mohamed Dessouky;David Nairn;Mohab H. Anis	2011	2011 IEEE International SOC Conference	10.1109/SOCC.2011.6085082	electronic engineering;engineering;design for manufacturability;engineering drawing;manufacturing engineering	EDA	21.545157570953712	57.0369385773063	23101
9d0a66d10e4610becf243cefabb1f00619663578	a design-for-debug (dfd) for noc-based soc debugging via noc	psmi;debugging;network on chip noc;design for testability;protocols;design for debug method;logic design;network on chip;clocks;network on chip design for testability logic design logic testing;nickel;design for disassembly debugging network on a chip testing monitoring clocks bandwidth logic data engineering design engineering;system on a chip;debug components;noc based soc debugging;system on chip soc;monitoring;system on chip;registers;debug components design for debug method noc based soc debugging network on chip system on chip on chip core debug transaction based debug debug interface unit debug data transfer;logic testing;debug data transfer;on chip core debug;psmi design for debug dfd system on chip soc network on chip noc;debug interface unit;transaction based debug;design for debug dfd	This paper presents design-for-debug (DfD) methods for the reuse of network-on-chip (NoC) as a debug data path in an NoC-based system-on-chip (SoC). We propose on-chip core debug supporting logics which can support transaction-based debug. A debug interface unit is also presented to enable debug data transfer through an NoC between an external debugger and a core-under-debug (CUD). The proposed approach supports debug of designs with multiple clock domains. It also supports collection of trace signatures to facilitate debug of long pattern sequences. Experimental results show that single and multiple stepping through transactions are feasible with moderately low area overhead. We also present simulation result to verify proper operation of the debug components.	debug;debugger;network on a chip;overhead (computing);simulation;stepping level;system on a chip;the times;type signature	Hyunbean Yi;Sungju Park;Sandip Kundu	2008	2008 17th Asian Test Symposium	10.1109/ATS.2008.15	x86 debug register;system on a chip;embedded system;computer architecture;parallel computing;computer science;network on a chip;background debug mode interface	EDA	10.871670720403396	53.55989901006172	23103
21347a8ddaf43211318357d88b105546637a2f3a	distributed rlc interconnect: analytical modelling expressions for crosstalk noise estimation	crosstalk noise estimation;crosstalk noise;rlc circuits crosstalk integrated circuit interconnections integrated circuit modelling integrated circuit noise;crosstalk;ic interconnections distributed rlc interconnect crosstalk noise estimation inductive coupling effects noise peak voltage worst case crosstalk noise effect;distributed rlc interconnect;chip;inductive coupling effects;worst case crosstalk noise effect;integrated circuit modelling;noise peak voltage;rlc circuits;analytical models crosstalk integrated circuit interconnections delay estimation wire transfer functions circuit optimization delay effects very large scale integration function approximation;integrated circuit interconnections;analytical modelling;ic interconnections;integrated circuit noise	As the chip dimensions increase with chip complexity, interconnects tend to get longer. With the longer on-chip interconnects coupled with a decrease in wire width and wire separation, inductive coupling effects have become non-negligible. Analytical modelling expressions for the estimation of the noise peak voltage of the victim line under worst-case crosstalk noise effect are presented.	best, worst and average case;crosstalk;electrical connection;inductive coupling;mathematical optimization;rlc circuit;spice;simulation	H. J. Kadim;Lacina M. Coulibaly	2006	2006 13th IEEE International Conference on Electronics, Circuits and Systems	10.1109/ICECS.2006.379666	embedded system;electronic engineering;electrical engineering	EDA	22.027350292010954	56.63794549660616	23154
3cc8a4ed336fc90a5e2bfe3ee0765f3a1ee140e5	fast memory region: 3d dram memory concept evaluated for jpeg2000 algorithm	random access memory;random access memory adaptation models lead plasmas;plasmas;lead;adaptation models;encoder procedure execution fast memory region 3d dram memory jpeg2000 algorithm 3d stacking technology connection impedance wide interface flexible interface dram layers locality principle;three dimensional integrated circuits data compression dram chips integrated circuit interconnections	3D stacking technology provides defined connection impedance and wide and flexible interface. These options enable new architecture approaches and memory concepts for DRAM layers. The most important findings on memory usage is locality principle. This principle is formulated as thumb rule therefore exact rate requires measure for given application using trace driven simulations. This big overhead is useless if application is unknown. Averaging and estimation offer solutions here. This work presents Fast Memory Region - a memory concept that provides an universal 3D DRAM hardware architecture that exploits locality principle for any application. The configuration step, in contrast to cache, is located in software. We evaluate the concept on JPEG2000 algorithm and show that, for encoder procedure execution, all necessary measure data can be extracted from short trace driven simulation runs. Simulations results show 5% reduction of run time overhead compared to cache acceleration only.	algorithm;cpu cache;characteristic impedance;computer simulation;dynamic random-access memory;encoder;jpeg 2000;locality of reference;overhead (computing);principle of locality;run time (program lifecycle phase);stacking;three-dimensional integrated circuit;universal 3d	Alex Schönberger;Klaus Hofmann	2014	2014 International Symposium on System-on-Chip (SoC)	10.1109/ISSOC.2014.6972443	uniform memory access;embedded system;interleaved memory;semiconductor memory;parallel computing;real-time computing;dynamic random-access memory;memory rank;sense amplifier;static random-access memory;cas latency;memory refresh;computer science;computer memory;memory controller;universal memory;extended memory;flat memory model;registered memory;cache-only memory architecture;memory map;non-uniform memory access	Arch	-1.28213888915175	49.69311588569752	23166
0155ebf5909ab8141e1fec69ed5b1d2a17b6e5ff	a new testing acceleration chip for low-cost memory tests	equipment cost;random access memory;semiconductor memory;turning;semiconductor memories;maintenance;dynamic random access memories;memory testing;optical filters;integrated memory circuits;life estimation random access memory costs circuit testing optical filters semiconductor memory turning maintenance graphics lithography;chip;test cost;low cost memory tests;lithography;ac testing reliability testing acceleration chip low cost memory tests semiconductor memories dynamic random access memories process complexity die size equipment cost test cost memory test concept;testing acceleration chip;integrated memory circuits dram chips integrated circuit testing;ac testing reliability;test methods;integrated circuit testing;turning point;life estimation;dynamic random access memory;circuit testing;memory test concept;die size;dram chips;graphics;process complexity	It is argued that the development of semiconductor memories has reached a turning point. In the multimegabit dynamic random access memories (DRAMs) of the future, major factors contributing to the chip cost are process complexity, die size, equipment cost, and test cost. If conventional test methods are used, test costs will grow at an especially rapid rate. A memory test concept called the testing acceleration chip, which could reduce future test costs a hundredfold and yet maintain AC testing reliability, is presented.<<ETX>>	dynamic random-access memory;memory tester;random access;semiconductor memory	Michihiro Inoue;Toshio Yamada;Atsushi Fujiwara	1993	IEEE Design & Test of Computers	10.1109/54.199800	lithography;chip;electronic engineering;semiconductor memory;parallel computing;dynamic random-access memory;computer hardware;telecommunications;computer science;graphics;optical filter;test method	EDA	13.045114907234572	58.70999792052798	23207
bcc968c7097cbb99ca9ece005088617395e788b7	retargetable code generation for digital signal processors	code generation;digital signal processor	ing from the description language, all previous approaches to (retargetable) embedded code generation are based on exactly one of those three styles: Behavioral models are used by Rimey /Hilfinger [RiHi88] and in CBC Instruction-set extraction behavioral mixed-level instruction set (register transfer patterns + alternative encodlngs) structural Figure 3.1 Functionality of instruction-set extraction 47 [FaKn93], structural models are the basis of MSSQ [Nowa87a], and mixed-level models are preferred by Wess [Wess92], Philips [SMT +95], and in CodeSyn [LMP94a] and CHESS [FVM95]. However, no attempt has been made so far to leave selection of a certain style to the user. This leads to the concept of instruction-set extraction (ISE), as depicted in fig. 3.1. ISE is a frontend, which accepts MIMOLA processor models in any of the above styles, and which yields the set of instructions available on the processor in form of register transfer patterns, as well as the corresponding partial instructions (or binary en co dings). The main advantages of ISE are the following: Versatility: The choice of a certain processor description style is not prescribed by the compiler, but is completely left to the user. Dependent on the· intended application and the available documentation, either more abstract or more hardware-oriented processor models can be used. Uniformity: Most previous approaches use very tool-specific processor description formats. ISE is based on a well-tried and well-documented hardware description language which provides a close link between the compiler and ECAD tools. The output of ISE is always an instruction set, independent of the granularity of the input model. Thus, code generation algorithms can operate on a uniform representation of the processor.	algorithm;american and british english spelling differences;central processing unit;circuit complexity;code generation (compiler);compiler;computer chess;digital signal processor;documentation;electronic design automation;embedded system;hardware description language;xfig	Rainer Leupers	1997		10.1007/978-1-4757-2570-4	computer architecture;parallel computing;computer science;programming language;code generation	EDA	2.4874177303904643	51.25770949043019	23228
208c9d823d9bf11bccc47ec8b0d4de185979d0b1	mapping strategies in data parallel programming models; the projection methods	data parallelism;projection method;data mapping	We Analyze data parallel programming of some general purpose methods in linear algebra. Speciically, the projection methods to solve very large linear system and/or eigenproblem. The expensive parts of these methods are their projection phases. This portion of these algorithms has generally, a very simple structure for such a programming model. It is composed essentially of matrix-vector multiplications and inner-products. Then, we simply need to nd a good data distribution in order to obtain a well adapted communication pattern and not loose too much storage space. We begin with a survey of data parallel behavior of some projec-some methods of data mapping onto virtual processors we point out that for a xed number of physical processors, the performances are a function of the mapping method. We will see also that the maximum size of the problems which can be solved on the architectures supporting the data parallel programming model is a function of the data mapping method. In conclusion we present the performances obtained on a Connection Machine 5 (CM5).	algorithm;central processing unit;connection machine;data parallelism;linear algebra;linear system;parallel computing;parallel programming model;performance	N. Emad	1999	Scalable Computing: Practice and Experience	10.12694/scpe.v2i4.153	mathematical optimization;computer science;theoretical computer science;algorithm	HPC	-2.262270726882844	38.214858282447224	23237
7a3c759f119944206dcca9cd1f6e5ca792747863	a new approach for circle detection on multiprocessors	cercle;parallelisme;distributed system;systeme reparti;metodologia;multiprocessor;deteccion;implementation;image;detection;line detection;methodologie;algorithme;algorithm;ejecucion;parallelism;sistema repartido;paralelismo;imagen;informatique theorique;circulo;arquitectura;methodology;multiprocesador;circle;architecture;computer theory;algoritmo;multiprocesseur;informatica teorica	This paper considers the problem of detecting circles in images on multiprocessors. We have deened a new transformation that converts circles in an image to a families of straight lines allowing the problem to be converted to line detection which can be solved by any line detection algorithms. Also, we have developed two algorithms for circle detection based on this new transformation. A simulation program for these algorithms implemented on a pyramid architecture is presented.	algorithm;edge detection;sensor;simulation	Alireza Kavianpour;S. Shoari;Nader Bagherzadeh	1994	J. Parallel Distrib. Comput.	10.1006/jpdc.1994.1025	computer science;artificial intelligence;architecture;image;methodology;distributed computing;implementation;algorithm	Vision	11.23519796505211	35.3636918789003	23296
912674081a94261c96eb0208b76e2ca0db0281eb	on reducing test power and test volume by selective pattern compression schemes	test data volume;minimization;design for testability;switching;scan chain;decoding;dissipation energie;test data compression;large test data volume;selective test pattern compression;selective test pattern compression test volume selective pattern compression chip design test strategies large test power dissipation large test data volume chip failure;compresion senal;test volume;integrated circuit testing integrated circuit design;energy dissipation;test data volume compression dft low power scan chain;compression signal;chip design;encoding circuit testing chip scale packaging costs test data compression decoding power dissipation design for testability large scale systems minimization;essai circuit integre;conception circuit integre;chip;integrated circuit design;test strategies;low power;chip failure;large test power dissipation;power dissipation;signal compression;conmutacion;low power electronics;defaillance;integrated circuit testing;disipacion energia;circuit testing;failures;compression;encoding;electronique faible puissance;article;fallo;dft;selective pattern compression;commutation;chip scale packaging;large scale systems	In modern chip designs, test strategies are becoming one of the most important issues due to the increase of the test cost, among them we focus on the large test power dissipation and large test data volume. In this paper, we develop a methodology to suppress the test power to avoid chip failures caused by large test power, and our methodology is also effective in reducing the test data volume and shift-in power. The proposed schemes and techniques are based on the selective test pattern compression, they can reduce considerable shift-in power by skipping the switching signal passing through long scan chains. The experimental results with ISCAS89 circuits demonstrate that our methodology can achieve significant improvement in the reduction of shift-in power and test data volume. Our approach also supports multiple scan chains.	apple multiple scan 14 display;overhead (computing);test card;test data;turing test	Chia-Yi Lin;Hsiu-Chuan Lin;Hung-Ming Chen	2010	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2009.2021061	embedded system;electronic engineering;computer science;engineering;electrical engineering;dissipation;automatic test pattern generation;test compression;integrated circuit design	EDA	19.87366317174175	53.469766933004124	23350
2b18df81605831b04068da760c7558e633ab945f	logic cells and interconnect strategies for nanoscale reconfigurable computing fabrics	c nanoscale reconfigurable computing fabric back gate terminal double gate ambipolar transistor fine grain logic reconfigurability cell logic technique logic gate tunable functionality reconfigurable interconnect strategy mapping method matrix occupation cluster size switch requirement interconnect architecture;double gate;reconfigurable architectures carbon nanotubes field effect transistors logic circuits logic gates;measurement;logic design;reconfigurable computing;carbon nanotubes;reconfigurable architectures;cntfets;logic circuits;cntfets measurement;nanotechnology;reconfigurable architecture;logic gates;reconfigurable architectures nanotechnology logic design;field effect transistors;logic gate	The back-gate terminal on double-gate ambipolar transistors can be used as a powerful vector to achieve fine-grain logic reconfigurability. This paper describes ways of exploiting this property to improve on standard cell logic techniques, and to build logic gates with tunable functionalities. Given the vastly reduced transistor count, conventional use of reconfigurable interconnect at the cell level would lead to large overhead, and new interconnect strategies are required. We explore two types of interconnect architectures (island-style and cell-matrix) and develop a mapping method to evaluate trade-offs between matrix occupation, cluster size and switch requirements.	logic gate;overhead (computing);reconfigurability;reconfigurable computing;requirement;standard cell;transistor count	Ian O'Connor;Kotb Jabeur;David Navarro;Nataliya Yakymets;Pierre-Emmanuel Gaillardon;M. Haykel Ben Jamaa;Fabien Clermidy	2010	2010 17th IEEE International Conference on Electronics, Circuits and Systems	10.1109/ICECS.2010.5724455	embedded system;electronic engineering;logic family;engineering;pass transistor logic;computer engineering	EDA	13.2153605409312	57.024465963032554	23364
0e361838b9554eb04a20e146a088eba5c020b436	a parallel method for computing the generalized singular value decomposition	computers;digital computers;time dependent;general and miscellaneous mathematics computing and information science;parallel algorithm;mathematical logic;generalized singular value decomposition;mathematics mathematical models 1987 1989;matrices;time dependence;array processors;algorithms;logic programs;programming 990210 supercomputers 1987 1989;supercomputers;parallel processing	The authors describe a new parallel algorithm for computing the generalized singular value decomposition of two n X n matrices, one of which is nonsingular. The procedure requires O(n) time and one triangular array of O(n/sup 2/) processors.	singular value decomposition	Franklin T. Luk	1985	J. Parallel Distrib. Comput.	10.1016/0743-7315(85)90027-9	computational science;parallel processing;mathematical logic;parallel computing;computer science;theoretical computer science;distributed computing;parallel algorithm;algorithm;matrix	HPC	-3.1572233946662656	37.45877884181919	23471
560b269d021367e6226b19bfcc71cdebd8c4e7f8	pgraph: efficient parallel construction of large-scale protein sequence homology graphs	dna;dynamic programming;hierarchical master worker paradigm;biology computing;environmental molecular sciences laboratory;protein sequence;parallel algorithm pgraph parallel construction large scale protein sequence homology graphs computational molecular biology pervasive application protein molecules;protein sequence computational modeling amino acids dna image edge detection dynamic programming;graphs;computational modeling;proteins;image edge detection;parallel protein sequence homology detection;amino acids;ubiquitous computing;parallel sequence graph construction;producer consumer model;ubiquitous computing biology computing graphs parallel algorithms proteins;producer consumer model parallel protein sequence homology detection parallel sequence graph construction hierarchical master worker paradigm;parallel algorithms	Detecting sequence homology between protein sequences is a fundamental problem in computational molecular biology, with a pervasive application in nearly all analyses that aim to structurally and functionally characterize protein molecules. While detecting the homology between two protein sequences is relatively inexpensive, detecting pairwise homology for a large number of protein sequences can become computationally prohibitive for modern inputs, often requiring millions of CPU hours. Yet, there is currently no robust support to parallelize this kernel. In this paper, we identify the key characteristics that make this problem particularly hard to parallelize, and then propose a new parallel algorithm that is suited for detecting homology on large data sets using distributed memory parallel computers. Our method, called pGraph, is a novel hybrid between the hierarchical multiple-master/worker model and producer-consumer model, and is designed to break the irregularities imposed by alignment computation and work generation. Experimental results show that pGraph achieves linear scaling on a 2,048 processor distributed memory cluster for a wide range of inputs ranging from as small as 20,000 sequences to 2,560,000 sequences. In addition to demonstrating strong scaling, we present an extensive report on the performance of the various system components and related parametric studies.	central processing unit;computation;computer;distributed memory;homology modeling;image scaling;parallel algorithm;parallel computing;peptide sequence;pervasive informatics;producer–consumer problem;scalability;sensor;sequence homology	Changjun Wu;Anantharaman Kalyanaraman;William R. Cannon	2012	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2012.19	parallel computing;computer science;bioinformatics;theoretical computer science;dynamic programming;protein sequencing;distributed computing;parallel algorithm;graph;computational model;ubiquitous computing;dna;algorithm	Comp.	0.6775414798049979	43.16203732156033	23514
f9050a0a28f0bba8d537988133154f513d696ede	path-delay-fault testable nonscan sequential circuits	concepcion asistida;design for testability;computer aided design;fsm path delay fault testable circuits nonscan sequential circuits finite state machine synthesis methods one hot encoding;caracteristique temporelle;integrated circuit;etude theorique;logic design;circuito secuencial;essai automatique;sequential circuits;circuit testing sequential analysis sequential circuits circuit faults delay robustness circuit synthesis flip flops automata encoding;circuit sequentiel;circuito integrado;prueba automatica;time curve;finite state machines;encoding logic testing sequential circuits finite state machines delays logic design design for testability;logic testing;estudio teorico;caracteristica temporal;conception assistee;path delay fault;temps retard;delay time;theoretical study;combinational circuit;machine etat fini;encoding;tiempo retardo;automatic test;finite state machine;circuit integre;delays;sequential circuit	In this paper we show that any finite state machine can he implemented by a fully path-delayfault testable nonscan sequential circuit. Synthesis methods are proposed, which use a one-hot encoding of states, a special circuit structure and at most one additional input. Combined with existing synthesis techniques for delay-fault testable combinational circuits, these methods can produce nonscan sequential circuits in which every path has a robust or validatable nonrobust test	combinational logic;finite-state machine;one-hot;sequential logic	Wuudiann Ke;Premachandran R. Menon	1995	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.384419	electronic engineering;real-time computing;computer science;mathematics;sequential logic;finite-state machine;algorithm	EDA	20.378568867249438	48.6995934433222	23516
34031a8a465ea8232ba8b9789c3ec5f49c1b63c4	impact of die thinning on the thermal performance of a central tsv bus in a 3d stacked circuit	thermal analysis;3dic;tsv bus;wide i o;die thinning	In three-dimensional integrated circuits (3DICs), aggressive wafer-thinning can lead to large thermal gradients. It is crucial to understand the interaction between process parameters, such as wafer thickness, and the temperature profile in order to design high-performance 3DICs. In this paper we examine how the temperature profile of through-silicon via (TSV) bus driver/receiver cells are impacted by die thinning. While die thinning limits the ability for heat to diffuse into the wafer, it can also decrease the capacitance of the TSV which in turn decreases the driver's power, leading to an overall lower working temperature in some circumstances. In this work we have investigated the thermal effects of stacking 2-8 thinned ICs with TSVs, over a range of die thicknesses from 100 µ m to 25 µ m . Decreasing the die thickness from 100 µ m to 50 µ m provided the best balance with a 20% reduction in bus power at a cost of less than a 2% increase in driver temperature for all cases between 2 and 8 tiers. HighlightsDie thinning generally leads to higher system temperatures.TSV bus capacitances decrease when dies are thinned.Decreased TSV bus capacitances lead to lower power dissipation.Die thinning can be used to simultaneously decrease both power and temperature.	thinning;through-silicon via	Samson Melamed;Fumito Imura;Hiroshi Nakagawa;Katsuya Kikuchi;Michiya Hagimoto;Yukoh Matsumoto;Masahiro Aoyagi	2015	Microelectronics Journal	10.1016/j.mejo.2015.09.007	embedded system;electronic engineering;engineering;engineering drawing;thermal analysis	EDA	13.757236524604021	58.19345182588878	23524
892736a693f21e6d70d9fd51b8bcfd7598ccaf3e	sequential circuit fault simulation by fault information tracing algorithm: fit	analytical models;convergence;circuit faults;fault simulation;clocks;sequential circuits;circuit simulation;fault analysis;permission;circuit faults sequential circuits circuit simulation analytical models electrical fault detection clocks permission combinational circuits convergence cause effect analysis;analytical method;fault detection;electrical fault detection;cause effect analysis;combinational circuits	‘Ilk paper presenta a sequential circuit fault simulation method using a Fault Information Tracing algorithm (FIT). FIT handles information which represents how faults on each line propagate to fault convergence gates. By tracing the information backward from primary output, the fault detectability is directly determined. This technique drastically reduces fault effect propagation, achieves precise results, and is applicable to single and multiple stuck-at faults. One of the problems in sequential circuit fault simulation with analytical methods is how to deal with the multiple fault effect which is caused by faults stored in storage elements. Sequential circuit fault simulation by FIT is accomplished by executing multiple fault analysis. The algorithm was implemented in C, and results for ISCAS’89 benchmark circuits are presented.	algorithm;benchmark (computing);sequential logic;simulation;software propagation	Yoshihiro Kitamura	1991		10.1145/127601.127649	embedded system;electronic engineering;fault;real-time computing;convergence;fault coverage;fault indicator;computer science;stuck-at fault;automatic test pattern generation;fault model;sequential logic;combinational logic;fault detection and isolation;algorithm;computer network	EDA	22.731187309354837	51.10067732910932	23553
16aeb97d9298d708b7a962b202c00ffd70be0141	minimum padding to satisfy short path constraints	short path constraint;minimum padding;satisfiability;linear program;longest path;combinational circuits;lower bound;cycle time;upper bound;combinational circuit;greedy heuristic	Combinational circuits are ojlen embedded in synchronous designs with rnernory elements at the input and output ports. A performance metric for a circuit is the cycle time of the clock signal. Correct circuit operatwn requires that all paths have a delay that lies between an upper bound and a lower bound. Traditional approaches in delay optimizatwn for combinational circuits [9, 3, 6] have dealt with methods to decrease the delay of the longest path. We address the issue of satisfying the lower bound constraints. Such aproblemalso arises in wavepipelining of circuits. We propose to handle shortpath constrains as a postprocessing step after traditional delay optimization techniques. There are two issues presented in this paper. We$rst discuss necessary and su.cient conditwns for successful delay insertion without increasing delays of any longpaths. In the secondpart, we present a naive approach to padding delays (greedy heuristic) and an algorithm based on linear progratruning. We describe an applicatwn of the theory to wave pipelining of circuits. Results are presented on a set of benchmark circuits, using two delay models.	benchmark (computing);clock signal;combinational logic;embedded system;greedy algorithm;heuristic;input/output;longest path problem;mathematical optimization;padding argument;pipeline (computing)	Narendra V. Shenoy;Robert K. Brayton;Alberto L. Sangiovanni-Vincentelli	1993		10.1145/259794.259821	mathematical optimization;real-time computing;linear programming;mathematics;combinational logic;upper and lower bounds;algorithm	EDA	17.108807930141708	51.61528955853493	23554
04fe539ba150c3adf6ca9b3a3a9353fcfc72b145	peak power minimization through datapath scheduling	resource constraint;difference operator;scheduling minimisation of switching nets linear programming high level synthesis;minimisation of switching nets;3 3 v peak power minimization integer linear programming models multiple supply voltages dynamic frequency clocking multicycling functional units resource constraints power delay product high level synthesis benchmark circuits datapath scheduling 5 0 v;voltage scheduling algorithm delay estimation circuits minimization integer linear programming frequency clocks energy consumption delay effects;high level synthesis;scheduling algorithm;scheduling;linear programming;integer linear program ming;power delay product;functional unit;peak power	In this paper, we describe new integer linear programming models and algorithms for datapath scheduling that aim at minimizing peak power while maintaining performance. The first algorithm, MVDFC combines both multiple supply voltages and dynamic frequency clocking for peak power reduction, while the second algorithm, MVMC explores multiple supply voltages and multicycling. The algorithms use the number and type of different functional units at different operating voltages as the resource constraints. The effectiveness of the proposed scheduling algorithms is studied by estimating the peak power consumption and the power delay product (PDP) of the datapath circuit being synthesised. The algorithms have been applied to various high level synthesis benchmark circuits under different resource constraints. Experimental results show that for the MVDFC, under various resource constraints using two supply voltage levels , average peak power reduction around and average PDP reduction of can be obtained. For the MVMC scheme, average peak power reduction is around and average PDP reduction is , for similar resource constraints.	algorithm;benchmark (computing);clock rate;datapath;high-level programming language;high-level synthesis;integer programming;linear programming;scheduling (computing)	Saraju P. Mohanty;N. Ranganathan;Sunil K. Chappidi	2003		10.1109/ISVLSI.2003.1183362	mathematical optimization;electronic engineering;real-time computing;mathematics	EDA	16.430706028123137	54.311907941634914	23572
1320d35182c88de4c1006930e5df273b5abd570e	a low-power routing architecture optimized for deep sub-micron fpgas	nanotechnology;90 nm low power routing architecture deep submicron fpga signal propagation delay leakage power dissipation embedded fpga stm technology minimized subthreshold current standby leakage reduction active leakage reduction 65 nm;low power;leakage power;leakage currents;propagation delay;integrated circuit interconnections;low power electronics;routing field programmable gate arrays switches leakage current delay power dissipation gate leakage hardware circuits silicon;nanotechnology field programmable gate arrays integrated circuit interconnections leakage currents low power electronics;deep sub micron;field programmable gate arrays	Signal propagation delay and leakage power dissipation of FPGAs mainly depend on the routing architecture. In this paper we propose solutions; adopted in an embedded FPGA developed both in 90nm and 65nm STM technology, which minimize the subthreshold current of the switch and connection blocks. Our approach leads to a reduction of more than one order of magnitude of standby leakage, up to 62% of active leakage of the routing architecture, having a small impact on signal delay, without silicon area increase	biasing;embedded system;field-programmable gate array;propagation delay;routing;software propagation;software transactional memory;spectral leakage;vii	Luca Ciccarelli;D. Loparco;Massimiliano Innocenti;Andrea Lodi;Claudio Mucci;Pier Luigi Rolandi	2006	IEEE Custom Integrated Circuits Conference 2006	10.1109/CICC.2006.320889	embedded system;propagation delay;electronic engineering;computer science;engineering;electrical engineering;leakage;field-programmable gate array;low-power electronics	EDA	14.664946588840746	57.08959186689285	23745
790013a37f01d5cb6475a9c0170feb9f67c228e4	towards protecting biometric templates without sacrificing performance	pins;authentication;cryptography;feature extraction;high definition video;face;lighting	The ideal biometric template protection scheme possesses the properties of irreversibility, revocability, unlinkability, and good performance. These properties protect the security of the biometrics system as well as users' privacy. Practical systems, however, fall short of this ideal. In this paper, we present a novel protection scheme that achieves this ideal under the circumstance that a subject's token and his biometric template are not concurrently exposed. Moreover, our scheme can add template protection to any face verifier. We do this by rendering virtual faces, rather than by devising new biometric features, which is the more common approach. Experimental evaluations using two public face recognition systems show that accuracy is not adversely affected with our scheme.	biometrics;facial recognition system	Jing Li;Yongkang Wong;Terence Sim	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7899771	face;feature extraction;computer science;cryptography;machine learning;lighting;authentication;geometry;internet privacy;computer security	Mobile	7.346022482920561	33.6841464031807	23754
8fd0f34c1d4f087031ef02d980e5c099db517268	automatic visual testing: a new, comprehensive element of cost-effective pcb testing strategies	cost effectiveness	Automatic Visual Testing (AVT) is a new class of testing for printed circuit boards. This technology detects greater than 99% of visible assembly errors, with low false reject rates. AVT decreases costs by reducing labor, detecting defects early, and increasing the capacity of other test equipment. Sample calculations show that in a typical plant, replacing inspectors with AVT reduces costs by 1/2 to 1/3. Furthermore, AVT makes it economically and strategically feasible to eliminate in-circuit testing. In fact, using AVT in combination with incoming part inspection and eliminating in-circuit testing cuts the total cost of testing, repair, escapes, programming and fixturing by 1/2 to 1/3.		Stephen P. Denker;Judy Cobb	1984			cost-effectiveness analysis;computer science;engineering;forensic engineering;engineering drawing	SE	24.125233662142538	53.77102079378031	23772
33278e2f30bb519db3ca49a1c22879cd7c1a5f3b	testing redundant asynchronous circuits by variable phase splitting	satisfiability;asynchronous circuit	An approach for stuck-at-i and delay-fault testing of redundant circuits without modifying the logic is proposed. The only requirement is the ability to control both phases of each variable independent of each other. The circuit becomes fully testable under very weak assumptions, equivalent to freedom from Single Cube Containment in two-level form. The main existing methods for asynchronous circuit synthesis are demonstrated to satisfy the assumptions and then are testable using the methodology. Heuristics to improve the approach include partial scan and non-scan testing.	asynchronous circuit;cube;heuristic (computer science)	Luciano Lavagno;Antonio Lioy;Michael Kishinevsky	1994			discrete mathematics;asynchronous circuit;theoretical computer science;mathematics;algorithm	EDA	19.71744900285304	48.296573719471134	23787
c0a60768f21e489011f761f6843234cad74ff24f	a hardware implementation of a run-time scheduler for reconfigurable systems	field programmable gate array;evaluation performance;optimisation;haute performance;reconfigurable system;performance evaluation;optimizacion;systeme embarque;execution time;gestion labor;clocks;optimization technique;flexibilidad;processor scheduling;reconfigurable architectures;reutilizacion;implementation;storage management;evaluacion prestacion;prefetching;resource manager;resource management;red puerta programable;multiprogramming;data mining;runtime;reseau porte programmable;storage management embedded systems multiprogramming processor scheduling reconfigurable architectures;reuse;embedded system;embedded systems;reconfigurable architecture;gestion tâche;optimal scheduling;scheduling;alto rendimiento;horloge;temps execution;flexibilite;optimization;temps retard;delay time;task graphs;task scheduling;task scheduling field programmable gate arrays fpgas reconfigurable architectures;tiempo ejecucion;implementacion;hardware runtime processor scheduling delay embedded system resource management prefetching information analysis data mining clocks;tiempo retardo;high performance;information analysis;clock;architecture reconfigurable;hardware implementation;reconfigurable hardware;ordonnancement;reloj;flexibility;reglamento;reutilisation;field programmable gate arrays fpgas;hardware;clock cycles hardware implementation run time scheduler reconfigurable systems new generation embedded systems task graphs prefetch techniques replacement techniques reconfiguration delays near optimal schedules local optimum decisions;replacement policy	New generation embedded systems demand high performance, efficiency, and flexibility. Reconfigurable hardware can provide all these features. However, the costly reconfiguration process and the lack of management support have prevented a broader use of these resources. To solve these issues we have developed a scheduler that deals with task-graphs at run-time, steering its execution in the reconfigurable resources while carrying out both prefetch and replacement techniques that cooperate to hide most of the reconfiguration delays. In our scheduling environment, task-graphs are analyzed at design-time to extract useful information. This information is used at run-time to obtain near-optimal schedules, escaping from local-optimum decisions, while only carrying out simple computations. Moreover, we have developed a hardware implementation of the scheduler that applies all the optimization techniques while introducing a delay of only a few clock cycles. In the experiments our scheduler clearly outperforms conventional run-time schedulers based on as-soon-as-possible techniques. In addition, our replacement policy, specially designed for reconfigurable systems, achieves almost optimal results both regarding reuse and performance.	clock signal;computation;computer multitasking;embedded system;experiment;field-programmable gate array;local optimum;mathematical optimization;prefetch input queue;reconfigurable computing;run time (program lifecycle phase);scheduling (computing);xfig	Juan Antonio Clemente;Javier Resano;Carlos González;Daniel Mozos	2011	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2010.2050158	clock;embedded system;parallel computing;real-time computing;computer multitasking;reconfigurable computing;computer science;resource management;operating system;reuse;data analysis;implementation;scheduling;field-programmable gate array	EDA	-0.41988084630485767	54.06707834398192	23814
bb66c540f59f2d67a46b8064b21ebd15793a1df5	high-speed signal processing using systolic arrays over finite rings	vlsi cellular arrays cmos integrated circuits computerised signal processing microprocessor chips;red sistolica;traitement signal;digital signal processing;microprocessor tajada bit;cmos integrated circuits;architecture systeme;filtre reponse impulsion finie;finite impulse response filter;circuit vlsi;systolic array;bit slice microprocessor;cellular arrays;filtro respuesta impulsion acabada;vlsi circuit;systolic network;critical path;signal processing;analyse performance;performance analysis;reseau systolique;vlsi;arquitectura sistema;residue number system;vlsi layout;circuito vlsi;array signal processing systolic arrays very large scale integration digital signal processing galois fields finite impulse response filter concurrent computing dynamic range computer architecture pipelines;system architecture;procesamiento senal;microprocesseur en tranche;modular architecture;high speed;3 micron ring operators systolic arrays finite rings digital signal processing dsp integer ring residue number systems closed operations field binary operators scaling operations finite mathematical systems pipeline cycle generic cell critical path analysis vlsi medium array structures vlsi layouts cmos;computerised signal processing;microprocessor chips;analisis eficacia	This paper presents a simple, modular, architecture for very fast digital signal processing elements. The computation is performed over finite rings (or fields) and is able to emulate processing over the integer ring using residue number systems. The computations are restricted to closed operations (ring or field binary operators) with the ability to perform limited scaling operations. Computations naturally defined over finite mathematical systems (e.g. Number Theoretic Transforms, Quadratic Residue 'complex' calculations, Recursive FIR filters over finite fields) are also easily implemented using this new approach. The technique evolves from the decomposition of each closed calculation using the ring/field associativity property. Linear systolic arrays, formed with multiple elements, each of a single generic form, are used for all calculations. The pipeline cycle is determined from the generic cell and is predicted to be very fast based on a critical path analysis. The cells are perfectly matched to the VLSI medium, and the resulting array structures are very dense indeed. Examples of DSP applications are given to illustrate the technique, and sample cell and array VLSI layouts are presented for a 3μ CMOS process.	cmos;computation;critical path method;digital signal processing;finite impulse response;image scaling;path analysis (statistics);quadratic residue;recursion (computer science);systolic array;very-large-scale integration	Maryam Taheri;Graham A. Jullien;William C. Miller	1988	IEEE Journal on Selected Areas in Communications	10.1109/49.1918	embedded system;residue number system;parallel computing;systolic array;computer science;electrical engineering;critical path method;digital signal processing;finite impulse response;signal processing;very-large-scale integration;cmos	Arch	14.015956588104142	43.28761173598026	23921
5942c56774cc59dc30c721d0890d6d16837e870d	wireworld++: a cellular automaton for simulation of nonplanar digital electronic circuits		An enhanced version of the Wireworld cellular automaton called Wireworld++ is introduced. It can be considered as a generalization of Wireworld suitable for modeling digital electronic circuits that have intersections of unconnected wires. As most electronic circuits except trivial ones have wire crossings, Wireworld++ is a more convenient cellular automaton for modeling digital electronics than the conventional Wireworld. Wireworld++ is two dimensional; it has a small number of states and simple and intuitive rules. Despite that, it allows simulation of three-dimensional elements of digital circuits, for instance, wire crossings or electronic components placed on both sides of printed circuit boards. The key electronic parts, such as logic gates, implemented in Wireworld++ exhibit more symmetry and utilize fewer cells than their Wireworld counterparts. Wireworld++ can also be applied to simulation of computing devices in a sub-excitable, light-sensitive Belousov– Zhabotinsky medium organized in a rectangular grid of vesicles.	cellular automaton;digital electronics;electronic circuit;electronic component;excitable medium;logic gate;printed circuit board;printing;regular grid;simulation;wireworld	Vladislav Gladkikh;Alexandr Nigay	2018	Complex Systems		mathematics;discrete mathematics;cellular automaton;electronic circuit;wireworld	EDA	12.502788847543131	50.32087164507115	23923
a17f1a2e9376dd654dac3c99e72e0abb25c3fad8	automatic post-layout flow validation tool for deep sub-micron process design kits	size 32 nm;semiconductor device testing;automatic post layout flow validation tool;tool validation deep submicron dsm benchmark circuit delay differences deltas process design kits pdk parasitic extraction pex;size 28 nm;parasitic extraction pex;pdk;integrated circuit;deep submicron dsm;process design kits pdk;ring oscillator;resistance;logic circuits;lvs;layout;automatic generation;deep submicron process design kit;process design;delay differences deltas;integrated circuit design;circuit simulation;accuracy;tool validation;integrated circuit modeling;size 22 nm automatic post layout flow validation tool deep submicron process design kit mosfet pdk benchmark circuit ring oscillator logic circuits passive delay circuit lvs layout versus schematic model lvs parasitic extraction pex dsm size 32 nm size 28 nm;dsm;multiple model;integrated circuit modeling delay capacitance resistance layout accuracy benchmark testing;pex;semiconductor device testing circuit cad circuit simulation integrated circuit design mosfet process design;mosfet;capacitance;circuit cad;deep sub micron;development time;benchmark circuit;passive delay circuit;layout versus schematic;benchmark testing;size 22 nm;model lvs parasitic extraction	This paper presents a novel automated post-layout flow validation tool to intensively test the MOSFETs and passive components in 32nm, 28nm and 22nm Process Design Kits (PDK). Benchmark circuits, such as, ring oscillator, logic circuits and passive delay circuits, are automatically generated, LVS (layout versus schematic) checked, extracted and simulated in multiple Model/LVS/Parasitic extraction(PEX) test flows. By using the proposed tool, the delay differences (deltas) between the different test flows are cross verified to assure the functionality and accuracy of Model, LVS and PEX before PDK release. Combined with field solver validation, the automated post-layout flow validation significantly improves the quality and reduces the development time of Deep Submicron (DSM) PDKs.	benchmark (computing);cmos;circuit design;common platform;electromagnetic field solver;judy array;linux virtual server;logic gate;parasitic element (electrical networks);process design kit;ring oscillator;schematic;very-large-scale integration	Pinping Sun;Cole Zemke;Wayne H. Woods;Nick Perez;Hailing Wang;Essam Mina;Barbara Dewitt	2011	2011 12th International Symposium on Quality Electronic Design	10.1109/ISQED.2011.5770769	layout versus schematic;layout;process design;embedded system;benchmark;electronic engineering;logic gate;computer science;engineering;electrical engineering;integrated circuit;ring oscillator;accuracy and precision;capacitance;resistance;engineering drawing;integrated circuit design	EDA	24.295351713637803	54.517173907665935	23984
b8e187ba5ebdfc036a2d5fccf30f7b47ab27fe93	simultaneous euv flare- and cmp-aware placement	ultraviolet lithography chemical mechanical polishing optimisation planarisation;metals layout optimization ultraviolet sources routing integrated circuit modeling analytical models;nonlinear analytical optimization framework simultaneous euv flare aware placement extreme ultraviolet lithography next generation lithography technology chemical mechanical polishing planarization process chip surface topography layout pattern distribution cmp variation optimization cmp aware placement sigmoid distribution model euv flare effect reduction metal aware pin model metal distribution	Extreme ultraviolet (EUV) lithography is one of the most promising next-generation lithography technologies, while chemical mechanical polishing (CMP) is the key planarization process for improving chip surface topography. The two techniques are highly related to layout pattern distribution and require different (even conflicting) distributions for EUV flare and CMP variation optimization to achieve better yields. Placement is a critical stage for controlling layout pattern distribution. In this paper, we propose the first work of simultaneous EUV flare-and CMP-aware placement to address the conflicting pattern distribution requirements with the two techniques. We present a sigmoid distribution model to reduce EUV flare effects and a metal-aware pin model to improve metal distribution. The two models are incorporated into a non-linear analytical optimization framework to achieve desired placement solutions. Experimental results show the effectiveness and efficiency of our proposed method.	algorithm;chemical-mechanical planarization;mathematical optimization;network congestion;nonlinear system;requirement;sigmoid function;topography	Chi-Yuan Liu;Yao-Wen Chang	2014	2014 IEEE 32nd International Conference on Computer Design (ICCD)	10.1109/ICCD.2014.6974689	electronic engineering;engineering drawing	EDA	21.499111880659626	58.08570255753481	23995
5315ae155d627da3bac86a97108a33f3cda17008	pin assignment with global routing for vlsi building block layout	assignment;concepcion asistida;asignacion;computer aided design;concepcion circuito;posicionamiento;implantation topometrie;integrated circuit layout;building block;routing;efficient algorithm;circuit design;circuit vlsi;assignation;layout;network routing;algorithme;algorithm;positioning;vlsi circuit;network routing vlsi circuit layout cad integrated circuit layout;global routing;linear time;vlsi;conception assistee;linear time optimal algorithm global routing vlsi building block layout block reshaping block positioning channel pin assignment;circuit layout cad;conception circuit;encaminamiento;circuito vlsi;routing very large scale integration shape floors tree graphs;acheminement;positionnement;algoritmo;implantacion topometria	In this paper, we will consider global routing and pin assignment in VLSI building block layout, and present an efficient algorithm which integrates global routing, pin assignment, block reshaping and positioning. The general flow of the proposed algorithm is the same as the one proposed in [1] and consists of two main phases. The first phase is to determine not only global routes and coarse pin assignment in the same way as [1], but also shapes and positions of blocks. The second phase is to compute the final pin assignment for channels. We generalize the channel pin assignment (CPA) problem in [1], in which the CPA problem is formulated for only channels formed by two blocks, to the CPA problem for channels formed by multiple blocks. We will propose a linear time optimal channel pin assignment algorithm, which is an extension of the algorithm in [1]. Experimental results show the effectiveness of the proposed algorithm.	algorithm;rs-232;routing;time complexity;very-large-scale integration	Tetsushi Koide;Shin'ichi Wakabayashi;Noriyoshi Yoshida	1996	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.552091	routing;electronic engineering;computer science;engineering;electrical engineering;theoretical computer science;computer aided design;engineering drawing;algorithm;computer network	EDA	15.348876590328901	50.80984613405206	24040
350a6dac3a53209879bf13f97dae5cb3cff7890f	utilizing synthesis to verify boolean function models	binary decision diagram complexity model;boolean functions binary decision diagrams monte carlo methods data structures digital systems circuit testing circuit simulation digital circuits logic testing digital integrated circuits;boolean functions;rtl hardware;boolean function;boolean function models;monte carlo methods boolean functions;standard cell library;monte carlo;rtl hardware boolean function models monte carlo data binary decision diagram complexity model standard cell library;reduction method;monte carlo methods;analytical model;monte carlo data;binary decision diagram	In this paper, we compare two different Boolean function reduction methods in order to justify the analytical model of the Monte Carlo data for Boolean function complexity. We use a binary decision diagram (BDD) complexity model (proposed earlier) and weigh it against the complexity behavior generated by Synopsys Design Compiler (DC). We use this synthesis tool (that utilizes a standard cell library) to generate RTL hardware description of Monte Carlo circuits as gate-level netlists. The two reduction methods (model and DC) transform an arbitrary function into a much-reduced representation of the same function. The comparison confirms that the behavior of Boolean function complexity using the model and the DC is visually and statistically similar; the similarity holds true for BDDs representing functions comprising a wide range of variables and minterms.	algorithm;binary decision diagram;boolean algebra;cell (microprocessor);compiler;complexity;experiment;influence diagram;level of detail;mathematical model;monte carlo method;netlist;standard cell	Azam Beg;P. W. Chandana Prasad;Walid Ibrahim;Emad Abu Shama	2008	2008 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2008.4541733	circuit complexity;boolean circuit;and-inverter graph;circuit minimization for boolean functions;discrete mathematics;decision tree model;hybrid monte carlo;standard boolean model;computer science;theoretical computer science;mathematics;boolean function;binary decision diagram;algorithm;monte carlo method	EDA	18.838060677310043	46.53699265290528	24059
28847a598a204a736f7b4a33311aed45cc56e44b	hades: architectural synthesis for heterogeneous dark silicon chip multi-processors	silicon;optimisation;silicon elemental semiconductors iterative methods microprocessor chips optimisation;real time;elemental semiconductors;si hades heterogeneous dark silicon chip multiprocessors iterative optimization based approach dark silicon heterogeneous cmp general purpose multithreaded applications degree of parallelism fixed dop scenarios variable dop scenarios;iterative methods;embedded systems;silicon optimization benchmark testing equations mathematical model instruction sets computer architecture;fault tolerance;microprocessor chips	In this paper, we propose an efficient iterative optimization based approach for architectural synthesis of dark silicon heterogeneous chip multi-processors (CMPs). The goal is to determine the optimal number of cores of each type to provision the CMP with, such that the area and power budgets are met and the application performance is maximized. We consider general-purpose multi-threaded applications with a varying degree of parallelism (DOP) that can be set at run-time, and propose an accurate analytical model to predict the execution time of such applications on heterogeneous CMPs. Our experimental results illustrate that the synthesized heterogeneous dark silicon CMPs provide between 19% to 60% performance improvements over conventional homogeneous designs for variable and fixed DOP scenarios, respectively.	central processing unit;dark silicon;degree of parallelism;general-purpose modeling;integrated circuit;iterative method;mathematical optimization;parallel computing;run time (program lifecycle phase);thread (computing)	Yatish Turakhia;Bharathwaj Raghunathan;Siddharth Garg;Diana Marculescu	2013	2013 50th ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2463209.2488948	embedded system;fault tolerance;electronic engineering;parallel computing;real-time computing;computer science;engineering;operating system;iterative method;silicon	EDA	-0.021003305355467544	53.1716191692369	24236
6f2ce30a213f680485b23b787ed3b27284f8d211	customized architecture technology for high performance computing		Customized Architecture is one of the technical road for exascale high performance computing. We will give an overview about FPGA customized architecture. Research experiences on deep learning algorithms accelerators for data analyzing, footprint and cipher algorithms accelerators for information processing, and matrix processing algorithms accelerators for scientific computing will be discussed.	algorithm;cipher;computational science;deep learning;field-programmable gate array;information processing;machine learning;supercomputer	Jingfei Jiang	2017		10.1145/3129457.3129500	cipher;computer architecture;reference architecture;data architecture;architectural technology;architecture;computer science;applications architecture;supercomputer;space-based architecture	HPC	2.892508695398613	41.36200875443306	24246
b24ef8021811cd4ef0fcc96a770657b664ee5b52	blockquicksort: avoiding branch mispredictions in quicksort	in place sorting quicksort branch mispredictions lean programs;004	Since the work of Kaligosi and Sanders (2006), it is well-known that Quicksort – which is commonly considered as one of the fastest in-place sorting algorithms – suffers in an essential way from branch mispredictions. We present a novel approach to address this problem by partially decoupling control from data flow: in order to perform the partitioning, we split the input in blocks of constant size (we propose 128 data elements); then, all elements in one block are compared with the pivot and the outcomes of the comparisons are stored in a buffer. In a second pass, the respective elements are rearranged. By doing so, we avoid conditional branches based on outcomes of comparisons at all (except for the final Insertionsort). Moreover, we prove that for a static branch predictor the average total number of branch mispredictions is at most n logn + O(n) for some small depending on the block size when sorting n elements. Our experimental results are promising: when sorting random integer data, we achieve an increase in speed (number of elements sorted per second) of more than 80% over the GCC implementation of C++ std::sort. Also for many other types of data and non-random inputs, there is still a significant speedup over std::sort. Only in few special cases like sorted or almost sorted inputs, std::sort can beat our implementation. Moreover, even on random input permutations, our implementation is even slightly faster than an implementation of the highly tuned Super Scalar Sample Sort, which uses a linear amount of additional space. 1998 ACM Subject Classification F.2.2 Nonnumerical Algorithms and Problems	block size (cryptography);branch predictor;c++;coupling (computer programming);dataflow;fastest;in-place algorithm;insertion sort;kerrison predictor;quicksort;randomness;samplesort;scalar processor;sort (c++);sorting algorithm;speedup	Stefan Edelkamp;Armin Weiss	2016		10.4230/LIPIcs.ESA.2016.38	merge sort;insertion sort;parallel computing;bucket sort;computer science;bubble sort;operating system;external sorting;sorting algorithm;block sort;comparison sort;mathematics;distributed computing;algorithm;introsort	Arch	1.3879987548756043	39.80551493607772	24254
d694c09a34a34e9ada0da50de80b7e57e7b53d0a	design and implementation of a radiation tolerant on-board computer for science technology satellite-3	triple modular redundant;science and technology;integrated circuit;radiation effect;clocks;sram chips artificial satellites field programmable gate arrays space communication links;field programmable gate arrays tunneling magnetoresistance clocks protons computer aided software engineering redundancy table lookup;computer aided software engineering;tmr scheme radiation tolerant on board computer science technology satellite 3 obc stsat 3 sram based fpga single event upset triple modular redundancy;redundancy;space communication links;design and implementation;artificial satellites;single event upset;field programmable gate arrays;tunneling magnetoresistance;table lookup;space application;protons;sram chips	This paper describes the design and implementation of a radiation tolerant on-board computer (OBC) for the science and technology satellite-3 (STSAT-3). SRAM-based FPGAs are replacing traditional integrated circuits for space applications. However, it is difficult to employ the approach in space applications without radiation tolerant schemes to deal with the radiation effects such as single event upset (SEU). To mitigate the SEU effect, we apply a triple modular redundancy (TMR) scheme to the STSAT-3 OBC based on FPGA. Although there is an overhead in area, power and minimum clock period, we notice through a radiation test in an irradiation facility that our TMR based OBC is immune to the radiation environments up to a proton energy of 20.3MeV. The radiation environment of the test is expected to be more severe than the environment in which STSAT-3 is to be located.	clock rate;experiment;field-programmable gate array;integrated circuit;internet branding;memory scrubbing;on-board data handling;overhead (computing);single event upset;static random-access memory;triple modular redundancy	Dong-Soo Kang;Kyoung-Son Jhang;Dae-Soo Oh	2010	2010 NASA/ESA Conference on Adaptive Hardware and Systems	10.1109/AHS.2010.5546260	embedded system;real-time computing;computer science;operating system;integrated circuit;redundancy;computer-aided software engineering;proton;satellite;field-programmable gate array;science, technology and society	EDA	8.876870582336227	59.69887552714229	24273
41afb60e7ff2f999d6aa7fdc9baac2f636d39ac0	two-dimensional matrix partitioning for parallel computing on heterogeneous processors based on their functional performance models	functional performance models;heterogeneous computing;data partitioning algorithms;iterative algorithm;data partitioning;performance model;parallel computer;heterogeneous processors;matrix multiplication;parallel matrix multiplication	The functional performance model (FPM) of heterogeneous processors has proven to be more realistic than the traditional models because it integrates many important features of heterogeneous processors such as the processor heterogeneity, the heterogeneity of memory structure, and the effects of paging. Optimal 1D matrix partitioning algorithms employing FPMs of heterogeneous processors are already being used in solving complicated linear algebra kernel such as dense factorizations. However, 2D matrix partitioning algorithms for parallel computing on heterogeneous processors based on their FPMs are unavailable. In this paper, we address this deficiency by presenting a novel iterative algorithm for partitioning a dense matrix over a 2D grid of heterogeneous processors and employing their 2D FPMs. Experiments with a parallel matrix multiplication application on a local heterogeneous computational cluster demonstrate the efficiency of this algorithm.	algorithm;angular defect;central processing unit;iterative method;linear algebra;matrix multiplication;paging;parallel computing;sparse matrix	Alexey L. Lastovetsky;Ravi Reddy	2009		10.1007/978-3-642-14122-5_15	parallel computing;matrix multiplication;computer science;theoretical computer science;distributed computing;iterative method;symmetric multiprocessor system	HPC	-3.3365259564362058	38.81008479356681	24343
83603117bc69a35e2e2257a3fb59f71acab53ba2	analysis of the signal reliability measure and an evaluation procedure	functional reliability;signal reliability;signal probability;signal reliability combinational circuits functional reliability probability of fault occurrence signal probability;digital circuits;combinational circuit;probability of fault occurrence;combinational circuits	The classical reliability measure of digital circuits, known as functional reliability, assumes that the circuit fails whenever a fault is present in it. It has long been known that this reliability measure is overly pessimistic since digital circuits may produce correct output signals even when some faults are present in them. A different reliability measure, known as signal reliability, is the probability that the circuit output is correct. This reliability measure is analyzed first and compared to the functional reliability measure. Next, a new procedure for the evaluation of the signal reliability measure is presented.	digital electronics	Israel Koren	1979	IEEE Transactions on Computers	10.1109/TC.1979.1675326	computer science;reliability block diagram;mathematics;combinational logic;algorithm;statistics	EDA	22.3174677620285	51.1578175276817	24399
c51a3aa795f279bcc11fe21782f8a153af8274a6	power modeling and analysis in early design phases	high level synthesis;integrated circuit modelling;docea aceplorer;synopsys platform architect mco;cyber-physical systems;electronic devices;industrial signal-processing;power modeling;spreadsheet-based power analysis;comparison;early design phase;electronic system level;industrial use case;power analysis;power modeling	Low power consumption of electronic devices has been an important requirement for many cyber-physical systems in field. Today, power dissipation is often estimated by spreadsheet-based power analysis. A leading-edge high-level power analysis method has the objective of providing high confidence levels in early design stages, where power design decisions have severe impact. This work examines and compares three high-level power analysis approaches (spreadsheet-based, Synopsys Platform Architect MCO, and DOCEA Aceplorer) by an industrial use case. The first chapter introduces into general power analysis concepts. Chapter two presents different power analysis methods and tools that are applied on an industrial signal-processing use case described in chapter three. Chapter four compares these tools and methods by selected criteria such as power modeling effort and quality of results. Chapter five concludes about the investigated methods.	cpu power dissipation;cyber-physical system;high- and low-level;quality of results;signal processing;spreadsheet	Bernhard Fischer;Christian Cech;Hannes Muhr	2014	2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;power-flow study;electronic engineering;real-time computing;simulation;power analysis;computer science;engineering;electrical engineering;operating system;common power format;high-level synthesis;power optimization;statistics	EDA	3.7896921543467803	55.020801982732074	24404
716fe0230eb001341f2b2e05d7d464697059e949	p-biblio-metres, a parallel data mining tool for the reconstruction of molecular networks	molecular network reconstruction;parallel programming;data text mining;parallelization;scalability;multithreading	Biblio-MetReS is a single-thread data mining application that facilitates the reconstruction of molecular networks based on automated text mining analysis of published scientific literature. This application is very CPU-intensive, requiring High Performace Computing (HPC). Due to the amount of execution tasks, it can be quite slow. Those tasks are repetitive and consist in mining the information from large sets of scientific documents, a process where the time-cost of the application could be improved through paralellization.  This paper presents a parallel version of Biblio-MetReS. The multithreading application P(arallel)-Biblio-MetReS distributes the work among copies of the same Java class, each mining a collection of documents obtained in a previous search phase from different literature sources of Internet. In this article, we compare performances between the parallel and non-parallel versions of the application and discuss scalability issues on multi-threading systems in the context of this application. Furthermore, we also optimize memory management and reutilization of document parsing results. Our experimental results corroborate the good performance of P-Biblio-MetReS, pinpointing specific aspects that still need to be improved.	central processing unit;data mining;java;memory management;multithreading (computer architecture);parsing;performance;scalability;scientific literature;text mining;thread (computing)	Ivan Teixido;Anabel Usie;Josep L. Lérida;Francesc Solsona;Jorge Comas;Nestor Torres;Hiren Karathia;Rui Alves	2013		10.1145/2488551.2488586	computer science;theoretical computer science;data mining;database	HPC	-1.364229339674933	42.58032277298099	24413
33ec6cf262dfd15a5d29522a7c82bee9113f6203	idap: a tool for high-level power estimation of custom array structures	microprocessor chips spice sram chips integrated circuit design;energy consumption semiconductor device modeling circuits power dissipation random access memory batteries costs switches permission embedded computing;power estimation;implementation styles high level power estimation custom array structures array circuit implementation implementation dependent array power estimator sram based arrays high level array description idap tool e500 processor core dynamic power dissipation spice simulations bitline voltage swing memory bit cell dimensions;implementation styles;arrays;high level synthesis;integrated circuit design;high level power estimation;estimation;cmos digital integrated circuits;arrays high level synthesis sram chips power consumption low power electronics spice cmos digital integrated circuits;power dissipation;low power electronics;semiconductor device modeling power dissipation microarchitecture energy consumption batteries costs switches bridge circuits power generation spice;spice simulation idap estimator implementation dependent array power estimator power estimation power dissipation array circuit implementation sram based arrays static ram random access memory array operations idap tool industrial design e500 processor core error margin;power consumption;idap;implementation dependent array power;spice;microprocessor chips;industrial design;sram chips	While array structures are a significant source of power dissipation,there is a lack of accurate high-level power estimatorsthat account for varying array circuit implementationstyles. We present a methodology and a tool, the ImplementationDependent Array Power (IDAP) estimator, that modelpower dissipation in SRAM based arrays accurately basedon a high-level description of the array, parameterized bythe array operations, the implementation styles, and varioustechnology dependent parameters. The methodologyis generic and the IDAP tool has been validated on industrialdesigns across a wide variety of array implementationsin the e500 processor core. For these industrial designs,IDAP generates high-level estimates for dynamic power dissipationthat are highly accurate with an error margin ofless than 22.2% of detailed (layout extracted) SPICE simulations.	array data structure;cmos;cpu power dissipation;high- and low-level;ibm notes;multi-core processor;protein data bank;spice;simulation;spectral leakage;static random-access memory	Mahesh Mamidipaka;Kamal S. Khouri;Nikil D. Dutt;Magdy S. Abadir	2003	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/ICCAD.2003.81	embedded system;estimation;electronic engineering;real-time computing;industrial design;engineering;dissipation;high-level synthesis;low-power electronics;statistics;integrated circuit design	EDA	18.310482777516466	56.82251926685393	24419
464ff77ad503608a1f53b64c86933040e4d46d64	a distributed algorithm for fault diagnosis in systems with soft failures	fault free units;consistent diagnoses;fault tolerant computing distributed processing failure analysis fault location;interconnection;bounded fault situations distributed algorithm fault diagnosis soft failures fully distributed networks system model faulty units distributed algorithm fault free units consistent diagnoses system status;system modeling;distributed networks;distributed processing;reseau ordinateur;computer network;failure analysis;faulty units;fault tolerant computing;diagnostic panne;fully distributed networks;bounded fault situations;fault diagnostic;system status;algorithme reparti;interconnexion;diagnostico pana;red ordenador;soft failures;system model;distributed algorithms fault diagnosis very large scale integration electrons digital arithmetic computer errors testing redundancy digital filters digital signal processing;distributed algorithm;interconeccion;fault diagnosis;fault location	The problem of diagnosis of soft failures at the system level in large and fully distributed networks of processors (or units) is considered. A system model in which each of the network's units is assumed to possess the ability to test (or evaluate) certain other units for the presence of failures is employed. Using this model and assuming that the total number of faulty units does not exceed a given bound, a distributed algorithm is presented which allows all the fault-free units to independently converge to correct and consistent diagnoses of the system status. This algorithm is also shown to be applicable to bounded fault situations where both units and communication links can be faulty. >	distributed algorithm	Che-Liang Yang;Gerald M. Masson	1988	IEEE Trans. Computers	10.1109/12.8723	embedded system;distributed algorithm;failure analysis;real-time computing;systems modeling;system model;computer science;interconnection;distributed computing	Embedded	23.63122100735284	44.49680144795946	24531
b383cba3974f647a820bbf43c3317c9f9e45b451	burn-in stress test of analog cmos ics	stress testing foundries application specific integrated circuits electromigration costs failure analysis hot carriers integrated circuit interconnections manufacturing;feasibility study;analogue integrated circuits;cmos analogue integrated circuits;integrated circuit testing;integrated circuit reliability;integrated circuit reliability cmos analogue integrated circuits analogue integrated circuits integrated circuit testing;burn in process analog cmos ic evosta extreme voltage stress test for analog cmos ic extreme temperature burn in stress test gate oxide reliability mixed signal cmos ic screening method field reliability screen defective products;stress testing	With the successful development of EVoSTA (Extreme-Voltage Stress Test for Analog CMOS ICs), this paper investigated whether the extreme-temperature burn-in stress test is properly applied for enhancing the gate-oxide reliability of mixed-signal/analog CMOS ICs. Burn-in is an effective screening method used in predicting, achieving, and enhancing field reliability of ICs. Today, almost all IC manufacturers perform 100% burn-in for various durations to screen defective products. However, the major problems associated with burn-in are the determination of exactly how long the burn-in process should continue, balancing appropriately the needs of reliability and the total costs, and what stress vectors should be applied? This paper conducted a feasibility study for resolving such issues.	burn-in;cmos;gate oxide;mixed-signal integrated circuit;norm (social)	Chin-Long Wey;Meng-Yao Liu	2004	13th Asian Test Symposium	10.1109/ATS.2004.28	mixed-signal integrated circuit;reliability engineering;feasibility study;electronic engineering;engineering;electrical engineering;integrated injection logic;stress testing	SE	22.639906552168974	54.97568160737118	24537
9a44f0cc5ce2cc13e94c8f5213496fc4ea93fbee	impact of active thermal management on power electronics design	reliability;active thermal management;chip utilization	Power electronic system design is typically constrained by the thermal limitation so by the overall losses and the peak current. To stay within the maximum current, reached only during transients, the system is typically overrated. Active thermal management is used to control the maximum temperature and the temperature swing to reduce failures that are mostly caused by them. In this paper it is proposed to use the active thermal management to reduce the switching losses or to move them to less stressed devices, during transients, such as a module can reach an higher current, without violating thermal constraints, and the need of overdesign can be reduced. Hence an optimal and cost effective design of power electronics system is achieved. Corresponding author. ma@tf.uni-kiel.de Tel: +49 (431) 880 6107; Fax: +49 (431) 880 6103 Impact of active thermal management on power electronics design M. Andresen , M. Liserre * Markus Andresen. ma@tf.uni-kiel.de Tel: +49 (431) 880 6107; Fax: +49 (431) 880 6103		Markus Andresen;Marco Liserre	2014	Microelectronics Reliability	10.1016/j.microrel.2014.07.069	control engineering;electronic engineering;engineering;electrical engineering;reliability;mathematics;statistics	EDA	20.276678473505395	59.327104116052	24595
089bdc784552da0d2c9e55194b74f46a2c0b51fc	a faster counterexample minimization algorithm based on refutation analysis	minimisation;computational complexity minimisation formal verification computability set theory;design automation;complexity analysis counterexample minimization algorithm refutation analysis sat solver brute force lifting algorithm time overhead multiple variable elimination model checking technology software verification hardware verification;hardware verification;software verification;computability;model checking technology;software systems;complexity analysis;minimization methods;set theory;data mining;sufficient conditions;refutation analysis;formal verification;theoretical analysis;computational complexity;counterexample minimization algorithm;performance analysis;multiple variable elimination;minimization methods algorithm design and analysis computer science performance analysis software systems hardware production systems data mining sufficient conditions design automation;production systems;time overhead;computer science;sat solver;algorithm design and analysis;brute force lifting algorithm;hardware	It is a hot research topic to eliminate irrelevant variables from counterexample, to make it easier to be understood. The BFL algorithm is the most effective counterexample minimization algorithm compared to all other approaches. But its time overhead is very large due to one call to SAT solver for each candidate variable to be eliminated. The key to reduce time overhead is to eliminate multiple variables simultaneously. Therefore, we propose a faster counterexample minimization algorithm based on refutation analysis in this paper. We perform refutation analysis on those UNSAT instances of BFL, to extract the set of variables that lead to UNSAT. All variables not belong to this set can be eliminated simultaneously as irrelevant variables. Thus we can eliminate multiple variables with only one call to SAT solver. Theoretical analysis and experiment result shows that, our algorithm can be 2 to 3 orders of magnitude faster than existing BFL algorithm, and with only minor lost in counterexample minimization ability.	algorithm;boolean satisfiability problem;overhead (computing);relevance;solver	ShengYu Shen;Ying Qin;Sikun Li	2005	Design, Automation and Test in Europe	10.1109/DATE.2005.14	algorithm design;minimisation;electronic design automation;formal verification;software verification;computer science;theoretical computer science;production system;computability;boolean satisfiability problem;programming language;computational complexity theory;algorithm;software system;set theory	EDA	18.09692275305375	47.99557410272113	24654
16a541eb62d600d295952e63d4c96a3bc71aefc7	application of avx (advanced vector extensions) for improved performance of the parfes - finite element parallel direct solver	structural engineering computing finite element analysis matrix multiplication microprocessor chips multiprocessing systems;bulldozer architecture avx technique advanced vector extensions technique parfes parallel finite element solver finite element analysis structural mechanics solid mechanics multicore computers dgemm matrix multiplication procedure intel mkl math kernel library acml amd core math library amd opteron 6276 processor;registers random access memory indexes arrays sparse matrices computers vectors;structural engineering computing;matrix multiplication;finite element analysis;multiprocessing systems;microprocessor chips	The paper considers application of the AVX (Advanced Vector Extensions) technique to improve the performance of the PARFES parallel finite element solver, intended for finite element analysis of large-scale problems of structural and solid mechanics using multi-core computers. The basis for this paper was the fact that the dgemm matrix multiplication procedure implemented in the Intel MKL (Math Kernel Library) and ACML (AMD Core Math Library) libraries, which lays down the foundations for achieving high performance of direct methods for sparse matrices, does not provide for satisfactory performance with the AMD Opteron 6276 processor, Bulldozer architecture, when used with the algorithm required for PARFES. The procedure presented herein significantly improves the performance of PARFES on computers with processors of the above architecture, while maintaining the competitiveness of PARFES with the Intel MKL dgemm procedure on computers with Intel processors.	amd core math library;advanced vector extensions;algorithm;central processing unit;computer;finite element method;library (computing);math kernel library;matrix multiplication;microkernel;multi-core processor;solver;sparse matrix	Sergiy Fialko	2013	2013 Federated Conference on Computer Science and Information Systems		computer architecture;parallel computing;computer hardware;matrix multiplication;computer science;operating system;finite element method;algorithm	HPC	-3.2591340976095937	39.55486048228691	24675
8e85716101d6cb3e65a1954b79aab463c6a7b22a	algorithm and architecture independent benchmarking with seak	bottleneck specification;performance benchmarking suite for embedded applications and kernels seak high performance embedded computing end user black box evaluation algorithmic preference architectural preference;kernel;tradeoff evaluation constraining problem bottleneck specification functional mission interface;measurement;computer architecture;functional;benchmark testing kernel parallel processing measurement computer architecture software algorithms;software algorithms;tradeoff evaluation;software performance evaluation embedded systems parallel processing software architecture;mission interface;constraining problem;parallel processing;benchmark testing	Many applications of high performance embedded computing are constrained by performance or power bottlenecks. We designed a new benchmark suite, the Suite for Embedded Applications and Kernels (SEAK), (a) to capture these bottlenecks in a way that encourages creative solutions, and (b) to facilitate rigorous tradeoff evaluation for their solutions. To avoid biases toward existing solutions, both algorithms and architecture are variables. Thus, each benchmark has a mission-centric (abstracted from a particular algorithm) and goal-oriented (functional) specification. To encourage solutions that are any combination of software or hardware, we use an end-user black-box evaluation. To inform procurement decisions, evaluations capture tradeoffs between performance, power, accuracy, size, and weight. We call our benchmarks future proof because they remain useful despite shifting algorithmic/architectural preferences. To create both concise and precise mission-centric specifications, we introduce two distinct benchmark classes. This paper describes the SEAK suite and presents an evaluation of sample solutions that highlights power and performance tradeoffs.	algorithm;benchmark (computing);biasing;black box;bottleneck (software);computer hardware;computer performance;correctness (computer science);embedded system;emoticon;functional specification;procurement;reference implementation;unicom system architect	Nathan R. Tallent;Joseph Manzano;Nitin Gawande;Seunghwa Kang;Darren J. Kerbyson;Adolfy Hoisie;Joseph K. Cross	2016	2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)	10.1109/IPDPS.2016.25	parallel processing;benchmark;parallel computing;kernel;real-time computing;simulation;computer science;theoretical computer science;operating system;distributed computing;measurement	Arch	-3.8196421667828364	49.19978639300001	24682
d271567a9f2d8e280b64b12a5769e22e020f8252	platform leadership in the ambient intelligence era	design automation;ambient intelligence;very large scale integration;home appliances;design optimization;technology management;application specific integrated circuits;system level design;robustness;ambient intelligence very large scale integration application specific integrated circuits home appliances technology management robustness system level design productivity design optimization design automation;productivity	Design reuse has become essential to cope with the ever-increasing design complexity. IP level reuse alone has proven insufficient. Platform based design allows the validation of a robust combination of IP blocks and provides a reference HW and SW baseline which can be supported with an integrated development environment. Several years ago we transitioned into the streaming data era with most systems serving as content generation appliances, content consumption appliances or content distribution equipment. Now we have entered the age of ambient intelligence where the streaming data is served up through wireless links. What will platform leadership look like in this new era? How will the SoC infrastructure change as we move to 90nm technology with more than 30M gate per square centimeter integration capacity? How are usage patterns changing and what represents the killer application that enhances the users quality of life by enabling more advanced interaction with the ambient intelligence? What is it going to take to make a step function improvement in system level design productivity? What happens when power optimization becomes the dominant design consideration? What about SoC affordability? What will the SoC design of the future look like? These are just some of the thought provoking issues that will be addressed in Bob Payne’s keynote.	ambient intelligence;baseline (configuration management);digital distribution;integrated development environment;killer application;level design;mathematical optimization;power optimization (eda);streaming media	Beth Payne	2003	Fourth International Symposium on Quality Electronic Design, 2003. Proceedings.	10.1109/ISQED.2003.1194701	embedded system;productivity;multidisciplinary design optimization;simulation;ambient intelligence;electronic design automation;computer science;systems engineering;engineering;technology management;application-specific integrated circuit;very-large-scale integration;electronic system-level design and verification;robustness;manufacturing engineering	EDA	9.99952789107085	55.964032012468465	24691
d7b8fe027c58fabbb6cbec448f899b5143fd9dcf	heterogeneous cots product integration to allow the comprehensive development of image processing systems	image processing;automated visual inspection;real time;code generation;commercial off the shelf;medical image;design and implementation;industrial production;image processing techniques	Image processing techniques are applied in a wide range of products. Automated visual inspection of industrial products, medical imaging or biometric person authentication are only a few examples. In order to process the great amount of data contained in images highly complex and time-consuming algorithms are needed. Furthermore, many of these applications require real-time performance making specific hardware devices indispensable. Currently, there exist several Commercial Off-The-Shelf (COTS) component libraries that help to implement these hybrid software/hardware systems. In addition, some powerful tools are available that allow prototyping and simulating image processing applications prior to their implementation. However, none of these tools allows to realistically coprototype and co-simulate both software and hardware simultaneously. This work presents a new approach to the development of image processing applications that tackles the question of how to fill the gap between design and implementation. A new graphical component-based tool has been implemented that allows building image processing applications from functional and architectural prototyping stages to software/hardware co-simulation and final code generation. Building this tool has been possible thanks to the synergy that arises from the integration of several preexistent software and hardware COTS components and tools.	image processing	Cristina Vicente-Chicote;Ana Toledo Moreo;Carlos Fernández Andrés	2005		10.1007/978-3-540-30587-3_7	embedded system;real-time computing;computer science;digital image processing;hardware architecture;computer engineering	Robotics	2.8202046973503476	47.06690570875402	24716
0bd233055fc5892d3d206a7bdf46e3fc1ef39e07	comparing three heuristic search methods for functional partitioning in hardware-software codesign	directed acyclic graph;hardware software codesign;simulated annealing;function block;heuristic search;heuristic search algorithms;hardware software partitioning;genetic algorithm;tabu search;functional partitioning;first fit	This paper compares three heuristic search algorithms: genetic algorithm (GA), simulated annealing (SA) and tabu search (TS), for hardware-software partitioning. The algorithms operate on functional blocks for designs represented as directed acyclic graphs, with the objective of minimising processing time under various hardware area constraints. The comparison involves a model for calculating processing time based on a nonincreasing first-fit algorithm to schedule tasks, given that shared resource conflicts do not occur. The results show that TS is superior to SA and GA in terms of both search time and quality of solutions. In addition, we have implemented an intensification strategy in TS called penalty reward, which can further improve the quality of results.	a* search algorithm;bin packing problem;combinatorial optimization;component-based software engineering;directed acyclic graph;genetic algorithm;heuristic;mathematical optimization;penalty method;quality of results;real-time clock;reconfigurable computing;scheduling (computing);simulated annealing;software release life cycle;software system;systems architecture;tabu search;video processing	Theerayod Wiangtong;Peter Y. K. Cheung;Wayne Luk	2002	Design Autom. for Emb. Sys.	10.1023/A:1016567828852	beam search;mathematical optimization;genetic algorithm;heuristic;simulated annealing;tabu search;computer science;theoretical computer science;machine learning;incremental heuristic search;best-first search;directed acyclic graph	EDA	0.1664461951270275	52.44261574928591	24743
c0cbbd9fb2fc5da8b44854446b23e6c464111209	a synchronization-free algorithm for parallel sparse triangular solves	linear algebra;paper;tesla k40;sparse direct solvers;ati;amd radeon r9 fury x;cuda;package;algorithms;computer science;opencl;nvidia geforce gtx titan x	The sparse triangular solve kernel, SpTRSV, is an important building block for a number of numerical linear algebra routines. Parallelizing SpTRSV on today’s manycore platforms, such as GPUs, is not an easy task since computing a component of the solution may depend on previously computed components, enforcing a degree of sequential processing. As a consequence, most existing work introduces a preprocessing stage to partition the components into a group of level-sets or colour-sets so that components within a set are independent and can be processed simultaneously during the subsequent solution stage. However, this class of methods requires a long preprocessing time as well as significant runtime synchronization overhead between the sets. To address this, we propose in this paper a novel approach for SpTRSV in which the ordering between components is naturally enforced within the solution stage. In this way, the cost for preprocessing can be greatly reduced, and the synchronizations between sets are completely eliminated. A comparison with the state-of-the-art library supplied by the GPU vendor, using 11 sparse matrices on the latest GPU device, show that our approach obtains an average speedup of 2.3 times in single precision and 2.14 times in double precision. The maximum speedups are 5.95 and 3.65, respectively. In addition, our method is an order of magnitude faster for the preprocessing stage than existing methods.	algorithm;barrier (computer science);cuda;computer memory;double-precision floating-point format;graphics processing unit;manycore processor;mathematical optimization;multi-core processor;numerical linear algebra;opencl api;overhead (computing);parallel computing;preprocessor;rough set;single-precision floating-point format;sparse matrix;speedup	Weifeng Liu;Ang Li;Jonathan D. Hogg;Iain S. Duff;Brian Vinter	2016		10.1007/978-3-319-43659-3_45	parallel computing;computer science;theoretical computer science;linear algebra;operating system;database;distributed computing;programming language;package	HPC	-3.0492441748378205	39.98129878030776	24747
6412147d6b86cdba3d4369df5552f497edef0d89	asynchronous high throughput noc under high process variation	clocks;synchronization;integrated circuit interconnections;delays integrated circuit interconnections clocks throughput switches synchronization ports computers;asynchronous noc switch clock distribution network network on chip interconnect network throughput robust design high process variation high throughput noc;ports computers;switches;asynchronous design process variation network on chip interconnect clock skew synchronous design;delays;throughput;switching circuits asynchronous circuits clock distribution networks integrated circuit design integrated circuit interconnections integrated circuit reliability network on chip	Asynchronous NoC switch is proposed as a robust design to mitigate the impact of process variation. Asynchronous and synchronous network on chip design are implemented to evaluate the impact of process variation on the network throughput. Network on chip interconnects and clock distribution network are considered under process variation with the advance in technology. The variation in logic and interconnect are included to evaluate the delay and throughput variation with different technologies. The throughput negligibly decreases under high process variation conditions in asynchronous NoC switch, while rapidly decreases by up to 25% in synchronous design at the same variation conditions.	clock signal;electrical connection;network on a chip;network switch;synchronization in telecommunications;synchronous circuit;throughput	Rabab Ezz-Eldin;Magdy A. El-Moursy;Hesham F. A. Hamed	2013	2013 IEEE 20th International Conference on Electronics, Circuits, and Systems (ICECS)	10.1109/ICECS.2013.6815429	asynchronous system;embedded system;electronic engineering;real-time computing;asynchronous circuit;computer science;network on a chip	EDA	15.19287564890311	57.11061266551897	24812
3d052341eeab49957c901610bbbb2faf3f308ee1	buffer insertion with adaptive blockage avoidance	domino circuits;crosstalk;time complexity;routing;layout;high performance design;shielding;power routing;steiner tree;buffer insertion;noise	Buffer insertion is a fundamental technology for VLSI interconnect optimization. Several existing buffer insertion algorithms have evolved from van Ginneken's classic algorithm. In this work, we extend van Ginneken's algorithm to handle blockages in the layout. Given a Steiner tree containing a Steiner point that overlaps a blockage, a local adjustment is made to the tree topology that enables additional buffer insertion candidates to be considered. This adjustment is adaptive to the demand on buffer insertion and is incurred only when it facilitates the maximal slack solution. This approach can be combined with any performance-driven Steiner tree construction. The overall time complexity has linear dependence on the number of blockages and quadratic dependence on the number of potential buffer locations. Experiments on several large nets confirm that high-quality solutions can be obtained through this technique with little CPU cost.	algorithm;central processing unit;mathematical optimization;maximal set;slack variable;steiner tree problem;time complexity;tree network;very-large-scale integration	Jiang Hu;Charles J. Alpert;Stephen T. Quay;Gopal Gandham	2002		10.1145/505388.505412	layout;time complexity;mathematical optimization;routing;crosstalk;steiner tree problem;computer science;noise;electromagnetic shielding;algorithm	EDA	15.619902373673556	52.56095686861561	24822
c84804785d86ea7efc82fd39e2308e8aaa8701cf	increasing microprocessor performance with tightly-coupled reconfigurable logic arrays	field programmable gate array;microprocessor;arquitectura circuito;reconfigurable logic;circuit architecture;red puerta programable;reseau porte programmable;architecture circuit;microprocesseur;microprocesador	Conventional approaches to increase the performance of microprocessors often do not provide the performance boost one has hoped for due to diminishing returns. We propose the extension of a conventional hardwired microprocessor with a reconngurable logic array, integrating both conventional and reconngurable logic on the same die. Simulations have shown that even a comparatively simple and compact extension allows performance gains of 2{4 times over conventional RISC processors of comparable complexity, making this approach especially interesting for embedded microprocessors. The integration scale in the semiconductor industry is incessantly increasing. This trend allowed the implementation of increasingly complex systems on a single die. Dynamic instruction tracing and cache hit rate measurements show that the eeectiveness of increased cache size and higher superscalarity is leveling oo beyond a certain threshold. For instance, doubling the cache size from 64 to 128 KByte has been shown to increase the hit rate by only 0.4% 1] under certain loads. Average CPI values of commercial superscalar processors remain poor. A CPI of 1.2{3.0 was reported for quad-issue Alpha 21164 at 300MHz 2], a quad-issue HaL SPARC at 118MHz achieves 0.83 CPI under standard load 3]. These numbers can at least partly be attributed to bottlenecks in the memory subsystem , however the question arises whether the cost of larger caches and additional superscalar function units can be justiied by these rather lackluster performance improvements. Consequently, alternative approaches have been investigated, of which only a few can be mentioned here. Since memory is a major factor of system performance and the processor-memory gap is widening, integration of memory or additional special-purpose logic on the same die with a microprocessor core looks attractive. Projects like IRAM 2] and PPRAM 4] have shown promising results from this venue. Other researchers are abandoning the conventional computing	bottleneck (software);cache (computing);central processing unit;complex systems;computer simulation;die (integrated circuit);embedded system;kilobyte;microprocessor;multi-core processor;period-doubling bifurcation;reconfigurable computing;sparc;semiconductor industry;superscalar processor;venue (sound system)	Sergei Sawitzki;Achim Gratz;Rainer G. Spallek	1998		10.1007/BFb0055271	embedded system;computer architecture;programmable logic array;computer science;programmable logic device;field-programmable gate array	Arch	-3.0256376803433778	49.02536825645252	24823
ec2d6ee715671517d72e0ed81b84ba71f93acf8f	23.3 a highly integrated smartphone soc featuring a 2.5ghz octa-core cpu with advanced high-performance and low-power techniques	silicon;clocks;system on chip ball grid arrays cmos digital integrated circuits high k dielectric thin films integrated circuit design long term evolution modems smart phones;clocks system on chip delays switches high definition video silicon logic gates;size 28 nm highly integrated smartphone soc advanced high performance technique low power technique high performance cpu design heterogeneous octa core cpu complex highly integrated mobile soc metal gate cmos die size copper pillars die to substrate interface substrate trace pitch bga integrated cellular modem rei 9 cat 4 lte fdd tdd cellular connectivity rf connectivity dc hspa td scdma edge 802 11ac bluetooth le multignss gps glonass beidou galileo qzss ant multimedia features high performance power vr series6 gpu wqxga displays image processor camera interface ultrahd video playback support h 264 vp9 frequency 2 5 ghz;logic gates;system on chip;high definition video;switches;delays	This paper describes the high-performance CPU design of a heterogeneous octa-core CPU complex, incorporated into a highly integrated mobile SoC for smartphone applications. The SoC is fabricated in a 28nm high-x metal-gate CMOS, and has a die size of 89mm2. Cu pillars are used for the die-to-substrate interface with fine substrate trace pitch. The SoC is packaged in a 14mmx14mm, 832 ball, 0.4mm pitch BGA. An integrated cellular modem supports rei. 9, cat. 4 LTE (FDD and TDD), while additional cellular and RF connectivity includes DC-HSPA+, TD-SCDMA, EDGE, 802.11ac, Bluetooth LE, multi-GNSS (GPS, GLONASS, Beidou, Galileo & QZSS), and ANT+. Multimedia features are highlighted by a high-performance Power-VR Series6 GPU, support for WQXGA displays (2560×1600), a 20Mpixel image processor and camera interface, and ultra-HD video playback support for H.264 and VP9.	ball grid array;beidou navigation satellite system;bluetooth;cmos;camera interface;central processing unit;compaq lte;die (integrated circuit);glonass;galileo (satellite navigation);global positioning system;graphics display resolution;graphics processing unit;h.264/mpeg-4 avc;image processor;low-power broadcasting;mobile app;mobile broadband modem;multi-core processor;processor design;quasi-zenith satellite system;radio frequency;rei toei;satellite navigation;smartphone;system on a chip;test-driven development;vp9	Hugh Mair;Gordon Gammie;Alice Wang;Sumanth Gururajarao;Ichiro Lin;HsinChen Chen;Wuan Kuo;Anand Rajagopalan;Wei-Zheng Ge;Rolf Lagerquist;Syed Rahman;C. J. Chung;Simon Wang;Lee-Kee Wong;Yi-Chang Zhuang;Kent Li;Junjie Wang;Minh Chau;Yijing Liu;Daniel Dia;Mark Peng;Uming Ko	2015	2015 IEEE International Solid-State Circuits Conference - (ISSCC) Digest of Technical Papers	10.1109/ISSCC.2015.7063107	system on a chip;embedded system;electronic engineering;real-time computing;logic gate;telecommunications;network switch;computer science;engineering;electrical engineering;operating system;silicon	Visualization	13.577937470320373	57.175570143818156	24826
beefe43c269232a61aef318c7ae88a986751b526	fpgas in data centers		FPGAs are slowly leaving the niche space they have occupied for decades.	field-programmable gate array;niche blogging	Gustavo Alonso	2018	ACM Queue	10.1145/3212477.3231573	computer engineering;computer security;field-programmable gate array;computer science	Theory	10.752674437980321	57.30847580862178	24827
c066f0178d96baf87aecd729c3d19425266c7151	a high performance carry-save to signed-digit recoder for fused addition-multiplication	carry logic;signed digit;codecs;multiplying circuits;multiplication operator;adders;signal processing;carry logic codecs multiplying circuits adders;proceedings paper;area time product high performance carry save to signed digit recoder fused addition multiplication cs2sd recoder signal processing;delay encoding filtering convolution routing timing;high performance	In this paper, we present a carry-save to signed-digit (CS2SD) recoder with high performance and small area. The recoder can be employed in common fused addition-multiplication operation which is widely used in various signal-processing applications. Our design shows 18% area-time product improvement over the traditional designs.		Wen-Chang Yeh;Chein-Wei Jen	2000		10.1109/ICASSP.2000.860095	arithmetic;multiplication operator;computer vision;codec;computer hardware;computer science;signal processing;mathematics;adder;statistics	Vision	12.20364674777869	43.22015499943964	24886
0c901c1e7cecd61bb01dafdf3a907ec74a6f3b8c	modular model of a logic circuit using object-oriented programming	logic circuits;object oriented programming;object oriented programming c language circuit simulation discrete event simulation logic circuits;process design;event driven algorithm logic circuit object oriented programming c objects;computer architecture;circuit simulation;c language;computational modeling;integrated circuit interconnections;event driven algorithm;c objects;logic circuit;object oriented modeling;object oriented modeling logic circuits object oriented programming hardware circuit simulation discrete event simulation computational modeling integrated circuit interconnections process design computer architecture;hardware;discrete event simulation	This paper presents a case study on a logic circuit model using object-oriented programming. For this purpose a modular model is used, which maps the structure of a logic circuit. Model is created as a set of C++ objects. A module in the logic circuit structure has the corresponding object in the model. The C++ objects also implement an event-driven algorithm of simulation	algorithm;c++;compiler;diagram;electronic circuit simulation;event-driven programming;executable;logic gate;map;schematic;vhdl	Stefan Senczyna	2006	Third International Conference on Information Technology: New Generations (ITNG'06)	10.1109/ITNG.2006.96	boolean circuit;computer architecture;logic synthesis;logic optimization;diode–transistor logic;state;asynchronous circuit;logic gate;logic family;computer science;theoretical computer science;sequential logic;signature;hardware description language;circuit extraction;programming language;register-transfer level;resistor–transistor logic	EDA	8.0123839513706	51.520440960681036	24925
5ddd5e7ea966affb87f34c3156872ecc8d9b72b6	rst flip-flop input equations	flip flop	There are several different usable combinations of the inputs of an RST flip-flop. It is shown how all of the possible combinations can be displayed simultaneously on three Karnaugh maps, facilitating the choice of the simplest input equations. The application equation for flip-flop Q characterized by a sequential problem is plotted on a map designated Q n+1 . Additional maps, (Q n+1 ) * and (Q n+1 )? are derived from Q n+1 . Cells corresponding to prime implicants not containing the variable Q are identified on these maps, and are used to enter the properly designated arbitrary elements on the R, S, and T maps of flip-flop Q. The method is based on the following theorem: ``If Q n+1 = (g 1 Q + g 2 Q?) n , and if F is the set of all prime implicants that do not contain the literals Q or ?, then the Boolean function g 1 g 2 is the union of all the prime implicants of Q n+1 that belong to the set F.u0027u0027 A simple illustrative example is included.	flops;flip-flop (electronics);intel matrix raid	Sureshchander	1970	IEEE Transactions on Computers	10.1109/T-C.1970.222845	embedded system;computer science;mathematics;algorithm	Visualization	20.315833661886714	43.29504295870835	24935
a1ffc7ef45dbbe5e4fac825856d98e06614f7843	system level simulation guided approach to improve the efficacy of clock-gating	relative power reduction model system level simulation guided approach clock gating dynamic power consumption hardware design register bank power analysis rtl high level synthesis hls based design flow system level simulation system level design;observability;analytical models;debugging;protocols;power analysis;intellectual property;system level simulation;clocks;design flow;register bank;low power electronics circuit simulation clocks high level synthesis integrated circuit design;clock gating;dynamic power consumption;system on a chip;embedded system;high level synthesis;integrated circuit design;circuit simulation;logic gates;estimation;registers;levels of abstraction;low power electronics;system level design;manufacturing;system level simulation guided approach;mathematical model;hardware design;clocks debugging hardware system on a chip manufacturing embedded system protocols observability intellectual property frequency;power reduction;relative power reduction model;power consumption;frequency;hls based design flow;rtl;transaction level;hardware	Clock-gating is a well known technique to reduce dynamic power consumption of a hardware design. In any clock-gating based power reduction flow, automatic selection of appropriate registers and/or register banks is extremely time-consuming because power analysis is performed at the RTL or lower level. In a high-level synthesis (HLS) based design flow, to achieve faster design closure, one must be able to decide the appropriate set of registers to clock gate even before generating RTL. System-level simulations are known to provide faster simulation, yet there is no solution, which utilizes systemlevel simulation to provide guidance to HLS to create clock-gated RTL. Since predicting power reduction at higher levels of abstraction is difficult due to the dependence of power on physical details, an accurate and efficient relative power reduction model is required. In this paper, we propose a novel system-level design methodology, which utilizes a ‘relative power reduction model’ that can help in predicting the impact of clock-gating on each register/bank quickly and accurately, by simulating the design at a cycle accurate transaction-level. As a result, our approach can automatically find the appropriate registers to clock-gate, guided by the system-level simulation.	benchmark (computing);clock gating;clock signal;design closure;electronic system-level design and verification;experiment;high- and low-level;high-level synthesis;level design;operand isolation;principle of abstraction;processor register;sequential logic;system-level simulation;what if	Sumit Ahuja;Wei Zhang;Sandeep K. Shukla	2010	2010 IEEE International High Level Design Validation and Test Workshop (HLDVT)	10.1109/HLDVT.2010.5496669	system on a chip;embedded system;communications protocol;estimation;real-time computing;observability;power analysis;logic gate;computer science;design flow;operating system;frequency;mathematical model;processor register;electronic system-level design and verification;manufacturing;high-level synthesis;clock gating;debugging;intellectual property;low-power electronics;integrated circuit design	EDA	15.050254994192	54.56705523254486	24969
6daf5334bd8550f31e6c763471df109c559041e7	multiple scan chain design for two-pattern testing	design for testability;core based test;circuit testing circuit faults sequential analysis combinational circuits delay sequential circuits computer science application software computer architecture integrated circuit technology;fault diagnosis boundary scan testing automatic test pattern generation design for testability sequential circuits integrated circuit testing;area overhead constraints multiple scan chain design two pattern testing fully automated approach chain based architecture sequential circuit test time;automatic test pattern generation;sequential circuits;boundary scan testing;scan chain insertion;delay testing;integrated circuit testing;design for test;fault model;fault diagnosis	Non-standard fault models often require the application of true-pattern testing. A fully-automated approach for generating a multiple scan chain-based architecture is presented so that two-pattern test sets generated for the combinational core can be applied to the sequential circuit. Test time and area overhead constraints are considered.	apple multiple scan 14 display	Ilia Polian;Bernd Becker	2001		10.1109/VTS.2001.923423	computer architecture;electronic engineering;scan chain;real-time computing;boundary scan;fault coverage;engineering;stuck-at fault;automatic test pattern generation;test compression;design for testing	HCI	20.791440640129792	51.775791238923794	24970
e1127c7fe6bf87553e02a87503e68797d9e5424f	two dimensional maximal elements problem on a reconfigurable optical pipelined bus system	maximal dominance;parallel algorithm;maximal element;computational geometry;reconfigurable pipelined optical bus system	In this paper, we present an efficient parallel algorithm for finding a set o f maximal elements on the xy-plane, which is based fin a linear array processors with optical pipelined bus system(LAPd°BS). Our parallel algorithm for solving the two dimensional maximal elements problem on the LARPBS model runs in O(?og n) time using O(n) number o f processors, where n is the number o f elements on the x-yplane.	central processing unit;maximal set;parallel algorithm	Haklin Kimm	1998		10.1145/330560.331001	maximal element;parallel computing;real-time computing;computational geometry;computer science;distributed computing;parallel algorithm	EDA	12.993115092601457	33.28094899448171	25010
274b297ef89cef84ae859413f5ebd11f6d8d4c72	a massively parallel, micro-grained vlsi architecture	plugs;edge detection;very large scale integration;prototypes;systolic array;wires;vlsi design;network topology;computer architecture;design and implementation;integrated circuit interconnections;signal and image processing;hypercubes;arithmetic;computer science;very large scale integration computer architecture hypercubes integrated circuit interconnections plugs arithmetic wires network topology computer science prototypes;connected component;high performance;vlsi architecture	T h i s paper describes the design and imp lemen ta tion,al aspects of a high performance micro-grained architecture with enhanced communica t ion capability. T h i s architecture i s capable of teraops performance. T h e architecture i s organized as a semi-systol ic array of processors. A prototyping s y s t e m f o r the architecture which will provide control, I /O, and a n interface t o a host s y s t e m f o r the micro-grained architecture i s specified. Beyond the research outlined here, t he prototyping s y s t e m can be used as a ‘?est-bed” f o r vari ous clnss/siirdent VLSI design projects. T h i s architecture will be useful f o r solaing a n u m b e r of import a n t problems, such as: edge detection, locating connected components , two-d imens iona l signal and image processing, sorting e l emen t s , and performing e l emen t p e r m u t a t i o 71.s.	central processing unit;connected component (graph theory);edge detection;flops;image processing;semiconductor industry;sorting;very-large-scale integration	Raminder Singh Bajwa;Robert Michael Owens;Mary Jane Irwin	1993		10.1109/ICVD.1993.669690	dataflow architecture;multilayered architecture;enterprise architecture framework;reference architecture;embedded system;space-based architecture;computer architecture;electronic engineering;parallel computing;database-centric architecture;image processing;computer science;applications architecture;cellular architecture;electrical engineering;hardware architecture;very-large-scale integration;data architecture;systems architecture;computer engineering	Robotics	5.6852456372568385	49.677731455383935	25031
a3d115e357d5bdbeddeca6e5859a5f381ad435fc	gate array layout automation: a tutorial	software metrics;design automation;abstract data types;programming abstraction;physical design;evaluation metric;data types	Gate array layout is becoming an increasingly important topic in design automation, as utilization of these components expands. In this tutorial a model of gate array physical design is first presented, introducing nomenclature used thereafter. The architecture of a typical gate array layout system is briefly discussed, establishing the environmental setting for more detailed examination of placement and routing techniques.  The placement problem is next defined, along with evaluation metrics and practical considerations related to placement. Constructive initial placement techniques based on cluster growth and mincut algorithms are next presented in some detail. Iterative improvement algorithms based on pairwise exchange, force-relaxation and unconnected set relocation are then presented. Coverage of gate array placement concludes with a comparison of results achieved with these methods, a brief exploration of the proper role of graphics placement, and enumeration of some promising topics for research in placement.	algorithm;automation;evaluation function;field-programmable gate array;graphics;iterative method;linear programming relaxation;max-flow min-cut theorem;minimum cut;physical design (electronics);place and route;relocation (computing);routing	Robert J. Smith	1981		10.1145/800175.809872	real-time computing;computer science;theoretical computer science;engineering drawing;placement	EDA	11.382849971371606	50.51307383505905	25131
7eda153bc3d347b206df1fa15ee007b23e99c1d8	silago-cog: coarse-grained grid-based design for near tape-out power estimation accuracy at high level		It is well known that ASICs have orders of magnitude higher power efficiency than general propose processors. However, due to the high engineering and manufacturing cost only handful of companies can afford to design ASICs. To reduce this cost numerous high-level synthesis tools have emerged since last 2-3 decades. In spite of these tools, ASIC design is still considered expensive because they fail to accurately predict the cost metrics. The inaccuracy is costly as it results in multiple iterations between RTL, logic synthesis, and physical design. The major reason behind this inaccuracy, at high level, is unavailability of information like wiring, orientation, and placement of hardware blocks. To tackle this issue, recent works have proposed to raise the abstraction of the physical design from standard cells to micro-architectural blocks physically organized in a structured grid based layout scheme. While these works have been successful in accurately predicting area and timing, to the best of our knowledge their effectiveness in accurately estimating power is yet to be determined. SiLago-CoG provides an efficient technique to characterize these blocks and estimate power at high level. Simulation and synthesis results reveal that SiLago-CoG provides up to 15X better power estimates in 680X less time at the cost of up to 50% additional area, compared to state-of-the-art.	algorithm;application-specific integrated circuit;central processing unit;cog (project);high- and low-level;high-level programming language;high-level synthesis;iteration;logic synthesis;performance per watt;physical design (electronics);power management;regular grid;semiconductor intellectual property core;simulation;tape-out;unavailability;wiring	Syed M. A. H. Jafri;Nasim Farahini;Ahmed Hemani	2017	2017 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)	10.1109/ISVLSI.2017.15	logic synthesis;unavailability;application-specific integrated circuit;real-time computing;physical design;common power format;register-transfer level;manufacturing cost;spite;computer science	EDA	21.08316859700033	57.945722326696426	25168
0ea8f85d507ab8220f920dc1ffe6574820dd0027	wattch: a framework for architectural-level power analysis and optimizations	microprocessors;power analysis;design process;microprocessor power dissipation;power estimation;architecture level;power efficiency;circuit design;design space;process design;chip;wattch;computer architecture;accuracy;circuit cad microprocessor chips power consumption computer architecture;permission;evaluation methodology;energy consumption;portable computers;power dissipation;circuits;circuit cad;power analysis tools;power consumption;power consumption power analysis power dissipation power performance tradeoffs power analysis tools wattch microprocessor power dissipation architecture level;energy consumption power dissipation microprocessors circuits process design portable computers cooling permission accuracy embedded computing;power performance tradeoffs;embedded computing;cooling;microprocessor chips	Power dissipation and thermal issues are increasingly significant in modern processors. As a result, it is crucial that power/performance tradeoffs be made more visible to chip architects and even compiler writers, in addition to circuit designers. Most existing power analysis tools achieve high accuracy by calculating power estimates for designs only after layout or floorplanning are complete. In addition to being available only late in the design process, such tools are often quite slow, which compounds the difficulty of running them for a large space of design possibilities. This paper presents Wattch, a framework for analyzing and optimizing microprocessor power dissipation at the architecture-level. Wattch is 1000X or more faster than existing layout-level power tools, and yet maintains accuracy within 10% of their estimates as verified using industry tools on leading-edge designs. This paper presents several validations of Wattch's accuracy. In addition, we present three examples that demonstrate how architects or compiler writers might use Wattch to evaluate power consumption in their design process. We see Wattch as a complement to existing lower-level tools; it allows architects to explore and cull the design space early on, using faster, higher-level tools. It also opens up the field of power-efficient computing to a wider range of researchers by providing a power evaluation methodology within the portable and familiar SimpleScalar framework.	cpu power dissipation;central processing unit;compiler;floorplan (microelectronics);microprocessor	David M. Brooks;Vivek Tiwari;Margaret Martonosi	2000		10.1145/339647.339657	chip;process design;embedded system;electronic circuit;computer architecture;parallel computing;real-time computing;power analysis;design process;electrical efficiency;computer science;dissipation;operating system;circuit design;accuracy and precision	Arch	2.908782537910725	54.879157362815526	25182
310a3a1e1a1583930addd4464ad134e39bd9ed40	c-graph: a highly efficient concurrent graph reachability query framework		Many big data analytics applications explore a set of related entities, which are naturally modeled as graph. However, graph processing is notorious for its performance challenges due to random data access patterns, especially for large data volumes. Solving these challenges is critical to the performance of industry-scale applications. In contrast to most prior works, which focus on accelerating a single graph processing task, in industrial practice we consider multiple graph processing tasks running concurrently, such as a group of queries issued simultaneously to the same graph. In this paper, we present an edge-set based graph traversal framework called C-Graph (i.e. Concurrent Graph), running on a distributed infrastructure, that achieves both high concurrency and efficiency for k-hop reachability queries. The proposed framework maintains global vertex states to facilitate graph traversals, and supports both synchronous and asynchronous communication. In this study, we decompose a set of graph processing tasks into local traversals and analyze their performance on C-Graph. More specifically, we optimize the organization of the physical edge-set and explore the shared subgraphs. We experimentally show that our proposed framework outperforms several baseline methods.	baseline (configuration management);big data;cloud computing;concurrency (computer science);data access;entity;experiment;graph (abstract data type);graph traversal;mathematical optimization;random-access memory;randomness;reachability;requirement;server (computing);tree traversal	Li Zhou;Ren Chen;Yinglong Xia;Radu Teodorescu	2018		10.1145/3225058.3225136	big data;distributed computing;vertex (geometry);concurrency;computer science;graph traversal;asynchronous communication;data access;reachability;graph	DB	-3.284979791109783	42.993113472652986	25187
3c9ba3b7c1488f40298e78b5ac3b67a6ce0d2b19	an algorithm for exploiting modeling error statistics to enable robust analog optimization	error uncertainty set increase;novel optimization strategy;equation-based optimization;robust analog optimization;robust optimization formulation;method yields solution;fitting error statistic;modeling error statistic;true feasible solution;superior solution point;feasible solution;accurate gp model;operational amplifiers;geometric programming;mathematical model;optimization;robust optimization;voltage controlled oscillator;cmos integrated circuits;transistor models;operant conditioning;robustness;uncertainty;transistors;model error;analog circuits;equivalence checking	Equation-based optimization using geometric programming (GP) for automated synthesis of analog circuits has recently gained broader adoption. A major outstanding challenge is the inaccuracy resulting from fitting the complex behavior of scaled transistors to posynomial functions. Fitting over a large region can be grossly inaccurate, and in fact, poor posynomial fit can lead to failure to find a true feasible solution. On the other hand, fitting over smaller regions and then selecting the best region, incurs exponential complexity.  In this paper, we advance a novel optimization strategy that circumvents these dueling problems in the following manner: by explicitly handling the error of the model in the course of optimization, we find a potentially suboptimal, but feasible solution. This solution subsequently guides a range-refinement process of our transistor models, allowing us to reduce the range of operating conditions and dimensions, and hence obtain far more accurate GP models.  The key contribution is in using the available oracle (SPICE simulations) to identify solutions that are feasible with respect to the accurate behavior rather than the fitted model. The key innovation is the explicit link between the fitting error statistics and the rate of the error uncertainty set increase, which we use in a robust optimization formulation to find feasible solutions.  We demonstrate the effectiveness of our algorithm on a two benchmarks: a two-stage CMOS operational amplifier and a voltage controlled oscillator designed in TSMC 0.18μm CMOS technology. Our algorithm is able to identify superior solution points producing uniformly better power and area values under gain constraint with improvements of up to 50% in power and 10% in area for the amplifier design. We also demonstrate that when utilizing the models with the same level of modeling error, our method yields solutions that meet the constraints while the violations for the standard method were as high as 45% and larger than 15% for several constraints.	algorithm;analogue electronics;cmos;curve fitting;geometric programming;mathematical optimization;operational amplifier;posynomial;refinement (computing);robust optimization;spice;simulation;time complexity;transistor model;voltage-controlled oscillator	Ashish Kumar Singh;Mario Lok;Kareem Ragab;Constantine Caramanis;Michael Orshansky	2010	2010 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)		operational amplifier;control engineering;mathematical optimization;electronic engineering;robust optimization;transistor model;geometric programming;uncertainty;voltage-controlled oscillator;analogue electronics;computer science;electrical engineering;operant conditioning;errors-in-variables models;mathematical model;formal equivalence checking;mathematics;cmos;transistor;statistics;robustness	EDA	24.011614543953712	57.776171633130836	25193
fc445ee263c97256c2461a4b9340e41dd0458a23	a novel substrate-triggered esd protection structure for a bus switch ic with on-chip substrate-pump	power supplies;ic test;cmos integrated circuits;mos devices;pins;i o to i o esd zapping;bus switch ic;variable structure systems;chip;electrostatic discharge;protection;hbm esd zapping;integrated circuit testing;on chip substrate pump;robustness;circuits;switches electrostatic discharge protection mos devices power supplies pins clamps robustness variable structure systems circuits;substrates;integrated circuit reliability;i o to i o esd zapping substrate triggered esd protection bus switch ic on chip substrate pump hbm esd zapping ic test;switches;substrate triggered esd protection;clamps;cmos integrated circuits electrostatic discharge integrated circuit reliability integrated circuit testing substrates	Experimental results show ESD level will become worse if more I/O pins are connected to the ground during I/O to I/O ESD zapping for the substrate pumped bus switch IC. As a result, it will fail +1KV during I/O to all other I/O HBM ESD zapping configurations. A new substrate-triggered ESD protection structure is proposed to increase the ESD robustness of this special bus switch product. The test results show the IC with this new ESD protection structure can pass the +3kV HBM ESD test.	cmos;clamper (electronics);eos;electron;high bandwidth memory;input/output;nmos logic;source-to-source compiler;value-driven design;very-large-scale integration	Paul C. F. Tong;Ping-Ping Xu;Wensong Chen;John Hui;Patty Z. Q. Liu	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1464806	chip;embedded system;electronic circuit;electronic engineering;electrostatic discharge;telecommunications;network switch;computer science;engineering;electrical engineering;cmos;robustness	EDA	18.950837303768235	56.39950519732719	25248
461cc9d3cf2ffc2540748f0ed13a4e2314945a87	boundary scan bist methodology for reconfigurable systems	built in self test impedance automatic testing system testing switches backplanes integrated circuit interconnections logic testing circuit testing driver circuits;reconfigurable system;reconfigurable architectures;boundary scan testing;response evaluation boundary scan bist methodology reconfigurable systems interconnect bist on line polling composite vectors encoded information test generation;built in self test;integrated circuit interconnections;integrated circuit testing;test generation;integrated circuit interconnections boundary scan testing built in self test reconfigurable architectures integrated circuit testing	The interconnect BIST is achieved by the on-line polling for the composite vectors that contain the encoded information for the test generation and response evaluation on selective drivers and receivers t o adapt t o the changing configuration.	boundary scan;built-in self-test;online and offline	Chauchin Su;Shung-Won Jeng;Yue-Tsang Chen	1998		10.1109/TEST.1998.743260	embedded system;computer architecture;electronic engineering;boundary scan;engineering	EDA	24.11854851074986	51.696360384076684	25269
637469f6789e8ec79af9041e4cb2c2b2e937566a	running on empty: getting work done on battery-free energy harvesting platforms	biomedical monitoring;debugging;sensors;biographies;energy harvesting;monitoring;energy storage	Summary form only given. Energy-autonomous computing and sensing devices have the potential to extend the reach of computing to a scale beyond either wired or battery-powered systems. This emerging class of battery-free devices harvests all their energy from the environment and typically operates off of a few microfarads of local energy storage. This poses a unique set of challenges for system architects and application developers as these energy constrained devices experience power intermittence which causes the system to reset and power-cycle unpredictably, tens to hundreds of times per second. Furthermore, workloads on energy harvesting devices are not only dictated by the available power but the order of execution is often not reversible due to the voltage dependencies of system peripherals. This talk will introduce several energy-autonomous computing platforms along with energy-interference-free debugging tools that allow workloads to be monitored in an unobtrusive manor.	autonomous robot;debugging;interference (communication);peripheral	Alanson P. Sample	2016	2016 IEEE International Symposium on Workload Characterization (IISWC)	10.1109/IISWC.2016.7581259	embedded system;parallel computing;real-time computing;computer hardware;computer science;sensor;operating system;debugging;energy storage;energy harvesting	Arch	-3.850206697081623	56.718011404212895	25288
0e23222c274f935f8724eed71526b6ab37ecf65e	efficient block-based medial axis transforms based on l∞ metric	medial axis transform;time algorithm;parallel algorithm;transforms image representation parallel algorithms;block based medial axis transform;linear array;parallel algorithms shape cybernetics image representation image reconstruction concurrent computing computer science pixel;image representation;forward chessboard distance transform problem;transforms;image representation block based medial axis transform l infin metric parallel algorithm time algorithm linear array reconfigurable pipelined bus system forward chessboard distance transform problem;reconfigurable pipelined bus system;distance transform;l infin metric;parallel algorithms	Almost all of the existing two-dimensional block-based medial axis transform (2D_BB_MAT) parallel algorithm are devised for solving the 2D_BB_MAT problem only and are very difficult to extend to a solution of the 3D_BB_MAT problem in parallel. In this paper, we propose efficient O(1) time algorithms for solving the hD_BB_MAT problems, where h is 2 or 3, on the linear array with a reconfigurable pipelined bus system (LARPBS) model. The proposed 2D_BB_MAT algorithm can easily extend to a solution of the 3D_BB_MAT problem in parallel by reducing the hD_BB_MAT problems to the forward chessboard distance transform problems. To the best of our knowledge, the proposed algorithms are the most efficient O(1) time algorithms compared with all the previously published algorithms known.	apache axis;medial graph	Yuh-Rau Wang	2006		10.1109/ICSMC.2006.384754	discrete mathematics;computer science;theoretical computer science;mathematics;parallel algorithm;algorithm	Robotics	11.777974640935943	35.530438070396336	25375
327ee8508675b3d657bb89fed879297a78f52ac5	"""""""challenges in testing"""""""		As technological advances in circuit design and manufacturing are made, testing procedures must accommodate diverse circuit description styles, large circuit sizes and new failure mechanisms. In this talk, we describe some of these challenges and how they are addressed in recent works in various areas of testing. In recent years, the formulations of testing problems have changed from “given a problem, find a solution” to “given a problem and quality measures, find a high-quality solution”. Quality guarantees provide ways to measure the distance between a given solution and an optimal solution. In addition they provide criteria for evaluating a new procedure that are more effective than for example comparison to previously proposed procedures. This is important when applying the procedures to very large circuits. Recent works address testing issues at increasingly higher levels of the design cycle and offer an integrated treatment of design and test. High-level failure models are considered as well as solutions that are completely independent of a failure model. At the same time, failure mechanisms at the transistor level gain importance as design features are reduced. We describe some of these works and the advantages of the two directions. Core or macro-based design consists of the incorporation of existing logic blocks (cores or macros) into a circuit. This design methodology reduces the time spent in designing the circuit. However, an embedded logic block may not be testable by a precomputed test set that ignores the environment where the block is embedded. Thus, macros-based design poses testing challenges that require existing testing methodologies to be reconsidered and new ones to be developed. Recently, testing techniques have been extended to the more general problems of design verification and validation. The approaches to hardware design verification and validation can be classified as simulation-based or formal methods. Simulation-based approaches are incomplete and do not guarantee the correctness of a design. Formal methods are not easily scalable to large circuits. + Research supported in part by NSF Grant No. MIP-9220549	circuit design;correctness (computer science);embedded system;failure cause;formal methods;ibm notes;logic block;precomputation;scalability;simulation;test set;transistor;verification and validation	Sudhakar M. Reddy	1996		10.1109/ATS.1996.10000		EDA	12.685150117302033	54.625089376720055	25390
0404409b9a35483666068760c8687f827b7a65d2	scaling, power and the future of cmos	cmos integrated circuits;power semiconductor devices;new technology;cmos technology;power efficiency;power semiconductor devices cmos integrated circuits integrated circuit design;chip performance;chip;nmos technology;integrated circuit design;cmos technology solid state circuits random access memory computer industry mos devices application software design optimization design methodology digital integrated circuits process design;design method;power optimized design methods;system design;power optimization;bipolars technology;bipolars technology cmos technology nmos technology chip performance power optimized design methods system design	"""In the mid 1980's the power growth that accompanied scaling forced the industry to focus on CMOS technology, and leave nMOS and bipolars for niche applications. Now 20 years later, CMOS technology is facing power issues of its own. After first reviewing the """"cause"""" of the problem, it will become clear that there are not easy solutions this time - no new technology or simple system/circuit change will rescue us. Power, and not number of devices is now the primary limiter of chip performance, and the need to create power efficient designs is changing how we do design. This talk will review power optimized design methods and shows how power is strongly tied to performance and that variability aversely effects power efficiency. Projecting forward, it shows that unless die size shrinks, in future technologies most of the devices will need to be idle most of the time which has strong ramifications for the both the underlying device and system design."""	cmos;die (integrated circuit);heart rate variability;image scaling;limiter;nmos logic;niche blogging;performance per watt;scalability;systems design	Mark Horowitz	2005	IEEE InternationalElectron Devices Meeting, 2005. IEDM Technical Digest.	10.1109/VLSID.2007.140	embedded system;electronic engineering;telecommunications;computer science;engineering;electrical engineering;cmos;computer engineering	Arch	12.083387021381007	56.94859756975907	25457
d4313832888557fe5dd5ac34244ee13d32c2ec70	a sparse symmetric indefinite direct solver for gpu architectures	multifrontal direct solver;bit compatibility;gpu;sparse linear systems;multifrontal;indefinite symmetric systems;ldl t factorization;direct solver	In recent years, there has been considerable interest in the potential for graphics processing units (GPUs) to speed up the performance of sparse direct linear solvers. Efforts have focused on symmetric positive-definite systems for which no pivoting is required, while little progress has been reported for the much harder indefinite case. We address this challenge by designing and developing a sparse symmetric indefinite solver SSIDS. This new library-quality LDLT factorization is designed for use on GPU architectures and incorporates threshold partial pivoting within a multifrontal approach. Both the factorize and the solve phases are performed using the GPU. Another important feature is that the solver produces bit-compatible results. Numerical results for indefinite problems arising from a range of practical applications demonstrate that, for large problems, SSIDS achieves performance improvements of up to a factor of 4.6 × compared with a state-of-the-art multifrontal solver on a multicore CPU.	central processing unit;cholesky decomposition;computer graphics;frontal solver;graphics processing unit;multi-core processor;numerical method;pivot element;rsa problem;sparse matrix	Jonathan D. Hogg;Evgueni E. Ovtchinnikov;Jennifer A. Scott	2016	ACM Trans. Math. Softw.	10.1145/2756548	mathematical optimization;parallel computing;computer science;theoretical computer science	Graphics	-2.3773148902183263	38.70791225240118	25518
aacfbbbf63a238fd8c32129e8655f0f41f1f0d3a	advances in multicore systems architectures		The ever increasing computation power demand and rapid advances in very large integration and communication technology have led to the development of highperformance multiand many-core computing systems. These systems enjoy parallelism at thread, instruction, task, and program levels. Moreover, in almost all cases, they have to meet tight power consumption requirements. Multicores are dominating all aspects of computing ranging frommobile devices to desktops and supercomputers. Some of the major areas of research and development are: on-chip networking, mapping and scheduling tasks, low-power design considerations, parallel programing tools, and multicore algorithms. This special issue presents recent research results on multicore systems; contributions cover different topics such as architecture, programming tools and applications, as well as other related aspects of multicore systems design. Contributed papers include some of the high-quality papers accepted at the CADS 2013 conference that took place in Tehran, which were selected based on the evaluation scores and presentation quality at the conference. We invited authors of these papers to extend and submit their work for this special issue. Additionally, we solicited high-quality papers from known researchers in the field as a result of our wide dis-	algorithm;computation;computer architecture;corpus-assisted discourse studies;low-power broadcasting;manycore processor;multi-core processor;parallel computing;programming tool;requirement;scheduling (computing);supercomputer;systems architecture;systems design	Hamid Sarbazi-Azad;Nader Bagherzadeh;G. Jaberipour	2015	The Journal of Supercomputing	10.1007/s11227-015-1487-8	distributed computing;architecture;computer science;thread (computing);computation;scheduling (computing);multi-core processor;systems design;ranging;information and communications technology	HPC	-2.030586894166748	46.18378974406923	25545
d7c9edd1f837f3a8fca132abc631a3442a982802	a generic collusion attack optimization strategy for traditional spread-spectrum and quantization index modulation fingerprinting	quantization index modulation;collusion attack;spread spectrum;digital fingerprinting;optimization	Collusion attack is known to be a cost-effective attack against digital fingerprinting. Under the constraint that the fingerprint embedding algorithm and the host signal are unavailable to the adversaries, it is not very easy to perform efficient collusion attack. To solve this problem, we propose a generic collusion attack optimization (GCAO) strategy for traditional spread-spectrum (SS) and quantization index modulation (QIM) fingerprinting. First, we analyze the differential signal of two fingerprinted copies of the same content generated by popular fingerprint embedding algorithms, including both traditional SS-based embedder and QIM-based embedder. Based on the analysis of the differential signal, we construct a fingerprint forensic detector that identifies which embedder is applied and what the embedding parameters are adopted. Then, we improve the earlier proposed SANO collusion attack. Two components outlined above constitute a generic collusion attack optimization strategy. The simulation results show that the proposed forensic detector provides trustworthy performance: the detector can correctly identify the fingerprint embedding algorithm, and the probabilities of estimating the fingerprint length and position are higher than 86 % and 87 % for SS-based embedding and 84 % and 80 % for QIM-based embedding, respectively. The further experimental results illustrate that the proposed GCAO attack provides a better tradeoff between the probability of being undetected and the quality of the attacked copy.	adversary (cryptography);algorithm;digital video fingerprinting;fingerprint (computing);frequency-hopping spread spectrum;mathematical optimization;modulation;simulation	Hui Feng;Langxiong Gan;Hefei Ling;Fuhao Zou;Zhengding Lu	2014	Multimedia Tools and Applications	10.1007/s11042-014-1948-8	telecommunications;computer science;internet privacy;spread spectrum;computer security	Security	8.207249515736391	33.57292070749679	25556
e096a273f76349a46166ae892ec2ea96aa296ad1	bti-induced aging under random stress waveforms: modeling, simulation and silicon validation	random stress waveform;silicon ageing cmos integrated circuits integrated circuit design integrated circuit modelling internal stresses mixed analogue digital integrated circuits negative bias temperature instability reaction diffusion systems;dvs random stress waveform bias temperature in stability reliability prediction;bias temperature in stability;stress integrated circuit modeling aging predictive models voltage control data models charge carrier processes;bias temperature instability;reliability prediction;si bias temperature instability bti induced aging stress waveforms silicon validation bti effect recovery phases aging prediction stress pattern dynamic voltage scaling dvs analog mixed signal designs compact modeling stress voltage reaction diffusion mechanisms rd mechanisms trapping detrapping mechanisms td mechanisms bti models aging analysis size 45 nm size 65 nm;dvs	The BTI effect, which consists of both stress and recovery phases, poses a unique challenge to long-term aging prediction, because the degradation rate strongly depends on the stress pattern. Previous approaches usually resort to an average, constant stress waveform to simplify the situation. They are efficient, but fail to capture the reality, especially under dynamic voltage scaling (DVS) or in analog/mixed signal designs where the stress waveform is much more random. This paper presents a suite of solutions that enable aging simulation under all possible stress conditions. Key contributions include: (1) Compact modeling of BTI when the stress voltage is varying. The results to both reaction-diffusion (RD) and trapping/detrapping (TD) mechanisms are derived. (2) Efficient simulation under DVS, leveraging the new BTI models; (3) Silicon validation at 45nm and 65nm, at both device and circuit levels. As the results illustrate, it is necessary to combine both RD and TD mechanisms to accurately predict aging under changing stress voltages. Our proposed work provides a general and comprehensive solution to aging analysis under random stress patterns.	btrieve;dynamic voltage scaling;elegant degradation;image scaling;macroscopic quantum self-trapping;mixed-signal integrated circuit;ruby document format;simulation;waveform	Ketul Sutaria;Athul Ramkumar;Rongjun Zhu;Renju Rajveev;Yao Ma;Yu Cao	2014	2014 51st ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2593069.2593101	electronic engineering;real-time computing;engineering;electrical engineering	EDA	21.063434100518986	58.76009485907774	25600
05681a66d5e4b8c10ab02d98be3ce2910e0a2323	physical design implementation of segmented buses to reduce communication energy	distributed memory;new technology;telecommunication network topology system buses ip networks;physical design;energy consumption wires costs network topology energy dissipation design optimization transistors communication networks switches communication switching;energy dissipation;embedded system;system buses;low power;energy optimization;community networks;energy consumption;macro block;netlist topology segmented buses communication energy reduction hard macro blocks network wires block ordering network energy optimization process ip block interconnection network wire energy dissipation activity aware floorplanning;ip networks;energy cost;memory hierarchy;telecommunication network topology	The amount of energy consumed for interconnecting the IP-blocks is increasing significantly due to the suboptimal scaling of long wires. To limit this energy penalty, segmented buses have gained interest in the architectural community. However, the netlist topology and the physical design stage significantly influence the final communication energy cost. We present in this paper an automated way to implement a netlist consisting of hard macro blocks, which are interconnected with heavily segmented buses in an energy optimal fashion for communication. We optimize the network wires energy dissipation in two separate, but related steps: minimizing the number of segments for active communication paths at the first step (block ordering), followed by the activity aware floorplanning step to minimize the physical length of these segments. Energy gains of up to a factor of 4 are achieved compared to a standard system implementation using a shared bus. Especially, the block ordering step contributes significantly to the network energy optimization process.	floorplan (microelectronics);ip camera;image scaling;mathematical optimization;monomial order;netlist;physical design (electronics)	Jin Guo;Antonis Papanikolaou;Paul Marchal;Francky Catthoor	2006	Asia and South Pacific Conference on Design Automation, 2006.	10.1145/1118299.1118311	physical design;embedded system;electronic engineering;real-time computing;distributed memory;computer science;engineering;dissipation;computer network	EDA	15.140406343876213	54.62814780498588	25604
84c44ccda4179e939d1c7ae9de2ed6cd245f2efc	a configurable heterogeneous multicore architecture with cellular neural network for real-time object recognition	hybrid parallelization strategy;configurable heterogeneous multicore architecture;tratamiento datos;feature level parallelism;complementary metal oxide semiconductor;data intensive image processing operation;dual mode linear processor array;object recognition;arquitectura red;deteccion blanco;parallel simd architecture;image processing;simd mimd cellular neural network multicore object recognition parallelism;systeme embarque;integrated circuit;complexite calcul;network on chip;network on chip platform;barreta lineal;linear array;modelo hibrido;cellular neural network;visual attention algorithm;procesamiento imagen;data processing;real time processing;metal oxide semiconductor technology configurable heterogeneous multicore architecture cellular neural network real time object recognition complex image processing task embedded system low power constraint dual mode linear processor array network on chip platform bioinspired attention computational complexity reduction visual attention algorithm salient image region selection dual mode parallel processor parallel simd architecture single instruction multiple data mode data intensive image processing operation pixel level parallelism feature level parallelism hybrid parallelization strategy cycle accurate architecture simulator;barrette lineaire;traitement donnee;circuito integrado;reconnaissance objet;pixel level parallelism;calculateur simd;real time systems cellular neural nets computational complexity embedded systems multiprocessing systems network on chip object recognition parallel architectures;cellular neural nets;architecture reseau;modele hybride;atencion visual;traitement image;system on a chip;embedded system;interconnection network;hybrid model;linear antenna arrays;paralelismo masivo;chip;single instruction multiple data;detection cible;algorithme;metal oxide semiconductor technology;reseau neuronal cellulaire;algorithm;tratamiento tiempo real;detection objet;computer architecture;embedded systems	As object recognition requires huge computation power to deal with complex image processing tasks, it is very challenging to meet real-time processing demands under low-power constraints for embedded systems. In this paper, a configurable heterogeneous multicore architecture with a dual-mode linear processor array and a cellular neural network on the network-on-chip platform is presented for real-time object recognition. The bio-inspired attention-based object recognition algorithm is devised to reduce computational complexity of the object recognition. The cellular neural network is utilized to accelerate the visual attention algorithm for selecting salient image regions rapidly. The dual-mode parallel processor is configured into single instruction, multiple data (SIMD) or multiple-instruction-multiple-data modes to perform data-intensive image processing operations while exploiting pixel-level and feature-level parallelisms required for the attention-based object recognition. The algorithm's hybrid parallelization strategy on the proposed architecture is adopted to obtain maximum performance improvement. The performance analysis results, using a cycle-accurate architecture simulator, show that the proposed architecture achieves a speedup of 2.8 times for the target algorithm over conventional massively parallel SIMD architecture at low hardware cost overhead. A prototype chip of the proposed architecture, fabricated in 0.13 mum complementary metal-oxide-semiconductor technology, achieves 22 frames/s real-time object recognition with less than 600 mW power consumption.	algorithm;analysis of algorithms;artificial neural network;british informatics olympiad;cmos;cellular neural network;computation;computational complexity theory;data-intensive computing;embedded system;image processing;low-power broadcasting;mimd;mobile phone;mobile robot;multi-core processor;network on a chip;outline of object recognition;overhead (computing);parallel computing;pixel;processor array;prototype;real-time clock;real-time transcription;simd;semiconductor;speedup	Kwanho Kim;Seungjin Lee;Joo-Young Kim;Minsu Kim;Hoi-Jun Yoo	2009	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2009.2031516	multi-core processor;embedded system;cellular neural network;parallel computing;real-time computing;computer science;cellular architecture;network on a chip	EDA	2.8858393177290993	47.050239646507265	25610
70ce410ec7a356de3124e2c5ef92b3836377126d	statistical defect-detection analysis of test sets using readily-available tester data	chip-detection capability;tester data;conventional method;readily-available tester data;statistical defect-detection analysis;actual ics;test quality;adaptive test metrics;confidence interval;statistical data analysis;test set;specially-generated test;statistical analysis;adaptive testing;chip;quality control;fault model	At substantial cost, conventional methods for evaluating test quality apply a specially-generated test set to a large population of manufactured chips. In contrast, a new time-efficient framework for evaluating test quality (FETQ) that uses tester data from normal production has been developed and validated. FETQ estimates the quality of both static and adaptive test metrics, where the latter guides test using the results of statistical data analysis. FETQ is innovative since instead of evaluating a single measure of effectiveness (e.g., number of unique defects detected), it provides a confidence interval of effectiveness based on the analysis of a collection of test sets. FETQ is demonstrated by measuring the chip-detection capability of several static and adaptive test metrics using tester data from actual ICs.	fault coverage;software bug;test set	Xiaochun Yu;R. D. Blanton	2011	2011 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)		chip;reliability engineering;quality control;electronic engineering;confidence interval;test set;test data;computer science;engineering;test compression;fault model;mathematics;test method;computerized adaptive testing;statistics;computer engineering	EDA	23.392554372738527	55.28893065772529	25664
9c4856e893e44bca20298b47181f9c1398ad69f6	thermal modelling of 3d multicore systems in a flip-chip package	silicon;thermal management packaging;thermal modelling;heat sink thermal resistance;3d multicore systems;difference operator;thermal performance;thermal resistance;flip chip devices;three dimensional;3d thermal model;single die system;heat sink;three dimensional displays;thermal resistance heat sinks three dimensional displays heat transfer silicon resistance heating;heat transfer;flip chip;thermal model;multiprocessing systems;three dimensional technology;heterogeneous integration;3d flip chip package;three dimensional integrated circuits flip chip devices multiprocessing systems thermal management packaging;heat sinks;three dimensional integrated circuits;resistance heating;single die system thermal modelling 3d multicore systems three dimensional technology heterogeneous integration 3d thermal model 3d flip chip package heat sink thermal resistance;steady state;transient heat transfer	Three-dimensional (3D) technology offers greater device integration, reduced signal delay and reduced interconnect power. It also provides greater design flexibility by allowing heterogeneous integration. In this work, a 3D thermal model of a multicore system is developed to investigate the effects of hotspot, and placement of silicon die layers, on the thermal performance of a modern flip-chip package. In this regard, both the steady-state and transient heat transfer analysis has been performed on the 3D flip-chip package. Two different thermal models were evaluated under different operating conditions. Through experimental simulations, we have found a model which has better thermal performance. The optimal placement solution is also provided based on the maximum temperature attained by the individual silicon dies. We have also provided the improvement that is required in the heat sink thermal resistance of a 3D system when compared to the single-die system.	academy;chip carrier;die (integrated circuit);flip chip;heat sink;java hotspot virtual machine;list of integrated circuit packaging types;multi-core processor;simulation;steady state;symmetric multiprocessing;system integration;thermal resistance	Kameswar Rao Vaddina;Tamoghna Mitra;Pasi Liljeberg;Juha Plosila	2010	23rd IEEE International SOC Conference	10.1109/SOCC.2010.5784700	embedded system;electronic engineering;electrical engineering;heat sink	EDA	22.48359532572448	60.14337247240806	25702
c155046b875549ba6f64774a28e44feddc985178	constructing small-signal equivalent impedances using ellipsoidal norms	vlsi computational geometry integrated circuit design matrix algebra;computational geometry;small signal equivalent impedance representation ellipsoidal norms vlsi designs logarithmic time computational geometry methods euclidean norm node pairs e approximate embedding matrix heuristic methods constraint criteria lower bound linear circuit elements model order reduction thermal analysis electromigration checks current rush analysis esd protection electrical overstress verification ir drop calculation;matrix algebra;integrated circuit design;impedance rlc circuits vectors approximation algorithms approximation methods laplace equations very large scale integration;vlsi	Analysis of VLSI designs and circuits often requires the construction of a small-signal equivalent impedance representation between prescribed node pairs. Examples include IR-drop calculation, electrical overstress verification, ESD protection, di/dt current rush analysis, electro-migration checks, thermal analysis and model order reduction. VLSI designs consisting of hundreds of millions (10e11) of linear circuit elements are now commonplace and thus any method which requires compute intensive calculations is difficult to apply for all the elements of the circuit. By definition, given a circuit with n elements, conventional analysis techniques have a Ω(η2) lower-bound to exhaustively enumerate all node-pairs which meet a constraint criteria. We present the first 0(nlgn + k) algorithm for answering queries of the form: ∃(x, y):((x, y) ϵ × n) ∧(Zeff (x, y) <; Z) where Zeff(x, y) is the equivalent impedance between nodes x and y, and k node-pairs meet the constraint. Calculating all node pairs for which these constraints are met is compute intensive using existing techniques, even using heuristic methods. In this paper a new technique based on method of projection using ellipsoidal norms is presented. Our proposed method employs recently discovered techniques from theoretical computer science to compute an e-approximate embedding matrix from which effective impedance of all node pairs can be estimated as easily as taking the Euclidean norm of column differences. Using computational geometry methods, existence queries can then be answered in logarithmic time. The method works for general circuits containing resistances, capacitances, and inductances.	approximation algorithm;characteristic impedance;computational geometry;data structure;enumerated type;euclidean distance;heuristic;linear circuit;model order reduction;small-signal model;theoretical computer science;time complexity;very-large-scale integration	Sandeep Koranne	2014	Fifteenth International Symposium on Quality Electronic Design	10.1109/ISQED.2014.6783369	embedded system;mathematical optimization;combinatorics;electronic engineering;computational geometry;computer science;engineering;electrical engineering;mathematics;very-large-scale integration;algorithm;statistics;integrated circuit design	EDA	17.413834613358738	49.417875649458104	25722
7774d6a3e946a5678d859780cc4babc6308395ca	tag compression for low power in dynamically customizable embedded processors	cache storage;technical activities guide tag energy dissipation embedded system microprocessors energy consumption microarchitecture minimization circuits application software hardware;reconfigurable architectures;energy dissipation;low power caches;very large scale integrated;embedded system;hot spot;embedded systems;data cache;low power;application specific integrated circuits;embedded processors;low power electronics;embedded systems microprocessor chips cache storage reconfigurable architectures application specific integrated circuits vlsi low power electronics;vlsi;application specific processors;power reduction;reconfigurable caches;embedded processor;reconfigurable caches tag compression low power embedded processors power reduction instruction data cache tag compression code data memory layouts tag arrays cache conflict identification active bitlines sense amps comparator cells hardware support very large scale integrated implementation energy dissipation cache subsystem application specific processors embedded systems low power caches;microprocessor chips;power minimization	We present a methodology for power reduction by instruction/data cache-tag compression for low-power embedded processors. By statically analyzing the code/data memory layouts for the application hot spots, a variety of proposed schemes for effective tag-size reduction can be employed for power minimization in instruction and data caches. The schemes rely on significantly reducing the number of tag bits stored in the tag arrays for cache-conflict identification, thus considerably decreasing the number of active bitlines, sense amps, and comparator cells. We present a set of tag compression techniques and evaluate each of them separately in terms of efficiency and required hardware support. A detailed very large scale integrated implementation has been performed and a number of experimental results on a set of embedded applications is reported for each technique. Energy dissipation decreases of up to 95% can be observed for the tag arrays, implying significant energy reductions in the range of 50% when amortized across the overall cache subsystem.	algorithm;amortized analysis;cpu cache;cache (computing);central processing unit;comparator;compile time;compiler;correctness (computer science);current sense amplifier;embedded system;experiment;low-power broadcasting;real life;tag cloud;tagged architecture	Peter Petrov;Alex Orailoglu	2004	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2004.829823	embedded system;computer architecture;parallel computing;tag ram;computer science;dissipation;operating system;application-specific integrated circuit;very-large-scale integration;hot spot;low-power electronics	EDA	-1.918701284202844	53.42679360828149	25733
07a91ffa1c0ee6ece25e57aa888632490a636044	a basic linear algebra compiler for embedded processors	linear algebra;best effort;optimization;embedded systems;linux;real time	Many applications in signal processing, control, and graphics on embedded devices require efficient linear algebra computations. On general-purpose computers, program generators have proven useful to produce such code, or important building blocks, automatically. An example is LGen, a compiler for basic linear algebra computations of fixed size. In this work, we extend LGen towards the embedded domain using as example targets Intel Atom, ARM Cortex-A8, ARM Cortex-A9, and ARM1176 (Raspberry Pi). To efficiently support these processors we introduce support for the NEON vector ISA and a methodology for domain-specific load/store optimizations. Our experimental evaluation shows that the new version of LGen produces code that performs in many cases considerably better than well-established, commercial and non-commercial libraries (Intel MKL and IPP), software generators (Eigen and ATLAS), and compilers (icc, gcc, and clang).	arm cortex-a8;arm cortex-a9;atlas;atom;central processing unit;clang;computation;computer;computer engineering;computer science;eigen (c++ library);embedded system;gnu compiler collection;general-purpose macro processor;general-purpose markup language;graphics;intel c++ compiler;intrinsic function;library (computing);linear algebra;math kernel library;raspberry pi 3 model b (latest version);signal processing	Nikolaos Kyrtatas;Daniele G. Spampinato;Markus Püschel	2015	2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)		best-effort delivery;embedded system;computer architecture;electronic engineering;parallel computing;real-time computing;computer science;operating system;programming language;algorithm	EDA	-0.2843615088323153	46.55386232345538	25753
7d605a195f61fc30f2d409899cd126390d342def	run-time monitoring of communication activities in a rapid prototyping environment	analytical models;debugging;communication architectures;instruments;run time monitoring;performance evaluation;software prototyping;prototypes;runtime environment;system monitoring;harmonic;computer architecture software prototyping real time systems performance evaluation system monitoring program debugging;embedded system;upper bound;embedded systems design;computer architecture;rapid prototyping environment;monitoring system;rapid prototyping;monitoring;embedded system design;architectural decisions;communication architectures run time monitoring communication activities rapid prototyping environment architectural decisions embedded systems design communication channels validation harmonic reconfigurable hardware monitoring system debugging performance evaluation design space exploration;runtime environment monitoring prototypes hardware instruments embedded system microelectronics upper bound delay analytical models;validation;communication activities;design space exploration;program debugging;microelectronics;reconfigurable hardware monitoring system;communication channels;reconfigurable hardware;hardware;real time systems	Architectural decisions in embedded systems design are often based on assumptions about properties of the communication channels. For validating them rapid prototyping combined with run-time monitoring plays a major role. In this paper we present HarMonIC1 – a reconfigurable hardware monitoring system for the run-time observation of communication channels. We will show how HarMonIC can be used to significantly improve debugging, performance evaluation, and design space exploration of communication architectures in embedded systems.	debugging;design space exploration;embedded system;emulator;field-programmable gate array;inter-process communication;performance evaluation;rapid prototyping;reconfigurability;system monitor;systems design	Andreas Kirschbaum;Jürgen Becker;Manfred Glesner	1998		10.1109/IWRSP.1998.676668	embedded system;system monitoring;real-time computing;reconfigurable computing;computer science;engineering;electrical engineering;harmonic;prototype;upper and lower bounds;programming language;debugging;microelectronics;computer engineering;channel	EDA	3.4970159613083314	54.169794280822245	25793
a4beaf747d9dab9aa6f5aea18a6e384827b1f047	fast and accurate architecture exploration for high performance and low energy vliw data-path				Ittetsu Taniguchi;Kohei Aoki;Hiroyuki Tomiyama;Praveen Raghavan;Francky Catthoor;Masahiro Fukui	2014	IEICE Transactions		computer architecture;parallel computing;real-time computing;computer science	Arch	5.193905481479963	49.996952447762034	25887
0b91ae4a40f7ecc40b4a16bee06afd4bd44cb673	package routability- and ir-drop-aware finger/pad assignment in chip-package co-design	package routability;chip-package co-design;ir-drop violation;chip core design;pad location;package routing;design interaction;ir-drop-aware finger;chip-package codesign;pad assignment;pad solution refinement;routing;printed circuits;planning;vhdl;electronics packaging;chip;integrated circuit design;simulation;chip scale packaging;noise reduction;flowcharts;pcb;signal integrity;hardware description language	Due to increasing complexity of design interactions between the chip, package and PCB, it is essential to consider them at the same time. Specifically the finger/pad locations affect the performance of the chip and the package significantly. In this paper, we have developed techniques in chip-package codesign to decide the locations of fingers/pads for package routability and signal integrity concerns in chip core design. Our finger/pad assignment is a two-step method: first we optimize the wire congestion problem in package routing, and then we try to minimize the IR-drop violation with finger/pad solution refinement. The experimental results are encouraging. Compared with the randomly optimized methods, our approaches reduce in average 42% and 68% of the maximum density in package and 10.61% of IR-drop for test circuits.	interaction;list of integrated circuit packaging types;network congestion;printed circuit board;randomness;refinement (computing);routing;signal integrity	Chao-Hung Lu;Hung-Ming Chen;Chien-Nan Jimmy Liu;Wen-Yu Shih	2009	2009 Design, Automation & Test in Europe Conference & Exhibition		embedded system;electronic engineering;computer hardware;computer science;engineering;printed circuit board	EDA	15.197205680873372	53.0315863631676	25961
f5521dcac4e07ab4cddde0323e58875653892b26	on synchronizing sequences and unspecified values in output responses of synchronous sequential circuits	sequences;sequential circuits;synchronizing sequences;synchronisation;synchronous sequential circuits;logic testing;logic testing sequential circuits sequences synchronisation fault diagnosis;fault diagnosis synchronizing sequences output responses synchronous sequential circuits unspecified values fault detection;fault detection and diagnosis;fault diagnosis;circuit faults fault diagnosis electrical fault detection sequential circuits fault detection circuit testing cities and towns	Unspecified values in output sequences of synchronous sequential circuits complicate fault detection and diagnosis. Special synchronizing sequences are proposed in this work to eliminate unspecified output values as early as possible. Their usefulness is demonstrated in fault diagnosis.	fault detection and isolation	Irith Pomeranz;Sudhakar M. Reddy	2000		10.1109/ICVD.2000.812639	synchronization;electronic engineering;real-time computing;telecommunications;computer science;stuck-at fault;sequence;sequential logic;algorithm	EDA	21.798199099312743	50.316478001739824	25984
6a811f7fe033cd4dddedb5b6a08638951619903a	high-resolution power profiling of gpu functions using low-resolution measurement	energy consumption;power value;current power consumption;high-resolution power profile;gpu function;power consumption;resulting power profile;power measurement;low-resolution measurement;power profile;short-term power change;high-resolution power;single power profile	In order to be able to minimise the energy consumption of an application program, information about the specific energy consumption is required. Modern Nvidia graphics processing units (GPUs) measure their current power consumption and the driver makes the value available to the application every 20 ms. However, for evaluating the energy consumption of GPU kernel functions, such a sampling interval might not be sufficient since the kernels may have a shorter execution time. This article proposes a method for generating high-resolution power profiles, which is the power consumption of a specific function depending on the progress of its execution. The method uses low-resolution measuring instruments offered by GPUs. Power measurements obtained during several executions of the function are combined into a single power profile. The resulting power profile contains power values in intervals which are much shorter than the sampling interval of the hardware driver so that even short-term power changes can be considered, e.g. for calculating the energy consumption of a single function. The article also shows how to extend the approach to an online generation of power profiles. Furthermore, an overview on the power profiles of some important functions, such as BLAS routines, is given.	blas;computer graphics;device driver;graphics processing unit;image resolution;industrial engineering;online and offline;overhead (computing);run time (program lifecycle phase);sampling (signal processing);self-tuning;simulation;unbalanced circuit	Jens Lang;Gudula Rünger	2013		10.1007/978-3-642-40047-6_80	parallel computing;real-time computing;operating system	HPC	-3.1280161938695565	55.55948991465495	26004
bb32aba8369dfccee8d90f92475cdcb373f10c83	wrapper design for multifrequency ip cores	desplazamiento frecuencia;multifrequency ip cores;appareillage essai;intellectual property core;frequency synchronization;integrated circuit;intellectual property;dissipation energie;logic design;clocks;implementation;testabilite;space exploration;circuito integrado;energy dissipation;wrapper core multifrequency system on a chip soc;indexing terms;system on a chip;testability;algorithme;tester channels;synchronisation;test response capture;algorithm;multiple clock domains;core;wrapper design;system on chip wrapper design multifrequency ip cores testability problems intellectual property cores multiple clock domains core wrapper test response capture capture windows embedded core tester channels internal scan chains tester bandwidth internal shift frequency power dissipation;sistema sobre pastilla;system on chip;deplacement frequence;aparato ensayo;power dissipation;logic testing;integrated circuit testing;testing equipment;synchronisation system on chip integrated circuit testing clocks;embedded core;bandwidth;testability problems;horloge;coeur propriete intellectuelle;tester bandwidth;internal shift frequency;testabilidad;intellectual property cores;disipacion energia;logic testing space exploration intellectual property clocks algorithm design and analysis logic design hardware frequency synchronization bandwidth power dissipation;internal scan chains;systeme sur puce;core wrapper;multiple clock domain;system on a chip soc;nucleo propiedad intelectual;implementacion;frequency shift;clock;algorithm design and analysis;capture windows;reloj;circuit integre;hardware;algoritmo;wrapper;multifrequency	This paper addresses the testability problems raised by intellectual property cores with multiple clock domains. The proposed solution is based on a novel core wrapper architecture and a new wrapper design algorithm. It is shown how multifrequency at-speed test response capture can be achieved via the design of capture windows without any structural modifications to the logic within the embedded core. The new features in the core wrapper architecture, which introduce limited hardware overhead, can also synchronize the external tester channels with the core's internal scan chains in the shift mode. Thus, the wrapper implementation space can be explored in order to efficiently utilize the available tester bandwidth while meeting the constraints on the maximum internal shift frequency that guarantees low testing time within the given power ratings. Using experimental data, the benefits of the proposed solution are demonstrated by analyzing the tradeoffs between the number of tester channels, testing time, area overhead, and power dissipation.	algorithm;cpu power dissipation;embedded system;microsoft windows;overhead (computing);semiconductor intellectual property core;wrapper function	Qiang Xu;Nicola Nicolici	2005	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2005.848811	system on a chip;embedded system;electronic engineering;real-time computing;telecommunications;computer science;engineering;electrical engineering;dissipation;operating system;algorithm	EDA	18.99063894463197	53.91813102377977	26010
650a7bf1f75568ed8cf45882800d4fabab8b57a0	statistical timing analysis using bounds and selective enumeration	statistical timing analysis;process variation;reliability;lower and upper bound;time complexity;clock network;performance;interconnect prediction;power supply noise;clock cycle prediction;random variable;algorithms;performance modeling;clock skew	The growing impact of within-die process variation has created the need for statistical timing analysis, where gate delays are modeled as random variables. Statistical timing analysis has traditionally suffered from exponential run time complexity with circuit size, due to the dependencies created by reconverging paths in the circuit. In this paper, we propose a new approach to statistical timing analysis which uses statistical bounds and selective enumeration to refine these bounds. First, we provide a formal definition of the statistical delay of a circuit and derive a statistical timing analysis method from this definition. Since this method for finding the exact statistical delay has exponential run time complexity with circuit size, we also propose a new method for computing statistical bounds which has linear run time complexity. We prove the correctness of the proposed bounds. Since we provide both a lower and upper bound on the true statistical delay, we can determine the quality of the bounds. If the computed bounds are not sufficiently close to each other, we propose the use of a heuristic to iteratively improve the bounds using selective enumeration of the sample space with additional run time. The proposed methods were implemented and tested on benchmark circuits. The results demonstrate that the proposed bounds have only a small error, which could be further reduced using selective enumeration with modest additional run time.	benchmark (computing);correctness (computer science);heuristic;ibm notes;microprocessor;monte carlo method;run time (program lifecycle phase);semiconductor research corporation;simulation;static timing analysis;statistical model;time complexity	Aseem Agarwal;David Blaauw;Vladimir Zolotov;Sarma B. K. Vrudhula	2002		10.1145/589411.589415	real-time computing;computer science;theoretical computer science;static timing analysis;statistics	EDA	22.811381886450075	57.766500655139765	26039
6d2b86e6b345ef2d70bc0b6fa5c087893bbf704e	energy model derivation for the dvfs automatic tuning plugin: tuning energy and power related tuning objectives		Energy consumption will become one of the dominant cost factors that will govern the next generation of large HPC centers. In this paper we present the Dynamic Voltage Frequency Scaling (DVFS) Plugin to automatically tune several energy related tuning objectives at a region-level of HPC applications. This plugin works with the Periscope Tuning Framework which provides an automatic tuning framework including analysis, experiment creation, and evaluation. The tuning actions are based on changes in the frequency via the DVFS. The tuning objectives include the tuning of energy consumption, total cost of ownership, energy delay product and power capping. The tuning is based on a model that relies on performance data and predicts energy consumption, time, and power consumption at different CPU frequencies. The derivation of the models for the DVFS plugin with the principal component analysis is included.	central processing unit;clock rate;dynamic voltage scaling;frequency capping;frequency scaling;plug-in (computing);principal component analysis;total cost of ownership;uninterruptible power supply	Carla Guillén;Carmen B. Navarrete;David Brayford;Wolfram Hesse;Matthias Brehm	2016	Computing	10.1007/s00607-016-0536-3	embedded system;real-time computing	HPC	-4.242912923176661	56.89365904673828	26049
f71393e48e37127d5cfde3b4b73fc1bd8451e889	fpga acceleration of the phylogenetic likelihood function for bayesian mcmc inference methods	software;field programmable gate array;multi core processor;paper;phylogeny;heterogeneous computing;large dataset;bayes theorem;graphics processor unit;biology;fpga;computational biology bioinformatics;chip;general purpose processor;computational science and engineering;likelihood functions;algorithms;floating point;combinatorial libraries;computer appl in life sciences;high performance;low power consumption;likelihood function;phylogenetic inference;genome sequence;microarrays;bioinformatics	Likelihood (ML)-based phylogenetic inference has become a popular method for estimating the evolutionary relationships among species based on genomic sequence data. This method is used in applications such as RAxML, GARLI, MrBayes, PAML, and PAUP. The Phylogenetic Likelihood Function (PLF) is an important kernel computation for this method. The PLF consists of a loop with no conditional behavior or dependencies between iterations. As such it contains a high potential for exploiting parallelism using micro-architectural techniques. In this paper, we describe a technique for mapping the PLF and supporting logic onto a Field Programmable Gate Array (FPGA)-based co-processor. By leveraging the FPGA's on-chip DSP modules and the high-bandwidth local memory attached to the FPGA, the resultant co-processor can accelerate ML-based methods and outperform state-of-the-art multi-core processors. We use the MrBayes 3 tool as a framework for designing our co-processor. For large datasets, we estimate that our accelerated MrBayes, if run on a current-generation FPGA, achieves a 10× speedup relative to software running on a state-of-the-art server-class microprocessor. The FPGA-based implementation achieves its performance by deeply pipelining the likelihood computations, performing multiple floating-point operations in parallel, and through a natural log approximation that is chosen specifically to leverage a deeply pipelined custom architecture. Heterogeneous computing, which combines general-purpose processors with special-purpose co-processors such as FPGAs and GPUs, is a promising approach for high-performance phylogeny inference as shown by the growing body of literature in this field. FPGAs in particular are well-suited for this task because of their low power consumption as compared to many-core processors and Graphics Processor Units (GPUs) [1].	approximation;central processing unit;computation (action);computational phylogenetics;coprocessor;digital signal processor;estimated;field-programmable gate array;float;general-purpose markup language;graphics processing unit;hl7publishingsubsection <operations>;heterogeneous computing;inference;iteration;likelihood functions;list of phylogenetics software;manycore processor;markov chain monte carlo;microprocessor;multi-core processor;numerous;parallel computing;phylogenetic analysis using parsimony *and other methods;pipeline (computing);protein array analysis;resultant;server (computing);speedup	Stephanie Zierke;Jason D. Bakos	2009		10.1186/1471-2105-11-184	parallel computing;real-time computing;computer science;bioinformatics;theoretical computer science;algorithm;field-programmable gate array;phylogenetics	HPC	-0.08919512379826063	43.94283200386262	26066
f4b0206976a14fbcaf98045c67862d85f5c9c50e	fault modeling and testing of gaas static random access memories	silicon;random access memory;static random access memory;circuit faults;process design;failure analysis;gallium arsenide;circuit testing;gallium arsenide circuit faults silicon circuit testing random access memory cryogenics temperature process design fault diagnosis failure analysis;temperature;fault model;fault diagnosis;cryogenics	Gallium Arsenide memories, which are now beginning to be used commercially, are subject to certain unusual parametric faults, not normally seen in silicon or other memory devices. This paper analyzes the causes of these parametric faults by first mapping the observed errors in the fabrication process to circuit behavior; these modified circuits are then shown to cause new types of pattern-sensitive faults and data retention problems. It is shown that by slightly modifying and reordering existing test procedures, all faults in these RAM’s can be adequately tesled.	random access	Sundarar Mohan;Pinaki Mazumder	1991		10.1109/TEST.1991.519731	process design;failure analysis;gallium arsenide;electronic engineering;real-time computing;static random-access memory;temperature;engineering;electrical engineering;stuck-at fault;fault model;silicon;cryogenics	Theory	22.68466587900787	54.08591632487326	26077
f23073a8dc690a34788bcf97de64768fa9967458	a configurable divider using digit recurrence	performance measure;divider circuits;logic design cmos logic circuits dividing circuits fixed point arithmetic floating point arithmetic;digital signal processing;fixed point operation;digit recurrence algorithms;elektroteknik och elektronik;measurement;floating point operation;cost function;logic design;clocks;division operation;power efficiency;space exploration;digital signal processing algorithms;design space;configurable divider;time factors;divider circuit;two s complement output quotient conversion;signal processing;cmos logic circuits;clocks signal processing algorithms hardware circuits space exploration time factors throughput signal processing cost function measurement;dividing circuits;dsp algorithms;on the fly;fixed point arithmetic;wordlength;floating point;circuits;efficient divider subblocks;floating point arithmetic;signal processing algorithms;quotient bits;modular architecture;hardware implementation;clock cycles per operation;throughput;hardware;two s complement output quotient conversion configurable divider digit recurrence algorithms division operation digital signal processing algorithms dsp algorithms hardware implementation divider circuit modular architecture efficient divider subblocks wordlength clock cycles per operation quotient bits fixed point operation floating point operation carry save arithmetic;carry save arithmetic	The division operation is essential in many digital signal processing algorithms. For a hardware implementation, the requirements and constraints on the divider circuit differ significantly with different applications. Therefore, it is not possible to design one divider component having optimal performance and cost for all target applications. Instead, the presented divider has a modular architecture, based on instantiation of small efficient divider subblocks. The configuration of the divider architecture is set by a number of parameters controlling wordlength, number of quotient bits, number of clock cycles per operation, and fixed or floating point operation. Digit recurrence algorithms with carry save arithmetic and on-the-fly two’s complement output quotient conversion are used to make the sub-blocks small, fast and power efficient. The modularity gives the designer freedom to elaborate different parameters to explore the design space. W O applications using the proposed divider are presented. Furthermore, an example divider circuit has been fabricated and performance measurements are included.	algorithm;clock signal;digital signal processing;flops;requirement;universal instantiation	Anders Berkeman;Viktor Öwall	2003		10.1109/ISCAS.2003.1206272	electronic engineering;real-time computing;computer science;floating point;electrical engineering;theoretical computer science;operating system;signal processing;mathematics;wilkinson power divider	EDA	12.209789692199344	46.008664614182166	26107
2c322733f632fa85d69ea9f9e1215949a3acddd0	accelerating the performance of stochastic encoding-based computations by sharing bits in consecutive bit streams	image coding;digital image processing;logic design;image coding streaming media encoding fault tolerance fault tolerant systems digital images energy consumption;stochastic computing;computer reliability;fault tolerant systems;streaming media;energy consumption;digital image processing computer reliability fault tolerance logic design stochastic computing;fault tolerance;encoding;digital images;image contrast stretching algorithm stochastic encoding based computation bit sharing probability random bit stream digital image processing algorithm	Stochastic encoding represents a value using the probability of ones in a random bit stream. Computation based on this encoding has good fault-tolerance and low hardware cost. However, one of its major issues is long processing time. We have to use a long enough bit stream to represent a value to guarantee that random fluctuations introduce only small errors to final computation results. For example, for most digital image processing algorithms, we need a 512-bit stream to represent an 8-bit pixel value stochastically to guarantee that the final computation error is less than 5%. To solve this issue, this paper proposes to share bits between adjacent bit streams to represent adjacent deterministic values. For example, in image processing applications, the bit stream which represents the current pixel value can share parts of the bits in the bit stream which represents the previous pixel value. We use an image contrast stretching algorithm to evaluate this method. Our experimental results show that the proposed methods can improve the performance by 90%.	8-bit;algorithm;bitstream;computation;decade (log scale);digital image processing;electronic circuit;fault tolerance;normalization (image processing);pixel;soft error;triple modular redundancy	Peng Li;David. J. Lilja	2013	2013 IEEE 24th International Conference on Application-Specific Systems, Architectures and Processors	10.1109/ASAP.2013.6567585	computer vision;fault tolerance;parallel computing;logic synthesis;real-time computing;bit plane;bit error rate;bit manipulation;computer science;theoretical computer science;digital image processing;bit field;digital image;encoding	Robotics	12.515216345389595	39.189259577183854	26118
fd3e42cb44bc41b209e7dc72ee517bbbf4f2a83a	zero aliasing compression based on groups of weakly independent outputs in circuits with high complexity for two fault models	functional properties;indexing terms;built in self test;error detection;combinational circuit;fault model	The goal of the presentation is the development of a suboptimal procedure for the solution of a high complexity problem, namely the minimal selection of the groups of weakly independent outputs for large combinational circuits. The knowledge about the groups of weakly independent outputs is usable to reduce both the necessary number of output check bits for the built-in self-test in the average more than 80% with respect to the zero aliasing and the gate area of a self-testing error detecting circuit. It is demonstrated the deductive relationship between the weak independence and the partially self-checking property of the accompanying subcircuit and the relationship between the partially selfchecking property and the groupability property. For the test of this structurally realized functional property in a circuit graph, reduction operations and distance operators for a given circuit graph were used. The results for stuck-at and stuck-open faults are discussed by means of the combinational ISCAS 85 benchmarks.	aliasing;fault model	Peter Böhlau	1994		10.1007/3-540-58426-9_137	electronic engineering;error detection and correction;index term;computer science;stuck-at fault;theoretical computer science;automatic test pattern generation;fault model;combinational logic;algorithm	EDA	21.764641758037772	49.10857471391704	26127
c97e0aae368beeba3b0c36e8dbda1ac272f8d43c	converting graphical dsp programs into memory constrained software prototypes	directed graphs;digital signal processing;target processor;digital signal processing software prototyping delay prototypes application software system on a chip partitioning algorithms rendering computer graphics software tools fires;memory requirements;programming environments;bottom up;recursive partitioning;software prototyping;application software;top down;prototypes;synchronous dataflow model;memory minimizing compilation techniques;prototyping environments;computer aided software engineering software prototyping digital signal processing chips signal processing directed graphs program compilers programming environments;system on a chip;chip;graphical programming;computer aided software engineering;data space requirements;signal processing;memory constrained software prototypes;dsp software prototyping system;digital signal processing chips;software tools;digital signal processing graphical dsp programs memory constrained software prototypes data space requirements on chip memory target processor memory minimizing compilation techniques dsp software prototyping system synchronous dataflow model acyclic graphs bottom up technique pairwise grouping of adjacent nodes top down technique generalized minimum cut operation prototyping environments memory requirements;pairwise grouping of adjacent nodes;acyclic graphs;synchronous dataflow;graphical dsp programs;bottom up technique;program compilers;rendering computer graphics;fires;performance optimization;on chip memory;top down technique;minimum cut;partitioning algorithms;generalized minimum cut operation	Since software prototypes of DSP applications are most efficient when their code and data space requirements can be accommodated entirely within the on-chip memory of the target processor, it is crucial to employ efficient memory-minimizing compilation techniques in a DSP software prototyping system. In this paper, we introduce two techniques for the combined minimization of code and data when compiling graphical programs that are based on the synchronous dataflow (SDF) model. The first method is a customization to acyclic graphs of a bottom-up technique, called Pairwise Grouping of Adjacent Nodes (PGAN), that was proposed earlier for general SDF graphs. We show that our customization significantly reduces the complexity of the general PGAN algorithm and performs optimally for a certain class of applications. The second approach is a top-down technique, called Recursive Partitioning by Minimum Cuts (RPMC), that is based on a generalized minimum cut operation. From an extensive experimental study, we conclude that RPMC and our customization of PGAN are complementary, and both should be incorporated into SDF-based prototyping environments in which the minimization of memory requirements is important.	algorithm;compiler;dataflow programming;dataspaces;digital signal processor;directed acyclic graph;experiment;minimum cut;recursion (computer science);requirement;software prototyping;top-down and bottom-up design;xslt/muenchian grouping	Shuvra S. Bhattacharyya;Praveen K. Murthy;Edward A. Lee	1995		10.1109/IWRSP.1995.518591	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;software engineering;signal processing;top-down and bottom-up design;programming language	EDA	0.6312088034855463	51.91762508844506	26176
d8236d560487ee9d06c6a7164c5fbc2bf57c1d55	on atpg for multiple aggressor crosstalk faults in presence of gate delays	multiple aggressor crosstalk faults;linear programming automatic test pattern generation circuit reliability crosstalk delays integer programming;crosstalk noise;crosstalk;automatic test pattern generation;pattern generation;automatic test pattern generation crosstalk circuit faults capacitance coupling circuits switches integrated circuit interconnections delay effects circuit testing peer to peer computing;circuit failure;integer programming;circuit reliability;integer linear program atpg multiple aggressor crosstalk faults gate delays circuit failure coupling capacitance pattern generation;linear programming;linear program;coupling capacitance;gate delays;integer linear program;delays;atpg	Crosstalk faults have emerged as a significant mechanism for circuit failure. Long signal nets are of particular concern because they tend to have a higher coupling capacitance to overall capacitance ratio. A typical long net also has multiple aggressors. In generating patterns to create maximal crosstalk noise on a net, it may not be possible to activate all aggressors logically or simultaneously. Therefore, pattern generation must focus on activating a maximal subset of aggressors switching around the same time the victim net switches. This is a well-known problem. In this paper, we present a novel solution assuming a unit delay model for the gates, combining 0-1 integer linear program (ILP) with traditional stuck-at fault ATPG. The maximal aggressor activation is formulated as a linear programming problem while the fault effect propagation is treated as an ATPG problem and the gate delays are subsumed by a circuit transformation. The proposed technique was applied to ISCAS 85 benchmark circuits. Results indicate that percentage of total capacitance that can be switched varies from 30-80%.	benchmark (computing);crosstalk;delay calculation;linear programming;maximal set;network switch;software propagation;stuck-at fault	Kunal P. Ganeshpure;Sandip Kundu	2007	2007 IEEE International Test Conference	10.1109/TEST.2007.4437563	embedded system;electronic engineering;real-time computing;computer science;linear programming;automatic test pattern generation;mathematics	EDA	18.424297973403295	52.21118033316683	26190
bf1e561b260a313a33fed0d8506267a471ca7eca	fast: fpga targeted rtl structure synthesis technique	logic arrays;field programmable gate arrays bipartite graph logic design input variables visualization;computation intensive process fast fpga rtl structure synthesis technique mapping techniques multi bit slice single bit slice clbs function decomposition disjunctive decomposition nondisjunctive decomposition;functional analysis logic arrays logic cad circuit cad vlsi;mapping rtl structures;functional analysis;functional decomposition;boolean network;vlsi;circuit cad;logic cad;article	In this paper we present an approach for mapping RTL structures onto FPGAs. This is in contrast to other FPGA mapping techniques which start from boolean networks. Each component part consists of single-hit or multi-bit slice of one or more closely connected RTL components and are realized using one or more CLBs. For Otis mapping onto CLBs, primarily function decomposition is employed. Conditions for some decompositions, disjunctive as well as non-disjunctive, useful in the FPGA context have been derived. As decomposition is a computation intensive process, some necessary conditions which are simple to check and eliminate a large percentage of trial partitions have been evolved.	bit slicing;boolean network;computation;disjunctive normal form;field-programmable gate array	A. R. Naseer;M. Balakrishnan;Anshul Kumar	1994		10.1109/ICVD.1994.282636	functional analysis;functional decomposition;computer architecture;electronic engineering;boolean network;computer science;theoretical computer science;very-large-scale integration;algorithm	EDA	14.018695180099053	47.15650590796182	26205
aa8fe9a4ac5e92300c3b1fdc589ebf0feffbdccf	a novel fixed-outline floorplanner with zero deadspace for hierarchical design	hierarchical design;aspect ratio;zero deadspace;fixed-outline constraint;strict ar constraint;novel fixed-outline floorplanner;previous fixed-outline floorplanners;total wirelength;basic idea;chip wirelength;soft module;sa process;sa iteration;mathematical model;binary trees;chip;aspect ratios;simulated annealing;quadratic equations;law;topology	Fixed-outline floorplanning, which enables hierarchical design, is considered more and more important nowadays. In this paper, a novel SA-based Fixed-outline Floorplanner with the Optimal Area utilization named SAFFOA is introduced to improve the total wirelength. The basic idea is to build and solve a group of four quadratic equations in four variables iteratively, which can handle the fixed-outline constraint of any aspect ratio. A new topological representation called Ordered Quadtree is then custom-made for this basic idea to facilitate its integration into SA iterations. After the fixed-outline constraint with 100% area utilization is achieved, we will solve the tradeoff between the chip area and wirelength and thus concentrate on the latter in SA process. Experimental results show that the chip wirelength is decreased by about 16.8% and 8.6% on average, compared with two previous fixed-outline floorplanners on soft modules, which are both proved to be better than Parquet. Besides, our method is still competitive on the wirelength, even if compared with some leading-edge outline-free floorplanners. At last, Local Refinement is also adopted to guide the SA process and reshape soft modules to meet the constraint on their aspect ratios (ARs). With its help, SAFFOA can still generate feasible floorplans with no deadspace under a strict AR constraint such as [0.5,2].	floorplan (microelectronics);iteration;quadratic equation;quadtree;refinement (computing)	Ou He;Sheqin Dong;Jinian Bian;Satoshi Goto;Chung-Kuan Cheng	2008	2008 IEEE/ACM International Conference on Computer-Aided Design	10.1145/1509456.1509473	mathematical optimization;aspect ratio;electronic engineering;telecommunications;computer science;mathematics;engineering drawing;algorithm;statistics	EDA	14.580147036211374	52.89325424644582	26261
5e77b1e781072ab8633b6c49f7c341a9cd6fcfe7	analysis of a novel full adder designed for implementing in carbone nanotube technology	carbon nanotube;mosfet;circuits	A novel low power-delay product full adder circuit is presented in this paper. A new approach is used in order to design full-swing full adder with low number of transistors. The proposed full adder is implemented in MOSFET-like Carbon nanotube technology and the layout is provided based on standard 32 nm technology from MOSIS. The simulation results using HSPICE show that, there are substantial improvements in both power and performance of the proposed circuit compared to latest designs. In addition, the proposed circuit has been implemented in conventional 32 nm process to estimate the advantages of using carbon-based transistors in digital designs over conventional silicon technology. The proposed circuit can be applied in ultra low power and very high speed applications.	adder (electronics);cmos;fan-out;mosis;power–delay product;programmed data processor;spice 2;simulation;transistor	Mahdiar Hosein Ghadiry;Mahdieh Nadi Senejani;Hosein Mohammadi;Asrulnizam Bin Abd Manaf	2012	Journal of Circuits, Systems, and Computers	10.1142/S0218126612500429	electronic circuit;electronic engineering;carbon nanotube;engineering;electrical engineering	EDA	16.58477872712807	57.469044752578135	26271
74050b42ed10b957ce96f07a9846a343337d3810	ternary logic network justification using transfer matrices	ternary logic matrix algebra;justification;vectors equations multivalued logic mathematical model logic gates switches computational modeling;multiple valued logic transfer function justification ternary logic;network transfer matrix ternary logic network justification matrix linear algebraic method vector matrix product calculation;matrix algebra;transfer function;ternary logic;multiple valued logic	"""A linear algebraic method is developed that allows for logic network justification problems to be solved. The method differs from previous techniques that require learning or solution space search techniques in that all possible justification solutions are determined through a single vector-matrix product calculation. The logic network is represented by a matrix that is defined as the """"justification"""" matrix. It is shown that the justification matrix is simply the transpose of the network transfer matrix and is thus easily obtained through a traversal of the network netlist. Example justification calculations are provided."""	feasible region;linear algebra;matrix multiplication;matrix representation;netlist;network planning and design;three-valued logic;transfer matrix;tree traversal	Mitchell A. Thornton;Jennifer Dworak	2013	2013 IEEE 43rd International Symposium on Multiple-Valued Logic	10.1109/ISMVL.2013.57	zeroth-order logic;discrete mathematics;logic optimization;algebraic sentence;intermediate logic;predicate functor logic;mathematics;signature;transfer function;state-transition matrix;substructural logic;second-order logic;algorithm	Arch	19.290486566014508	45.120547571606906	26297
2d214666d03cc8471577912d1b021a01d7e0664b	scalable task-oriented parallelism for structure based incomplete lu factorization	linear algebra;incomplete lu factorization;gigabit ethernet;parallel computer;gaussian elimination;grid computing;high performance;use case;sparse matrices	ILU(k) is an important preconditioner widely used in many linear al gebra solvers for sparse matrices. Unfortunately, there is still no highly scalable parallelILU(k) algorithm. This paper presents the first such scalable algorithm. For example, the new algor ithm achieves 50 times speedup with 80 nodes for general sparse matrices of dimension 160,000 that are diagonally dominant. The algorithm assumes that each node has sufficient memory to hold the matrix. The parallelism is task-oriented. We present experimental results for k = 1 andk = 2, which are the most commonly used cases in the practical applications. The results are presen t d for three platforms: a departmental cluster with Gigabit Ethernet; a high-performance cluster using an InfiniBand interconnect; and a simulation of a Grid computation with two or three participating sites.	algorithm;computation;diagonally dominant matrix;gigabit;grid computing;incomplete lu factorization;infiniband;parallel computing;preconditioner;qr decomposition;scalability;simulation;sparse matrix;speedup;the matrix	Xin Dong;Gene Cooperman	2008	CoRR		parallel computing;incomplete cholesky factorization;incomplete lu factorization;computer science;theoretical computer science;distributed computing	HPC	-2.720686752853874	38.598192755099795	26378
673094b71677525b2441ac715c5dd9bc4460c6fc	scalable n-worst algorithms for dynamic timing and activity analysis		As the overheads for ensuring the correctness of electronic designs continue to increase with continued technology scaling and increased variability, better-than-worst-case (BTWC) design has gained significant attention. Many BTWC design techniques utilize dynamic timing and activity information for design analysis and optimization. These techniques rely on path-based analysis that enumerates the exercised paths in a design as targets for analysis and optimization. However, path-based dynamic analysis techniques are not scalable and cannot be used to analyze full processors and full applications. On the other hand, graph-based techniques like those that form the foundational building blocks of electronic design automation tools are scalable and can efficiently analyze large designs. In this paper, we extend graph-based analysis to provide the fundamental dynamic analysis tools necessary for BTWC design, analysis, and optimization. Specifically, we present scalable graph-based techniques to report the N-worst exercised paths in a design for three metrics — timing criticality (slack), activity (toggle count), and activity subject to delay constraints. Compared to existing path-based techniques, our scalable dynamic analysis techniques improve average performance by 977 x, 163 x, and 113 x, respectively, and enable scalable analysis for a full processor design running full applications.	algorithm;best, worst and average case;cmos;central processing unit;correctness (computer science);electronic design automation;feature toggle;heart rate variability;image scaling;mathematical optimization;processor design;scalability;self-organized criticality;slack variable;spatial variability	Hari Cherupalli;John Sartori	2017	2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)	10.1109/ICCAD.2017.8203830	artificial neural network;computer science;real-time computing;scalability;correctness;overhead (business);processor design;multilayer perceptron;electronic design automation;graph	EDA	14.865395863827114	54.8048873132622	26455
cb4fc5a3b405e0749642679e1c19769debef7c43	a modular approach to motorola powerpc compilers	motorola powerpc compiler;modular approach	"""The need for balancehetween software and hardware is a well-known principle of RISC microprocessor design methodologies. In order to achieve a high level of performance, RISC microprocessors are designed to allow compilers to take full advantage of the pipelines and resources available. The PowerPC family of microprocessors is being designed to he used for many purposes, ranging from low-power embedded controllers to powerful, supercomputerclass multiprocessor systems. This diversity of uses will lead to an equally diverse set of operating system environments, including AIX, Macintosh OS, Solaris and Windows/NT, among others. Despite the multitude of PowerPC processor and system configurations being developed, there remains a need for highly optimizing compilers that utilize both the base PowerPC Architecture as well as other implementations of the chips and systems designed around the architecture. Motorola has developed a highly optimizing, modular compilation environment that can be quickly adapted to various PowerPC microprocessor and system configurations. This suite of C, C++ and Fortran compilers is designed to meet the following criteria: l Highly optimizing, ensuring opri-mal performance for PowerPC microprocessors l Highly retargetable, ensuring rapid time-to-market l Highly configurable, supporting multiple object file and debugging formats l Compliant to software standards, ensuring portability of code between chips This article will describe the modular structure of the compiler, the data and information flow through the major phases of the compiler, and offer some discussion on architecture and implementation-specific optimizations currently performed in the PowerPC compilers. Through examples and descriptions, an understanding can be achieved as to how the Motorola PowerPC compilers are designed to provide the high performance and diversity that is essential to the PowerPC Architectwe. The Motorola compilation system, based partially on technology acquired from Apogee Software for the 88000 architecture, consists of a series of components that collectively =a """" provide highly optimized PowerPC microprocessor code for a wide range of source languages, object file and debugging formats, and system environments. Conceptually, the heart of the Motorola compilation system is a common core that integrates multiple front ends with target-specific code generators to provide consistently high performance across an extremely wide range of target environments (see"""	aix;c++;debugging;embedded system;fortran;high-level programming language;low-power broadcasting;macintosh operating systems;microprocessor;microsoft windows;motorola 88000;multiprocessing;object file;operating system;optimizing compiler;pipeline (computing);powerpc g4;processor design;software portability;windows nt	Julie Shipnes;Mike Philip	1994	Commun. ACM	10.1145/175208.175215	computer architecture;parallel computing;powerpc;operating system;power architecture	Arch	0.058673901176041855	48.041784398345264	26476
f9340e57e9fbba9317b4eaf8bc967232df039e4a	efficient rns to binary converters for the new 4-moduli set {2n, 2n+1-1, 2n-1, 2n-1-1}	moduli sets;computer arithmetic;rns to binary reverse converter;residue number system rns	In this paper, we propose efficient designs of residue number system (RNS) to binary converter for the balanced moduli set {2n, 2n+1 − 1, 2n − 1, 2n−1 − 1} where n has even values. This new moduli set is completely free from modulo-(2k + 1)-type which results in high-speed modulo arithmetic channels for RNS. Also, mixed-radix conversion (MRC) algorithm is used to achieve both an arithmeticbased and reduced-complexity two-level RNS to binary converter architectures. The proposed moduli set provides fast arithmetic operation with higher speed of the reverse converter comparing to other five moduli set which is found in literature.	adder (electronics);algorithm;arithmetic logic unit;institute of electronics, information and communication engineers;level structure;modulo operation;residue number system	Mohammad Esmaeildoust;Keivan Navi;Mohammadreza Taheri;Amir Sabbagh Molahosseini;Siavash Khodambashi	2012	IEICE Electronic Express	10.1587/elex.9.1	arithmetic;electronic engineering;discrete mathematics;mathematics	EDA	15.9737048211456	43.89287443205401	26504
c97dd866eff99a259c090f6b10440863c47db445	discrete fourier transform processors using cordic	chirp;band pass filters;cordic;redundant arithmetic;very large scale integration;bit serial;frequency measurement;bit rate;dft processor;parallel architectures;digital filters;fourier transforms;discrete fourier transform;performance analysis;vlsi;fast fourier transforms;arithmetic;cost effectiveness;digital signal processing chips;digital arithmetic;discrete fourier transforms arithmetic very large scale integration band pass filters chirp performance analysis bit rate fast fourier transforms frequency measurement digital filters;pipeline processing computerised signal processing digital arithmetic digital signal processing chips fourier transforms parallel architectures;discrete fourier transforms;vlsi dsp cordic discrete fourier transform bit serial redundant arithmetic dft processor;computerised signal processing;pipeline processing;dsp	The author presents an analysis of the cost-effectiveness of discrete Fourier transform processors, based on CORDIC modules such as the bit-serial, parallel with non-redundant and redundant arithmetic, and pipelined. The performance of each processor is analyzed with respect to the time to process one frequency output and the number of modules required. It is shown that the CORDIC-based DFT processor is a prospective solution in VLSI to be used for a wide range of input bit rate. >	cordic;central processing unit;discrete fourier transform	Jeong-A Lee;Kiseon Kim	1991		10.1109/GLSV.1991.143976	electronic engineering;parallel computing;computer science;electrical engineering;theoretical computer science;discrete fourier transform;very-large-scale integration	Vision	12.342885233141224	44.10330629274373	26513
f3c9773c56c7a84b3c132efd7dd7d49d6336a86a	parity bit calculation and test signal compaction for bist applications	circuit faults;application software;compaction circuit testing built in self test test pattern generators circuit faults automatic testing application software explosions performance analysis very large scale integration;very large scale integration;automatic testing;design technique;built in self test;test pattern generators;compaction;performance analysis;explosions;circuit testing	Parity bit checking and pseudo-exhaustive testing are two design techniques which have been widely discussed in the BIST literature but have seldom been employed in practice because of the exponential nature of the processes involved. In this paper we describe several procedures designed to avoid these exponential explosions. Specifically we show how the parity of a large combinational function can (often) be quickly calculated. This is accomplished by an examination of the circuit realization itself particularly with regard to the connectivity between the various inputs and outputs. We then show how this same approach can be used to partition circuits so that they can be tested efficiently with a relatively small number of test patterns. Using these methods we were able to calculate the parity bits for more than 80% of ISCAS benchmark circuits' outputs. Interestingly enough, only 15% of these outputs were found to be parity-odd, but for these cases high fault coverage was invariably found to result. Several examples are included.	built-in self-test;data compaction;parity bit	Sungju Park;Sheldon B. Akers	1991		10.1109/TEST.1991.519769	compaction;embedded system;electronic engineering;application software;real-time computing;white-box testing;computer science;engineering;electrical engineering;automatic test pattern generation;operating system;test compression;very-large-scale integration;data-driven testing;algorithm	Arch	21.357137721340916	50.664560913162575	26561
531fc05dfa1f3009604d580c267ab8d5955158e7	the parallel implicit elimination (pie) method for the solution of linear systems	shared memory;parallel pivoting strategy;g 1 3;linear system;f 1 2;implicit matrix elimination;gaussian elimination;parallel implementation	In this paper the Gaussian Elimination and Implicit Matrix Elimination methods are compared. Timings on a shared memory computer confirm the superiority of the new method for both the sequential and parallel implementations.	system of linear equations	David J. Evans;Abdul Rahman Bin Abdullah	1994	Parallel Algorithms Appl.	10.1080/10637199408915461	shared memory;gaussian elimination;parallel computing;computer science;theoretical computer science;operating system;mathematics;linear system;algorithm;tridiagonal matrix algorithm	EDA	-2.4214940522636925	37.62735669633983	26592
50c0626d5b626cdaf6b5a2c37d4cbf5a21e82e6d	integrating physical level design and high level synthesis for simultaneous multi-cycle transient and multiple transient fault resiliency of application specific datapath processors	tecnologia electronica telecomunicaciones;tecnologias;grupo a	Article history: Received 29 November 2015 Received in revised form 2 March 2016 Accepted 4 March 2016 Available online xxxx Radiation induced faults in digital systems have started gatheringmajor attention in recent years due to increasing reliability concern for future technologies. For future technologies, multiple transient faults (MTF) originating from a single radiation hit are expected to occur more frequently. Further, due to continuous massive scaling in device geometry, a particlewithmoderate linear energy transfer (LET) values is expected to affectmore than one module/device during striking. Additionally, incessant escalation in operating speedwith evolution of technology has increased likelihood of multi-cycle transient (MCT) faults in digital circuits. This calls for novel solutions for concurrently tackling multi-cycle transient and multi-transient fault resiliency at a higher design abstraction level such as behavioral level. This paper proposes a novel approach for generating simultaneous multi-cycle transient and multiple transient fault resilient designs during high level synthesis (HLS) of application specific datapath processors using the framework of dual modular redundancy. Results of the proposed approach on benchmarks indicated generation of low cost MCT–MFT resilient designs during HLS within acceptable runtime. © 2016 Elsevier Ltd. All rights reserved.	abstraction layer;central processing unit;datapath;digital electronics;dual modular redundancy;high-level programming language;high-level synthesis;image scaling;level design;media foundation;mobile data terminal;privilege escalation	Deepak Kachave;Anirban Sengupta	2016	Microelectronics Reliability	10.1016/j.microrel.2016.03.006	embedded system;electronic engineering;real-time computing;engineering;electrical engineering	EDA	20.900122065391955	57.767368059547195	26613
56c0d001ed40e0d351461464614ba7dc62134d6c	high performance computing for hyperspectral image analysis: perspective and state-of-the-art	geophysical image processing;hyperspectral image analysis;field programmable gate array;cluster computing;gpus;image processing;high performance computing;computer graphic equipment;distributed computing;parallel processing computer graphic equipment field programmable gate arrays geophysical image processing;hyperspectral sensors;hyperspectral imaging high performance computing hyperspectral image analysis remotely sensed hyperspectral image processing algorithms multicomputer clusters heterogeneous networks specialized hardware architectures parallel hyperspectral processing chain;computer networks;hardware architecture;computer architecture;remote sensing;high performance computer;fpgas;fpgas hyperspectral imaging high performance computing cluster computing distributed computing hardware implementations multi cores gpus;specialized hardware architectures;graphic processing unit;clustering algorithms;parallel hyperspectral processing chain;multicomputer clusters;image analysis;hardware implementations;high performance computing hyperspectral imaging image analysis hyperspectral sensors image processing computer architecture field programmable gate arrays clustering algorithms computer networks hardware;field programmable gate arrays;hyperspectral imaging;heterogeneous networks;hyperspectral image;remotely sensed hyperspectral image processing algorithms;hardware implementation;parallel processing;multi cores;heterogeneous network;hardware	The main purpose of this paper is to describe available (HPC)-based implementations of remotely sensed hyperspectral image processing algorithms on multi-computer clusters, heterogeneous networks of computers, and specialized hardware architectures such as field programmable gate arrays (FPGAs) and graphic processing units (GPUs). Combined, the revision of existing techniques conducted in this paper, along with the description of performance results for a parallel hyperspectral processing chain on different architectures, delivers an excellent snapshot of the state-of-the-art in the area of HPC-based hyperspectral image processing and a thoughtful perspective of the potential and emerging challenges of applying HPC paradigms to hyperspectral imaging problems.	algorithm;computer cluster;field-programmable gate array;graphics processing unit;image analysis;image processing;snapshot (computer storage)	Antonio J. Plaza;Qian Du;Yang-Lang Chang	2009	2009 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2009.5417729	parallel processing;computer vision;parallel computing;image analysis;heterogeneous network;image processing;computer science;theoretical computer science;hyperspectral imaging;hardware architecture;field-programmable gate array;remote sensing	Arch	0.8421905555096629	39.19585000344663	26628
9e52fe3a28bdc266f4706843fc4aa57e50366225	on test and diagnostics of flash memories	flash memory;design for testability;industrial flash chips flash memories testing flash memory diagnosis nonvolatile on chip storage elements design for testability built in self test built in self repair failure analysis fpga;integrated circuit testing flash memories integrated memory circuits design for testability built in self test failure analysis;flash memory nonvolatile memory automatic testing system testing costs built in self test system on a chip random access memory algorithm design and analysis manufacturing;integrated memory circuits;chip;failure analysis;built in self test;integrated circuit testing;built in self repair;flash memories	Embedded flash memory has been widely used in applications that require non-volatile on-chip storage elements. However, test and diagnostics of flash memories needs further investigation so that the overall cost of the products can be reduced. This paper presents the challenges and issues for test and diagnostics of flash memories, based on our recent experiences. We also suggest improvement of the test and diagnosis flow, including design-for-testability (DFT) using built-in self-test (BIST), built-in self-repair (BISR), and failure analysis. In addition, we present a configurable flash memory tester using FPGA for low-cost testing and diagnostics. Experimental results on industrial flash chips justify the effectiveness of our test and diagnostics system.	adobe flash;algorithm;built-in self-test;deployment environment;design for testing;embedded system;failure analysis;field-programmable gate array;flash memory;marina;memory tester;non-volatile memory;random-access memory;windows update;yang	Chih-Tsun Huang;Jen-Chieh Yeh;Yuan-Yuan Shih;Rei-Fu Huang;Cheng-Wen Wu	2004	13th Asian Test Symposium	10.1109/ATS.2004.65	chip;flash file system;embedded system;failure analysis;parallel computing;computer hardware;telecommunications;computer science;engineering;flash memory emulator;design for testing;computer memory;universal memory;non-volatile random-access memory	EDA	20.987360646334746	52.799561014560055	26636
dd3bdbadc11803124f7a6b81f40e1b5fbf6f39fd	multiplication by a constant is sublinear	sublinear time;formal proof;constant integer multiplication;null;signal processing algorithms digital signal processing cryptography discrete cosine transforms throughput image processing digital arithmetic signal design cost function hardware;integer programming;sublinear time double base number system constant integer multiplication formal proof;double base number system	This paper explores the use of the double-base number system (DBNS) for constant integer multiplication. The DBNS recoding scheme represents integers - in this case constants in a multiple-radix way in the hope of minimizing the number of additions to be performed during constant multiplication. On the theoretical side, we propose a formal proof which shows that our recoding technique diminishes the number of additions in a sublinear way. Therefore, we prove Lefevre's conjecture that the multiplication by an integer constant is achievable in sublinear time. In a second part, we investigate various strategies and we provide numerical data showcasing the potential interest of our approach.	algorithm;computation;experiment;formal proof;level of measurement;numerical analysis;pattern search (optimization);time complexity	Vassil S. Dimitrov;Laurent Imbert;Andrew Zakaluzny	2007	18th IEEE Symposium on Computer Arithmetic (ARITH '07)	10.1109/ARITH.2007.24	arithmetic;formal proof;discrete mathematics;multiplication algorithm;integer programming;computer science;mathematics;algorithm;algebra	Theory	14.599326877233372	44.09700453564156	26725
1f7abc46cd88f757aa7afff9d037d5805ab88736	latchplanner: latch placement algorithm for datapath-oriented high-performance vlsi designs	datapath-friendly placement;wirelength estimation;placement engine;latch ordering;latch clusters;latch placement algorithm;datapath context;high quality placement;datapath structure;commercial placement engine;high cost;local latch placement;linear programming;network flow optimization;latchplanner;approximated wirelength estimation;logic design;vlsi;datapath-oriented high-performance vlsi design;flip-flops;latch sizing;latch clustering;global latch placement;datapath-oriented high-performance vlsi designs;logic structure recognition;conventional placement algorithm	In this paper, we present a novel algorithm for latch placement, LatchPlanner which enables a placement engine to deliver high quality placement for datapath-oriented design. Datapath-oriented VLSI designs are in general hand-crafted by human at high cost, as understanding and capturing datapath structure is critical for the performance. The conventional placement algorithms by itself cannot exploit the underlying datapath due to lack of logic structure recognition and inaccurate/approximated wirelength estimation. LatchPlanner addresses such drawbacks by placing and fixing latches in the datapath context, a key element in datapath structure. By taking placed/fixed latches as constraints, a placer can find a more datapath-friendly placement effectively, which results in higher-quality hardware. LatchPlanner begins latch clustering/sizing/ordering to prepare the following steps, a) global latch placement based on linear programming to place latch clusters, and b) local latch placement based on network flow optimization to place latches within each cluster. Experimental results on eighteen industrial benchmarks show that LatchPlanner improves total wirelength by 32%, total negative slack by 25%, and area by 3% without CPU overhead over a commercial placement engine, and delivers near semi-custom-quality solutions.	approximation algorithm;central processing unit;cluster analysis;datapath;display resolution;emoticon;flow network;linear programming;mathematical optimization;overhead (computing);semiconductor industry;slack variable	Minsik Cho;Hua Xiang;Haoxing Ren;Matthew M. Ziegler;Ruchir Puri	2013	2013 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)		mathematical optimization;electronic engineering;parallel computing;logic synthesis;real-time computing;computer science;engineering;linear programming;very-large-scale integration	EDA	15.164278655480475	53.07255990151429	26726
0bdf14f2c54362da1f08f7409822f944bef0238e	generic pipelined processor modeling and high performance cycle-accurate simulator generation	pipelines computational modeling computer simulation hardware software design mirrors resource management hazards embedded computing high performance computing;mirrors;reduced colored petri net;strongarm;hardware software codesign;performance evaluation;hardware software design;strongarm generic pipelined processor modeling performance cycle accurate simulator generation hardware software design reduced colored petri net rcpn model xscale;high performance computing;performance;resource management;hazards;rcpn model;hardware architecture;circuit simulation;computational modeling;pipelines;cycle accurate simulator generation;colored petri net;xscale;generic pipelined processor modeling;exponential growth;petri nets circuit simulation pipeline processing performance evaluation hardware software codesign microprocessor chips;petri nets;software design;high performance;computer simulation;embedded computing;pipeline processing;microprocessor chips;hardware	Detailed modeling of processors and high performance cycle-accurate simulators are essential for today's hardware and software design. These problems are challenging enough by themselves and have seen many previous research efforts. Addressing both simultaneously is even more challenging, with many existing approaches focusing on one over another. In this paper, we propose the Reduced Colored Petri Net (RCPN) model that has two advantages: first, it offers a very simple and intuitive way of modeling pipelined processors; second, it can generate high performance cycle-accurate simulators. RCPN benefits from all the useful features of Colored Petri Nets without suffering from their exponential growth in complexity. RCPN processor models are very intuitive since they are a mirror image of the processor pipeline block diagram. Furthermore, in our experiments on the generated cycle-accurate simulators for XScale and StrongArm processor models, we achieved an order of magnitude (~15 times) speedup over the popular SimpleScalar ARM simulator.	assembly language;central processing unit;charge-coupled device;compiler;computer architecture simulator;diagram;digital signal processor;experiment;hazard (computer architecture);instruction pipelining;instruction set simulator;international symposium on microarchitecture;maynard electronics;memoization;petri net;proceedings of the ieee;processor design;programming language design and implementation;semiconductor;simulation;software design;speedup;strongarm;time complexity;very-large-scale integration;xscale	Mehrdad Reshadi;Nikil D. Dutt	2005	Design, Automation and Test in Europe	10.1109/DATE.2005.166	computer simulation;embedded system;exponential growth;computer architecture;parallel computing;real-time computing;performance;hazard;computer science;electrical engineering;software design;resource management;operating system;hardware architecture;pipeline transport;computational model;petri net	EDA	6.892020707985812	51.7388363418216	26786
027461934720222ff27496aa38c32015dfa0c0ae	skew aware polarity assignment in clock tree	peak current;polarity assignment;vlsi design;chip;clock tree;power ground noise;optimization;clock skew;power	In modern sequential VLSI designs, clock tree plays an important role in synchronizing different components in a chip. To reduce peak current and power/ground noises caused by clock network, assigning different signal polarities to clock buffers is proposed in previous work. Althogh peak current and power/ground noises are minimized by signal polarities assignment, an assignment without timing information may increase the clock skew significantly. As a result, a timing-aware signal polarities assigning technique is necessary. In this paper, we propose a novel signal polarities assigning technique which can not only reduce peak current and power/ground noises simultaneously but also render the clock skew in control. The experimental result shows that the clock skew produced by our algorithm is 94% of original clock skew in average while the clock skew produced by three algorithms (Partition, MST, Matching) [5] are 235%, 272%, and 283%, respectively. Moreover, our algorithm is as efficient as the three algorithms of [5] in reducing peak current and power/ground noises.	algorithm;clock network;clock signal;clock skew;minimum spanning tree	Po-Yuan Chen;Kuan-Hsien Ho;TingTing Hwang	2007	2007 IEEE/ACM International Conference on Computer-Aided Design	10.1145/1497561.1497574	chip;clock synchronization;embedded system;real-time computing;clock angle problem;vector clock;telecommunications;clock domain crossing;clock skew;computer science;self-clocking signal;power;timing failure;clock drift;very-large-scale integration;synchronous circuit;clock gating;digital clock manager;clock signal;cpu multiplier	EDA	16.787297937221364	55.12579078425898	26838
ef3ef41bd94a1adbf4c13352902a77d5e081e23a	dynamically reconfigurable protocol transducer	reconfigurable protocol transducer;protocols;network synthesis;dynamic reconfiguration;copy protection;netlist reconfigurable protocol transducer protocol transducer synthesis ip core soc design automatic protocol transducer partial transducers layout design hard macro;layout design hard macro;automatic protocol transducer;soc design;reconfigurable architecture;netlist;ip core;partial transducers;industrial property;field programmable gate arrays;protocol transducer synthesis;protocols transducers hardware reconfigurable architectures operating systems computer architecture cryptography circuits reconfigurable logic design engineering;protocols copy protection field programmable gate arrays industrial property network synthesis;dynamic loading	Protocol transducer synthesis is one of the most significant issues for efficient IP core reuse in SoC design. The authors proposed automatic protocol transducer synthesis method (Watanabe et al., 2006), (Ishikawa et al., 2006). In this paper, an application of the protocol synthesis method to reconfigurable architecture on FPGA was proposed that enable to utilize various IPs dynamically. In coarsegrained reconfigurable architectures such as hardware OS, protocol transducers should be also dynamically reconfigured to make the dynamically loaded IPs able to communicate with each other. Our basic approach is division of a protocol transducer into partial ones. A whole transducer is constructed from these partial transducers by simply putting them side by side physically. Each partial transducer can be given in either layout design hard macro or in netlist	communications protocol;field-programmable gate array;input/output;ishikawa diagram;netlist;operating system;reconfigurability;reconfigurable computing;semiconductor intellectual property core;transducer;word lists by frequency	Shota Watanabe;Yuji Ishikawa;Kenshu Seto;Satoshi Komatsu;Masahiro Fujita	2006	2006 IEEE International Conference on Field Programmable Technology	10.1109/FPT.2006.270343	network synthesis filters;embedded system;communications protocol;real-time computing;netlist;computer science;field-programmable gate array	EDA	6.386870748553387	52.67364600204777	26839
6ff060c9fcabc0f0876ddb4348e20b0428a3568c	fully depleted soi (fdsoi) technology		Fully depleted SOI (FDSOI) has become a viable technology not only for continued CMOS scaling to 22 nm node and beyond but also for improving the performances of legacy technology when retrofitting to old technology nodes. In this paper, we provide an overview of FDSOI technology, including the benefits and challenges in FDSOI design, manufacturing, and ecosystem. We articulate that FDSOI is potential cornerstone for China to catch up and leapfrog in semiconductor technology.	cmos;ecosystem;image scaling;leapfrog integration;legacy system;performance;semiconductor;silicon on insulator	Kangguo Cheng;Ali Khaki-Firooz	2016	Science China Information Sciences	10.1007/s11432-016-5561-5	computer engineering;mathematical optimization;strain engineering;legacy system;mathematics;retrofitting;silicon on insulator;internet of things;cmos	EDA	11.90153921118249	57.58075727582096	26841
632074ca238322c469e9332643fc96c677a62063	task assignment heuristics for parallel and distributed cfd applications	workload;tratamiento paralelo;modelizacion;graph theory;distributed system;algoritmo paralelo;overset grids;evaluation performance;parallel processing high performance computing;human performance;teoria grafo;haute performance;one step method;systeme reparti;parallel algorithm;performance evaluation;traitement parallele;high performance computing;metodo un paso;evaluacion prestacion;heuristic method;distributed computing;multigrille;metodo heuristico;fluid mechanics;theorie graphe;distributed cfd;mecanique fluide;workloads psychophysiology;multiblock grids;algorithme parallele;computational fluid dynamics;grid;modelisation;sistema repartido;heuristic methods;tg;rejilla;multigrid;methode un pas;charge travail;multigrilla;alto rendimiento;parallel processing computers;grille;calculo repartido;performance prediction;task assignment;to;computational grids;methode heuristique;task graphs;tasks;mecanique fluide numerique;carga trabajo;subjects;mecanica fluido numerica;modeling;high performance;calcul reparti;mecanica fluido;parallel processing;parallel cfd	A Task Graph (TG) model is proposed for representing a single discrete step of multi-block overset grid Computational Fluid Dynamics (CFD) applications. The TG model is used to balance the computational workload across the overset grids and to reduce inter-grid communication costs. Based on the constraints inherent in CFD applications, assignment heuristics are developed and enhanced by integrating the status of processing units and communication costs. Extensive performance evaluation on a synthetic TG is reported. Also, a TG derived from a realistic problem with eight million grid points is used as a test case.	heuristic (computer science)	Noé Lopez-Benitez;M. Jahed Djomehri;Rupak Biswas	2007	IJCSE	10.1504/IJCSE.2007.015745	parallel processing;human performance technology;parallel computing;simulation;systems modeling;computational fluid dynamics;computer science;graph theory;parallel algorithm;grid;algorithm;multigrid method;fluid mechanics	HPC	-2.8490279970716372	35.94374136040995	26843
0065f94490815e5add1218307121ff05bc1565c1	automatic generation of design constraints in verifying high performance embedded dynamic circuits	automatic test pattern generation;circuits state space methods switches cost accounting microprocessors performance evaluation registers hardware clocks pins;application specific integrated circuits embedded systems microprocessor chips integrated circuit design circuit analysis computing circuit cad automatic test pattern generation;automatic generation;embedded systems;integrated circuit design;design environment;application specific integrated circuits;circuit cad;circuit analysis computing;motorola mpc7455 microprocessor automatic design constraints generation embedded dynamic circuit verification design constraints design verification design artifacts design environments potential dynamic circuit environment assumptions custom designed embedded dynamic circuits;high performance;microprocessor chips	Design constraints are artifacts that model an environment of a design under verification by restricting input stimuli to plausible valuations. Judicious usage of design constraints can be effective in eliminating false verification results. Given a particular verification problem, however, it is a difficult proposition to write down all the necessary constraints. We present a technique for automatic generation of design constraints from simple user-provided information about potential design environments. Our method generates a set of design constraints representing varying degrees of assumptions about potential environments of a dynamic circuit. We also present experimental results on verification of custom designed embedded dynamic circuits taken from the Motorola MPC7455 microprocessor.		Jayanta Bhadra;Narayanan Krishnamurthy	2002		10.1109/TEST.2002.1041763	physical design;embedded system;computer architecture;electronic engineering;real-time computing;computer science;automatic test pattern generation;circuit design;application-specific integrated circuit;functional verification;computer engineering;integrated circuit design	EDA	11.077043448036585	52.18231481603386	26860
ff074db9787db7876b4398adad3e48029398087a	a custom gzip decoder for dtv application	decoding;hardware description languages;telecommunication computing;digital television;telecommunication computing decoding digital television hardware description languages;frequency 300 mhz dtv application custom gzip decoder normal gzip stream partial decoding approach verilog hdl tsmc technology digital tv size 55 nm;decoding digital tv syntactics loading standards registers throughput	In this paper, we present a custom GZIP decoder for DTV application. Not only could GZIP decoder decompress the normal GZIP stream, but it also speed up the decompression for DTV application. The architecture exploits the principles of pipelining to the maximum extent in order to obtain high speed and throughput. A partial decoding approach is proposed to deal with the decompression with the limited memory. The decoder has been implemented with Verilog HDL and then synthesize the design with TSMC 55nm (GP) technology. The proposed design exhibits an operation frequency of 300MHz and costs about 233K equivalent gates. The throughput of our design is about 95MB/s.	data compression;double data rate;hardware description language;memory-mapped i/o;netbsd gzip / freebsd gzip;pipeline (computing);throughput;verilog	Ke Zhu;Weidong Liu;Jiang Du	2013	2013 IEEE International Symposium on Circuits and Systems (ISCAS2013)	10.1109/ISCAS.2013.6571938	embedded system;electronic engineering;real-time computing;digital television;telecommunications;computer science;hardware description language	EDA	11.11412318583776	44.35074641258318	26915
df045ba7017adad508a05b454c7123cb0f13f82c	fpga-accelerated complex event processing	software;fpga pipelined cep circuit software oriented event language hardware based acceleration digital circuit field programmable gate array big data complex event processing;field programmable gate arrays real time systems software acceleration big data vehicles servers;acceleration;servers;big data;field programmable gate arrays big data;vehicles;field programmable gate arrays;real time systems	This paper introduces an example of real-time “big data” processing systems accelerated by field-programmable gate arrays (FPGAs), which will open up a novel design field for digital circuit engineers. Contrary to the perception that software on commodity servers dominates such large-scale processing requirements, there are various chances for utilizing hardware for the acceleration. One of the most promising applications is complex event processing (CEP), which requires hardware-based acceleration due to it having to process massive amounts of data in real time. We propose a design flow for compiling software-oriented event language into highly parallelized and pipelined CEP circuits, which enables our system to achieve a strikingly high performance of 20 Gbps. A sophisticated mechanism for integrating archives of previously arrived data with streams of current events also makes the FPGA-accelerated processing system applicable to a wide range of realistic “big data” applications.	archive;big data;compiler;complex event processing;data rate units;design flow (eda);digital electronics;field-programmability;field-programmable gate array;parallel programming model;real-time clock;requirement	Takashi Takenaka;Hiroaki Inoue;Takeo Hosomi;Yuichi Nakamura	2015	2015 Symposium on VLSI Circuits (VLSI Circuits)	10.1109/VLSIC.2015.7231349	embedded system;real-time computing;computer hardware;computer science	EDA	1.7444004525094223	47.196230263905214	26919
03f9635584258bfde293fbc3feddd48c306236f2	transformed hct for parallel huffman decoding	huffman coding;look up table lut;huffman code table hct;parallel huffman decoding	This paper proposes a novel and generic Huffman code table HCT transform and a simple parallel Huffman decoding method. Codes of the original HCT are left-aligned, reordered in value, and partitioned into sub-bands. Two kinds of modification to the codes are introduced in order to reduce the number of sub-bands. The Huffman decoder can be implemented with a minimized size of single LUT, and the parallel decoding can be completed easily, at a constant rate of up to one code per cycle. An example of MP3 decoder and AAC decoder has been designed to demonstrate the efficiency of the proposed method. Copyright © 2014 John Wiley & Sons, Ltd.	adaptive huffman coding;windows hardware certification kit	Guoyu Wang;Hongsheng Zhang;Mingying Lu	2015	I. J. Circuit Theory and Applications	10.1002/cta.2044	arithmetic;prefix code;canonical huffman code;computer science;theoretical computer science;modified huffman coding;mathematics;algorithm;deflate;huffman coding	Theory	9.872524028593341	37.83440274192719	26929
53dd3f8116f18308fef79b306798ce5c803faaf0	fault diagnosis in memory bist environment with non-march tests	off line processing fault diagnosis memory bist nonmarch tests failure identification embedded memory array flexible logic memory failure;embedded systems;built in self test;integrated circuit testing;semiconductor storage built in self test embedded systems fault diagnosis integrated circuit testing;semiconductor storage;fault diagnosis	This paper presents a new BIST-based fault diagnosis scheme for non-march tests of complexity O(n^2). It can be used to identify failures in embedded memory arrays using galloping pattern tests. The proposed solution employs scalable and flexible logic to record test responses, with no negative impact on at-speed test. It enables recording of responses produced by failures hard to handle by conventional march tests. This, in turn, allows accurate isolation of memory failures during off-line processing.	built-in self-test;embedded system;online and offline;scalability	Grzegorz Mrugalski;Artur Pogiel;Nilanjan Mukherjee;Janusz Rajski;Jerzy Tyszer;Pawel Urbanek	2011	2011 Asian Test Symposium	10.1109/ATS.2011.48	embedded system;electronic engineering;real-time computing;fault coverage;computer science;engineering;stuck-at fault	EDA	20.974315614444546	51.799819145437816	26950
8c3f69b0f2b75499b60ae8f1b5ccf8186f576cb8	a performance estimation tool for video applications	digital signal processing;digital signal processing application software design engineering hardware system performance frequency helium instruments computer science consumer electronics;instruments;design engineering;application software;helium;performance estimation;consumer electronics;system performance;data flow graph;computer science;frequency;simulation tool;hardware	"""A trend in the consumer electronics market is the demand for new applications that have a lot of similarities to older applications but the new ones impose more challenging and special-purpose performance requirements. In the digital signal processing (DSP) industry, this clearly reflects a transition from the design regime of general DSP to the application-specific DSP. A key in effecting this transition is the engineering capability to make the design specification """"match"""" the application, before detailed design starts. In this paper, we present an effective simulation tool to assist in the generation of accurate design specifications. It models the application by a parameter-driven conditional data-flow graph (CDFG) and the hardware (HW) architecture by a configurable HW graph. The simulator takes the application CDFG and HW graph as the input and carries the simulation at a low level to catch the detailed HW activities. Experimental results show that our tool is not only able to accurately estimate system performance, but also helps to identify performance bottlenecks and to find the optimal software (SW) implementation solution for video applications."""	computer hardware;data-flow analysis;dataflow;digital signal processing;requirement;simulation	Zhengting He;Cheng Peng;Aloysius K. Mok	2006	12th IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS'06)	10.1109/RTAS.2006.6	embedded system;application software;real-time computing;simulation;computer science;operating system;digital signal processing;data-flow analysis;frequency;computer performance;helium;programming language	Embedded	2.7271037478382176	54.46733665570569	27004
ed6c75a393b95c9c1b2b263600b50136ada68dce	fast-squarer circuits using 3-bit-scan without overlapping bits	vlsi;squarer circuit;multiplication;cmos	This paper presents a novel technique to design fast-squaring circuits. The proposed approach speeds up squaring operations combining the 3-bit scan without overlapping bits and the folding technique.#R##N##R##N#Several hardware implementations of squarer circuits designed as described here are characterized for several operand wordlengths. Obtained results demonstrate that, using the ST 90 nm 1V CMOS technology, a 32-bit squarer exploiting the novel way of generating partial products reaches a 769 MHz running frequency, dissipates less than 19.3 mW on average and occupies ∼91 000µm2 of silicon area. Copyright © 2010 John Wiley & Sons, Ltd.		Stefania Perri;Pasquale Corsonello	2011	I. J. Circuit Theory and Applications	10.1002/cta.685	electronic engineering;computer science;electrical engineering;mathematics;very-large-scale integration;cmos;multiplication;algorithm	EDA	13.546293966975588	44.720291965058365	27011
29ec0f23d6f20d249be957ba98d8d714967afe7d	the semiconductor industry's nanoelectronics research initiative: motivation and challenges	cmos integrated circuits;research initiatives cmos integrated circuits integrated circuit manufacture nanoelectronics;nri semiconductor industry nanoelectronics research initiative complimentary metal oxide semiconductor cmos technology charge switching based devices;cmos technology;performance evaluation;semiconductor industry;electronics industry nanoelectronics research initiatives cmos technology fets power generation threshold voltage cmos logic circuits power semiconductor switches costs;industries;computer architecture;complimentary metal oxide semiconductor;fets;threshold voltage;cmos logic circuits;electronics industry;nanoelectronics;power generation;research initiatives;research consortium nanoelectronics logic transistors;charge switching based devices;nanoelectronics research initiative;power semiconductor switches;switches;nri;logic transistors;metal oxide semiconductor;integrated circuit manufacture;research consortium	In this presentation, the scaling challenges facing current Complimentary Metal Oxide Semiconductor (CMOS) technology will be discussed, along with the ultimate limits for charge-switching based devices. From this motivation, the current status of the Nanoelectronics Research Initiative (NRI) will be discussed, with an overview of the current research topics being investigated at the NRI centers.	cmos;image scaling;semiconductor industry	Jeff Welser	2009	2009 46th ACM/IEEE Design Automation Conference	10.1145/1629911.1629993	embedded system;electronic engineering;computer science;engineering;electrical engineering;cmos	EDA	12.26918391339061	57.83881589940063	27045
1cc0820d6508ed50a16bb7e435e8bdd33e6cecc0	broadcasting in an n-grid with a given neighborhood template	graph theory;whi;lattice theory;combinatorics;neighborhood template;network broadcasting neighborhood template originator vertex shouting scheme whispering scheme infinite n dimensional grid lattice theory combinatorics vertex number;broadcasting polynomials intelligent networks lattices combinatorial mathematics h infinity control;originator vertex;graph theory broadcasting telecommunication networks lattice theory;whispering scheme;broadcasting;vertex number;shouting scheme;infinite n dimensional grid;telecommunication networks;network	In a process of broadcasting, a particular vertex, called the originator, broadcasts information by mean of calls to all the vertices of the network. Each call requires a time unit, and a vertex can call only its neighbors. The process is called shouting, (resp. whispering) if a vertex can call all its neighbors (resp. only one of its neighbors) at a time. Q.F. Stout defines u(t) (resp. w(t)) as the maximum number of vertices that may be informed at time t by any shouting scheme (resp. whispering). In this paper, we consider the particular case when the network is an infinite n-dimensional grid with a given neighborhood template F. Without restricting the form of the set F, we determine u(t) and an equivalent to w(t). We also give a whispering scheme that is nearly optimal. Our proofs mainly use techniques from lattice theory and combinatorics to determine the number of vertices at distance t from 0.	c date and time functions;vertex (geometry);vertex (graph theory)	Christine Garcia;Claudine Peyrat	1995		10.1109/HICSS.1995.375499	graph theory;lattice;broadcasting	Theory	20.705217411872287	34.476531034091956	27058
79d8f2be4d315f7c589b87e41ebcc5860b0f04c7	a fully connected layer elimination for a binarizec convolutional neural network on an fpga		A pre-trained convolutional deep neural network (CNN) is widely used for embedded systems, which requires highly power-and-area efficiency. In that case, the CPU is too slow, the embedded GPU dissipates much power, and the ASIC cannot keep up with the rapidly progress of the CNN variations. This paper uses a binarized CNN which treats only binary 2-values for the inputs and the weights. Since the multiplier is replaced into an XNOR circuit, we can realize a high-performance MAC circuit by using many XNOR circuits. In the paper, we eliminate internal FC layers excluding the last one, then, insert a binarized average pooling layer, which can be realized by a majority circuit for binarized (1/0) values. In that case, since the weight memory is replaced into the 1's counter, we can realize a compact and faster CNN than the conventional ones. We implemented the VGG-11 benchmark CNN for the CIFAR10 image classification task on the Xilinx Inc. Zedboard. Compared with the conventional binarized implementations on an FPGA, the classification accuracy was almost the same, the performance per power efficiency is 5.1 better, as for the performance per area efficiency, it is 8.0 times better, and as for the performance per memory, it is 8.2 times better.	application-specific integrated circuit;artificial neural network;benchmark (computing);central processing unit;computer vision;convolutional neural network;deep learning;division by zero;embedded system;field-programmable gate array;graphics processing unit;performance per watt;xnor gate	Hiroki Nakahara;Tomoya Fujii;Shimpei Sato	2017	2017 27th International Conference on Field Programmable Logic and Applications (FPL)	10.23919/FPL.2017.8056771	field-programmable gate array;application-specific integrated circuit;computer science;parallel computing;real-time computing;convolutional neural network;artificial neural network;electronic circuit;central processing unit;xnor gate;contextual image classification	EDA	3.957505982976301	43.398152380751746	27099
5ae6fa6d96ed1d3ba28fd76ea8489feb39d7f2f0	a multiprocessor soc architecture with efficient communication infrastructure and advanced compiler support for easy application development	application development;distributed memory;architectural design;tecnologia electronica telecomunicaciones;crossbar interconnect;point to point;chip;system on chip;vlsi;multiprocessor on a chip;tecnologias;grupo a;concurrent process;system on chips	This paper presents a Multiprocessor System-on-Chips (MPSoC) architecture used as an execution platform for the new C-language based MPSoC design framework we are currently developing. The MPSoC architecture is based on an existing SoC platform with a commercial RISC core acting as the host CPU. We extend the existing SoC with a multiprocessor-array block that is used as the main engine to run parallel applications modeled in our design framework. Utilizing several optimizations provided by our compiler, an efficient inter-communication between processing elements with minimum overhead is implemented. A host-interface is designed to integrate the existing RISC core to the multiprocessor-array. The experimental results show that an efficacious integration is achieved, proving that the designed communication module can be used to efficiently incorporate off-the-shelf processors as a processing element for MPSoC architectures designed using our framework.	compiler;multiprocessing;system on a chip	Mohammad Zalfany Urfianto;Tsuyoshi Isshiki;Arif Ullah Khan;Dongju Li;Hiroaki Kunieda	2008	IEICE Transactions	10.1093/ietfec/e91-a.4.1185	chip;system on a chip;embedded system;computer architecture;parallel computing;distributed memory;telecommunications;point-to-point;computer science;very-large-scale integration;rapid application development	HPC	1.7719288228737808	49.46851275895593	27139
05cb1f088ed826fc8de0bf509fc15b27d3e74d20	delay and area optimization in standard-cell design	libraries;electric resistance;optimisation;discrete optimization;capacitive loading delay area optimization standard cell design heuristic approach vlsi circuit design cell library templates driving capabilities intrinsic delay;very large scale integration;vlsi circuit design;intrinsic delay;delay design optimization libraries electric resistance circuit synthesis capacitance switches very large scale integration ear roundoff errors;design optimization;driving capabilities;ear;capacitive loading;standard cell design;roundoff errors;vlsi;capacitance;heuristic approach;vlsi delays logic cad optimisation;technology mapping;switches;logic cad;cell library;templates;circuit synthesis;delays;area optimization	This paper presents a heuristic approach to the optimal selection of standard cells in VLSI circuit design. We are considering a cell library composed of several templates (3-5) for each type of cell. These templates differ in area, driving capabilities, intrinsic delay, and capacitive loading. When realizing a logically synthesized circuit, we select the best templates from the cell library to minimize the total area of the cells under delay constraints. We have found a very successful heuristic approach to attack this discrete optimization problem. Experimental results show that this approach runs very fast, with the complexity of &Ogr;(n2), and improves the results obtained from the technology mapping of misII. [15]	cell (microprocessor);circuit design;discrete optimization;heuristic;mathematical optimization;optimization problem;standard cell	Shen Lin;Malgorzata Marek-Sadowska;Ernest S. Kuh	1990		10.1145/123186.123301	control engineering;discrete optimization;embedded system;electronic engineering;computer science;engineering;electrical engineering;very-large-scale integration	EDA	14.205648132236725	52.34919580528936	27169
b6297c6ec4fcbd8ce9843dc9677db37dd18d18d6	an asic for high performance stepper motor control	protocols;keyboards;motor drives;application software;very large scale integration;application specific integrated circuits motor drives application software communication system control computer interfaces protocols keyboards displays eprom very large scale integration;application specific integrated circuits;displays;eprom;communication system control;computer interfaces;modes of operation;high performance;motor control	The paper discusses the design of an ASIC for high performance stepper motor contml. The features such as serial communication with multidrop application, a user fraendly high level command interface and indezed mode of operation have been introduced in the design.	application-specific integrated circuit	K. V. S. H. Rao;A. Kansal;Chandra Shekhar;M. Srinivas;V. N. S. N. Rao	1992		10.1109/ICVD.1992.658074	motor control;embedded system;communications protocol;electronic engineering;application software;real-time computing;computer hardware;computer science;operating system;application-specific integrated circuit;very-large-scale integration;eprom	HPC	7.073032073649146	49.000410312027746	27179
0e614af72d50ad70e56df7d37f66822c9019ef35	when merging and branch predictors collide	performance evaluation;sorting;performance analysis;merging	Merging is a building block for many computational domains. In this work we consider the relationship between merging, branch predictors, and input data dependency. Branch predictors are ubiquitous in modern processors as they are useful for many high performance computing applications. While it is well known that the performance and the branch prediction accuracy go hand-in-hand, these have not been studied in the context of merging. We thoroughly test merging using multiple input array sizes and values using the same code and compile optimizations. As the number of possible keys increase, so the do the number of branch mis-predictions - resulting in reduced performance. The reduction in performance can be as much as 5X. We explain this phenomenon using a visualization technique called Merge Path that intuitively shows this. We support this visualization approach with modeling, thorough testing, and analysis on multiple systems.	branch predictor;central processing unit;compiler;data dependency;supercomputer	Oded Green	2014		10.1109/IA3.2014.9	parallel computing;real-time computing;computer science;theoretical computer science	HPC	-3.1241340504326356	44.70682166111156	27216
348d72628091c4de5c9536de2682b4fea5e2aafb	compression mode diagnosis enables high volume monitoring diagnosis flow	system on chip automatic test equipment automatic test pattern generation boundary scan testing cmos integrated circuits failure analysis fault diagnosis integrated circuit testing statistical analysis;cmos integrated circuits;130 nm compression mode diagnosis high volume monitoring diagnosis flow scan test fail data diagnosis cmos technology monitoring diagnosis methodology production test floor compressed scan technology fault diagnosis system on a chip product infineon technologies;system on a chip product;cmos technology;scan test fail data diagnosis;production test floor;infineon technologies;automatic test pattern generation;automatic test equipment;monitoring diagnosis methodology;system on a chip;compressed scan technology;boundary scan testing;failure analysis;statistical analysis;system on chip;testing cmos technology condition monitoring fault diagnosis manufacturing processes optimized production technology system on a chip feedback throughput failure analysis;compression mode diagnosis;timing optimization;130 nm;integrated circuit testing;statistics;high throughput;high volume monitoring diagnosis flow;fault diagnosis	Diagnosis of scan test fail data plays a crucial role in enhancing ramp up of new CMOS technology generations. To enable faster feedback it is preferable to establish a monitoring diagnosis methodology on the production test floor. This paper reports result of a study on using test time optimized compressed scan technology and associated new algorithms for fault diagnosis. Data is based on a system-on-a-chip (SoC) product that is manufactured using Infineon Technologies' 130 nm process. A comparison with uncompressed scan test and diagnosis shows feasibility of implementing a monitoring diagnosis flow with compressed scan test serving the high throughput test flow	air traffic control radar beacon system;algorithm;automatic identification and data capture;built-in test equipment;cmos;die (integrated circuit);event dispatching thread;fail-safe;failure analysis;fault model;feedback;image scanner;overhead (computing);ramp simulation software for modelling reliability, availability and maintainability;system on a chip;throughput;truncation	Andreas Leininger;Peter Muhmenthaler;Wu-Tung Cheng;Nagesh Tamarapalli;Wu Yang;Kun-Han Tsai	2005	IEEE International Conference on Test, 2005.	10.1109/TEST.2005.1583972	system on a chip;embedded system;electronic engineering;real-time computing;computer science;engineering;test compression;cmos;statistics	EDA	22.165754764011794	53.55550066524813	27294
2ea04577a1c7990d0e27df8bb2b00339fcea678f	virtual prototyping of heterogeneous dynamic platforms using open virtual platforms	hardware registers computational modeling delays artificial intelligence software computer architecture;software;runtime adaptive systems high level simulation dynamic heterogeneous platform open virtual platforms ovp;computer architecture;computational modeling;registers;reconfigurable accelerator model virtual prototyping open virtual platforms heterogeneous dynamic computing platforms general purpose processors gpp operating system ovp high level simulations i o device;virtual prototyping input output programs;artificial intelligence;delays;hardware	Heterogeneous dynamic computing platforms are one of the big trends in today's electronic world. These platforms typically feature different General-Purpose-Processors (GPP) combined with accelerators on a reconfigurable layer. However, this necessitates specialized programming models and an Operating System (OS) for dealing with the dynamicity. To allow the early development of the system software, the tool-chain and to help in the design space exploration, high-level simulations offer a possible solution. In this paper we evaluate the application of Open Virtual Platforms (OVP) for the modelling of a heterogeneous dynamic platform. The OVP high-level simulations are extended by introducing peripherals to model the abilities of such a platform. Specifically, a generic I/O device, a communication device for a distributed platform and an accelerator interface with a reconfigurable accelerator model is implemented and integrated into a simulated platform. Different approaches are presented and the insights and results gained during the development process are discussed regarding speed and applicability to the use case.	debugging;design space exploration;field-programmable gate array;general-purpose markup language;graph partition;high- and low-level;input/output;operating system;peripheral;reconfigurable computing;simulation;toolchain;tron;virtual machine	Leonard Masing;Stephan Werner;Jürgen Becker	2015	10th IEEE International Symposium on Industrial Embedded Systems (SIES)	10.1109/SIES.2015.7185053	embedded system;real-time computing;computer science;operating system;processor register;computational model	Embedded	-1.4235642282095426	48.95885376253382	27295
50002d032f42daf00fa623095c1ac6609716da62	power estimation techniques for fpgas	digital circuit;modelizacion;switching activity;cmos integrated circuits;field programmable gate array;circuit integre cmos;switching;diseno circuito;interconnection;modele empirique;capacitancia;integrated circuit;integrated circuit layout;circuit design;circuito integrado;red puerta programable;reseau porte programmable;circuit numerique;interconexion;modelisation;estimation;interconnexion;conmutacion;low power electronics;circuito numerico;empirical model;implantation circuit integre;modelo empirico;conception circuit;capacitance;temps retard;delay time;power consumption;consommation energie electrique;modeling;electronique faible puissance;tiempo retardo;power;commutation;circuit integre;field programmable gate arrays fpgas;capacite electrique	The dynamic power consumed by a digital CMOS circuit is directly proportional to both switching activity and interconnect capacitance. In this paper, we consider early prediction of net activity and interconnect capacitance in field-programmable gate array (FPGA) designs. We develop empirical prediction models for these parameters, suitable for use in power-aware layout synthesis, early power estimation/planning, and other applications. We examine how switching activity on a net changes when delays are zero (zero delay activity) versus when logic delays are considered (logic delay activity) versus when both logic and routing delays are considered (routed delay activity). We then describe a novel approach for prelayout activity prediction that estimates a net's routed delay activity using only zero or logic delay activity values, along with structural and functional circuit properties. For capacitance prediction, we show that prediction accuracy is improved by considering aspects of the FPGA interconnect architecture in addition to generic parameters, such as net fanout and bounding box perimeter length. We also demonstrate that there is an inherent variability (noise) in the switching activity and capacitance of nets that limits the accuracy attainable in prediction. Experimental results show the proposed prediction models work well given the noise limitations.	activity recognition;cmos;computer-aided design;fan-out;field-programmability;field-programmable gate array;heart rate variability;low-power broadcasting;minimum bounding box;perimeter;routing	Jason Helge Anderson;Farid N. Najm	2004	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2004.831478	embedded system;estimation;electronic engineering;systems modeling;computer science;engineering;electrical engineering;integrated circuit;interconnection;circuit design;power;capacitance;integrated circuit layout;cmos;digital electronics;empirical modelling;field-programmable gate array;low-power electronics	EDA	18.477377879633742	54.48635605368342	27317
05c654a27736e164f82f76279bdc1f048cdd2dab	smart navigation for firefighters in hazardous environments: a ban-based approach	indoor navigation;real time;localization;hazard estimation;sensor network;navigation;ban;firefighting	Recent advances in integrated electronic devices motivated the use of Body Area Networks in many applications including monitoring, localization, tracking and navigation. In this paper we introduce an indoor navigation approach based on Body Area Network to assist firefighters in finding their way to save human lives and to combat fires. For this we develop a technique based on a real-time graph called Temporal Weighted Graph that provides some special functions such as localization, navigation, communication, and hazard estimation. Then we implement a real time solution aiming to predict firefighters' isolation time in an indoor space by estimating the horizon of risk deterioration in the graph. And finally, we demonstrate the importance of the presented technique in assisting firefighters during the navigation process. A set of simulation scenarios are conducted to evaluate the performance of the solution.		M'hamed Chammem;Sarra Berrahal;Noureddine Boudriga	2012		10.1007/978-3-642-37015-1_8	embedded system;navigation;simulation;wireless sensor network;internationalization and localization;telecommunications;computer science;computer security;mobile robot navigation;firefighting	Robotics	4.66862444423962	35.1722573231175	27335
5df5300e9fcc80a0106852f93b05ab0c30926c40	a vlsi programmable cellular automata array for multiplication in gf (2n)	cellular automata		cellular automaton;grammatical framework	Chang Nian Zhang;Ming Y. Deng;Ralph Mason	1999				Theory	17.297520823441456	42.83039406848117	27351
0f2f1f9335ed9ec4208cee33e54280a2381e5b4d	eccentricity of the nodes of otis-cube and enhanced otis-cube		In this paper we have classified the nodes of OTIS-cube based on their eccentricities. OTIS (optical transpose interconnection system) is a large scale optoelectronic computer architecture, proposed in [1], that benefit from both optical and electronic technologies. We show that radius and diameter of OTIS-Qn is n + 1 and 2n + 1 respectively. We also show that average eccentricity of OTIS-cube is (3n/2 + 1). In [10], a variant of OTIS-cube, called Enhanced OTIS-cube (E-OTIS-Qn) was proposed. E-OTIS-Qn is regular of degree n + 1 and maximally fault-tolerant. In this paper we have given a classification of the nodes of E-OTIS cube and derived expressions for the eccentricities of the nodes in each class. Based on these results we show that radius and diameter of E-OTIS-Qn is n+ 1 and ⌊4n+ 4/3⌋ respectively. We have also computed the average eccentricity of E-OTIS-Qn for values of n upto 20.	computer architecture;cube;distance (graph theory);emoticon;fault tolerance;interconnection	Rajib K. Das	2013	CoRR		algorithm	Arch	23.559663616349088	35.627384514538015	27368
408888f52aa6ab70a4fdaaddb71af242ba9ef579	a methodology for fast and accurate yield factor estimation during global routing	global routing stage;yield factor estimator;proposed yield factor estimator;representative post-routing yield optimizations;post-routing yield optimizations;final yield;yield factor estimation;final yield factor hotspots;key yield factor;typical yield optimization solution;accurate yield factor estimation;network routing;ring oscillator;estimation theory;vlsi;design for manufacture;integrated circuit design	In this paper, a novel and computationally efficient methodology to accurately estimate key yield factors during the global routing stage is presented. Such an yield factor estimator at the global routing stage is essential since it can used to either get an early estimate of the final yield of the same design (i.e. the yield after applying the required sequence of detailed routing and post-routing yield optimizations) and/or to improve the final yield of the design by making the solution at the end of global routing more amenable to post-routing yield optimizations. The proposed yield factor estimator is inherently flexible and can easily be programmed to estimate during global routing a variety of key yield factors of the same design after a typical sequence of detailed routing and representative post-routing yield optimizations has been applied. Examples are provided to show how the yield factor estimator can be used to predict short and open critical area and metal density after typical yield optimization solutions like wire-spreading, wire-widening and metal filling, respectively. Experimental results presented in the paper show that the proposed yield factor estimator can predict final yield factor hotspots/values with a high degree of accuracy. The proposed estimator is also shown to be more suited for the purpose of yield factor estimation compared with typical metrics at the global routing stage like congestion.	algorithmic efficiency;mathematical optimization;network congestion;routing	Subarna Sinha;Charles C. Chiang	2007	2007 IEEE/ACM International Conference on Computer-Aided Design	10.1145/1326073.1326173	routing;electronic engineering;real-time computing;computer science;engineering;electrical engineering;ring oscillator;very-large-scale integration;estimation theory;design for manufacturability;statistics;integrated circuit design	EDA	15.82568880361165	53.2170391267226	27380
27cc94c4128e681f99f736bfbd949b84b058ac66	automatic test pattern generation on multiprocessors: a summary of results	search space;vlsi design;backtrack search;test generation;combinational circuit;automatic test pattern generator	"""Test generation of combinational circuits is an impor tant step in the VLSI design process. Unfortunately, the problem is highly computation-intensive and, for circuits encountered in practice, test generation t ime can often be enormous. In this paper , we present a parallel formulation of a backtrack search algorithm called PODEM, which has been the most successful algorithm for this problem. The sequential P O D E M algorithm consumes most of its execution time in generating a test for """"hard-to-detect"""" (HTD) faults and is often unable to detect them even after a large number of backtracks. Our parallel formulation a t tempts to overcome these l imitat ions by part i t ioning the search space in order to search it concurrently using multiple processors. We present speedup results and performance analyses of our formulation on a 128 processor Symult s2010 mult icomputer . Our results show tha t parallel search techniques provide good speedups (45-106 on 128 processors) as well as high fault coverage of the HTD faults in reasonable time as compared to the uniprocessor implementat ion. Tree search is an integral par t of several AI systems. Effective parallel processing of search problems is impor tant in developing high performance knowledge-based systems. Results from this paper show tha t tree search can be effectively parallelized on large scale parallel processors in the context of practical problems. *This work was partially supported by Army Research Office grant ~ DAAG29-84-K-0060 to the Artificial Intelligence Laboratory, Office of Naval Research Grant N00014-86-K-0763 to the Computer Science Department, at the University of Texas at Austin. ?A large part of this research was performed while the first and second authors were at the University of Texas at Austin."""	artificial intelligence;backtracking;central processing unit;combinational logic;computation;computer science;fault coverage;knowledge-based systems;parallel computing;run time (program lifecycle phase);search algorithm;speedup;t-tree;uniprocessor system;very-large-scale integration	Sunil Arvindam;Vipin Kumar;V. Nageshwara Rao;Vineet Singh	1989		10.1007/BFb0018367	computer architecture;parallel computing;computer science;theoretical computer science;automatic test pattern generation	HPC	2.033723647914809	40.35124151792843	27388
87cb1e9bd8a094f26450d28b19135e7da6ab1d62	on the use of hard faults to generate test sets	cell placement;longest path;physical design;scheduling;critical path;ic placement;heuristics	"""In test set embedding Built-In Self-Test (BIST) schemes a pre-computed test set is embedded into the sequence generated by a hardware module. In this work we first apply a sequence of pseudorandom patterns and then try to embed patterns for the remaining faults (i.e. """"extremely hard"""" faults) in the sequence generated by the hardware generator."""	built-in self-test;embedded system;precomputation;pseudorandomness;test set	Ioannis Voyiatzis;Costas Efstathiou	2015		10.1145/2801948.2801951	physical design;parallel computing;real-time computing;longest path problem;computer science;operating system;critical path method;heuristics;distributed computing;scheduling	EDA	20.159673013569524	51.53248432818877	27404
41afd373242c5e8637ee1654ce08524b2edfcebb	detection of resistive shorts in deep sub-micron technologies	circuit faults;low voltage;threshold voltage;circuit testing delay logic testing laboratories threshold voltage switches costs temperature modems circuit faults;logic testing;next generation;circuit testing;modems;deep sub micron;temperature;switches	Current-based tests are the most effective methods available to detect resistive shorts. Delta I DDQ testing is the most sensitive variant and can handle off-state currents of 10-100 mA of a single core. Nevertheless this is not sufficient to handle the next generations of very deep sub-micron technologies. Moreover delay-fault testing and very-low voltage testing are not a real alternative for the detection of resistive shorts. The main limitation of ∆IDDQ testing is the intra-die variation of the threshold voltage which results in variations in the off-state current. Two methods are investigated that improve the detection capabilities of∆IDDQ testing. The first method reduces the impact of intra-die variation by reducing the amount of logic that switches states. This method can handle very large off-state currents although at the cost of a substantial increase in test time. The second method investigates the correct scaling of the intra-die variations as a function of temperature. We show that both methods improve the detection capabilities of ∆IDDQ testing.	device under test;effective method;experiment;gate oxide;iddq testing;image scaling;network switch;spectral leakage	Bram Kruseman;Stefan van den Oetelaar	2003		10.1109/TEST.2003.1271072	reliability engineering;embedded system;electronic engineering;temperature;network switch;engineering;electrical engineering;threshold voltage;low voltage	SE	22.13266084007932	54.6521574732875	27438
439b1402274704e9fafdbf136cdb000856d51dcb	reducing fault latency in concurrent on-line testing by using checking functions over internal lines	fault simulation;logic testing;concurrent on-line testing;fault coverage;fault detection time reduction;fault latency reduction;fault simulation;internal line checking functions	We describe a method to reduce the fault latency, i.e., the time it takes to detect a fault after it occurs, during concurrent on-line testing. A high fault latency can negatively affect the fault coverage in various ways. The fault latency is reduced by using what we call checking functions. A checking function cf/sub i/ expresses the function of a line g/sub i/ in the circuit as a function of one or more other lines. During concurrent on-line testing, the value of g/sub i/ is compared to the value of cf/sub i/. A mismatch indicates the presence of a fault. The advantage of checking functions is that they only use lines that already exist in the circuit. We demonstrate that benchmark circuits have large numbers of checking functions to choose from. We also demonstrate the increase in fault coverage and the reductions in fault detection times possible by using checking functions.	benchmark (computing);byzantine fault tolerance;fault coverage;fault detection and isolation;online and offline	Irith Pomeranz;Sudhakar M. Reddy	2004	19th IEEE International Symposium on Defect and Fault Tolerance in VLSI Systems, 2004. DFT 2004. Proceedings.	10.1109/DFT.2004.50	reliability engineering;embedded system;fault;real-time computing;fault coverage;fault indicator;computer science;engineering;stuck-at fault;segmentation fault;fault model;general protection fault;fault detection and isolation;software fault tolerance;affect	EDA	21.212484927263834	51.303339857586835	27516
48eed631513f6fc8e56750ef53838a83032da297	an accurate analysis of the effects of soft errors in the instruction and data caches of a pipelined microprocessor	different memory configuration;soft errors;pipelined microprocessor;extensive analysis;soft core;accurate analysis;architectural solution;data cache;sparc architecture;processor failure rate;soft error;data caches;high-end processor;logic simulation;pll;cache memory;computer architecture;computational modeling;failure rate;testing;test;failure analysis;dft;network analysis;fault tolerance	Instruction and data caches are well known architectural solutions that allow significantly improving the performance of high-end processors. Due to their sensitivity to soft errors they are often disabled in safety critical applications, thus sacrificing performance for improved dependability. In this paper we report an accurate analysis of the effects of soft errors in the instruction and data caches of a soft core implementing the SPARC architecture. Thanks to an efficient simulation-based fault injection environment we developed, we are able to present in this paper an extensive analysis of the effects of soft errors on a processor running several applications under different memory configurations. The procedure we followed allows the precise computation of the processor failure rate when the cache is enabled even without resorting to expensive radiation experiments.	central processing unit;computation;dependability;experiment;failure rate;fault injection;microprocessor;sparc;simulation;soft error	Maurizio Rebaudengo;Matteo Sonza Reorda;Massimo Violante	2003			embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;software testing	Arch	5.219781563358873	58.11243799827688	27528
8d87c1e6be2ef00b58ed8e2741a75d0d240a2116	mathematics of the design of a parallel mapping assembly algorithm - combining smith-waterman and hirschberg's lcs methods		This paper focuses on mathematical definitions and results that prove the correctness of a parallel algorithm for mapping assembly. The mathematical concepts and facts discussed here establish the reach and limitations of a combination of Smith-Waterman local alignment method and Hirschberg’s divide-andconquer longest common subsequence determination method. The parallel algorithm, whose correctness is proved, is a general method that works best for solving the problem of the local alignment of a short and a very large sequence, such as an entire genome. The method is thus, suitable for mapping assembly, where millions of short sequence segments, the so-called reads, are aligned with a whole genome.	computation;concatenation;correctness (computer science);embarrassingly parallel;hirschberg's algorithm;lightweight portable security;longest common subsequence problem;message passing interface;openmp;parallel algorithm;parallel computing;qr decomposition;run time (program lifecycle phase);smith–waterman algorithm;time complexity;uniform resource identifier	Jaime Seguel	2014		10.5220/0004883802210226		Comp.	1.3290856103219062	36.88108700961247	27594
cc7de5a2b29a49faa68ea3ba3c27c6139288ddee	a fast distributed mapping algorithm	simulated annealing;network topology;heuristic search;parallel computer;load balance;communication optimization	Generating an efficient program for a parallel computer requires that the distribution of the processes on the processors comprising the parallel computer is most optimal. This paper presents a new method for a load balanced and communication optimized process distribution onto an arbitrary processor (network) topology. As opposed to many other approaches for this problem, the presented algorithm is fully distributed and based on a purely local method. It has shown to be much faster compared to the classical methods like simulated annealing, heuristic search, etc. 1 T h e M a p p i n g P r o b l e m The so called mapping problem is fundamental in distributed or parallel computing. From a topological viewpoint, the mapping may be defined as follows: Is there a mapping of a system of communicating processes onto a processor network such that the neighboring processes are assigned to neighboring processors ? Since mapping a set of communicating processes onto processor networks is known to be NP-comple te [Bok81], it makes no sense to t ry and develop exact algorithms for solving the problem. Furthermore, as such mapping algorithms should be integrated into distributed operating systems, they should be able to produce suboptimal solutions very quickly. The mapping algorithm presented here (called the M a p p e r ) , is fully distributed and delivers optimal or suboptimal solutions in a short time. It has been implemented for mapping a static system of communicating processes onto a processor network with distributed memory architecture. The Mapper has been implemented in o e c a m , and can be run on an arbitrary Transputer network. A parallel program expressed as a set of communicating sequential processes [Hoa85] can be represented as a graph, where each process forms a vertex and each communication path between any two processes forms an edge. Similarly a processor network can be defined	algorithm;central processing unit;communicating sequential processes;distributed memory;distributed operating system;graph (discrete mathematics);heuristic;mapper;network topology;parallel computing;simulated annealing;transputer;vertex (graph theory)	Jacques E. Boillat;Peter G. Kropf	1990		10.1007/3-540-53065-7_119	mathematical optimization;simulated annealing;computer science;machine learning;distributed computing;adaptive simulated annealing;metaheuristic	HPC	3.250555154945611	36.85219362752468	27679
9b0e64d991d1db9e07c892b3bdea22144bf04618	an algorithm to solve 3d guard zone computation problem		The guard zone computation problem finds vast applications in the field of VLSI physical design automation and design of embedded systems, where one of the major purposes is to find an optimized way to place a set of 2D blocks on a chip floor. Each (group of) circuit component(s) C i is associated with a parameter δ i , such that a minimum clearance zone of width δ i is to be maintained around C i . In this paper, we introduce the problem in its 3D version. Considering 3D simple solid objects makes the guard zone computation problem more complex and helps to solve many real life problems like VLSI physical design, Geographical Information System, motion control in robotics, and embedded systems. In this paper, we develop an algorithm to compute guard zone of a 3D solid object detecting and excluding overlapped regions among the guard zonal regions, if any.	algorithm;computation;computational problem	Ranjan Mehera;Piyali Datta;Arpan Chakraborty;Rajat Kumar Pal	2015		10.1007/978-81-322-2653-6_18	mathematical optimization	Robotics	13.852140347959958	52.10672076789629	27694
5ab6a7c0df0e5653c51904886fa76414e79fc4a8	thorough testing of any multiport memory with linear tests	fault simulation;multiport networks;time complexity;integrated memory circuits;circuit faults circuit testing built in self test spice costs interference read write memory computational modeling analytical models circuit simulation;memory consistency;circuit complexity;integrated circuit testing;random access storage;fault coverage;multiport networks integrated memory circuits random access storage integrated circuit testing fault simulation circuit complexity spice;fault model;spice;memory cell array multiport memory linear tests thorough testing fault coverage test length fault models p ports defect injection spice simulation single port faults two port faults time complexity spot defects weak faults optimal tests realistic models functional models shorts opens bridges fault probabilities	The quality of tests, in terms of fault coverage and test length, is strongly dependent on the used fault models. This paper presents realistic fault models for multiport memories with p ports, based on defect injection and SPICE simulation. The results show that the fault models for -port memories consist of p classes: single-port faults, two-port faults,..., -port faults. In addition, the paper discusses the test procedure for such memories; it shows that the time complexity of the required tests is not exponential proportionally with , as published by different authors, but it is linear, irrespective of the number of ports of which the multiport memory consists.	fault coverage;fault model;spice;simulation;software bug;time complexity	Said Hamdioui;Ad J. van de Goor	2002	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.980260	time complexity;circuit complexity;electronic engineering;parallel computing;real-time computing;fault coverage;computer science;stuck-at fault;automatic test pattern generation;fault model;algorithm	EDA	22.491684210898928	52.86639670341814	27701
8fa265a1a80d4cd2ecacc1c53508d19ad6ca3729	green supercomputing comes of age	energy efficiency;and green destiny;supercomputers large scale systems cooling energy efficiency power engineering and energy power engineering computing power system reliability power generation earth computational modeling;and green destiny green supercomputing energy efficiency supercomputing energy efficient supercomputing green computing hec large scale data center computing;energy efficient;large scale data center computing;power aware computing parallel machines;hec;energy efficient supercomputing;power aware computing;large scale;data center;green supercomputing;parallel machines;power consumption;power aware computing energy efficient supercomputing power consumption;green computing;supercomputing	Energy-efficient (green) supercomputing has traditionally been viewed as passe, even to the point of public ridicule. But today, it's finally coming into vogue. This article describes the authors' view of this evolution. Ignoring power consumption as a design constraint will result in supercomputing systems with high operational costs and diminished reliability. Fortunately, improving energy efficiency in supercomputers (and computer systems in general) has become an emergent task for the IT industry. Hopefully, this article inspires further innovations and breakthroughs by providing a historical perspective. From Green Destiny to BG/L to innovative power-aware supercomputing prototypes, we envision that holistic power-aware technologies will be available and largely exploited in most, if not all, future supercomputing systems.	design closure;emergence;holism;job control (unix);supercomputer	Wu-chun Feng;Xizhou Feng;Rong Ge	2008	IT Professional	10.1109/MITP.2008.8	computational science;supercomputer;parallel computing;computer science;theoretical computer science;efficient energy use;law	HPC	-4.289885807882268	48.607235979742065	27720
85237e7de0289999a3c27c69422cadd473d0d9ca	rational interpolation examples in performance analysis	performance evaluation randomised algorithms interpolation real time systems;rational interpolation;interpolation;performance evaluation;randomised algorithms;rare event simulation;reliability evaluation;interpolation performance analysis computational modeling real time systems analytical models application software algorithm design and analysis probability discrete event simulation computational complexity;real time systems rational interpolation examples performance analysis computer systems randomized algorithms fault probability calculation;performance analysis;randomized algorithm;real time systems	The rational interpolation approach has been applied to performance analysis of computer systems previously. In this paper, we demonstrate the effectiveness of the rational interpolation technique in the analysis of randomized algorithms and the fault probability calculation for some real-time systems.	interpolation;profiling (computer programming)	C. Liu;W.-B. Gong;C. Mani Krishna	2001	IEEE Trans. Computers	10.1109/12.954515	mathematical optimization;simulation;interpolation;computer science;theoretical computer science;randomized algorithm;algorithm	EDA	4.847223446606238	35.932343265089976	27747
e677af9b717a2d428e1cda14aa7c41a3eccd1f0f	packing techniques for virtex-5 fpgas	field programmable gate array;look up table;logic density;placement;performance;packing;fpgas;optimization;functional requirement;field programmable gate arrays;technology mapping;power	Packing is a key step in the FPGA tool flow that straddles the boundaries between synthesis, technology mapping and placement. Packing strongly influences circuit speed, density, and power, and in this article, we consider packing in the commercial FPGA context and examine the area and performance trade-offs associated with packing in a state-of-the-art FPGA---the Xilinx® VirtexTM-5 FPGA. In addition to look-up-table (LUT)-based logic blocks, modern FPGAs also contain large IP blocks. We discuss packing techniques for both types of blocks. Virtex-5 logic blocks contain dual-output 6-input LUTs. Such LUTs can implement any single logic function of up to 6 inputs, or any two logic functions requiring no more than 5 distinct inputs. The second LUT output has reduced speed, and therefore, must be used judiciously. We present techniques for dual-output LUT packing that lead to improved area-efficiency, with minimal performance degradation. We then describe packing techniques for large IP blocks, namely, block RAMs and DSPs. We pack circuits into the large blocks in a way that leverages the unique block RAM and DSP layout/architecture in Virtex-5, achieving significantly improved design performance.	boolean algebra;digital signal processor;elegant degradation;field-programmable gate array;lookup table;random-access memory;set packing	Taneem Ahmed;Paul D. Kundarewich;Jason Helge Anderson	2009	TRETS	10.1145/1575774.1575777	embedded system;parallel computing;computer science;field-programmable gate array	EDA	-0.2554774720848692	53.120785550174844	27800
0c722671f66d148b2528cd102e9f6f36c433e94d	visual iot: ultra-low-power processing architectures and implications		Visual IoT is a rapidly growing usage based on rich visual sensing, processing, and analytics. One approach for addressing visual IoT challenges is to move some computation closer to the edge device where data is captured. This article begins with a description of three key implications in ultra-low-power visual edge processing: the data footprint is constrained due to SRAM power, the available power-efficient computation is limited, and the ability to process large-scale data is challenging. To explore suitable approaches, the authors review three case studies: small-scale visual recognition for digits and characters, medium-scale visual recognition for hand gestures, and large-scale visual processing requiring video summarization. They show that co-designing algorithms and architectures for ultra-low-power processing in edge devices helps address the key challenges.	algorithm;computation;edge detection;edge device;low-power broadcasting;static random-access memory	Vui Seng Chua;Julio Zamora-Esquivel;Anindya Sao Paul;Thawee Techathamnukool;Carlos Flores Fajardo;Nilesh Jain;Omesh Tickoo;Ravi Iyer	2017	IEEE Micro	10.1109/MM.2017.4241343	visual processing;real-time computing;edge device;static random-access memory;automatic summarization;visualization;feature extraction;computation;computer science;analytics	Visualization	-0.48347146566517185	59.41528169851437	27873
dac70ac8e706beb8a450723cdf5589886d06ca83	energy reduction by built-in body biasing with single supply voltage operation	mosfet circuits;size 65 nm energy reduction single supply voltage operation energy efficiency lsi industry built in body biasing technique pmosfet nmosfet independent body biases built in body bias generation circuits bbg circuits aes cipher modules decipher modules dynamic voltage scaling scheme cell based design application circuit area overhead reduction post silicon minimum energy consumption;system on chip;energy consumption;threshold voltage;mosfet circuits energy conservation integrated circuit design large scale integration;integrated circuit modeling;energy consumption ciphers mosfet circuits threshold voltage integrated circuit modeling system on chip delays;ciphers;adaptive body biasing body biasing energy per cycle reduction;delays	Energy-efficiency has become the driving force of today's LSI industry. In order to achieve minimum energy operation of LSI, we propose a built-in body biasing technique which generates independent body biases for nMOSFET and pMOSFET separately. We design and fabricate an application circuit integrated with our proposed built-in body bias generation (BBG) circuits in a 65-nm process. The application circuit consists of AES cipher and decipher modules. The BBG does not require an external supply and it is compatible with a dynamic voltage scaling scheme for the application circuit. Cell-based design of the BBG circuit has been applied to facilitate automatic place and route. Both of the AES and the BBG circuits have been routed simultaneously to reduce design and area overhead. In post-silicon, supply voltage and body bias voltages are selected to achieve the minimum energy consumption for a target frequency. From the measurement results, more than 20% of energy reduction is achieved compared with adjusting supply voltage alone.		Norihiro Kamae;Islam A. K. M. Mahfuzul;Akira Tsuchiya;Tohru Ishihara;Hidetoshi Onodera	2015	Sixteenth International Symposium on Quality Electronic Design	10.1109/ISQED.2015.7085421	system on a chip;embedded system;electronic engineering;computer science;engineering;electrical engineering;threshold voltage	EDA	17.3874448380512	57.48347275115432	27908
03c6c393dc5f02a1d6e2666334b237d69db1452f	randomized pursuit-evasion in graphs	graphe non oriente;non directed graph;mammalia;lapin;games on graphs;vertebrata;strategie joueur;hunter vs rabbit game;estrategia jugador;jeu 2 personnes;connected graph;upper bound;juego 2 personas;grafo no orientado;directed graph;graphe oriente;lagomorpha;two person game;grafo orientado;rabbit;graphe connexe;lower bound;player strategy;conejo;grafo conexo	We analyze a randomized pursuit-evasion game on graphs. Thi game is played by two players, ahunterand arabbit. LetG be any connected, undirected graph with n nodes. The game is played in rounds and in each round both the hunter and the ra bbit re located at a node of the graph. Between rounds both the hunter and the rabbit can stay at he current node or move to another node. The hunter is assumed to be restrictedto the graphG: in every round, the hunter can move using at most one edge. For the rabbit we investigate two models: in one model the rabbit is restricted to the same graph as the hunter, and in the other model the rabbit isunrestricted, i.e., it can jump to an arbitrary node in every round. We say that the rabbit is caughtas soon as hunter and rabbit are located at the same node in a round. The goal of the hunter is to catch the rabbit in as few ro unds as possible, whereas the rabbit aims to maximize the number of rounds until it is caught. Give n a randomized hunter strategy for G, theescape lengthfor that strategy is the worst case expected number of rounds it takes the hunter to catch the rabbit, where the worst case is with re gards to all (possibly randomized) rabbit strategies. Our main result is a hunter strategy for g eneral graphs with an escape length of onlyO(n log(diam(G))) against restricted as well as unrestricted rabbits. This bo und is close to optimal since (n) is a trivial lower bound on the escape length in both models. F urthermore, we prove that our upper bound is optimal up to constant factors a gainst unrestricted rabbits.	best, worst and average case;call of duty: black ops;compile time;douady rabbit;evasion (network security);goto;graph (discrete mathematics);pursuit-evasion;randomized algorithm	Micah Adler;Harald Räcke;Naveen Sivadasan;Christian Sohler;Berthold Vöcking	2002		10.1007/3-540-45465-9_77	combinatorics;simulation;mathematics;upper and lower bounds;algorithm	Theory	18.107247772441973	33.236228399559145	27957
c5c37288a0c5a96a4f61dc55b6da313b8faa7310	hexagonal parallel pattern transformations	blow up;blowing up and shrinking;blowing up and shrinking hexagonal modular fields nearest neighbor logic parallel computer;nearest neighbor;modular fields;parallel computer;nearest neighbor logic;hexagonal	The concept of the two-dimensional (2-D) parallel computer with square module arrays was first introduced by Unger. It is the purpose of this paper to discuss the relative merits of square and hexagonal module arrays, to propose an operational symbolism for the various basic hexagonal modular transformations which may be performed by these comupters, to illustrate some logical circuit implementation, and to describe a few elementary applications.	logic gate;parallel computing	Marcel J. E. Golay	1969	IEEE Transactions on Computers	10.1109/T-C.1969.222756	combinatorics;discrete mathematics;parallel computing;computer science;machine learning;mathematics;hexagonal crystal system;k-nearest neighbors algorithm	Visualization	17.498267426800343	42.79714228606957	28051
74bde44852d9742176fcfb8e1620c3d4da559960	low-power design of variable block-size ldpc decoder using nanometer technology	ieee 802 11;power 168 mw;codecs;iterative decoding;decoding;ieee 802 11n standard;nanometer technology;tdmp decoding;parity check codes;frequency 450 mhz;power 168 mw low power design variable block size ldpc decoder nanometer technology ldpc decoding smsa decoding algorithm tdmp decoding vlsi technology ieee 802 11 frequency 450 mhz temperature 349 48 k;tdmp;nanotechnology;variable block size;parity check codes decoding computer architecture very large scale integration pipelines concurrent computing routing prototypes clocks frequency;tdmp ldpc nanometer vlsi technology ieee 802 11n;temperature 349 48 k;variable block size ldpc decoder;low power;power dissipation;pipelines;vlsi technology;wireless lan block codes codecs nanotechnology parity check codes vlsi;parallel computer;vlsi;nanometer vlsi technology;low power design;smsa decoding algorithm;wireless lan;ldpc decoding;block codes;ldpc;ieee 802 11n;throughput;hardware	This paper presents a low-power, variable block-size and irregular LDPC decoding. Our proposed LDPC decoder uses nanometer technology running the well-known TDMP and SMSA decoding algorithm. We further improved the design with pipeline structure, parallel computation and without any memory unit. Therefore, we can utilize only one routing network to route three different block-size data. The prototype architecture is being implemented on 90 nm VLSI technology. Because this VLSI technology has multi-Vth layers, we can make the design more effective. Compared to recent state-of-the-art architectures, the proposed variable block-size LDPC decoder has 450 MHz clock frequency, 349.48 K gate counts, 168 mW power dissipation, and 1.215 Gbps throughput.	algorithm;block size (cryptography);clock rate;computation;computer memory;data rate units;low-density parity-check code;low-power broadcasting;parallel computing;pipeline (computing);prototype;routing;throughput;very-large-scale integration;whole earth 'lectronic link	Chih-Hung Lin;Alex Chien-Lin Huang;Robert Chen-Hao Chang;Kuang-Hao Lin	2010	Proceedings of 2010 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2010.5537590	block code;throughput;ieee 802.11;electronic engineering;codec;parallel computing;real-time computing;low-density parity-check code;soft-decision decoder;computer science;dissipation;pipeline transport;very-large-scale integration;statistics	Arch	11.034383396015755	45.25239261910187	28063
dad18b1a3d880a5c8ffa055b0bbb8f8db8eced06	automatic testing of analog ics for latent defects using topology modification		An automatic, defect-oriented method is proposed for activating latent defects in analog and mixed-signal integrated circuits. Based on the topology modification technique, added stress transistors generate voltage stress that activates these latent defects. This contrasts with burn-in testing which uses increased temperatures as a fault activation mechanism. Moreover, this Design-for-Testability algorithm gives the trade-off between fault activation rate, silicon area cost and testing time for different test solutions. Both CMOS and DMOS devices are handled to accommodate the testing of high-voltage circuits. When applied to latent gate oxide defects in a voltage regulator circuit, an activation rate of up to 76.7% is achieved. In comparison, the stressing by increased supply voltage only reaches 28%. For the same testing time, this improvement comes at the expense of three additional stress transistors with a silicon area overhead of less than 1%.	algorithm;burn-in;cmos;design for testing;fits;gate oxide;mixed-signal integrated circuit;overhead (computing);pareto efficiency;software bug;transistor;voltage regulator	Nektar Xama;Anthony Coyette;Baris Esen;Wim Dobbelaere;Ronny Vanhooren;Georges G. E. Gielen	2017	2017 22nd IEEE European Test Symposium (ETS)	10.1109/ETS.2017.7968215	electronic engineering;mixed-signal integrated circuit;voltage regulator;computer science;gate oxide;logic gate;electronic circuit;integrated circuit;integrated injection logic;topology;cmos	EDA	21.238116353891204	54.94668603424193	28071
76fc763b0d006beb7229446c044a6484382f1793	the j-k gate	integrated circuit;ex or gate;programmable gate circuit;or gate;arithmetic logic unit;j k gate;inverter;digital circuits;nand gate;standard gate;logic circuit;latch;digital circuit design;digital circuit design ex or gate inverter j k gate latch logic circuit nand gate nimp gate or gate programmable gate circuit standard gate zero one true complement element;zero one true complement element;nimp gate	For many years two-input NAND, AND, NOR, OR and EX-OR gates have been commercially available as cheap quad TTL integrated circuits (IC's). Ideas on quad programmable two-input gates (three inputs per gate) have been published in the literature but have not reached the commercial stage [1]. However, complicated and expensive arithmetic logic units (ALU's) programmable to perform 16 different functions on two 4-bit arguments have become commercially available. The four sections of ALU's are not individually programmable.	4-bit;and gate;arithmetic logic unit;integrated circuit;nand gate;transistor–transistor logic	R. M. M. Oberman	1976	IEEE Transactions on Computers	10.1109/TC.1976.1674572	and-or-invert;real-time computing;nor logic;xor gate;logic gate;computer hardware;programmable logic array;computer science;integrated circuit;arithmetic logic unit;nand gate;or gate;digital electronics;inverter;algorithm	Visualization	15.941105419741266	45.432129379258534	28078
04d8eee6b1fc15bdcb51937d1ebd6e134f153191	a gpu based parallel hierarchical fuzzy art clustering	multiprocessors gpu parallel hierarchical fuzzy clustering art adaptive resonance theory;hierarchical clustering;pattern clustering;paper;computer graphic equipment;coprocessors;fuzzy set theory;nvidia geforce gtx 480;cuda;parallel architectures;clustering;nvidia;pattern clustering adaptive resonance theory computer graphic equipment coprocessors fuzzy set theory multiprocessing systems parallel architectures;multiprocessing systems;computer science;adaptive resonance theory;subspace constraints graphics processing unit kernel training programming educational institutions neural networks	Hierarchical clustering is an important and powerful but computationally extensive operation. Its complexity motivates the exploration of highly parallel approaches such as Adaptive Resonance Theory (ART). Although ART has been implemented on GPU processors, this paper presents the first hierarchical ART GPU implementation we are aware of. Each ART layer is distributed in the GPU's multiprocessors and is trained simultaneously. The experimental results show that for deep trees, the GPU's performance advantage is significant.	adaptive resonance theory;central processing unit;cluster analysis;graphics processing unit;hierarchical clustering	Sejun Kim;Donald C. Wunsch	2011	The 2011 International Joint Conference on Neural Networks	10.1109/IJCNN.2011.6033584	computer architecture;parallel computing;computer science;adaptive resonance theory;theoretical computer science;machine learning;hierarchical clustering;fuzzy set;cluster analysis;coprocessor	EDA	-1.9574594392360558	43.12217203252099	28091
263a06e9f38c07e7b14c4750c6110246bcbf53e4	reconfigurable nanowire transistors with multiple independent gates for efficient and programmable combinational circuits		We present MUX based programmable logic circuits built from newly proposed compact and efficient designs of combinational logic gate. These are enabled by reconfigurable Schottky barrier nanowire transistors with multiple independent gates, which can be dynamically switched between p- and n-type functionality. It will be shown that a single device can be used to replace paths of several transistors in series. This leads to topological differences and increased flexibility in circuit design. We found that especially complex functions, like Majority and Parity gates of many inputs, which are generally avoided in standard CMOS technology, benefit from the new device type. This can be exploited to directly map reconfigurable building blocks, e.g. dynamically switching NAND to NOR. Exemplary 6-functional logic circuits will be shown, which exhibit up to 80% reduction in transistor count, while maintaining the same functionality as compared to the CMOS reference design. Logical effort analysis indicates that 20% less circuit delay and 33% less normalized dynamic power consumption can be achieved.	blit (computer terminal);cmos;circuit design;combinational logic;logic gate;logical effort;nand logic;nor gate;programmable logic device;reconfigurable computing;reference design;schottky barrier;series and parallel circuits;transistor count	Jens Trommer;Andre Heinzig;Tim Baldauf;Thomas Mikolajick;Walter M. Weber;Michael Raitza;Marcus Völp	2016	2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)		and-or-invert;electronic engineering;nmos logic;nor logic;logic gate;programmable logic array;computer science;electrical engineering;programmable logic device;pass transistor logic;resistance;cmos;transistor;computer engineering	EDA	16.227554244778947	58.102293118287555	28139
bcacbd695ee3deed64ff4abf8be4f5dc5eb31a65	network modeling for distributed simulations of unbalanced power systems	power distribution system;distributed system;distributed computing;test bed;distributed computation;power system;network model;power flow;electric power;distributed simulation;distributed algorithm;power distribution systems;distributed control	With the increasing presence of distributed intelligence throughout power systems, the possibilities for distributed control and operation schemes are becoming progressively more attractive and feasible. Multi-phase distribution power flow is a tool which calculates the operating state of an electric power distribution system and is used to support all other planning and operation applications. This paper will derive models for the distributed analysis and simulation of distribution systems and will present their use in calculating the power flow solution using physically remote distributed processors.#R##N##R##N#To perform the power flow, distributed analysis models for multi-phase distribution systems have been developed and are embedded in a distributed algorithm. These include network partition models and equivalent source and load models which are used to represent each of the subsystems in the distributed analysis. A distributed processor test bed has been designed to test the performance of the models in distributed simulations. Results will be presented which validate the accuracy of the proposed models and algorithm.	ibm power systems;simulation;unbalanced circuit	Michael Kleinberg;Karen Miu;Chika O. Nwankpa	2007		10.1145/1357910.1358111	distributed algorithm;power-flow study;real-time computing;electric power;computer science;theoretical computer science;network model;brooks–iyengar algorithm;distributed computing;electric power system;distributed design patterns;testbed	HPC	6.066668263239233	35.63140019671476	28165
1ad3f5679d3a8b09b47e3742c16181e5aa366e5a	unified approach for performance evaluation and debug of system on chip at early design phase	libraries;analytical models;axi;performance evaluation;clocks;transactional level modelling axi bus cycle accurate latency systemc system on chip;computational modeling;ip networks computational modeling analytical models bandwidth performance evaluation clocks libraries;system on chip;event based systemc kernel performance evaluation system on chip debug early design phase system level debug bus cycle accurate hardware ip models transaction level modeling soc simulations computational delays timing features;bandwidth;ip networks;latency;transactional level modelling;system on chip computer debugging performance evaluation;bus cycle accurate;systemc	This paper proposes a novel approach for System Level Debug and Performance Evaluation that exploits the signal level and clock cycle accuracy existing in Bus Cycle Accurate hardware IP models along with the advantages of untimed Transaction Level Modeling. The developed toolset can be integrated in SoC simulations in a nonintrusive manner which secretly embeds performance figures and debug information in dumped simulation database at signal and transaction level. Proposed approach suggests modeling the SoC components with only functional accuracy in which the computational delays are added using the timing features provided by event based SystemC kernel. The components are modeled with clock cycle and signal level accuracy at the interface. Profiling results shows that the proposed approach outperforms several state-of-art methodologies in terms accuracy, adaptability and simulation speed by an order of magnitude of 102. The developed toolset can effectively be used in a co-simulation environment with IPs at different abstraction levels.	clock signal;co-simulation;debug;performance evaluation;profiling (computer programming);signal-to-noise ratio;simulation;system on a chip;systemc;transaction-level modeling	Nishit Gupta;Sunil Alag	2015	2015 Eighth International Conference on Contemporary Computing (IC3)	10.1109/IC3.2015.7346716	system on a chip;computer architecture;latency;parallel computing;real-time computing;computer science;operating system;computational model;bandwidth	EDA	-0.528318518318846	56.33963431578425	28202
2178c299bcf8f45660397b0ee70792f5bb027c04	dart: a dynamically reconfigurable architecture dealing with future mobile telecommunications constraints	low energy;dynamic reconfiguration;reconfigurable architectures;reconfigurable architectures energy consumption 3g mobile communication telecommunications gsm multiaccess communication arithmetic field programmable gate arrays computer architecture multimedia systems;multimedia systems;computer architecture;3g mobile communication;energy consumption;mobile telecommunication;arithmetic;field programmable gate arrays;gsm;high performance;telecommunications;multiaccess communication	In addition to the high performance requirements inherent to multimedia processings or to W-CDMA, future generation mobile telecommunications brings new constraints to the semiconductor design world. In fact, to support these processings, a system will have to be very flexible, in order to support the various algorithms allowed by the norm and the addition of new services, while keeping an energy consumption level compatible with the portability notion of this system. In order to associate high performances and low energy consumption in a flexible system, we developed a dynamically reconfigurable architecture called DART. The aim of this paper is to present this architecture and to estimate its level of performance and its adequacy with future generation mobile telecommunication systems.	algorithm;dart (programming language);performance;reconfigurability;reconfigurable computing;requirement;semiconductor;software portability	Raphaël David;Daniel Chillet;Sébastien Pillement;Olivier Sentieys	2002		10.1109/IPDPS.2002.1016554	gsm;embedded system;parallel computing;real-time computing;computer science;operating system;distributed computing;mobile station;field-programmable gate array;computer network	EDA	1.8556211382301504	57.8743162752328	28238
46a036142f7801b1cb5e54e3e45062a0cb177ebe	impact of interconnect variations on the clock skew of a gigahertz microprocessor	microprocessors;cmos technology;clocks;statistical independence;interconnect testing;wires;clocks integrated circuit interconnections microprocessors cmos technology timing delay estimation manufacturing wires permission capacitance;field programmable gate arrray;order reduction;permission;integrated circuit interconnections;manufacturing;context dependent;capacitance;clock skew;hierarchical test;delay estimation;timing	Due to the large die sizes and tight relative clock skew margins, the impact of interconnect manufacturing variations on the clock skew in today's gigahertz microprocessors can no longer be ignored. Unlike manufacturing variations in the devices, the impact of the interconnect manufacturing variations on IC timing performance cannot be captured by worst/best case corner point methods. Thus it is difficult to estimate the clock skew variability due to interconnect variations. In this paper we analyze the timing impact of several key statistically independent interconnect variations in a context-dependent manner by applying a previously reported interconnect variational order-reduction technique. The results show that the interconnect variations can cause up to 25% clock skew variability in a modern microprocessor design.	best, worst and average case;clock skew;context-sensitive language;heart rate variability;microprocessor;processor design;semiconductor device fabrication;spatial variability;variational principle	Ying Liu;Sani R. Nassif;Lawrence T. Pileggi;Andrzej J. Strojwas	2000		10.1145/337292.337365	independence;embedded system;electronic engineering;real-time computing;clock skew;computer science;engineering;context-dependent memory;timing failure;capacitance;manufacturing;cmos;interconnect bottleneck;statistics	EDA	22.329035659040002	57.68409143176106	28271
053790901f713c8fc032e7910a6774e4af9faef4	energy optimization of subthreshold-voltage sensor network processors	solar cells;cmos integrated circuits;sensor systems and applications;optimisation;235 mv microarchitectural energy optimization sensor network processor subthreshold voltage circuit design leakage power cpi efficiency spice level analysis memory architecture cmos bulk silicon solar cell 130 nm;microarchitecture;microprocessor chips program processors optimisation spice memory architecture solar cells cmos integrated circuits sensor fusion;sensor systems and applications application software computer networks circuit synthesis microarchitecture performance analysis memory architecture cmos process photovoltaic cells manufacturing;energy demand;application software;sensor network processor;235 mv;circuit design;cmos process;design space;sensor network;computer networks;energy optimization;leakage power;extremely high energy;memory architecture;spice level analysis;130 nm;microarchitectural energy optimization;manufacturing;performance analysis;photovoltaic cells;cpi efficiency;bulk silicon solar cell;sensor fusion;energy scavenging;spice;program processors;cmos;circuit synthesis;microprocessor chips;silicon solar cell;subthreshold voltage circuit design	Sensor network processors and their applications are a growing area of focus in computer system research and design. Inherent to this design space is a reduced processing performance requirement and extremely high energy constraints, such that sensor network processors must execute low-performance tasks for long durations on small energy supplies. In this paper, we demonstrate that subthreshold-voltage circuit design (400 mV and below) lends itself well to the performance and energy demands of sensor network processors. Moreover, we show that the landscape for microarchitectural energy optimization dramatically changes in the subthreshold domain. The dominance of leakage power in the subthreshold regime demands architectures that i) reduce overall area, ii) increase the utility of transistors, while iii) maintaining acceptable CPI efficiency. We confirm these observations by performing SPICE-level analysis of 21 sensor network processors and memory architectures. Our best sensor platform, implemented in 130nm CMOS and operating at 235 mV, only consumes 1.38 pJ/instruction, nearly an order of magnitude less energy than previously published sensor network processor results. This design, accompanied by bulk-silicon solar cells for energy scavenging, has been manufactured by IBM and is currently being tested.	cmos;central processing unit;circuit design;computer;mathematical optimization;microarchitecture;network processor;spice;sensor;solar cell;spectral leakage;transistor	Leyla Nazhandali;Bo Zhai;Javin Olson;Anna Reeves;Michael Minuth;Ryan Helfand;Sanjay Pant;Todd M. Austin;David Blaauw	2005	32nd International Symposium on Computer Architecture (ISCA'05)	10.1109/ISCA.2005.26	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;cmos	Arch	-1.7038732589575314	56.13001844637871	28284
29e23374ed983b549b839e2453c77af63a6750a4	a theory of error-rate testing	circuit faults circuit testing electrical fault detection fault detection error analysis system testing error free operation speech graphics digital communication;fault simulator;fault simulation;automatic test pattern generation;error tolerance;intelligible testing;test generation atpg error rate error tolerance intelligible testing;indexing terms;error tolerant application;chip;error rate testing theory;fault tolerant computing;digital communication;fault detection;logic testing;logic testing automatic test pattern generation combinational circuits error detection fault diagnosis fault tolerant computing;error rate;test generation;fanout free circuit;atpg procedure;error detection;combinational circuit;combinational circuit error rate testing theory error tolerant application fault detection fanout free circuit fault simulator atpg procedure intelligible testing;fault diagnosis;combinational circuits;atpg	We have entered an era where chip yields are decreasing with scaling. A new concept called intelligible testing has been previously proposed with the goal of reversing this trend for classes of systems which do not require completely error-free operation. Such error tolerant applications include audio, speech, graphics, video, and digital communications. Analyses of such applications have identified error rate as one of the key metrics for error severity, where error rate is defined as the percentage of vectors for which the value at outputs deviates from the corresponding error-free value. In error-rate testing, every fault with an error rate less than a threshold specified by the application is called an acceptable fault; all other faults are called unacceptable. The objective of error-rate testing is to detect every unacceptable fault while detecting none of the acceptable faults. In this paper we develop a theory of error-rate testing. First we study fanout-free circuits with primitive gates and identify new relationships between error rates and fault equivalence and dominance, develop a new test generation procedure, and prove that in such circuits it is possible to detect every unacceptable fault without detecting any acceptable fault. We then analyze more general circuits, including those containing complex gates and fanouts, and show that the above result may not hold for such circuits. We then use a modified version of a classical test generator and a classical fault simulator to obtain empirical data that show that even in arbitrary circuits, it is possible to detect every unacceptable fault while detecting only a fraction of acceptable faults.	error-tolerant design;fan-out;fault simulator;graphics;image scaling;reversing: secrets of reverse engineering;sensor;simulation;speech recognition;theory;turing completeness;video	Shideh Shahidi;Sandeep Gupta	2006	2006 International Conference on Computer Design	10.1109/ICCD.2006.4380853	reliability engineering;electronic engineering;real-time computing;fault coverage;computer science;stuck-at fault;automatic test pattern generation;fault model;combinational logic;algorithm	EDA	23.35971818094504	47.872051029812965	28297
1db8f2e376c5c0a3b93d97b612b296544aef9362	on supernode transformation with minimized total running time	distributed memory;minimizing running time;distributed memory systems;concurrent computing distributed computing polynomials cost function partitioning algorithms delay effects;parallel architectures distributed memory systems parallelising compilers;nested loops;parallelizing compilers;tiling;minimizing running time parallel program distributed memory parallel computer total execution time optimal supernode size supernode transformation tiling cutting hyperplanes supernode partitioning parallelizing compilers distributed memory multicomputer;supernode partitioning;parallel architectures;parallelising compilers;indexation;parallel computer;communication cost;distributed memory multicomputer;parallel programs	With the objective of minimizing the total execution time of a parallel program on a distributed memory parallel computer, this paper discusses how to nd an optimal supernode size and optimal supernode relative side lengths of a supernode transformation (also known as tiling). We identify three parameters of supernode transformation: supernode size, relative side lengths, and cutting hyperplane directions. For algorithms with perfectly nested loops and uniform dependencies, for suuciently large supernodes and number of processors, and for the case where multiple supernodes are mapped to a single processor, we give an order n polynomial whose real positive roots include the optimal supernode size. For two special cases: (1) two dimensional algorithm problems and (2) n-dimensional algorithm problems where the communication cost is dominated by the startup penalty and therefore, can be approximated by a constant, we give a closed form expression for the optimal supernode size, which is independent of the supernode relative side lengths and cutting hyperplanes. For the case where the algorithm iteration index space and the supernodes are hyperrectangular, we give closed form expressions for the optimal supernode relative side lengths. Our experiment shows a good match of the closed form expressions with experimental data.	approximation algorithm;central processing unit;dtime;distributed memory;iteration;parallel computing;polynomial;run time (program lifecycle phase);supernode (circuit);tiling window manager	Edin Hodzic;Weijia Shang	1998	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.679213	parallel computing;real-time computing;distributed memory;nested loop join;computer science;operating system;database;distributed computing;programming language	Theory	14.047456240646579	32.37979487213312	28338
d48fa51edc0a7d381cfe35b477a5da9abe1676e5	implementation and analysis of jpeg2000 system on a chip		This paper presents a novel implementation of the JPEG2000 standard as a system on a chip (SoC). While most of the research in this field centers on acceleration of the EBCOT Tier I encoder, this work focuses on an embedded solution for EBCOT Tier II. Specifically, this paper proposes using an embedded softcore processor to perform Tier II processing as the back end of an encoding pipeline. The Altera NIOS II processor is chosen for the implementation and is coupled with existing embedded processing modules to realize a fully embedded JPEG2000 encoder. The design is synthesized on a Stratix IV FPGA and is shown to out perform other comparable SoC implementations by 39% in computation time.	jpeg 2000;system on a chip	John M. McNichols;Eric J. Balster;William F. Turri;Kerry L. Hill	2012		10.1007/978-3-642-33191-6_54	system on a chip;network on a chip	Logic	11.255833993814818	41.01045783741454	28356
d01fbcf7c4a9728ad0f5fa4da3a21e04b3249cef	automated design tool execution in the ulysses design environment	automatic control;silicon;concepcion asistida;automated design;computer aided design;design automation;design process;integrated circuit;integrated circuit layout;leaf cell boundary conditions;routing;leaf cell boundary conditions ulysses design environment vlsi computer aided design tool integration design automation multiple partial designs design cycle silicon compilation task tool failures routing channel congestion;boundary conditions;very large scale integration;circuit vlsi;circuito integrado;design automation very large scale integration automatic control integrated circuit layout design methodology process design silicon routing boundary conditions user interfaces;tool failures;process design;vlsi circuit;design cycle;boundary condition;design environment;routing channel congestion;vlsi;conception assistee;multiple partial designs;ulysses design environment;circuit layout cad;tool integration;vlsi circuit layout cad;circuito vlsi;user interfaces;silicon compilation task;circuit integre;design methodology	Ulysses is a VLSI computer-aided-design (CAD) environment that effectively addresses the problems associated with CAD tool integration. Specifically, Ulysses allows for the integration of a collection of individual CAD tools into a design automation (DA) system that will execute a codified design methodology. Ulysses can track the multiple partial designs that result during a complete design cycle. Furthermore, Ulysses allows the designer to interrupt the design process at any time and take control. An example involving a silicon compilation task is presented to illustrate the ability of Ulysses to execute a sequence of CAD tools automatically to generate a viable layout for an IC. This example also illustrates Ulysses' ability to recover from CAD tool failures that may result if a layout cannot be completed due to routing channel congestion or overconstrained leaf-cell boundary conditions. >	design tool;ulysses iii	Michael L. Bushnell;Stephen W. Director	1989	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.21847	embedded system;electronic engineering;real-time computing;simulation;boundary value problem;computer science;engineering;electrical engineering;operating system;computer aided design;automatic control;very-large-scale integration	EDA	12.650056772083385	51.194569131750505	28358
947820898e372bc7952ec90d7d5c97417004beff	pr3: power efficient and low latency baseband processing for lte femtocells		In order to provide greater network capacity, the use of small base stations such as Femtocells has increased to allow higher spectrum reuse. In these Femtocells, base station designers have started to explore the use of general purpose multi-core architectures to provide greater flexibility. Multi-core architectures allow power-performance trade-off possibilities through techniques such as Dynamic Voltage Frequency Scaling (DVFS) and Power Gating. In this work, we propose a power management framework based on reinforcement learning called PR3, which uses both DVFS and Power Gating. Our approach is unique as it introduces a feedback from the network scheduler and baseband processor to the Power Governor, so that information about both the network and computation workloads are included in the decision making. Evaluation on a hardware platform (Odroid XU3) running PHY LTE uplink baseband processing benchmark, shows that PR3performs well in terms of both power and latency. It is able to save upto 50% power while maintaining low processing latency. PR3is also adaptive, making it effective over a wide range of traffic loads.	algorithm;baseband processor;benchmark (computing);best, worst and average case;compaq lte;computation;dynamic voltage scaling;frequency scaling;interrupt latency;multi-core processor;network scheduler;network traffic control;odroid-c2;phy (chip);power gating;power management;reinforcement learning;scheduling (computing);telecommunications link	Marco M&#x00FC;ller;Mun Choon Chan;Tulika Mitra	2018	IEEE INFOCOM 2018 - IEEE Conference on Computer Communications	10.1109/INFOCOM.2018.8486276	computer network;latency (engineering);network scheduler;latency (engineering);frequency scaling;power management;power gating;computer science;baseband processor;baseband	Mobile	-1.6529357539953633	59.600785898472374	28371
be0ec8042371add3267850d035e9da2bbd433736	dynamic boolean algebras	transition logic;logic simulators;boolean algebra;digital design;transition logic boolean algebra digital design logic simulators transient errors;transient errors	Switching algebra is unable to represent the dynamic behavior of digital circuits. There are several known methods for modeling the dynamics of circuits, using either multivalued algebras or specialized operators. None of them preserves the framework of switching algebra; therefore, existing analysis and synthesis methods developed by switching theory cannot be used.	boolean algebra;digital electronics;switching circuit theory	Sany M. Leinwand;T. Lamdan	1980	IEEE Transactions on Computers	10.1109/TC.1980.1675483	boolean algebra;boolean circuit;and-inverter graph;circuit minimization for boolean functions;discrete mathematics;boolean domain;boolean expression;product term;term algebra;theoretical computer science;boolean algebras canonically defined;mathematics;combinational logic;complete boolean algebra;interior algebra;digital electronics;algorithm;two-element boolean algebra;free boolean algebra;algebra	EDA	19.543458914219254	45.92691334413576	28377
e1b3d18146cd2bfbc34088a9ec54aec13cb2ad2b	neuromorphic cellular neural network processor for intelligent internet-of-things		We discuss the architecture, implementation and testing of a neuromorphic Cellular Neural Network (CNN) processor for intelligent IoT devices. The processor is based on a simplicial piecewise linear CNN architecture that allows implementation of linear and nolinear CNNs. A linear array of 64 processing element (PE) with column-shared computation resources, tightly coupled to two data memory caches was synthesized and fabricated in a 55nm CMOS technology using custom layout libraries. The fabricated chip achieves an overall performance of 2.95 TOPS/W with dynamic energy dissipation efficiency of 86.4fJ per OP at V=500mV. The processor can implement different types of processing on 2D data arrays, such as gray-scale morphology, gradient flow, median filters, and approximate Gaussian filters, among others.	approximation algorithm;artificial neural network;cmos;cellular neural network;charge-coupled device;clock gating;computation;computational resource;gradient;grayscale;library (computing);linear function;local variable;mathematical morphology;median filter;multi-core processor;network processor;neuromorphic engineering;nonlinear system;piecewise linear continuation;spectral leakage;static random-access memory;tops	Martin Villemur;Pedro Julián;Yasue Nishiyama;Andreas G. Andreou	2018	2018 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2018.8351668	chip;cellular neural network;electronic engineering;computation;piecewise linear function;gaussian;dissipation;computer science;computer hardware;cmos;neuromorphic engineering	Arch	5.128506632502736	42.47022769189185	28425
27768ab3a01fa70af3866acc5181b925a4822613	an automated methodology for memory-conscious mapping of dsp applications on coarse-grain reconfigurable arrays	minimisation;processing element;reconfigurable architectures;storage management;data reuse;digital signal processing bandwidth application software reconfigurable architectures energy consumption embedded system delay very large scale integration laboratories design engineering;2 dimensional;memory access;reconfigurable architecture;parallel architectures;digital signal processing chips;parallelism automated memory conscious mapping methodology dsp applications coarse grain reconfigurable arrays computational intensive applications data reuse data memory bandwidth minimization distributed foreground storage elements memory accesses execution time coarse grain reconfigurable architecture memory management power consumption;power consumption;coarse grained;parallel architectures reconfigurable architectures digital signal processing chips minimisation storage management power consumption;memory bandwidth	The paper presents a memory-conscious mapping methodology of computationally intensive applications on coarse-grain reconfigurable arrays. By exploiting the inherently abundant amounts of data reuse in DSP applications, the methodology tries to minimize the data memory bandwidth, which constitutes a major bottleneck for the applications performance. This is achieved by using the distributed foreground storage elements in the architecture and by properly placing operations in the processing elements. The methodology considers a realistic 2-dimensional coarse-grain reconfigurable architecture template which can model a large number of existing coarse-grain architectures. Experimental results show that memory accesses and execution time are reduced, since the mapping methodology efficiently exploits the data reuse opportunities. The need for taking into account memory bandwidth limitations is also illustrated.	digital signal processor;memory bandwidth;memory management;program transformation;reconfigurable computing;run time (program lifecycle phase)	Michalis D. Galanis;Grigoris Dimitroulakos;Constantinos E. Goutis	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1465388	embedded system;minimisation;computer architecture;two-dimensional space;parallel computing;real-time computing;computer science;operating system;mathematics;memory bandwidth;statistics	EDA	-0.11536389528741321	51.0603738470264	28428
a765642233336ffea97c23e147e3222c23026635	modeling and analysis for performance and power	energy efficiency;analytical models;efficient system;lattice quantum chromodynamics;performance evaluation;lattices;fast multipole method;power efficiency;energy and power modeling;computer architecture;dennard scaling;computational modeling;fast multipole method performance modeling energy and power modeling scientific computing sparse matrix vector multiply lattice quantum chromodynamics;feature extraction;sparse matrix vector multiply;feature extraction computational modeling analytical models computer architecture sparse matrices graphics processing unit lattices;scientific computing;constant power;architecture design;architectural features;graphics processing unit;application performance;efficient system application performance architectural features architecture design dennard scaling constant power power efficiency energy efficiency;performance modeling;sparse matrices	Accurately modeling application performance for specific architectures allows us to understand and analyze the impact of various architectural features on performance which will ultimately lead to improved performance and better architecture design choices for efficiency and scalability on future systems. However, with the end of Dennard scaling, processors can no longer maintain constant power per unit area as before and consequently power and energy efficiencies has become arguably an even more important factor than performance for future systems. Unfortunately, performance and power/energ efficiencies are not metrics that can be optimized separately, but are intricately dependent factors. Therefore, in order to design efficient systems that can meet both performance and energy/power requirements that are becoming ever more stringent, an acccurate performance-energy model is required so that we can analyze applications on given architectures in terms of both performance and power/energy. With such a model, we can better understand the impact of application and architectural features on performance and power and design applications and systems to maximize their efficiencies.	algorithm;blocking (computing);bottleneck (software);central processing unit;dennard scaling;dynamic voltage scaling;heuristic;high-level programming language;image scaling;iterative method;requirement;scalability	JeeWhan Choi;Richard W. Vuduc	2012	2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum	10.1109/IPDPSW.2012.304	parallel computing;electrical efficiency;sparse matrix;feature extraction;fast multipole method;computer science;theoretical computer science;lattice;efficient energy use;computational model	Arch	-2.8091123043498025	46.7317270097199	28456
993212ce8a4d20b1a66e4c7f962aa4a2466d8009	on an efficient closed form expression to estimate the crosstalk noise in the circuit with multiple wires	rc circuits crosstalk failure analysis integrated circuit interconnections integrated circuit noise logic circuits;multiple wires crosstalk noise coupled rc circuit;closed form expression;crosstalk noise;signal waveform;integrated circuit;crosstalk;multiple wires;logic circuits;chip;failure analysis;rc circuits;integrated circuit interconnections;0 18 micron;0 18 micron closed form expression crosstalk noise integrated circuit on chip wires signal waveform logic failures rc circuit;coupled;rc circuit;integrated circuit noise;logic failures;on chip wires;noise;crosstalk wires capacitance closed form solution integrated circuit interconnections noise level integrated circuit noise coupling circuits circuit simulation space vector pulse width modulation	In recent years, as the density of the components in the integrated circuit grows, the distance between the on-chip wires becomes smaller. The crosstalk is more important and will change the signal waveform or generate logic failures and then degrade the performance and the reality of the circuit. In this paper, we focus on the value of the crosstalk noise. We find a new closed-form expression to derive a general formula for n wires and take two wires and three wires for examples. We refer to the 0.18-mum technology and utilize our approach to estimate the crosstalk noise. As a result, our average error percentage is below 10%	crosstalk;integrated circuit;quadratic formula;waveform	Po-Hao Chang;Jia-Ming Chen;Chao-Ying Shen	2006	APCCAS 2006 - 2006 IEEE Asia Pacific Conference on Circuits and Systems	10.1109/APCCAS.2006.342429	rc circuit;embedded system;electronic engineering;telecommunications;engineering;electrical engineering;control theory	EDA	22.333548241412647	56.25277707730727	28523
997458851bd363d428702da9328e26238bf5df53	dram industry trend	fabrication;random access memory;biographies;demand and supply;shipbuilding industry;random access memory shipbuilding industry fabrication textile industry demand forecasting biographies marketing and sales electricity supply industry;electricity supply industry;process migration;textile industry;marketing and sales;demand forecasting	This presentation starts with DRAM market overview, demand side DRAM bit shipment, content per box trend, followed by the DRAM density migration and the technology migration trend, including process migration, from micrometer to nanometer technology. As technology advances, 300mm fabrication and new generation products become the centerpiece of the future development of the DRAM industry. We present here worldwide 300mm capacity development forecast and the transition of DDR, DDR2, and DDR3, with a brief introduction of DDR3 features and advantages. Then we summarize the demand and supply trend of the DRAM industry. Finally, we conclude our presentation with the historical DRAM cell development and the comparison between Trench and Stack technologies.	cell (microprocessor);double data rate;dynamic random-access memory;process migration	Pei-Lin Pai	2006	2006 IEEE International Workshop on Memory Technology, Design, and Testing (MTDT'06)	10.1109/MTDT.2006.11	process migration;textile industry;demand forecasting;engineering;supply and demand;shipbuilding;fabrication	Arch	12.308683806445732	58.417128356382555	28548
3c7e65bca692a4304068d1ecc9dd54eefcdda96a	fixed-point square roots	digital signal processing;convergence;dsp fixed point square root bit precision;newton raphson based square rooting method fixed point square roots arithmetic operation dsp algorithms fixed point dsp processors finite wordlength effect convergence rate;newton raphson method;drntu engineering electrical and electronic engineering;fixed point;conference paper;program processors digital signal processing convergence chebyshev approximation table lookup signal processing algorithms;bit precision;fixed point arithmetic;digital signal processing chips;square root;chebyshev approximation;signal processing algorithms;newton raphson method digital signal processing chips fixed point arithmetic;table lookup;program processors;dsp	Square root (SQRT) is a common arithmetic operation used in many DSP algorithms. In this paper, we evaluate square rooting methods suitable for implementation on fixed-point (FxP) DSP processors with a fast multiplying unit. The finite wordlength effect on the square rooting methods is highlighted, and it is shown that the theoretically derived convergence rate for the Newton-Raphson (NR) based square rooting methods are not suitable for FxP processor. Also, the most efficient methods for 8-bit and 16-bit FxP processors are identified.	16-bit;8-bit;algorithm;central processing unit;digital signal processor;fixed point (mathematics);fixed-point arithmetic;methods of computing square roots;newton's method;noise reduction;rate of convergence	Abhishek Seth;Woon-Seng Gan	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288231	mathematical optimization;computer science;electrical engineering;theoretical computer science;digital signal processing;mathematics	Robotics	13.862474235679052	44.098838229616454	28603
ee84f36d453583fee92a66eff0689cd9982e8756	exploiting parallelism in configurable architectures through custom array mapping	storage allocation;parallelisme;field programmable gate array;evaluation performance;arquitectura red;compilateur;performance evaluation;integrated circuit;execution time;internal memory;reconfigurable architectures;evaluacion prestacion;storage allocation field programmable gate arrays memory architecture parallel architectures reconfigurable architectures;kernel codes;data access pattern;circuito integrado;red puerta programable;architecture reseau;compiler;custom array mapping;carta de datos;reseau porte programmable;circuit a la demande;algorithme;algorithm;codificacion;configurable architecture;custom circuit;parallelism;circuito integrato personalizado;paralelismo;parallel architectures;loop based computation;chemin critique;memory architecture;critical path;mappage;coding;parallel architecture parallelism configurable architecture custom array mapping storage allocation loop based computation internal memory data access pattern memory bandwidth kernel codes field programmable gate array;temps execution;mapping;network architecture;parallel architecture;field programmable gate arrays;allocation memoire;tiempo ejecucion;asignacion memoria;memory bandwidth;circuit integre;compilador;recorrido critico;codage;algoritmo	Configurable architectures offer the unique opportunity of customising the storage allocation to meet specific applications' needs. A compiler approach to map the arrays of a loop-based computation to internal memories of a configurable architecture with the objective of minimising the overall execution time is described. An algorithm that considers the data access patterns of the arrays along the critical path of the computation as well as the available storage and memory bandwidth is presented. Experimental results are presented which demonstrate the application of this approach for a set of kernel codes when targeting a field-programmable gate-array. The results reveal that the proposed algorithm outperforms the naive and custom data layout techniques by an average of 33% and 15% in terms of execution time, while taking into account the available hardware resources.	parallel computing	Nastaran Baradaran;Pedro C. Diniz	2007	IET Computers & Digital Techniques	10.1049/iet-cdt:20060181	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;field-programmable gate array	EDA	-0.3427680514123704	52.39429259152489	28675
8886c39c7849788627d3489cfdf7e9a89c6dc27d	a 3-way simd engine for programmable triangle setup in embedded 3d graphics hardware	hardware design languages;interpolation;cmos technology;optimized format converter;166 mhz;motion pictures;49 1 mw;rasterization engine;computer graphics;conference;166 mhz rasterization engine triangle pixel data conversion linear interpolation triangle rasterizer 3 way simd architecture programmable triangle setup engine embedded 3d graphics hardware programmable tse division free rasterizer matrix vector multiplication optimized format converter cmos 0 13 micron 49 1 mw;computer graphic equipment;cmos digital integrated circuits computer graphic equipment computer graphics interpolation parallel processing programmable circuits embedded systems matrix multiplication circuit optimisation;embedded system;embedded systems;performance improvement;programmable tse;engines;cmos digital integrated circuits;energy consumption;linear interpolation;0 13 micron;embedded 3d graphics hardware;programmable circuits;triangle rasterizer;programmable triangle setup engine;division free rasterizer;matrix converters;3 way simd architecture;matrix multiplication;power consumption;circuit optimisation;frequency;engines graphics cmos technology embedded system matrix converters frequency interpolation hardware design languages energy consumption motion pictures;3d graphics;cmos;matrix vector multiplication;parallel processing;graphics;triangle pixel data conversion	A triangle setup engine (TSE) generates setup parameters for rasterization in 3D graphics hardware. Since the TSE must be flexible for various embedded systems, a programmable TSE is proposed, especially optimized for a division-free rasterizer and it shows a performance improvement up to 4.76 times, derived by a 3-way SIMD architecture with special instructions for matrix-vector multiplication. It is integrated with an optimized format converter in 0.13 /spl mu/m CMOS technology. Its gate count is 210 kgates and the power consumption is 49.1 mW at 166 MHz operation frequency.	3d computer graphics;cmos;embedded system;game engine;gate count;glossary of computer graphics;graphics hardware;mathematical optimization;matrix multiplication;operand forwarding;rasterisation;simd;the matrix	Kyusik Chung;Donghyun Kim;Lee-Sup Kim	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1465643	embedded system;parallel processing;electronic engineering;computer hardware;matrix multiplication;computer science;operating system;cmos	Arch	11.994028826125897	41.76059216855165	28682
d6072d396b922a07f751d181c5a0e2d13b849d6c	a pin permutation algorithm for improving over-the-cell channel routing	computer aided design;maximal channel density pin permutation algorithm over the cell channel routing standard cell design greedy algorithm gate positions terminal positions;integrated circuit layout;etude theorique;routing greedy algorithms very large scale integration terminology polynomials computer applications software engineering;routing;gate positions;very large scale integration;enchufe electrico;pin connexion;greedy algorithms;terminal positions;software engineering;structure tunnel;maximal channel density;polynomials;computer applications;cellular arrays;network routing;experimental result;permutation;circuit layout cad network routing cellular arrays logic cad vlsi integrated circuit layout;fiche electrique;standard cell design;permutacion;estudio teorico;resultado experimental;vlsi;conception assistee;greedy algorithm;circuit layout cad;terminology;encaminamiento;over the cell channel routing;channel structure;theoretical study;resultat experimental;logic cad;estructura tunel;integrated circuits;circuit integre;acheminement;pin permutation algorithm	In standard cell de.wgn, some of the cell terminals and gates are permutable. Therefore, it is important for an over-the-cell channel router to take advantage of this so as to obtain better results. A dynamic programming based algorithm M presented to determine proper gate and ierminai positions such that, when overthe-ce[l routers are used, the area above and below the channel can be utilized more effectively and the channel density can be greatly reduced. Experimental results show that our proposed algorithm indeed considerably reduces the channel density.	algorithm;channel router;dynamic programming;router (computing);routing;standard cell	Cliff Yungchin Hou;C. Y. Roger Chen	1992		10.1109/43.402506	routing;electronic engineering;greedy algorithm;computer science;electrical engineering;theoretical computer science;computer aided design;very-large-scale integration;engineering drawing;algorithm;computer network	EDA	15.589010990432717	50.56591504581793	28737
4ec6afbb566b68a6f9a390e13936e0cc6eb681ad	a tightly-coupled multi-core cluster with shared-memory hw accelerators	system on chip application program interfaces multiprocessing systems shared memory systems synchronisation;program processors synchronization acceleration registers computer architecture hardware hardware design languages;synchronisation;shared memory systems;system on chip;application program interfaces;multiprocessing systems;multiprocessor system on chip technology tightly coupled multicore cluster shared memory hw accelerators tightly coupling hardware accelerators mpsoc platforms accelerator definition streamlining accelerator instantiation architectural templates run time techniques communication cost minimization synchronization cost minimization tightly coupled processors hardware processing units zero copy communication api hwpu	Tightly coupling hardware accelerators with processors is a well-known approach for boosting the efficiency of MPSoC platforms. The key design challenges in this area are: (i) streamlining accelerator definition and instantiation and (ii) developing architectural templates and run-time techniques for minimizing the cost of communication and synchronization between processors and accelerators. In this paper we present an architecture featuring tightly-coupled processors and hardware processing units (HWPU), with zero-copy communication. We also provide a simple programming API, which simplifies the process of offloading jobs to HWPUs.	application programming interface;central processing unit;hardware acceleration;high-level synthesis;mpsoc;multi-core processor;shared memory;shattered world;universal instantiation;zero-copy	Masoud Dehyadegari;Andrea Marongiu;Mohammad Reza Kakoee;Luca Benini;Siamak Mohammadi;Nasser Yazdani	2012	2012 International Conference on Embedded Computer Systems (SAMOS)	10.1109/SAMOS.2012.6404162	computer architecture;parallel computing;real-time computing;computer science	EDA	-1.8225244813398576	49.13833030734992	28900
71d5f2e599cd336f6482e0c72b90728fd43a766a	vlsi architecture design of weighted mode filter for full-hd depth map upsampling at 30fps	histograms;random access memory;complexity theory;memory management;vlsi computer vision filters sram chips;histograms random access memory engines hardware complexity theory memory management throughput;vlsi architecture weighted mode filter depth upsampling;engines;size 40 nm one the fly maximum finding source spreading static random access memory tsmc technology histogram processing large window source data access sram bandwidth real time depth upsampling engine computer vision application high resolution depth map full hd depth map upsampling weighted mode filter vlsi architecture design;throughput;hardware	High-resolution depth maps are necessary for advanced computer vision applications but difficult to generate on portable devices. In this paper, we aim to provide a realtime depth upsampling engine using weighted mode filtering to alleviate the hardware requirement in such scenarios. An appropriate filter window size is essential for both performance and complexity, and extensive experiments are conducted to choose an 8×8 window. The design bottlenecks are then mainly twofold: high SRAM bandwidth due to large-window source data access and complex histogram processing for a high depth-label count up to 128. These issues were addressed by two proposed techniques accordingly: source spreading and one-the-fly maximum finding. Based on the synthesis results using TSMC 40nm technology, the proposed architecture can provide Full-HD depth upsampling at 43 fps with 247k logic gates and 5.4 kbytes of SRAM.	computer vision;data access;depth map;experiment;image resolution;logic gate;mobile device;overhead (computing);source data;static random-access memory;upsampling;very-large-scale integration	Li-De Chen;Yu-Ling Hsiao;Chao-Tsung Huang	2016	2016 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2016.7538865	embedded system;throughput;electronic engineering;real-time computing;computer science;histogram;statistics;memory management	EDA	12.872388601451439	40.25635120671061	29022
9a304993a76f8d05d366f52a75be3517c4995bd9	dynamic thermal clock skew compensation using tunable delay buffers	spatial temperature distribution;distribucion espacial;timing behavior;campo temperatura;thermal nonuniformities;haute performance;tunable circuit;concepcion sistema;clocks;clock network;performance;thermal gradient;compensation thermique;circuit accordable;temporal variation;temperature aware design methodology algorithms designi performance clock tree clock skew tunable delay buffers;high performance circuits;buffer circuits;algorithme;algorithm;temperature induced delay variations dynamic thermal clock skew compensation tunable delay buffers thermal gradients high performance circuits timing behavior hold setup constraints spatial temperature distribution clock network thermal nonuniformities clock tree;repartition spatiale;integrated circuit design;temperature aware design methodology;thermal behavior;champ temperature;thermal compensation;comportamiento termico;spatial distribution;temperature induced delay variations;clock tree;thermal design;system design;delay circuits;ligne retard;tunable delay buffers;designi;alto rendimiento;linea retardo;horloge;algorithms;dynamic thermal clock skew compensation;compensacion termica;circuito acordable;temps retard;delay time;clock tree design;thermal gradients;clocks delay tunable circuits and devices temperature sensors integrated circuit interconnections temperature distribution power dissipation energy management thermal management buffer storage;temperature;hold setup constraints;tiempo retardo;high performance;clock;clock skew;temperature distribution buffer circuits clocks delay circuits integrated circuit design;temperature distribution;conception systeme;reloj;delay line;comportement thermique;algoritmo;design methodology	The thermal gradients existing in high-performance circuits may significantly affect their timing behavior, in particular by increasing the skew of the clock net and/or altering hold/setup constraints, possibly causing the circuit to operate incorrectly. The knowledge of the spatial distribution of temperature can be used to properly design a clock network that is able to compensate such thermal non-uniformities. However, re-design of the clock network is effective only if temperature distribution is stationary, i.e., does not change over time. In this work, we specifically address the problem of dynamically modifying the clock tree in such a way that it can compensate for temporal variations of temperature. This is achieved by exploiting the buffers that are inserted during the clock network generation, by transforming them into tunable delay elements. Temperature-induced delay variations are then compensated by applying the proper tuning to the tunable buffers, which is computed off-line and stored in a tuning table inserted in the design. We propose an algorithm to minimize the number of inserted tunable buffers, as well as their tunable range (which directly relates to complexity). Results show that clock skew is kept within original bounds with minimum area and power penalty. The maximum increase in power is 23.2% with most benchmarks exhibiting less than 5% increase in power.	algorithm;benchmark (computing);clock network;clock signal;clock skew;gradient;hot swapping;online and offline;stationary process	Ashutosh Chakraborty;Karthik Duraisami;Ashoka Visweswara Sathanur;Prassanna Sithambaram;Luca Benini;Alberto Macii;Enrico Macii;Massimo Poncino	2006	ISLPED'06 Proceedings of the 2006 International Symposium on Low Power Electronics and Design	10.1145/1165573.1165612	clock;electronic engineering;real-time computing;design methods;temperature;performance;clock domain crossing;clock skew;computer science;engineering;control theory;timing failure;clock drift;synchronous circuit;clock gating;digital clock manager;temperature gradient;integrated circuit design;cpu multiplier;systems design	EDA	17.95606227852026	55.281702753091665	29033
274edb61c680d283ad6d18d14dafa9d958a99b76	a methodology for power-aware pipelining via high-level performance model evaluations	software optimizations;optimisation;kernel;power aware pipelining;high level performance model evaluation;code optimization;multiprocessor systems;pipeline processing multiprocessing systems optimisation;resource management;chip multiprocessor;pipeline processing energy consumption design optimization software design embedded software software performance multiprocessing systems monitoring design methodology engines;cpu power consumption;memory access;engines;estimation;chip multiprocessors;performance model;schedules;cpu power consumption power aware pipelining high level performance model evaluation embedded software multiprocessor system systemc based power model;systemc based power model;multiprocessing systems;design space exploration;memory allocation;power consumption;chip multiprocessors design space exploration power modeling software optimizations;multiprocessor system;power demand;power modeling;pipeline processing;embedded software	Power is one of the major constraints considered during the design of embedded software. In order to reduce power consumption without sacrificing performance, software needs to be optimized in order to run as efficiently as possible on a given platform. When attempting to optimize the mapping of a piece of software on a multiprocessor system, designers often face the chicken-and-egg problem of whether to schedule tasks first, or do memory allocation first, as either step will affect the different optimization opportunities the other may provide. Because each optimization will affect the system’s power consumption, it is critically important to be able to monitor the effects these transformations have. In this paper we present a methodology that allows designers to quickly evaluate the impact each code optimization will have in the system’s power. Our exploration engine relies on SystemC-based power/performance models to quickly and accurately evaluate the dynamic power due to memory accesses as well as the expected CPU power consumption.	cpu power dissipation;central processing unit;computer memory;embedded software;embedded system;level of detail;loss function;mathematical optimization;memory management;multiprocessing;parallel computing;pipeline (computing);program optimization;systemc;throughput	Luis Angel D. Bathen;Yongjin Ahn;Nikil D. Dutt;Sudeep Pasricha	2009	2009 10th International Workshop on Microprocessor Test and Verification	10.1109/MTV.2009.19	embedded system;estimation;computer architecture;parallel computing;kernel;real-time computing;embedded software;schedule;computer science;resource management;operating system;program optimization;programming language;memory management	EDA	-3.03592150941919	53.80338653853692	29062
e04a254d2246753ba7c257b451236c68a387187a	reconoc: a reconfigurable network-on-chip	field programmable gate array;topology;dynamic reconfiguration;network on chip;routing;reconfigurable architectures;run time reconfiguration;fpga;system on a chip;automatic generation;network topology;integrated circuit design;dynamic data;run time reconfigurable;software reconfiguration reconoc design reconfigurable network on chip noc topology xilinx virtex 2 pro fpga tmap dynamic datafolding toolflow;technology and engineering;logic testing;dynamic data folding;reconfigurable architectures field programmable gate arrays integrated circuit design logic testing network topology network on chip;field programmable gate arrays;tmap;dynamic data folding fpga run time reconfiguration network on chip tmap;table lookup;reconfigurable hardware;field programmable gate arrays topology network topology routing delay table lookup system on a chip	This article presents the design of RecoNoC: a compact, highly flexible FPGA-based network-on-chip (NoC), that can be easily adapted for various experiments. In this work, we enhanced this NoC with dynamically reconfigurable shortcuts. These can be used to alter the NoC's topology to adapt to the system's communication needs. The design has been implemented and tested on a Xilinx Virtex-2 Pro FPGA, using the TMAP dynamic datafolding toolflow to automatically generate the reconfigurable hardware and the software reconfiguration procedures. The results show that, using dynamic datafolding, the overhead of introducing this shortcut mechanism is limited.	experiment;field-programmable gate array;keyboard shortcut;network on a chip;overhead (computing);reconfigurability	Robbe Vancayseele;Brahim Al Farisi;Wim Heirman;Karel Bruneel;Dirk Stroobandt	2011	6th International Workshop on Reconfigurable Communication-Centric Systems-on-Chip (ReCoSoC)	10.1109/ReCoSoC.2011.5981529	embedded system;computer architecture;parallel computing;computer science;network on a chip;field-programmable gate array	EDA	6.197056978662745	55.240010187448775	29064
d3c451aafd895846e08948f74304d50995975c32	efficient charge assignment and back interpolation in multigrid methods for molecular dynamics	multigrid method;molecular dynamics;multigrid methods;molecular dynamic;back interpolation	The assignment of atomic charges to a regular computational grid and the interpolation of forces from the grid back to the original atomic positions are crucial steps in a multigrid approach to the calculation of molecular forces. For purposes of grid assignment, atomic charges are modeled as truncated Gaussian distributions. The charge assignment and back interpolation methods are currently bottlenecks, and take up to one-third the execution time of the multigrid method each. Here, we propose alternative approaches to both charge assignment and back interpolation where convolution is used both to map Gaussian representations of atomic charges onto the grid and to map the forces computed at grid points back to atomic positions. These approaches achieve the same force accuracy with reduced run time. The proposed charge assignment and back interpolation methods scale better than baseline multigrid computations with both problem size and number of processors.		Sanjay Banerjee;John A. Board	2005	Journal of computational chemistry	10.1002/jcc.20220	mathematical optimization;molecular dynamics;chemistry;theoretical computer science;computational chemistry;physics;multigrid method;quantum mechanics	HPC	-4.313302919535814	37.49361373033014	29073
169226869611330b518b0e6427e2e9e590bffc64	flexfilm - an image processor for digital film processing	004;digital film fpga reconfigurable stream based architechture weak programming sdram controller qos communication centric communication scheduli	Digital film processing is characterized by a resolution of at least 2K (2048x1536 pixels per frame at 30 bit/pixel and 24 pictures/s, data rate of 2.2 GBit/s); higher resolutions of 4K (8.8 GBit/s) and even 8K (35.2 GBit/s) are on their way. Real-time processing at this data rate is beyond the scope of today's standard and DSP processors, and ASICs are not economically viable due to the small market volume. Therefore, an FPGA-based approach was followed in the FlexFilm project. Different applications are supported on a single hardware platform by using different FPGA configurations.#R##N##R##N#The multi-board, multi-FPGA hardware/software architecture is based on Xilinx Virtex-II Pro FPGAs which contain the reconfigurable image stream processing data path, large SDRAM memories for multiple frame storage and a PCI express communication backbone network. The FPGA-embedded CPU is used for control and less computation intensive tasks.#R##N##R##N#This paper will focus on three key aspects: a) the used design methodology which combines macro component configuration and macro-level floorplanning with weak programmability using distributed microcoding, b) the global communication framework with communication scheduling and c) the configurable, multi-stream scheduling SDRAM controller with QoS support by access prioritization and traffic shaping.#R##N##R##N#As an example, a complex noise reduction algorithm including a 2.5 dimensions DWT and a full 16x16 motion estimation at 24 fps requiring a total of 203 Gops/s net computing performance and a total of 28 Gbit/s DDR-SDRAM frame memory bandwidth will be shown.	image processor	Sven Heithecker;Amilcar do Carmo Lucas;Rolf Ernst	2006			embedded system;parallel computing;real-time computing;computer science	EDA	1.411327534422502	48.65228228147143	29076
2d7420e8312e7c8b6fa524f56b17c0bea87d3da0	address bus encoding techniques for system-level power optimization	vlsi;circuit optimisation;integrated circuit design;microprocessor chips;redundancy;i/o interfaces;address bus encoding techniques;address streams;bus line transitions;complex vlsi circuits;global power;power budget;switching activity;system-level address buses;system-level power optimization	The power dissipated by system-level buses is the largest contribution to the global power of complex VLSI circuits. Therefore, the minimization of the switching activity at the I/O interfaces can provide significant savings on the overall power budget. This paper presents innovative encoding techniques suitable for minimizing the switching activity of system-level address buses. In particular, the schemes illustrated here target the reduction of the average number of bus line transitions per clock cycle. Experimental results, conducted on address streams generated by a real microprocessor, have demonstrated the effectiveness of the proposed methods.	address bus;bus encoding;clock signal;input/output;mac address;microprocessor;power optimization (eda);very-large-scale integration	Luca Benini;Giovanni De Micheli;Donatella Sciuto;Enrico Macii;Cristina Silvano	1998			embedded system;electronic engineering;parallel computing;real-time computing;electronic design automation;computer science;electrical engineering;very-large-scale integration	EDA	15.745449930676378	54.725253321596945	29176
91b898c6410956215f2ae45e5f9f864f894e5dbd	seamless high speed simulation of vhdl components in the context of comprehensive computing systems using the virtual machine faumachine	electronic engineering computing;hardware description languages;hardware-software codesign;virtual machines;faumachine;hdl simulation;pci sound card;vhdl components;computing systems;hardware-software testing;manufacturing process;seamless high speed simulations;software development;virtual machine	Testing the interaction between hard- and software is only possible once prototype implementations of the hardware exist. HDL simulations of hardware models can help to find defects in the hardware design. To predict the behavior of entire software stacks in the environment of a complete system, virtual machines can be used. Combining a virtual machine with HDL-simulation enables to project the interaction between hard- and software implementations, even if no prototype was created yet. Hence it allows for software development to begin at an earlier stage of the manufacturing process and helps to decrease the time to market. In this paper we present the virtual machine FAUmachine that offers high speed emulation. It can co-simulate VHDL components in a transparent manner while still offering good overall performance. As an example application, a PCI sound card was simulated using the presented environment.	emulator;hardware description language;prototype;seamless3d;simulation;software development;sound card;vhdl;virtual machine	Stefan Potyra;Matthias Sand;Volkmar Sieh;Dietmar Fey	2010	Proceedings of the 2010 Winter Simulation Conference		embedded system;computer architecture;real-time computing;computer science;virtual machine;virtual finite-state machine	HPC	4.71906110919366	51.40134840877678	29220
298f5560c1bda6381e5ebb9f7b5bc55e26655b80	atomic computing - a different perspective on massively parallel problems		As the size of parallel computing systems inexorably increases, the proportion of resource consumption (design effort, operating power, communication and calculation latency) absorbed by ‘non-computing’ tasks (communication and housekeeping) increases disproportionally. The SpiNNaker (Spiking neural net architecture) engine [1,2] sidesteps many of these issues with a novel architectural model: it is an isotropic ‘mesh’ of (ARM9) cores, connected via a hardware communication network. The topology allows uniform scalability up to a hard limit of just over a million cores, and the communications network – hardware handling packets of 72 bits – achieves a bisection bandwidth of 5 billion packets/s. The state of the machine is maintained in over 8TB of 32-bit memory, physically distributed throughout the system. There is no central processing ‘overseer’ or synchronised clock. This paper discusses opportunities and challenges in applying the SpiNNaker architecture, within neural simulation and beyond.		Andrew D. Brown;Rob Mills;Jeffrey S. Reeve;Kier Dugan;Steve B. Furber	2013		10.3233/978-1-61499-381-0-334	parallel computing;real-time computing;computer science;operating system;distributed computing;algorithm	HPC	-2.288538883163933	47.45706212745744	29238
b646f8e744c2f95fe15396e2398b9d52d4e8e8dd	automated test system design based on tellus for in-vehicle can network	power supplies;software;circuit faults;virtual instrumentation automatic test software controller area networks field buses protocols relays vehicular ad hoc networks;testing;vehicles;ets automated test system design in vehicle can network electrical modules passenger cars beijing automotive technology co batc ipc 610l vehicle protocol tester can bus interference generator can spider programmable power supply self made relay module digital i o cards diagnostic testing tool electrical tester system tellus software computer software labview ecu network tests car models network node test;relays;testing and validation in vehicle can network automated test system design;testing software hardware vehicles power supplies relays circuit faults;hardware	Aiming at the requirements of high efficiency, high reliability in testing electrical modules of in-vehicle network, Based on CAN network environment of passenger-cars in Beijing Automotive Technology Co., Ltd.(BATC), by means of IPC-610L, vehicle protocol tester Tellus™ Pro(Tellus), CAN bus interference generator CAN Spider, programmable power supply, self-made relays module, Digital I/O cards, diagnostic testing tool Electrical Tester System(ETS), and by programming with Tellus software and other upper computer software on the platform of LabVIEW, an automated test system used for CAN network nodes in vehicles is designed. It can be used for various ECU network tests, diagnostic tests from different car models. After conducting a series of network node test on the system, the validity of this test system is then verified.	can bus;communications protocol;device under test;engine control unit;input/output;interference (communication);labview;power supply;rationalfunctionaltester;relay;requirement;system integration;systems design;test automation	Xinhong Yang;Zhihua Yu;Mu Xiao;Guangbin Ji;Zhijie Wang	2014	2014 6th International Congress on Ultra Modern Telecommunications and Control Systems and Workshops (ICUMT)	10.1109/ICUMT.2014.7002089	embedded system;real-time computing;computer science;operating system;software testing;computer security;computer network	Embedded	8.486418567383305	53.74182213415712	29267
34650fccb0dfff1d23cb6b8e45d0cb21ae3c6b95	data criticality estimation in software applications	circuit faults;fault tolerant;application software;protection;redundancy;software safety;digital systems;fault tolerance;timing analysis;application software redundancy fault tolerance circuit faults digital systems hardware frequency single event upset protection software safety;single event upset;frequency;fault injection;hardware	In safety-critical applications it is often possible to exploit software techniques to increase system’s faulttolerance. Common approaches are based on data redundancy to prevent data corruption during the software execution. Duplicating most critical variables only can significantly reduce the memo ry and performance overheads, while still guaranteeing very good results in terms of fault-tolerance improvement. This paper presents a new methodology to compute the criticality of variables in target software applications. Instead of resorting to time consuming fault injection experiments, the proposed solution is based on the runtime analysis of the variables’ behavior logged during the execution of the target application under different workloads.	analysis of algorithms;criticality matrix;data redundancy;experiment;fault injection;fault tolerance;self-organized criticality	Alfredo Benso;Stefano Di Carlo;Giorgio Di Natale;Paolo Prinetto;Luca Tagliaferri	2003		10.1109/TEST.2003.1270912	reliability engineering;embedded system;fault tolerance;parallel computing;real-time computing;n-version programming;computer science;engineering;stuck-at fault;software reliability testing;operating system;software fault tolerance	SE	6.862704041837502	59.63508740059869	29303
b54047b88a4fd66705f910017d7b843be74e4362	application of on-line arithmetic algorithms to the svd computation: preliminary results	arithmetic signal processing algorithms large scale integration singular value decomposition logic arrays matrix decomposition costs logic design delay concurrent computing;online singular value decomposition online arithmetic radix 2 floating point online operations;singular value decomposition;digital arithmetic;floating point	A scheme for the singular value decomposition (SVD) problem, based on online arithmetic, is discussed. The design, using radix-2 floating-point online operations, implemented in the LSI HCMOS gate-array technology, is compared with a compatible conventional arithmetic implementation. The preliminary results indicate that the proposed online approach achieves a speedup of 2.4-3.2 with respect to the conventional solutions, with 1.3-5.5 more gates and more than 6 times fewer interconnections. >		Paul K.-G. Tu;Milos D. Ercegovac	1991		10.1109/ARITH.1991.145568	discrete mathematics;arbitrary-precision arithmetic;computer science;floating point;theoretical computer science;operating system;saturation arithmetic;mathematics;singular value decomposition;algorithm	Theory	12.004340918998071	44.210989122009856	29316
c06520c8800bcd13cd0f089b81980829373acf4b	extending standard cell library for aging mitigation	internal node signal probabilities extending standard cell library aging mitigation transistor aging bias temperature instability bti nanoscale technology nodes circuit delay standard cells timing margin reduction nonuniform extension input signal probabilities realistic workloads;realistic workloads;input signal probabilities;internal node signal probabilities;aging mitigation;standard cells;extending standard cell library;bti;transistors;transistor aging;bias temperature instability;circuit delay;timing margin reduction;nonuniform extension;nanoscale technology nodes	Transistor aging, mostly due to bias temperature instability (BTI), is one of the major unreliability sources at nano-scale technology nodes. BTI causes the circuit delay to increase and eventually leads to a decrease in the circuit lifetime. Typically, standard cells in the library are optimised according to the design time delay; however, because of the asymmetric effect of BTI, the rise and fall delays might become significantly imbalanced over the lifetime. In this study, the BTI effect is mitigated by balancing the rise and fall delays of the standard cells at the excepted lifetime. The authors find an optimal tradeoff between the increase in the library size and the lifetime improvement by non-uniform extension of the library cells for various ranges of the input signal probabilities. The simulation results reveal that this technique can prolong the circuit lifetime by around 150% with a negligible area overhead. Moreover, the effect of different realistic workloads on the distribution of internal node signal probabilities is investigated. This is done to obtain the sensitivity of the proposed static (design time) approach to different workloads during system lifetime. The results show that the proposed approach is still efficient if the workload changes during the runtime.	standard cell	Saman Kiamehr;Mojtaba Ebrahimi;Farshad Firouzi;Mehdi Baradaran Tahoori	2015	IET Computers & Digital Techniques	10.1049/iet-cdt.2014.0142	embedded system;electronic engineering;real-time computing;engineering;electrical engineering;transistor	EDA	19.56087402207269	59.12758730536913	29317
d237c30db9548af939eca732fe7df93e48fd060d	prop: a recursive paradigm for area-efficient and performance oriented partitioning of large fpga netlists	large fpga netlists;total number;resynthesis;minimum lower bound;critical path delay;optimization;minimum number;paradigm prop;consistent reduction;partitioning;recursive paradigm;logic node;total device cost;fpga;device partition;prop paradigm;lower bound;critical path;field programmable gate arrays;recursive partitioning;sequential circuits	In this paper, we introduce a new recursive partitioning paradigm PROP which combines (p)artitioning, (r)eplication, (o)ptimization, to be followed by another recursion of (p)artitioning, etc. We measure the quality of partitions in terms of total device cost, logic and terminal utilization, and critical path delay. Traditionally, the minimum lower bound into which a given netlist can be partitioned is determined by disregarding the logic interconnect while distributing the logic nodes into a minimum number of devices. PROP paradigm challenges this assumption by demonstrating feasible partitions of some large netlists such that the number of device partitions is smaller than minimum lower bounds postulated initially. Overall, we report consistent reductions in the total number of partitions for a wide range of combinational and sequential circuit benchmarks while, on the average, reducing critical path delay as well.	combinational logic;critical path method;decision tree learning;disk partitioning;field-programmable gate array;netlist;programming paradigm;recursion;sequential logic	Roman Kuznar;Franc Brglez	1995		10.1145/224841.225132	embedded system;electronic engineering;real-time computing;computer science;theoretical computer science;field-programmable gate array	EDA	14.328770148496574	48.78900967520209	29326
e68ce75b5d5e3dfbd7ee07cbbf765c77e18bdcd6	strongly diagnosable systems under the comparison diagnosis model	strongly t diagnosable system;multiprocessor interconnection networks;graph theory;network problems diagnostics topology diagnostics graph theory;matching composition network;topology;reliability;graph theory t diagnosable system strongly t diagnosable system fault diagnosis interconnection network matching composition network;multiprocessor systems;multiprocessor interconnection networks fault diagnosis graph theory;testing;diagnosability;interconnection network;computational modeling;comparison diagnosis model;network problems;t diagnosable;diagnostics;strongly;multiprocessing systems;t diagnosable system;strongly t diagnosable;multiprocessor interconnection fault diagnosis multiprocessing systems computational modeling testing reliability;matching composition networks;multiprocessor interconnection;fault diagnosis	A system is t-diagnosable if all faulty nodes can be identified without replacement when the number of faults does not exceed t, where t is some positive integer. Furthermore, a system is strongly t-diagnosable if it is t-diagnosable and can achieve (t+1)-diagnosable except for the case where a node's neighbors are all faulty. In this paper, we propose some conditions for verifying whether a class of interconnection networks, called matching composition networks (MCNs), are strongly diagnosable under the comparison diagnosis model.	interconnection;verification and validation	Sun-Yuan Hsieh;Yu-Shu Chen	2008	IEEE Transactions on Computers	10.1109/TC.2008.104	parallel computing;real-time computing;computer science;graph theory;reliability;distributed computing;software testing;computational model;statistics	Robotics	23.80705513685127	44.22955023336356	29341
9a3ce3d24b3e70bb067cc260bc43bb99613f2935	the impact of dynamic voltage and frequency scaling on multicore dsp algorithm design [exploratory dsp]	digital signal processing;signal processing algorithms multicore processing hardware digital filters digital signal processing accuracy;dynamic voltage;frequency scaling;computational resource allocation;computer architecture;integrated circuit design;accuracy;engineering and technology;teknik och teknologier;parallel architectures;multicore dsp algorithm design;energy saving dynamic voltage frequency scaling multicore dsp algorithm design digital signal processing multicore processor optimizing algorithm parallel core computer architecture computational resource allocation transmission resource allocation;parallel architectures circuit optimisation digital signal processing chips integrated circuit design multiprocessing systems;multicore processing;digital filters;digital signal processing chips;optimizing algorithm;multicore processor;multiprocessing systems;circuit optimisation;signal processing algorithms;parallel core;transmission resource allocation;energy saving;hardware	We connect to the two recent IEEE Signal Processing Magazine special issues on digital signal processing (DSP) on multicore processors (November 2009 and March 2010) and address an issue that was not addressed in the articles there, which we believe has important consequences for DSP algorithm design in the future. The basic observation that we start out with is that in DSP algorithm design, there is very often a tradeoff between the computational effort spent (in terms of the number of operations) and the quality/accuracy of the algorithm output. Herein we discuss the problems that emerge when optimizing algorithms for circuits that support the operation of several parallel cores that operate at different speeds. The understanding, formulation, and solution of these optimization problems require a cross-disciplinary approach that models the interplay between circuits, computer architecture, signal processing, and optimization. While allocation of computational and transmission resources for the purpose of saving energy is an established research topic in other fields (sensor network life time maximization being a notable example), there appears to be relatively little open literature on the type of problems that we discuss here. Taken together, we believe that the challenges we pose are important and that the signal processing algorithm design community is well positioned to tackle them.	algorithm design;central processing unit;computation;computer architecture;digital signal processing;dynamic frequency scaling;expectation–maximization algorithm;exploratory testing;mathematical optimization;multi-core processor	Erik G. Larsson;Oscar Gustafsson	2011	IEEE Signal Processing Magazine	10.1109/MSP.2011.940410	multi-core processor;computer architecture;parallel computing;real-time computing;computer science;operating system	Arch	-1.248010502604436	54.99354837223045	29385
3a1e9e9990d008ea7ac3da982750efe3754b430b	applicability of using internal gpgpus in industrial control systems	kernel;elektroteknik och elektronik;performance evaluation;sensors;electrical engineering electronic engineering information engineering;production engineering computing control engineering computing graphics processing units industrial control multiprocessing systems parallel processing;computer and information science;graphics processing units benchmark testing performance evaluation kernel process control sensors;real time industrial control gpgpu;graphics processing units;process control;data och informationsvetenskap;parallelism internal gpgpu applicability industrial control systems high end embedded cpu multiple cores integrated graphics processors;benchmark testing	Industrial control systems are continuously increasing in functionality, connectivity, and levels of integration, and as a consequence they require more computational power. At the same time, these systems have specific requirements related to cost, reliability, timeliness, and thermal power dissipation, which put restrictions on the hardware and software used. Today the high-end embedded CPUs not only provide multiple cores, but also integrated graphics processors (GPU) at close to no additional cost. The use of GPUs for general processing have several potential values in industrial control systems; 1) the added computational power and the high parallelism could pave way for new functionality and 2) the integrated GPU could potentially replace other hardware and thereby reduce the overall cost. In this paper we investigate the applicability of using integrated GPUs in industrial control systems. We do this by evaluating the performance of GPUs with respect to computational problem types and sizes typically found in industrial control systems. In the end we conclude that GPUs are no obvious match for industrial control systems and that several hurdles remain before a wide adoption can be motivated.	central processing unit;computation;computational problem;control system;embedded system;general-purpose computing on graphics processing units;graphics processing unit;parallel computing;requirement	Markus Lindgren;Kristian Sandström;Thomas Nolte;Daniel Hallmans	2014	Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA)	10.1109/ETFA.2014.7005096	embedded system;benchmark;computer architecture;parallel computing;kernel;real-time computing;computer science;sensor;instrumentation and control engineering;operating system;process control;control theory;programming language	Arch	4.366974101285187	57.37438132405355	29387
a32f7adc5c1995fcf06978bd709c73aca04e878b	design rule checking and vlsi	verification;equipment;concepcion asistida;topology;computer aided design;metodologia;integrated circuit;systeme fast mask;regle production;performance;materiel;topologie;circuit vlsi;design rule checking;circuito integrado;modo exploracion;methodologie;topologia;systeme conversationnel;configuracion geometrica;algorithme;algorithm;algorritmo;vlsi circuit;interactive system;equipo;sistema conversacional;conception assistee;tolerancia dimensional;procesador;systeme parallele;parallel system;rendimiento;circuito vlsi;verificacion;methodology;processeur;scanning mode;mode balayage;processor;circuit integre;sistema paralelo;configuration geometrique;production rule;geometrical configuration;regla produccion;tolerance dimensionnelle;dimensional tolerance	This paper describes a parallel connection machine architecture which is capable of assisting with the DRC of VLSI circuits. Because of the systems architecture a significant performance improvement can be expected from our machine.	algorithm;computer science;connection machine;exception handling;james r. wait;sed;series and parallel circuits;simulation;systems architecture;very-large-scale integration	O. P. Traynor;J. R. Malone	1987	Integration	10.1016/0167-9260(87)90020-4	embedded system;electronic engineering;verification;performance;computer science;engineering;design rule checking;integrated circuit;computer aided design;methodology;engineering drawing;algorithm	AI	17.855725591743038	38.83768096042412	29405
1537135fd4454cc49ac55a86146030c20e6167ab	reliability analysis of nonrepairable cold-standby systems using sequential binary decision diagrams	logic gates boolean functions data structures fault trees discrete fourier transforms redundancy;sequence dependence;sequential bdd sbdd;dynamic fault tree dft;fault tree;system reliability;system structure;state space methods;system component failure parameters nonrepairable cold standby systems sequential binary decision diagrams real world systems cold standby redundancy power resources fault tolerance cold standby units faulty online component replacement state space based methods simulation based methods exclusion based methods state space explosion problem analytical method combinatorial reliability analysis sbdd model reliability evaluation expression time to failure distributions system structure;fault tolerant;state space methods binary decision diagrams fault tolerance maintenance engineering redundancy;boolean functions;state space based methods;system dynamics;state space explosion problem;nonrepairable cold standby systems;boolean function;maintenance engineering;sbdd model;cold standby redundancy;binary decision diagram bdd;simulation based methods;sequential bdd sbdd binary decision diagram bdd cold standby system dynamic fault tree dft sequence dependence;binary decision diagrams;redundancy;logic gates;data structures;analytical method;state space;fault tolerance;reliability evaluation expression;cold standby units;discrete fourier transform;time to failure distributions;real world systems;reliability analysis;exclusion based methods;cold standby system;faulty online component replacement;discrete fourier transforms;power resources;logic gate;state space explosion;data structure;combinatorial reliability analysis;fault trees;sequential binary decision diagrams;system component failure parameters;binary decision diagram	Many real-world systems, particularly those with limited power resources, are designed with cold-standby redundancy for achieving fault tolerance and high reliability. Cold-standby units are unpowered and, thus, do not consume any power until needed to replace a faulty online component. Cold-standby redundancy creates sequential dependence between the online component and standby components; in particular, a standby component can start to work and then fail only after the online component has failed. Traditional approaches to handling the cold-standby redundancy are typically state-space-based or simulation-based or inclusion/exclusion-based methods. Those methods, however, have the state-space explosion problem and/or require long computation time particularly when results with a high degree of accuracy are desired. In this paper, we propose an analytical method based on sequential binary decision diagrams (SBDD) for combinatorial reliability analysis of nonrepairable cold-standby systems. Different from the simulation-based methods, the proposed approach can generate exact system reliability results. In addition, the system SBDD model and reliability evaluation expression, once generated, are reusable for the reliability analysis with different component failure parameters. The approach has no limitation on the type of time-to-failure distributions for the system components or on the system structure. Application and advantages of the proposed approach are illustrated through several case studies.	binary decision diagram;computation;dynamical system;failure cause;fault tolerance;hot spare;markov chain;portable document format;programming tool;reliability engineering;simulation;state space;time complexity;triple modular redundancy;world-system	Liudong Xing;Ola Tannous;Joanne Bechta Dugan	2012	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2011.2170415	fault tolerance;real-time computing;fault tree analysis;data structure;logic gate;computer science;boolean function;algorithm	SE	24.08604832601029	45.81219459018176	29436
acf4e3f5e98265d59d538a4db3b087f9d298434f	vlsi/wsi designs for folded cube-connected cycles architectures	cube connected cycles;reliability;fault tolerant;building block;reconfigurable architectures;layout;indexing terms;wsi design vlsi design folded cube connected cycles architecture parallel architecture layout redundancy reconfigurable fccc universal building block fault tolerant fccc;yield;fault tolerant computing;redundancy;parallel architectures;very large scale integration hypercubes parallel architectures redundancy fault tolerance wires;fault tolerance;hypercube networks parallel architectures vlsi wafer scale integration redundancy reconfigurable architectures fault tolerant computing;wafer scale integration;vlsi;parallel architecture;building block design;hypercube networks	This paper presents VLSI/WSI designs for a recently introduced parallel architecture known as the folded cube-connected cycles ( F C C C ) . We first discuss two layouts for the FCCC, in which there is no component redundancy. Then we incorporate redundancy, and present locally and globally reconfigurable FCCCs. We also discuss the design of universal building blocks for the construction of fault-tolerant FCCCs of various dimensions.	cube;cube-connected cycles;elegant degradation;fault tolerance;parallel computing;very-large-scale integration;wafer-scale integration	M. P. Sebastian;P. S. Nagendra Rao;Lawrence Jenkins	1996		10.1109/ICVD.1996.489610	fault tolerance;computer architecture;parallel computing;computer science;theoretical computer science	EDA	13.699987838240958	48.97484888148437	29543
9621993b55dea6313055397dd6d0e585f94807d8	a register file with transposed access mode	computer architecture discrete cosine transforms video signal processing;video signal processing;video processing;discrete cosine transform;computer architecture;discrete cosine transforms;discrete cosine transforms registers convolution partitioning algorithms discrete wavelet transforms hardware computer aided instruction laboratories computer architecture acceleration;register file;transposition steps register file architecture transposed access mode partitioned instructions column wise processing 2d separable image video processing 2d convolution 2d discrete cosine transform	2D convolution and 2D transforms, such as wavelet transform and discrete cosine transform (DCT), are widely used in image and video processing. To reduce the computation complexity, these algorithms are often implemented in two separable passes of 1D processing (e.g., row-wise processing followed by column-wise processing). For example, the number of multiplications of a direct N x N 2D DCT is N, while it is 2N if two passes of 1D DCTs are used.	algorithm;computation;convolution;discrete cosine transform;operand;register allocation;register file;video processing;wavelet transform	Yoochang Jung;Stefan G. Berg;Donglok Kim;Yongmin Kim	2000		10.1109/ICCD.2000.878341	parallel computing;transform coding;lapped transform;computer hardware;computer science;theoretical computer science;discrete cosine transform;video processing;register file;algorithm	Arch	11.563520691275318	39.25450439748229	29545
43e505e1b7f11e4c0db5ec7a061f0f7397c1c3b2	small-world architecture for parallel processors	avarage path length;small world architecture csp synchronization method network architecture small world network short path communication parallel architecture vlsi multiprocessor network parallel processing cpu module parallel processors;vlsi multiprocessor network;avarage path length vlsi multiprocessor network a small world network global clock csp synchronization method;clocks;very large scale integration;computer architecture;synchronization;vlsi microprocessor chips multiprocessing systems parallel architectures synchronisation;global clock;a small world network;program processors computer architecture very large scale integration synchronization peer to peer computing clocks;peer to peer computing;program processors;csp synchronization method	The CPU module is composed of networks including a lot of multiprocessors, and the parallel processing is done between such processors. The most important elements in a VLSI multiprocessor network are the component of networks. The key in the network is to execute the communication between a lot of nodes faultlessly while securing the scalability. In this paper, we introduce a parallel architecture with short path communication, featuring a random connection in Small World Network, and present network architecture without global clock using communicative process by CSP synchronization method.	apl;central processing unit;manycore processor;multi-core processor;multiprocessing;network architecture;parallel computing;scalability;simulation;very-large-scale integration	Hideki Mori;Minoru Uehara	2016	2016 30th International Conference on Advanced Information Networking and Applications Workshops (WAINA)	10.1109/WAINA.2016.44	synchronization;computer architecture;parallel computing;real-time computing;telecommunications;computer science;operating system;very-large-scale integration	HPC	1.2589381235562127	60.059244189685984	29567
820eead78e4fd17c0884b3eae2af1b11a9c320fc	relieving the burden of track switch in modern hard disk drives	metodo caso peor;evaluation performance;posicionamiento;multimedia;performance evaluation;asymmetry;evaluacion prestacion;simulation;simulacion;multimedia application;asymetrie;disco duro;hard disk;sector geometry;positioning;disk drive;track align;audio and video;performance model;lector disco;methode cas pire;asimetria;track switch;computer hardware;point of view;lecteur disque;hard disk drive;worst case method;materiel informatique;disque dur;hardware;positionnement	In this work, we propose a novel hard disk technique, “AV Disk”, for modern multimedia applications. Modern hard disk drives adopt complex sector layout mechanisms to reduce track and head switch overhead. While these complex sector layout mechanism can reduce average overhead involved in the track and head switch, they bring larger variability in the overhead. From a multimedia application’s point of view, it is important to minimize the worst case I/O latency rather than to improve the average IO latency. We focus our effort to minimize track switch overhead as well as the variability in track switch overhead involved in disk I/O. We propose that track of the hard disk drive is aligned with a certain IO size. In this work, we develop an elaborate performance model with which we can compute the optimal IO unit size for multimedia applications. We propose that hard disk controller is responsible for positioning data blocks in the hard disk platter in such a manner that I/O units are not placed across the track boundaries, where a single I/O unit has size of 32–128 KByte. Optimal IO unit size is used in aligning the tracks in hard disk drives. We develop Skewed Sector Sparing technique in aligning a track with a given IO size. However, when the I/O unit for alignment is increased to 128 KByte, 17% of the disk space becomes unusable. Despite the decreased storage area, track aligning technique increases the overall performance of the hard disk. According to our simulation-based experiment, overall disk performance increases about 5–25%. Given that capacity of hard disk increases 100% every year, we cautiously regard it as reasonable tradeoff to increase the I/O latency of the disk.		Jongmin Gim;Youjip Won	2010	Multimedia Systems	10.1007/s00530-010-0218-5	embedded system;hard disk drive performance characteristics;real-time computing;disk sector;computer hardware;telecommunications;computer science;electrical engineering;operating system;disk mirroring;logical disk;asymmetry	OS	-1.8583000296051857	54.82050266022731	29572
4e1c97f841bfeac067363d51c8aa47d1a492558c	a new sbst algorithm for testing the register file of vliw processors	vliw processor;intrinsic faults;memory occupation register file testing vliw processors feature size reduction nanometer technology devices software based self test approaches logic defect detection cross bar switch architecture intrinsic faults very long instruction word processors fault simulation execution cycles;register file testing;circuit faults;fault simulation;logic defect detection;vliw processors;software based self test;testing;vliw;computer architecture;parallel architectures electronic engineering computing fault simulation file organisation instruction sets logic testing multiprocessing systems;built in self test;memory occupation;registers vliw program processors computer architecture circuit faults built in self test hardware;parallel architectures;fault simulation testing software based self test very long instruction word processors;registers;logic testing;very long instruction word;nanometer technology devices;fault coverage;register file;electronic engineering computing;execution cycles;multiprocessing systems;software based self test approaches;program processors;cross bar switch architecture;feature size reduction;very long instruction word processors;instruction sets;hardware;file organisation	Feature size reduction drastically influences permanent faults occurrence in nanometer technology devices. Among the various test techniques, Software-Based Self-Test (SBST) approaches have been demonstrated to be an effective solution for detecting logic defects, although achieving complete fault coverage is a challenging issue due to the functional-based nature of this methodology. When VLIW processors are considered, standard processor-oriented SBST approaches result deficient since not able to cope with most of the failures affecting VLIW multiple parallel domains. In this paper we present a novel SBST algorithm specifically oriented to test the register files of VLIW processors. In particular, our algorithm addresses the cross-bar switch architecture of the VLIW register file by completely covering the intrinsic faults generated between the multiple computational domains. Fault simulation campaigns comparing previously developed methods with our solution demonstrate its effectiveness. The results show that the developed algorithm achieves a 97.12% fault coverage which is about twice better than previously developed SBST algorithms. Further advantages of our solution are the limited overhead in terms of execution cycles and memory occupation.	algorithm;central processing unit;computation;crossbar switch;fault coverage;overhead (computing);register file;sensor;simulation	Davide Sabena;Matteo Sonza Reorda;Luca Sterpone	2012	2012 Design, Automation & Test in Europe Conference & Exhibition (DATE)	10.1109/DATE.2012.6176506	embedded system;computer architecture;parallel computing;real-time computing;computer science;very long instruction word;operating system	EDA	7.40178870222917	58.842639705718575	29576
10e2fb92ed81726c7da9be27b7a7395924a4a314	integrated logic synthesis using simulated annealing	complex objects;elektroteknik och elektronik;berakningsmatematik;electrical engineering electronic engineering information engineering;rule based;rule based optimization;simulated annealing;satisfiability;computational mathematics;logic synthesis;optimality criteria;technology mapping;optimal algorithm;circuit optimization	Conventional logic synthesis flows are composed of three separate phases: technology independent optimization, technology mapping, and technology dependent optimization. A fundamental problem with such a three-phased approach is that the global logic structure is decided during the first phase without any knowledge of the actual technology parameters considered during later phases. Although technology dependent optimization algorithms perform some limited logic restructuring, they cannot recover from fundamental mistakes made during the first phase, which often results in non-satisfiable solutions. In this paper, we present a method for integrating the three synthesis phases using an annealing algorithm as optimization framework. The annealing-based search is driven by a complex objective function, combining both technology independent as well as technology dependent optimization criteria. Our experimental results shown that, on average, the presented approach can improve the area and delay of circuits optimized with script rugged of SIS by 11.2% and 32.5% respectively.	algorithm;logic synthesis;loss function;mathematical optimization;optimization problem;rugged computer;simulated annealing	Petra Färm;Elena Dubrova;Andreas Kuehlmann	2011		10.1145/1973009.1973095	rule-based system;embedded system;mathematical optimization;electronic engineering;logic synthesis;engineering optimization;logic optimization;simulated annealing;numerical analysis;computer science;theoretical computer science;machine learning;algorithm;metaheuristic;satisfiability	EDA	16.595054576683527	51.25872502837413	29661
30bf0d3ade2e0ddd16c46b76c31f979cf88aa281	word-length optimization and error analysis of a multivariate gaussian random number generator	field programmable gate array;random number generator;building block;fpga;eigenvalue decomposition;mathematical analysis;multivariate gaussian distribution;upper bound;general purpose processor;error analysis;word length;monte carlo simulation;word length optimization	Monte Carlo simulation is one of the most widely used techniques for computationally intensive simulations in mathematical analysis and modeling. A multivariate Gaussian random number generator is one of the main building blocks of such a system. Field Programmable Gate Arrays (FPGAs) are gaining increased popularity as an alternative means to the traditional general purpose processors targeting the acceleration of the computationally expensive random number generator block. This paper presents a novel approach for mapping a multivariate Gaussian random number generator onto an FPGA by automatically optimizing the computational path with respect to the resource usage. The proposed approach is based on the Eigenvalue decomposition algorithm which decomposes the design into computational paths with different precision requirements. Moreover, an error analysis on the impact of the error due to truncation is performed in order to provide upper bounds of the error inserted into the system. The proposed methodology optimises the usage of the available FPGA resources leading to area efficient designs without any significant penalty on the overall performance. Experimental results reveal that the hardware resource usage on an FPGA is reduced by a factor of two in comparison to current methods.	analysis of algorithms;approximation;central processing unit;computation;datapath;error analysis (mathematics);field-programmable gate array;gibbs sampling;john d. wiley;mathematical optimization;mean squared error;monte carlo method;numerical recipes;programmable logic device;random number generation;reconfigurable computing;requirement;risk management;simulation;tridiagonal matrix algorithm;truncation	Chalermpol Saiprasert;Christos-Savvas Bouganis;George A. Constantinides	2009		10.1007/978-3-642-00641-8_23	embedded system;mathematical optimization;parallel computing;computer science;theoretical computer science;algorithm;field-programmable gate array	EDA	10.496175831645049	45.52177641862897	29690
5debbea8d6cdca6cf9aab5abdf4b78cea93c60e1	a two-item floating point fused dot-product unit with latency reduced	fused dot product unit;fused fft butterfly unit;floating point arithmetic	This paper presents a floating point fused dot-product (FDP) unit with latency reduced. The proposed FDP unit performs the dot-product operation of four floating point numbers: ab ± cd and is implemented with dual-path algorithm. The proposed FDP is modeled in Verilog-HDL and synthesized using TSMC 65 nm technology library. Synthesis results show that our proposed FDP unit is 24_30% faster and 36.4% less area than the fastest FDP in previous work. We also use the proposed FDP unit and our previously designed FAS (fused add-subtract) unit to implement a FFT Radix-2 Butterfly (R2BF) unit. The latency of our proposed R2BF unit is improved roughly by 34% and the area is reduced by 41.6%, compared to the fastest 2__-complement butterfly unit. © IEICE 2016.		Mingjiang Wang;De Liu;Boya Zhao	2016	IEICE Electronic Express	10.1587/elex.13.20160937	arithmetic;embedded system;computer hardware;computer science;floating point;mathematics	HCI	11.494630384692368	44.52789949765734	29796
6fa371fb95e52f95627d696bdd9117d5c6f2b7a4	functional testing of microprocessors with graded fault coverage	silicon;graded fault coverage;microprocessors circuit faults circuit testing logic testing integrated circuit technology automatic test pattern generation sequential analysis integrated circuit modeling silicon test equipment;microprocessors;embedded cores;test set size;circuit faults;functional testing;automatic test pattern generation;intel 8086;sequential analysis;functional fault model;computer testing;test application time reduction;system on chip;integrated circuit technology;motorola 68000;atpg functional testing microprocessors graded fault coverage test application time reduction functional fault model test set size system on chip embedded cores embeded processors intel 8086 motorola 68000;integrated circuit modeling;logic testing;integrated circuit testing;fault coverage;circuit testing;test equipment;embeded processors;fault model;automatic test pattern generation microprocessor chips integrated circuit testing logic testing computer testing;development time;microprocessor chips;atpg	The goal of this paper is to reduce the test application time for microprocessors. Since functional fault model is used for testing microprocessors, test development time is greatly reduced. But functional test generation leads to a large number of tests. The size of the test set is an important factor, as it determines both the storage for test instructions in the test equipment, as well as the test application time. The problem becomes still more serious when the processor is embedded as a core in a system-on-chip. Hence, in this paper, we try to address the problem of reducing the number of tests. We use the available structural information about the microprocessors to drop some of the functional tests. Some valid assumptions about the faults that are present in the microprocessor, e.g., only single stuck at faults are present, is made to reduce the number of tests. We develop fault-grading concepts and use them to reduce the number of tests. We generate tests for Intel 8086, Motorola 68000 microprocessors using functional testing procedures and reduce the number of tests using our fault grader.	built-in test equipment;embedded system;fault coverage;fault model;functional testing;microprocessor;motorola 68000;system on a chip;test set;test-driven development	Rajesh Kannah;C. P. Ravikumar	2000		10.1109/ATS.2000.893626	computer architecture;electronic engineering;real-time computing;fault coverage;computer science;engineering;automatic test pattern generation;operating system;dynamic testing;statistics;random test generator	Embedded	21.013886993198263	52.64760538228852	29812
28af98a500ad4b22e0e525d9642b081f02a28ba0	architecture design space exploration for streaming applications through timing analysis	processing element;architectural design;multiprocessor system on chip;timing analysis;synchronous dataflow	In this paper we compare the maximum achievable th roughput of different memory organisations of the processing elements tha constitute a multiprocessor system on chip. This is done by modelling the mapping of a task with input and output channels on a processing element as a homogeneous synchronou s dataflow graph, and use maximum cycle mean analysis to derive the throughpu t. In a HiperLAN\2 case study we show how these techniques can be used to derive the required clock frequency and communication latencies in order to meet the applic ation’s throughput requirement on a multiprocessor system on chip that has one of the i nv stigated memory organisations. Introduction Advances in silicon technology enable multi-processor system-on-chip (MPSoC) devices to be built. MPSoCs provide high computing power in an energy-efficient way, making them ideal for multimedia consumer applications. Multimedia applications of ten operate on one or more streams of input data, for example: base-band processing, audi o/video (de)coding, and image processing. An MPSoC consists of Processing Elements ( PE). For scalability reasons we envision that in the near future MPSoCs will include a N etwork-on-Chip (NoC) for communication between PEs, as i.e. [1]. Multimedia applications can be modelled conveniently using a task gr aph, where the vertices represent functions and the edges data dependencies. The dat a streams through the graph from function to function. A subclass of multimedia applications operates under hard real-time constraints: throughput and latency requirements are put on the inputs and outputs of the ta sk graph. To satisfy these requirements, methods are needed that allow reasoni ng, predicting and guaranteeing the application performance for a given mapping on a multi -processor architecture. Using such an analysis method different architectures ca n be compared, so that for given timing requirements the architecture that runs at the l ow st clock frequency can be found. This paper analyses the temporal behaviour of multimedia applications mapped on a multiprocessor architecture by modelling the mapping with Homogeneous Synchronous DataFlow (HSDF) graphs and applying the associated analysis t echniques. The contribution of this paper is that it shows how these analysis techniques can be used for design space exploration, to find an architecture instance given the timing cons trai ts and given an optimisation criterion (in our case clock frequency) which has its influence on the energy efficiency. We explore different memory organisations for the PEs and their consequences for the clock frequency of the processor and the requirements imposed on the NoC. The approach is based on the following assumptions: i) an upper bound on the tas k’s execution time can be given; ii) upper bounds on the data communication la tencies can be given. Finding a tight upper bound on the execution time of a piece of code is a hard problem, but using techniques as presented by Li this can be done [2]. Wh en multiple tasks are mapped on the same processor, then a scheduling policy needs to be applied on this processor that provides an upper bound on the waiting time of the task. An upper bound on the communication latencies can be given by a communication infrast ructure that provides guaranteed latency such as [1][3]. Poplavko [4] uses SDF inter-processor communication (IPC) graphs [5] t o find minimal buffer sizes by accurately modelling the Æthereal NoC [3] and analysing the temporal behaviour of a JPEG decoder mapped on an MPSoC consisting of ARM proces sors and the Æthereal NoC. We do not aim for buffer minimization but aim for an a rchitecture that meets the applications timing constraints at low energy consumption. An untimed HSDF graphs is similar to a Marked Graph Petri Net [6]. The time semantics applied here for HSDF graphs is similar to time Petri Nets [7]. The organisation of this paper is as follows. In Section 1, the organisa tio of the MPSoC template is given. The HSDF model of computation and its ass oci ted analysis technique is presented in Section 2. In Section 3, the different memory organisations for the PEs are presented and their throughput is analysed, after which in S ection 4 the consequences are described when an application is mapped over multiple PEs. Section 5 describes a case study in which the data processing part of a H iperLAN\2 receiver is mapped on a MPSoC consisting of a number of M ONTIUM processing tiles [8], after which we conclude in Section 6. 1. System Organization An abstract representation of the multiprocessor system considere d in this paper is given in Figure 1. It consists of multiple Processing Elements (PEs) that are connected to a Network-on-Chip (NoC) through Network Interfaces (NI). A PE includes a processor, instruction memory, and data memory; the processor is for instance a domain-specific or general purpose processor. One or several tasks ( τi) can execute on a PE. When communicating tasks are mapped on the same PE then the communication c h nnel between them is mapped on the local memory. When communicating tasks are mapped on different PEs then the channel is mapped over the local memories of both PEs and the NoC is used to transport data from one PE to the other. Tasks only access the PE’s local memory. PE1 PE2 PEn RTOS τ1 τ5 τ2 τ3 τ4	arm architecture;clock rate;data dependency;data-flow analysis;dataflow;design space exploration;graph (discrete mathematics);image processing;input/output;inter-process communication;jpeg;linear algebra;mpsoc;marked graph;mathematical optimization;microprocessor;model of computation;multiprocessing;network interface;network on a chip;petri net;real-time clock;real-time computing;requirement;run time (program lifecycle phase);streams;scalability;scheduling (computing);system on a chip;throughput	Maarten Wiggers;Nikolay Kavaldjiev;Gerard J. M. Smit;Pierre G. Jansen	2005			computer architecture;parallel computing;real-time computing;computer science;static timing analysis	Embedded	-0.2189280871347224	54.96253850625064	29816
1faf21f80b446b2aba580df8ecb7581a2dc6111c	hardware architecture considerations in the we32100 chip set	information systems;metastasis;very large scale integration;hardware architecture;chip;computer architecture;software architecture;access protocols;communication protocol;hardware computer architecture software architecture access protocols metastasis information systems microprocessor chips very large scale integration;microprocessor chips;hardware	The interchip communication protocols for the WE32100 microprocessor chip set had to be designed to support several ambitious architectural goals.	chipset;microprocessor	Michael L. Fuccio;Benjamin Ng	1986	IEEE Micro	10.1109/MM.1986.304741	chip;reference architecture;communications protocol;software architecture;space-based architecture;computer architecture;desktop and mobile architecture for system hardware;parallel computing;telecommunications;computer science;applications architecture;cellular architecture;hardware architecture;very-large-scale integration;information system;data architecture;systems architecture	Arch	5.0572689913249596	49.93333811142344	29817
41b0a7363bac7c066ec9516ff6bd958b5d9308c0	a study of two approaches for reconfiguring fault-tolerant systolic arrays	reconfiguration;tolerancia falta;computers;processing element;digital computers;red sistolica;reconfiguracion;general and miscellaneous mathematics computing and information science;combinatorics;fault tolerant;multiprocessor;redundancy cellular arrays fault tolerant computing;redundancia;combinatoria;mathematical logic 990210 supercomputers 1987 1989;sistema informatico;combinatoire;systolic array;computer system;faulty cells;polynomials;cellular arrays;fault tolerant computing;redundancy;minimal fault pattern faulty systolic arrays redundancy faulty cells square array fault tolerance;systolic network;fault tolerance systolic arrays circuit faults redundancy fabrication polynomials very large scale integration pattern analysis geometry process design;array processors;fault tolerance;square array;reseau systolique;algorithms;systeme informatique;faulty systolic arrays;multiprocesador;minimal fault pattern;functions;tolerance faute;fault tolerant computers;redondance;multiprocesseur	Presents a critical study of two approaches, the classical RC-cut approach and H.T. Kung and M.S. Lam's (Proc. 1984 MIT Conf. Advanced Res. VLSI p.74-83, 1984) RCS-cut approach, for reconfiguring faulty systolic arrays. The amount of cell (processing element) redundancy needed to ensure successful reconfiguration into an n*n array is considered. It is shown that no polynomial bounded redundancy is sufficient for the classical approach, whereas O(n/sup 2/log n) redundancy is sufficient for the Kung and Lams approach. The number of faulty cells that can be tolerated in a given array regardless of their locations is characterized and derived. It is shown that, for both approaches, in almost all cases a square array has better fault tolerance than a rectangular array having the same number of cells. A minimal fault pattern in a 2n*2n array with 3n+1 faults that is not reconfigurable into an n*n array using either of the two approaches is established. >		Clement W. H. Lam;Hon Fung Li;Rajagopalan Jayakumar	1989	IEEE Trans. Computers	10.1109/12.24292	embedded system;fault tolerance;parallel computing;computer science;theoretical computer science;algorithm	Visualization	13.616027025764701	35.842849761634795	29880
19324b6483313dde2ae36c517fe35742fe2ab275	occupancy estimation in smart buildings using audio-processing techniques	smart building;occupany estimation;audio processing	In the past few years, several case studies have illustrated that the use of occupancy information in buildings leads to energy-efficient and low-cost HVAC operation. The widely presented techniques for occupancy estimation include temperature, humidity, CO2 concentration, image camera, motion sensor and passive infrared (PIR) sensor. So far little studies have been reported in literature to utilize audio and speech processing as indoor occupancy prediction technique. With rapid advances of audio and speech processing technologies, nowadays it is more feasible and attractive to integrate audio-based signal processing component into smart buildings. In this work, we propose to utilize audio processing techniques (i.e., speaker recognition and background audio energy estimation) to estimate room occupancy (i.e., the number of people inside a room). Theoretical analysis and simulation results demonstrate the accuracy and effectiveness of this proposed occupancy estimation technique. Based on the occupancy estimation, smart buildings will adjust the thermostat setups and HVAC operations, thus, achieving greater quality of service and drastic cost savings.	motion detector;passive optical network;quality of service;signal processing;simulation;smart tv;speaker recognition;speech processing	Qian Huang;Zhenhao Ge;Chao Lu	2015	CoRR		embedded system;simulation;acoustics;building automation;audio signal processing;computer science	EDA	3.853469717853917	33.607508248562354	29912
1bf4c5d1d80699fff7dfefd9b0cd974ed75d618d	a new design technique for column compression multipliers	column compression techniques;arithmetique ordinateur;multiplicador binario;twos complement multipliers column compression multipliers adders dadda cc multiplier;multiplying circuits;multipliers;producto matriz;circuit vlsi;contrainte;digital arithmetic adders multiplying circuits;allocation;upper bound;computer arithmetic;design technique;twos complement multiplier;vlsi circuits;adders;compresion;borne inferieure;dadda multiplier;aritmetica ordenador;multiplicateur binaire;binary multiplier;circuit multiplicateur;afectacion;digital arithmetic;affectation;additionneur;compression;borne superieure;produit matrice;very large scale integration counting circuits signal processing computer architecture delay effects wiring high performance computing compressors nearest neighbor searches;matrix product;lower bound;cota superior;cota inferior;adder allocation;stresses	In this paper, a new design technique for column-compression (CC) multipliers is presented. Constraints for column compression with full and half adders are analyzed and, under these constraints, considerable flexibility for implementation of the CC multiplier, including the allocation of adders, and choosing the length of the final fast adder, is exploited. Using the example of an 8 × 8 bit CC multiplier, we show that architectures obtained from this new design technique are more area efficient, and have shorter interconnections than the classical Dadda CC multiplier. We finally show that our new technique is also suitable for the design of two's complement multipliers.	adder (electronics);carry-lookahead adder;two's complement	Zhongde Wang;Graham A. Jullien;William C. Miller	1995	IEEE Trans. Computers	10.1109/12.403712	parallel computing;mathematics;upper and lower bounds;algorithm	EDA	14.599699933005539	44.5130507128926	29929
c01aff8276b8e0f9561dc27c473aca79ecce9d41	integration issues on the development of an h.264/avc video decoder soc for sbtvd set top box	real time;h 264 avc;video compression;video processing;consumer electronics;chip;memory access;low latency;mpeg 4 aac;system on chip;system integration;storage capacity;external memory controller;memory hierarchy;external memory;collaborative design;ip integration;high definition;set top box	Embedded consumer electronics like video processing systems require large storage capacity and high bandwidth memory access. Also, those systems are built from heterogeneous processing units, designed specifically to perform dedicated tasks in order to maximize the processing efficiency. A single off-chip memory is shared between the processing units to reduce power and save costs. The external memory access is the system bottleneck when decoding high definition video sequences in real time. A four level memory hierarchy was designed to manage the decoded video in macroblock granularity with low latency. The inclusion of the memory hierarchy in the system has also implications on system integration and IP reuse in a collaborative design. This work presents some issues in the integration of the memory hierarchy on the system and practical strategies used to solve them. This architecture was validated and is being progressively prototyped using a Xilinx Virtex-5 FPGA board.	bottleneck (engineering);computer memory;dudebro: my shit is fucked up so i got to shoot/slice you ii: it's straight-up dawg time;field-programmable gate array;high bandwidth memory;image processor;macroblock;memory bandwidth;memory hierarchy;memory management controller;multiplexing;overhead (computing);phy (chip);parser;requirement;semiconductor intellectual property core;set-top box;system integration;system on a chip;systems design;video decoder;video processing	André Borin Soares;Alexsandro Cristovão Bonatto;Altamiro Amadeu Susin	2011		10.1145/2020876.2020906	data compression;chip;system on a chip;uniform memory access;distributed shared memory;shared memory;embedded system;interleaved memory;electronic engineering;real-time computing;computer hardware;computer science;operating system;overlay;video processing;conventional memory;extended memory;flat memory model;registered memory;memory map;system integration;memory management;low latency	OS	10.781079979664476	40.85322213709168	29993
472a0929d4647e3f0fd788a241f83ff38d3af0a2	instruction code mapping for performance increase and energy reduction in embedded computer systems	energy conservation;cache storage;evaluation performance;interconnection;calculateur embarque;performance evaluation;integrated circuit;embedded computer systems;canal bus;low power cache memories energy conservation;evaluacion prestacion;computer aided instruction;sistema informatico;canal colector;cache memory;circuito integrado;computer system;estimacion a priori;carta de datos;system on a chip;embedded system;antememoria;interconexion;computer aided instruction embedded computing energy consumption embedded system computer architecture system on a chip computational modeling energy conservation potential energy mobile computing;a priori estimation;cache misses;computer architecture;embedded systems;codificacion;antememoire;computational modeling;real world application;low power;codes;energy consumption;mappage;conservation energie;energy reduction;interconnexion;coding;low power electronics;low power electronics cache storage codes instruction sets embedded systems;boarded computer;conservacion energetica;estimation a priori;cache memories instruction code mapping energy reduction embedded computer systems preprocessing step cache misses energy consumption memory size;bus channel;preprocessing step;systeme informatique;mapping;procesador;instruction code mapping;power consumption;consommation energie electrique;processeur;potential energy;mobile computing;electronique faible puissance;calculador embarque;processor;memory size;circuit integre;embedded computing;codage;cache memories;instruction sets	In this paper, we present a novel and fast constructive technique that relocates the instruction code in such a manner into the main memory that the cache is utilized more efficiently. The technique is applied as a preprocessing step, i.e., before the code is executed. Our technique is applicable in embedded systems where the number and characteristics of tasks running on the system is known a priori. The technique does not impose any computational overhead to the system. As a result of applying our technique to a variety of real-world applications we observed through simulation a significant drop of cache misses. Furthermore, the energy consumption of the whole system (CPU, caches, buses, main memory) is reduced by up to 65%. These benefits could be achieved by a slightly increased main memory size of about 13% on average.	bus (computing);cpu cache;central processing unit;computer data storage;embedded system;iso 10303;opcode;overhead (computing);preprocessor;simulation	Sri Parameswaran;Jörg Henkel	2005	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2004.842936	system on a chip;embedded system;electronic engineering;parallel computing;real-time computing;cache coloring;cpu cache;energy conservation;computer science;operating system;integrated circuit;interconnection;potential energy;instruction set;coding;mobile computing;computational model;cache pollution;code;low-power electronics	Embedded	-2.8316413620198917	54.09079397210243	30018
5ecb884a81aa75d94c66bd90352c779d46b0cfed	algorithms to maximize yield and enhance yield/area of pipeline circuitry by insertion of switches and redundant modules	n-stage pipeline;switch;nanoscale technologies;time complexity;soc architecture;pipeline architecture;pipeline circuitry;nano-scale technology;soc architectures;steering logic resources;pipeline;system-on-chip;computational complexity;algorithm;important aspect;redundancy;redundant module;appropriate number;redundant copy;switches-redundant modules;pipeline architectures;yield/area;optimal algorithm;pipeline processing;switches;algorithm design and analysis;system on a chip;fault tolerance;logic;pipelines;network on a chip;system on chip	Increasing yield is important, especially for nano-scale technologies. Also, pipelines are an important aspect of many SoC architectures. In this paper we present new approaches to improve the yield and yield/area of pipeline architectures by using (1) an appropriate number of redundant copies for each module, and (2) sufficient steering logic resources. We present an optimal algorithm of time complexity O(n3) that adds redundant modules to an n-stage pipeline so as to maximize yield. Experimental results indicate that for parameter values of interests, this algorithm also improves the yield/area of the pipeline, especially when the yield for some modules is low.	algorithm;electronic circuit;gnu nano;network switch;pipeline (computing);system on a chip;time complexity	Mohammad Mirza-Aghatabar;Melvin A. Breuer;Sandeep K. Gupta	2010	2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)		system on a chip;electronic engineering;parallel computing;real-time computing;computer science;network on a chip;algorithm	EDA	13.814581027854302	53.74016950418401	30026
627c2c5f6c24411e44a74e40b1791018cec4f2c4	acceleration of rsa processes based on hybrid arm-fpga cluster		Cooperation of software and hardware with hybrid architectures, such as Xilinx Zynq SoC combining ARM CPU and FPGA fabric, is a high-performance and low-power platform for accelerating RSA Algorithm. This paper adopts the none-subtraction Montgomery algorithm and the Chinese Remainder Theorem (CRT) to implement high-speed RSA processors, and deploys a 48-node cluster infrastructure based on Zynq SoC to achieve extremely high scalability and throughput of RSA computing. In this design, we use the ARM to implement node-to-node communication with the Message Passing Interface (MPI) while use the FPGA to handle complex calculation. Finally, the experimental results show that the overall performance is linear with the number of nodes. And the cluster achieves 6×∼9× speedup against a multi-core desktop (Intel i7-3770) and comparable performance to a many-core server (288-core). In addition, we gain up to 2.5× energy efficiency compared to these two traditional platforms.	algorithm;cathode ray tube;central processing unit;desktop computer;field-programmable gate array;low-power broadcasting;manycore processor;message passing interface;montgomery modular multiplication;multi-core processor;rsa (cryptosystem);scalability;server (computing);speedup;system on a chip;throughput	Xu Bai;Lei Jiang;Qiong Dai;Jiajia Yang;Jianlong Tan	2017	2017 IEEE Symposium on Computers and Communications (ISCC)	10.1109/ISCC.2017.8024607	field-programmable gate array;parallel computing;throughput;computer science;algorithm design;scalability;arm architecture;speedup;chinese remainder theorem;message passing interface	Embedded	7.956801627115105	44.7732855117776	30076
78515c41d3c964ef5f96368486ea2edd01b0d915	principles and construction of msd adder in ternary optical computer	signed digit;adder;modified signed digit msd adder;optical computing;data editing technique;satisfiability;ternary optical computer toc;numerical calculation;pipeline computing;parallel computer;user requirements;modified signed digit msd	The two remarkable features of ternary values and a massive unit with thousands bits of parallel computation will make the ternary optical computer (TOC) with modified signed-digit (MSD) adder more powerful and efficient than ever before for numerical calculations. Based on the decrease-radix design presented previously, a TOC can satisfy either a user requiring huge capacity for data calculations or one with a moderate amount of data, if it is equipped with a prepared adder. Furthermore, with the application of pipelined operations and the proposed data editing technique, the efficiency of the prepared adder can be greatly improved, so that each calculated result can be obtained almost within one clock cycle. It is hopeful that by employing a MSD adder, users will be able to enter a new dimension with the creation of a new multiplier, new divider, as well as new matrix operator in a TOC in the near future.	adder (electronics);capability maturity model;carry-lookahead adder;carry-skip adder;clock signal;computation;numerical analysis;optical computing;optical disc authoring;parallel computing	Yi Jin;Yunfu Shen;Junjie Peng;Shiyi Xu;Guangtai Ding;Dongjian Yue;Haihang You	2010	Science China Information Sciences	10.1007/s11432-010-4091-9	computer hardware;computer science;theoretical computer science;user requirements document;serial binary adder;optical computing;carry-save adder;pipeline;algorithm;adder;satisfiability	Theory	14.68568762839433	43.20223800702281	30099
f203109323ca6f7f3f5c139a6128c6fbb8c180da	compiled hardware acceleration of molecular dynamics code	software;compiled hardware acceleration;biology computing;namd;software based execution;pipelined circuit;logic design;clocks;fpga based code acceleration;atomic clocks;hardware field programmable gate arrays software acceleration blades clocks atomic clocks;hardware accelerator;given biomolecular environment;acceleration;molecular dynamics method;molecular dynamics method biology computing field programmable gate arrays logic design;roccc compiler toolset;molecular dynamic;blades;molecular dynamics code;field programmable gate arrays;sgi altix 4700;md simulation;rasc rc100 blade;rasc rc100 blade compiled hardware acceleration molecular dynamics code given biomolecular environment fpga based code acceleration namd software based execution pipelined circuit roccc compiler toolset sgi altix 4700;hardware	The objective of molecular dynamics (MD) simulations is to determine the shape of a molecule in a given biomolecular environment. These simulations are very demanding computationally, where simulations of a few milliseconds can take days or months depending on the number of atoms involved. Therefore, MD simulations are a prime candidate for FPGA-based code acceleration. We have investigated the possible acceleration of the commonly used MD program NAMD. This code is highly optimized for software based execution and does not benefit from an FPGA-based acceleration as written. We have therefore developed a modified version, based on the calculations NAMD performs, that streams a set of data through a highly pipelined circuit on the FPGA. We have used the ROCCC compiler toolset to generate the circuit and implemented it on the SGI Altix 4700 fitted with a RASC RC100 blade.	compiler;field-programmable gate array;hardware acceleration;nanoscale molecular dynamics;simulation	Jason R. Villarreal;Walid A. Najjar	2008	2008 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2008.4630035	acceleration;atomic clock;embedded system;molecular dynamics;computer architecture;parallel computing;logic synthesis;real-time computing;hardware acceleration;computer science;operating system;field-programmable gate array	EDA	0.21974710152295548	45.34747019433457	30110
00b8f2497c6ecf7da78d96bc323e698ff3a73004	a bus delay reduction technique considering crosstalk	crosstalk wires delay effects cmos technology switches capacitance interference very large scale integration timing equations;on chip bus;integrated circuit design cmos digital integrated circuits vlsi crosstalk capacitance delays;cache;crosstalk;intellectual property;repeater inserted bus bus delay reduction technique crosstalk interference cmos technology horizontal coupling capacitance wire load vlsi design onchip bus delay signal transition timing skew;vlsi design;system on a chip;integrated circuit design;technology scaling;low power;estimation;cmos digital integrated circuits;vlsi;capacitance;delays	As the CMOS technology scaled down, the horizontal coupling capacitance between adjacent wires plays dominant part in wire load, crosstalk interference becomes a serious problem for VLSI design. We focused on delay increase caused by crosstalk. On-chip bus delay is maximized by crosstalk effect when adjacent wires simultaneously switch for opposite signal transition directions. This paper proposes a bus delay reduction technique by intentional skewing signal transition timing of adjacent wires. An approximated equation of bus delay shows our delay reduction technique is effective for repeaterinserted bus. The result of SPICE simulation shows that the total bus delay reduction by from 5% to 20% can be achieved.	approximation algorithm;cmos;crosstalk;interference (communication);overhead (computing);spice;signal transition;simulation;very-large-scale integration	Kei Hirose;Hiroto Yasuura	2000		10.1145/343647.343815	embedded system;electronic engineering;real-time computing;delay calculation;computer science;engineering;electrical engineering;very-large-scale integration	EDA	17.115808146460175	55.94904331683219	30141
368c01963eeee14a9b3d11e9cc5a244c7761580c	the impact of locality on the detection of cycles in the broadcast congested clique model		The broadcast congested clique model is a message-passing model of distributed computation where n nodes communicate with each other in synchronous rounds. The joint input to the n nodes is an undirected graph G on the same set of nodes, with each node receiving the list of its immediate neighbors in G. In each round each node sends the same message to all other nodes, depending on its own input, on the messages it has received so far, and on a public sequence of random bits. One parameter of this model is an upper bound b on the size of the messages, also known as bandwidth. In this paper we introduce another parameter to the model. We study the situation where the nodes, initially, instead of knowing their immediate neighbors, know their neighborhood up to a fixed radius r. In this new framework we study one of the hardest problems in distributed graph algorithms, this is, the problem of detecting, in G, an induced cycle of length at most k (Cycle≤k) and the problem of detecting in G an induced cycle of length at least k + 1 (Cycle>k). For r = 1, we exhibit a deterministic, one-round algorithm for solving Cycle≤k with b = O(n logn) if k is even, and b = O(n2/(k−1) logn) if k is odd. We also prove, assuming the Erdős Girth Conjecture, that this result is tight for k ≥ 4, up to logarithmic factors. More precisely, if each node, instead of being able to see only its immediate neighbors, is allowed to see up to distance r = bk/4c, and if we also allowed randomization and multiple rounds, then the number of rounds R needed to solve Cycle≤k must be such that R · b = Ω(n) if k is even, and R · b = Ω(n2/(k−1)) if k is odd. On the other hand, we show that, if each node is allowed to see up to distance r = bk/2c+ 1, then a polylogarithmic bandwidth is sufficient for solving Cycle>k with only two rounds. Nevertheless, if nodes were allowed to see up to distance r = bk/3c, then any one-round algorithm that solves Cycle>k needs the bandwidth b to be at least Ω(n/ logn).	bandwidth (signal processing);computation;computational complexity theory;degeneracy (graph theory);deterministic algorithm;distributed computing;erdős number;erdős–faber–lovász conjecture;girth (graph theory);graph (discrete mathematics);graph theory;induced path;locality of reference;message passing;polylogarithmic function;randomized algorithm;sensor;treewidth	Florent Becker;Pedro Montealegre-Barba;Ivan Rapaport;Ioan Todinca	2018		10.1007/978-3-319-77404-6_11	discrete mathematics;clique;computation;locality;upper and lower bounds;computer science;bandwidth (signal processing);broadcasting;graph	Theory	18.30454955538152	33.58427747970084	30142
0044a9d46f2f8e823c02c68a5045492b72ecbebc	adaptive image filtering using run-time reconfiguration	parallel architectures reconfigurable architectures image processing adaptive filters;image filtering;image processing;reconfigurable architectures;run time reconfiguration;adaptive filters;run time reconfigurable;parallel architectures;adaptive filters filtering runtime hardware pixel field programmable gate arrays concurrent computing distributed computing table lookup computational modeling;multiple filtering windows adaptive image filtering run time reconfiguration filtering window coefficients rtr pixel regular appearance;adaptive image filtering	......................................................................................................................................... iv Chapter 1: Introduction and Motivation ......................................................................................... 1 Chapter 2: Background ................................................................................................................... 5 Chapter 3: Implementation ........................................................................................................... 26 Chapter 4: Results and Future Work............................................................................................. 58 References..................................................................................................................................... 61 Vita................................................................................................................................................ 63	run time (program lifecycle phase)	Nitin Srivastava;Jerry L. Trahan;Ramachandran Vaidyanathan;Suresh Rai	2003		10.1109/IPDPS.2003.1213332	adaptive filter;parallel computing;real-time computing;image processing;computer science;theoretical computer science	NLP	9.297005296554909	39.435735262185	30161
a5b016f8bd64d69fdd7ed6b36f981ddcf29dde03	video quality-of-service for consumer terminals - a novel system for programmable components	consumer terminals;mpeg 2 decoding;resource limitation;quality of service hardware software quality quality management resource management power system management software algorithms samarium robustness computer architecture;scalable video;decoding;dsp board;dsp board video quality of service consumer terminals programmable components programmable platforms scalable video algorithm sojhvare modules resource usage output signal strategy manager quality of service resource manager qos rm media processing programmable architectures mpeg 2 decoding image enhancement qos control sojhvare resource limited hardware;output signal;video coding quality of service telecommunication terminals programmable circuits consumer electronics decoding image enhancement telecommunication network management multimedia communication;telecommunication control;resource manager;resource management;video quality;consumer electronics;resource limited hardware;telecommunication terminals;programmable components;quality of service resource manager;programmable architectures;video coding;computer architecture;software architecture;image enhancement;quality of service hardware software algorithms software quality quality management resource management power system management robustness computer architecture decoding;media processing;power system management;scalable video algorithm sojhvare modules;programmable circuits;background recording video quality of service consumer terminals programmable components scalable video algorithm software modules output signal quality strategy manager quality of service resource manager cost effective media processing programmable architectures mpeg 2 decoding image enhancement algorithms qos control software resource limited hardware dsp board single dedicated media processor picture in picture window applications;multimedia communication;samarium;software algorithms;cost effectiveness;digital signal processing chips;robustness;control engineering computing	Future consumer terminals will be based on programmable platforms instead of dedicated hardware. Novel 'scalable video algorithm' (SVA) software modules provide trade-offs between resource usage and quality of the output signal. SVAs together with a strategy manager (SM) and a quality-of-service resource manager (QoS-RM) aim for robust and cost-effective media processing in software on programmable architectures. We have developed some SVAs including MPEG-2 decoding and image enhancement and the QoS control software to illustrate the power of the overall system. The resource limited hardware is a currently available DSP board.	algorithm;digital signal processor;image editing;mpeg-2;quality of service;scalability	Christian Hentschel;Reinder J. Bril;Yingwei Chen;Ralph A. C. Braspenning;Tse-Hua Lan	2002	2002 Digest of Technical Papers. International Conference on Consumer Electronics (IEEE Cat. No.02CH37300)	10.1109/TCE.2003.1261242	embedded system;software architecture;real-time computing;cost-effectiveness analysis;quality of service;computer science;video quality;resource management;operating system;samarium;software quality;robustness;computer engineering	EDA	10.258307257067314	41.17462546728521	30204
0dd232c411bf6bc0c1674a79bbb88d5cb48585b7	statistical design space exploration for application-specific unit synthesis	automated design;platform based design;constraint optimization;cost function;extreme value theory statistical design application specific unit synthesis semi automated design high level synthesis rtl design design space exploration unsupervised monte carlo design;monte carlo methods application specific integrated circuits circuit optimisation high level synthesis integrated circuit design;stochastic automata networks;space exploration;semi automated design;design optimization;design space;multimedia systems;space exploration high level synthesis cost function permission constraint optimization design optimization sampling methods boosting embedded software graphics;unsupervised monte carlo design;system level analysis;extreme value theory;high level synthesis;integrated circuit design;boosting;permission;application specific integrated circuits;rtl design;application specific unit synthesis;statistical design;design space exploration;sampling methods;circuit optimisation;monte carlo;monte carlo methods;graphics;embedded software	The capability of performing semi-automated design space exploration is the main advantage of high-level synthesis with respect to RTL design. However, design space exploration performed during high-level synthesis is limited in scope, since it provides promising solutions that represent good starting points for subsequent optimizations, but it provides no insight about the overall structure of the design space. In this work we propose unsupervised Monte-Carlo design exploration and statistical characterization to capture the key features of the design space. Our analysis provides insight on how various solutions are distributed over the entire design space. In addition, we apply extreme value theory [11] to extrapolate achievable bounds from the sampling points.	design space exploration;extrapolation;extreme value theory;high- and low-level;high-level synthesis;metropolis–hastings algorithm;monte carlo;register-transfer level;sampling (signal processing);semiconductor industry;unsupervised learning	Davide Bruni;Alessandro Bogliolo;Luca Benini	2001		10.1145/378239.379039	constrained optimization;real-time computing;simulation;computer science;theoretical computer science;statistics;monte carlo method	EDA	14.331845672489878	55.25364597539153	30241
a57c0426b612b43e289b2c0543010f8735201482	trends in emerging on-chip interconnect technologies	chip;invited papers	In deep submicron (DSM) VLSI technologies, it is becoming increasingly harder for a copper based electrical interconnect fabric to satisfy the multiple design requirements of delay, power, bandwidth, and delay uncertainty. This is because electrical interconnects are becoming increasingly susceptible to parasitic resistance and capacitance with shrinking process technology and rising clock frequencies, which poses serious challenges for interconnect delay, power dissipation and reliability. On-chip communication architectures such as buses and networks-on-chip (NoC) that are used to enable inter-component communication in multi-processor systems-on-chip (MPSoC) designs rely on these electrical interconnects at the physical level, and are consequently faced with the entire gamut of challenges and drawbacks that plague copper-based electrical interconnects. To overcome the limitations of traditional copper-based electrical interconnects, several research efforts have begun looking at novel interconnect alternatives, such as on-chip optical interconnects, wireless interconnects and carbon nanotube-based interconnects. This paper presents an overview and current state of research for these three promising interconnect technologies. We also discuss the existing challenges for each of these technologies that remain to be resolved before they can be adopted as replacements for copper-based electrical interconnects in the future.	capacitor plague;clock rate;electrical connection;mpsoc;multiprocessing;network on a chip;parasitic element (electrical networks);requirement;switched fabric;system on a chip;very-large-scale integration	Sudeep Pasricha;Nikil D. Dutt	2008	IPSJ Trans. System LSI Design Methodology	10.2197/ipsjtsldm.1.2	chip;electronic engineering;computer science;engineering;electrical engineering	EDA	12.135805129160126	56.68546365524139	30244
8ae1e2b892c6ec9967e47bcfdb8b699fd5951572	system esd robustness by co-design of on-chip and on-board protection measures	modelizacion;conception conjointe;optimisation;protective device;optimizacion;concepcion sistema;diseno conjunto;cout developpement;electronic component;development cost;physical design;dispositivo proteccion;chip;chip on board packaging;high strength current;modelisation;electrostatic discharge;integrated circuit bonding;codesign;cost optimization;decharge electrostatique;system design;assemblage circuit integre;robustesse;courant intense;corriente intensa;robustness;composant electronique;optimization;modeling;technologie puce sur carte;conception systeme;dispositif protection;robustez;design methodology;componente electronico	System-level ESD robustness is a crucial feature for any electronic system. However, a consistent design methodology including IC-level protection, on-board protection and physical design of the module is still missing. The idea of a simple correlation between IC-level and system-level ESD robustness levels is misleading. A thorough characterization of the high current behaviour of IO circuit and on-board protection elements provides the necessary data for a simulation based co-design of on-chip and on-board protection measures. The constraints for characterization and modelling are discussed and the various protection measures for improved system-level ESD robustness are presented. Applying this methodology allows the development of a cost optimized system-level ESD protection throughout the stages of a system design.	on-board data handling	Harald Gossner;Werner Simbürger;Matthias Stecher	2010	Microelectronics Reliability	10.1016/j.microrel.2010.07.146	chip;co-design;physical design;electronic engineering;systems modeling;electrostatic discharge;design methods;telecommunications;engineering;electrical engineering;electronic component;robustness;systems design	EDA	18.646503460159284	54.74843604609054	30264
361087d0a4af4355660126bc576fc6c595990f5e	the minimum width routing of a 2-row 2-layer polycell-layout	minimum width;minimum width routing;theoretical result;2-row 2-layer polycell-layout;2-layer channel area;applicable routing;new routing principle;space technology;logic simulation;routing	This paper presents a new routing principle that leads to an algorithm to realize the minimum width of the 2-layer channel area between two rows of terminals to be interconnected. Besides the theoretical results, practically applicable routing algorithms based on our principle are developed.	algorithm;routing	Tatsuya Kawamoto;Yoji Kajitani	1979	16th Design Automation Conference		routing table;routing domain;embedded system;routing;electronic engineering;static routing;real-time computing;equal-cost multi-path routing;computer science;engineering;dynamic source routing;multipath routing;destination-sequenced distance vector routing;logic simulation;space technology;link-state routing protocol;triangular routing;path vector protocol;routing;computer network	EDA	14.812025008462887	51.69921171352851	30393
eb0bd84a21c6305072689268267ad9ab87cf9bd9	parallel accelerators for glimmerhmm bioinformatics algorithm	field programmable gate array;multi threading;multithreaded operational environment;hidden markov model;reconfigurable logic;parallel accelerators;genomic data;computer graphic equipment;fpga platform;gpu;gene finding;algorithm design and analysis field programmable gate arrays graphics processing unit hidden markov models bioinformatics computer architecture context;fpga;glimmerhmm bioinformatics algorithm;coprocessors;bioinformatics gene finding fpga gpu;computer architecture;hidden markov models;multicore server parallel accelerators glimmerhmm bioinformatics algorithm genomic data biologically functional genomic dna fpga platform graphic processing unit multithreaded operational environment;biologically functional genomic dna;multi threading bioinformatics computer graphic equipment coprocessors field programmable gate arrays hidden markov models;functional genomics;graphic processing unit;exponential growth;field programmable gate arrays;multicore server;graphics processing unit;computational biology;algorithm design;context;algorithm design and analysis;bioinformatics	In the last decades there is an exponential growth in the amount of genomic data that need to be analyzed. A very important problem in biology is the extraction of the biologically functional genomic DNA from the actual genome of the organisms. There have been proposed many computational biology algorithms that solve the gene finding problem which utilize various approaches; GlimmerHMM is considered one of the most efficient such algorithms. This paper presents two different accelerators for the GlimmerHMM algorithm. One of them is implemented on a modern FPGA platform exploiting the parallelism that reconfigurable logic offers and the other one utilizes a GPU (Graphic Processing Unit) taking advantage of a highly multithreaded operational environment. The performance of the implemented systems is compared against the one achieved when the official distribution of the algorithm is executed on a high-end multi-core server; the speedup initiated, for the most compute intensive part, is up to 200× for the FPGA-based system and up to 34× for the GPU-based system.	algorithm;bioinformatics;central processing unit;computational biology;data mining;database trigger;field-programmable gate array;gene prediction;graphics processing unit;lookup table;multi-core processor;pci express;parallel computing;ray tracing (graphics);reconfigurable computing;server (computing);server-side;speedup;thread (computing);time complexity;tree traversal	Nafsika Chrysanthou;Grigorios Chrysos;Euripides Sotiriades;Ioannis Papaefstathiou	2011	2011 Design, Automation & Test in Europe	10.1109/DATE.2011.5763024	embedded system;algorithm design;parallel computing;computer science;bioinformatics;theoretical computer science;operating system;algorithm;hidden markov model;field-programmable gate array	EDA	0.15263622726172185	43.17666495287585	30403
a07dc94439be3d88e8e59433382ece0522145f07	test challenges in embedded stt-mram arrays	microprocessors;resistance;testing;thermal stability;computer architecture;stability analysis;computer architecture microprocessors thermal stability magnetic tunneling resistance testing stability analysis;magnetic tunneling	Spin Transfer Torque Magnetic Random Access Memory (STT-MRAM) is an emerging memory technology which exhibits non-volatility, high density and nanosecond read and write times. These attributes of STT-MRAM make it suitable for last level embedded caches. However, the defects and corresponding fault models of STT-MRAM are not as extensively explored as in SRAM and therefore, there is a growing need for defect and fault analysis. Moreover, stochastic retention failure of STTM-RAM imposes a large burden in testing time. Conventional test schemes for retention of STT-MRAM need to be optimized for testing a large-size embedded STT-MRAM array. This work presents a review of the different defect and fault mechanisms as well as a BIST architecture and circuit to reduce testing time in characterization and manufacturing tests for retention. We address the effect of resistive and capacitive defects and identify retention test setup for measuring worst case retention.	best, worst and average case;built-in self-test;embedded system;fault model;magnetoresistive random-access memory;non-volatile memory;random access;software bug;static random-access memory;volatility	Insik Yoon;Arijit Raychowdhury	2017	2017 18th International Symposium on Quality Electronic Design (ISQED)	10.1109/ISQED.2017.7918289	thermal stability;embedded system;electronic engineering;von neumann stability analysis;real-time computing;computer science;engineering;software testing;resistance	EDA	20.759137644966827	59.17264485399064	30406
8e11f0d86fbde13a6553794a6c792e2eb4914ae8	proofs: a super fast fault simulator for sequential circuits	high performance;sequential circuits;logic circuits;super fast fault simulator;logic cad;proofs;software implementation;differential fault simulation;memory requirement;single fault propagation;sequential circuit;available fault simulator;parallel fault simulation;complexity;benchmark circuit;fault location;logic testing;software performance;discrete event simulation;computational modeling;fault detection;combinational circuits	This paper describes PROOFS, a super fast fault simulator for synchronous sequential logic circuits. PROOFS achieves high performance by combining all the advantages in differential fault simulation, single fault propagation, and parallel fault simulation to minimize the memory requirements, to reduce events that need to be simulated, and to simplify the complexity of the software implementation. The experimental results of PROOFS and other available fault simulators on 20 benchmark circuits showed that PROOFS is the best.	benchmark (computing);fault simulator;logic gate;requirement;sequential logic;simulation;software propagation	Wu-Tung Cheng;Janak H. Patel	1990			physical design;embedded system;electronic engineering;fault coverage;computer science;stuck-at fault;fault model;sequential logic;software fault tolerance	EDA	20.126083759767788	49.704849823032234	30535
986c1e6b4480e3411422ec8a8d96293fae1dedeb	brief announcement: stabilizing consensus with the power of two choices	common value;consensus problem	Consensus problems occur in many contexts and have therefore been extensively studied in the past. In the original consensus problem, every process initially proposes a value, and the goal is to decide on a single value from all those proposed. We are studying a slight variant of the consensus problem called the stabilizing consensus problem [2]. In this problem, we do not require that each process irrevocably commits to a final value but that eventually they arrive at a common, stable value without necessarily being aware of that. This should work irrespective of the states in which the processes are starting. In other words, we are searching for a self-stabilizing algorithm for the consensus problem. Coming up with such an algorithm is easy without adversarial involvement, but we allow some adversary to continuously change the states of some of the nodes at will. Despite these state changes, we would like the processes to arrive quickly at a common value that will be preserved for as many time steps as possible (in a sense that almost all of the processes will store this value during that period of time). Interestingly, we will demonstrate that there is a simple algorithm for this problem that essentially needs logarithmic time and work with high probability to arrive at such a stable value, even if the adversary can perform arbitrary state changes, as long as it can only do so for a limited number of processes at a time. Our approach. We are focussing on synchronous message-passing systems with adversarial state changes. More precisely, we are given a fixed set of n processes that are numbered from 1 to n. The time proceeds in synchronized rounds .I n each round, every process can send out one or more requests, receive replies to its requests, and perform some local computation based on these replies. We assume that every process can send a message to any other process (i.e., there are no connectivity constraints) and faithfully follows the stated protocol (given its current state, which might have been changed by the adversary). Stabilizing consensus problem. We are studying a slight variant of the original consensus problem called the stabilizing consensus problem [2]. In this problem, we may start with an arbitrary set of initial output values. As in the usual convention, a configuration includes all of the processes’ local states. A configuration C is called output-stable if in all possible executions starting from C ,t he	power of two	Benjamin Doerr;Leslie Ann Goldberg;Lorenz Minder;Thomas Sauerwald;Christian Scheideler	2010		10.1007/978-3-642-15763-9_50	consensus;computer science;artificial intelligence;uniform consensus;mathematics;distributed computing;chandra–toueg consensus algorithm;algorithm	Crypto	17.673896147382152	35.50337236989181	30577
d1f8c686f3e5ff21cc64dad776679c99fa0feb0e	combining data reuse exploitationwith data-level parallelization for fpga targeted hardware compilation: a geometric programming framework	integrated memory circuits buffer storage data analysis field programmable gate arrays geometric programming;random access memory;memory management;random access memory system on a chip arrays memory management field programmable gate arrays parallel processing optimization;data reuse exploitation;geometric program;integrated memory circuits;data reuse;buffer storage;on chip memory constraint;design space;journal article;system on a chip;chip;arrays;data analysis;performance improvement;on chip memory utilization;optimization;field programmable gate arrays;fpga targeted hardware compilation;geometric programming;performance optimization;data level parallelization;on chip memory utilization data reuse exploitation data level parallelization fpga targeted hardware compilation geometric programming buffering exploitation on chip memory constraint;buffering exploitation;parallel processing;data level parallelism	A geometric programming framework is proposed in this paper to automate exploration of the design space consisting of data reuse (buffering) exploitation and loop-level parallelization, in the context of FPGA-targeted hardware compilation. We expose the dependence between data reuse and data-level parallelization and explore both problems under the on-chip memory constraint for performance-optimal designs within a single optimization step. Results from applying this framework to several real benchmarks demonstrate that given different constraints on on-chip memory utilization, the corresponding performance-optimal designs are automatically determined by the framework, and performance improvements up to 4.7 times have been achieved compared with the method that first explores data reuse and then performs parallelization.	benchmark (computing);central processing unit;compiler;field-programmable gate array;geometric programming;logic synthesis;mathematical optimization;optimal design;parallel computing;solver	Qiang Liu;George A. Constantinides;Kostas Masselos;Peter Y. K. Cheung	2008	2008 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2008.4629928	chip;system on a chip;embedded system;parallel processing;computer architecture;parallel computing;real-time computing;geometric programming;computer science;operating system;data parallelism;data analysis;field-programmable gate array;automatic parallelization;memory management	EDA	-1.583883639370181	52.76590244255025	30617
d27df19d0ee97f937044b8a8af963262dbc6d10d	leveraging ieee 1641 for tester-independent ate software	semiconductor device testing;software;semiconductor device testing electronic engineering computing;instruments;military aerospace market;frequency measurement;hardware independent tester interface;semiconductor test development;semitest domain;logic gates;hardware independent tester interface ieee 1641 tester independent ate software semiconductor test development military aerospace market semitest domain;synchronization;tester independent ate software;electronic engineering computing;ieee 1641;use case;voltage measurement;conferences;software testing aerospace testing system testing semiconductor device testing military standards aerospace industry standards development instruments software standards mathematical model	Three critical issues for semiconductor test development are test reuse, portability, and translation from design information. In the military/aerospace market, the IEEE standard for signal and test definition (IEEE 1641-2004) addresses similar requirements and may be highly relevant to the semi-test domain. The authors evaluate this standard's applicability to semi-test mixed signal use cases for possible direct use as, or inspiration for, a hardware-independent tester interface for ATE.	mixed-signal integrated circuit;requirement;semiconductor industry;software portability	Bethany Van Wagenen;Jon Vollmar;Dan Thornton	2008	2008 IEEE International Test Conference	10.1109/TEST.2008.4700608	use case;reliability engineering;embedded system;synchronization;electronic engineering;logic gate;telecommunications;computer science;engineering;electrical engineering;operating system;software engineering;ieee 802.11h-2003;computer engineering	Visualization	9.416533058914089	54.04103312399682	30622
cb3ddebb3a2a1b83cde81e6cb49352862df8de63	post-mapping transformations for low-power synthesis	low power	We propose a logic synthesis system that includes power optimization after technology mapping. Our approach is unique in that our post-mapping logic transformations take into account information on circuit delay, capacitance, arrival times, glitches, etc, to provide much better accuracy than previously proposed technology-independent power optimization methods. By changing connections in a mapped circuit that was earlier restructured for lower switching activity, we achieve power improvements up to 59% in case of area-optimized circuits and 38% in delay-optimized circuits. The average power reduction is 15% and 13% for the above cases respectively, with reductions also in area and delay. The transformations are based on the transition density model of circuit switching activity and the concept of permissible logic functions. The techniques presented here are applicable equally well to both synchronous and asynchronous circuits. The power measurements are done under a general delay model. y This work was supported by the National Science Foundation (NSF) under grant MIP-9308426. Submitted to VLSI Design, 1995.	asynchronous circuit;circuit switching;glitch;ibm notes;logic synthesis;markov chain;mathematical optimization;power optimization (eda);synchronization (computer science);very-large-scale integration	Rajendran Panda;Farid N. Najm	1998	VLSI Design	10.1155/1998/96768	electronic engineering;real-time computing;logic optimization;asynchronous circuit;telecommunications;computer science;electrical engineering;algorithm	EDA	17.277822540603513	55.07298825136746	30638
63a39826d694873cc8d9bdfbb48dcfc577d568db	efficient implementation of multi-moduli architectures for binary-to-rns conversion	digital signal processing;topology;hardware dynamic range digital signal processing delay logic functions memory architecture;residue number systems;serial structures binary to rns conversion binary to rns multimoduli architectures common intermediate results rns moduli channels topology memoryless multimoduli architectures parallel implementation serial implementation state of the art structures area reductions parallel structures;parallel architectures;efficient implementation;memory architecture;logic functions;dynamic range;topology parallel architectures residue number systems;hardware	This paper presents a novel approach to improve the existing Binary-to-RNS multi-moduli architectures. These architectures reduce the complexity by sharing common intermediate results among various RNS moduli channels. Two types of multi-moduli architectures are distinguished depending on whether the functionality is implemented serially or in parallel. A novel choice of the weights associated to the inputs provides huge improvement when applied to the most efficient topology known to date. Experimental results suggest that the proposed memoryless multi-moduli architectures achieve speedups of 2.02 and 1.79 for parallel and serial implementations, respectively, in comparison with the most efficient state-of-the-art structures. Furthermore, such implementations herein proposed have demonstrated that area reductions of 5.02% and 44.02% are achieved for parallel and serial structures, respectively.	computation;electronic circuit;fma instruction set;iterative method;parallel computing;precomputation;residue number system	Héctor Pettenghi;Leonel Sousa;Jude Angelo Ambrose	2012	17th Asia and South Pacific Design Automation Conference	10.1109/ASPDAC.2012.6165068	embedded system;computer architecture;dynamic range;electronic engineering;parallel computing;computer science;theoretical computer science;digital signal processing	EDA	12.189323539539673	44.220276791480195	30660
cf7f5a6f18ca010ca69505717800ba41dbc1a7e1	a transputer-based instrument for the esa/nasa cluster mission	transputer-based instrument;nasa cluster mission	This paper describes the evolution and implementation of a multiple transputer based instrument, the Digital Wave Processor (DWP), for the ESA Cluster mission. The DWP k a fault-tolerant instrument responsible for the control of five wave-processing experiments, and the data processing and compression of their output. Novel features include low-power modes of the DWP where some transputers are powered off and others may run at a reduced clock speed.	clock rate;data compression;esa;experiment;fault tolerance;low-power broadcasting;transputer	C. M. Dunford;Jon A. Thompson;K. H. Yearby	1991	Concurrency - Practice and Experience	10.1002/cpe.4330030407	computer science;parallel computing;transputer;data processing;clock rate	Robotics	3.309982757975174	45.4896345473145	30664
c1dbd73eebafed354dfee4f848a7fdd77767ec2d	using xfuzzy environment for the whole design of fuzzy systems	xfuzzy environment;fuzzy controller;design process;image processing;fuzzy systems java process design fuzzy logic software tools hardware application software robots navigation image processing;java design fuzzy logic fuzzy systems hardware description languages;comunicacion de congreso;rule based;formal specification language;hardware description languages;fuzzy logic;vhdl xfuzzy environment java fuzzy logic based system design;vhdl;fuzzy logic based system design;membership function;design;fuzzy systems;autonomous robot;fuzzy system;java;fuzzy classifier	Since 1992, Xfuzzy environment has been improving to ease the design of fuzzy systems. The current version, Xfuzzy 3, which is entirely programmed in Java, includes a wide set of new featured tools that allow automating the whole design process of a fuzzy logic based system: from its description (in the XFL3 language) to its synthesis in C, C++ or Java (to be included in software projects) or in VHDL (for hardware projects). The new features of the current version have been exploited in different application areas such as autonomous robot navigation and image processing.		Iluminada Baturone;Francisco Jose Moreno-Velo;Santiago Sánchez-Solano;Angel Barriga Barros;Piedad Brox Jiménez;Andrés Gersnoviez;María Brox	2007	2007 IEEE International Fuzzy Systems Conference	10.1109/FUZZY.2007.4295420	fuzzy logic;design;real-time computing;design process;membership function;vhdl;computer science;artificial intelligence;theoretical computer science;programming language;java;fuzzy control system	EDA	4.825665839043753	51.73804645120006	30701
2b7d201febc5716ee0077fc76e242006e2cfd019	exploiting suspected redundancy without proving it	sequential redundancy removal;correctness preserving transformations;checking;bounded search;redundancy exploitation;logic design;sequential circuits;synergistic transformation;robustness scalability permission reachability analysis merging;formal verification;redundancy;permission;sequential equivalence checking;merging;robustness;verification algorithms;scalability;sequential equivalence;speculatively reduced model;redundancy sequential circuits logic design formal verification;correctness preserving transformations redundancy exploitation sequential redundancy removal synergistic transformation verification algorithms speculatively reduced model bounded search sequential equivalence checking;reachability analysis	We present several improvements to general-purpose sequential redundancy removal. (1) We propose using a robust variety of synergistic transformation and verification algorithms to process the individual proof obligations. This enables greater speed and scalability, and identifies a significantly greater degree of redundancy, than prior approaches. (2) We generalize upon traditional redundancy removal and utilize the speculatively-reduced model to enhance bounded search, without needing to complete any proofs.	algorithm;general-purpose modeling;redundancy (engineering);scalability;synergy	Hari Mony;Jason Baumgartner;Viresh Paruthi;Robert Kanzelman	2005	Proceedings. 42nd Design Automation Conference, 2005.	10.1145/1065579.1065700	dual modular redundancy;discrete mathematics;logic synthesis;scalability;formal verification;computer science;theoretical computer science;mathematics;sequential logic;redundancy;programming language;algorithm;robustness	EDA	18.667846341059043	47.99706115190211	30708
11d01258655e4f128d83e1c3976220b49800b0bf	binary decision diagrams and beyond: enabling technologies for formal verification	enabling technology;efficient graph algorithm;boolean function;binary decision diagram;improved performance;obdd application;cad application;ordered binary decision diagrams;obdd-based method;graph-based function representation;logic synthesis;formal verification;replication;function representation;boolean functions;integer programming;decision tables;gradient decent;logic design	Ordered Binary Decision Diagrams (OBDDs) have found widespread use in CAD applications such as formal verification, logic synthesis, and test generation. OBDDs represent Boolean functions in a form that is both canonical and compact for many practical cases. They can be generated and manipulated by efficient graph algorithms. Researchers have found that many tasks can be expressed as series of operations on Boolean functions, making them candidates for OBDD-based methods. The success of OBDDs has inspired efforts to improve their efficiency and to expand their range of applicability. Techniques have been discovered to make the representation more compact and to represent other classes of functions. This has led to improved performance on existing OBDD applications, as well as enabled new classes of problems to be solved. This paper provides an overview of the state of the art in graph-based function representations. We focus on several recent advances of particular importance for formal verification and other CAD applications.	algorithm;binary decision diagram;computer-aided design;formal verification;graph theory;logic synthesis	Randal E. Bryant	1995		10.1145/224841.225047	discrete mathematics;logic synthesis;integer programming;computer science;theoretical computer science;mathematics;boolean function;algorithm	EDA	17.536799343649797	47.673000479047644	30732
1625147d3d36e2791098b75ecf4b2bfa06eea4c1	a study of routability estimation and clustering in placement	software;state of the art academic placers routability estimation clustering ibm place 2 0 standard cell benchmark suite;pattern clustering;latched circuits;circuits clustering algorithms performance gain software performance state estimation permission scalability runtime wire;routing;state of the art academic placers;data mining;binning optimization;estimation;clustering;ssta;ibm place 2 0 standard cell benchmark suite;clustering algorithms;routability estimation;benchmark testing;conferences	This paper studies the effects of clustering as a pre-processing step and routability estimation in the placement flow. The study shows that when clustering and routability estimation are considered, the placer effectively improves the routed wirelength for the circuits of IBM-PLACE 2.0 standard-cell Benchmark Suite [1] and results in the best average routed wirelength when compared against state-of-the-art academic placers.	benchmark (computing);cluster analysis;preprocessor;routing;standard cell	Kalliopi Tsota;Cheng-Kok Koh;Venkataramanan Balakrishnan	2009	2009 IEEE/ACM International Conference on Computer-Aided Design - Digest of Technical Papers	10.1145/1687399.1687468	embedded system;parallel computing;real-time computing;computer science;cluster analysis;statistics	EDA	15.139946346436579	53.15449322886207	30815
735887aca7641ba39b36ad10d3bce4780bf38d7a	parallel and simd optimization of image feature extraction	image features;image processing;branch prediction;single instruction multiple data;cumulant;content based image retrieval	Image feature extraction is widely used in content-based image retrieval(CBIR), computer version and all kinds of image processing applications. In this paper, we introduce some parallel and SIMD optimizations of image-feature extraction to overcome the disadvantages of original methods. We mainly use both thread-parallel optimization and Single Instruction Multiple Data (SIMD) optimizations. And especially, for some hot point, we use SIMD logical operations to eliminate the random conditional branches which cannot be effectively predicted by CPU Branch Prediction. We experimented our optimized implementation on multi-core systems, and various images were used to test the image-feature extraction results and performance. All the parallel and SIMD optimizations work out a good cumulative performance speedup.	branch predictor;central processing unit;computer;feature (computer vision);feature extraction;hardware acceleration;high- and low-level;image processing;logical connective;mathematical optimization;multi-core processor;multiprocessing;paradiseo;parallel computing;simd;speedup;task parallelism	Ming Qi;Guangzhong Sun;Guoliang Chen	2011		10.1016/j.procs.2011.04.051	parallel computing;simd;image processing;computer science;theoretical computer science;machine learning;feature;branch predictor;statistics;cumulant	HPC	5.821278505375386	39.47694229943946	30837
ec7f7ba984e014e4620efa1832c4d0e3406c626b	fault tolerant reconfigurable device based on autonomous-repair cells	configuration memory upsets;fault tolerance mission critical systems space missions field programmable gate arrays logic devices reconfigurable logic programmable logic arrays programmable logic devices routing satellites;fault tolerant;reconfigurable architectures;reconfigurable devices;cell to cell routing;space radiation;critical field;fault tolerance;radiation hardening electronics;field programmable gate arrays;reconfigurable architectures fault tolerance field programmable gate arrays radiation hardening electronics;programmable logic;cell to cell routing field programmable gate arrays fault tolerance autonomous repair cells reconfigurable devices configuration memory upsets space radiation programmable logic;autonomous repair cells	"""Reconfigurable devices are expected to be utilized in the mission-critical fields such as space development, because a system update and a pseudo-repair can be achieved remotely by reconfiguration. However, conventional reconfigurable devices suffer from configuration memory upsets caused by radiation in space. This paper proposes an architecture of a fault-tolerant reconfigurable device based on island-style FPGA. The device consists of """"autonomous-repair cells"""" which repairs these upsets autonomously and dynamically. To determine the architecture, we analyze four autonomous repair techniques of the cell experimentally. Then, optimal autonomous repair techniques are applied to both of programmable logics and cell-to-cell routing resources. Through evaluation, we show that proposed device achieves more than 10 years average lifetime against configuration upsets even in a severe situation such as a satellite orbit"""	autonomous robot;experiment;fault tolerance;field-programmable gate array;mission critical;reconfigurable computing;routing	Kentaro Nakahara;Shin'ichi Kouyama;Tomonori Izumi;Hiroyuki Ochi;Yukihiro Nakamura	2006	2006 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2006.311252	erasable programmable logic device;embedded system;fault tolerance;real-time computing;macrocell array;programmable logic array;computer science;programmable logic device;simple programmable logic device;programmable array logic	EDA	8.851260296771063	59.32305449148944	30854
10a4c511274113de0e9ff6b1c59cc745e08709c5	asynchronous transpose-matrix architectures	data compression;computer architecture discrete cosine transforms data compression video coding pipeline processing;high data rate;video compression;power asynchronous transpose matrix architectures matrix transposition operation video compression algorithm video decompression algorithm image compression algorithm image decompression algorithm discrete cosine transform inverse discrete cosine transform distributed arithmetic applications high data rates minimum power dissipation portable applications clocked solution two phase signaling four phase signaling latency;discrete cosine transform;video coding;computer architecture;discrete cosine transforms;power dissipation;distributed arithmetic;pipelines discrete cosine transforms clocks throughput digital signal processing discrete transforms circuits power dissipation delay costs;pipeline processing	"""• So far, we've assumed that the process is addressing """" memory """" • In most systems, (user) processes use """" virtual """" addresses ! Gives the process the illusion that it directly addresses all real memory ! Gives the process the illusion that there is more real memory than is really available 3 How Virtual Memory Works • Memory is divided into blocks called pages ! Each address has two parts • Low bits: location of item within a page • High bits: page number ! Pages are mapped to different parts of the real memory or stored on denser (but slower) media (typically disk) 4 Paging Example High bits in address Low bits … Page Table Memory Memory page All of memory is divided into pages A page table entry is required for each memory page Low bits in address give location within page"""	page (computer memory);page table;paging	José A. Tierno;Prabhakar Kudva	1997		10.1109/ICCD.1997.628904	data compression;electronic engineering;parallel computing;transform coding;lapped transform;computer science;theoretical computer science;discrete cosine transform;algorithm	Arch	11.879880119337521	39.39841344904064	30921
6fa1caad99cd8cd0560b3da4e55129d8b31ede64	dynamic thermal management for finfet-based circuits exploiting the temperature effect inversion phenomenon	cmos integrated circuits;finfets delays temperature cmos integrated circuits threshold voltage energy consumption integrated circuit modeling;optimum chip temperature operating point dynamic thermal management finfet circuits temperature effect inversion power consumption minimization circuit delay superthreshold supply voltage leakage power dissipation delay leakage power trade off;thermal management packaging delays integrated circuit packaging low power electronics mos integrated circuits;finfets;thermal management low power designs finfet;energy consumption;threshold voltage;thermal mangement;integrated circuit modeling;temperature;low power designs;thermal management;finfet;delays	Due to limits on the availability of the energy source in many mobile user platforms (ranging from handheld devices to portable electronics to deeply embedded devices) and concerns about how much heat can effectively be removed from chips, minimizing the power consumption has become a primary driver for system-on-chip designers. Because of their superb characteristics, FinFETs have emerged as a promising replacement for planar CMOS devices in sub-20nm CMOS technology nodes. However, based on extensive simulations, we have observed that the delay vs. temperature characteristics of FinFET-based circuits are fundamentally different from that of the conventional bulk CMOS circuits, i.e., the delay of a FinFET circuit decreases with increasing temperature even in the super-threshold supply voltage regime. Unfortunately, the leakage power dissipation of the FinFET-based circuits increases exponentially with the temperature. These two trends give rise to a tradeoff between delay and leakage power as a function of the chip temperature, and hence, lead to the definition of an optimum chip temperature operating point (i.e., one that balances concerns about the circuit speed and power efficiency.) This paper presents the results of our investigations into the aforesaid temperature effect inversion (TEI) and proposes a novel dynamic thermal management (DTM) algorithm, which exploits this phenomenon to minimize the energy consumption of FinFET-based circuits without any appreciable performance penalty. Experimental results demonstrate 40% energy saving (with no performance penalty) can be achieved by the proposed TEI-aware DTM approach compared to the best-in-class DTMs that are unaware of this phenomenon.	algorithm;cmos;embedded system;mobile computing;mobile device;operating point;performance per watt;semiconductor;simulation;spectral leakage;system on a chip;text encoding initiative;thermal management of high-power leds;value-driven design	Woojoo Lee;Yanzhi Wang;Tiansong Cui;Shahin Nazarian;Massoud Pedram	2014	2014 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)	10.1145/2627369.2627608	electronic engineering;real-time computing;thermal management of electronic devices and systems;temperature;engineering;electrical engineering;threshold voltage;thermodynamics;cmos	EDA	19.399071751399664	59.11496836387603	30948
9d8fc90131df8336ba967893955ad750fff125ef	static race verification for networks with reconvergent clocks	pruning strategy static race verification reconvergent clocks combinational paths static timing verifier min max delays static timing verification high speed integrated circuits;logic design timing delays combinational circuits;logic design;clocks delay latches logic circuits timing;low power;high speed integrated circuits;delays;combinational circuits;timing	We introduce a new method for pruning combinational paths in a static timing verifier. Our method is fast and robust, even for networks with min-max delays on reconvergent clock trees. We will show that current techniques for static timing verification are either not robust or not efficient for such networks. We will also show that these networks are becoming increasingly important with newer low-power, high-speed integrated circuits. We propose a new pruning strategy for such networks which is both correct and efficient.	12-hour clock	Joel Grodstein;Nick Rethman;N. Nassif	1998		10.1109/ICCD.1998.727100	electronic engineering;parallel computing;logic synthesis;real-time computing;computer science;combinational logic;static timing analysis;algorithm	Logic	18.54877834350986	51.462188735374134	30996
6100405e8ae4ea50600482c05764e5d1e31057ab	algorithms and hardware structures for unobtrusive real-time compression of instruction and data address traces	on chip trace compression module;hardware structure hardware structures unobtrusive real time compression data address traces communication bandwidth trace specific compression algorithms program execution traces debugging tools on chip trace compression module gzip;gzip;compression algorithm;program execution traces;workload characterization;data compression;hardware structure;compression algorithms;real time;computer aided instruction;computer debugging;hardware structures;system under test;runtime;design optimization;trace specific compression algorithms;software performance;program optimization;chip;computer architecture;unobtrusive real time compression;software development;bandwidth;communication bandwidth;compression ratio;debugging tools;program debugging computer debugging data compression data handling;software debugging;program debugging;experimental evaluation;data handling;hardware algorithm design and analysis runtime computer aided instruction computer architecture software debugging software performance design optimization bandwidth compression algorithms;data address traces;algorithm design and analysis;performance tuning;quantitative evaluation;software implementation;hardware	Instruction and data address traces are widely used by computer designers for quantitative evaluations of new architectures and workload characterization, as well as by software developers for program optimization, performance tuning, and debugging. Such traces are typically very large and need to be compressed to reduce the storage, processing, and communication bandwidth requirements. However, preexisting general-purpose and trace-specific compression algorithms are designed for software implementation and are not suitable for runtime compression. Compressing program execution traces at runtime in hardware can deliver insights into the behavior of the system under test without any negative interference with normal program execution. Traditional debugging tools, on the other hand, have to stop the program frequently to examine the state of the processor. Moreover, software developers often do not have access to the entire history of computation that led to an erroneous state. In addition, stepping through a program is a tedious task and may interact with other system components in such a way that the original errors disappear, thus preventing any useful insight. The need for unobtrusive tracing is further underscored by the development of computer systems that feature multiple processing cores on a single chip. In this paper, we introduce a set of algorithms for compressing instruction and data address traces that can easily be implemented in an on-chip trace compression module and describe the corresponding hardware structures. The proposed algorithms are analytically and experimentally evaluated. Our results show that very small hardware structures suffice to achieve a compression ratio similar to that of a software implementation of gzip while being orders of magnitude faster. A hardware structure with slightly over 2 KB of state achieves a compression ratio of 125.9 for instruction address traces, whereas gzip achieves a compression ratio of 87.4. For data address traces, a hardware structure with 5 KB of state achieves a compression ratio of 6.1, compared to 6.8 achieved by gzip	algorithm;computation;data compression;debugging;digital footprint;experiment;general-purpose modeling;interference (communication);mathematical optimization;netbsd gzip / freebsd gzip;performance tuning;program optimization;real-time clock;real-time transcription;requirement;run time (program lifecycle phase);software developer;stepping level;surround sound;system under test;tracing (software)	Milena Milenkovic;Aleksandar Milenkovic;Martin Burtscher	2007	2007 Data Compression Conference (DCC'07)	10.1109/DCC.2007.10	data compression;computer architecture;parallel computing;real-time computing;computer science;algorithm;statistics	Arch	-3.822532720557082	50.912503648623435	31043
4df61c010a7c5954b8206fc85ce79be189316c0a	firm-core virtual fpga for just-in-time fpga compilation (abstract only)	processor architecture;parallelism;speculation;networking;routing algorithm;just in time;technology mapping;reconfigurable hardware	Just-in-time (JIT) compilation has been used in many applications to enable standard software binaries to execute on different underlying processor architectures, yielding software portability benefits. We previously introduced the concept of a standard hardware binary to achieve similar portability benefits for hardware, using a JIT compiler to compile the hardware binary to an FPGA. Our JIT compiler includes lean versions of technology mapping, placement, and routing algorithms that implement the standard hardware binary on a simple custom FPGA fabric designed specifically for JIT compilation. While directly implementing a custom FPGA fabric on silicon may be feasible for some applications, we investigated the option of implementing the simple FPGA fabric as a circuit mapped to a physical FPGA - a virtual FPGA. We described our simple fabric in structural VHDL, synthesized the fabric onto a Xilinx Spartan-IIE FPGA, and mapped 18 benchmark circuits onto the resulting virtual FPGA. Our results show a 6X decrease in performance and a 100X increase in hardware resource usage for the virtual FPGA approach compared to mapping the circuits directly to the physical FPGA. For applications in which hardware portability is essential, a designer could leverage the large capacity of current commercially available FPGAs to implement a virtual FPGA with tens of thousands of configurable gates, providing about the same amount of configurable logic as FPGAs produced in the mid 1990s. Nevertheless, the large overheads clearly indicate the need to develop a virtual FPGA approach tuned to physical fabrics in order to reduce the overhead.	algorithm;benchmark (computing);binary file;compiler;field-programmable gate array;just-in-time compilation;overhead (computing);place and route;routing;software portability;spartan;vhdl	Roman L. Lysecky;Kris Miller;Frank Vahid;Kees A. Vissers	2005		10.1145/1046192.1046247	embedded system;computer architecture;speculation;parallel computing;real-time computing;microarchitecture;reconfigurable computing;computer science;operating system;fpga prototype	EDA	0.6437705274621623	49.24776875331437	31141
0c90a3dd89168ddb3ea02852dff36bd7d7d6a945	built-in self-reconfiguring systems for mesh-connected processor arrays with spares on two rows/columns	orthogonal spare scheme;reconfiguration algorithms;reconfigurable system;spare processing elements;probability;concurrent computing;image processing;reconfigurable architectures;very large scale integration;linear array;reconfiguration methods;switches concurrent computing hardware runtime educational institutions image reconstruction signal processing image processing very large scale integration wafer scale integration;network architectures;probability vlsi microprocessor chips integrated circuit reliability fault tolerant computing redundancy reconfigurable architectures parallel architectures;runtime;opposite spare scheme;orthogonal spare scheme built in self reconfiguring systems mesh connected processor arrays reconfiguration methods faulty pes spare pes spare processing elements network architectures reconfiguration algorithms reconfiguration probabilities run time reliabilities fabrication time yields opposite spare scheme;fault tolerant computing;redundancy;parallel architectures;built in self reconfiguring systems;image reconstruction;signal processing;wafer scale integration;reconfiguration probabilities;mesh connected processor arrays;vlsi;faulty pes;network architecture;integrated circuit reliability;switches;run time reliabilities;microprocessor chips;spare pes;fabrication time yields;hardware	W e discuss some reconfiguration methods where faulty PES are compensated for by spare PES located in two rows/columns in/arozlnd a mesh-connected array since they have advantages that the numbers of spare PES and the network overheads for reconstructions are relatively small. First, we discuss how arrangements of spare PES and network architectures affect the efficiencies of reconfigurations. As arrangements of spare PES, we consider the cases where either spare linear arrays face each other or locate orthogonally. As replacements of faulty PES by spare PES, we consider the straight shifts using double or single tracks. W e give reconfiguration algorithms for the proposed methods. The efficiencies of reconfigurations fo. the methods, that is, the reconfiguration probabilities, are compared with each other. Finally, we give built-in self-reconfiguring systems for the proposed methods by hardware. This implies that the proposed methods are effective i n enhancing the run-time reliabilities as well as the fabrication-time yields of the processor arrays.	algorithm;built-in self-test;column (database);columns;packetized elementary stream;potential energy surface;run time (program lifecycle phase);self-reconfiguring modular robot	Itsuo Takanami	2000		10.1109/DFTVS.2000.887159	embedded system;parallel computing;real-time computing;network architecture;concurrent computing;image processing;computer science;signal processing;very-large-scale integration	EDA	8.234563482381066	57.27847231554761	31309
60c37e2b7c6f3256337afa87b9528d3e4c45daf4	a design diversity metric and reliability analysis for redundant systems	analytical models;triple modular redundant;failure analysis circuit faults hardware field programmable gate arrays protection redundancy very large scale integration software systems analytical models electromagnetic interference;system failure rate;stuck at fault model;circuit faults;fault simulation;design faults;hardware systems;very large scale integration;common mode;automatic testing;reliability modeling;hardware systems design diversity metric reliability analysis redundant systems common mode failures diverse systems analytical reliability models system failure rate mission time triple modular redundant systems duplex modular redundant systems independent multiple module failures design faults self testing properties stuck at fault model discrete time model software systems;software systems;software fault tolerance;triple modular redundant systems;automatic testing redundancy fault tolerant computing software fault tolerance fault simulation modules logic testing;mission time;failure analysis;protection;fault tolerant computing;redundancy;self testing properties;logic testing;redundant systems;design diversity metric;failure rate;reliability analysis;diverse systems;electromagnetic interference;analytical reliability models;field programmable gate arrays;common mode failures;modules;independent multiple module failures;duplex modular redundant systems;hardware;discrete time model	Design diversity has long been used to protect redundant systems against common-mode failures. The conventional notion of diversity relies on “independent” generation of “different” implementations. This concept is qualitative and does not provide a basis to compare the reliabilities of two diverse systems. In this paper, for the first time, we present a metric to quantify diversity among several designs. Based on this metric, we derive analytical reliability models that show a simple relationship between design diversity, system failure rate, and mission time. In addition, we present simulation results to demonstrate the effectiveness of design diversity in Duplex and Triple Modular Redundant (TMR) systems. For independent multiple-module failures, we show that, mere use of different implementations does not always guarantee higher reliability compared to redundant systems with identical implementations — it is important to analyze the reliability of redundant systems using our metric. For common-mode failures and design faults, there is a significant gain in using different implementations — however, as our analysis shows, the gain diminishes as the mission time increases. Our simulation results also demonstrate the usefulness of diversity for enhancing the self-testing properties of redundant systems.	duplex (telecommunications);failure rate;redundancy (engineering);reliability engineering;simulation;triple modular redundancy	Subhasish Mitra;Nirmal R. Saxena;Edward J. McCluskey	1999		10.1109/TEST.1999.805794	electromagnetic interference;reliability engineering;embedded system;failure analysis;electronic engineering;real-time computing;common-mode signal;computer science;engineering;modular programming;failure rate;very-large-scale integration;redundancy;software fault tolerance;field-programmable gate array;software system	Embedded	9.764136201226886	59.6050774382741	31334
4bc155c6df3879b42297a0a9af23c7f8827e2c15	abstracts of papers presented at socs 2017 in the previously published paper track			system on a chip	Alex S. Fukunaga;Akihiro Kishimoto	2017				EDA	6.077774079677297	50.45468440158101	31369
dfbdf22f8ce1c34b7eb4cedfbe585369ff0ba3d1	efficient design of full adder and subtractor using 5-input majority gate in qca		Quantum dot cellular automata is the recent trend in the field of technology for the designing of any digital circuit involving inverters and majority gates that has the potential to substitute the age old technology of CMOS at the order of Nano level. Herein a full adder and full subtractor circuit is proposed using 5-input majority gate. The new full adder and subtractor reduced the requirement of occupied area, number of cells and energy dissipation. QCAPro tool is used for the calculation of energy dissipation. QCA designer 2.0.3 is used to design and simulate the circuits. A 4-bit ripple carry adder is also designed by one bit full adder.	4-bit;adder (electronics);automata theory;cmos;clock signal;digital electronics;gnu nano;inverter (logic gate);majority function;mike lesser;qualitative comparative analysis;quantum dot cellular automaton;ripple effect;simulation;subtractor	Ramanand Jaiswal;Trailokya Nath Sasamal	2017	2017 Tenth International Conference on Contemporary Computing (IC3)	10.1109/IC3.2017.8284336	subtractor;artificial intelligence;computer vision;nano-;electronic circuit;digital electronics;quantum dot cellular automaton;computer science;electronic engineering;adder;cmos	EDA	16.334699986574783	45.485509318348264	31381
6a12dfd3ccd604644b7c58ff8113ce543667cd74	mitigate erroneous operations of 2t-2mtj stt-mram based on dynamic voltage threshold	magnetic tunnel junction mtj;dynamic voltage threshold mos dtmos;spin transfer torque stt;magnetic random access memory mram;circuit optimization		magnetoresistive random-access memory	Haoyue Tang;Zhenyu Zhao;Lianhua Qu;Quan Deng;Huan Li;Wei Guo	2016	IEICE Electronic Express	10.1587/elex.13.20160533	embedded system;electronic engineering;engineering;electrical engineering	HCI	17.6660493320935	59.6720989587394	31394
eb4965d37b18a38c1bb2a4a394a8d815fd9d2602	design methodology for over-temperature and over-current protection of an ldo voltage regulator by using electro-thermal simulations		This paper presents a methodology for designing over-temperature and over-current protection (OTP and OCP) circuits for low drop-out voltage regulators (LDOs). The OTP monitors the die temperature developed within the LDO and disables its output stage when the temperature reaches a certain, user-defined, level (the OTP activation point). If the LDO output current reaches a set threshold (the OCP activation point), the OCP takes control of it, keeping the current value to an acceptable level. The proposed methodology involves running iteratively electrical, thermal and electro-thermal simulations. It addresses three major issues: first, it allows the designer to identify the suitable layout placement of the OTP and OCP sensors, based on the temperature distribution within the LDO power-stage. Second, the OTP and OCP activation points can be set accurately by taking into account coupled electro-thermal phenomena and the unavoidable differences between the temperature and current sensed by the protection circuits and those developed within the worst-case LDO section. Finally, the LDO design can be fine-tuned considering complex scenarios of real-life operation and test requirements. An LDO was designed using this methodology and the paper provides a direct comparison between the expected (simulated) results and measurements performed on the silicon implementation.	simulation;voltage regulator	Cosmin-Sorin Plesa;Marius Neag;Cristian Boianceanu;Andrei Negoita	2017	Microelectronics Reliability	10.1016/j.microrel.2017.03.028	control engineering;electronic engineering;real-time computing;engineering	EDA	16.878900296627087	55.55578482870343	31457
2d4977e9627aa631285b22f8c226984a47d2f583	comparative analysis: power reversible comparator circuits 90 nm technology	gate diffusion input gdi;comparators circuits;cmos logic circuits;transmission gate technology tg comparator gate diffusion input gdi;logic gates transistors cmos integrated circuits delays very large scale integration cmos technology educational institutions;spice;size 90 nm power reversible comparator circuits transmission gate technology comparator gate diffusion input comparator combinational logic cmos low voltage operation reversible gates tanner tools h spice tools;transmission gate technology tg comparator;spice cmos logic circuits combinational circuits comparators circuits;combinational circuits	In this paper we design GDI and TG based Reversible Comparator circuit using 90 nm Technology. Reversible or information loss less circuits have applications in nanotechnology, digital signal processing, communication, computer graphics and cryptography. They are also a fundamental requirement in the emerging field of quantum computing. Reversible gates are evaluated in terms of number of transistor count, critical path, garbage outputs and one to one mapping. Here transistor implementation of the reversible gates is done by using a combination CMOS-GDI circuit, TG Circuits, which provides the optimal solution for combinational logic, saving 1/3 the power, half the area and 10% in delay relative to a CMOS implementation. GDI circuits provide some measure of enhanced hazard tolerance and are more suitable for low voltage operation. Here transistor implementation of reversible gates is done by using Tanner tools and H-spice tools. Since the proposed work is explored for the first time ever in literature, the contribution of this paper will create a new thread of research in the area of reversible logic circuit.	cmos;combinational logic;comparator;computer graphics;critical path method;cryptography;digital signal processing;graphics device interface;logic gate;quantum computing;quantum gate;reversible computing;tanner graph;transformational grammar;transistor count	Sumit Khurana;Amit Grover;Neeti Grover	2013	2013 7th Asia Modelling Symposium	10.1109/AMS.2013.21	embedded system;and-or-invert;electronic engineering;nor logic;adiabatic circuit;logic gate;computer science;gate equivalent;electrical engineering;pass transistor logic;integrated injection logic;cmos	EDA	16.20083893586148	45.51322578986799	31471
b45ee71f51b5fff188fd2faf8da7c8ba39d75f61	using leds for visible light communication and as a wake-up mechanism in the internet of things		The design and implementation of wireless sensor nodes for the Internet of Things require paying exceptional attention to their sources of energy consumption in order to extend their battery lifetime. These nodes spend most of the time in standby mode and they only wake up to perform a quick defined task, usually sensing and measuring or transmitting information. In this demo, LEDs are used for Visible Light Communication. A low-cost, small-size and low-power consumption LED-based interface circuit connected to an I/O microcontroller pin is proposed and tested. Moreover, this circuit also operates as a wake-up mechanism, aiding to achieve longer standby time and thus lower energy consumption. Our experiments are shown in actual commercial applications developed for a European project.	experiment;input/output;internet of things;low-power broadcasting;microcontroller;sleep mode;transmitter;wake (cipher)	Edgar Ripoll Vercellone;Vicent Ferrandiz;Jordi Aubert;Manel Gasulla	2017		10.1145/3131672.3136995	microcontroller;computer network;computer science;battery (electricity);light-emitting diode;energy consumption;efficient energy use;visible light communication;standby power;wireless	Mobile	2.5631756381717894	34.07753796356447	31508
49d089d8824edbab221dc02b577c454f1805472f	analysis and comparison in the energy-delay-area domain of nanometer cmos flip-flops: part ii—results and figures of merit	energy efficiency;network topology cmos integrated circuits flip flops high speed integrated circuits integrated circuit interconnections low power electronics nanotechnology;cmos integrated circuits;leakage;optimisation;interconnection;cmos technology;cmos technology energy delay area domain nanometer cmos flip flops;optimizacion;integrated circuit;clocks;energy efficient;circuito secuencial;flip flop circuits;estudio comparativo;flip flops;circuit vlsi;rendement energetique;circuit sequentiel;circuito integrado;nanotechnology;tecnologia mos complementario;nanometer cmos flip flops;design optimization;circuit topology;interconexion;network topology;etude comparative;flip flops cmos technology delay clocks energy efficiency circuit topology master slave performance analysis latches design optimization;flip flops ffs;vlsi circuit;low power;ffs technology;circuit bistable;interconnects;integrated circuit interconnections;rendimiento energetico;interconnexion;low power electronics;comparative study;performance analysis;nanometer technologies;clocking;vlsi;horloge;optimization;energy delay area domain;vlsi clocking energy delay tradeoff energy efficiency flip flops ffs high speed interconnects leakage logical effort low power nanometer technologies;temps retard;latches;energy delay tradeoff;logical effort;delay time;circuito vlsi;technologie ffs;technologie mos complementaire;figure of merit;master slave;factor merito;energetic efficiency;electronique faible puissance;tiempo retardo;clock;high speed;high speed integrated circuits;flip flop;reloj;circuit integre;complementary mos technology;facteur merite;sequential circuit	In Part II of this paper, a comparison of the most representative flip-flop (FF) classes and topologies in a 65-nm CMOS technology is carried out. The comparison, which is performed on the energy-delay-area domain, exploits the strategies and methodologies for FFs analysis and design reported in Part I. In particular, the analysis accounts for the impact of leakage and layout parasitics on the optimization of the circuits. The tradeoffs between leakage, area, clock load, delay, and other interesting properties are extensively discussed. The investigation permits to derive several considerations on each FF class and to identify the best topologies for a targeted application.	cmos;clock signal;coupling (computer programming);flops;fastest;flip-flop (electronics);level design;mathematical optimization;pulse generator;race condition;spectral leakage;transistor	Massimo Alioto;Elio Consoli;Gaetano Palumbo	2011	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2010.2041377	embedded system;electronic engineering;computer science;engineering;electrical engineering;operating system;efficient energy use;cmos	EDA	18.155616931440523	54.7244733802245	31526
5a58e5eb6cc1682675e40db8fe077f9981b5a1a0	reliability aware gate sizing combating nbti and oxide breakdown	vlsi ageing integrated circuit reliability negative bias temperature instability probability vectors;negative bias temperature instability;probability;nbti;oxide breakdown;vectors;ageing;vlsi;reliability aware gate sizing formulation signal probability vector obd induced circuit lifetime nbti induced delay degradation aging nanometer vlsi circuit negative bias temperature instability oxide breakdown;integrated circuit reliability;gate sizing nbti oxide breakdown;logic gates delays degradation reliability mos devices;gate sizing	Negative Bias Temperature Instability (NBTI) and Oxide Breakdown (OBD) are two key reliability concerns for nanometer VLSI circuits. Gate over-sizing has been done in the past to mitigate the effect of NBTI and aging to meet performance constraints. However, this could make the entire circuit more prone to OBD. In this paper, we propose a new gate sizing formulation that considers both NBTI-induced delay degradation and OBD-induced circuit lifetime. Since NBTI and OBD are highly sensitive to the input vectors in a conflicting way, we consider their dependencies on signal probabilities. Moreover, we take into account the degradation in rise slew due to NBTI which could affect the fall delay/slew of the inverting gates in the next stage, and this has not been considered in previous work on NBTI aware gate sizing. Experimental results on industry strength benchmarks demonstrate that by incorporating OBD into holistic gate sizing, we can achieve more reliable circuit without compromising the circuit performance and area.	algorithm;benchmark (computing);cmos;elegant degradation;holism;mathematical optimization;negative-bias temperature instability;overhead (computing);physical symbol system;spectral leakage;static timing analysis;very-large-scale integration	Subhendu Roy;David Z. Pan	2014	2014 27th International Conference on VLSI Design and 2014 13th International Conference on Embedded Systems	10.1109/VLSID.2014.14	negative-bias temperature instability;electronic engineering;electrical engineering;forensic engineering	EDA	19.974591564263108	57.85626690050166	31534
7630700d8e71b46c5f70ab5a6f04ff64164bd7a0	evaluating the impact of spike and flicker noise in phase change memories	phase change materials;memory cell flicker noise impact evaluation spike noise impact evaluation phase change memories simulation based analysis hspice simulation array level cell level state switching phenomena binary pcm memories;resistance;frequency flicker noise spike noise phase change memory pcm resistance;phase change materials 1f noise resistance mosfet switches arrays;phase change memories circuit simulation flicker noise;arrays;spike noise;mosfet;1f noise;switches;frequency;flicker noise;phase change memory pcm	This paper presents a simulation-based analysis of spike and flicker noise in a Phase Change Memory (PCM); this investigation is based on HSPICE simulation by taking into account cell-level (with its neighbors) and array-level considerations. State switching phenomena in binary PCM memories are dealt in detail to assess the impact of these two types of noise. It is shown that a lower feature size is of concern for flicker noise in terms of value and percentage variation (while not substantially affecting array-level performance). This paper also shows that spike noise has a radically different behavior: spike noise shows a dependency on the PCM resistance more than the type of state of the PCM. It increases substantially when the amorphous resistance increases and has a nearly constant value when the memory cell is changing to an amorphous state.	flicker noise;in-phase and quadrature components;memory cell (binary);phase-change memory;spice 2;simulation	Salin Junsangsri;Fabrizio Lombardi;Jie Han	2015	2015 IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFTS)	10.1109/DFT.2015.7315126	flicker noise;burst noise;electronic engineering;telecommunications;network switch;noise temperature;computer science;engineering;electrical engineering;frequency;resistance;phase noise;noise generator	Arch	20.811994683888358	59.634976137737304	31591
6d2b9a78ce1897ab39e7a2491e8cf348684de527	spec-based repeater insertion and wire sizing for on-chip interconnect	dynamic programming;moment matching;method of moments;integrated circuit layout;dynamic pro gramming;repeaters wire delay estimation delay effects integrated circuit interconnections runtime dynamic programming constraint optimization time factors context modeling;integrated circuit modelling integrated circuit interconnections delay estimation integrated circuit layout method of moments circuit layout cad dynamic programming circuit optimisation vlsi;chip;circuit simulation;rc interconnect delay optimization spec based repeater insertion spec based wire sizing on chip interconnect ic interconnect optimization transition time constraints user specified delay user specified transition time rc nets moment matching techniques rc delays driver repeater delay model;integrated circuit modelling;integrated circuit interconnections;timing optimization;vlsi;circuit layout cad;circuit optimisation;delay estimation;time constraint	Recently Lillis,et al.presented an elegant dynamic programming approach to RC interconnect delay optimization through driver sizing, repeater insertion, and, wire sizing which employs the Elmore delay model for RC delay estimation and a crude repeater delay model. This approach, however, ignores an equally important aspect of interconnect optimization: transition time constraints at the sinks. More importantly, Elmore delay techniques because of their inherent inaccuracy are not suited to spec-based esign which is directed towards synthesizing nets with user-specified delay/transition time requirements at the sinks. In this paper we present techniques for delay and transition time optimization for RC nets in the context of accurate moment-matching techniques for computing the RC delays and transition times, and an accurate driver/repeater delay model. The asymptotic increase in runtime over the Elmore delay model is O (q2) where q is the order of the moment-matching approximation. Experiments on industrial nets indicate that this increase in runtime is acceptable. Our algorithm yields delay and transition time estimates within 5% of circuit simulation results.	algorithm;approximation;dynamic programming;electronic signatures in global and national commerce act;electronic circuit simulation;elmore delay;mathematical optimization;rc time constant;repeater insertion;requirement;rise time;spice;spec#;waveform	Noel Menezes;Charlie Chung-Ping Chen	1999		10.1109/ICVD.1999.745201	chip;embedded system;electronic engineering;real-time computing;method of moments;delay calculation;telecommunications;computer science;engineering;electrical engineering;elmore delay;dynamic programming;integrated circuit layout;very-large-scale integration;statistics	EDA	17.087758152097656	54.17534671264397	31592
2f9deb43c1c42feff8ae94086648ccd99a052242	evolutionary fault tolerance method based on virtual reconfigurable circuit with neural network architecture		With the continuous development of computer and electronics, the idea of artificial intelligence has been integrating into the fault tolerance research. As a valuable and prospective intelligent fault tolerance technique in high reliability and high safety applications, the evolvable hardware fault tolerance technique is becoming an important and widely applicable method. However, this technique confronts two difficult problems: evolved circuit scale and evolution efficiency. Toward these problems, we present a programmable architecture called neural network architecture-based virtual reconfigurable circuit (NNA-VRC), and an evolutionary fault tolerance method based on this programmable architecture. The NNA-VRC-based evolution method simplifies the structure and configuration of programmable architecture, avoids illegal interconnections during the circuit evolution, and implements high level (module level) evolution. The experiments of this paper show that a function module scale circuit is evolved efficiently. Furthermore, NNA-VRC-based evolution method can recovery from many injected fault patterns, behaving a strong feature of fault tolerance.		Jian Gong;Mengfei Yang	2018	IEEE Trans. Evolutionary Computation	10.1109/TEVC.2017.2779874	electronics;architecture;machine learning;artificial neural network;artificial intelligence;field-programmable gate array;mathematics;fault tolerance;real-time computing;evolvable hardware	EDA	8.789370857738902	48.637409588156885	31604
98edd711c43f9e64723077abd49ea6a4fdca4126	addressing optimization for loop execution targeting dsp with auto-increment/decrement architecture	dsp code generator;loop execution;optimizing dsp compiler;auto-increment/decrement feature;digital signal processing chips;address register allocation;optimization;dsp processors;optimising compilers;decrement architecture;register allocation;registers;memory management;application software;computer architecture;code generation;digital signal processing;computer science	Targeting DSP with Auto-Increment/Decrement Architecture Wei-Kai Cheng Youn-Long Lin Computer Science Department, Tsing Hua University Hsin-Chu, Taiwan 30043, R.O.C. Abstract Since most DSP applications access large amount of data stored in the memory, a DSP code generator must minimize the addressing overhead. In this paper, we propose a method for addressing optimization in loop execution targeted toward DSP processors with autoincrement/decrement feature in their address generation unit. Our optimization methods include a multi-phase data ordering and a graph-based address register allocation. The proposed approaches have been evaluated using a set of core algorithms targeted towards the TI TMS320C40 DSP processor. Experimental results show that our system is indeed more e ective compared to a commercial optimizing DSP compiler.	address generation unit;algorithm;algorithmic efficiency;central processing unit;code generation (compiler);compiler;computer science;digital signal processor;for loop;geforce 900 series;increment and decrement operators;local interconnect network;mathematical optimization;memory address register;opti 929;optimization problem;overhead (computing);path (graph theory);program optimization;register allocation	Wei-Kai Cheng;Youn-Long Lin	1998			computer architecture;parallel computing;real-time computing;computer science;texas instruments davinci	EDA	-0.3095834396901336	51.732902208560525	31613
61e120e085d0599b017c5036b655b9a38ec33930	routing-architecture-aware analytical placement for heterogeneous fpgas	placement;physical design;field programmable gate arrays delays routing wires law random access memory;fpga;network routing field programmable gate arrays;field programmable gate arrays heterogeneous fpga routing architecture aware analytical placement segmented routing heterogeneous circuit architecture cost function complex block density model;fpga physical design placement	Placement is a crucial stage for FPGA implementation. Most FPGA placers optimize their placement results by minimizing half-perimeter wirelength (HPWL). Due to the segmented routing architecture in FPGAs, however, the HPWL function cannot model routed wirelength and delay well. The mismatch of the HPWL function might lead to inferior routing results. Further, heterogeneous circuit blocks in a modern FPGA make the placement problem more complex. Consequently, it is desirable to consider the segmented routing and heterogeneous circuit architecture for FPGA placement. This paper presents a routing-architecture-aware analytical placement algorithm for heterogeneous FPGAs. Our algorithm proposes a routing-architecture-aware cost function to make placement results adapt to the corresponding routing architecture, and a complex block density model to effectively handle the heterogeneity. Experimental results show that our placer can achieve 9% smaller critical path delay and 5% shorter routed wirelength with shorter runtime, compared to the state-of-the-art academic placer.	algorithm;critical path method;field-programmable gate array;loss function;perimeter;routing	Sheng-Yen Chen;Yao-Wen Chang	2015	2015 52nd ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2744769.2744903	physical design;embedded system;electronic engineering;parallel computing;real-time computing;computer science;engineering;routing;field-programmable gate array;placement	EDA	14.651530355722674	53.15476979683596	31676
5b33bc162c445a54c54b7eb01cfb5a9183fe159a	reconfigurable computing - evolution of von neumann architecture	programmable chip;integrated circuit;reconfigurable computing;reconfigurable architectures;chip;von neumann architecture evolution;multimedia processor;next generation;turning point;power consumption;reconfigurable chip architecture;mobile application;logic function	With the rapid progress of semiconductor technology, billions transistors can be integrated on a single silicon chip. However, increasing power consumption, complicated system function and huge investment will slow down the development of process technology and impact the integrated circuit products. While technology goes toward to 22nm, few companies can support the development of complicated chips and much less the fabs to manufacture them. A configurable, programmable chip that can implement not only the necessary logic function but also the computing/processing might be the next generation architecture. This presentation will start from classic Von Neumann architecture, evolves it step by step toward to a reconfigurable chip architecture, and explore the possibility to develop a multimedia processor for mobile applications. As semiconductor technology is being on a turning point, some strategies will also be discussed.	boolean algebra;integrated circuit;mobile app;reconfigurable computing;semiconductor fabrication plant;transistor;von neumann architecture	Shaojun Wei	2010		10.1109/FPT.2010.5681475	chip;embedded system;reconfigurable computing;computer science;operating system;integrated circuit	Arch	12.37678095441104	58.546854412854444	31686
90a105ba695cc51eafd00e745c56089dcd3f0386	layout decomposition for hybrid e-beam and dsa double patterning lithography		The printability problem of chip making becomes challenging in advanced process nodes. At present, various lithography technologies such as multiple patterning (MP), directed self-assembly (DSA), electron beam (e-beam), and their combinations are being considered. In this paper, the corresponding layout decomposition problems for contact/via generation are studied. In particular, we investigate the simultaneous DSA template and e-beam throughput optimization. First, we present an exact method based on an ILP formulation. Then, a graph-based algorithm is developed. The co-optimization problem for DSA double patterning with e-beam is formulated as a minimum hitting set problem. A primal-dual based algorithm is then derived for solving the problem effectively. Experimental results show that compared with a two-stage method, our method can achieve around 20.6% throughput improvement and 18.7% template cost reduction.	algorithm;benchmark (computing);electron;emoticon;explanation-based learning;iteration;mathematical optimization;optimization problem;ordered pair;self-assembly;set cover problem;throughput;vertex (geometry);vertex (graph theory);vertex cover	Yunfeng Yang;Fan Yang;Wai-Shing Luk;Changhao Yan;Xuan Zeng;Xiangdong Hu	2017	2017 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2017.8050912	throughput;chip;electronic engineering;lithography;computer science;multiple patterning;electron beam processing;graph	EDA	14.611159033022725	53.16721961191875	31731
1b5c5ccbbfbf76d6a84a2ecea70a442c4dcc914b	why aren't operating systems getting faster as fast as hardware?	operating system;file system;memory bandwidth	This note evaluates several hardware platforms and operating systems using a set of benchmarks that test memory bandwidth and various operating system features such as kernel entry/exit and file systems. The overall conclusion is that operating system performance does not seem to be improving at the same rate as the base speed of the underlying hardware. Copyright  1989 Digital Equipment Corporation d i g i t a l Western Research Laboratory 100 Hamilton Avenue Palo Alto, California 94301 USA	benchmark (computing);memory bandwidth;operating system;palo	John K. Ousterhout	1990			flash file system;hardware compatibility list;embedded system;embedded operating system;parallel computing;real-time operating system;device file;operating system	OS	-1.506853141279836	45.22113577749489	31746
10fcaef2da74568229cd0adb973038befd827f7f	test data compression and test time reduction of longest-path-per-gate tests based on illinois scan architecture	data compression;test data compression;automatic test pattern generation;sequential circuits;longest path;resistive opens test data compression test data volume reduction test application time reduction longest path per gate tests illinois scan architecture delay test set atpg sequential circuits localized delay defects resistive shorts;logic testing;integrated circuit testing;test data compression circuit testing automatic test pattern generation sequential circuits wire logic testing sequential analysis delay effects gas detectors robustness;integrated logic circuits;delays;integrated logic circuits data compression automatic test pattern generation logic testing sequential circuits integrated circuit testing delays	Localized delay defects, like resistive shorts, resistive opens, etc., can be effectively detected by testing the longest testable path through each wire (or gate) in the circuit. Such a delay test set is referred to as a longest-path-per-wire test set. In this paper we study test data volume and test application time reduction techniques for such tests based on the Illinois scan architecture. We present a novel ATPG flow to quickly determine longest-path-per-wire test sets under constraints imposed by the Illinois scan architecture. Results of experiments on ISCAS sequential circuits are presented. On an average we achieve a test data volume reduction of 2.79X and number of test cycles reduction of 3.28X for robust path delay, tests (as compared to the case without Illinois scan). The corresponding numbers for non-robust tests are 3.58X and 4.24X.	data compression;longest path problem;test data	Manish Sharma;Janak H. Patel;Jeff Rearick	2003		10.1109/VTEST.2003.1197628	data compression;electronic engineering;scan chain;real-time computing;longest path problem;computer science;automatic test pattern generation;test compression;sequential logic;algorithm	EDA	19.938952623298043	53.04978445507763	31762
5b13559dfdd36f5af3ddb2fc87dbbf526e22606f	integrating bist techniques for on-line soc testing	automotive engineering;volume production test;microprocessor chips system on chip built in self test integrated circuit testing;user defined logic core bist technique built in self test on line soc testing system on chip integrated circuits ip volume production test test architecture mission mode fault detection;performance evaluation;integrated circuit yield;integrated circuit;on line testing;user defined logic core;built in self test circuit testing performance evaluation system on a chip integrated circuit manufacture coupling circuits integrated circuit yield manufacturing processes production automotive engineering;coupling circuits;system on a chip;system on chip integrated circuits;chip;manufacturing processes;built in self test;complex system;system on chip;ip;fault detection;integrated circuit testing;production;circuit testing;bist technique;design for test;on line soc testing;test architecture;integrated circuit manufacture;mission mode fault detection;microprocessor chips	Today's complex system-on-chip integrated circuits include a wide variety of functional IPs whose correct manufacturing must be guaranteed by IC producers. Infrastructure IPs are increasingly often inserted to achieve this purpose; such blocks, explicitly designed for test, are coupled with functional IPs both to obtain yield improvement during the manufacturing process and to perform volume production test. In some fields (e.g., the automotive one) there is a strong need for flexible and reusable test architectures able to guarantee effective and low-cost solutions for mission-mode fault detection capabilities within complex SoCs. In this paper, we propose to reuse structures inserted to support the manufacturing test to perform non-concurrent on-line test of SoCs. The feasibility of this approach and its costs have been evaluated on a real case of study including processor, memory and user defined logic cores.	built-in self-test;complex system;fault detection and isolation;integrated circuit;online and offline;system on a chip	Alberto Manzone;Paolo Bernardi;Michelangelo Grosso;Maurizio Rebaudengo;Ernesto Sánchez;Matteo Sonza Reorda	2005	11th IEEE International On-Line Testing Symposium	10.1109/IOLTS.2005.38	system on a chip;embedded system;complex systems;electronic engineering;computer science;engineering;computer engineering	Embedded	10.534208231002523	54.08506409211645	31793
ea0310cfcd38472b5d72cdab2aceefa7852fc882	a recursive technique for computing lower-bound performance of schedules	behavioral synthesis;performance evaluation;search space;data flow graphs;processor scheduling flow graphs space exploration filters read only memory optimal control scheduling algorithm greedy algorithms relaxation methods integer linear programming;search problems data flow graphs scheduling firmware high level synthesis elliptic filters digital filters performance evaluation;elliptic filters;firmware;rom asapuc time step solution space exploration bad solution pruning search space reduction recursive technique lower bound performance data path schedules as soon as possible under constraint data flow graph predecessor nodes leaf nodes greedy technique lower bound estimation local microcode generation behavioral synthesis optimal microcode sequence elliptic wave filter benchmark multiport ram;data flow graph;high level synthesis;scheduling;digital filters;search problems;lower bound	Executive Committee Technical Program Committee PCs, Workstations, Mainframes: Changing Roles and Markets The Power PC Architecture: From Palmtops to Teraflops Symbolic Analysis Methods for Masks, Circuits, and Systems p. 6 Wearable Computers: Merging Information Space with the Workspace p. 10 Design for Testability: Today and in the Future p. 14 A Recursive Technique for Computing Lower-Bound Performance of Schedules p. 16 Lower Bounds on the Iteration Time and the Number of Resources for Functional Pipelined Data Flow Graphs p. 21	design for testing;flops;iteration;mainframe computer;recursion (computer science);traffic flow (computer networking);workspace;workstation	Michel Langevin;Eduard Cerny	1993		10.1109/ICCD.1993.393412	embedded system;firmware;mathematical optimization;electronic engineering;real-time computing;digital filter;computer science;theoretical computer science;operating system;machine learning;data-flow analysis;upper and lower bounds;high-level synthesis;scheduling;algorithm	HPC	2.82213477555239	37.5548776059299	31821
bc24d465f7c15d64fcdfd55c417fa84287a01d12	an efficient online direction-preserving compression approach for trajectory streaming data	streaming data;gpu;big data;compression computing;parallel processing	Online trajectory compression is an important method of efficiently managing massive volumes of trajectory streaming data. Current online trajectory methods generally do not preserve direction information and lack high computing performance for the fast compression. Aiming to solve these problems, this paper first proposed an online direction-preserving simplification method for trajectory streaming data, online DPTS by modifying an offline direction-preserving trajectory simplification (DPTS) method. We further proposed an optimized version of online DPTS called online DPTS by employing a data structure called bound quadrant system (BQS) to reduce the compression time of online DPTS. To provide a more efficient solution to reduce compression time, this paper explored the feasibility of using contemporary general-purpose computing on a graphics processing unit (GPU). The GPU-aided approach paralleled the major computing part of online DPTS that is the SP-theo algorithm. The results show that by maintaining a comparable compression error and compression rate, (1) the online DPTS outperform offline DPTS with up to 21% compression time, (2) the compression time of online DPTS algorithm is 3.95 times faster than that of online DPTS, and (3) the GPU-aided method can significantly Email address: lizhe.wang@gmail.com (Lizhe Wang) Preprint submitted to Journal of LTEX Templates September 13, 2016 reduce the time for graph construction and for finding the shortest path with a speedup of 31.4 and 7.88 (on average), respectively. The current approach provides a new tool for fast online trajectory streaming data compression.	algorithm;computer graphics;data compression;data structure;email;general-purpose markup language;graphics processing unit;level of detail;maxwell (microarchitecture);online and offline;pathfinding;shortest path problem;speedup;stream (computing);streaming media	Ze Deng;Wei Han;Lizhe Wang;Rajiv Ranjan;Albert Y. Zomaya;Wei Jie	2017	Future Generation Comp. Syst.	10.1016/j.future.2016.09.019	parallel processing;parallel computing;real-time computing;simulation;big data;computer science;theoretical computer science;database;distributed computing	HPC	1.4941823773686087	38.56664631970721	31829
2ea63834757cb74947eefac6c53e00d16b513612	e-save: saving energy by smart serves	smart energy;mobile apps smart service cloud computing smart energy;energy usage patterns;mobile apps;smarter energy usage;context aware activity monitoring;power system measurement;data mining;smart grid;autonomous energy saving opportunities;smart grids;power engineering computing;energy consumption behavior;smart power grids;monitoring;smart serves;energy consumption;energy consumption monitoring communities smart grids mobile communication data mining adaptation models;mobile communication;e save prototype system smart serves smart grid nonintrusive monitoring energy consumption behavior autonomous energy saving opportunities smarter energy usage context aware activity monitoring energy usage optimization energy usage patterns;nonintrusive monitoring;ubiquitous computing;communities;adaptation models;smart service;e save prototype system;cloud computing;energy usage optimization;ubiquitous computing power engineering computing power system measurement smart power grids	The Smart Grid holds promise for transforming the behavior of individuals and communities towards a more efficient and greener use of electric power. There is a growing demand for non-intrusive monitoring of energy consumption behavior at multiple scales and also for autonomous energy saving opportunities for both individuals and communities. In this demo, we present a sentient service framework for a smart application, named E-SAVE that aims to monitor activities by a user or community and configure the environment of devices for a smarter energy usage. The key building block to the smart application capability is context aware activity monitoring and energy usage optimization by recognizing appropriate energy-usage patterns pertaining to various activities, areas, and devices. We have developed the E-SAVE prototype system to monitor the usage of energy by a user or community and configure the environment of devices for smart usage of energy.	autonomous robot;mathematical optimization;prototype;sentience	Swati Soni;Yugyung Lee	2012	2012 IEEE International Symposium on a World of Wireless, Mobile and Multimedia Networks (WoWMoM)	10.1109/WoWMoM.2012.6263730	embedded system;real-time computing;telecommunications;computer science;operating system;smart grid;computer security;ubiquitous computing	Mobile	1.1007435037451496	32.79305984532233	31900
6be2177a3a818f807350cf713544b58db74b2b03	the benefits of using variable-length pipelined operations in high-level synthesis	cache;high level synthesis;dram;memory latency	Current high-level synthesis systems synthesize arithmetic units of a fixed known number of stages, and the scheduler mainly determines when units are activated. We focus on scheduling techniques for the high-level synthesis of pipelined arithmetic units where the number of stages of these operations is a free parameter of the synthesis. This problem is motivated by the ability to automatically create pipelined functional units, such as multipliers, with different pipe lengths. These units have different characteristics in terms of parallelism level, clock latency, frequency, etc. This article presents the Variable-length Pipeline Scheduler (VPS). The ability to synthesize variable-length pipelined units expands the known scheduling problem of high-level synthesis to include a search for a minimal number of hardware units (operations) and their desired number of stages. The proposed search procedure is based on algorithms that find a local minima in a d-dimensional grid, thus avoiding the need to evaluate all possible points in the space. We have implemented a C language compiler for VPS targeting FPGAs. Our results demonstrate that using variable-length pipeline units can reduce the overall resource usage and improve the execution time when synthesized onto an FPGA. The proposed search is sufficiently fast, taking only a few seconds, allowing an interactive mode of work. A comparison with xPilot shows a significant saving of hardware resources while maintaining comparable execution times of the resulting circuits. This work is an extension of a previous paper [Ben-Asher and Rotem 2008]	algorithm;clock rate;compiler;field-programmable gate array;high- and low-level;high-level synthesis;maxima and minima;parallel computing;run time (program lifecycle phase);scheduling (computing);virtual private server	Yosi Ben-Asher;Nadav Rotem	2013	ACM Trans. Embedded Comput. Syst.	10.1145/2539036.2539048	embedded system;parallel computing;real-time computing;cas latency;cache;computer science;operating system;high-level synthesis;dram	EDA	-1.245671381999277	51.976147601287444	31936
073b7ea9365974d74fde7d75c170746601e2ac7d	vaet-stt: a variation aware estimator tool for stt-mram based memories	microprocessors;reliability;resistance;magnetic tunneling computer architecture tools microprocessors switches reliability resistance;computer architecture;tools;switches;magnetic tunneling	Spin Transfer Torque Magnetic Random Access Memory (STT-MRAM) is a promising candidate to replace CMOS based on-chip memories due to its advantages such as non-volatility, high density and scalability. However, its stochastic switching and higher sensitivity to process variation compared to CMOS memories can significantly affect its performance, energy and reliability. Although a few works exist which analyze the impact of process variation at the bit-cell level, such analysis at the system level is missing. We have bridged this gap in our work. Specifically, we quantify the effect of stochasticity and process variations from the cell-level to the overall memory system and perform a variation-aware memory configuration optimization for energy or performance while meeting reliability constraints. Our system-level variation-aware framework has been built on top of the well-known NVSim engine. The results show that our framework can provide more realistic margins and the optimized variation-aware memory configuration could be significantly different from the conventional framework.	bit cell;cmos;magnetoresistive random-access memory;mathematical optimization;non-volatile memory;peripheral;random access;scalability;stochastic process;volatility;warez;whole earth 'lectronic link	Sarath Mohanachandran Nair;Rajendra Bishnoi;Mohammad Saber Golanbari;Fabian Oboril;Mehdi Baradaran Tahoori	2017	Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017	10.23919/DATE.2017.7927221	embedded system;electronic engineering;real-time computing;network switch;computer science;engineering;electrical engineering;operating system;reliability;resistance	EDA	19.803378588668195	59.34442683293284	32004
7ffe57e4c9b321048edf8c7a13ad805e7a5f52e6	intermediate-level vision tasks on a memory array architecture	algoritmo paralelo;vision ordenador;architecture systeme;parallel algorithm;computer vision tasks;image processing;linear array;procesamiento imagen;region labeling;transformacion hough;traitement image;algorithme parallele;computer vision;algorithms;arquitectura sistema;image analysis;vision ordinateur;hough transformation;procesador;hough transform;transformation hough;processeur;system architecture;convex hull;connected component;high speed;hardware implementation;parallel processing;processor;memory array architecture	With the fast advances in the area of computer vision and robotics there is a growing need for machines that can “understand images” at very high speed. A conventional von Neumann computer is not suitable for this purpose, because it takes a tremendous amount of time to solve most typical image analysis problems. Thus, it is now imperative to study computer vision in a parallel processing framework in order to reduce the processing time. In this paper we demonstrate the applicability of a simple memory array architecture to some intermediate-level computer vision tasks. This architecture, called theAccess Constrained Memory Array Architecture (ACMAA) has a linear array of processors which concurrently access distinct rows or columns of an array of memory modules. Because of its efficient local and global communication capabilities ACMAA is well suited for low-level as well as intermediate-level vision tasks. This paper presents algorithms for connected component labeling, determination of area, perimeter and moments of a labeled region, convex hull of a region, and Hough transform of an image. ACMAA is well suited to an efficient hardware implementation because it has a modular structure, simple interconnect and limited global control.	algorithm;central processing unit;column (database);computer vision;connected component (graph theory);connected-component labeling;convex hull;dimm;high- and low-level;hough transform;image analysis;imperative programming;parallel computing;perimeter;robotics;von neumann architecture	Poras T. Balsara;Mary Jane Irwin	1992	Machine Vision and Applications	10.1007/BF01212431	hough transform;hashed array tree;parallel processing;computer vision;image analysis;image processing;computer science;theoretical computer science;computer graphics (images)	Arch	10.9770649206751	35.63432979234482	32035
2786bd16617f3783529e65c9a1039d25a45486be	autonomous power management for embedded systems using a non-linear power predictor		Embedded systems execute applications that exercise the hardware differently depending on the computation task, generating varying workloads with time. Energy minimization can be reached exploring the optimal CPU frequency for each workload. We propose an autonomous and online approach, capable of minimizing energy through adaptation to these workload variations even in an unknown environment. In the proposed approach we use a reinforcement learning algorithm that suitably selects the appropriate CPU frequency based on workload predictions to minimize energy consumption. The proposed approach is validated through simulation using real smartphone data, an ARM Cortex A7 processor used in a commercial smartphone with Android 4.4.4 version was employed. Our proposed approach demonstrated to have an improvement in the Q-learning cost function and can effectively minimize energy consumption by up to 29% compared to the existing approaches.	arm architecture;algorithm;android;apple a7;autonomous robot;branch predictor;central processing unit;computation;embedded system;energy minimization;kerrison predictor;linux;loss function;mobile app;nonlinear system;open-source software;power management;q-learning;reinforcement learning;simulation;smartphone;virtual reality	Sidartha A. L. Carvalho;Daniel C. Cunha;Abel G. da Silva Filho	2017	2017 Euromicro Conference on Digital System Design (DSD)	10.1109/DSD.2017.68	workload;real-time computing;power management;computer science;energy consumption;computation;embedded system;android (operating system);central processing unit;arm architecture;reinforcement learning	EDA	-3.696454214611444	58.40176997085299	32055
3cfc0b1e3c19ffb422f0c98754c382a9d8fbbc0b	fully homomorphic simd operations	homomorphic simd operation;homomorphic scheme;encrypted database lookup;aes homomorphically;homomorphic public key encryption;simd style operation;simd operation;data element;slow key generation process;vercauteren system;key generation method	At PKC 2010 Smart and Vercauteren presented a variant of Gentry’s fully homomorphic public key encryption scheme and mentioned that the scheme could support SIMD style operations. The slow key generation process of the Smart–Vercauteren system was then addressed in a paper by Gentry and Halevi, but their key generation method appears to exclude the SIMD style operation alluded to by Smart and Vercauteren. In this paper, we show how to select parameters to enable such SIMD operations, whilst still maintaining practicality of the key generation technique of Gentry and Halevi. As such, we obtain a somewhat homomorphic scheme supporting both SIMD operations and operations on large finite fields of characteristic two. This somewhat homomorphic scheme can be made fully homomorphic in a naive way by recrypting all data elements seperately. However, we show that the SIMD operations can be used to perform the recrypt procedure in parallel, resulting in a substantial speed-up. Finally, we demonstrate how such SIMD operations can be used to perform various tasks by studying two use cases: implementing AES homomorphically and encrypted database lookup.	aes instruction set;encryption;key generation;lookup table;pkc (conference);public-key cryptography;simd;while	Nigel P. Smart;Frederik Vercauteren	2011	IACR Cryptology ePrint Archive	10.1007/s10623-012-9720-4	theoretical computer science	Crypto	8.497548703513813	44.1454262635515	32065
486f69ed86f1b51425b99de2e75afb599d69eca8	a proposal of a single-synchronized solver suited to large scale linear systems on parallel computers with distributed memory	computer science all		distributed memory;linear system;parallel computing;solver;system of linear equations	Seiji Fujino;Keiichi Murakami;Kosuke Iwasato	2013		10.3233/978-1-61499-381-0-173	computational science;parallel computing;computer science;theoretical computer science	HPC	-3.4263577703803496	37.635774742341845	32083
b77d6b762eee6025b28c0cbe8526a1eea66d68a2	augmented recurrence hopping based run-length coding for test data compression applications	circuit under test (cut);augmented recurrence hopping based run-length coding (arhrlc);run-length coding (rlc);huffman coding (hc);shifted alternating frequency directed run-length coding (safdr)	The advancement in technologies has been increasing with increase in integrating scales which allows fabricating millions of transistors on a chip. This demands the efficient testing circuit to evaluate the fault present, where the large volume of data volume needs to be tested during manufacturing and fabrication. Therefore, the challenging fact arises in methodologies to achieve the test data compression. Even though various techniques in the present scenario reduce the testing time, improvement in data volume reduction is still a demanding factor. The proposed scheme aims to obtain a Large Compression Ratio. By using Augmented Recurrence Hopping based Run-Length Coding (ARHRLC) Test Data volume can be reduced and data can be compressed. The proposed ARHRLC coding technique which compares the group code based test vector and it’s duplicate. Data sequence can be decreased in terms volume and area overhead. From the analysis of ISCAS 89 benchmark circuit’s performance shows the proposed coding scheme outperforms the conventional test data compression methods. The Augmented Recurrence Hopping based Run-Length Coding procedure skips the repeated sequence in the test vector group and estimate conversely perfect example of a separate test information section or various test information fragment. The test result demonstrates that the compression proportion and calculation time is lessened.	data compression;frequency-hopping spread spectrum;run-length encoding;test data	K. Radhika;D. Mohanageetha	2018	Wireless Personal Communications	10.1007/s11277-018-5372-7	coding (social sciences);computer science;real-time computing;chip;compression ratio;test vector;compression (physics);test data;group code	Mobile	20.204173329499678	53.12019585873666	32112
6ab41a7d1e979999a5d7b95ed1ed9d6f95edd6be	optimizing stream program performance on cgra-based systems?	heterogeneous multiprocessors hmp;energy efficiency;task allocation scheduling;os kernel;system on chip soc;operating system;power aware systems;load balancing;linux scheduler;integer linear programming based solution stream program performance optimization cgra based systems coarse grained reconfigurable architecture coprocessors dsp kernel digital signal processing multimedia kernel compute intensive kernels stream applications dsp application mapping streamit language;mpsoc;embedded software;reconfigurable architectures integer programming linear programming;kernel optimization runtime memory management schedules prefetching discrete cosine transforms	Coarse-Grained Reconfigurable Architectures (CGRAs), often used as coprocessors for DSP and multimedia kernels, can deliver highly energy-efficient execution for compute-intensive kernels. Simultaneously, stream applications, which consist of many actors and channels connecting them, can provide natural representations for DSP applications, and therefore be a good match for CGRAs. We present our results of mapping DSP applications written in StreamIt language to CGRAs, along with our mapping flow. One important challenge in mapping is how to manage the multitude of kernels in the application for the limited local memory of a CGRA, for which we present a novel integer linear programming-based solution. Our evaluation results demonstrate that our software and hardware optimizations can help generate highly efficient mapping of stream applications to CGRAs, enabling far more energy-efficient executions (7× worse to 50× better) compared to using state-of-the-art GP-GPUs.	coprocessor;digital signal processor;graphics processing unit;integer programming;linear programming;optimizing compiler;reconfigurable computing	Hongsik Lee;Dong Nguyen;Jongeun Lee	2015	2015 52nd ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2744769.2744884	embedded system;parallel computing;real-time computing;embedded software;computer science;load balancing;operating system;efficient energy use;programming language	EDA	-1.4639154565214085	50.74303847621306	32118
e3295b5351252e1858efe47475f5920019d3ba59	algorithm optimization using a rule-based system. a case study: the direct kinematic solution in robotics	rule based system;real time;numerical calculation;computer architecture;robot arm;cost effectiveness;logic programs	The computation burden of intensive numerical real-time algorithms is a problem encountered in robotics and many other fields. A cost-effective solution for the implementation of these algorithms requires knowledge of computer architecture, compiler technology and algorithms. A cost-effective numeric processing methodology using a combined hardware-software approach and taking advantage of logic programming tools is presented. The methodology is based on optimizing the numerical calculation process of the algorithm. It also enables the specification of hardware resources. The process uses a rule-based-system (RBS) implemented in the logic programming language Prolog to automatically reduce the number of operations in the numerical execution of the algorithm and optimizes the use of hardware resources. The methodology provides a solution for the problems of handshake overhead and algorithm translation efficiency.	algorithm;mathematical optimization;robotics;rule-based system	Helnye Azaria;Adrian Dvir	1993	Journal of Intelligent and Robotic Systems	10.1007/BF01257947	rule-based system;simulation;cost-effectiveness analysis;robotic arm;computer science;artificial intelligence;theoretical computer science;machine learning;algorithm	Robotics	3.0810854946954187	39.61582479194913	32167
3e4f4582511f60c22f4762c64b22328a1ef42764	memory access optimized routing scheme for deep networks on a mobile coprocessor	peak performance memory access optimized routing scheme deep networks mobile coprocessor hardware accelerated real time implementation deep convolutional neural network dcnn mobile platform 3d convolution hardware accelerator streaming data bandwidth limited system data reuse computational resources xilinx zynq 7000 all programmable soc weight level parallelization node level parallelization;system on chip convolution coprocessors memory architecture neural nets;welding three dimensional displays computational modeling artificial neural networks convolution streaming media	In this paper, we present a memory access optimized routing scheme for a hardware accelerated real-time implementation of deep convolutional neural networks (DCNNs) on a mobile platform. DCNNs consist of multiple layers of 3D convolutions, each comprising between tens and hundreds of filters and they generate the most expensive operations in DCNNs. Systems that run DCNNs need to pass 3D input maps to the hardware accelerators for convolutions and they face the limitation of streaming data in and out of the hardware accelerator. The bandwidth limited systems require data reuse to utilize computational resources efficiently. We propose a new routing scheme for 3D convolutions by taking advantage of the characteristic of DCNNs to fully utilize all the resources in the hardware accelerator. This routing scheme is implemented on the Xilinx Zynq-7000 All Programmable SoC. The system fully explores weight level and node level parallelization of DCNNs and achieves a peak performance 2x better than the previous routing scheme while running DCNNs.	artificial neural network;computational resource;convolution;convolutional neural network;coprocessor;hardware acceleration;map;mathematical optimization;mobile operating system;parallel computing;real-time clock;routing;streaming media	Aysegul Dundar;Jonghoon Jin;Vinayak Gokhale;Berin Martini;Eugenio Culurciello	2014	2014 IEEE High Performance Extreme Computing Conference (HPEC)	10.1109/HPEC.2014.7040963	parallel computing;real-time computing;computer science;theoretical computer science	Arch	3.6246342206889572	43.9032828340723	32185
432ecaadb7c6939cc32fc1fe0e8013c0f04da603	a fast modular inversion fpga implementation over gf(2m) using modified x2n unit		Modular inversion is an important step in Elliptic Curve Cryptography (ECC). Itoh-Tsujii's algorithm (ITA) is a commonly applied modular inversion algorithm. This paper proposes a novel architecture of modular inversion implementation based on ITA. The modular multiplier and 2n units are modified into non-iterative logic to increase clock frequency. Least-clock-cycle-ITA and High-speed-ITA architectures are proposed to minimize latency in different situations. Both architectures are implemented on FPGA Virtex-5. LCC-ITA completes modular inversion in 9 clock cycles with maximum clock frequency 126.1MHz, while HS-ITA completes two modular inversions in 20 clock cycles with maximum clock frequency 177.6MHz. The performance of the proposed architectures is 56% and 134% higher than the best work before.	clock rate;clock signal;computation;elliptic curve cryptography;exclusive or;field-programmable gate array;iteration;itoh-tsujii inversion algorithm;time complexity	Jiakun Li;Zhe Li;Chengbo Xue;Jingqi Zhang;Wei Gao;Shan Cao	2018	2018 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2018.8351614	field-programmable gate array;latency (engineering);electronic engineering;architecture;elliptic curve cryptography;gf(2);clock rate;computer science;modular design;logic gate	Embedded	9.93889275724925	44.72584640084719	32191
2a73274bd21acc938b2f05b42c5e9e1489e3b3e1	effective ir-drop reduction in at-speed scan testing using distribution-controlling x-identification	integrated circuit testing;ir-drop reduction;at-speed scan testing;distribution-controlling x-identification;test data modification;test relaxation	Test data modification based on test relaxation and X-filling is the preferable approach for reducing excessive IR-drop in at-speed scan testing to avoid test-induced yield loss. However, none of the existing test relaxation methods can control the distribution of identified don't care bits (X-bits), thus adversely affecting the effectiveness of IR-drop reduction. In this paper, we propose a novel test relaxation method, called Distribution-Controlling X-Identification (DC-XID), which controls the distribution of X-bits identified from a set of fully-specified test vectors for the purpose of effectively reducing IR-drop. Experimental results on large industrial circuits demonstrate the effectiveness and practicality of the proposed method in reducing IR-drop, without any impact on fault coverage, test data volume, or test circuit size.	amiga walker;combinational logic;don't-care term;fault coverage;glitch;international conference on computer-aided design;l (complexity);linear programming relaxation;power supply;relaxation (approximation);relaxation (iterative method);test data;test vector;very-large-scale integration	Kohei Miyase;Kenji Noda;Hideaki Ito;Kazumi Hatayama;Takashi Aikyo;Yuta Yamato;Hiroshi Furukawa;Xiaoqing Wen;Seiji Kajihara	2008	2008 IEEE/ACM International Conference on Computer-Aided Design	10.1145/1509456.1509480	control engineering;electronic engineering;real-time computing;fault coverage;logic gate;network switch;computer science;engineering;automatic test pattern generation;test compression;software testing	EDA	20.686254597602645	53.41693890623357	32318
d4101acb6c27fcfd307d6154e71f59aab7ee6192	a pipeline architecture for factoring large integers with the quadratic sieve algorithm	11y05;general and miscellaneous mathematics computing and information science;distributed networks;pipeline architecture;68m05;mathematical logic;factoring algorithms;computer networks;computer network;computer architecture;computer calculations;data flow processing;algorithms;logic programs;data flow;programming 990210 supercomputers 1987 1989;quadratic sieve;digital number	We describe the quadratic sieve factoring algorithm and a pipeline architecture on which it could be efficiently implemented. Such a device would be of moderate cost to build and would be able to factor 100-digit numbers in less than a month. This represents an order of magnitude speed-up over current implementations on supercomputers. Using a distributed network of many such devices, it is predicted much larger numbers could be practically factored. Key words, pipeline architecture, factoring algorithms, quadratic sieve AMS(MOS) subject classifications. 11Y05, 68M05	algorithm;integer factorization;pipeline (computing);quadratic sieve;speedup;supercomputer	Carl Pomerance;Jeffrey W. Smith;Randy Tuler	1988	SIAM J. Comput.	10.1137/0217023	data flow diagram;mathematical optimization;combinatorics;mathematical logic;quadratic sieve;computer science;theoretical computer science;mathematics;programming language;algorithm;algebra	Crypto	-0.16715864790040297	38.71151176203567	32338
7c7399b0426bc6408be2b3c12dddcba93dfa87bc	a new complete diagnosis patterns for wiring interconnects	multiprocessor interconnection networks;a new complete diagnosis patterns for wiring interconnects;test patterns;interconnect wiring;complete diagnosis patterns;card module;interconnect test generation algorithm;automatic test equipment;대한전자공학회;testing;boundary scan design techniques;packaging;sungju park;probes;boundary scan testing;chip;test generation algorithms;design technique;test pattern generators;multiple interconnect fault diagnosis complete diagnosis patterns interconnect wiring card module boundary scan design techniques chip to chip interconnection test generation test patterns test generation algorithms interconnect test generation algorithm;permission;registers;대한전자공학회 1995년도 하계종합학술대회 논문집 vol 18 no 1;test generation;the institute of electronics engineers of korea;circuit testing;wiring testing permission test pattern generators registers computer science fault diagnosis packaging automatic test equipment probes;computer science;wiring;circuit analysis computing;chip to chip interconnection test generation;multiple interconnect fault diagnosis;fault diagnosis	It is important to test the various kinds of interconnect faults between chips on a card/module. When boundary scan design techniques are adopted, the chip to chip interconnection test generation and application of test patterns is greatly simpli ed. Various test generation algorithms have been developed for interconnect faults. A new interconnect test generation algorithm is introduced. It reduces the number of test patterns by half over present techniques. It also guarantees the complete diagnosis of mutiple interconnect faults.	algorithm;boundary scan;electrical connection;interconnection;test card;wiring	Sungju Park	1996		10.1145/240518.240556	chip;embedded system;automatic test equipment;packaging and labeling;electronic engineering;parallel computing;telecommunications;computer science;engineering;operating system;test compression;software testing;processor register	EDA	21.48935846708112	51.5232693266805	32404
29effbd37324f690c6edbea0d34f3f683a00c9a7	battery-aware code partitioning for a text to speech system	embedded systems;integrated circuit design;low-power electronics;microprocessor chips;battery aware embedded system design;battery lifetimes;code partitioning;execution time;multicore embedded processors;text to speech system	The advent of multi-core embedded processors has brought along new challenges for embedded system design. This paper presents an efficient, battery aware, code partitioning technique for a text to speech system, which is executed on a multi-core embedded processor. The system achieves significant performance improvements both in terms of execution time as well as battery lifetimes. The mentioned technique provides a new paradigm for battery aware embedded system design which can be easily extended to other applications	central processing unit;embedded system;multi-core processor;programming paradigm;run time (program lifecycle phase);speech synthesis;systems design	Anirban Lahiri;Anupam Basu;Monojit Choudhury;Srobona Mitra	2006	Proceedings of the Design Automation & Test in Europe Conference		embedded system;electronic engineering;application software;real-time computing;computer hardware;computer science;operating system;digital signal processing;speech synthesis;low-power electronics;integrated circuit design	EDA	0.9454984163647563	54.55866761147296	32483
f22e26a4d63e1a89ba9983f0835f18f9857beb7e	a new crossbar architecture based on two serial memristors with threshold	memristors nonvolatile memory;memristors;memory cell crossbar architecture serial memristor threshold characteristic power efficiency nonvolatile memory system sneak path problem;nonvolatile memory;random access storage memristors	This paper presents a memory crossbar based on two serial memristors with threshold characteristic to eliminate the effect of sneak paths, which is a key issue in crossbar memory system leading to great degradation in their performance and power efficiency. At first, we analyze the threshold characteristic of memristor and propose a memristor model with threshold. Based on this model, the paper presents the design and simulation of a non-volatile memory system utilizing two serial memristors with different polarities as a memory cell. This scheme solves the sneak-path problem by taking advantage of the threshold characteristic and the performance with having always high resistance state in all the memory cells, which is validated by simulation results. The scheme also possesses the superior properties of remarkable compatibility and high density.	crossbar switch;elegant degradation;mechwarrior: living legends;memory cell (binary);memristor;non-volatile memory;performance per watt;random-access memory;spice;simulation;time complexity;volatile memory	Xiaoping Wang;Min Chen;Yi Shen;Xiaoya Hu	2015	2015 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2015.7280312	semiconductor memory;parallel computing;memristor;non-volatile memory;resistive random-access memory;computer hardware;computer science;memistor;machine learning;computer memory	EDA	17.11442465679231	60.30528200716845	32486
26a481c5c79924bea8b741fc5b9e34a8dc0ab25d	using failure prediction to improve fpga scrubbing	random access memory;error correction codes;temperature sensors;field programmable gate arrays;power demand;hardware	Programmable hardware devices, specifically FPGAs, are increasingly being used in critical applications. State-of-the-art devices use SRAM memory for configuration purposes, which is very sensitive to faults. Previous studies have shown that, the vast majority of the generated errors have a high latency, and that some failures are due to the accumulation of errors. To overcome these threats, manufacturers, such as Xilinx, allow the designer to periodically refresh the configuration memory cells, through a mechanism known as scrubbing. The decision on its use is based on system availability requirements, and designer knowledge about the implemented system and his operating environment. In this paper we present an approach to automate scrubbing using failure prediction, though the use of both FPGA device health, from internal sensors data, and external environmental conditions. This solution will not only relieve the designer from the scrubbing specification, but also reduces the device's power consumption and scrubbing intrusiveness.	data scrubbing;embedded system;experiment;field-programmable gate array;memory scrubbing;operating environment;propagation of uncertainty;requirement;sensor;static random-access memory	Jose Luis Nunes;João Carlos Cunha;Mário Zenha Rela	2016	2016 Seventh Latin-American Symposium on Dependable Computing (LADC)	10.1109/LADC.2016.29	reliability engineering;embedded system;electronic engineering;real-time computing;computer science;engineering;operating system;memory scrubbing;database;distributed computing;data scrubbing;computer security;field-programmable gate array	Embedded	8.713259238869643	60.051154223683035	32531
c8189d59a38ce38ae6f2759e2d7cccd276ce73e4	digital system simulation: current status and future trends or darwin's theory of simulation	philosophical view;design verification task;future direction simulation;design verification;digital system simulation;future trend;fault simulation;current research issue;current status;hardware;very large scale integration;design automation;geometric modeling	This paper presents a philosophical view of the changing field of design verification. The evolution of simulation and fault simulation are briefly summarized. Current research issues in design verification are discussed. The paper concludes with a discussion of the future direction simulation will take, along with other techniques destined to compete with simulation for the design verification task.	darwin;simulation	Melvin A. Breuer;Alice C. Parker	1981	18th Design Automation Conference		electronic engineering;simulation;electronic design automation;computer science;theoretical computer science;geometric modeling;very-large-scale integration	EDA	8.606391597397069	54.08376554351855	32562
1f38f1a32056d2c3525fc6b1010d4eb92a9f4406	slim: simultaneous logic-in-memory computing exploiting bilayer analog oxram devices		Von Neumann architecture based computers isolate/physically separate computation and storage units i.e. data is shuttled between computation unit (processor) and memory unit to realize logic/ arithmetic and storage functions. This to-and-fro movement of data leads to a fundamental limitation of modern computers, known as the memory wall. Logic in-Memory (LIM) approaches aim to address this bottleneck by computing inside the memory units and thereby eliminating the energy-intensive and time-consuming data movement. However, most LIM approaches reported in literature are not truly “simultaneous” as during LIM operation the bitcell can be used only as a Memory cell or only as a Logic cell. The bitcell is not capable of storing both the Memory/Logic outputs simultaneously. Here, we propose a novel ‘Simultaneous Logic in-Memory’ (SLIM) methodology that allows to implement both Memory and Logic operations simultaneously on the same bitcell in a nondestructive manner without losing the previously stored Memory state. Through extensive experiments we demonstrate the SLIM methodology using non-filamentary bilayer analog OxRAM devices with NMOS transistors (2T-1R bitcell). Detailed programming scheme, array level implementation and controller architecture are also proposed. Furthermore, to study the impact of introducing SLIM array in the memory hierarchy, a simple image processing application (edge detection) is also investigated. It has been estimated that by performing all computations inside the SLIM array, the total Energy Delay Product (EDP) reduces by ~ 40x in comparison to a modern-day computer. EDP saving owing to reduction in data transfer between CPU  Memory is observed to be ~ 780x.	central processing unit;computation;computer;edge detection;electronic data processing;experiment;image processing;memory cell (binary);memory hierarchy;nmos logic;random-access memory;transistor;von neumann architecture	Sandeep Kaur Kingra;Vivek Parmar;Che-Chia Chang;Boris Hudec;Tuo-Hung Hou;Manan Suri	2018	CoRR			Arch	7.37480318927302	40.1047455505666	32589
664345f58d9f7ffad3a0ce4fc6a7275a76e9cf8c	balancing register pressure and context-switching delays in asti systems	hardware to software migration;performance guarantee;pid controller;real time;asynchronous software thread integration;fine grain concurrency;communication protocol;register file;software implemented communication protocols;design space exploration;software thread integration;exhaustive search	This paper makes two contributions to Asynchronous Software Thread Integration (ASTI). First, it presents methods to calculate worst-case secondary thread performance statically. This will enable real-time performance guarantees for the system in future work. Second, it improves the run-time performance of integrated threads by partitioning the register file, allowing faster coroutine calls. Determining the ideal partitioning of the register file is non-trivial if the registers are heterogeneous, which is a common case. We use an exhaustive search to explore the limits of performance possible.We have implemented these analyses in our research compiler Thrint and a shell script to create an automated system for design space exploration. We present experimental results showing the secondary thread performance attainable on an 8-bit embedded microcontroller of the AVR architecture. We automatically integrate two embedded protocols (CAN and MIL-STD-1553B) and two secondary threads (PID controller and serial host interface) and find that in most cases the AVR's 32 registers are adequate for both threads with no slowdown. In two cases slowdowns reach 1.8%, a negligible penalty.	8-bit;atmel avr;best, worst and average case;brute-force search;compiler;coroutine;design space exploration;embedded system;mil-std-1553;microcontroller;pid;real-time computing;register allocation;register file;shell script	Siddhartha Shivshankar;Sunil Vangara;Alexander G. Dean	2005		10.1145/1086297.1086335	pid controller;embedded system;communications protocol;computer architecture;parallel computing;real-time computing;win32 thread information block;computer science;operating system;brute-force search;programming language;register file	Embedded	-3.8386635590354814	51.18653383178189	32622
c9a658f2656dfa29733b7b5782aca7023c5a7ef6	a block-based approach for soc global interconnect electrical parameters characterization	diaphonie;evaluation performance;interconnection;performance evaluation;integrated circuit;crosstalk;evaluacion prestacion;circuito integrado;automatic evaluation;system on a chip;extraccion parametro;parameter extraction;network analysis;interconexion;diafonia;extraction parametre;sistema sobre pastilla;interconnexion;systeme sur puce;temps retard;design space exploration;delay time;analyse circuit;tiempo retardo;analisis circuito;circuit integre	A method for SoC global interconnect characterization is presented. Buses are partitioned in blocks whose electrical characterization is done using reduced size primitives and extending the results to the original structure. The accuracy is measured on typical metrics like delays, crosstalk peaks and reabsorbing time. This work is the basis for an automatic evaluator of interconnect metrics to be used in SoC design space explorations and verification. 1		M. Addino;Mario R. Casu;Guido Masera;Gianluca Piccinini;Maurizio Zamboni	2003		10.1007/978-3-540-39762-5_14	system on a chip;embedded system;real-time computing;crosstalk;network analysis;telecommunications;computer science;artificial intelligence;integrated circuit;interconnection	EDA	18.471053174064444	54.51612594138445	32678
d35eae0eca5da51727a387843e36abf6c60b659a	a reconfigurable clock polarity assignment flow for clock gated designs	rails;network synthesis;logic design;clocks;polarity assignment;inverters;physical design;clock gating;network synthesis clocks logic design logic gates;clock network synthesis reconfigurable clock polarity assignment flow clock gated design post silicon reconfigurability xor gate clock tree sleep mode busy mode mode specific reduction sink level worst case peak current clock gating information power consumption global skew nonsink node;total power;logic gates;clocks logic gates rails driver circuits inverters transistors switches;polarity assignment clock gating clock network synthesis physical design optimization;transistors;physical design optimization;driver circuits;clock network synthesis;switches;logic gate	This paper presents a clock polarity assignment flow which permits post-silicon reconfigurability. The proposed method inserts xor gates at one level of the clock tree to facilitate the polarity assignment. The polarity of the xor gates can be reconfigured for different modes of clock gating (sleep mode, busy mode, etc.) such that a mode-specific reduction of the peak current can be achieved. Experimental results show that the worst case peak current on a clock tree can be reduced by 33.3% by assigning polarity to xor gates at the sink level of the clock tree. An additional 12.8% reduction in the worst case peak current can be achieved by reconfiguring the polarity assignment based on the clock gating information. The proposed flow increases the area by 7.1% but reduces both the total power consumption by 23.8% and the global skew increase (due to polarity assignment) from 19.3 to 8.8 ps. The insertion of xor gates at the non-sink nodes is also studied to further reduce the global skew increase and the area overhead.	best, worst and average case;clock gating;clock signal;exclusive or;overhead (computing);pdf/a;reconfigurability;sleep mode	Jianchao Lu;Ying Teng;Baris Taskin	2012	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2011.2147339	embedded system;electronic engineering;parallel computing;real-time computing;logic gate;clock domain crossing;clock skew;computer science;engineering;timing failure;clock gating;digital clock manager;cpu multiplier	EDA	16.857237742711046	55.35981936427519	32719
77d4fb23ce0b5499016f2c162a5430d04f976542	using simpoint for accurate and efficient simulation	simpoint;simulation;sampling;level of detail;clustering;architecture evaluation;fast forwarding;algorithm design;simulation environment	Modern architecture research relies heavily on detailed pipeline simulation. Simulating the full execution of a single industry standard benchmark at this level of detail takes on the order of months to complete. This problem is exacerbated by the fact that to properly perform an architectural evaluation requires multiple benchmarks to be evaluated across many separate runs. To address this issue we recently created a tool called SimPoint that automatically finds a small set of Simulation Points to represent the complete execution of a program for efficient and accurate simulation. In this paper we describe how to use the SimPoint tool, and introduce an improved SimPoint algorithm designed to significantly reduce the simulation time required when the simulation environment relies upon fast-forwarding.	algorithm;benchmark (computing);fast forward;level of detail;pipeline (computing);simulation;technical standard	Erez Perelman;Greg Hamerly;Michael Van Biesbrouck;Timothy Sherwood;Brad Calder	2003		10.1145/781027.781076	algorithm design;sampling;real-time computing;simulation;computer science;theoretical computer science;operating system;level of detail;cluster analysis	Arch	-3.839364089754412	46.575779581554706	32726
7b556b349119c6d520b88151d6a700c604b79b0e	trends in processor architecture		Processors have undergone a tremendous evolution throughout their history. A key milestone in this evolution was the introduction of the microprocessor, term that refers to a processor that is implemented in a single chip. The first microprocessor was introduced by Intel under the name of Intel 4004 in 1971. It contained about 2,300 transistors, was clocked at 740 KHz and delivered 92,000 instructions per second while dissipating around 0.5 watts.	clock rate;microarchitecture;microprocessor;transistor;watts humphrey	Antonio Gonzalez	2018	CoRR		parallel computing;computer science;microarchitecture	Arch	6.898435697697526	50.62826371320829	32737
ac64131deaf3d54a48f4cce5e719c1defbfb33d9	approximate computing for energy-efficient error-resilient multimedia systems	energy conservation;probability;multimedia systems;power aware computing;statistical analysis;human perception approximate computing energy efficient error resilient multimedia systems consumer devices image processing video processing pattern recognition facial recognition data mining data synthesis user base digital signal data processing quality requirements noise tolerant algorithms statistical computations probabilistic computations;statistical analysis energy conservation multimedia systems power aware computing probability	The rapid advancement in scaled silicon technology has resulted in the influx of numerous consumer devices with a plethora of applications. Multimedia applications which use image and video processing, pattern or facial recognition, data mining and synthesis have seen a significant increase in user base. These applications not only demand complex signal processing of digital data to achieve quality requirements specified by the user, but also need to operate in an energy­ efficient manner, posing a significant design challenge. It should also be noted that majority of these applications have an inherent error-resiliency. This arises from the fact that:	approximate computing;data mining;digital data;facial recognition system;requirement;signal processing;video processing	Kaushik Roy	2013		10.1109/DDECS.2013.6549776	embedded system;electronic engineering;real-time computing;energy conservation;telecommunications;computer science;theoretical computer science;operating system;probability;data mining;algorithm;statistics	EDA	-0.539631274667635	59.35210367233673	32745
3cf464d61246103b12c64d4f790d8e40c639ffb8	optimizing sparse matrix-multiple vectors multiplication for nuclear configuration interaction calculations	extended roofline model sparse matrix multiplication block eigensolver nuclear configuration interaction;distributed memory parallel approaches sparse matrix multiple vector multiplication optimization nuclear configuration interaction calculation light atomic nuclei ci approach extremal eigen pairs many body nuclear hamiltonian matrix many body fermion dynamics for nuclei code block eigen solver eigen value computations spmm_t mfdn code compressed sparse block matrix format csb matrix format performance model spmm kernel compressed sparse row matrix format csr matrix format;nuclear configuration interaction;sparse matrix multiplication;vectors configuration interactions distributed memory systems eigenvalues and eigenfunctions many body problems matrix multiplication nuclear structure theory parallel processing physics computing sparse matrices;sparse matrices vectors instruction sets bandwidth eigenvalues and eigenfunctions arrays wave functions;extended roofline model;block eigensolver	Obtaining highly accurate predictions on the properties of light atomic nuclei using the configuration interaction (CI) approach requires computing a few extremal Eigen pairs of the many-body nuclear Hamiltonian matrix. In the Many-body Fermion Dynamics for nuclei (MFDn) code, a block Eigen solver is used for this purpose. Due to the large size of the sparse matrices involved, a significant fraction of the time spent on the Eigen value computations is associated with the multiplication of a sparse matrix (and the transpose of that matrix) with multiple vectors (SpMM and SpMM_T). Existing implementations of SpMM and SpMM_T significantly underperform expectations. Thus, in this paper, we present and analyze optimized implementations of SpMM and SpMM_T. We base our implementation on the compressed sparse blocks (CSB) matrix format and target systems with multi-core architectures. We develop a performance model that allows us to understand and estimate the performance characteristics of our SpMM kernel implementations, and demonstrate the efficiency of our implementation on a series of real-world matrices extracted from MFDn. In particular, we obtain 3-4 speedup on the requisite operations over good implementations based on the commonly used compressed sparse row (CSR) matrix format. The improvements in the SpMM kernel suggest we may attain roughly a 40% speed up in the overall execution time of the block Eigen solver used in MFDn.	block size (cryptography);blue gene;collection of computer science bibliographies;computation;compute kernel;computer architecture;configuration interaction;dynamic random-access memory;eigen (c++ library);eigenvalue algorithm;graph theory;image scaling;intel edison;kernel (operating system);lobpcg;lanczos resampling;locality of reference;many-body problem;mathematical optimization;matrix multiplication;multi-core processor;optimizing compiler;performance tuning;personal identification number;run time (program lifecycle phase);solver;sparse matrix;speedup	Hasan Metin Aktulga;Aydin Buluç;Samuel Williams;Chao Yang	2014	2014 IEEE 28th International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2014.125	parallel computing;single-entry matrix;theoretical computer science	HPC	-2.5760786930332134	39.42837994733433	32774
e4ef33aa73ce22b3d7a43a62ee905fa4acb6b0c1	high performance pattern matching and data remanence on graphics processing units	digital forensic;tesla k20c;paper;aho corasick;bmh;forensic;gpu;suffix tree;cuda;remanence;pattern matching;algorithms;tesla k20;sequence matching;string matching;amazon;pfac		computer graphics;data remanence;graphics processing unit;pattern matching	Xavier J. A. Bellekens	2016			parallel computing;computer hardware;computer science;theoretical computer science	ML	-0.040500708583953236	42.48027229597227	32786
d3353ba17991f24915729d39cfe9246c821dfe11	zippy - a coarse-grained reconfigurable array with support for hardware virtualization	hardware virtualization;coarse grained reconfigurable array;platform virtualization;decoding;reconfigurable architectures;application virtualization;virtual machining;computer networks;computer architecture;reconfigurable architecture;virtual machines;multicontext hybrid cpu;platform virtualization hardware virtual machining reconfigurable architectures computer architecture application virtualization concrete computer networks computer science decoding;adpcm decoder zippy coarse grained reconfigurable array hardware virtualization multicontext hybrid cpu;zippy;computer science;coarse grained;adpcm decoder;virtual machines reconfigurable architectures;concrete;hardware	This paper motivates the use of hardware visualization on coarse-grained reconfigurable architectures. We introduce Zippy, a coarse-grained multi-context hybrid CPU with architectural support for efficient hardware virtualization. The architectural details and the corresponding tool flow are outlined. As a case study, we compare the non-virtualized and the virtualized execution of an ADPCM decoder.	adaptive differential pulse-code modulation;algorithm;central processing unit;end-to-end principle;hardware virtualization;netlist;run time (program lifecycle phase);simulation	Christian Plessl;Marco Platzner	2005	2005 IEEE International Conference on Application-Specific Systems, Architecture Processors (ASAP'05)	10.1109/ASAP.2005.69	embedded system;computer architecture;parallel computing;computer science;operating system;hardware virtualization	EDA	3.8036301902396503	49.356617865203546	32841
046036872e277b74b107943a3c059a499dce3eee	powerantz: distributed power sharing strategy for network on chip	network on a chip energy management power system management energy consumption financial management large scale systems system on a chip permission adaptive systems system performance;network on chip;routing;power management schemes powerantz distributed power sharing strategy network on chip;network on chip ant system distributed power management;power distribution;system on a chip;distributed power management;chip;power management schemes;power integrated circuits network on chip;complex system;energy consumption;lead;power management;powerantz;distributed power sharing strategy;power demand;power integrated circuits;throughput;ant system	Advent of Network-on-Chip (NoC) based complex system designs made on-chip power management a challenging issue. Power management schemes have been proposed to tackle the problem. But they fail to provide optimal sharing when the power budget distribution varies significantly among on-chip components that are placed further apart on a chip. This paper presents PowerAntz, a distributed power management strategy for NoC based systems. This adaptive and distributed approach to power sharing across various components of a large chip is shown to be a scalable solution. Our experiments have demonstrated PowerAntz to be up to 30% more effective in distributing power budget compared to existing strategies. Further, it also achieves up to 21.25% improvement in power utilization while keeping overhead as low as zero in best case.	best, worst and average case;complex system;experiment;network on a chip;overhead (computing);power management;scalability;system on a chip	Suman Kalyan Mandal;Rabi N. Mahapatra	2008	Proceeding of the 13th international symposium on Low power electronics and design (ISLPED '08)	10.1145/1393921.1393968	chip;system on a chip;embedded system;routing;complex systems;throughput;lead;electronic engineering;real-time computing;computer science;engineering;network on a chip	EDA	15.81621951248082	55.19778510232972	32864
530d142df22adc5ab740cf839c86d705182c43b9	jack-on: a low-cost wireless sensor interface		JACK-ON is a wireless sensor interface using sine tone oscillators. Sensor resistances alter oscillator frequencies or amplitudes, and the audio output of oscillators is sent through a generic wireless audio transceiver. The wireless audio receiver is plugged into the computer’s sound card, where oscillator frequencies and amplitudes are analyzed in software. Thus, analog to digital conversion is achieved through the sound card in the computer receiving the audio signal. It is hoped that this method provides a cheap and robust wireless sensor transmission solution. In this paper, we look at some design issues and early results gathered from the JACK-ON system. 1. CURRENT OPTIONS FOR WIRELESS SENSOR INTERFACING Capturing large-scale gestures can be inexpensively achieved through camera based motion-tracking systems. Small-scale gestures can also be captured using these systems if the camera is focused on a small area, as in Barry Moon’s ‘Postcard’ (2003) for chair with live audio/video processing [1]. However, this approach implies that the performance space is small and focused. The most popular method for capturing small-scale gestural information involves using physical sensors manipulated by the performer. Capturing gestures in a performance such as dance, where the overriding concern is freedom of movement, a wireless sensor interface is needed. This generally involves digitizing voltages (sensor resistances) into MIDI, or some other proprietary format, before transmitting them wirelessly. There are currently very few commercially available sensor interfaces dedicated to wireless communication. The Kroonde Gamma [2] is one very expensive example, costing $1495. There is also the option of using one of several interfaces that convert sensor voltages to MIDI, such as the I-Cube [3], and sending MIDI data wirelessly using something like the Midistream. Other, noncommercial, interfaces have been developed by small research labs, or individuals, for use in their own creative work, such as the Eos Pods [4] developed at NYU. Sacha Atkinson developed his SM2 by hacking a wireless games controller. This is a very cheap solution, but requires careful conditioning of sensor voltages, and is susceptible to changes in response due to battery charge. Another downside to hacking games controllers is that they are in a constant cycle of obsolescence, meaning the one you hack today may not be available to hack tomorrow. The SM2 circuit is shown in Figure 1. Figure 1. SM2 Circuit Hacking games controllers has interested many other researchers in the field, evidenced by the Oik project at Steim [5].	analog-to-digital converter;cube;eos;midi;sensor;sound card;symposium on principles of database systems;tracking system;transceiver;transmitter;video processing	Barry Moon;Sacha Atkinson	2005			wireless sensor network;sensor node;key distribution in wireless sensor networks;wi-fi array;wireless network interface controller;mobile wireless sensor network;ant;electro-optical sensor	Mobile	4.044787654486043	34.34075673512124	32891
c90e6145f42a34e2ae2f9212948b8fa07a625ae5	decoding of stochastically assembled nanoarrays	self assembly;decoding assembly nanowires wires semiconductor materials semiconductivity switches epitaxial layers uncertainty stochastic processes;decoding;probabilistic method;uncertainty handling;stochastic processes nanowires nanoelectronics assembling uncertainty handling decoding;design strategies stochastically assembled nanoarrays nanotechnologies uncertainty controlling stochastic self assembly architectural strategy manufacturing strategies nanoarrays assembling crossbars structure parallel nanowires discovery process decoding circuitry probabilistic method;stochastic processes;assembling;nanoelectronics;nanowires	A key challenge that face nanotechnologies is controlling the uncertainty introduced by stochastic self-assembly. In this paper we explore architectural and manufacturing strategies to cope with this uncertainty when assembling nanoarrays, crossbars composed of two orthogonal sets of coded parallel nanowires. Because the encodings of nanowires that are assembled into a nanoarray cannot be predicted in advance, a discovery process is needed and specialized decoding circuitry must be employed. We have developed a probabilistic method of analysis so that various design strategies can be evaluated.	binary logarithm;electronic circuit;netware;self-assembly	Benjamin Gojman;Eric Rachlin;John E. Savage	2004	IEEE Computer Society Annual Symposium on VLSI	10.1109/ISVLSI.2004.1339502	structural engineering;electronic engineering;engineering;nanotechnology	Embedded	10.68784410318958	58.72312670030143	32955
85eb5c0eeab570fe3ca6bd49e377f2282f20e3c8	low overhead delay testing of asics	automatic test pattern generation;application specific integrated circuits;design automation;cost effectiveness;fault coverage;chip;integrated circuit design	Delay testing has become increasingly essential as chip geometries shrink. Low overhead or cost effective delay test methodology is successful when it results in a minimal number of effective tests and eases the demands on an already burdened IC design and test staff. This work describes one successful method in use by IBM ASICs that resulted in a slight total test pattern increase, generally ranging between 10 and 90%. Example ICs showed a pattern increase of as little as 14% from the stuck-at fault baseline with a transition fault coverage of 89%. In an ASIC business, a large number of ICs are processed, which does not allow for the personnel to understand how to test each individual IC design in detail. Instead, design automation software that is timing and testability aware ensures effective and efficient tests. The resultant tests detect random spot timing delay defects. These types of defects are time zero related failures and not reliability wearout mechanisms.	application-specific integrated circuit;baseline (configuration management);fault coverage;integrated circuit design;overhead (computing);resultant;stuck-at fault;test card	Pamela S. Gillis;Francis Woytowich;Andrew Ferko;Kevin McCauley	2004	2004 International Conferce on Test	10.1109/ITC.2004.118	chip;reliability engineering;embedded system;electronic engineering;real-time computing;cost-effectiveness analysis;fault coverage;electronic design automation;telecommunications;computer science;engineering;automatic test pattern generation;application-specific integrated circuit;integrated circuit design	EDA	21.256149795236354	53.53119170923746	32969
3d8512a45f5fed9ec5c1cdbd59c4f9bbad177978	an integrated algorithm for memory allocation and assignment in high-level synthesis	storage allocation;nonuniform access speeds;integrated algorithm;complexite;random access memory;design automation;behavioral synthesis;scheduling effect;processor scheduling;data flow graphs;data arrays;kalman filters;filters;unscheduled data flow graph;critere performance;synthese haut niveau;memory assignment;kalman filters high level synthesis processor scheduling sram chips dram chips storage allocation data flow graphs;chip;filter design;benchmark filter designs;high level synthesis;memory assignemt;scheduling algorithm;specification donnee;permission;heuristic algorithms;analyse comportementale;memory exploration strategy high level synthesis integrated algorithm memory allocation memory assignment data arrays behavioral specification fast on chip memories array mapping scheduling effect nonuniform access speeds benchmark filter designs globally best memory configurations unscheduled data flow graph dram sram;memory design;conception assistee;filtre;globally best memory configurations;high level synthesis random access memory algorithm design and analysis permission filters scheduling algorithm processor scheduling design automation costs heuristic algorithms;sram;behavioral specification;memory allocation;dram;array mapping;memory exploration strategy;algorithm design and analysis;dram chips;ordonnancement;fast on chip memories;sram chips	With the increasing design complexity and performance requirement, data arrays in behavioral specification are usually mapped to fast on-chip memories in behavioral synthesis. This paper describes a new algorithm that overcomes two limitations of the previous works on the problem of memory-allocation and array-mapping to memories. Specifically, its key features are (1) a tight link to the scheduling effect, which was totally or partially ignored by the existing memory synthesis systems, and supporting (2) non-uniform access speeds among the ports of memories, which greatly diversify the possible (practical) memory configurations. Experimental data on a set of benchmark filter designs are provided to show the effectiveness of the proposed exploration strategy in finding globally best memory configurations.	algorithm;benchmark (computing);high- and low-level;high-level synthesis;memory management;scheduling (computing)	Jaewon Seo;Taewhan Kim;Preeti Ranjan Panda	2002		10.1145/513918.514072	chip;kalman filter;embedded system;algorithm design;interleaved memory;electronic engineering;parallel computing;real-time computing;static random-access memory;electronic design automation;computer science;theoretical computer science;operating system;filter design;high-level synthesis;scheduling;dram;memory management	EDA	-1.7463034367984889	53.357072711885415	33054
85ab9414722c314fa2cc9f3bb0cbd606cbe7119a	a low-cost vlsi architecture of multiple-size idct for h.265/hevc		In this paper, we present an area-efficient 4/8/16/32-point inverse discrete cosine transform (IDCT) architecture for a HEVC decoder. Compared with previous work, this work reduces the hardware cost from two aspects. First, we reduce the logical costs of 1D IDCT by proposing a reordered parallel-in serial-out (RPISO) scheme. By using the RPISO scheme, we can reduce the required calculations for butterfly inputs in each cycle. Secondly, we reduce the area of transpose architecture by proposing a cyclic data mapping scheme that can achieve 100% I/O utilization of each SRAM. To design a fully pipelined 2D IDCT architecture, we propose a pipelining schedule for row and column transform. The results show that the normalized area by maximum throughput for the logical IDCT part can be reduced by 25%, and the memory area can be reduced by 62%. The maximum throughput reaches 1248 Mpixels/s, which can support real-time decoding of a 4 K × 2 K 60 fps video sequence. key words: HEVC, IDCT, SRAM, area-efficient, video coding	data compression;discrete cosine transform;high efficiency video coding;input/output;maximum throughput scheduling;pipeline (computing);pixel;real-time clock;static random-access memory;very-large-scale integration	Heming Sun;Dajiang Zhou;Peilin Liu;Satoshi Goto	2014	IEICE Transactions		static random-access memory;computer science	EDA	12.494712038325712	40.222973219975486	33078
beb5af45b60668aa0b53e7f8512ec4a7204a8d50	an efficient streaming and decoding architecture for stored fgs video	estensibilidad;desciframiento;streaming media decoding scalability bit rate bandwidth mpeg 4 standard discrete cosine transforms pipeline processing ip networks quality of service;arquitectura circuito;data compression;decodage;decoding;video signal processing;buffer storage;circuit architecture;code standards;transform coding;indexing terms;pipeline decoding architecture efficient streaming architecture efficient decoding architecture stored fgs video fine granularity scalability video coding mpeg 4 standard bitplane coding dct residues compressed bitstream rate scalability frame buffer scanning bitplane decoding frame duplication base layer decoding enhancement layer decoding;video coding;enhancement layer;fine granular scalable;codage video;discrete cosine transforms;scheduling;telecommunication standards;architecture circuit;traitement signal video;digital signal processor;procesador oleoducto;ordonamiento;processeur signal numerique;extensibilite;scalability;pipeline processing video coding data compression decoding discrete cosine transforms transform coding buffer storage code standards telecommunication standards;procesador senal numerica;processeur pipeline;ordonnancement;pipeline processor;pipeline processing;timing	Fine granularity scalability (FGS) is the latest video-coding tool provided in Amendment 2 of the MPEG-4 standard. By taking advantage of bitplane coding of DCT residues, the compressed bitstream can be truncated at any location to support the finest rate scalability in the enhancement layer. However, both frame buffer scanning several times in bitplane decoding and frame duplication simultaneously for base and enhancement layer decoding make FGS difficult in its implementation. In this paper, we propose a corresponding pair of efficient streaming schedule and pipeline decoding architecture to deal with the prescribed problems. The design may be applied to the case of streaming stored FGS videos and benefit FGS-related applications.	arithmetic coding;bitstream;discrete cosine transform;framebuffer;scalability;streaming media;whole genome sequencing	Yi-Shin Tung;Ja-Ling Wu;Po-Kang Hsiao;Kan-Li Huang	2002	IEEE Trans. Circuits Syst. Video Techn.	10.1109/TCSVT.2002.800855	data compression;digital signal processor;real-time computing;scalability;transform coding;index term;telecommunications;computer science;theoretical computer science;scheduling;statistics	Arch	12.747025740147325	39.29635984998772	33132
aa01b45864ccb0cafb00096884e6d12870a216f0	contract-based system-level composition of analog circuits	libraries;front end;platform based design;composition;correct by construction system level composition contract based system level composition analog circuit;analog;integration analog platform composition contract assume guarantee platform based design uwb radio frequency system;correct by construction system level composition;contracts;platform;integration;system performance;analog circuits contracts system level design time to market system performance design methodology process design circuit optimization dynamic range libraries;law;process design;analog circuits;artificial neural networks;analog circuit;general solution;radio frequency;contract;automatic detection;assume guarantee;contract based system level composition;system integration;system;system level design;dynamic range;mathematical model;analogue circuits;time to market;mixers;hierarchical design;uwb;circuit optimization;design methodology;ultra wide band	Efficient system-level design is increasingly relying on hierarchical design-space exploration, as well as compositional methods, to shorten time-to-market, leverage design re-use, and achieve optimal performances. However, in analog electronic systems, circuit behaviors are so tightly dependent on their interface conditions that accurate system performance estimations based on characterizations of individual stand-alone circuits is a hard task. Since there is no general solution to this problem, analog system integration has traditionally used ad-hoc solutions heavily dependent on designers' experience. In this paper, we build upon the analog platform-based design methodology by exploiting contracts to enforce correct-by-construction system-level composition. Contracts intuitively capture the thought process of a designer, who aims at guaranteeing circuit performance only under specific assumptions (e.g. loading and dynamic range) on the interface properties. Our approach allows automatic detection and composition of compatible components in a given library. We apply our methodology to an ultra-wide band receiver front-end to show that contracts allow pre-designed IP components to be smoothly integrated and design decisions to be reliably made at a higher abstraction level, both key factors to improve designer productivity.	abstraction layer;analogue electronics;dynamic range;electronic system-level design and verification;hoc (programming language);level design;performance;platform-based design;smoothing;system integration;ultra-wideband	Xuening Sun;Pierluigi Nuzzo;Chang-Ching Wu;Alberto L. Sangiovanni-Vincentelli	2009	2009 46th ACM/IEEE Design Automation Conference	10.1145/1629911.1630066	embedded system;electronic engineering;real-time computing;simulation;telecommunications;analogue electronics;computer science;engineering;electrical engineering;ultra-wideband;computer performance;artificial neural network	EDA	9.293585863462775	55.585752803054994	33153
5df2f839026adaa8a94b1e5d8a4ea6924002e597	how to evolve complex combinational circuits from scratch?	reliability;optimisation binary decision diagrams combinational circuits evolutionary computation;boolean functions;data structures boolean functions reliability logic gates;logic gates;data structures;conventional optimization complex combinational circuits evolutionary circuit design complex large circuits arithmetic circuits randomly seeded initial population circuit structure evolutionary algorithm candidate circuit binary decision diagram bdd functional similarity hamming distance evolutionary design process benchmark circuits lgsynth91 set 28 input frg1 circuit	One of the serious criticisms of the evolutionary circuit design method is that it is not suitable for the design of complex large circuits. This problem is especially visible in the evolutionary design of combinational circuits, such as arithmetic circuits, in which a perfect response is requested for every possible combination of inputs. This paper deals with a new method which enables us to evolve complex circuits from a randomly seeded initial population and without providing any information about the circuit structure to the evolutionary algorithm. The proposed solution is based on an advanced approach to the evaluation of candidate circuits. Every candidate circuit is transformed to a corresponding binary decision diagram (BDD) and its functional similarity is determined against the specification given as another BDD. The fitness value is the Hamming distance between the output vectors of functions represented by the two BDDs. It is shown in the paper that the BDD-based evaluation procedure can be performed much faster than evaluating all possible assignments to the inputs. It also significantly increases the success rate of the evolutionary design process. The method is evaluated using selected benchmark circuits from the LGSynth91 set. For example, a correct implementation was evolved for a 28-input frg1 circuit. The evolved circuit contains less gates (a 57% reduction was obtained) than the result of a conventional optimization conducted by ABC.	benchmark (computing);binary decision diagram;circuit design;combinational logic;continuous design;evolutionary algorithm;hamming distance;influence diagram;integrated circuit;mathematical optimization;randomness	Zdenek Vasícek;Lukás Sekanina	2014	2014 IEEE International Conference on Evolvable Systems	10.1109/ICES.2014.7008732	equivalent circuit;boolean circuit;data structure;logic gate;computer science;engineering;theoretical computer science;machine learning;reliability;boolean function;algorithm	EDA	21.036344591284976	50.119525095109104	33157
4629ff8fcdb1fada82c1f41a1897a6f98989e5fa	a test design method for floating gate defects (fgd) in analog integrated circuits	cmos analogue integrated circuits;analogue integrated circuits;circuit simulation;fault simulation;integrated circuit testing;transient analysis;cmos;fgds;analog integrated circuits;circuit simulators;fault simulation;floating gate defects;floating gate transistor;simulator output;test design method;test design methodology;transient test;trapped charge;undetectable values	A unified approach to fault simulation for FGDs is in-troduced. Instead of a direct fault simulation, the proposedapproach calculates indirectly from the simulator output thesets of undetectable values of the trapped charge on thefloating gate transistor. It covers all potential gate chargesof an FGD at one or more transistors and allows the appli-cation of conventional circuit simulators for simulating DC,AC and transient test.Based on this fault simulation, a test design methodologyis presented that can determine all test sets that detect allFGDs for all possible values of gate charge.	simulation;test design;transistor	Michael Pronath;Helmut E. Graeb;Kurt Antreich	2002			mixed-signal integrated circuit;physical design;embedded system;electronic engineering;design methods;engineering;automatic test pattern generation;integrated circuit;circuit design;circuit extraction;computer engineering	EDA	23.48276626818781	52.6918606095901	33201
0b995806c1b5fa047e7dd5a657a9c887feee4a5b	matrix: a reconfigurable computing architecture with configurable instruction distribution and deployable resources	cmos integrated circuits;reconfigurable computing;building block;resource allocation;reconfigurable architectures;parallel architectures;functional unit;field programmable gate arrays;coarse grained;0 5 micron matrix reconfigurable computing architecture configurable instruction distribution deployable resources application specific regularity datapaths computations unified configurable network instruction store memory element computational element cmos process;parallel architectures reconfigurable architectures cmos integrated circuits field programmable gate arrays	MATRIX is a novel, coarse-grain, reconfigurable computing architecture which supports confgurable instruction distribution. Device resources are allocated to controlling and describing the computation on a per task basis. Application-specific regularity allows us to compress the resources allocated to instruction control and distribution, in many situations yielding more resources for datapaths and computations. The adaptability is made possible by a multi-level configuration scheme, a unified configurable network supporting both datapaths and instruction distribution, and a coarse-grained building block which can serve as an instruction store, a memory element, or a computational element. In a 0.5,~ CMOS process, the 8-bit functional unit at the heart of the MATRIX architecture has a footprint of roughly 1.5mmx 1.2mm, making single dies with over a hundred function units practical today. At this process point, IOOMHz operation is easily achievable, allowing MATRIX components to deliver on the order of 10 Goph (8-bit ops).	8-bit;arithmetic logic unit;cmos;computation;computer architecture;cycle basis;dataflow;datapath;entity;execution unit;general-purpose markup language;multi-level governance;network switch;reconfigurable computing;serialization;software deployment;the matrix;value (computer science)	Ethan Mirsky;André DeHon	1996		10.1109/FPGA.1996.564808	embedded system;parallel computing;real-time computing;reconfigurable computing;resource allocation;computer science;operating system;cmos;field-programmable gate array	Arch	1.276787348331931	49.135946672649524	33236
d6d8558fdc5a335de13d77cd860cdb5b6ddee67d	algorithmic complexity analysis on data transfer rate and data storage for multidimensional signal processing	dataflow model algorithm architecture co exploration data transfer rate data storage;data transfer signal processing algorithms algorithm design and analysis complexity theory memory measurement space exploration;video coding;smoothing methods;video coding computational complexity multidimensional signal processing smoothing methods;computational complexity;multidimensional signal processing;h 264 avc algorithmic complexity analysis data transfer rate data storage multidimensional signal processing visual computing visual quality algorithm architecture co exploration design space exploration data granularities	Algorithmic complexity, such as data storage size and data transfer rate, is dramatically increased in multidimensional signal processing, including visual computing exploiting temporal and spatial information to achieve better visual quality. This paper present a systematic method, which is a new paradigm of designing on the complex multidimensional signal and is called as algorithm/architecture co-exploration, to efficiently quantify the algorithmic complexity, including data storage and data transfer rate, whose characteristics are independent from platforms. By exploring design space based on the dataflow with different executing orders and various data granularities, the trade-off between data storage size and data transfer rate is made by a systematic manner and hence the algorithm could be smoothly mapped onto architecture. Case studies reveal that our framework can effectively characterize the complexity of algorithms, and that the extracted complexity can facilitate design space exploration at various data granularities.	algorithm;analysis of algorithms;computational complexity theory;computer data storage;dataflow;design space exploration;multidimensional signal processing;programming paradigm;smoothing;visual computing	Gwo Giun Lee;Chun-Fu Chen;He-Yuan Lin	2013	SiPS 2013 Proceedings	10.1109/SiPS.2013.6674500	multidimensional signal processing;parallel computing;real-time computing;computer science;theoretical computer science;machine learning;computational complexity theory;algorithm	HPC	10.91262511598236	39.02416492072793	33259
8f7cdb42cfda7b01cc1cd5bbd0d6a6771a45a57d	a linear-system operator based scheme for evaluation of multinomials	graph theory;radix 2 online computational scheme;serial interconnection;performance evaluation;serial interconnection linear system operator multinomial evaluation radix 2 online computational scheme fixed point number representation system graph representation linear equation redundant adder multiplexer;redundant adder;input variables;bayesian methods;delay effects;mathematical operators fixed point arithmetic graph theory;linear system;multiplexing;mathematical operators;fixed point;input output;bayesian methods equations hardware arithmetic computer science transforms delay effects multiplexing input variables performance evaluation;graph representation;transforms;fixed point arithmetic;arithmetic;multiplexer;multinomial evaluation;fixed point number representation system;operational transformation;computer science;linear system operator;linear equations;linear equation;hardware	We present a radix-2 online computational scheme for evaluating multinomials in a fixed-point number representation system. Its main advantage is that it can adapt to any evaluation graph representing the multinomial. Evaluation graphs are efficient representations of multinomials in a factored form. The proposed scheme maps subgraphs of the evaluation graph using linear-system operators. These operators transform the expressions represented by the subgraphs into systems of linear equations. The linear equations are then solved in an online, most-significant-digit-first fashion. The scheme produces, after an initial delay, one output digit per iteration for inputs within range. The iteration time is equal to the sum of the delays of a redundant adder, multiplexer, register and a selection unit and is independent of the size of the multinomial and the precision of the inputs/outputs. The initial delay is proportional to the diameter of the evaluation graph and the maximum number of children of any addition node in the graph. The proposed method lends itself to implementation using simple, highly regular hardware with serial interconnections between modules.	adder (electronics);algorithm;bayesian network;conditional (computer programming);emoticon;fixed-point arithmetic;graph (abstract data type);iteration;linear equation;linear system;multinomial logistic regression;multiplexer;scheduling (computing);significant figures;sysop;system of linear equations	Pavan Adharapurapu;Milos D. Ercegovac	2005	17th IEEE Symposium on Computer Arithmetic (ARITH'05)	10.1109/ARITH.2005.8	combinatorics;discrete mathematics;computer science;graph theory;theoretical computer science;operating system;mathematics;linear equation;algorithm;algebra	Embedded	19.300990789044327	43.60584460483097	33267
12648a1fc309e29a61621ce03998ffc3ca53c504	design of testable vlsi circuits with minumum area overhead	testable vlsi;prueba;optimisation;concepcion circuito;optimizacion;integrated circuit;autotest integre;automatic testing;interconnections;circuit design;circuit vlsi;vlsi automatic testing integrated logic circuits logic testing;circuito integrado;test;grafico;vlsi circuit;built in self test;logic gates;graph;graphe;logic testing;vlsi;graph model testable vlsi built in self test logic gates interconnections;optimization;conception circuit;integrated logic circuits;graph model;circuito vlsi;circuit testing very large scale integration built in self test registers automatic testing integrated circuit interconnections design for testability galois fields algorithm design and analysis propulsion;circuit integre	One of the techniques to tackle the increasing complexity of testing VLSI circuits is to incorporate built-in self test (BIST) structures. However, incorporation of such BIST structures calls for increased area overhead due to additional logic gates and interconnections. It is very important to keep this area overhead to a minimum. This correspondence presents a simple graph model of the area overhead minimization problem, for circuits into which BIST modifications are to be incorpo-	built-in self-test;graph (discrete mathematics);logic gate;overhead (computing);very-large-scale integration	Prasad R. Chalasani;Sudipta Bhawmik;Anurag Acharya;Parimal Pal Chaudhuri	1989	IEEE Trans. Computers	10.1109/12.35841	logic gate;computer science;electrical engineering;theoretical computer science;integrated circuit;circuit design;software testing;very-large-scale integration;graph;algorithm	EDA	16.39454663046981	49.99409995380288	33281
1ae772f1150a2e2dcf1dd83c5d1fa36abc7c66bb	approaches to the soc ip-blocks' design with errors' mitigation	protocols;single event upsets;manufacturing;markov processes;field programmable gate arrays;hardware;fault trees	Developing of failsafe systems is the problem solving in different sectors. Hardware design is not an exception. This sector imposes additional requirements constructing failover controllers, such as chip area and energy consumption. This article provides types and causes of errors to define their effect on SoC and offers the general idea of errors resilient SoC that consists of error detecting, error fixing and error mitigating mechanism. Also in the paper reconfiguration is considered as the error mitigation mechanism and methods used in practice to implement reconfiguration are presented. Before producing controller or other IP-block with error mitigation, first of all it is necessary to assess possible variants of failure and ways to improve its failsafe. Therefore methods of failure assessments are presented. As the result — Markov chain is chosen as the assessment tool and example of constructing of Markov chain for not reconfigurable and reconfigurable controller of transport layer protocol is presented. Comparative analyses of the results are carried out.	exception handling;fail-safe;failover;markov chain;problem solving;requirement;sensor;system on a chip	Valentin Rozanov;Elena Suvorova	2016	2016 19th Conference of Open Innovations Association (FRUCT)	10.23919/FRUCT.2016.7892201	embedded system;real-time computing;engineering;distributed computing	EDA	7.093036672153697	57.86199651108493	33335
6dc224a4dc3f75554ad68fc7b30e7c342c6f1818	hierarchical buffered routing tree generation	bottom up;design flow;routing design optimization integrated circuit interconnections polynomials driver circuits signal generators design automation very large scale integration steiner trees dynamic programming;trees mathematics;three dimensional;network routing;chip;optimization problem;polynomial time algorithm;network topology;integrated circuit design;integrated circuit interconnections;interconnect delay hierarchical buffered routing tree generation performance driven buffered routing tree generation very large scale integrated circuits vlsi circuits bottom up construction algorithm local neighborhood search strategy polynomial time algorithm exponential size solution subspace buffered rectilinear steiner routing tree net driver sink nodes problem variants maximum total area constraint minimum required time at driver constraint 3d solution curve propagation construction phase dynamic programming fanout optimization integrated circuits local order perturbation;vlsi;circuit layout cad;circuit optimisation;neighborhood search;integrated circuit interconnections vlsi integrated circuit design network routing trees mathematics circuit layout cad network topology circuit optimisation;high speed integrated circuits	neighborhood search strategy, our polynomial time algorithm finds the optimum solution in an exponential-size solution sub-space. The final output is a buffered rectilinear Steiner routing tree that connects the driver of a net to its sink nodes. The two variants of the problem, i.e., maximizing the required time at the driver subject to a maximum total area constraint and minimizing the total area subject to a minimum required time at the driver constraint, are handled by propagating three-dimensional solution curves during the construction phase. Experimental results demonstrate the effectiveness of our algorithm compared to other techniques .	algorithm;local search (optimization);p (complexity);polynomial;regular grid;routing;steiner tree problem;time complexity	Amir H. Salek;Jinan Lou;Massoud Pedram	2002	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.998627	chip;optimization problem;three-dimensional space;mathematical optimization;routing;electronic engineering;computer science;design flow;theoretical computer science;top-down and bottom-up design;very-large-scale integration;network topology;computer network;integrated circuit design	EDA	15.574003795901055	51.243989798184494	33344
9fd240b0eb5a011afc2b12fa5e171742cbd7245e	evaluation and tradeoffs for out-of-order execution on reconfigurable heterogeneous mpsoc	processor scheduling;program processors hardware hazards ip networks processor scheduling programming;hazards;system on chip field programmable gate arrays logic design microprocessor chips;field programmable gate array out of order execution schemes multiprocessor system on chip mpsoc instruction level scoreboarding algorithm;期刊论文;ip networks;programming;program processors;task level scheduling multiprocessor system on chip mpsoc out of order ooo execution scoreboarding;hardware	Out-of-order (OoO) execution schemes show incredible promise for task-level parallelism in multiprocessor system-on-chip (MPSoC) designs. However, the main challenge of the OoO execution lies in the analysis of the intertask dependences. In this paper, we address this challenge by applying the instruction-level scoreboarding algorithm at the task level. Furthermore, we introduce both software-based static and dynamic implementations on top of a heterogeneous MPSoC prototyped on a field-programmable gate array fabric. Our experimental results show that our approach can achieve up to 94.75% and 97.68% of the theoretical speedup. Finally, we present an in-depth analysis of the tradeoff between our dynamic and static approaches.	algorithm;data-flow analysis;dataflow;digital signal processor;field-programmability;field-programmable gate array;graphics processing unit;jpeg;mpsoc;multiprocessing;out-of-order execution;parallel computing;real life;run time (program lifecycle phase);scoreboarding;speedup;task parallelism;test case	Qi Guo;Xi Li;Chao Wang;Xuehai Zhou	2016	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2015.2389893	embedded system;programming;computer architecture;parallel computing;real-time computing;hazard;computer science;operating system	EDA	-2.105110451199869	50.10560210347199	33365
c12bdd3b37381a580a5760dc99385462ea890aef	applications of computational geometry to vlsi layout pattern design	verification;mascara;concepcion asistida;adaptacion;topology;computer aided design;base donnee;interconnection;configuration rectangulaire;geometric transformation;image processing;connexion electrique;integrated circuit;calculating time;routing;regle production;topologie;recubrimiento;computational geometry;circuit vlsi;database;procesamiento imagen;base dato;compactacion;graphic plotting;overlay;circuito integrado;systeme mecanique;interseccion;modo exploracion;compactage;recouvrement;traitement image;rectangular configuration;methodologie;acceleration;topologia;boolean algebra;systeme conversationnel;configuracion geometrica;algorithme;algorithm;temps calcul;algorritmo;vlsi circuit;compaction;ligne geometrique;trace graphique;adaptation;transformacion geometrica;interconnexion;trazado grafico;transformation geometrique;algebre boole;conception assistee;encaminamiento;masque;vlsi layout;circuito vlsi;verificacion;intersection;tiempo calculo;mask;scanning mode;mode balayage;configuracion rectangular;aceleracion;circuit integre;configuration geometrique;interconeccion;algebra boole;production rule;line geometry;geometrical configuration;linea geometrica;acheminement;regla produccion	Abstract   In the last couple of decades, many algorithms for solving geometric problems have been proposed in the field of computational geometry. This field has been making remarkable advances since there are wide-spread applications such as pattern recognition, computer graphics, geographic information processing, etc., and a large number of computer scientists have investigated various geometric problems.  Recently, it turned out that the basic algorithms and data structures proposed by the computer scientists can be also applied to LSI layout pattern design. It is the objective of this paper to review how computational geometry is applied to the LSI layout problems such as mask pattern design verification, manipulation of mask data and image processing, wire routing, etc. Among some algorithms to be described, the plane-sweep method may be most remarkable because it can be commonly used to various problems.	computational geometry	Masao Sato;Tatsuo Ohtsuki	1987	Integration	10.1016/0167-9260(87)90021-6	acceleration;compaction;boolean algebra;routing;electronic engineering;verification;geometric transformation;image processing;computational geometry;computer science;electrical engineering;integrated circuit;interconnection;computer aided design;line;intersection;overlay;mask;engineering drawing;algorithm;adaptation	Theory	17.868611049844347	38.73524843369041	33496
1d75e9d1bf7086916afacdbc9ea45ebf11fd63b6	an automated fine-grain pipelining using domino style asynchronous library	hdl asynchronous eda synthesis qdi asic;hdl;design automation;qdi;clocks;vlsi electronic design automation asynchronous circuits pipeline processing clocks;quasi delay insensitive circuits register transfer level synthesis asynchronous eda asiq hdl fine grain pipelining domino style asynchronous library clocked circuits design automation vlsi clock frequency asynchronous circuits clocked designs;asynchronous circuit;synthesis;pipeline processing libraries clocks circuit synthesis delay asynchronous circuits electronic design automation and methodology signal design registers signal synthesis;quasi delay insensitive;vlsi;asynchronous circuits;asic;asynchronous eda;register transfer level;pipeline processing;electronic design automation	Register transfer level (RTL) synthesis model which simplified the design of clocked circuits allowed design automation boost and VLSI progress for more than a decade. Shrinking technology and progressive increase in clock frequency are bringing clock to its physical limits. Asynchronous circuits, which are believed to replace globally clocked designs in the future, remain out of the competition due to the design complexity of some automated approaches and poor results of other techniques. Successful asynchronous designs are known but they are primarily custom. This work sketches an automated approach for automatically re-implementing conventional RTL designs as fine-grain pipelined asynchronous quasi-delay-insensitive (QDI) circuits and presents a framework for automated synthesis of such implementations from high-level behavior specifications. Experimental results are presented using our new dynamic asynchronous library.	asynchronous circuit;clock rate;delay insensitive circuit;high- and low-level;pipeline (computing);register-transfer level;very-large-scale integration	Alexander B. Smirnov;Alexander Taubin;Ming Su;Mark G. Karpovsky	2005	Fifth International Conference on Application of Concurrency to System Design (ACSD'05)	10.1109/ACSD.2005.3	asynchronous system;embedded system;computer architecture;real-time computing;asynchronous circuit;electronic design automation;computer science;application-specific integrated circuit;very-large-scale integration;register-transfer level	EDA	14.363965752149404	55.099181655532725	33584
66397b62848245ed13ef6734917386f2820aacfe	designing efficient irregular networks for heterogeneous systems-on-chip	custom architecture;heterogeneous systems;network on chip;chip;network topology;a priori knowledge;complex system;topology synthesis;routing algorithm;parallel computer;deadlock;domain specificity	Networks-on-chip will serve as the central integration platform in future complex systems-on-chip (SoC) designs, composed of a large number of heterogeneous processing resources. Most researchers advocate the use of traditional regular networks like meshes, tori or trees as architectural templates which gained a high popularity in general-purpose parallel computing. However, most SoC platforms are special-purpose tailored to the domain-specific requirements of their application. They are usually built from a large diversity of heterogeneous components which communicate in a very specific, mostly irregular way. In this work, we propose a methodology for the design of customized irregular networks-on-chip, called INoC. We take advantage of a priori knowledge of the communication characteristic of the application to generate an optimized network topology and routing algorithm. We show that customized irregular networks are clearly superior to traditional regular architectures in terms of performance at comparable implementation costs for irregular workloads. Even more, they inherently offer a high degree of scalability and expansibility which allows to adapt the network to an arbitrary number of nodes with a given communication demand. This can normally not be accomplished by traditional approaches.	system on a chip	Christian Neeb;Norbert Wehn	2008	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2007.07.006	chip;embedded system;complex systems;parallel computing;real-time computing;a priori and a posteriori;computer science;theoretical computer science;deadlock;operating system;distributed computing;network on a chip;network topology;computer network	EDA	2.1569922966428012	60.00319483863169	33614
5e28741bd2faeba704cfb31d4cace767187b2472	on the performance evaluation of fully asynchronous processor architectures	performance evaluation;computer architecture discrete event simulation performance evaluation;computer architecture;parallel discrete event simulation environment performance evaluation asynchronous processor architectures asynchronous control paradigm micronet concurrent functional units fine grained instruction level parallelism occam2;pipelines delay computer architecture circuit synthesis synchronization computer science performance analysis discrete event simulation asynchronous circuits process design;discrete event simulation	This paper evaluates and analyses the influence of an asynchronous control paradigm on the performance of processor architectures. The idea of a micronet is introduced which models the datapath as a network of concurrent functional units which communicate with each other asynchronously. This allows the efficient exploitation of fine-grained instruction-level parallelism (ILP). A micronet-based asynchronous processor (MAP) architecture is described in Occam2 and simulated in a parallel discrete event simulation environment. Suitable metrics are introduced for measuring the performance of the MAP datapath.	asynchronous circuit;datapath;instruction-level parallelism;micronet 800;parallel computing;performance evaluation;programming paradigm;simulation;occam	D. K. Arvind;Vinod E. F. Rebello	1995		10.1109/MASCOT.1995.378702	computer architecture;parallel computing;real-time computing;computer science;discrete event simulation	Arch	-3.1168634475996257	50.79127299446323	33621
08445e35de71fdc32d87fb7736b3a05320d439c3	interconnect design for deep submicron ics	deep submicron ics;required-arrival-time steiner tree higher-order moment signal delay and integrity;shortest-path steiner tree;non-monotone signal response;steiner routings;topology optimization;required-arrival-time steiner tree;bounded-radius steiner tree;accurate delay calculation;signal delay;layout optimization problem;smooth trade-off;steiner tree;shortest path;higher order;optimization problem	In this paper, we study the interconnect layout optimization problem under a higher-order RLC model to optimize not just delay, but also waveform for RLC circuits with non-monotone signal response. We propose a unified approach that considers topology optimization, wiresizing optimization, and waveform optimization simultaneously. Our algorithm considers a large class of routing topologies, ranging from shortest-path Steiner trees to bounded-radius Steiner trees and Steiner routings. We construct a set of required-arrival-time Steiner trees or RATS-trees, providing a smooth trade-off among signal delay, waveform, and routing area. Using a new incremental moment computation algorithm, we interleave topology construction with moment computation to facilitate accurate delay calculation and evaluation of waveform quality. Experimental results show that our algorithm is able to construct a set of topologies providing a smooth trade-off among signal delay, signal settling time, voltage overshoot, and routing cost.	algorithm;combinatorial optimization;computation;delay calculation;mathematical optimization;optimization problem;overshoot (signal);rlc circuit;routing;settling time;shortest path problem;steiner tree problem;topology optimization;very-large-scale integration;waveform;monotone	Jason Cong;David Z. Pan;Lei He;Cheng-Kok Koh;Kei-Yong Khoo	1997	1997 Proceedings of IEEE International Conference on Computer Aided Design (ICCAD)	10.1145/266388.266534	optimization problem;mathematical optimization;topology optimization;electronic engineering;parallel computing;real-time computing;higher-order logic;steiner tree problem;computer science;mathematics;shortest path problem;algorithm	EDA	16.21552235309558	51.93404620980024	33649
613ba231307174932c8603570916873299a6d552	generation of embedded rams with built-in test using object-oriented programming	vlsi;built-in self test;circuit layout cad;integrated circuit testing;knowledge based systems;object-oriented programming;random-access storage;edif-netlist;ramtex;automatic insertion;built-in self-test;design tool;embedded rams;gate level;method knowledge base;design process;knowledge base;object oriented programming;it management	In this paper we present the design tool RAMTEX. It manages the automatic insertion of a Built-In Self-Test (BIST) during the design process. Given a high-level description of an embedded RAM, RAMTEX selects the optimal BIST procedure of all procedures existing in its Method Knowledge Base. The selection process is done by the valuation of weighted parameters provided by a human designer. The BIST-RAM is then synthesized by RAMTEX which includes automatically all the necessary expansions for a self test. The result is an EDIF-netlist at the gate level.RAMTEX is part of the DESIGNER system, a behavioral CAD tool under development in the KEE environment at the University of Hannover.	built-in self-test;computer-aided design;design tool;embedded system;high- and low-level;knowledge base;netlist;random-access memory;value (ethics)	Michael Zimmermann;Manfred Geilert	1990			knowledge base;design process;computer science;automatic test pattern generation;very-large-scale integration;object-oriented programming	EDA	11.589211505307057	51.458328146499944	33670
f8f8260b05408650d9ebd8f685f61b2223a9e3dc	agent based distributed parallel tunneling algorithms	distributed system;algoritmo paralelo;multiagent system;systeme reparti;informatique mobile;parallel algorithm;agent mobile;agent based;agente movil;distributed computing;optimum global;intelligence artificielle;global optimum;algorithme parallele;feasibility;sistema repartido;distributed environment;mathematical programming;calculo repartido;artificial intelligence;global optimization;parallel implementation;inteligencia artificial;mobile agent;sistema multiagente;mobile computing;programmation mathematique;optimo global;calcul reparti;programacion matematica;practicabilidad;faisabilite;systeme multiagent	This paper presents an efficient and scalable approach to solve global optimization problem by distributed parallel tunneling using mobile agents.To use the parallel and concurrent characteristic of the tunneling algorithms and increase the speed of the computing, we use mobile agents as a tool for parallel implementation in a distributed environment. The experimental results demonstrate the feasibility and the potential of the proposed method.	algorithm;tunneling protocol	Yuanqiao Wen;Shengsheng Yu;Jingli Zhou;Liwen Huang	2004		10.1007/978-3-540-30501-9_49	simulation;computer science;artificial intelligence;mobile agent;distributed computing;parallel algorithm;global optimum;mobile computing;distributed computing environment;global optimization	ML	-3.1338400171076177	33.70489406921218	33720
bcb9e10ae1b29c48a433e5de6823a1849c7cfb24	cad tools for bist/dft and delay faults	cad tool;delay fault		built-in self-test;computer-aided design	Spyros Tragoudas	1999		10.1201/9781420049671.ch68	cad;computer architecture;computer science	EDA	9.82208862068042	51.962659544183445	33760
2adb880f73003daf9d03075417beaf9e91397dfd	an application-specific design methodology for on-chip crossbar generation	multiprocessor interconnection networks;timing closure;design automation;power efficient interconnection architecture;design process;traffic streams;tool support;network on chip;actual traffic trace analysis;application software;cad;power efficiency;traffic control;traffic rates;wires;physical design;telecommunication traffic cad digital simulation microprocessor chips multiprocessor interconnection networks system on chip;power system interconnection;timing closure application specific bus crossbar floorplan networks on chips nocs systemc systems on chips socs;synthesized crossbar design;satisfiability;chip;design complexity handling;transaction latencies reduction;application specific;register transfer level models;computer architecture;crossbar;telecommunication traffic;optimal crossbar architecture design;simulation based design approach;computer aided design tool;system on chip;systems on chips socs;energy consumption;power consumption estimation;multi processor system on chip;cycle accurate systemc simulation;application specific design methodology networks on chip transaction latencies reduction synthesized crossbar design industry standard tools register transfer level models cycle accurate systemc simulation power consumption estimation wiring complexity traffic streams traffic rates actual traffic trace analysis simulation based design approach optimal crossbar architecture design computer aided design tool design complexity handling soc multiprocessor systems on chips power efficient interconnection architecture on chip crossbar generation;multiprocessor systems on chips;bus;soc;networks on chip;time to market;multiprocessing systems;networks on chips nocs;power consumption;application specific design methodology;on chip crossbar generation;industry standard tools;design methodology traffic control energy consumption application software wires power system interconnection computer architecture multiprocessing systems time to market design automation	Designing a power-efficient interconnection architecture for multiprocessor systems-on-chips (MPSoCs) satisfying the application performance constraints is a nontrivial task. In order to meet the tight time-to-market constraints and to effectively handle the design complexity, it is essential to provide a computer-aided design tool support for automating this task. In this paper, we address the issue of ldquoapplication-specific design of optimal crossbar architecturerdquo satisfying the performance requirements of the application and optimal binding of the cores onto the crossbar resources. We present a simulation-based design approach that is based on the analysis of the actual traffic trace of the application, considering local variations in traffic rates, temporal overlap among traffic streams, and criticality of traffic streams. Our approach is physical design aware, where the wiring complexity of the crossbar architecture is also considered during the design process. This leads to detecting timing violations on the wires early in the design cycle and to having accurate estimates of the power consumption on the wires. We apply our methodology onto several MPSoC designs, and the synthesized crossbar platforms are validated for performance by cycle-accurate SystemC simulation of the designs. The crossbar matrix power consumption values are based on the synthesis of the register transfer level models of the designs, obtained using industry standard tools. The experimental case studies show large reduction in communication architecture power consumption (45.3% on average) and total wirelength (38% on average) for the MPSoC designs when compared with traditional design approaches. The synthesized crossbar designs also lead to large reduction in transaction latencies (up to 7 ) when compared with the existing design approaches.	computer-aided design;criticality matrix;crossbar switch;design tool;interconnection;mpsoc;multiprocessing;physical design (electronics);quality of service;register-transfer level;requirement;scalability;sensor;simulation;system on a chip;systemc;technical standard;tracing (software);wiring	Srinivasan Murali;Luca Benini;Giovanni De Micheli	2007	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2006.888284	system on a chip;embedded system;computer architecture;electronic engineering;parallel computing;real-time computing;electronic design automation;computer science;operating system;network on a chip	EDA	2.4839594510805174	58.972251884392406	33801
5e9aebba318fe8caa422fd7a6f5b1397695167da	cycle-based algorithm used to accelerate vhdl simulation	high performance;simulation environment	Cycle-based algorithm has very high performance for the simulation of synchronous design, but it is confined to synchronous design and it is not as accurate as event-driven algorithm. In this paper, a revised cycle-based algorithm is proposed and implemented in VHDL simulator. Event-driven simulation engine and cycle-based simulation engine have been imbedded in the same simulation environment and can be used to asynchronous design and synchronous design respectively. Thus the simulation performance is improved without losing the flexibility and accuracy of event-driven algorithm.	algorithm;asynchronous circuit;event-driven programming;simulation;synchronous circuit;vhdl	Yang Xun;Liu Mingye	2000	Journal of Computer Science and Technology	10.1007/BF02948875	dynamic simulation;computer architecture;real-time computing;simulation;computer science	EDA	0.06673511806862553	56.2662442541635	33802
e4f23a7155eac1492ea863b07090abcfdc2d0c20	guest editor introduction: special issue on multiprocessor-based embedded systems	embedded system	The hardware and software architectures of embedded systems have undergone a significant evolution with the increase in their functional complexity and performance expectations. Embedded systems are no longer necessarily characterised by a single fixed application. The need to execute several different, although related, applications, combined with time-to-market pressures have led to architectures with multiple processors on a system-on-a-chip (SoC). This brings back into relevance traditional multiprocessor environments: processor allocation and task scheduling become important topics; integration of software and the building of abstraction layers to make it immune to changing of underlying hardware platforms becomes essential; and network and communication architectures need careful analysis. However, the domain-specific nature of embedded systems continues to lead to new challenges and optimisation opportunities. Making the hardware architecture flexible leads to several new interesting problems which can partially reuse existing multiprocessor related solutions, but also need fundamental innovations to extract efficiency. In this edition we present invited papers from several leading research groups on the topic of multiprocessor-based embedded systems. In the paper “A fast and accurate technique for mapping parallel applications on stream-oriented MPSoC platforms with communication-awareness”, Ruggeiro et al. present a framework for task allocation and scheduling in a multi-processor on chip platform. The combination of integer programming with constraint programming is demonstrated to result in optimal solutions reasonably fast. This approach helps improve performance for multiprocessor based systems that are not susceptible to	central processing unit;constraint programming;embedded system;integer programming;mpsoc;mathematical optimization;multiprocessing;relevance;scheduling (computing);system on a chip	Preeti Ranjan Panda	2007	International Journal of Parallel Programming	10.1007/s10766-007-0060-3	computer science	Embedded	-0.2210864201748551	55.36783876022328	33840
18fe2501b7524dadc2f3e03249355ab61d8412fd	application of the novel associative programmable array-structure multi-match-pla in synthesis of decomposed finite state machines	finite state machine	Abstract   A method is presented for decomposing a finite state machine based on identifying subgraphs in the state transition graph consisting only of unconditional transitions, extracting these subgraphs and implementing them as separate machines. Implementation is performed targeting an hierarchical control architecture based on the novel associative programmable array-structure Multi-Match-PLA which is a very efficient implementation of a complex content-addressable memory in terms of area. First experimental results are included.		Jörg Kottsieper;Klaus Waldschmidt	1993	Microprocessing and Microprogramming	10.1016/0165-6074(93)90181-J	parallel computing;computer science;theoretical computer science;machine learning;distributed computing;programming language;algorithm	EDA	8.655809836485666	47.18434134896167	33842
e139ea97b6315b98e2e82a6a6ce8c407eff7b917	precise evaluation of the fault sensitivity of ooo superscalar processors		Since superscalar processors lead the market, their resiliency evaluation by means of fault injection grows in importance. Fault injection strategies usually trade-off their levels of accuracy: low-level HW-based methods are accurate, but very expensive, need special equipment and the actual hardware, and lack controllability; while high-level simulation-based strategies are flexible, fast, easily accessible and have high controllability, but are not accurate since they are based on models that do not always reflect the low-level implementation, mainly when it comes to complex designs like out-of-order multiple-issue processors. In this work, we propose a cycle-accurate fault injection platform for superscalar processors, which has a smart checkpointing mechanism to accelerate injection time, attenuating the short-comings imposed by the aforementioned fault injection methods while providing the same level of abstraction as detailed RTL models. Leveraging from this new platform, we evaluate a complex and parameterizable Out-of-Order processor (BOOM) by experimenting with different issue widths and analyzing the sensitivity of several hardware structures of the processor.	algorithm;application checkpointing;central processing unit;experiment;fault injection;fault tolerance;high- and low-level;simulation;superscalar processor;vii	Rafael Billig Tonetto;Gabriel L. Nazar;Antonio Carlos Schneider Beck	2018	2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)	10.23919/DATE.2018.8342082	real-time computing;computer science;parallel computing;controllability;fault injection;superscalar	EDA	5.106720736514567	58.29479439102555	33876
2f90bb5984a67644ddbfdcd3a745225134d2f0f6	bram implementation of a single-event upset sensor for adaptive single-event effect mitigation in reconfigurable fpgas		In this paper, we study the performance of a Block RAM (BRAM)-based embedded radiation sensor for adaptive single-event effect mitigation in FPGAs. To achieve this, we designed custom BRAM wrappers to extend the Xilinx BRAM macros with scrubbing and error correction, for both free and used BRAMs (utilized by the user). A case study demonstrates that in a system with all 298 BRAMs (of the Virtex-5QV) in use for sensing, a system failure rate of λ = 0.05 a−1 can be expected, for the BRAM sensor itself. The resource overhead of this single-event upset sensor is 4.9 % of the FPGA resources (maximum of lookup tables and flip-flops). We conclude that our single-event upset sensor is robust enough to measure radiation levels reliably in real time. Our work is part of the Fraunhofer On-Board Processor, which is part of a geostationary earth orbit communication satellite.	communications satellite;embedded system;error detection and correction;experiment;flops;failure rate;field-programmable gate array;l-system;lookup table;memory scrubbing;overhead (computing);random-access memory;sensor;single event upset;thermal design power	Robért Glein;Philipp Mengs;Florian Rittner;Rainer Wansch;Albert Heuberger	2017	2017 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)	10.1109/AHS.2017.8046352	error detection and correction;field-programmable gate array;redundancy (engineering);communications satellite;computer science;real-time computing;geostationary orbit;embedded system;upset;single event upset;lookup table	Mobile	7.910871768807712	59.240135951488405	33985
b3499794eb6dca0b94c2ee5e42a7eb6478001906	optimal automatic periodic multiprocessor scheduler for fully specified flow graphs	traitement signal;scheduling adaptive filters digital filters multiprocessing systems;synchronous generators;digital signal processing;signal generators;multiprocessor scheduling;digital signal processing digital filters terminology signal generators synchronous generators signal processing algorithms scheduling algorithm adaptive scheduling;concurrent computing;multiprocessor;processor scheduling;graph flow;scattering;flow graphs;digital filter;flujo grafo;nonadaptive filters;digital filtering;tratamiento numerico;scheduling algorithm;adaptive filters;descripcion;flot graphe;scheduling;signal processing;digital filters;analyse performance;performance analysis;adaptive scheduling;dsp algorithms;filtrado numerico;scheduling problem;terminology;digital processing;fully specified flow graphs;multiprocessing systems;multiprocesador;signal processing algorithms;algoritmo optimo;algorithme optimal;description;optimal algorithm;procesamiento senal;digital filters optimal automatic periodic multiprocessor scheduler dsp algorithms adaptive filters nonadaptive filters fully specified flow graphs;traitement numerique;filtrage numerique;optimal automatic periodic multiprocessor scheduler;analisis eficacia;multiprocesseur	The basic terminology, a review of the underlying theory, a detailed description, and the performance of an automatic multiprocessing scheduler are presented. This scheduler generates an optimal deterministic synchronous multiprocessor realization of a digital signal processing (DSP) algorithm. It provides a reasonable solution to the optimal multiprocessing scheduling problem for the class of digital filters commonly used in the DSP community. The experimental study of the performance of this scheduler emphasizes adaptive and nonadaptive digital filters. >	multiprocessing;scheduling (computing)	Pedro R. Gelabert;Thomas P. Barnwell	1993	IEEE Trans. Signal Processing	10.1109/78.193223	computer vision;parallel computing;real-time computing;digital filter;concurrent computing;computer science;signal processing;distributed computing;scheduling	Embedded	5.186767397507444	46.9115062848541	33988
12c5e83dd78bd3ffd2866df53e8e9016ee1449d3	exploring efficiency of ring oscillator- based temperature sensor networks on fpgas	power overhead;efficiency;ring oscillator;area overhead;fpga;soft sensor;thermal overhead;exploration;temperature;thermal map accuracy;network of temperature sensor	Due to technology advances and complexity of designs, thermal issue is a bottleneck in electronics designs. Various dynamic thermal management techniques have been proposed to address this issue. To effectively apply thermal management techniques, providing an accurate thermal map of chips is highly required. For this goal, a network of temperature sensors ought to be provided. There are various implementations for temperature sensors and network of sensors on Field Programmable Gate Arrays (FPGAs). This work defines and formulates four metrics and criteria, in terms of area, thermal, and power overheads and thermal map accuracy for exploring and evaluating efficiency of different implementations of Ring Oscillator-based Temperature Sensor (ROTS) networks on FPGAs and reports the comparison results for 12 networks with various sensor configurations. According to our metrics and experiments, the sensor that it is composed of NOT gates with open latches and RNS ring counter has lower thermal and power overheads compared to other configurations. Moreover, in this work, a new ROTS is presented that occupies 25% less resources than the most compact temperature sensor. Also, it provides 1.72 times higher sensitivity than the best sensitive ROTS design.	bottleneck (engineering);experiment;field-programmable gate array;heat map;residue number system;ring counter;ring oscillator;sensor;thermal management (electronics)	Navid Rahmanikia;Amirali Amiri;Hamid Noori;Farhad Mehdipour	2015		10.1145/2684746.2689104	embedded system;parallel computing;real-time computing;exploration;temperature;soft sensor;computer science;ring oscillator;efficiency;field-programmable gate array	EDA	15.23045376856427	58.60982761139964	34007
377ad0ef404ca07410baeb5bc38de6fbb858d012	3d network-on-chip design for embedded ubiquitous computing systems		Ubiquitous High-Performance Computing applications, such as cyber-physical systems, sensor networks and virtual reality require the support of terascale embedded systems that can deliver higher performance than current systems. Multi-cores, many-cores and heterogeneous processors are becoming the typical design choices to satisfy these requirements. Inter core communication has been one of the major challenges as it affects the bandwidth and power consumption of the multi-cores processors. Network-onChips (NoCs) present a fast and scalable interconnection solution to fulfill the requirements of Ubiquitous High-Performance Computing applications. This paper proposes a novel 3D Network-on-Chip called Octagon for Ubiquitous Computing (OUC) that is designed for Embedded Ubiquitous Computing Systems. OUC provides low network diameter and path diversity. Simulation results show that the proposed topology achieves a significant decrease in latency and an average 21.54% and 12.89% improvement in average throughput under hotspot and uniform traffic pattern respectively. © 2016 Elsevier B.V. All rights reserved.	central processing unit;cyber-physical system;embedded system;inter-process communication;interconnection;java hotspot virtual machine;multi-core processor;network on a chip;requirement;scalability;simulation;terascale (microarchitecture);throughput;ubiquitous computing;virtual reality	Zheng Wang;Huaxi Gu;Yawen Chen;Yingtang Yang;Kun Wang	2017	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2016.10.002	embedded system;parallel computing;real-time computing;computer science;operating system;distributed computing;computer network	EDA	0.3040201933331131	60.41342392158537	34024
ab749c2cbc9615f2bde770d78e1a6eb284e2834b	interfacing an fpga with an external circuit and applications		WHAT IS A FIELD PROGRAMMABLE GATE ARRAY? According to Xilinx (http://www.xilinx.com/), the inventors of the FPGA: “Field Programmable Gate Arrays (FPGAs) are semiconductor devices that are based around a matrix of configurable logic blocks (CLBs) connected via programmable interconnects. FPGAs can be reprogrammed to desired application or functionality requirements after manufacturing.” For engineers or scientists who want to make design decisions at the hardware level, an FPGA is a useful development tool. Figure 2. Astable multivibrator circuit diagram with LEDs. Interfacing an FPGA with an External Circuit and Applications By Alexander DeForge	circuit diagram;field-programmable gate array;multivibrator;requirement;semiconductor device;speaker wire	Alexander DeForge	2017	ACM Crossroads	10.1145/3155224	field-programmable gate array;world wide web;computer science;embedded system;interfacing	EDA	6.717393036124211	49.51289475137058	34109
0ce806d54cd11a8f02a2ad6a6c7bc3ae7488c34b	predicting variability in nanoscale lithography processes	analytical models;support vector machines electronic engineering computing integrated circuit layout learning artificial intelligence nanolithography;histograms;machine learning algorithms;process variation;lithography simulations;integrated circuit layout;support vector machines;large scale full chip analysis;training;design for manufacture;kernel methods;semiconductor device measurement;support vector machine algorithms;layout;photo lithography;chip;lithography;large scale;manufacturing processes;machine learning techniques;machine learning;layout patterns;target layout representation;integrated circuits nanoscale lithography processes layout patterns lithography simulations large scale full chip analysis machine learning techniques support vector machine algorithms target layout representation;integrated circuit modeling;transforms;nanolithography;kernel method;predictive models;electronic engineering computing;kernel methods photo lithography process variation modeling variability machine learning;nanoscale lithography processes;support vector machine;learning artificial intelligence;lithography manufacturing processes design for manufacture machine learning analytical models predictive models support vector machines machine learning algorithms integrated circuit modeling semiconductor device measurement;integrated circuits;modeling variability	As lithography process nodes shrink to sub-wavelength levels generating acceptable layout patterns becomes a challenging problem. Traditionally, complex convolution based lithography simulations are used to estimate areas of high variability. These methods are slow and infeasible for large scale full chip analysis. This work proposes a solution to this problem by using machine learning techniques to identify layout areas that are more prone to variability. A novel target layout representation is proposed, and the latest support vector machine (SVM) algorithms are used to detect variability within standard cells and between cells in a simulated full chip layout.	algorithm;convolution;experiment;heart rate variability;inter-rater reliability;machine learning;sensor;simulation;spatial variability;support vector machine	Dragoljub Gagi Drmanac;Frank Liu;Li-C. Wang	2009	2009 46th ACM/IEEE Design Automation Conference	10.1145/1629911.1630053	lithography;support vector machine;kernel method;electronic engineering;computer science;machine learning;engineering drawing	EDA	23.14777866341955	56.042238166556004	34146
81a3aea4e3d1ae9afb05fa265d1986f5bf167a8f	microcomputer for the dynamics laboratory		A microcomputer developed for use in a dynamics laboratory of a mechanical engineering department is discussed. The microcomputer is based on the Motorola 6802 microprocessor and includes an EPROM burning circuit for flexibility. Special mathematical routines have been developed for the computer. These routines are designed for use with 4-digit floating-point numbers to gain speed with minimal sacrifice of accuracy. The special routines include a data approximation and smoothing routine for signal processing, and routines to compute frequently used mathematical functions. The routines are written in 6802 assembly language and are stored in 2 kbyte of external EPROM. An application of the microcomputer used as a component in a speed controller is also discussed.	microcomputer	James Martin;Jaeho Kim	1986	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(86)90152-3	embedded system;parallel computing;real-time computing;computer hardware;computer science;electrical engineering;operating system	EDA	7.008917029418338	48.65621817598707	34171
9bc2f60a981fb5be9085a0926d4465ea88905276	secure lempel-ziv-welch (lzw) algorithm with random dictionary insertion and permutation	lempel ziv;lempel ziv welch algorithm;security analysis;complexity theory;encryption scheme;data compression;encryption;dictionaries indexes security cryptography encoding algorithm design and analysis complexity theory;lzw;random dictionary insertion;digital rights management;random dictionary permutation;indexes;digital rights management lzw compression encryption;security analysis lempel ziv welch algorithm random dictionary insertion random dictionary permutation encryption scheme bit wise xor module;cryptography;dictionaries;compression;data compression cryptography;security;encoding;bit wise xor module;algorithm design and analysis;digital right management	In this paper, we propose an efficient encryption scheme by introducing randomness into the Lempel-Ziv-Welch (LZW) algorithm. This scheme utilizes random dictionary insertion and permutation, and incorporates with a bit-wise XOR module. Security analysis results show that the proposed scheme provides high level of security without any coding efficiency loss, compared with a standard LZW algorithm.	algorithm;algorithmic efficiency;cryptosystem;dictionary;encryption;exclusive or;experiment;high-level programming language;lempel–ziv–stac;lempel–ziv–welch;randomness;welch's method	Jiantao Zhou;Oscar C. Au;Xiaopeng Fan;Peter Hon-Wah Wong	2008	2008 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2008.4607417	data compression;arithmetic;computer science;cryptography;information security;theoretical computer science;digital rights management;security analysis;compression;encryption;algorithm;encoding;statistics	EDA	8.93458911964561	36.63740072856857	34181
3f979edf3d52ead844b469322563e594df36f96e	provably sublinear point multiplication on koblitz curves and its hardware implementation	field programmable gate array;nonadjacent form expansions;non adjacent form;sublinearity;fpga implementation;elliptic curve cryptography;sublinear point multiplication;cryptography;field programmable gate arrays cryptography;multiple base expansions;parallel processing sublinear point multiplication koblitz curves hardware implementation multiple base expansions sublinear type nonadjacent form expansions fpga implementation performance data;sublinearity elliptic curve cryptography field programmable gate arrays koblitz curves multiple base expansions parallel processing;field programmable gate arrays;performance data;koblitz curves;hardware implementation;parallel processing;sublinear type	We describe algorithms for point multiplication on Koblitz curves using multiple-base expansions of the form k = Sigmaplusmntaua(tau-1)b and k = Sigmaplusmntaua(tau - mu)b(tau2 - mutau - 1)c. We prove that the number of terms in the second type is sublinear in the bit length of k, which leads to the first provably sublinear point multiplication algorithm on Koblitz curves. For the first type, we conjecture that the number of terms is sublinear and provide numerical evidence demonstrating that the number of terms is significantly less than that of tau-adic nonadjacent form expansions. We present details of an innovative FPGA implementation of our algorithm and performance data demonstrating the efficiency of our method. We also show that implementations with very low computation latency are possible with the proposed method because parallel processing can be exploited efficiently.	bit-length;computation;critical path;experiment;field-programmable gate array;interrupt latency;level of measurement;lookup table;microsoft windows;multiplication algorithm;numerical analysis;parallel computing;precomputation;window function	Vassil S. Dimitrov;Kimmo Järvinen;Michael J. Jacobson;W. F. Chan;Zhun Huang	2006	IEEE Transactions on Computers	10.1109/TC.2008.65	embedded system;parallel processing;discrete mathematics;parallel computing;computer science;theoretical computer science;mathematics;field-programmable gate array	Comp.	9.36758014499099	43.92547205238818	34217
412dde1684d22c83e52264ffca50735c8b995f44	low-cost and shared architecture design of recursive dft/idft/imdct algorithms for digital radio mondiale system	digital audio broadcast;recursive estimation;architectural design;inverse modified cosine transform;idft;discrete fourier transforms algorithm design and analysis computer architecture hardware signal processing algorithms computational complexity receivers;recursive algorithm digital radio mondiale system dft idft imdct discrete fourier transform inverse dft inverse modified cosine transform recursive dft recursive idft;recursive estimation digital audio broadcasting discrete cosine transforms discrete fourier transforms;digital radio mondiale system;inverse modified cosine transform digital audio broadcasting discrete fourier transform recursive architecture;receivers;recursive dft;computer architecture;computational complexity;discrete cosine transforms;recursive idft;discrete fourier transform;recursive architecture;digital audio broadcasting;imdct;digital radio mondiale;recursive algorithm;inverse dft;cosine transform;signal processing algorithms;discrete fourier transforms;dft;hardware implementation;algorithm design and analysis;hardware	This paper presents a shared architecture design for computations of Discrete Fourier Transform (DFT), Inverse DFT (IDFT), and Inverse Modified Cosine Transform (IMDCT) algorithms. The proposed method has following advantages, such as 1) Low hardware costs compared with conventional recursive DFT (RDFT) and recursive IDFT (RIDFT) algorithms. For hardware implementation, 2 real multipliers and 8 real adders are required in the proposed architecture. Compared with previous approaches, it can greatly reduce 80% of number of multipliers and 53% of number of adders. 2) The proposed RDFT-based IMDCT architecture has four times data throughput more than the other existing recursive algorithm 3) Computational cycles of the proposed RDFT-based IMDCT algorithm can be greatly reduced by 50% more than our previous recursive IMDCT algorithms. Hence, the proposed RDFT / RIDFT / RDFT-based IMDCT design is more suitable for portable devices in Digital Radio Mondiale (DRM) system.	algorithm;computation;discrete fourier transform;modified discrete cosine transform;personal digital assistant;recursion (computer science);throughput;very-large-scale integration	Shin-Chi Lai;Sheau-Fang Lei;Ching-Hsing Luo	2010	2010 Sixth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIHMSP.2010.76	discrete mathematics;computer science;theoretical computer science;discrete fourier transform;algorithm	EDA	11.620164114841012	43.79222665413168	34292
31c292db55938febdbd8bacbf29abf14afa1dc2e	parallel integration of planetary systems on gpus	simulation;planetary system;planetary systems;direct numerical simulation;cuda;gpgpu;relative efficiency;parallel computer;graphic processing unit	Direct numerical simulations of large numbers of planetary systems are key to the study extrasolar planetary systems. We investigate the potential of Graphics Processing Units (GPUs) to dramatically accelerate such calculations and enable new types of analyses. Here we report on a CUDA implementation of two different integrators, Verlet and Bulirsch-Stoer; that is, we are contrasting the relative efficiency of a low-memory-footprint, small-time-steps integrator with a more complex but highly stable integrator for best use of GPU resources.  We verify that GPUs can serve as highly efficient dedicated parallel computing co-processors of the CPU host: the wall-clock time of integrating a large number of systems is reduced by two orders of magnitude compared to a comparable implementation we used so far on a modern CPU. We verify further that it is not only possible to port a complex integration algorithm like Bulirsch-Stoer to the restrictive GPU environment but that it can outperform the simpler Verlet integrator.	bulirsch–stoer algorithm;cuda;central processing unit;graphics processing unit;memory footprint;numerical analysis;parallel computing;planetary scanner;simulation;verlet integration	Jianwei Gao;Eric B. Ford;Jörg Peters	2008		10.1145/1593105.1593123	computational science;parallel computing;computer hardware;computer science	HPC	-4.355838523725267	39.339374357135114	34348
9cf0746acd1cac27cbaa4d1e581117e126e69b6e	session abstract		Test and verification challenges force industry to explore various partitioning alternatives for the limited test resources. One such alternative involves moving the test instruments inside the chip. This session will present three such solutions where logic analyzer have been embedded in the FPAGs for better observability and reduced design debug times.	embedded system;logic analyzer	Ajay Khoche;Peter Muhmenthaler	2006	24th IEEE VLSI Test Symposium	10.1109/VTS.2006.77		EDA	10.32960201159876	54.5540297630626	34422
80fb8a6459232879e2d228f431c378c0151fcbc8	a modular power-aware microsensor with >1000x dynamic power range	linux;energy conservation;microcontrollers;microsensors;power control;radio tracking;vehicles;linux-based processor module;acoustic vehicle tracking application;advanced sleep mode;duty cycle;hard real-time function;independent power-down control;isolation switch;local power microcontroller;modular power-aware microsensor;module subsystem;operational dynamic power range;power efficiency;power switch;radio module;resource processing;wheeled vehicle	We introduce a power-aware microsensor architecture supporting a wide operational power range (from <1mW to >10W). The platform consists of a family of modules that follow a common set of design principles. Each module includes a local power microcontroller, power switches, and isolation switches to enable independent power-down control of modules and module subsystems. Processing resources are scaled appropriately on each module for their role in the collective system. Hard real-time functions are migrated to the sensor and radio modules for improved power efficiency. The optional Linux-based processor module supports high duty cycling and advanced sleep modes. Our reference hardware implementation is described in detail in this paper. Seven different modules have been developed. We utilize an acoustic vehicle tracking application to demonstrate how the architecture operates and report on results from field tests on tracked and wheeled vehicles.	acoustic cryptanalysis;duty cycle;linux;microcontroller;network switch;norm (social);performance per watt;real-time clock;real-time operating system;sensor;vehicle tracking system	Brian Schott;Michael Bajura;Joseph Czarnaski;Jaroslav Flidr;Tam Tho;Li Wang	2005	IPSN 2005. Fourth International Symposium on Information Processing in Sensor Networks, 2005.		microcontroller;embedded system;real-time computing;electrical efficiency;energy conservation;power control;computer science;operating system;component;linux kernel;duty cycle	Arch	2.9991487153619607	34.52827411241338	34432
48f4c840c1f3c593c7129e62c6df8711b071650c	a compiler back-end for reconfigurable, mixed-isa processors with clustered register files	pattern clustering;specification languages c language instruction sets parallel processing pattern clustering program compilers reconfigurable architectures software engineering;reconfigurable architectures;registers computer architecture resource management hazards reduced instruction set computing clustering algorithms;reduced instruction set computing;resource management;low level virtual machine mixed isa compiler clustered vliw dynamic reconfigurable architecture;hazards;software engineering;clustered vliw;computer architecture;c language;registers;specification languages;mixed isa compiler;clustering algorithms;dynamic reconfigurable architecture;resource aware reconfiguration llvm compiler back end reconfigurable mixed isa processors clustered register files reconfigurable tile based architectures tile dynamic interconnection run time scalable issue width instruction set architecture rsiw c c mixed isa software development clustered vliw isa parallel operations architecture description language mixed isa code generation;program compilers;parallel processing;low level virtual machine;instruction sets	Reconfigurable tile-based architectures can dynamically interconnect several tiles in order to establish processor instances with varying resource, performance, and energy characteristics at run time. These flexible processor instances offer a new degree of freedom for adapting to changing applications' requirements while optimizing resource and energy consumption. Our solution for dynamic interconnection of tiles requires a flexible Run-Time Scalable Issue-Width (RSIW) Instruction Set Architecture (ISA) that changes dependent on the configuration. In order to enable high-level programmability of our architecture in C/C++ a novel compiler back-end is needed. In this paper we address this necessity by presenting a novel LLVM compiler back-end targeting the reconfigurable RSIW ISA and supporting mixed-ISA software development. RSIW is comparable to clustered-VLIW ISAs since it expresses parallel operations within the ISA and explicitly uses clustered register files. Therefore, we extended our architecture description language based RISC LLVM back-end by representations of parallel operations as well as compilation passes for clustering and scheduling of parallel operations as well as mixed-ISA code generation. Based on the novel back-end we compare the performance characteristics of several applications compiled for and simulated on different configurations. Additionally, we demonstrate resource-aware reconfiguration by a mixed-ISA application scenario.	approximation algorithm;architecture description language;automatic parallelization;bottom-up parsing;c++;central processing unit;cluster analysis;code generation (compiler);compiler;executable;greedy algorithm;high- and low-level;image scaling;instruction selection;instruction set simulator;interconnection;llvm;programmer;reconfigurability;register allocation;register file;requirement;run time (program lifecycle phase);scheduling (computing);simulation;software development;speedup;very long instruction word;whole earth 'lectronic link	Timo Stripf;Ralf König;Patrick Rieder;Jürgen Becker	2012	2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum	10.1109/IPDPSW.2012.60	parallel processing;reduced instruction set computing;computer architecture;parallel computing;real-time computing;hazard;computer science;resource management;operating system;instruction set;processor register;cluster analysis;programming language	Arch	-3.3656055963077836	49.68630134079079	34457
4a2a24348ad90e6a1e0f268917430717fe3706f9	fault insertion techniques and models for digital logic simulation	fault simulation;design and development;digital logic	During the past few years it has become increasingly apparent that in order to design and develop highly reliable and maintainable digital logic systems it is necessary to be able to accurately simulate those systems. Not only is it necessary to be able to simulate a logic net as it was intended to behave, but it is also necessary to be able to model or simulate the behavior of the logic net when it contains a physical defect. (The representation of a physical defect is known as a fault.) The behavioral simulation of a digital logic net which contains a physical defect, or fault, is known as digital fault simulation.	boolean algebra;diode;formal system;hazard analysis;logic simulation;software bug	Stephen A. Szygenda;Edward W. Thompson	1972		10.1145/1480083.1480112	control engineering;electronic engineering;computer science;stuck-at fault;logic simulation;fault model;register-transfer level;computer engineering	EDA	23.854492975772317	52.14335646571411	34502
23e91aba0d65447ff3c4eb7f2c0fc7dafe0e498a	loosely-stabilizing leader election on arbitrary graphs in population protocols without identifiers nor random numbers	004;loose stabilization population protocols leader election	In the population protocol model Angluin et al. proposed in 2004, there exists no self-stabilizing leader election protocol for complete graphs, arbitrary graphs, trees, lines, degree-bounded graphs and so on unless the protocol knows the exact number of nodes. To circumvent the impossibility, we introduced the concept of loose-stabilization in 2009, which relaxes the closure requirement of self-stabilization. A loosely-stabilizing protocol guarantees that starting from any initial configuration a system reaches a safe configuration, and after that, the system keeps its specification (e.g. the unique leader) not forever, but for a sufficiently long time (e.g. exponentially large time with respect to the number of nodes). Our previous works presented two loosely-stabilizing leader election protocols for arbitrary graphs; One uses agent identifiers and the other uses random numbers to elect a unique leader. In this paper, we present a loosely-stabilizing protocol that solves leader election on arbitrary graphs without agent identifiers nor random numbers. By the combination of virus-propagation and token-circulation, the proposed protocol achieves polynomial convergence time and exponential holding time without such external entities. Specifically, given upper bounds N and ∆ of the number of nodes n and the maximum degree of nodes δ respectively, it reaches a safe configuration within O(mn3d+mN∆2 logN) expected steps, and keeps the unique leader for Ω(Ne ) expected steps where m is the number of edges and d is the diameter of the graph. To measure the time complexity of the protocol, we assume the uniformly random scheduler which is widely used in the field of the population protocols. 1998 ACM Subject Classification G.2.2. Graph Theory	analysis of algorithms;communications protocol;directed graph;entity;graph (discrete mathematics);graph theory;identifier;interaction;leader election;polynomial;population protocol;scheduling (computing);self-stabilization;software propagation;time complexity	Yuichi Sudo;Fukuhito Ooshita;Hirotsugu Kakugawa;Toshimitsu Masuzawa	2015		10.4230/LIPIcs.OPODIS.2015.14	computer science;leader election;distributed computing	Theory	17.39489965415081	34.72836279493041	34551
55fe0869c0a3d577cd99dd6ddbd20ccb19662b30	a 23.6mb/mm2 sram in 10nm finfet technology with pulsed pmos tvc and stepped-wl for low-voltage applications		The emergence of cloud computing and big data analytics, accompanied by a sustained growth of battery-powered mobile devices, continues to drive the importance of energy and area efficient CPU and SoC designs. Low-voltage operation remains one of the primary approaches for active power reduction, but SRAM l/MIN can limit the minimum operating voltage. Device size quantization continues to be a challenge for compact 6T SRAM design in FinFET technologies, where careful co-optimization of the technology and assist circuit design is required for high-density low-voltage array implementations. This paper presents two SRAM array designs in a 10nm low-power CMOS technology featuring 3rd generation FinFET transistors: a high-density 23.6Mb/mm2 array and a low-voltage 20.4Mb/mm2 array.	big data;cmos;central processing unit;circuit design;cloud computing;emergence;low-power broadcasting;mathematical optimization;mobile device;pmos logic;static random-access memory;system on a chip;tatsunoko vs. capcom:;transistor	Zheng Guo;Daeyeon Kim;Satyanand Nalam;Jami Wiedemer;Xiaofei Wang;Eric Karl	2018	2018 IEEE International Solid - State Circuits Conference - (ISSCC)	10.1109/ISSCC.2018.8310251	static random-access memory;computer science;electronic engineering;transistor;pmos logic;low voltage;logic gate;circuit design;ac power;cmos	EDA	15.927529497512772	60.26499315618341	34557
e49e75fca725742e12bfbb58c02a5a8017a0e89d	a 64 mb sram in 32 nm high-k metal-gate soi technology with 0.7 v operation enabled by stability, write-ability and read-ability enhancements	stability analysis;logic gate;integrated circuit;high k metal gate;logic gates;silicon on insulator;static random access memory;thermal stability	A 64 Mb SRAM macro has been fabricated in a 32 nm high-k metal-gate SOI technology. The SRAM features a 0.154 μm2 bit-cell, the smallest to date for a 32 nm SOI product. A 0.7 V VDDMIN operation is enabled by three assist features. Stability is improved by a bit-line regulation scheme which reduces charge injection into the bit-cell. Enhancements to the write path include an increase of 40% of bit-line boost voltage. Finally, a bit-cell-tracking delay circuit improves both performance and yield across the process space.	bit cell;equivalent oxide thickness;failure rate;high-κ dielectric;image scaling;line regulation;megabyte;modulation;silicon on insulator;static random-access memory;system on a chip;thickness (graph theory)	Harold Pilo;Igor Arsovski;Kevin Batson;Geordie Braceras;John Gabric;Robert M. Houle;Steve Lamphier;Carl Radens;Adnan Seferagic	2012	IEEE Journal of Solid-State Circuits	10.1109/JSSC.2011.2164730	electronic engineering;static random-access memory;logic gate;computer hardware;computer science;engineering;electrical engineering	Arch	17.53290244460828	59.70880053852428	34585
ba9c46fc9261d2c1169ffdda3c90403f16adc1a7	system-level modelling and analysis of embedded reconfigurable cores for wireless systems	systemc hdl cosimulation scenario;ip integration design flow;c based wimax system system level modelling analysis embedded reconfigurable core wireless system ip integration design flow multiple context representation system on chip soc design systemc hdl cosimulation scenario interoperability reconfigurable fft fast fourier transform viterbi architecture;multiple context representation;c based wimax system;hardware description languages;reconfigurable fft;integrated design;fast fourier transform;soc design;system on chip;object oriented modeling power system modeling wimax reconfigurable architectures system on a chip viterbi algorithm energy consumption hardware design methodology design engineering;wireless system;fast fourier transforms;embedded reconfigurable core;system level modelling analysis;wimax fast fourier transforms hardware description languages industrial property open systems system on chip;industrial property;interoperability;power consumption;wimax;open systems;wireless systems;viterbi architecture	A new system-level approach is needed to incorporate re-configurability in IP-integration design flow, in order to speed up the designer's productivity. To incorporate reconfiguration aspects of IPs, a multiple-context representation of the different functionalities is used that will be mapped on the re-configurable block during different run-time periods. Co-simulation scenario is proposed as a part of a system-on-chip (SoC) design and modelling. SystemC-HDL co-simulation scenario provides a way of checking interoperability of a single designed HW module with the SystemC model. As a case study, novel reconfigurable FFT and Viterbi architectures are modelled in SystemC, and co-simulated in a C-based WiMAX system. Area and power consumption of main blocks in WiMAX are analysed.	co-simulation;embedded system;fast fourier transform;hardware description language;interoperability;simulation;system on a chip;systemc	Ali Ahmadinia;Balal Ahmad;Ahmet T. Erdogan;Tughrul Arslan	2007	2007 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2007.4380762	embedded system;fast fourier transform;parallel computing;real-time computing;computer science;operating system	EDA	4.271193313924106	52.7511055951618	34593
ae8528f028465b6e1e6b707e001d122cf2ff97ff	betweenness centrality revisited on four processors		The betweenness centrality measure has been widely adopted in various graph analytics applications, such as community detection and brain network analysis. Due to the high intensity of BC computation and rapid data growth, there have been a number of studies on parallel BC computation, either on CPUs or GPUs. However, there has not been a comprehensive comparative study on the BC algorithm on different processors. In this paper, we revisit shared-memory parallel BC computation on four kinds of processors, including multi-core CPUs, many-core GPUs, and two generations of Intel MIC processors. We find that, with suitable parallelization strategies and data-oriented optimizations, commodity multi-core CPUs are the fastest, followed by the second generation MIC. These two processors are faster than the state-of-the-art GPU implementations across all kinds of graphs. In comparison, the GPU outperforms the first generation MIC only on small-diameter graphs and is the slowest on the other kinds of graphs.	bcjr algorithm;betweenness centrality;central processing unit;computation;fastest;graphics processing unit;manycore processor;multi-core processor;network theory;parallel computing;second generation multiplex plus;shared memory;xeon phi	Lipeng Wang;Xiaoying Jia;Qiong Luo	2017	2017 IEEE 23rd International Conference on Parallel and Distributed Systems (ICPADS)	10.1109/ICPADS.2017.00085	network analysis;implementation;computation;xeon phi;distributed computing;computer science;graph;betweenness centrality;analytics	HPC	-2.8428719148684114	42.887748925115304	34599
e363a0cf086dbfc396af688e5882750383961408	over the horizon	cmos technology logic testing cable tv optical fiber cables optical fibers cmos integrated circuits buffer storage charge coupled devices cmos logic circuits logic arrays;cmos integrated circuits;logic arrays;cable tv;technical committee;cmos technology;buffer storage;charge coupled devices;optical fibers;optical fiber cables;cmos logic circuits;logic testing;over the horizon	If there was a line of continuity throughout the various sessions of the Computer Elements Technical Committee meeting last June in Vail, Colorado, it was that we are entering a period of technology consolidation. Advances foreseen in the near future are predicted to be extensions of areas of application utilizing existing technologies. The stated need for higher speed or more powerful technologies was only obvious at this meeting by its absence.	scott continuity;semiconductor consolidation	Clark Adams	1976	Computer	10.1109/C-M.1976.218496	logic gate;logic family;telecommunications;computer science;pass transistor logic;integrated injection logic;pull-up resistor;cmos;resistor–transistor logic	HCI	12.745368926265243	58.9897175007781	34674
9ed16008f799a596566b64e398fa811589d04b02	xstat: statistical x-filling algorithm for peak capture power reduction in scan tests	electricity and electronics	Excessive power dissipation can cause high voltage droop on the power grid, leading to timing failures. Since test power dissipation is typically higher than functional power, test peak power minimization becomes very important in order to avoid test induced timing failures. Test cubes for large designs are usually dominated by don't care bits, making X-leveraging algorithms promising for test power reduction. In this paper, we show that X-bit statistics can be used to reorder test vectors on scan based architectures realized using toggle-masking flip flops. Based on this, the paper also presents an algorithm namely balanced X-filling that when applied to ITC'99 circuits, reduced the peak capture power by 7.4% on the average and 40.3% in the best case. Additionally XStat improved the running time for Test Vector Ordering and X-filling phases compared to the best known techniques.	algorithm	Satya Trinadh;Seetal Potluri;Shankar Balachandran;Ch. Sobhan Babu;V. Kamakoti	2014	J. Low Power Electronics	10.1166/jolpe.2014.1302	electronic engineering;real-time computing;computer science;engineering;electrical engineering;test compression	EDA	20.28990105944408	54.414165513734666	34716
7cc5036bf475bf352ce765b0d2b33d8bf01a74da	direct solution of large, sparse systems of equations on a distributed memory computer	distributed memory;system of equations		distributed memory	William Dearholt;Steven Castillo;Derek Barnes	2000			parallel computing;computer science;system of linear equations;distributed computing;distributed memory;theoretical computer science	HPC	-3.370261376955137	37.938877423120836	34736
2e9cf7564e15c3112960b8acf19f1c7c5155e05f	alu synthesis from hdl descriptions to optimized multi-level logic	digital arithmetic;logic cad;alu synthesis;hdl descriptions;arithmetic and logic unit;bit-slices;optimized multi-level logic;random logic	We developed a new tool for automatic ALU synthesis that combines the translation from an HDL to logic level and subsequent multi-level logic synthesis. The existing tools treat AL Us as random logic, i.e., they neglect the regularity of AL Us. Therefore these tools do not achieve good results for AL Us. In contrast, our tool partitions the ALU into blocks such as bit-slices, just like in manual designs. Comparisons with existing tools show significant improvements.	algorithm;arithmetic logic unit;binary decoder;charge-coupled device;chen–ho encoding;entity–relationship model;error detection and correction;hardware description language;international conference on computer-aided design;literal (mathematical logic);logic level;logic optimization;logic synthesis;network synthesis filters;program optimization;random logic;reed–muller code	Frank Buijs	1992			embedded system;computer architecture;logic synthesis;logic optimization;logic gate;logic family;computer science;theoretical computer science;arithmetic logic unit;sequential logic;digital electronics;register-transfer level;algorithm;74181	Logic	18.955140967124613	46.5415060192864	34755
53fe0546291642acb374c699fa58424ad81d4ab3	implementing real-time algorithms by using the aaa prototyping methodology	computer aided design;real time;signal and image processing;performance prediction	This paper presents a system-level methodology (AAA) for signal and image processing algorithms onto circuit architecture. This AAA (Algorithm Architecture Adequation) methodology is added and implemented in an existing software dedicated to the fast prototyping and optimization of real-time embedded algorithms onto multicomponent architectures. The resulting tool, called SynDExIC, is a free graphical Computer-Aided Design (CAD) software. It supports different features: algorithm and architecture specifications, data path and control path synthesis, performances prediction, optimizations and RTL generation. Introduction Digital signal processing applications, including image processing algorithms, require growing computational power especially when they are executed under real-time constraints. This power may be achieved by the high performance of multicomponent architectures based on programmable components (processors) which offer flexibility and non programmable components (reconfigurable circuits) which offer higher performances with less flexibilities. Consequently, there is a need for dedicated high level design methodology, associated to efficient software environments to help the real-time application designer to solve the specification, validation optimization and synthesis problems. Several research efforts have addressed the issue of design space exploration and performance analysis of the embedded systems. Therefore, some methodologies and tools have been developed to help designers for the implementation process. Among these different researches for multicomponent designs, one may cite the SPADE methodology [LWVD01] and the CODEF tool [ACCG01]. Although these tools are very efficient, none of them is able to bring together under real-time and resources constraints: unified models, graphical specification, performances prediction, generation of distributed and optimized executives for programmable part and generation of optimized RTL 2 code for configurable part. Actually, the tool associated to AAA methodology [GS03] is named SynDEx. It supports mainly multiprocessor architectures but does not allow to address the optimization and VHDL generation process of configurable part. Hence, there is a need to extend the AAA for circuit and to develop an associated tool. The presented work is an intermediate step for tending towards a codesign tool associated to the AAA methodology. There are several tools allowing to automate the optimized hardware implementation for reconfigurable circuits from a high-level design specification. One may cite Esterel Studio tool, where the user captures a design specification and then automatically generates the hardware description in HDL-RTL. Another highlevel synthesis framework, SPARK [GDGN03], provides a number of code transformations techniques. SPARK takes behavioral C code as input and generates Register Transfer Logic (RTL) code. This RTL code may be synthesized and mapped onto an FPGA. One may also cite Simulink which is an extension of MATLAB that allows to create algorithm in a graphical fashion. This allows the engineer to visually follow algorithms without getting lost in low level code. Simulink uses a System Generator which takes this graphical algorithmic approach and extends it to FPGA development by using special Simulink blocks. However, none of them is integrated or interfaced with a codesign tool for the multicomponent implementations. The remainder of this paper is centered on the AAA/SynDEx-IC and the implementation of image processing applications by using SynDEx-IC tool. In Section 1, the transformation flow used by AAA methodology is introduced. In Section 2 the software tool SynDEx-IC which implements the AAA methodology for circuits is presented. Section 3 introduces the implementation of image processing onto an FPGA. Finally, Section 4 concludes and discusses the future work. 1. AAA Methodology for integrated circuits AAA is supported by SynDEx tool which is based on dedicated heuristics for the distribution and scheduling of a given algorithm onto programmable components. SynDEx uses graph theory in order to model multiprocessor architectures, applicative algorithms, the optimization and code generation. We will extend the AAA methodology for integrated circuits. In the case where the implementation does not satisfy the constraints specified by the user, we apply an optimization process in order to reduce the latency by increasing the number of circuit resources used. Algorithm specification The algorithm is modelled by an oriented hyper-graph Gal of operations (graph vertices O), its execution is partially ordered by their data-dependences (oriented graph edges D with D,Gal = (O,D)). In the algorithm graph, each Implementing real-time algorithms by using the AAA prototyping methodology 3	aaa (video game industry);algorithm;applicative programming language;central processing unit;code generation (compiler);computation;computer-aided design;data-flow analysis;dataflow;design space exploration;digital signal processing;embedded system;esterel;field-programmable gate array;graph theory;graphical user interface;hardware description language;heuristic (computer science);high- and low-level;high-level programming language;image processing;integrated circuit;level design;logic synthesis;matlab;mathematical optimization;microsoft outlook for mac;multiprocessing;orientation (graph theory);partition problem;performance;point of view (computer hardware company);profiling (computer programming);programming tool;real-time clock;real-time computing;real-time locating system;real-time transcription;reconfigurable computing;register-transfer level;spark;scheduling (computing);simulink;vhdl	Pierre Niang;Thierry Grandpierre;Mohamed Akil	2007		10.1007/978-0-387-72258-0_3	computer architecture;simulation;computer science;computer engineering	EDA	3.171300927865508	51.95167900165124	34763
1d5e737fe4170c5306a926a8f1bfa2c072b79de5	reconfigurable multithreading architectures: a survey	conceptual model;reconfigurable architecture;multithreaded architecture	This paper provides a survey on the existing proposals in the field of reconfigurable multithreading (ρMT) architectures. Until now, the reconfigurable architectures have been classified according to implementation or architectural criteria, but never based on their ρMT capabilities. More specifically, we identify reconfigurable architectures that provide implicit, explicit or no architectural support for ρMT. For each of the proposals, we discuss the conceptual model, the limitations and the typical application domains. We also summarize the main design problems and identify some key research questions related to highly efficient ρMT support. In addition, we discuss the application prospectives and propose possible research directions for future investigations.	application domain;computer architecture;field-programmable gate array;foreach loop;multithreading (computer architecture);open research;reconfigurable computing;scheduling (computing);simultaneous multithreading;taxonomy (general);thread (computing)	Pavel G. Zaykov;Georgi Kuzmanov;Georgi Gaydadjiev	2009		10.1007/978-3-642-03138-0_29	embedded system;computer architecture;parallel computing;real-time computing;computer science;conceptual model	EDA	-1.166304054996035	48.89671451954847	34786
e1668b8309c8004c45e4ab831591a9d400fbc77b	a circuit implementation method for memristor crossbar with on-chip training		This paper studies circuit realization of artificial neural network (ANN) implemented in a memristor crossbar. We propose an on-chip training circuit by adding an extra memristor column for the storage of negative gradient. With proper control circuit, the proposed circuit can perform both backward and forward signal propagations. For demonstrate we have written a Verilog-AMS based implementation of multi-layer perceptron to validate the proposed design. In addition to reporting simulation results, we also point out the AMS-based simulation bottleneck that requires further investigation.		Yonglei Zhao;Guoyong Shi	2018	2018 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)	10.1109/APCCAS.2018.8605612	chip;computer science;memristor;system on a chip;perceptron;electronic engineering;crossbar switch;artificial neural network;bottleneck	EDA	4.300695544500665	42.98492563135551	34795
3b316807b5468ffe12ee35322a262639ea83e933	piparazzi: a test program generator for micro-architecture flow verification	automatic test program generator microarchitecture flow verification piparazzi test generator microprocessors constraint satisfaction problem functional bugs performance bugs functional verification piparazzi tool model based scheme risc architecture cisc architecture;reduced instruction set computing automatic test pattern generation software tools automatic test software microprocessor chips constraint theory hardware software codesign;testing automatic programming microprocessors modems computer bugs random number generation concrete engines investments cost function;hardware software codesign;automatic test pattern generation;reduced instruction set computing;program generation;automatic test software;constraint theory;test generation;software tools;constraint satisfaction problem;microprocessor chips	Because of their complexity, modern microprocessors need new tools that generate tests for micro-architectural events. Piparazzi is a test generator, developed at IBM, that generates (architectural) test programs for microarchitectural events. Piparazzi uses a declarative model of the micro-architecture and the user's definition of the required event to create an instance of a Constraint Satisfaction Problem (CSP). It then uses a dedicated CSP solver to generate a test program that covers the specific event. We show how Piparazzi yields significant improvements in covering micro-architectural events, by describing its technology and by exhibiting experimental results. Piparazzi has already been successful in finding both functional and performance bugs that could only be discovered using an exact micro-architectural model of the processor.	branch predictor;central processing unit;constraint satisfaction problem;declarative programming;microarchitecture;microprocessor;modeling language;software bug;solver	Allon Adir;Eyal Bin;Ofer Peled;Avi Ziv	2003	Eighth IEEE International High-Level Design Validation and Test Workshop	10.1109/HLDVT.2003.1252470	embedded system;reduced instruction set computing;computer architecture;parallel computing;test data generation;real-time computing;computer science;automatic test pattern generation;operating system;programming language;test case;constraint satisfaction problem;test management approach;test harness;random test generator	EDA	7.209645049249674	52.81530615480739	34803
7b82927b7e50f9f4344e4cc3574737c0060f4ed1	combining multiplication methods with optimized processing sequence for polynomial multiplier in gf(2 k )	ecc;hardware implementation;polynomial multiplication	In this paper we present an approach for optimizing the implementation of hardware multipliers in GF(2k). We investigate two different strategies namely the reduction of the complexity of the multiplication methods and the combination of different multiplication methods as a means to reduce the area and/or energy consumption of the hardware multiplier. As a means to explore the design space concerning the segmentation of the operands and the selection of the most appropriate multiplication methods we introduce an algorithm which determines the best combination of the multiplication methods. In order to assess the validity of our approach we have benchmarked it against theoretical results reconstructed from literature and against synthesis results using our inhouse 130 nm technology. The former revealed that our designs are up to 32 per cent smaller than those given in literature, the latter showed that our area prediction is extremely accurate.	polynomial	Zoya Dyka;Peter Langendörfer;Frank Vater	2011		10.1007/978-3-642-34159-5_10	arithmetic;multiplication algorithm;theoretical computer science;mathematics;algorithm	Theory	10.619378000232922	43.994076444297576	34811
87c91b92a9c18c595c1c82959f561e829b010e60	realization of arbitrary logic functions by completely monotonic functions and its applications to threshold logic	completely monotonic functions;index terms augmented variable completely monotonic functions expansion diagram mutual monotonicity synthesis of arbitrary logic functions threshold functions;expansion diagram;index terms augmented variable;index terms 8212;indexing terms;threshold logic;augmented variable completely monotonic functions expansion diagram mutual monotonicity synthesis of arbitrary logic functions threshold functions;synthesis of arbitrary logic functions;mutual monotonicity;threshold functions;completely monotonic function	Abstract—The concept of mutual monotonicity developed in the previous paper[1] is applied to the compound synthesis of an arbitrary logic function by a combination of completely monotonic functions or in most cases by threshold functions.	boolean algebra	Shuzo Yajima;Toshihide Ibaraki	1968	IEEE Transactions on Computers	10.1109/TC.1968.229393	combinatorics;mathematical analysis;discrete mathematics;index term;computer science;mathematics	Vision	19.78148827684042	44.66929584305169	34831
20df3937afcfd72543fd4eb20a8461bd051e4ccc	exploring the viability of the cell broadband engine for bioinformatics applications	high performance computing;accelerators;cell processor;cell broadband engine;general purpose processor;memory access;high performance computer;parallel computer;code size;computational biology;bioinformatics	This paper evaluates the performance of bioinformatics applications on the Cell Broadband Engine recently developed at IBM. In particular we focus on two highly popular bioinformatics applications - FASTA and ClustalW. The characteristics of these bioinformatics applications, such as small critical time-consuming code size, regular memory accesses, existing vectorized code and embarrassingly parallel computation, make them uniquely suitable for the Cell processing platform. The price and power advantages afforded by the Cell processor also make it an attractive alternative to general purpose processors. We report preliminary performance results for these applications, and contrast these results with the state-of-the-art hardware.	bioinformatics;cell (microprocessor);central processing unit;clustalw/clustalx;computation;computational biology;data dependency;docking (molecular);embarrassingly parallel;fasta;hmmer;hidden markov model;interference (communication);kernel (operating system);macromolecular docking;mathematical optimization;medical imaging;parallel computing;sequence alignment;smith–waterman algorithm;superscalar processor;working set size	Vipin Sachdeva;Michael Kistler;William Evan Speight;Tzy-Hwa Kathy Tzeng	2007	2007 IEEE International Parallel and Distributed Processing Symposium	10.1016/j.parco.2008.04.001	supercomputer;parallel computing;computer hardware;computer science;theoretical computer science;operating system	HPC	-0.4890955045938945	43.49433894874673	34839
fae44ce01498056269d6d1e24f5c36fe77fa95b3	using multi-core hw/sw co-design architecture for accelerating k-means clustering algorithm		The capability of classifying and clustering a desired set of data is an essential part of building knowledge from data. However, as the size and dimensionality of input data increases, the run-time for such clustering algorithms is expected to grow superlinearly, making it a big challenge when dealing with BigData. K-mean clustering is an essential tool for many big data applications including data mining, predictive analysis, forecasting studies, and machine learning. However, due to large size (volume) of Big-Data, and large dimensionality of its data points, even the application of a simple k-mean clustering may become extremely time and resource demanding. Specially when it is necessary to have a fast and modular dataset analysis flow. In this paper, we demonstrate that using a two-level filtering algorithm based on binary kd-tree structure is able to decrease the time of convergence in K-means algorithm for large datasets. The two-level filtering algorithm based on binary kd-tree structure evolves the SW to naturally divide the classification into smaller data sets, based on the number of available cores and size of logic available in a target FPGA. The empirical result on this two-level structure over multi-core FPGA-based architecture provides 330× speed-up compared to a conventional software-only solution.	algorithm;big data;cluster analysis;computation;data mining;data point;direct memory access;field-programmable gate array;k-means clustering;level structure;machine learning;mathematical optimization;multi-core processor;pci express;parallel computing;shattered world;speedup;time complexity;transceiver;tree structure	Hadi Mardani Kamali	2018	CoRR		architecture;computer science;distributed computing;big data;multi-core processor;machine learning;curse of dimensionality;data point;k-means clustering;cluster analysis;artificial intelligence;data set	ML	2.0196844950877093	42.2177726171408	34844
9cb39e04023317e586f0d4074e482e2d823560fc	sense of direction: definitions, properties, and classes		An extensive body of evidence exists of the impact that specific edge labelings have on the communication complexity of distributed problems. It has been long suspected that these very different labelings share a common property, named sense of direction. In spite of the large number of investigations, and of the obvious practical importance, a formal characterization of this property did not exist. In this paper, we finally provide a formal definition of sense of direction, making explicit the very specific relationship between three factors: the labeling, the topological structure, and the local view that an entity has of the system. In a way, sense of direction is the capability of a node in the system to use the labeling to translate the local view of its neighbors into its own. Using the formal definition as an observational platform, we describe several properties which allow the translation process to be possible beyond the immediate neighborhood. Finally, we identify four general classes of labelings and analyze their properties; these classes include all the labelings used in the literature. q 1998 John Wiley & Sons, Inc. Networks 32:	communication complexity;john d. wiley;sense	Paola Flocchini;Bernard Mans;Nicola Santoro	1998	Networks	10.1002/(SICI)1097-0037(199810)32:3%3C165::AID-NET1%3E3.0.CO;2-I	translation;combinatorics;definition;computer science;artificial intelligence;graph theory;mathematics;property;algorithm	Theory	17.388059065037744	35.47427258975005	34904
6e76597721cfc648b4358dcffb8e766ed8fe6afc	functional test generation based on combined random and deterministic search methods	functional testing;search method;adjacent stimuli;functional test generation;generating function;random search	The aim of this paper is to explore some features of the functional test generation problem, and on the basis of the gained experience, to propose a practical method for functional test generation. In the paper presented analysis of random search methods and adjacent stimuli generation allowed formulating a practical method for generating functional tests. This method incorporates the analyzed termination conditions of generation, exploits the advantages of random and deterministic search, as well as the feature that the sets of the selected input stimuli can be merged easily in order to obtain a better set of test patterns.	black box;deterministic algorithm;expectation–maximization algorithm;functional testing;heuristic;iterative method;random search;search algorithm;simulation;test card;test design	Eduardas Bareisa;Vacius Jusas;Kestutis Motiejunas;Rimantas Seinauskas	2007	Informatica, Lith. Acad. Sci.		mathematical optimization;generating function;random search;theoretical computer science;functional testing;mathematics;algorithm	AI	20.62789679499798	50.303256685748494	34972
651ad2cadd7267bae54c03b531ddf9bb1c301802	hardware efficient, deterministic qcac matrix based compressed sensing encoder architecture for wireless neural recording application		Wireless neural recording technologies are severely constrained by the limited energy efficiencies and telemetry bandwidth, while data compression or feature extraction techniques can be utilized to relax the burdens on the wireless data link. Compressed Sensing (CS) is an emerging approach for efficient data compression in wireless sensing applications. However, state-of-the-art CS encoder designs still lead to large area and energy overheads. This paper presents a novel CS encoder hardware design by incorporating deterministic measurement matrix, namely Quasi-Cyclic Array Code (QCAC) matrix, to improve overall area and power metrics over prior arts, while still preserving comparable signal recovery performance based on classic reconstruction algorithms. We demonstrate the advantages of the proposed QCAC-CS encoder design for spike data compression in neural recording application. Compared to the state-of-the-art CS encoder designs, QCAC-based CS encoder achieves on average (with compression ratio ranging from 0.0625 to 0.25) 42.7% and 49.5% reduction in encoder area and total power consumption, respectively. And the compressed spikes from the QCAC-CS encoder can be recovered with comparable performance toward random matrix based CS encoder designs.	algorithm;compressed sensing;data compression;detection theory;encoder;feature extraction;standard cell	Wenfeng Zhao;Biao Sun;Tong Wu;Zhi Yang	2016	2016 IEEE Biomedical Circuits and Systems Conference (BioCAS)	10.1109/BioCAS.2016.7833769	embedded system;electronic engineering;real-time computing;computer science	EDA	3.6553266942533824	41.80505339837928	34973
fc22cdfcf4b9d85c9c0cc07522b0390c8a7455ac	a hierarchical approach to clock routing in high performance systems	integrated circuit layout;mcm clock routing high performance systems hierarchical routing scheme path balanced clock tree minimal total wirelength hierarchical design large vlsi circuits;network routing;multichip modules;digital integrated circuits;vlsi;circuit layout cad;hierarchical design;digital circuits;digital integrated circuits network routing vlsi multichip modules circuit layout cad integrated circuit layout timing digital circuits;high performance;clocks routing circuits very large scale integration timing frequency parasitic capacitance delay costs partitioning algorithms;timing	In this paper, we present an hierarchical clock routing scheme, which minimizes the longest source to sink path, and obtains a path balanced clock tree with minimal total wirelength. Our scheme takes into consideration, the hierarchical design of a circuit. Our approach is applicable to large VLSI circuits and MCM's. The algorithm has been implemented and experimental results are encouraging. >	routing	Wasim Khan;Sreekrishna Madhwapathy;Naveed A. Sherwani	1994		10.1109/ISCAS.1994.408839	routing;electronic engineering;parallel computing;real-time computing;asynchronous circuit;ic layout editor;computer science;engineering;electrical engineering;design layout record;integrated circuit layout;very-large-scale integration;digital electronics;static timing analysis;routing;clock signal	HPC	15.378552556991293	52.07988635452539	35030
0bb7cf2351bc362bf24a800eff96de7e71683ba2	a very efficient algorithm for correction of skewed documents		This paper proposes a fast and efficient algorithm for rotating binary images. As compared to conventional algorithms, the proposed one saves around 90% of memory consumption. Moreover, it requires no floating-point multiplication, and the major operations in it are just memory access and simple integer addition. These make it very suitable for correction of skewed document images.	algorithm;binary image	Steven Sheung-On Choy;Yuk-Hee Chan;Shiu-Wing Hui;Wan-Chi Siu	2000	2000 10th European Signal Processing Conference		computer hardware;computer science;theoretical computer science;algorithm	HPC	12.140147146329687	36.731761961847184	35062
6d0058d14ce82059a41b8c0e6ae1c6251870b44a	bayesian virtual probe: minimizing variation characterization cost for nanoscale ic technologies via bayesian inference	silicon;belief networks;process variation;error reduction;integrated circuit;silicon measurement data bayesian virtual probe variation characterization cost nanoscale ic testing bayesian inference nanoscale manufacturing process bvp method information theory statistics sampling locations optimal set test structures spatial variations industrial examples;bayesian inference;spatial variation;bayesian methods;semiconductor device measurement;inference mechanisms;probes;integrated circuit design;accuracy;variation characterization process variation integrated circuit;integrated circuit testing;bayesian methods probes costs testing manufacturing processes pulp manufacturing information theory statistical analysis sampling methods particle measurements;variation characterization;entropy;electronic engineering computing;efficiency measurement;integrated circuit testing belief networks electronic engineering computing inference mechanisms integrated circuit design;information theory;covariance matrix	The expensive cost of testing and characterizing parametric variations is one of the most critical issues for today's nanoscale manufacturing process. In this paper, we propose a new technique, referred to as Bayesian Virtual Probe (BVP), to efficiently measure, characterize and monitor spatial variations posed by manufacturing uncertainties. In particular, the proposed BVP method borrows the idea of Bayesian inference and information theory from statistics to determine an optimal set of sampling locations where test structures should be deployed and measured to monitor spatial variations with maximum accuracy. Our industrial examples with silicon measurement data demonstrate that the proposed BVP method offers superior accuracy (1.5x error reduction) over the VP approach that was recently developed in [12].	compressed sensing;digital image processing;fourier analysis;information theory;jim hall (programmer);machine learning;matrix analysis;pattern recognition;sampling (signal processing);selection algorithm;signal processing;signal reconstruction;sparse matrix;vp/css	Wangyang Zhang;Xin Li;Rob A. Rutenbar	2010	Design Automation Conference	10.1145/1837274.1837342	spatial variability;econometrics;entropy;covariance matrix;mathematical optimization;electronic engineering;information theory;bayesian probability;computer science;integrated circuit;accuracy and precision;silicon;process variation;bayesian inference;statistics;integrated circuit design	EDA	23.088114392218955	59.59788978967883	35063
afd091bb6befa00b1e43a5db286ec8c47ca59d77	automatic adl-based operand isolation for embedded processors	power efficiency;design complexitiy;low power optimization technique;design time;electronic system level design;low power consumption;well-known power optimization technique;power optimization;operand isolation;embedded processor;significant power reduction;application software;system on chip;application specific instruction set processor;high level synthesis;registers;architecture description language;power generation;electronic system level;data mining;low power electronics;automation;register transfer level;embedded systems;hardware description languages;instruction sets;embedded system	Cutting-edge applications of future embedded systems demand highest processor performance with low power consumption to get acceptable battery-life times. Therefore, low power optimization techniques are strongly applied during the development of modern application specific instruction set processors (ASIPs). Electronic system level design tools based on architecture description languages (ADL) offer a significant reduction in design time and effort by automatically generating the software tool-suite as well as the register transfer level (RTL) description of the processor. In this paper, the automation of power optimization in ADL-based RTL generation is addressed. Operand isolation is a well-known power optimization technique applicable at all stages of processor development. With increasing design complexity several efforts have been undertaken to automate operand isolation. In pipelined datapaths, where isolating signals are often implicitly available, the traditional RTL-based approach introduces unnecessary overhead. We propose an approach which extracts high-level structural information from the ADL representation and systematically uses the available control signals. Our experiments with state-of-the-art embedded processors show a significant power reduction (improvement in power efficiency)	architecture description language;central processing unit;datapath;electronic system-level design and verification;embedded system;experiment;high- and low-level;level design;mathematical optimization;operand isolation;overhead (computing);performance per watt;pipeline (computing);power optimization (eda);programming tool;register-transfer level	Anupam Chattopadhyay;B. Geukes;David Kammler;Ernst Martin Witte;Oliver Schliebusch;Harold Ishebabi;Rainer Leupers;Gerd Ascheid;Heinrich Meyr	2006	Proceedings of the Design Automation & Test in Europe Conference		system on a chip;electricity generation;embedded system;architecture description language;computer architecture;application software;parallel computing;real-time computing;electrical efficiency;computer science;operating system;automation;instruction set;processor register;hardware description language;high-level synthesis;power optimization;register-transfer level;low-power electronics	EDA	2.829851791167528	54.339414579960355	35065
82657b0ef063ece21f8f806d1d319607e0885235	modelling and exploration of a reconfigurable array using systemc tlm	filter algorithm;protocols;filter algorithm reconfigurable array systemc tlm reconfigurable architecture memory cells processing cells instruction set processor communication intensive inner loops processor communication self synchronizing protocol;elektroteknik och elektronik;transaction level model;reconfigurable architectures cellular arrays instruction sets protocols;reconfigurable array;reconfigurable architectures;memory cells;cellular arrays;instruction set processor;reconfigurable architecture;communication intensive inner loops;reconfigurable architectures random access memory computational modeling delay information technology protocols communication system control data mining filters global communication;systemc tlm;self synchronizing protocol;coarse grained;simulation model;processing cells;instruction sets;processor communication	This paper presents a coarse-grained reconfigurable architecture based on an array of processing and memory cells. Memory cells are distributed and placed close to processing cells to reduce memory bottlenecks. Processing cells are instruction set processors with enhanced performance for communication-intensive inner loops. Processor communication is performed using a self-synchronizing protocol that simplifies algorithm mapping and manages unpredictable time variations. The reconfigurable architecture is described as a scalable and parameterizable SystemC transaction level model, which allows rapid architectural exploration. Our exploration environment SCENIC is used to setup scenarios, control the simulation models and to extract performance data during simulation. A case study demonstrates different implementations of a filter algorithm, and how exploration is used to tune and optimize for execution time, latency, or used resources.	cartography;central processing unit;flash memory;grams;peterson's algorithm;reconfigurable computing;run time (program lifecycle phase);scalability;signal processing;simulation;systemc	Henrik Svensson;Thomas Lenart;Viktor Öwall	2008	2008 IEEE International Symposium on Parallel and Distributed Processing	10.1109/IPDPS.2008.4536521	embedded system;communications protocol;computer architecture;parallel computing;real-time computing;computer science;operating system;simulation modeling;instruction set	Arch	4.184737297704281	47.18912321576877	35130
1b25ada810edda1c92301ca81118dcc2bdf86837	genetic mapping of hard real-time applications onto noc-based mpsocs — a first approach	analytical models;topology;network on chip;multiprocessing;communication flows;routing;genetic map;communication flows genetic mapping hard real time applications noc based mpsoc network on chip multicore processors embedded systems industry;interference;schedulability analysis;genetic mapping;genetic algorithms analytical models real time systems routing topology genetics interference;embedded system;genetics;embedded systems industry;ranking function;network on chip embedded systems multiprocessing systems;embedded systems;hard real time applications;task mapping;noc based mpsoc;genetic algorithm;genetic algorithms;multicore processors;multiprocessing systems;task mapping network on chip multiprocessing;analytical model;hard real time;real time systems	Despite its significance to embedded systems industry and research communities, little research has been done on providing guarantees for hard real-time applications running over multicore processors based on wormhole Networks-on-Chip (NoCs). This work takes advantage of recent work on schedulability analysis that is tailored to such platforms, and uses it as a ranking function in a genetic algorithm that is able to evolve task mappings which allow all tasks and communication flows to meet their deadlines in all possible scenarios.	central processing unit;embedded system;evolutionary algorithm;genetic algorithm;interference (communication);multi-core processor;network on a chip;ranking (information retrieval);real-time clock;real-time computing;scheduling analysis real-time systems;synthetic data;system on a chip;traffic exchange	Paris Mesidis;Leandro Soares Indrusiak	2011	6th International Workshop on Reconfigurable Communication-Centric Systems-on-Chip (ReCoSoC)	10.1109/ReCoSoC.2011.5981532	embedded system;parallel computing;real-time computing;genetic algorithm;computer science;distributed computing;network on a chip	Embedded	1.478989485163973	60.012184265394005	35164
75d6d492369b19db4f3cc3ad1ce3a10d4b5d71cc	efficient fixed-size systolic arrays for the modular multiplication	red sistolica;processor scheduling;systolic array;optimisation combinatoire;computer architecture;architecture ordinateur;systolic network;informatique theorique;reseau systolique;procesador oleoducto;arquitectura ordenador;ordonnancement processeur;processeur pipeline;combinatorial optimization;modular multiplication;pipeline processor;optimizacion combinatoria;computer theory;informatica teorica	In this paper, we present an efficient fixed-size systolic array for Montgomery's modular multiplication. The array is designed by the LPGS (Locally Parallel Globally Sequential) partition method [14] and can perform efficiently modular multiplication for the input data with arbitrary bits. Also, we address a computation pipelining technique, which improves the throughput and minimizes the buffer size used. With the analysis of VHDL simulation, we discuss a gap between a theoretical optimal number of partition and an empirical one.		Sung-Woo Lee;Hyun-Sung Kim;Jung-Joon Kim;Tae-Geun Kim;Kee-Young Yoo	1999		10.1007/3-540-48686-0_44	mathematical optimization;modular arithmetic;computer architecture;parallel computing;systolic array;combinatorial optimization;computer science;theoretical computer science;operating system;mathematics	Crypto	11.835743567017756	34.26221319962948	35165
ff821c7c3b7b345da289ac2ea2fe6fe63d67450c	fpga implementation of hs1-siv		This work describes a hardware implementation of HS1-SIV with regular cipher parameter settings for the second round of the CAESAR competition. The implementation encompasses both the HS1-SIV hardware implementation, which is conforming to the specifications of the authenticated cipher, as well as a hardware API. The implemented API is conforming to the specifications of the GMU Hardware API for authenticated ciphers. On the target device Xilinx Virtex-7, using Xilinx XST High Level Synthesis, we achieved a throughput of 122.20 Mbit/s and an area of 103,214 LUTs with the data length of the message and the associated data set at 64 bytes and the data length of the key set at 32 bytes. Our performance results suggest that the area overhead of the API is between 8% (8-byte data length) and 15% (2048-byte data length) in comparison the the cipher-core.	application programming interface;authentication;byte;caesar;cipher;field-programmable gate array;high-level synthesis;megabit;overhead (computing);throughput	Gerben Geltink;Sergei Volokitin	2016		10.5220/0005950100410048	fpga prototype	Arch	8.438619669396523	45.73157096290788	35178
a7432dfa528ff633131c30530e183f887557054d	globally constrained locally optimized 3-d power delivery networks	bridge power lines 3d power delivery networks 3d pdn constrained optimization problem power grid optimization solution current density constraints electromigration limit heat flux 3d integrated circuits through silicon via tsv;power delivery networks pdns algorithms circuit analysis optimization;thermal analysis;power supply noise;integrated circuit modeling heat sinks metals capacitance analytical models resistance temperature measurement;power delivery;3d ics;three dimensional integrated circuits circuit optimisation current density electromigration integrated circuit interconnections;through silicon vias;three dimensional integration	Design of power delivery network (PDN) is a constrained optimization problem. An ideal PDN must limit voltage drop that results from switching circuits' transients, satisfy current density constraints that arise from electromigration limits, yet use only minimal metal resources so that design density targets can be met. It should also provide an efficient thermal conduit to address heat flux. Furthermore, an ideal PDN should be a regular structure to facilitate design productivity and manufacturability, yet be resilient to address varying power demands across its distribution area. In 3-D ICs, these problems are further constrained by the need to minimize through-silicon via (TSV) area and bridge power lines of different dimensions across tiers, while addressing varying power demands in lateral and vertical directions. In this paper, we propose an unconventional power grid optimization solution that allows us to resize each tier individually by applying tier-specific constraints and yet be optimal in a multitier network, where each tier is locally resized while globally constrained. Tier-specific constraints are derived from electrical and thermal targets of 3-D PDNs. Two resizing algorithms are presented that optimize 3-D PDNs standalone or 3-D PDNs together with TSVs. We demonstrate these solutions on a three-tier setup where significant area savings can be achieved.	algorithm;constrained optimization;constraint (mathematics);design for manufacturability;electromigration;lateral thinking;mathematical optimization;multitier architecture;optimization problem;through-silicon via;upsizing (database)	Aida Todri;Sandip Kundu;Patrick Girard;Alberto Bosio;Luigi Dilillo;Arnaud Virazel	2014	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2013.2283800	electronic engineering;engineering;electrical engineering;engineering drawing;thermal analysis	EDA	16.583779621855008	53.853423785550305	35206
74e9167a9500c6f5502c84756b14754e4fa48cb9	temporal planning for compilation of quantum approximate optimization circuits		We investigate the application of temporal planners to the problem of compiling quantum circuits to emerging quantum hardware. While our approach is general, we focus our initial experiments on Quantum Approximate Optimization Algorithm (QAOA) circuits that have few ordering constraints and thus allow highly parallel plans. We report on experiments using several temporal planners to compile circuits of various sizes to a realistic hardware architecture. This early empirical evaluation suggests that temporal planning is a viable approach to quantum circuit compilation.	approximation algorithm;compiler;experiment;quantum circuit	Davide Venturelli;Minh Do;Eleanor G. Rieffel;Jeremy Frank	2017		10.24963/ijcai.2017/620	machine learning;artificial intelligence;computer science;electronic circuit;quantum	AI	4.692174445425856	40.31067979392525	35255
320942b45c90b061a41cfe959650c9bed7d701a6	an energy efficient mobile device for assisted living applications	energy conservation;optimisation;frequency 915 mhz location accuracy network communication system uptime power consumption wi fi localization optimization wi fi transceiver emu hardware design emergency call transmission range optimized energy characteristics power management fall detection functionality rss measurement radio signal strength measurement energy efficiency wireless communication indoor localization falling accident elderly assisted living application energy efficient mobile device frequency 1 ghz frequency 433 mhz frequency 868 mhz;telecommunication power management;wireless lan assisted living biomedical communication energy conservation indoor radio mobile radio optimisation power consumption radio transceivers telecommunication power management;assisted living;mobile radio;indoor radio;wireless lan;power consumption;radio transceivers;indoor location system hardware mobile devices energy efficiency wi fi sub 1ghz radio transceivers;biomedical communication	Common problems of many elderly include falling accidents and getting lost while leaving their common surroundings unattended. We developed a mobile device that includes indoor localization based on the energy-efficiency Sub 1GHz wireless communication via radio signal strength (RSS) measurements and fall detection functionality. Power management is a critical part for the success of such a device. The authors suggest the use of Sub 1GHz transceivers (using the 433, 868 or 915 MHz frequency bands) with their optimized energy characteristics and increased transmission ranges for localization and emergency calls in case of a fall. Therefore, we present a hardware design of an energy efficient mobile device, the so-called EMU, that includes Sub 1GHz and Wi-Fi transceivers. An optimization of the Wi-Fi localization increased the uptime from 7 h to 9:25 hours. Further, the utilization of Sub 1GHz for localization has more than doubled the uptime to 21 hours, as shown in two benchmarks (measuring perchip power-consumption and system uptime while performing network communication), while achieving a comparable location accuracy.	frequency band;internationalization and localization;mathematical optimization;mobile device;power management;rss;radio wave;transceiver;uptime	Sebastian J. F. Fudickar;Max Frohberg;Sebastian Taube;Philipp Mahr;Bettina Schnor	2012	2012 IEEE Online Conference on Green Communications (GreenCom)	10.1109/GreenCom.2012.6519629	embedded system;electronic engineering;telecommunications;engineering	Mobile	1.8538040626140708	34.222911452109166	35264
6f4390ea48a86fd3d1c840043cf8726e24669904	designing closer to the edge	model design;on chip bus;process variation;cache;intellectual property;system on a chip;low power;estimation;power dissipation;deep sub micron;modeling and analysis	Modern deep submicron CMOS processes cost $2B or more to develop, qualify and deploy. Yet the incremental impact of each technology generation has been steadily decreasing due to a variety of phenomena such as increasing wire delay, power dissipation and reliability limits, and increasing process tolerances. This increase is portrayed in Figure 1 which shows the SIA Roadmap[1] predictions of variability for five technologies in the 250 to 70nm gate length regime. These observations lead to the conclusion that we need to make better use of existing and future manufacturing processes in order to recoup our investment. When using of an existing process, the designer get the dual benefits of low cost and process maturity at the cost of (a) lower performance and (b) lower integration density. While the lower integration density is somewhat unavoidable, it is often possible to get more performance out of an existing technology by better understanding of the process tolerances and trading off functional yield vs. performance. Given the above, it is clear that we need to understand and model design tolerances arising from processing variations. Until recently, it was sufficient to model such process-induced variations as intra-die shifts in device performance. However, in the deep sub-micron regime, within-die wire and device variations are comparable to die-to-die variations. This results in the need for new characterization, modeling and analysis techniques to handle these variations. To re-enforce these ideas, consider the simple canonical circuit in figure 2 composed of a source buffer driving an identical destination buffer through a length of minimum-width wire. We examine the relative impact of wire and device variability on the delay for various technology generations. Across technologies we maintained the W=L ratio for the buffer and found the maximum wire length beyond which inserting a buffer between the source and destination would lower overall delay[2]: Lmax = p 2( B +RBCB)=RwCw, where B ,RB andCB are the delay, output resistance and input capacitance of the buffer, andRw andCw are per unit length of the wire. Taking the same five technologies in figure 1 and computingLmax. The results are shown in table 1 and figure 2 which explains the wire geometrical parameters. The table shows a super-linear (relative to Leff ) decrease in the length Lmax vs. process generation, which shows and increase in the influence of interconnect. When we extend the analysis to include the impact of the device and wire variations on the delay variations we see that the contributions of of device and wire variability to total delay variability remain fairly constant (table 2) which is important because it means that this canonical circuit is a good gauge to differentiate circuits based on their sensitivity to device and wire variations. If we do the analysis without scaling transistor widths (case (A) in table 2), or scaling wire length at the same rate as Leff (case (B) in table 2) we get very different results. In this tutorial we will expand on the ideas above, review the important trends in design uncertainty which directly drives design tolerance and hence performance. We will review a number of research and applied approaches to design for manufacturability. The need to track process tolerances as a technology matures will be stressed. This tracking is important since it acts as an information conduit between design and fabrication groups and enables designers to adapt the design to lower tolerances where possible. 1 Permission to make digital or hard copies of part or all of this work or personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. DATE 2000, Paris, France © ACM 2000 1-58113-244-1/00/03 ...$5.00 636 5 10 15 20 25 30 35 40 45 50 97 99 01 03 05 07 percent	cmos;cpu power dissipation;capability maturity model;computer performance;design for manufacturability;emoticon;heart rate variability;image scaling;output impedance;service in informatics and analysis;spatial variability;transistor;very-large-scale integration;web design	Sani R. Nassif	2000		10.1145/343647.343877	system on a chip;embedded system;estimation;electronic engineering;real-time computing;cache;computer science;engineering;dissipation;network on a chip;process variation;intellectual property;statistics	EDA	22.1622022827315	57.8682350999569	35313
c368f24e3949cf9951f4a6612066319871c37185	an efficient algorithm for rlc buffer insertion	vlsi buffer circuits circuit optimisation inductance integrated circuit design rlc circuits;dynamic programming;dynamic programming framework;rc buffering;inductance effect;rlc buffer insertion;efficient algorithm;restrictive cost bucketing;buffer insertion algorithms;delay effects;dynamic program;minimization methods;inductance costs delay effects timing algorithm design and analysis dynamic programming circuit optimization runtime minimization methods integrated circuit interconnections;runtime;buffer circuits;efficient delay update;integrated circuit design;van ginneken lillis algorithm;buffer cost minimization;cost minimization;rlc circuits;integrated circuit interconnections;linear time;vlsi;inductance;ismail s algorithm efficient algorithm rlc buffer insertion buffer insertion algorithms inductance effect circuit optimization ultra fast buffering techniques dynamic programming framework restrictive cost bucketing efficient delay update rc buffering van ginneken lillis algorithm buffer cost minimization;circuit optimisation;ultra fast buffering techniques;ismail s algorithm;buffer insertion;algorithm design and analysis;circuit optimization;timing	Traditional buffer insertion algorithms neglect the impact of inductance effect, which often introduces large error in circuit optimization. On the other hand, ultra-fast buffering techniques are always desirable as buffering is such a widely used technique in industry. It is a challenge to design an RLC buffering algorithm which excels in both runtime and solution quality. In this paper, such an algorithm is proposed. The new algorithm works under the dynamic programming framework and runs in provably linear time for multiple buffer types due to two novel techniques: restrictive cost bucketing and efficient delay update. Experiment results on industrial netlists demonstrate that the new algorithm consistently outperforms van Ginneken/Lillis algorithm (van Ginneken, 1990) and (Lillis, 1996) for RC buffering and all known RLC buffering algorithms. Without buffer cost minimization, the new algorithm saves up to 8.5% buffer area and provides up to 4times speedup over Ismail's algorithm (Ismail, 2001), When buffer cost minimization is handled, the new algorithm uses 33.4% fewer buffers than van Ginnenken-Lillis's algorithm, and saves up to 5.3% buffer area and gives up to 5times speedup compared to the algorithm in (Jiang et al., 2006)	algorithm;characteristic impedance;digital electronics;dynamic programming;insertion sort;linear approximation;mathematical optimization;rlc circuit;speedup;time complexity;z-buffering	Zhanyuan Jiang;Shiyan Hu;Jiang Hu;Weiping Shi	2007	8th International Symposium on Quality Electronic Design (ISQED'07)	10.1109/ISQED.2007.33	time complexity;algorithm design;electronic engineering;parallel computing;real-time computing;inductance;computer science;engineering;electrical engineering;dynamic programming;very-large-scale integration;rlc circuit;algorithm;integrated circuit design	EDA	16.795926572917654	53.1566681176938	35315
512e52d9fc5a2c44bef2d0cfbd7f243d1a992360	efficient fault tolerant parallel matrix-vector multiplications	soft errors matrix vector multiplications;system analysis and design;testing;self checking capability fault tolerant parallel matrix vector multiplication parallel matrix processing digital signal processing digital communication system error correction code;parallel processing digital signal processing chips error correction codes fault tolerance matrix multiplication;robustness;field flow fractionation testing robustness system analysis and design;field flow fractionation	Parallel matrix processing is a typical operation in many systems, and in particular matrix-vector multiplication is one of the most common operations in modern digital signal processing and digital communication systems. This paper proposes a fault tolerant design for parallel matrix-vector multiplications. The scheme combines ideas from Error Correction Codes with the self-checking capability of matrix-vector multiplication.	digital signal processing;fault tolerance;matrix multiplication	Zhen Gao;Pedro Reviriego;Juan Antonio Maestro	2016	2016 IEEE 22nd International Symposium on On-Line Testing and Robust System Design (IOLTS)	10.1109/IOLTS.2016.7604665	field flow fractionation;electronic engineering;parallel computing;computer science;engineering;theoretical computer science;software engineering;software testing;structured systems analysis and design method;robustness	Arch	13.961221040448063	45.03344467521992	35329
56752310e112e00c267dd87aa804ae2369732e3a	variation-aware supply voltage assignment for simultaneous power and aging optimization	negative bias temperature instability delay degradation integrated circuit modeling leakage current;degradation;very large scale integration;upper bound;integrated circuit design;leakage power;logic gates;energy saving variation aware supply voltage assignment simultaneous power optimization aging optimization negative bias temperature instability reliability concern circuit designer power consumption reduction statistical platform circuit power minimisation aging aware timing constraint nbti induced circuit delay degradation;integrated circuit modeling;low power electronics;negative bias temperature instability nbti;dynamic power;supply voltage assignment sva dynamic power leakage power negative bias temperature instability nbti;supply voltage assignment sva;integrated circuit reliability;low power electronics integrated circuit design integrated circuit reliability	As technology scales, negative bias temperature instability (NBTI) has become a major reliability concern for circuit designers. And the growing process variations can no longer be ignored. Meanwhile, reducing power consumption remains to be one of the design goals. In this paper, a variation-aware supply voltage assignment (SVA) technique combining dual Vdd assignment and dynamic Vdd scaling is proposed on a statistical platform, to minimize circuit power under an aging-aware timing constraint. The experimental results show that our SVA technique can mitigate on average 62% of the NBTI-induced circuit delay degradation. Compared with guard-banding and single Vdd scaling approaches, our approach saves more energy.	colour banding;elegant degradation;guard digit;image scaling;negative-bias temperature instability	Yu Wang;Yu Cao;Yuchun Ma;Huazhong Yang	2012	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2011.2168433	electronic engineering;real-time computing;degradation;logic gate;engineering;electrical engineering;dynamic demand;very-large-scale integration;upper and lower bounds;low-power electronics;integrated circuit design	EDA	19.738594356902624	58.00939625393033	35372
6a5dd6c933651e63ccc06aef8d6fde0472fbf57e	automatic communication-driven virtual prototyping and design for networked embedded systems	esl design flow;communication refinement;virtual prototyping;systemc tlm	This work presents a communication-driven virtual prototyping approach integrated in an existing ESL design methodology to automatically synthesize, evaluate, and optimize a data-flow application for mixed hardware/software and even networked MPSoCs. While existing synthesis tools are suitable for individual subsystems (e.g., software tasks for CPUs, hardware accelerators), the problem of establishing the communication between different subsystems that may even be simulated at different levels of abstraction is still challenging. As a remedy, we introduce the concept of bridge components in our architecture model that, during virtual prototyping, serve as integrators between subsystems that may have different communication protocols and be simulated at different levels of abstraction (e.g., TLM, behavioral level, RTL). We propose to consider bridges throughout the complete ESL design flow: Already during Design Space Exploration (DSE), the characteristics of bridge components such as implementation cost and additional latency on the application can be taken into account. Moreover, we extend the exploration model of the DSE to include required communication-related design decisions, i.e., the mapping of binary code for software tasks and the selection of different synchronization patterns for the communication. For virtual prototyping of implementation candidates derived by the DSE, the bridge components enable to automatically disassemble the system into subsystems and hand each subsystem over to an individual synthesis tool. When integrating the subsystems together, our methodology also synthesizes the interfaces for all bridges which significantly simplifies system integration. As a proof of concept, we present (I) a distributed control application that is transformed into a virtual prototype consisting of six subsystems and (II) a data-flow application from the video processing domain transformed into a virtual prototype consisting of three subsystems. The resulting subsystems can be concurrently simulated at TLM, behavioral level, and RTL. The experiments give evidence of the proposed technique's applicability, the achieved productivity gain, and the resulting simulation performance at the considered levels of abstraction.	embedded system	Joachim Falk;Tobias Schwarzer;Liyuan Zhang;Michael Glaß;Jürgen Teich	2015	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2015.08.008	embedded system;parallel computing;real-time computing;computer science;operating system;programming language	EDA	3.639927162236521	52.55067541276501	35379
144ce16310f515d988007af4ab553a5c56af63eb	behavioral synthesis techniques for intellectual property protection	ecc behavioral synthesis techniques intellectual property protection dynamic watermarking technique cad tools compilation tools reusable core components reusable core based design paradigm design constraints timing constraints author signature encoding metrics signature data hiding scheduling assignment allocation transformations error correcting codes;watermarking;data hiding;error correction codes;intellectual property protection design automation hardware watermarking permission timing data encapsulation job shop scheduling productivity;behavioral synthesis;intellectual property;intellectual property protection;field programmable gate array fpga;data encapsulation;high level synthesis;error correction code;quantitative analysis;circuit cad;industrial property error correction codes circuit cad high level synthesis timing data encapsulation;industrial property;timing;time constraint	The economic viability of the reusable core-based design paradigm depends on the development of techniques for intellectual property protection. We introduce the first dynamic watermarking technique for protecting the value of intellectual property of CAD and compilation tools and reusable core components. The essence of the new approach is the addition of a set of design and timing constraints which encodes the author's signature. The constraints are selected in such a way that they result in minimal hardware overhead while embedding the signature which is unique and difficult to detect, remove and forge. We establish the first set of relevant metrics which forms the basis for the quantitative analysis, evaluation, and comparison of watermarking techniques. We develop a generic approach for signature data hiding in designs, which is applicable in conjunction with an arbitrary behavioral synthesis task, such as scheduling assignment, allocation, and transformations. Error correcting codes are used to augment the protection of the signature data from tampering attempts. On a large set of design examples, studies indicate the effectiveness of the new approach in a sense that the signature data, which are highly resilient, difficult to detect and remove, and yet easy to verify, can be embedded in designs with very low hardware overhead.		Inki Hong;Miodrag Potkonjak	1999		10.1145/309847.310085	embedded system;electronic engineering;real-time computing;computer science;theoretical computer science;operating system;computer security;algorithm;intellectual property	EDA	6.4072015052198035	56.82842810136584	35448
1089961e8e5fb8f636a3470f4d9ed89992998155	methodology for predicting off-state reliability in gan power transistors		Results of a lifetest across temperature and drain voltage on off-state high power GaN FET test structures are presented. The times to failure (tf) are fitted to a combination of the Arrhenius model (ln(tf) ∼ inverse temperature) and the linear field model (ln(tf) ∼ drain voltage). The estimated activation energy (Ea) is 2.1 eV and the estimated linear field parameter (γ) is 0.03 V−1. Reliability parameters estimated from the test structure data are used to predict the FIT rate for a product level FET using linear scaling of the gate width. Further, the effect of a burn-in and a transient voltage under a duty cycle on the FIT rate are modeled. The FIT rate of the product level FET is larger than that of the test structure. The burn-in and transient voltage similarly reduce the reliability. Contour plots are given that allow trade-offs between these factors in order to meet reliability requirements.	transistor	Charles S. Whitman	2014	Microelectronics Reliability	10.1016/j.microrel.2013.09.010	reliability engineering;electronic engineering;engineering;electrical engineering	Arch	21.83361251995286	59.62555872671389	35480
896aabe6172d0a6ec6bda8b761c99065bcba1083	monolithic 3d ic designs for low-power deep neural networks targeting speech recognition		In recent years, deep learning has become widespread for various real-world recognition tasks. In addition to recognition accuracy, energy efficiency is another grand challenge to enable local intelligence in edge devices. In this paper, we investigate the adoption of monolithic 3D IC (M3D) technology for deep learning hardware design, using speech recognition as a test vehicle. M3D has recently proven to be one of the leading contenders to address the power, performance and area (PPA) scaling challenges in advanced technology nodes. Our study encompasses the influence of key parameters in DNN hardware implementations towards energy efficiency, including DNN architectural choices, underlying workloads, and tier partitioning choices in M3D. Our post-layout M3D designs, together with hardware-efficient sparse algorithms, produce power savings beyond what can be achieved using conventional 2D ICs. Experimental results show that M3D offers 22.3% iso-performance power saving, convincingly demonstrating its entitlement as a solution for DNN ASICs. We further present architectural guidelines for M3D DNNs to maximize the power saving.	algorithm;application-specific integrated circuit;artificial neural network;deep learning;grand challenges;image scaling;low-power broadcasting;multitier architecture;sparse matrix;speech recognition;three-dimensional integrated circuit	Kyungwook Chang;Deepak Kadetotad;Yu Cao;Jae-sun Seo;Sung Kyu Lim	2017	2017 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)	10.1109/ISLPED.2017.8009175	implementation;edge device;electronic engineering;artificial neural network;real-time computing;computer science;efficient energy use;three-dimensional integrated circuit;deep learning;speech recognition;artificial intelligence	Arch	4.077110615863302	42.46912245223319	35544
eac9779223e939c14b9e6862e15374f37ce234a8	acryp-proc: flexible asymmetric crypto processor for point multiplication		Flexibility is one of the driving agents for 5G architecture development to incorporate enhanced mobile broad band, machine type communication (MTC), and ultra-reliable MTC. While considering flexibility in the domain of security/reliability, we present a unified and flexible hardware architecture, implementing point multiplication algorithm for both elliptic curve cryptography (ECC) and Binary Huff Curves (BHC). The proposed architecture can be used in the scenarios where the users can tradeoff between the algorithmic execution time and different reliability/security levels. To establish an area overhead for the unified design, dedicated architectures for ECC and BHC over the  $GF(2^{m})$  field are implemented in the first step and compared with state-of-the-art. Then, a unified architecture for ECC and BHC is implemented. The performance results of proposed unified architecture illustrate the trade-off between execution time and security level.	elliptic curve cryptography;high-frequency direction finding;multiplication algorithm;overhead (computing);proceedings of the ieee;public-key cryptography;run time (program lifecycle phase)	Malik Imran;Muhammad Rashid;Atif Raza Jafri;Muhammad Najam-ul-Islam	2018	IEEE Access	10.1109/ACCESS.2018.2828319	parallel computing;field-programmable gate array;architecture;throughput;elliptic curve cryptography;computer science;hardware architecture;distributed computing;multiplication;binary number;multiplication algorithm	HPC	8.521695489222544	45.425170367214626	35556
529aec9658107675faaa4ea6ca23499c5105544f	novel dynamic gate topology for superpipelines in dsm technologies	fine grained pipelining;logic design;comunicacion de congreso;delay noise trade off fine grained pipelining dynamic logic;dynamic logic;network topology;delay noise trade off;logic gates delays topology clocks noise transistors pipeline processing;delay circuits;pipeline processing delay circuits integrated circuit noise logic design network topology;kogge stone adder dynamic gate topology superpipelines dsm technologies dynamic logic fine grained pipeline high performance functional units commercial applications frequency targets noise susceptibility logic design functional limitation labor intensive design fine grained pipelines inverting functionalities competitive delay noise tradeoffs function independent delays gate static output stage carry merge chain;integrated circuit noise;pipeline processing	Dynamic logic is well suited to implement very fine-grained pipelining for high performance functional units and has been successfully applied in commercial applications. Technology scaling and current increasing frequency targets have augmented the main problems exhibited by conventional dynamic gates topologies: larger leakage and coupling leading to higher noise susceptibility, logic design constrained by their functional limitation, being able to implement only non inverting functions and the labor-intensive design required due to timing challenges of fine grained pipelines used for high throughput. Development of novel topologies aiming to cope with all these challenges is an area of active research. In this paper, we describe a novel topology that addresses all the above stated problems. The proposed gate implements inverting functionalities, exhibits very competitive delay-noise tradeoffs and it is well suited to implement building blocks with function-independent delays which can simplify design. Unlike previous reported solutions, it is the gate static output stage which is modified. The novel topology is analyzed and evaluated, and the Carry-Merge chain of a Kogge-Stone adder is designed as an application example.	adder (electronics);dynamic logic (digital electronics);emoticon;image scaling;kogge–stone adder;network topology;pipeline (computing);spectral leakage;throughput	Juan Núñez;Maria J. Avedillo;José M. Quintana	2013	2013 Euromicro Conference on Digital System Design	10.1109/DSD.2013.141	dynamic logic;embedded system;logic synthesis;real-time computing;delay calculation;logic gate;logic family;computer science;operating system;pass transistor logic;network topology;computer network	EDA	14.644775526708067	55.964819634642545	35570
098071fe5b211d7b8fde490824458f0524c0a04d	bus-aware microarchitectural floorplanning	bus routability;high performance processor;minimal increase;power objective;conflicting objective;floorplanning engine;bus-aware microarchitectural floorplanning;fast performance-aware bus;important floorplanning objective;thermal constraint;engines;microarchitecture;physical design;computer architecture;fpga;computational modeling;routing;instructions per cycle	In this paper we present the first bus-aware microarchitectural floorplanning. Our goal is to study the impact of bus routability on other important floorplanning objectives including area, performance, power, and thermal. We developed a fast performance-aware bus routing algorithm, which is integrated into the floorplanning engine to ensure routability while optimizing other conflicting objectives. Our related experiments performed on high performance processors show that we obtain 100% routability at the cost of minimal increase on area, performance, and power objectives under thermal constraint.	algorithm;central processing unit;experiment;floorplan (microelectronics);microarchitecture;routing	Daehyun Kim;Sung Kyu Lim	2008	2008 Asia and South Pacific Design Automation Conference		physical design;embedded system;computer architecture;parallel computing;computer science;engineering;field-programmable gate array;instructions per cycle	EDA	13.962891745947603	52.800357257657815	35575
075f3992f2eda34247de94adaf8e5607fe3a7ebb	5th generation touchscreen controller for mobile phones and tablets	digital signal processing;touch sensitive screens flexible displays smart phones;filtering;smartphone mobile phones tablets 5th generation touchscreen controller tsg5_i touchscreen controller tsg5 rx afe architecture flexible timing generation;random access memory;acceleration;system on chip;robustness;system on chip controllers digital signal processing hardware robustness random access memory;hardware	Presents a collection of slides covering the following topics: design goals for mobile phones and tablets; architectural elements; Gen5 architecture overview; TSG5_I touchscreen controller; TSG5 RX AFE architecture; flexible timing generation application; Gen 4/5 comparison; current status and future developments of smartphone and tablet devices.	analog front-end;mobile phone;smartphone;tablet computer;touchscreen	Milton Ribeiro;John Carey	2013	2013 IEEE Hot Chips 25 Symposium (HCS)	10.1109/HOTCHIPS.2013.7478319	embedded system;electronic engineering;computer hardware;engineering	Arch	4.784039279827507	49.32003892011289	35586
45b78eb390f3c7bb8167b889ea2ba15fbade4859	impact of mid-bond testing in 3d stacked ics	test flows 3d integration cost modeling test cost;integrated circuit yield;sic mid bond testing 3d stacked ic test flow cost optimization 3d costar variable fault coverage logistics cost outsourced processing steps manufacturing 3d sic cost fault coverages cost reduction die yields;integrated circuit design;silicon compounds;integrated circuit testing;three dimensional integrated circuits fault diagnosis integrated circuit design integrated circuit testing integrated circuit yield silicon compounds;logistics stacking testing packaging three dimensional displays manufacturing companies;three dimensional integrated circuits;fault diagnosis	In contrast to planar ICs, during the manufacturing of three-dimensional stacked ICs (3D-SICs) several tests such as pre-bond, mid-bond, post-bond and final tests can be applied. This in turn results into a huge number of test flows/strategies. Selecting appropriate and efficient test flow (for given design and manufacturing parameters such as stack size, die yield, stack yield, etc) is crucial for overall cost optimization. To evaluate the test flows, a case study is performed in which 3D-COSTAR is used to compare the overall cost of producing a 3D-SIC using variable fault coverage during the mid-bond tests. In addition, we investigate the impact of the logistics cost for various test flows. The impact of logistics costs depend on the outsourced processing steps during the manufacturing. Simulation results show, for our parameters, that by choosing an appropriate test flow the overall 3D-SIC cost for appropriate fault coverages can reduce the overall cost up to 20% for a 5-layered 3D-SIC with die yields of 90%.	fault coverage;logistics;mathematical optimization;simplified instructional computer;simulation;wafer (electronics)	Mottaqiallah Taouil;Said Hamdioui;Erik Jan Marinissen;Sudipta Bhawmik	2013	2013 IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFTS)	10.1109/DFT.2013.6653603	reliability engineering;electronic engineering;engineering;automatic test pattern generation;test compression;integrated circuit design	EDA	23.564713651291434	55.503366158974686	35652
98479c476a74d9043c094f3421bf57141c739cb5	improving the performance of gpu-based genetic programming through exploitation of on-chip memory	many core gpu;parallel programming;genetic programming	Genetic Programming (GP) (Koza, Genetic programming, MIT Press, Cambridge, 1992) is well-known as a computationally intensive technique. Subsequently, faster parallel versions have been implemented that harness the highly parallel hardware provided by graphics cards enabling significant gains in the performance of GP to be achieved. However, extracting the maximum performance from a graphics card for the purposes of GP is difficult. A key reason for this is that in addition to the processor resources, the fast on-chip memory of graphics cards needs to be fully exploited. Techniques will be presented that will improve the performance of a graphics card implementation of tree-based GP by better exploiting this faster memory. It will be demonstrated that both L1 cache and shared memory need to be considered for extracting the maximum performance. Better GP program representation and use of the register file is also explored to further boost performance. Using an NVidia Kepler 670GTX GPU, a maximum performance of 36 billion Genetic Programming Operations per Second is demonstrated.	genetic programming;graphics processing unit	Darren M. Chitty	2016	Soft Comput.	10.1007/s00500-014-1530-3	genetic programming;parallel computing;computer hardware;computer science;theoretical computer science;machine learning	Logic	-0.5790003032403229	45.67801252654392	35728
0390b77183446d674e36cfbff2b0920a03229e15	principles of design automation system for very large scale computer design	higher level;detailed logic;entire computer design;design stage;function test programsautomated logic;design automation system;general purpose computer;progresshighly automated physical design;following design method;design automatioon system;large scale computer design;function level logic simulation;design automation;logic synthesis;computer simulation;design methodology;logic design;physical design;design method;computational modeling;design for testability;functional testing	The paper describes the outline of the design automation system which provides the following design methods: <list><item>Higher level structural drawings capable of expressing designs with Boolean equations and truth tables. </item><item>Function level logic simulation of the entire computer design with extensive use of function test programs </item><item>Automated logic synthesis generating detailed logic just as designers expect </item><item>Delay check promising the most truthful timing evaluation at each design stage in progress </item><item>Highly automated physical design and diagnosis data generation </item></list>The latest very large scale general purpose computer HITACHI M-680H/682H utilizes these methods to successfully reduce its development time.	computer architecture;electronic design automation;logic simulation;logic synthesis;physical design (electronics)	Yasuhiro Ohno;Masayuki Miyoshi;Norio Yamada;Toshihiko Odaka;Tokinori Kozawa;Kooichiro Ishihara	1986	23rd ACM/IEEE Design Automation Conference	10.1145/318013.318069		EDA	9.955325475575215	51.60413521106096	35734
cde4e5c0b70da7e6c39e1ff5b9f2e64e4aecf586	a tile-based egpu with a fused universal processing engine and graphics coprocessor cluster		As various applied sensors have been integrated into embedded devices, the Embedded Graphics Processing Unit (EGPU) has assumedmore processing tasks, which requires an EGPUwith higher performance. A tile-based EGPU is proposed that can be used in both general-purpose computing and 3D graphics rendering. With fused, scalable, and hierarchical parallelism architecture, the EGPU has the ability to address nearly 100 million vertices or fragments and achieves 1 GFLOPS per second at a clock frequency of 200MHz. A fused and scalable architecture, constituted by Universal Processing Engine (UPE) and Graphics Coprocessor Cluster (GCC), ensures that the EGPU can adapt to various graphic processing scenes and situations, achieving more efficient rendering. Moreover, hierarchical parallelism is implemented via the UPE. Additionally, tiling brings a significant reduction in both system memory bandwidth and power consumption. A 0.18 μm technology library is used for timing and power analysis. The area of the proposed EGPU is 6.5mm ∗ 6.5mm, and its power consumption is approximately 349.318mW. Experimental results demonstrate that the proposed EGPU can be used in a System on Chip (SoC) configuration connected to sensors to accelerate its processing and create a proper balance between performance and cost.	3d computer graphics;best, worst and average case;clipper;clock rate;coprocessor;embedded system;flops;general-purpose modeling;graphics processing unit;memory bandwidth;modified huffman coding;parallel computing;rendering (computer graphics);scalability;sensor;single instruction, multiple threads;system on a chip;the unix programming environment;tiling window manager	Yang Wang;Li Zhou;Tao Sun;Yanhu Chen;Lei Wang;Shaotao Sun	2016	J. Sensors	10.1155/2016/7281031	embedded system;real-time computing;computer hardware;computer science	EDA	1.733648426228826	46.51708440124429	35739
57d5b65f6ec15df73c68e483a7383b6742d13190	hardware variability-aware duty cycling for embedded sensors	energy aware embedded software;arm cortex m3 processor;temperature dependent power variation;embedded sensor;temperature sensors;battery powered long running sensing application;hardware variability;hardware variability duty cycling energy aware embedded software;temperature dependence;temperature measurement power measurement temperature dependence power demand temperature sensors capacitance;duty cycle abstraction;temperature 20 c to 60 c hardware variability aware duty cycling embedded sensor temperature dependent power variation battery powered long running sensing application arm cortex m3 processor duty cycle abstraction tinyos datasheet power specification power consumption;embedded systems;power aware computing;duty cycling;datasheet power specification;tinyos;capacitance;temperature 20 c to 60 c;temperature measurement;power consumption;power demand;hardware variability aware duty cycling;power measurement;microprocessor chips;power aware computing embedded systems microprocessor chips	Instance and temperature-dependent power variation has a direct impact on quality of sensing for battery-powered long-running sensing applications. We measure and characterize the active and leakage power for an ARM Cortex M3 processor and show that, across a temperature range of 20 -60, there is a 10% variation in active power, and a variation in leakage power. We introduce variability-aware duty cycling methods and a duty cycle (DC) abstraction for TinyOS which allows applications to explicitly specify the lifetime and minimum DC requirements for individual tasks, and dynamically adjusts the DC rates so that the overall quality of service is maximized in the presence of power variability. We show that variability-aware duty cycling yields a improvement in total active time over schedules based on worst case estimations of power, with an average improvement of across a wide variety of deployment scenarios based on the collected temperature traces. Conversely, datasheet power specifications fail to meet required lifetimes by 7%-15%, with an average 37 days short of the required lifetime of 1 year. Finally, we show that a target localization application using variability-aware DC yields a 50% improvement in quality of results over one based on worst case estimations of power consumption.	arm cortex-m;best, worst and average case;computer data storage;datasheet;duty cycle;embedded system;heart rate variability;image scaling;load balancing (computing);peripheral;quality of results;quality of service;real-time clock;real-time computing;requirement;router (computing);schedule (computer science);scheduling (computing);sensor;software deployment;spatial variability;spectral leakage;tinyos;tracing (software)	Lucas Francisco Wanner;Charwak Apte;Rahul Balani;Puneet Gupta;Mani B. Srivastava	2013	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2012.2203325	embedded system;electronic engineering;real-time computing;temperature measurement;engineering;electrical engineering;capacitance	Mobile	-3.4991043807154103	57.467497501707655	35744
5c2e99f1d8f2b62860fd3fe583464cb9a4998e8c	a heuristic algorithm for gate assignment in one-dimensional array approach	concepcion asistida;assignment problem;logic arrays;computer aided design;design automation;puerta logica;interconnection;modelo 1 dimension;integrated circuit;modele 1 dimension;logic circuits;circuito integrado;circuito logico;minimization methods;tecnologia mos complementario;algorithme;porte logique;algorithm;algorritmo;circuit logique;heuristic algorithms;heuristic algorithms minimization methods logic arrays circuit testing logic circuits system testing design automation;interconnexion;conception assistee;arquitectura;system testing;circuit testing;technologie mos complementaire;logic circuit;logic gate;architecture;circuit integre;interconeccion;heuristic algorithm;complementary mos technology;one dimensional model;transistor	In this paper, we present a new approach for the one-dimensional gate assignment problem. The original minimization problem is transformed into a restricted problem, and then a new heuristic algorithm is applied to it. The solution obtained by the algorithm is interpreted as a solution for the original problem. The whole process of the approach has been implemented and tested with various examples. Experimental results show that our approach can approximately produce optimum solutions.	algorithm;assignment problem;heuristic (computer science)	Takashi Fujii;Hideya Horikawa;Tohru Kikuno;Noriyoshi Yoshida	1987	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.1987.1270259	electronic engineering;electronic design automation;logic gate;computer science;generalized assignment problem;engineering;theoretical computer science;computer aided design;weapon target assignment problem;algorithm	EDA	16.331060095601153	50.03032783698592	35798
ade637774f14c479e31b9bd91ad260f385e5a327	silicon implementation of a chemical reaction-diffusion processor for computation of voronoi diagram	reaction diffusion system;reaction diffusion chip;nonstandard computing;cellular automata;chemical reaction;voronoi diagram	Reaction–diffusion (RD) chemical systems are known to realize sensible computation when both data and results of the computation are encoded in concentration profiles of chemical species; the computation is implemented via spreading and interaction of either diffusive or phase waves, while a silicon RD chip is an electronic analog of the chemical RD systems where the concentration profiles of chemicals are represented by voltage distributions on the chip’s surface. In this paper, we present a prototype RD chip implementing a chemical RD processor for a wellknown NP-complete problem of computational geometry — computation of a Voronoi diagram. We offer experimental results for fabricated RD chips and compare the accuracy of information processing in silicon analogs of RD processors and their experimental “wetware” prototypes.	cellular automaton;central processing unit;computation;computational geometry;information processing;np-completeness;nonlinear system;prototype;ruby document format;voronoi diagram;wetware (brain)	Tetsuya Asai;Ben de Lacy Costello;Andrew Adamatzky	2005	I. J. Bifurcation and Chaos	10.1142/S0218127405013903	cellular automaton;discrete mathematics;chemical reaction;voronoi diagram;computer science;theoretical computer science;mathematics	HPC	18.20053353328395	41.97602438063663	35863
e106c18ccf328aa19991635af612a65ccc9252ee	finding the best efficiency in actionscript based web applications on example of fft algorithm	actionscript;benchmark;flex;amf;php;eeg;c;alchemy		actionscript;algorithm;fast fourier transform	Piotr Wierzgala;Grzegorz M. Wójcik;Marcin Smolira	2012	Bio-Algorithms and Med-Systems	10.1515/bams-2012-0027	embedded system;computer hardware;computer science;operating system	PL	4.684872108593662	48.44193189245957	35888
8d74cfa54920c120cf2f9853c07ceb5845c38352	process deviations and spot defects: two aspects of test and test development for mixed-signal circuits	performance test;product line;hard fault;test pattern selection;structural integrity;soft fault	By their nature, mixed-signal circuits have to be tested for both structural integrity and parametric performance. For the example of data converters we review test pattern selection strategies geared towards structural and performance testing. We introduce a novel test pattern selection strategy that merges both objectives, and by that we achieve a significant reduction in the size of the set of test patterns applied on the production line.	mixed-signal integrated circuit	Carsten Wegener;Michael Peter Kennedy;Bernd Straube	2001	J. Electronic Testing	10.1023/A:1012703202816	structural engineering;reliability engineering;fault coverage;engineering;automatic test pattern generation;test compression;engineering drawing	EDA	23.508563157422994	55.01546802765259	35910
91bc05708adbf72f5f19c78305f8866a0d183874	hierarchical redundancy for array-structure wsis	tolerancia falta;configuracion;hierarchy;hierarchical redundancy;array structure;integrated circuit;redundancia;circuito integrado;integration;wafer;redundancy;integracion;fault tolerance;jerarquia;wafer scale integration;pastilla electronica;pastille electronique;processeur massivement parallele;configuration;hierarchie;structure table;massively parallel processor;tolerance faute;circuit integre;redondance	Abstract#R##N##R##N#This paper proposes hierarchical redundancy configurations for defect tolerance of large-area monolithic LSIs (wafer-scale integrated circuits or WSIs) with array structures. It also specifies how much the integration scales can be increased by expanding hierarchical levels, based on four kinds of one-dimensional/two-dimensional array-connection models. Hierarchical redundancy helps to extend the defect tolerance coverage to additional replacement circuits while maintaining a high level of spare effectiveness. An estimate based on the evaluation of nonredundant additional replacement-circuit scales indicates that four-level hierarchical redundancy configurations can increase integration scales by a factor of 64 to 1024 compared to nonredundant LSIs.		Nobuo Tsuda	1993	Systems and Computers in Japan	10.1002/scj.4690240702	embedded system;fault tolerance;array data structure;parallel computing;computer science;integrated circuit;redundancy;configuration;wafer;hierarchy	NLP	18.860288791312843	52.31840725315171	35953
53734a19ab3174a9b0b03cfadc46f084533e4945	"""comments on """"fast architecture for decimal digit multiplication"""""""		This paper proposes some corrections and comments to the BCD multiplier presented in the paper “Fast architecture for decimal digit multiplication”, published in the Journal of Microprocessors and Microsystems (Volume 39, 2015, issues 4–5, pages 296–301). Some corrections are first proposed to the presented binary multiplier, while the discussion is later extended to some issues that have been found regarding the binary-to-BCD converter. © 2016 Elsevier B.V. All rights reserved.	binary multiplier;binary-coded decimal;microprocessor	Encarnación Castillo;Antonio Lloris-Ruíz;Antonio García;Luis Parrilla;Diego P. Morales	2016	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2016.07.021	computer science;algorithm	EDA	15.92843553693409	44.440170729969026	35955
8e80a6e27c048ff90ee10a4d20010927736a6287	elemental: a new framework for distributed memory dense matrix computations	libraries;parallel computing;linear algebra;general and miscellaneous mathematics computing and information science;performance;shape;high performance	Parallelizing dense matrix computations to distributed memory architectures is a well-studied subject and generally considered to be among the best understood domains of parallel computing. Two packages, developed in the mid 1990s, still enjoy regular use: ScaLAPACK and PLAPACK. With the advent of many-core architectures, which may very well take the shape of distributed memory architectures within a single processor, these packages must be revisited since the traditional MPI-based approaches will likely need to be extended. Thus, this is a good time to review lessons learned since the introduction of these two packages and to propose a simple yet effective alternative. Preliminary performance results show the new solution achieves competitive, if not superior, performance on large clusters.	computation;distributed memory;elemental;lapack;manycore processor;message passing interface;parallel computing;scalapack;sparse matrix	Jack Poulson;Bryan Marker;Robert A. van de Geijn;Jeff R. Hammond;Nichols A. Romero	2013	ACM Trans. Math. Softw.	10.1145/2427023.2427030	parallel computing;performance;shape;computer science;theoretical computer science;linear algebra;mathematics;distributed computing;programming language;algorithm;computing with memory;algebra	HPC	-3.865316898354563	40.453064943775345	35983
32667cebc0b73683a00d4a1148ba407ae43309ad	analyzing the efficiency of block-cyclic checkerboard partitioning in neville elimination	cyclic method;distributed system;algoritmo paralelo;matriz bloque;dimensionnement;evaluation performance;systeme reparti;parallel algorithm;performance evaluation;evaluacion prestacion;dimensioning;simultaneidad informatica;algorithme parallele;methode matricielle;concurrency;sistema repartido;matrix computation;parallel systems;matrice bloc;metodo ciclico;matrix method;pc cluster;metodo matriz;methode cyclique;block matrix;dimensionamiento;simultaneite informatique	In this paper we analyze the performance of the Neville method when a block-cyclic checkerboard partitioning is used. This partitioning can exploit more concurrency than the striped method because the matrix computation can be divided out among more processors than in the case of striping. Concretely, it divides the matrix into blocks and maps them in a cyclic way among the processors. The performance of this parallel system is measured in terms of efficiency, which in this case is close to one when the optimum block size is used and it is run on a Parallel PC Cluster.	analysis of algorithms;neville's algorithm	Policarpo Abascal;Pedro Alonso;Raquel Cortina;Irene Díaz;José Ranilla	2003		10.1007/978-3-540-24669-5_124	matrix method;parallel computing;concurrency;computer science;parallel algorithm;numerical linear algebra;dimensioning;block matrix;algorithm	DB	-2.3014462546617778	36.7321849602953	35988
50a1c9367e6216721c150e873f40d4c8e46bb659	finding the longest simple path in cyclic combinational circuits	logic design computational complexity delays combinational circuits;combinational circuits delay circuit optimization sun computer science timing upper bound algorithm design and analysis adders;logic design;time complexity;longest path;exponential time complexity longest simple path cyclic combinational circuits delay np hard optimal algorithm;computational complexity;combinational circuit;optimal algorithm;delays;combinational circuits	"""A circuit's performance is usually measured by the delay of its longest path. A combinational circuit may contain cycles. We shall call these type of circuits """"cyclic combinational circuits"""". In order to find the delay of a cyclic combinational circuit we need to find the delay of the longest simple path, however traditional longest path algorithms cannot be applied to circuits containing cycles. Finding the longest simple path in a cyclic combinational circuit has been shown to be NP-hard. In this paper we propose an optimal algorithm to solve this problem. The approach we use here is first to replace cycles with matched sub-circuits and then compute the longest simple path of the expanded circuit. The matched sub-circuits are carefully constructed to preserve the path information of the original cyclic combinational circuit. Since the problem is NP-hard, the proposed algorithm has exponential time complexity in the worst case. Nevertheless, the experimental results demonstrate the feasibility of the proposed algorithm."""	combinational logic;path (graph theory)	Yaun-Chung Hsu;Shangzhi Sun;David Hung-Chang Du	1998		10.1109/ICCD.1998.727102	boolean circuit;longest path problem;computer science;theoretical computer science;sequential logic;combinational logic;algorithm	EDA	18.080266367436824	50.219587484363146	36108
b2e54858aae4df8b9a0b2e066f9c20255cfaec9d	exhaustive key search on clusters of gpus	brute force search;kernel;graphics processing units throughput kernel computer architecture dispatching search problems instruction sets;cuda;computer architecture;cuda brute force search;settore ing inf 05 sistemi di elaborazione delle informazioni;cuda exhaustive key search gpu clusters cryptographic hash functions salting technique substantial computing resources heterogeneous network hierarchical network md5 hash functions sha1 hash functions linear scalability hash key search near maximal throughput nvidia devices;graphics processing units;search problems;dispatching;instruction sets;throughput;pattern clustering cryptography graphics processing units parallel architectures	Exhaustive search is generally a last resort for solving a problem: each possible state of a system is generated and evaluated against a condition to find if the problem solution is attained. In some cases, for example in the reversal of cryptographic hash functions that make use of the salting technique, there are very few valid alternatives. However, the set of candidate solutions can be extremely large and therefore very substantial computing resources are needed to walk through the search space in a reasonable time. On the other hand, exhaustive search is very often embarrassingly parallel and so the task can be easily accelerated by distributing the work on a multitude of devices. In this paper we propose a pattern to parallelize general exhaustive searches on a heterogeneous and hierarchical network of computing nodes. We validate this pattern by applying it to the reversal of MD5 and SHA1 hash functions, both at coarse grain (work dispatching among nodes) and at fine grain (work made by each thread), reaching linear scalability with increasing computing power of the participating nodes. In particular, we show how to implement and optimize the hash key search on a GPGPU, achieving near-maximal throughput on various models of NVIDIA devices programmed with CUDA.	brute-force attack;brute-force search;cuda;central processing unit;complexity;cryptographic hash function;embarrassingly parallel;general-purpose computing on graphics processing units;graphics processing unit;md5;manycore processor;maximal set;multi-core processor;parallel computing;password cracking;sha-1;salt (cryptography);scalability;throughput;tree network	Davide Barbieri;Valeria Cardellini;Salvatore Filippone	2014	2014 IEEE International Parallel & Distributed Processing Symposium Workshops	10.1109/IPDPSW.2014.131	security of cryptographic hash functions;double hashing;throughput;parallel computing;kernel;real-time computing;hash function;computer science;theoretical computer science;operating system;instruction set;brute-force search;distributed computing;programming language;algorithm;swifft	HPC	-0.14828816401111464	41.45370195922235	36121
25ab9d7cd53d0a307e122c3b8b72886157199076	application-directed voltage scaling	voltage control;low priority;energy priority scheduling;power aware applications;commande tension;bursty video decoder application directed voltage scaling energy consumption bursty behavior average execution time clock speed processor voltage energy priority scheduling power aware applications high priority tasks complexity strongarm based variable voltage platform video decoder processor power consumption;integrated circuit;decoding;processor power consumption;video signal processing;clocks;processor scheduling;application directed voltage scaling;clock speed;low complexity;circuito integrado;complexity;control tension;indexing terms;video decoder;priority scheduling;adaptive software;low voltage;scheduling algorithm;low power;baja tension;operating system;high priority;energy consumption;optimal scheduling;scheduling;low power electronics;high priority tasks;basse tension;statistics;video signal processing low power electronics decoding digital signal processing chips;horloge;digital signal processing chips;bursty behavior;ordonamiento;procesador;power consumption;consommation energie electrique;processeur;optimal scheduling processor scheduling clocks energy consumption scheduling algorithm decoding statistics operating systems voltage control power measurement;average execution time;strongarm based variable voltage platform;voltage scaling;electronique faible puissance;clock;processor;ordonnancement;reloj;circuit integre;power measurement;power awareness;operating systems;bursty video decoder;processor voltage	Clock (and voltage) scheduling is an important technique to reduce the energy consumption of processors that support voltage scaling. It is difficult, however, to achieve good results using only statistics from the operating system level when applications show bursty (unpredictable) behavior. We take the approach that such applications must be made power-aware and specify their average execution time(AET) and the deadline to the scheduler controlling the clock speed and processor voltage. This paper describes ourenergy priority scheduling(EPS) algorithm supporting power-aware applications. EPS orders tasks according to how tight their deadlines are and how often tasks overlap. Low-priority tasks are scheduled first, since they can be easily preempted to accommodate for high-priority tasks later. The EPS algorithm does not always yield the optimal schedule, but has a low complexity. We have implemented EPS on a StrongARM-based variable-voltage platform. We conducted experiments with a modified video decoder that estimates the AET of each frame. Measurements show that application-directed voltage scaling reduces processor power consumption with 50% for the bursty video decoder without missing any frame deadlines.	algorithm;central processing unit;clock rate;dynamic voltage scaling;encapsulated postscript;experiment;image scaling;operating system;preemption (computing);scheduling (computing);strongarm;video decoder	Johan A. Pouwelse;Koen Langendoen;Henk J. Sips	2003	IEEE Trans. VLSI Syst.	10.1109/TVLSI.2003.814324	embedded system;electronic engineering;real-time computing;telecommunications;computer science;operating system;scheduling;computer network	Embedded	-4.450882432896888	59.16067083068673	36130
898c4fdf1f6f513343cf1df4afb659edeef69724	an efficient arithmetic sum-of-product (sop) based multiplication approach for fir filters and dft	carry logic;multiplying circuits;clocks;finite impulse response filter;adders discrete fourier transforms delay equations finite impulse response filter mathematical model clocks;adder cascade based approach arithmetic sum of product based multiplication sop based multiplication fir filters discrete fourier transform fir filter circuit sop based dft engine column compression algorithm operands single hybrid adder ripple carry adder early arriving lower order bits kogge stone adder carry select adder early arriving higher order bits multiple constant multiplication problem rag n approach cell library algorithmic runtime dft problem digital filters area penalty;multiplying circuits adders carry logic discrete fourier transforms fir filters;adders;mathematical model;fir filters;discrete fourier transforms	In this paper, we present an arithmetic Sum-of-Product (SOP) based approach to implement an efficient Discrete Fourier Transform (DFT) as well as an FIR filter circuit. Our SOP based DFT engine uses an improved column compression algorithm, and also handles the sign of the input efficiently. The partial products of the computation are compressed down to 2 operands, which are then added using a single hybrid adder (which is comprised of a ripple carry adder for the early-arriving lower-order bits, a Kogge-Stone adder for the slower middle bits, and a carry-select adder for the early-arriving higher order bits). The DFT can also be cast as an instance of the Multiple Constant Multiplication (MCM) problem. We compare our SOP-based DFT implementation with the RAG-n approach, the best in-class existing implementation for the MCM problem. RAG-n utilizes a cascade of adders, and attempts to heuristically minimize the number of adders by sharing them across different computations of the DFT. We implemented both approaches using a 45 nm cell library, and demonstrate that our approach yields a faster DFT engine (by about 12-13%), with a small (about 5%) area penalty and a significantly better algorithmic runtime. Our approach is able to complete for DFT problems with a much higher bit precision than the RAG-n approach. The approach of our paper is generalized to implement digital filters as well, and we demonstrate that our approach realizes FIR filters with hard-to-implement coefficients with a 4× speedup and 1.4× area penalty compared to two recent adder-cascade based approaches [1].	adder (electronics);algorithm;carry-lookahead adder;carry-select adder;cell (microprocessor);coefficient;computation;digital filter;discrete fourier transform;disjunctive normal form;finite impulse response;heuristic;logic synthesis;multi-chip module;operand;ripple effect;speedup	Rajeev Kumar;Ayan Mandal;Sunil P. Khatri	2012	2012 IEEE 30th International Conference on Computer Design (ICCD)	10.1109/ICCD.2012.6378640	electronic engineering;theoretical computer science;finite impulse response;serial binary adder;carry-save adder;adder;statistics	EDA	14.524548821358447	46.73619894641946	36150
ac77b1f381c21966363228571c3435cebbb0e14f	on enhancing power benefits in 3d ics: block folding and bonding styles perspective	power benefit;three dimensional displays design automation through silicon vias educational institutions abstracts random access memory timing;three dimensional integrated circuits bonding processes integrated circuit design low power electronics microprocessor chips optimisation power consumption;power optimization power benefits 3d ic block folding bonding styles design methodologies power consumption commercial grade microprocessor opensparc t2 3d floor planning face to face vias two tier 3d design;bonding style;3d ic;block folding;power benefit 3d ic block folding bonding style	Low power is widely considered as a key benefit of 3D ICs, yet there have been few thorough design studies on how to maximize power benefits in 3D ICs. In this paper, we present design methodologies to reduce power consumption in 3D ICs using a large-scale commercial-grade microprocessor (OpenSPARC T2). To further improve power benefits in 3D ICs on top of the traditional 3D floorplanning, we study the impact of block folding and bonding styles. We also develop an effective method to place face-to-face vias for our 2-tier 3D design for power optimization. With aforementioned methods combined, our 3D designs provide up to 20.3% power reduction over the 2D counterpart under the same performance.	3d printing;client–server model;effective method;floorplan (microelectronics);mathematical optimization;microprocessor;opensparc;power optimization (eda);via (electronics)	Moongon Jung;Taigon Song;Yang Wan;Yarui Peng;Sung Kyu Lim	2014	2014 51st ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2593069.2593167	three-dimensional integrated circuit;embedded system;electronic engineering;engineering;electrical engineering;engineering drawing;power optimization	EDA	13.483775546294007	54.38478368221239	36175
048a20f6c89386daf1687e0470488fc3772d0e67	constraint-driven test scheduling for noc-based systems	network on chip noc;noc based systems;constraint driven test scheduling;system on chip soc testing;network on chip;test access mechanism;test scheduling algorithm;on chip integrated network;nonpreemptive scheduling;next generation system chips;test access mechanism constraint driven test scheduling noc based systems on chip integrated network network on chip next generation system chips embedded core based system chips test scheduling algorithm routing path built in self test nonpreemptive scheduling system on chip testing;microeletronica;chip;routing path;test scheduling network on chip noc system on chip soc testing test access mechanism tam;embedded systems;scheduling algorithm;built in self test;system on chip;scheduling;embedded core based system chips;integrated circuit testing;next generation;test scheduling;system testing network on a chip system on a chip integrated circuit interconnections routing hardware next generation networking scheduling algorithm built in self test benchmark testing;system on chip testing;artigo de periodico;test access mechanism tam;system on chip built in self test embedded systems integrated circuit testing scheduling	On-chip integrated network, the so-called network-on-chip (NoC), is becoming a promising communication paradigm for the next-generation embedded core-based system chips. The reuse of the on-chip network as test access mechanism has been recently proposed to handle the growing complexity of testing NoC-based systems. However, the NoC reuse is limited by the on-chip routing resources and various constraints. Therefore, efficient test-scheduling methods are required to deliver feasible test time while meeting all the constraints. In this paper, the authors propose a comprehensive approach to test scheduling in NoC-based systems. The proposed scheduling algorithm is based on the use of dedicated routing path that is suitable for nonpreemptive test. The algorithm is improved by incorporating both preemptive and nonpreemptive tests. In addition, BIST, precedence, and power constraints were taken into consideration. Experimental results for the ITC'02 system-on-chip benchmarks show that the nonpreemptive scheduling based on dedicated path can efficiently reduce test application time compared to previous work, and the improved method provides a practical solution to the real-world NoC-based-system testing with both preemptive and nonpreemptive cores. It is also shown that various constraints can be incorporated to deliver a comprehensive test solution	algorithm;built-in self-test;embedded system;heuristic;np-completeness;network on a chip;preemption (computing);programming paradigm;routing;scheduling (computing);system on a chip;system testing	Érika F. Cota;Chunsheng Liu	2006	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2006.881331	embedded system;parallel computing;real-time computing;computer science;engineering;operating system;network on a chip;scheduling	EDA	11.857817210592541	53.80688962408985	36234
0ecb70b7c7906ad896d884e69775fc64cbad93ae	grid enabled optimization	distributed system;optimisation;haute performance;systeme reparti;optimizacion;aerodynamics;distributed computing;design optimization;aerofoil;grid;sistema repartido;profil aerodynamique;mathematical programming;rejilla;alto rendimiento;grille;calculo repartido;perfil aerodinamico;optimization;aerodynamique;grid computing;programmation mathematique;high performance;calcul reparti;programacion matematica;aerodinamica	In this paper, we present a scalable parallel framework, which employs grid computing technologies, for solving computationally expensive and intractable design problems. Using an aerodynamic airfoil design optimization problem as an example the application of the grid computing strategies is dis-	analysis of algorithms;grid computing;mathematical optimization;optimization problem;scalability	Hee-Khiang Ng;Yew-Soon Ong;Terence Hung;Bu-Sung Lee	2005		10.1007/11508380_31	multidisciplinary design optimization;simulation;aerodynamics;computer science;artificial intelligence;airfoil;distributed computing;grid;grid computing	HPC	-3.2100130163548086	33.86250326219135	36247
844ee5f9a1c82675e52e8f4c7a7fe1f601171f1c	impact and design guideline of monolithic 3-d ic at the 7-nm technology node		Monolithic 3-D (M3D) IC is one of the potential technologies to break through the challenges of continued circuit power and performance scaling. In this paper, for the first time, we demonstrate the power benefits of M3D and present design guideline in a 7-nm FinFET technology node. The predictive 7-nm process design kit (PDK) and the standard cell library using both high-performance (HP) and low-standby-power (LSTP) device technologies are developed based on NanGate 45-nm PDK using accurate dimensional, material, and electrical parameters from publications and a commercial-grade tool flow. We implement full-chip M3D designs utilizing industry-standard physical design tools, and gauge the impact of M3D technology on performance, power, and area metrics. We also provide the design guidelines as well as a new partitioning methodology to improve M3D design quality. This paper shows that M3D designs outperform 2-D counterparts by 16% and 16.5% on average in terms of isoperformance total power reduction with 7-nm HP and LSTP cell library, respectively. This demonstrates the power benefits of M3D technology in both HP and low-power future generation devices.	clock signal;design space exploration;graph dynamical system;image scaling;low-power broadcasting;multitier architecture;performance per watt;physical design (electronics);process design kit;semiconductor device fabrication;simulation;standard cell	Kyungwook Chang;Kartik Acharya;Saurabh Sinha;Brian Cline;Greg Yeric;Sung Kyu Lim	2017	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2017.2686426	electronic engineering;computer science;physical design;logic gate;process design;standard cell;guideline	EDA	3.5581752730848226	58.38064341425962	36262
b3546a13eaa917f7f790fbf9ba235ae27b078df5	hardware design of a fast, parallel random tree path planner	trajectory planning algorithm;uav unmanned aerial vehicle;path planning problems;path planning;near neighbour nn search speed;hardware path planning algorithm design and analysis computer architecture heuristic algorithms indexes field programmable gate arrays;algorithm;computer architecture;indexes;reconfigurable architectures field programmable gate arrays logic design path planning random access storage;2d environment fast parallel random tree path planner rrt method rapidly exploring random trees method hardware architecture near neighbour search speed nn fpga natural parallel computing ability tree nodes block rams independent traversal query module memory throughput parallel master slave mode;heuristic algorithms;field programmable gate arrays;algorithm design and analysis;rapidly exploring random trees rrt;hardware	The Rapidly-Exploring Random Trees (RRT) method has been proved successful and efficient for solving path planning problems. Most recent work focuses on optimizing RRT itself and presents the results achieved from software. In this paper a dedicated hardware architecture for FPGA implementation of Rapidly-Exploring Random Tree (RRT) path planning is developed. The proposed architecture fully takes advantage of FPGAs' natural parallel computing ability. The near neighbour (NN) search speed is improved by splitting the whole search space into several sub-spaces; correspondingly the tree nodes are stored in separate block RAMs and each Block RAM owns its independent traversal query module to enhance the memory throughput. Furthermore, to speed up the space exploration, each single path planning unit consists of two cooperating RRT modules which grow trees from each end of the path. One complete path planner involves several path planning units which work in a parallel master-slave mode to increase the probability of obtaining available path. Implementation in a 2D environment shows good path planning performance of the hardware design, with a 30x speed improvement compared to a PC implementation.	field-programmable gate array;motion planning;parallel computing;random-access memory;rapidly-exploring random tree;throughput	Size Xiao;Adam Postula;Neil W. Bergmann	2015	2015 International Conference on Field Programmable Technology (FPT)	10.1109/FPT.2015.7393151	database index;embedded system;algorithm design;parallel computing;fast path;any-angle path planning;computer science;theoretical computer science;distributed computing;motion planning;field-programmable gate array	Robotics	0.4364201263335996	41.12202281718855	36281
b69fcba48ca4a018eb9638e322b6c300d9ed66f6	a loop structure optimization targeting high-level synthesis of fast number theoretic transform		Multiplication with a large number of digits is heavily used when processing data encrypted by a fully homomorphic encryption, which is a bottleneck in computation time. An algorithm utilizing fast number theoretic transform (FNTT) is known as a high-speed multiplication algorithm and the further speeding up is expected by implementing the FNTT process on an FPGA. A high-level synthesis tool enables efficient hardware implementation even for FNTT with a large number of points. In this paper, we propose a methodology for optimizing the loop structure included in a software description of FNTT so that the performance of the synthesized FNTT processor can be maximized. The loop structure optimization is considered in terms of loop flattening and trip count reduction. We implement a 65,536-point FNTT processor with the loop structure optimization on an FPGA, and demonstrate that it can be executed 6.9 times faster than the execution on a CPU.	central processing unit;computation;field-programmable gate array;high- and low-level;high-level synthesis;homomorphic encryption;mathematical optimization;multiplication algorithm;theory;time complexity	Kazushi Kawamura;Masao Yanagisawa;Nozomu Togawa	2018	2018 19th International Symposium on Quality Electronic Design (ISQED)	10.1109/ISQED.2018.8357273	real-time computing;loop optimization;field-programmable gate array;encryption;computation;high-level synthesis;multiplication algorithm;computer science;multiplication;bottleneck	Arch	8.385234814627891	44.289814132768086	36293
877e858e250208d076f9aa1bc1db28dd8a278782	a framework for supporting real-time applications on dynamic reconfigurable fpgas	software;hardware and architecture;real time;fpga;computer networks and communications;heterogeneous;dpr;reconfigurable	Computing platforms are evolving towards heterogeneous architectures including processors of different types and field programmable gate arrays (FPGAs), used as hardware accelerators for speeding up specific functions. The increasing capacity and performance of modern FPGAs, with their partial reconfiguration capabilities, have made them attractive in several application domains, including space applications.This paper proposes a framework for supporting the development of safety-critical real-time systems that exploit hardware accelerators developed through FPGAs with dynamic partial reconfiguration capabilities.A model is first presented and then used to derive a response-time analysis to verify the schedulability of a real-time task set under given constraints and assumptions. Although the analysis is based on a generic model, the proposed framework has been conceived to account for several real-world constraints present on today's platforms and has been practically validated on the Zynq platform, showing that it can actually be supported by state-of-the-art technologies. Finally, a number of experiments are reported to evaluate the worst-case performance of the proposed approach on synthetic workload.	best, worst and average case;central processing unit;dhrystone;erika enterprise;experiment;field-programmable gate array;hardware acceleration;heterogeneous computing;operating system;real-time clock;real-time computing;real-time transcription;scheduling (computing);synthetic intelligence;system call	Alessandro Biondi;Alessio Balsini;Marco Pagani;Enrico Rossi;Mauro Marinoni;Giorgio C. Buttazzo	2016	2016 IEEE Real-Time Systems Symposium (RTSS)	10.1109/RTSS.2016.010	embedded system;computer architecture;parallel computing;real-time computing;reconfigurable computing;computer science;operating system;field-programmable gate array	Embedded	-2.2162961920748194	50.277481913866055	36347
cb51134b66f21a8103a771daa436b15ac574d0b9	design-hierarchy aware mixed-size placement for routability optimization	routability driven placer;block distributions;optimisation;design hierarchy aware mixed size placement;network synthesis;optimisation network routing network synthesis network topology;metals;routability optimization;routing;peak current;large scale mixed size circuit placement;force;network routing;wire;network topology;large scale;net topology estimation design hierarchy aware mixed size placement routability optimization large scale mixed size circuit placement routability driven placer block distributions;low power;state replication;state encoding;optimization;routing force optimization equations algorithm design and analysis wire metals;algorithm design and analysis;finite state machine;net topology estimation	Routability is a mandatory metric for modern large-scale mixed-size circuit placement which typically needs to handle hundreds of large macros and millions of small standard cells. However, most existing academic mixed-size placers either focus on wirelength minimization alone, or do not consider the impact of movable macros on routing. To remedy these insufficiencies, this paper formulates design-hierarchy information as a novel fence force in an analytical placement framework. Unlike a state-of-the-art routability-driven placer that simply removes net bounding boxes during placement, this paper utilizes two different optimization forces, the global fence force and the local spreading force, to determine the positions of both standard cells and macros. We utilize design-hierarchy information to determine block distributions globally, and locally we add additional spreading forces to preserve sufficient free space among blocks by a net-topology estimation. With the interactions between these two forces, our placer can well balance routability and wirelength. Experimental results show that our placer can achieve the best routability and routing time among all published works.	cluster analysis;interaction;mathematical optimization;network congestion;routing	Yi-Lin Chuang;Gi-Joon Nam;Charles J. Alpert;Yao-Wen Chang;Jarrod A. Roy;Natarajan Viswanathan	2010	2010 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)	10.1109/ICCAD.2010.5654234	mathematical optimization;routing;electronic engineering;computer science;engineering;finite-state machine;engineering drawing	EDA	15.39262239988337	52.93780680489806	36357
fc97d1d20f816d3e314a7a12b33d2111f5a6f215	architectural framework for dynamically adaptable multiprocessors regarding aging, fault tolerance, performance and power consumption			fault tolerance	Aleksandar Simevski	2015				HPC	-3.260454460904945	50.39806819524993	36365
260b8bb1669d189fec97af772559c7850be39641	same/different fault dictionary: an extended pass/fail fault dictionary with improved diagnostic resolution	faulty circuit;preselected output vector;fault free circuit;diagnostic resolution;fault dictionary;baseline output vector fault dictionary diagnostic resolution faulty circuit fault free circuit preselected output vector;integrated circuit reliability;integrated circuit reliability cause effect analysis fault diagnosis;baseline output vector;cause effect analysis;fault diagnosis	A new type of fault dictionary called a same/different fault dictionary is described. The same/different fault dictionary is similar to a pass/fail fault dictionary in that it contains a single bit bi, j for every modelled fault fi and test vector tj. However, in a pass/fail fault dictionary, bi, j is determined by comparing the output vector of the faulty circuit with the output vector of the fault-free circuit; whereas in a same/different fault dictionary, bi, j is determined by comparing the output vector of the faulty circuit with a preselected output vector called a baseline output vector. By selecting appropriately the baseline output vectors for all the test vectors, it is possible to obtain increased diagnostic resolution with a same/different fault dictionary compared with a pass/fail fault dictionary. A heuristic procedure for selecting baseline output vectors is described and experimental results are presented.	data dictionary	Irith Pomeranz;Sudhakar M. Reddy	2009	IET Computers & Digital Techniques	10.1049/iet-cdt:20080017	electronic engineering;fault indicator;engineering;electrical engineering;stuck-at fault;automatic test pattern generation;machine learning	Theory	22.20039445918926	50.891545046257434	36392
c77515010eb1c83854f58d5e374c68145d9be50d	enhancing the area efficiency of fpgas with hard circuits using shadow clusters	field programmable gate arrays logic circuits routing multiplexing area measurement table lookup fabrics tiles strontium application software;logic density gap field programmable gate arrays fpga logic cluster application specific integrated circuits hard circuits shadow clusters;logic design;shadow clusters;logic density gap;hard circuits;fpga logic cluster;application specific integrated circuits;fixed ratio;cost effectiveness;logic design application specific integrated circuits field programmable gate arrays;field programmable gate arrays	There is a dramatic logic density gap between field-programmable gate arrays (FPGAs) and application-specific integrated circuits, and this gap is the main reason FPGAs are not cost-effective in high-volume applications. Modern FPGAs narrow this gap by including “hard” circuits such as memories and multipliers, which are very efficient when they are used. However, if these hard circuits are not used, they go wasted (including the very expensive programmable routing that surrounds the logic), and have a negative impact on logic density. In this paper, we present an architectural concept, called shadow clusters, which seeks to mitigate this loss. A shadow cluster is a standard FPGA logic “cluster” (typically consisting of a group of lookup tables and flip-flops) that is placed “behind” every hard circuit, and can programmably, through simple, small multiplexers, replace the hard circuit in the event it is not needed. A shadow cluster is effective because the largest area cost, by far, in an FPGA is for the programmable routing that connects the logic. The shadow cluster area cost is small, and yet it enables more consistent employment of the programmable routing across applications with varying demand for hard circuits. We introduce new terminology to describe the economics of hard circuits on FGPAs, and provide a scientific way to measure the area effectiveness. We measure the area efficiency of FPGAs with and without shadow clusters, and show that a modern commercial architecture (with a fixed ratio of multipliers to soft logic) would gain 4.7% in area efficiency by employing shadow clusters. Indeed, every architecture we studied under “reasonable” conditions never showed a loss of area efficiency. Furthermore, we show that most area-efficient architecture that employs the shadow cluster concept is 12.5% better than the most area-efficient architecture without shadow clusters.	application-specific integrated circuit;benchmark (computing);flops;field-programmability;field-programmable gate array;lookup table;multiplexer;routing;shadow mapping	Peter Jamieson;Jonathan Rose	2006	2006 IEEE International Conference on Field Programmable Technology	10.1109/FPT.2006.270384	embedded system;nmos logic;logic synthesis;real-time computing;macrocell array;logic optimization;diode–transistor logic;cost-effectiveness analysis;logic level;logic gate;logic family;computer hardware;reconfigurable computing;programmable logic array;computer science;programmable logic device;pass transistor logic;complex programmable logic device;application-specific integrated circuit;simple programmable logic device;digital electronics;programmable array logic;pmos logic;field-programmable gate array;resistor–transistor logic	EDA	13.612206018627944	55.95100377693941	36398
0bde55037fbec8de3536d4422342fda2c8a0779e	automatic synthesis of efficient intrusion detection systems on fpgas	graph theory;intruder detector;field programmable gate array;evaluation performance;diseno circuito;optimisation;decomposition domaine;teoria grafo;haute performance;architecture systeme;descomposicion grafo;domain decomposition;performance evaluation;optimizacion;network security;reconfigurable architectures;reutilizacion;evaluacion prestacion;securite informatique;circuit design;descomposicion dominio;intrusion detection;red puerta programable;theorie graphe;system performance;reseau porte programmable;application intensive;reuse;computer security;intensive application;seguridad informatica;fpga architecture;intrusion detection systems;graph algorithm;alto rendimiento;arquitectura sistema;optimization;conception circuit;detecteur intrus;system architecture;aplicacion intensiva;detector intruso;high performance;architecture reconfigurable;systeme detection intrusion;intrusion detection system;reutilisation;graph decomposition;decomposition graphe	This paper presents a methodology and a tool for automatic synthesis of highly efficient intrusion detection systems using a high-level, graph-based partitioning methodology and tree-based lookahead architectures. Intrusion detection for network security is a compute-intensive application demanding high system performance. The tools implement and automate a customizable flow for the creation of efficient field programmable gate array (FPGA) architectures using system-level optimizations. Our methodology allows for customized performance through more efficient communication and extensive reuse of hardware components for dramatic increases in area-time performance	bitap algorithm;clock rate;comparator;data rate units;database;field-programmable gate array;high- and low-level;intrusion detection system;mathematical optimization;network security;parsing;preprocessor;string searching algorithm;throughput	Zachary K. Baker;Viktor K. Prasanna	2004	IEEE Transactions on Dependable and Secure Computing	10.1007/978-3-540-30117-2_33	intrusion detection system;embedded system;real-time computing;simulation;computer science;graph theory;network security;operating system;systems architecture	EDA	2.3263602195656703	55.62074315458663	36403
70b8fe574951a812d570d96f5ed48360c404e6bd	optimization on cell-library design for digital application specific printed electronics circuits	libraries;printed circuit design application specific integrated circuits digital integrated circuits integrated circuit design logic gates;mos devices;optimization ratioed pmos only design styles nand gates nand3 inverters nor2 nand2 library compositions logic synthesis vlsi design flow printed electronics designs digital application specific printed electronics circuits cell library design;routing;very large scale integration;mos devices libraries very large scale integration integrated circuit modeling routing optimization logic gates;logic gates;integrated circuit modeling;optimization;printed electronics asic aspec otft standard cell transistor count logic synthesis technology mapping	This paper presents an investigation about the ideal composition of cell libraries to be used for digital Application Specific Printed Electronics Circuits (ASPECs). Printed/organic/flexible electronics is becoming more and more important over the last years, and it seems that the industry will continue growing as new possible applications arise, and the existing ones are being improved due to better designs and fabrication processes, even moving towards integrating logic circuitry together with sensors and actuators. This paper presents considerations for developing (ASPECs), trying to keep a similar approach to the typical ASIC procedures. The work presented herein adopted a cell-based design methodology addressed to printed electronics (PE) designs. Such methodology allows us to propose a design flow for PE similar to the VLSI design flow, comprising logic synthesis, mapping, placement, and routing. In order to evaluate different library compositions, a set of benchmark has been mapped with six different combinations of mapping tools and associated libraries. The obtained results show that a simple library composed of just three cells - either NAND2, NOR2 and inverters or NAND, NAND3 and inverters - performs very well in terms of transistor count. NAND gates are usually preferred options for ratioed PMOS-only design styles. Using a more complex cell library can produce reductions of around 25% in terms of transistor count, but produce increases of around 23% as well.	as-interface;application-specific integrated circuit;benchmark (computing);design flow (eda);electronic circuit;flexible electronics;inverter (logic gate);library (computing);logic synthesis;nand gate;pmos logic;place and route;printed electronics;routing;sensor;standard cell;transistor count;very-large-scale integration	Manuel Llamas;Mohammad Mashayekhi;Jordi Carrabina;Jody Maick Matos;André Inácio Reis	2014	2014 24th International Workshop on Power and Timing Modeling, Optimization and Simulation (PATMOS)	10.1109/PATMOS.2014.6951885	routing;electronic engineering;nor logic;logic optimization;logic gate;logic family;tape-out;computer science;engineering;electrical engineering;pass transistor logic;place and route;very-large-scale integration;integrated injection logic;digital electronics;register-transfer level;routing;computer engineering	EDA	13.166866484814232	51.96507997401962	36461
11e961baa9448b7069be6518dffcf8cc6bf6bd3c	entropic minimization of multiple-valued functions	heuristic decision making;minimisation of switching nets logic design many valued logics;gas discharge devices;sum of products;many valued logics;four valued functions entropic minimisation multiple valued functions multiple valued switching function heuristic decision making binary switching function;logic design;very large scale integration;minimisation of switching nets;logic;random variables;minimization methods;binary switching function;multiple valued functions;entropic minimisation;probability distribution;gas discharge devices decision trees decision making logic very large scale integration minimization methods np complete problem random variables probability distribution;multiple valued switching function;value function;multiple valued;decision trees;information theoretic;np complete problem;four valued functions	An information theoretic procedure for minimizing multiple-valued switching func t ion is outlined. The procedure h a s been developed a s an extension o f prev ious works about heuris t ic decision making and binary switching func t ion minimization for the sum of products expansion of the func t ion to be minimized. I n order to show how the proposed procedure works, a complete example for a 3-variable 4-valued func t ion is given. I n adition, certain global r e su l t s referring to 4-variable 4valued func t ions are shown.	information theory	Antonio Lloris-Ruíz;Juan Francisco Gómez-Lopera;Ramón Román-Roldán	1993		10.1109/ISMVL.1993.289586	probability distribution;mathematical optimization;combinatorics;discrete mathematics;computer science;mathematics;logic;statistics	AI	19.834388662934973	44.77370481733549	36466
f580ef9126610bd159ffaf2bafd88873defa7236	analog design optimization by means of a tabu search approach	design tool;spice analog design optimization tabu search heuristic device sizing designer specifications global minimum cost function iterations analog simulation program;cost function;analogue integrated circuits circuit optimisation circuit cad spice integrated circuit design;design optimization;integrated circuit design;analogue integrated circuits;design optimization cost function spice computational modeling simulated annealing ieee members process design circuit topology minimization circuit simulation;digital design;tabu search;circuit cad;circuit optimisation;spice	Although many design tools have been developed for digital design, analog design tools are just being introduced. This paper deals with device sizing of analog cells. A new method using the Tabu Search heuristic is proposed. It minimizes a given cost function, which depends on a set of designer specifications. The results obtained show that this method can approach very close the global minimum of the cost function with only a few number of iterations. Hence, an analog simulation program, such as SPICE, can be used for the evaluation of the cost function at each iteration. >	mathematical optimization;tabu search	Miguel Angel Aguirre Echánove;Jorge Chávez;Antonio Torralba;Leopoldo García Franquelo	1994		10.1109/ISCAS.1994.408875	physical design;mathematical optimization;electronic engineering;multidisciplinary design optimization;tabu search;computer science;engineering;circuit design;circuit extraction;computer engineering;integrated circuit design	EDA	14.412212106262695	50.9572287356774	36502
8899a437f74a49b74e1c00029aa032a201ea1434	dense subgraphs on dynamic networks	estimation;distributed algorithm;at least k dense subgraphs;graph density;dynamic networks	In distributed networks, it is often useful for the nodes to be aware of dense subgraphs, e.g., such a dense subgraph could reveal dense substructures in otherwise sparse graphs (e.g. the World Wide Web or social networks); these might reveal community clusters or dense regions for possibly maintaining good communication infrastructure. In this work, we address the problem of self-awareness of nodes in a dynamic network with regards to graph density, i.e., we give distributed algorithms for maintaining dense subgraphs that the member nodes are aware of. The only knowledge that the nodes need is that of the dynamic diameter D, i.e., the maximum number of rounds it takes for a message to traverse the dynamic network. For our work, we consider a model where the number of nodes are fixed, but a powerful adversary can add or remove a limited number of edges from the network at each time step. The communication is by broadcast only and follows the CONGEST model. Our algorithms are continuously executed on the network, and at any time (after some initialization) each node will be aware if it is part (or not) of a particular dense subgraph. We give algorithms that (2+ )-approximate the densest subgraph and (3 + )-approximate the at-least-k-densest subgraph (for a given parameter k). Our algorithms work for a wide range of parameter values and run in O(D log1+ n) time. Further, a special case of our results also gives the first fully decentralized approximation algorithms for densest and at-least-k-densest subgraph problems for static distributed	adversary (cryptography);approximation algorithm;dense subgraph;distributed algorithm;self-awareness;social network;sparse matrix;traverse;world wide web	Atish Das Sarma;Ashwin Lall;Danupon Nanongkai;Amitabh Trehan	2012		10.1007/978-3-642-33651-5_11	distributed algorithm;estimation;dense graph;computer science;theoretical computer science;distributed computing;statistics	Theory	19.12935304752029	34.60060863758257	36517
d99061f8d4fb2d8d76aa7ef4e3d5be0fed6cff7a	multithreaded and spark parallelization of feature selection filters	machine learning;spark;feature selection;multithreading	Vast amounts of data are generated every day, constituting a volume that is challenging to analyze. Techniques such as feature selection are advisable when tackling large datasets. Among the tools that provide this functionality, Weka is one of the most popular ones, although the implementations it provides struggle when processing large datasets, requiring excessive times to be practical. Parallel processing can help alleviate this problem, effectively allowing users to work with Big Data. The computational power of multicore machines can be harnessed by using multithreading and distributed programming, effectively helping to tackle larger problems. Both these techniques can dramatically speed up the feature selection	big data;distributed computing;feature selection;multi-core processor;multithreading (computer architecture);parallel computing;parallel processing (dsp implementation);spark;thread (computing);weka	Carlos Eiras-Franco;Verónica Bolón-Canedo;Sabela Ramos;Jorge Gonz&#x00E1;lez-Dom&#x00ED;nguez;Amparo Alonso-Betanzos;Juan Touriño	2016	J. Comput. Science	10.1016/j.jocs.2016.07.002	parallel computing;multithreading;spark;computer science;theoretical computer science;operating system;machine learning;data mining;feature selection;algorithm	HPC	-1.7132613544051793	42.11258115083688	36546
d957583ea9b953411a9da1742ecd2232fb51652b	a study of tapered 3-d tsvs for power and thermal integrity	3 d integration;through silicon vias inductance analytical models switching circuits integrated circuit modeling capacitance heat sinks;3d ic through silicon vias tapered 3d tsv power integrity thermal integrity 3d integration heterogeneous technology switching currents power delivery paths physical design power dissipation thermal dissipation decoupling capacitors tsv on die package parasitics resonance effects power gating ten tier system power distribution heat dissipation;three dimensional integrated circuits capacitors cooling electronics packaging;capacitors;power delivery;power and thermal analysis;power delivery 3 d integration power and thermal analysis;electronics packaging;three dimensional integrated circuits;cooling	3-D integration presents a path to higher performance, greater density, increased functionality and heterogeneous technology implementation. However, 3-D integration introduces many challenges for power and thermal integrity due to large switching currents, longer power delivery paths, and increased parasitics compared to 2-D integration. In this work, we provide an in-depth study of power and thermal issues while incorporating the physical design characteristics unique to 3-D integration. We provide a qualitative perspective of the power and thermal dissipation issues in 3-D and study the impact of Through Silicon Vias (TSVs) size for their mitigation. We investigate and discuss the design implications of power and thermal issues in the presence of decoupling capacitors, TSV/on-die/package parasitics, various resonance effects and power gating. Our study is based on a ten-tier system utilizing existing 3-D technology specifications. Based on detailed power distribution and heat dissipation models, we present a comprehensive analysis of TSV tapering for alleviating power and thermal integrity issues in 3-D ICs.	c4 engine;clock rate;coupling (computer programming);multitier architecture;physical design (electronics);power gating;resonance;thermal management (electronics);through-silicon via	Aida Todri;Sandip Kundu;Patrick Girard;Alberto Bosio;Luigi Dilillo;Arnaud Virazel	2013	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2012.2187081	embedded system;power module;electronic engineering;capacitor;power electronic substrate;engineering;electrical engineering;switched-mode power supply;electronic packaging	Arch	19.47642058507964	56.93360081690513	36567
8e52e2e91fe78e572bfacd694ee1562ed6671581	racer: a reconfigurable constraint-length 14 viterbi decoder	racer;reconfigurable architectures;custom backplane racer reconfigurable constraint length 14 viterbi decoder xilinx xc4010 fpgas;xilinx xc4010 fpgas;custom backplane;reconfigurable architectures viterbi decoding field programmable gate arrays;viterbi decoder;reconfigurable constraint length 14 viterbi decoder;field programmable gate arrays;viterbi decoding	This paper describes the architecture and implementation of a constraint-length 14 Viterbi Decoder that achieves a decoding rate of 41 Kbitsh. The system uses 36 Xilinx XC4010 FPGAs with seven processor cards and a custom backplane to implement a multi-ring general cascade Viterbi decoder architecture. The paper will also show how to achieve decoding rates of I Mbit/s using current FPGA technology. Comparisons are made to JPL's Big Viterbi Decoder, which uses custom ASICs.	application-specific integrated circuit;backplane;convolutional code;field-programmable gate array;megabit;viterbi decoder	David Yeh;Gennady Feygin;Paul Chow	1996		10.1109/FPGA.1996.564746	computer architecture;parallel computing;computer science;viterbi decoder	Mobile	10.911452738519872	44.72255955645663	36610
42123394a34a6a5a9a180d5d7038fea4834057e4	dynamic reconfiguration approach for high speed turbo decoding using circular rings	channel decoding;map;dynamic reconfiguration;qpp;custom reconfigurable;contention free;state machine;memory access;parallel turbo;permutation polynomial;vlsi;power consumption;domain specific;turbo decoding;high speed;domain specificity	A static and dynamic reconfiguration method and apparatus that can be utilized for parallel turbo decoding is described. The methodology is based on the property of a class of polynomials that can provide contention free permutation. A new ring interconnect methodology is presented that can be used for quadratic permutation polynomial (QPP) based interleaved memory access. An efficient interleave-deinterleave mechanism using QPP interleaver is also presented for a new power driven reconfigurable solution. It consists of multiple SISO modules connected by a mix of dynamic and static reconfigurable interconnect with an efficient memory segmentation that matches the underlying reconfigurable fabric. The parallel blocks are controlled by a unified state machine mapped external to the reconfigurable fabric. The overall array is implemented on 90nm Toshiba standard cell library occupying an area of 11.11 mm2 with reconfigurable speeds of 7.8-131.28 Mb/sec and the corresponding power consumption of 53-762 mW.	finite-state machine;forward error correction;interleaved memory;memory segmentation;permutation polynomial;simulation interoperability standards organization;standard cell	Imran Ahmed;Cheran M. Vithanage	2009		10.1145/1531542.1531651	embedded system;electronic engineering;parallel computing;real-time computing;computer science;domain-specific language;operating system;map;very-large-scale integration;finite-state machine;programming language	EDA	5.590954673925102	51.34501997026517	36634
8a11680965a9b90185026b0e7554db4e97777095	cross-layer approaches for an aging-aware design of nanoscale microprocessors			microprocessor	Fabian Oboril	2015				EDA	10.420631344370122	58.142381647213085	36670
e15362bcb3781ebf0106647d6e228eb05c119330	dynamic frequency scaling based power saving algorithm for a portable kitchen tv	video coding digital multimedia broadcasting domestic appliances mobile television power aware computing television applications;domestic appliances;television applications;decoding tv streaming media transform coding signal processing algorithms software algorithms time frequency analysis;video coding;power aware computing;digital multimedia broadcasting;terrestrial digital multimedia broadcasting dynamic frequency scaling based power saving algorithm portable kitchen tv mpeg 4 video decoder atsc t dmb supporting combo platform cpu operating frequency video frame mobile multimedia terminals advanced television system committee;mobile television	This paper presents a dynamic frequency scaling technique to save the processing power of MPEG-4 video decoder implemented in a portable kitchen TV, which is an ATSC/T-DMB supporting combo platform. The operating frequency of CPU is dynamically changed in accordance to the estimated processing burden of each video frame.	atsc standards;algorithm;central processing unit;clock rate;dynamic frequency scaling;image scaling;video decoder	Won-Jong Kim;Tae-Ho Roh;Kyou-Jung Son;Seong-Pil Moon;Chang-Hwan Jang;Tae-Gyu Chang	2013	2013 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2013.6486848	embedded system;internet television;broadcast television systems;video;digital television;telecommunications;computer science;digital broadcasting;video capture;multimedia;video processing	Robotics	11.77218391402282	40.54041413407262	36696
82733ac9f7faa3667031f2c008af47bf65270776	a randomized parallel backtracking algorithm	algoritmo paralelo;parallel backtracking;parallel algorithm;search problems parallel algorithms;fault tolerant;multiprocessor;prolog;logical programming;speedups;algorithme parallele;algorithme;algorithm;aleatorizacion;programmation logique;high reliability;fault tolerance;interprocessor communication;backtracking;randomisation;fault tolerance parallel backtracking randomization speedups interprocessor communication high reliability flexibility;search problems;multiprocesador;randomization;programacion logica;flexibility;algoritmo;parallel algorithms;multiprocesseur	A new technique of parallel backtracking using randomization is proposed. The main advantage of randomization is that good speedups are possible with little or no interprocessor communication. The speedup obtainable using this technique is in general, problem dependent. In precisely those cases where the problem size becomes very large, randomization is extremely successful achieving good speedups. The technique also ensures high reliability, flexibility, and fault tolerance.	analysis of algorithms;backtracking;fault tolerance;inter-process communication;randomized algorithm;speedup	Virendra K. Janakiram;Dharma P. Agrawal;Ravi Mehrotra	1988	IEEE Trans. Computers	10.1109/12.9745	fault tolerance;parallel computing;computer science;distributed computing;parallel algorithm;algorithm	EDA	10.476890779454198	33.55375151784115	36717
f2615c98be3407f634a62be7656735471b46616b	bi-direction slice arrangement algorithm for mpeg video transmission over atm networks			atm turbo;algorithm;moving picture experts group	Enmin Song;Reza Sotudeh;Said Boussakta;Hong Liu	2002			computer science;parallel computing;wafer;transmission (mechanics);multiview video coding	Embedded	12.952686145406213	38.468130077583425	36728
49dfdecb7c05f0d860b280987f2aaa9fda97ac89	embedded system synthesis under memory constraints	hardware software codesign;distributed heterogeneous embedded system;hardware software cosynthesis;embedded system;embedded systems;system on chip;embedded systems genetic algorithms hardware software codesign;genetic algorithm;genetic algorithms;embedded system memory management genetic algorithms hardware processor scheduling information technology time factors microprocessors bandwidth embedded computing;ly cos cosynthesis system genetic algorithm embedded system synthesis memory constraints	This paper presents a genetic algorithm to solve the system synthesis problem of mapping a time constrained single-rate system specification onto a given heterogeneous architecture which may contain irregular interconnection structures. The synthesis is performed under memory constraints, that is, the algorithm takes into account the memory size of processors and the size of interface buffers of communication links, and in particular the complicated interplay of these. The presented algorithm is implemented as part of the LYcos cosynthesis system.	central processing unit;embedded system;genetic algorithm;interconnection	Jan Madsen;Peter Bjørn-Jørgensen	1999		10.1145/301177.301526	embedded system;parallel computing;real-time computing;computer science	EDA	1.3060331129404776	52.956119889349544	36743
1161a0ea0f256f92e78336b585dac2f148bdf364	embedding of tori and grids into twisted cubes	mesh embedding;disjoint meshes;embedding;algoritmo paralelo;disjoint hypercubes;hypercube;hypercube embedding;nombre entier;parallel algorithm;implementation;linear time algorithm;disjoint tori;algorithme temps lineaire;torus embedding;maillage;68wxx;interconnection network;algorithme parallele;unit;multi dimensional;algorithm;integer;celdarada;or algorithm;toro;torus;informatique theorique;tore;twisted cubes;entero;68r10;plongement;algorithme qr;interconnection networks;grid pattern;inmersion;implementacion;47a20;red interconexion;68w10;unite;unidad;computer theory;reseau interconnexion;informatica teorica;hipercubo	The hypercube is one of the most popular interconnection networks since it has a simple structure and is easy to implement. An n-dimensional twisted cube, TQn, is an important variation of hypercube Qn and preserves many of its desirable properties. The problem of how to embed a family of disjoint meshes (or tori) into a host graph has attracted great attention in recent years. However, there is no systematic method proposed to generate the desired meshes and tori in TQn. In this paper, we develop two systematic linear time algorithms for embedding disjoint multi-dimensional tori into TQn, n ≥ 7, as follows: (1) for a positive integer m with b n 2 c ≤ m ≤ n − 4, a family of 2 m disjoint k-dimensional tori of size 2s1 × 2s2 × · · · × 2sk each can be embedded with unit dilation, where k ≥ 2 and ∑k i=1 si ≤ n − m, and (2) for a positive integer m with 2 ≤ m ≤ n − 5, a family of 2 m disjoint k-dimensional tori of size 2s1 × 2s2 × · · · × 2sk each can be embedded with unit dilation, where k ≥ 2, si ≥ 2, ∑k i=1 si ≤ n−m, andmax1≤i≤k{si} ≥ n− 2m. Moreover, we also provide similar embedding results formeshes and hypercubes. Our resultsmean that a family of torus-structured (mesh-structured, or hypercube-structured) parallel algorithms can be executed on the same twisted cube efficiently and in parallel. © 2010 Elsevier B.V. All rights reserved.	dilation (morphology);embedded system;interconnection;klee–minty cube;olap cube;parallel algorithm;time complexity;twisted;twisted pair;unstructured grid	Pao-Lien Lai;Chang-Hsiung Tsai	2010	Theor. Comput. Sci.	10.1016/j.tcs.2010.06.029	integer;combinatorics;unit;topology;torus;embedding;mathematics;geometry;parallel algorithm;implementation;hypercube;algebra	Theory	23.825335166571318	35.67013143734244	36764
96fafcd81de9c5c5d8abc48c2238cfa756cdb314	exploiting cloud computing for algorithm development	meshless methods;algorithm development;software architecture cloud computing computational electromagnetics program verification;photonic crystals;program verification;qa75 electronic computers computer science;computer architecture;accuracy;software architecture;turnaround time reduction cloud computing algorithm development computational electromagnetics cloud based architecture verification process hardware utilisation;cloud computing computer architecture hardware algorithm design and analysis software algorithms accuracy mathematical model;mathematical model;software algorithms;computational electromagnetics;meshless methods cloud computing algorithm development photonic crystals;algorithm design and analysis;tk electrical engineering electronics nuclear engineering;cloud computing;hardware;qa76 computer software	We consider the application of cloud computing to the process of algorithm development. We introduce a case study focusing on the development of a novel algorithm in computational electromagnetics, illustrating several challenging areas for algorithm developers where cloud-based architectures can deliver enhanced productivity and potentially save costs. The development, verification and tuning of our algorithm have all been assisted by cloud-based technologies. Our preliminary results both demonstrate the potential of the algorithm to solve the problems accurately, and of cloud-based architectures to accelerate the development and verification process. We propose that cloud-based architectures will in the future play a greater role in the development of algorithms; saving costs by improving hardware utilisation, and reducing turnaround time.	algorithm;cloud computing;computational electromagnetics;computer;image resolution;simulation;speedup;total cost of ownership	Neil S. O'Brien;Steven J. Johnston;Elizabeth E. Hart;Kamal Djidjeli;Simon J. Cox	2011	2011 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery	10.1109/CyberC.2011.60	embedded system;computational science;algorithm design;software architecture;photonic crystal;cloud computing;computer science;meshfree methods;theoretical computer science;operating system;mathematical model;accuracy and precision;computational electromagnetics;statistics	HPC	0.2736523847499982	45.72870622709413	36775
205cb9825303d7bc0fcace1193bd2bf09d64c222	analysis of parallel sub-swarm pso with the same total particle numbers	sub swarm;particle number particle swarm optimization parallel processing sub swarm;particle swarm optimization;slave processor parallel subswarm pso total particle numbers parallelization particle swarm optimization computational tasks;particle swarm optimization algorithm design and analysis computer science clustering algorithms approximation algorithms parallel algorithms;particle number;parallel processing;particle swarm optimisation parallel algorithms	In this paper, we re-visit the parallel PSO schemes and adopt a different perspective to parallelize PSO. In the adopted scheme, the original particle swarm is divided into several sub-swarms under the condition that the total particle numbers are the same, thus keeping nearly the same computational tasks. Each slave processor then individually runs a part of the sub-swarms. Experiments are also conducted to show the property and the performance of the scheme.	computation;experiment;parallel computing;particle swarm optimization	Tzung-Pei Hong;Jui-Chi Chen;Wen-Yang Lin;Chun-Hao Chen	2015	2015 18th International Conference on Network-Based Information Systems	10.1109/NBiS.2015.92	parallel processing;mathematical optimization;multi-swarm optimization;parallel metaheuristic;computer science;theoretical computer science;machine learning;particle swarm optimization;metaheuristic;particle number	HPC	-0.10713205258971803	40.68580233656134	36831
2628c1349231caee6bb88884f5bca1347f048b16	divide and concatenate: a scalable hardware architecture for universal mac	message authentication code;optimization technique;simulation;reactions;cell;biology;hardware architecture;cryptographic algorithm;hash function;reconfigurable hardware	We present a cryptographic architecture optimization technique called divide-and-concatenate based on two observations: (i) the area of a multiplier and associated data path decreases quadratically and their speeds increase gradually as their operand size is reduced. (ii) in hash functions, message authentication codes and related cryptographic algorithms, two functions are equivalent if they have the same collision probability property. In the proposed approach we divide a 2w-bit data path into two w-bit data paths and concatenate their results to construct an equivalent 2w-bit data path. We applied this technique on NH hash. When compared to the 100% overhead associated with duplicating a straightforward 32-bit pipelined NH hash data path, the divide-and-concatenate approach yields a 94% increase in throughput with only 40% hardware overhead. The NH hash associated message authentication code UMAC architecture with collision probability 2-32 that uses four equivalent 8-bit divide-and-concatenate NH hash data paths yields a throughput of 79.2 Gbps with only 3840 FPGA slices when implemented on a Xilinx FPGA.	32-bit;8-bit;algorithm;concatenation;cryptography;data rate units;field-programmable gate array;hash function;mathematical optimization;message authentication code;nethack;operand;overhead (computing);scalability;the 100;throughput;umac	Bo Yang;Ramesh Karri;David A. McGrew	2003		10.1145/968280.968353	message authentication code;cell;embedded system;security of cryptographic hash functions;double hashing;parallel computing;hash function;perfect hash function;chemical reaction;merkle tree;umac;reconfigurable computing;sha-2;collision resistance;computer science;theoretical computer science;operating system;hash chain;hash-based message authentication code;hardware architecture;distributed computing;rolling hash;cryptographic hash function;fowler–noll–vo hash function;mdc-2;swifft;hash tree	Arch	9.541996524108947	45.72811914003969	36854
04ba127cc1f1269fabbc7a63247d6a976571fea0	brief announcement: decidable graph languages by mediated population protocols	communication graph;graph decision mediated population;constant size set;impossibility result;disconnected communication graph;graph language;brief announcement;decidable graph language;population protocol model;protocol model;mediated population protocol	communication graph;graph decision mediated population;constant size set;impossibility result;disconnected communication graph;graph language;brief announcement;decidable graph language;population protocol model;protocol model;mediated population protocol		Ioannis Chatzigiannakis;Othon Michail;Paul G. Spirakis	2009		10.1007/978-3-642-04355-0_24	discrete mathematics;directed graph;null graph;simplex graph;mathematics;voltage graph;distance-hereditary graph;distributed computing;complement graph;algorithm	Logic	16.754043560189295	35.05638687656344	36855
e79d5e09980e6f954c415003758fdec91ffe9546	utilizing on-chip resources for testing embedded mixed-signal cores	system on a chip;chip;mixed signal test;soc test;embedded system test	For mixed-signal cores on System-on-a-Chip (SoC) platforms, the current methodology in test development is to use special test modes for block isolation such that mixed-signal cores are accessible from the chip boundary through a well-defined interface. Since the access mechanism to the core is preserved, this method facilitates fast test development when the core is re-used on another SoC. In order to obtain the shortest per-device test times on low-cost test platforms, we explore the option of operating the SoC in its designed functional mode where all on-chip resources are fully available for test support. We demonstrate this new method for a microcontroller with embedded ADCs. For high-volume products, the ultimate target is to minimize test costs by maximizing the efficiency of testing multiple devices in parallel on one tester. We demonstrate two benefits of testing in a functional mode that increases parallel test efficiency: (1) Simultaneous testing of multiple on-chip cores, and (2) On-chip post-processing to reduce the amount of test data.	central processing unit;data acquisition;embedded system;microcontroller;mixed-signal integrated circuit;speedup;system on a chip;test data;video post-processing;virtual data room	Carsten Wegener;Heinz Mattes;Stéphane Kirmser;Frank Demmerle;Sebastian Sattler	2009	J. Electronic Testing	10.1007/s10836-009-5118-2	chip;system on a chip;embedded system;real-time computing;computer hardware;telecommunications;computer science;engineering;automatic test pattern generation;test compression	EDA	11.102325193018645	54.34487578372009	36916
b4b89ff5be7b0ee8ec46d2b9581e6edcba74cfb1	hardware adaptation for multimedia application case study: augmented reality	software;system on chip augmented reality field programmable gate arrays multimedia computing;three dimensional displays computer architecture field programmable gate arrays hardware augmented reality multimedia communication software;computer architecture;three dimensional displays;multimedia communication;field programmable gate arrays;augmented reality;xilinx ml 507 embedded system 3d application partially dynamic reconfiguration pdr augmented reality ar profiling;partially dynamic reconfiguration feature embedded electronic multimedia systems augmented reality soc system on chip adaptation technique xilinx fpga 3d application;hardware	Embedded electronic multimedia systems to a wide emerged in recent years. These systems are more complex and diversified. The SoC (System on Chip) must respect many important constraints. It must operate in extreme conditions (network problems, limited energy consumption...). In this paper, we present an adaptation technique based on the dynamic reconfiguration features of Xilinx FPGA. The purpose of this technique is to reduce the computing time of a complex multimedia application by adding a variable number of hardware computing units. A case study is presented on a real system using the Xilinx design environment to develop a 3D application. This used application is the augmented reality (AR).	augmented reality;embedded system;field-programmable gate array;system on a chip	Tarek Frikha;Nader Ben Amor;Khaled Lahbib;Jean-Philippe Diguet;Mohamed Abid	2014	2014 6th International Conference of Soft Computing and Pattern Recognition (SoCPaR)	10.1109/SOCPAR.2014.7008042	embedded system;augmented reality;real-time computing;computer hardware;computer science;field-programmable gate array	EDA	3.1382653333583495	53.57860043450542	36935
a05ce5a9d8a25a10341858f32b70c8d7f7442195	a hierarchical environment for interactive test engineering	graphical user interfaces vlsi interactive systems hierarchical systems integrated circuit testing automatic testing logic testing redundancy design for testability;design for testability;hierarchical design system;circuit faults;fault simulation;hierarchical circuit specifications;design engineering;hierarchical environment;interactive test engineering;circuit testing system testing power engineering and energy circuit faults visualization automatic test pattern generation algorithm design and analysis design engineering automatic testing computer science;automatic test pattern generation;automatic testing;hierarchical systems;module specific test strategies;power engineering and energy;visualization;graphical user interfaces;redundancy;logic testing;integrated circuit testing;vlsi;system testing;test generation;flexible test tools;circuit testing;functionality;computer science;redundancy identification;interactive systems;algorithm design and analysis;redundancy identification hierarchical environment interactive test engineering test generation fault simulation hit system hierarchical design system hierarchical circuit specifications visualization flexible test tools module specific test strategies functionality;hit system	Co’nventional tools for test generation and fault simulat ion appear t o the test engineer as black boxes which neiither communicate their results in a convenient way, nor G I ~ ~ O W for any interactive guidance by the test engineer. In contrast, the H I T system presented in this paper support,$ interactive test engineering, thus combining the power of gate level test generation algorithms with the high level knowledge of the test engineer. Since the HIT system has been integrated into a hierarchical design system (CADIC), the results of test tools can be visualized at the hierarchical’ circuit specifications given by the designer. Based on this visualization, the critical, untestable areas of the circuit can be easily located. Additionally, the test engineer is supplied with flexible test tools, which allow to actively guide the test development process. Thus, module specific test strategies can be applied or high level knowledge aboul the functionality of the overall circuit can be ‘communicated’ to speed-up test generation and redundancy identificdion. An application example shows that with simple strategies for initeractive test engineering the results of test getirerat ion can be improved dramatically.	algorithm;black box;data redundancy;high-level programming language;test engineer	Thomas Burch;Joachim Hartmann;Günter Hotz;M. Krallmann;U. Nikolaus;Sudhakar M. Reddy;Uwe Sparmann	1994		10.1109/TEST.1994.527988	reliability engineering;embedded system;algorithm design;computer architecture;simulation;visualization;computer science;automatic test pattern generation;design for testing;graphical user interface;very-large-scale integration;redundancy;system testing;test case;test management approach;computer engineering;test harness	SE	11.890186386137652	51.138849257321496	37035
1d764279a581a8dd856046e556e5bb0ff588b9ff	on hamiltonian properties of unidirectional hypercubes	fault hamiltonicity;interconnection networks;unidirectional hypercubes	The study of fault Hamiltonicity is an important topic in studying the structures of interconnection networks. Indeed, many advanced results have been obtained for undirected interconnection networks. However, much less is known for the directed counterparts. In the note, we consider the fault Hamiltonicity for unidirectional hypercubes.	hamiltonian (quantum mechanics)	Chun-Nan Hung;Eddie Cheng;Tao-Ming Wang;Lih-Hsing Hsu	2015	Inf. Process. Lett.	10.1016/j.ipl.2015.01.006	theoretical computer science;distributed computing;computer network	DB	23.867001744286313	35.103629979324	37102
61962c7c7fd6625d686c2f3c6da707f5a14721f9	an effective hybrid test data compression method using scan chain compaction and dictionary-based scheme	very large scale integration hybrid test data compression method scan chain compaction fixed length index dictionary based compression scheme ate multiple internal scan chains soc test;test application time;fixed length index dictionary based compression scheme;data compression;decoding;very large scale integration;test data compression;automatic test equipment;testing;vlsi automatic test equipment data compression integrated circuit testing integrated logic circuits logic testing system on chip;chip;ate;indexes;hybrid test data compression method;logic gates;compaction;system on chip;indexation;dictionaries;test data compression compaction circuit testing logic testing circuit faults decoding system testing very large scale integration electronic equipment testing data engineering;logic testing;integrated circuit testing;vlsi;full scan circuit;compression ratio;integrated logic circuits;soc test;scan chain compaction;multiple internal scan chains;dictionary based compression scheme;dictionary based compression scheme test data compression test application time full scan circuit scan chain compaction	In this paper, we propose a new test data compression method for reducing test data volume and test application time. The proposed method consists of two steps: scan chain compaction and dictionary-based compression scheme. The scan chain compaction provides a minimum scan chain depth by using compaction of the compatible scan cells in the scan chain. The compacted scan chain is partitioned to the multiple internal scan chains for using the fixed-length index dictionary-based compression scheme that provides the high compression ratio and the fast testing time. The proposed compression method delivers compressed patterns from the ATE to the chip and drives a large number of multiple internal scan chains using only a single ATE input and output. Experimental results for the ISCAS-89 test benches show that the test data volume and testing time for the proposed method are less than previous compression schemes.	clock signal;data compaction;data compression ratio;data dictionary;fastest;handshaking;input/output;test data;turing test	Taejin Kim;Sunghoon Chun;YongJoon Kim;Myung-Hoon Yang;Sungho Kang	2008	2008 17th Asian Test Symposium	10.1109/ATS.2008.58	embedded system;electronic engineering;scan chain;computer hardware;telecommunications;computer science;engineering;test compression;very-large-scale integration;engineering drawing;statistics	EDA	19.82954603181752	52.62056268413239	37109
41ea6ef773148652b1389e85aec55f05ed6e8afa	distributed dynamic bdd reordering	verification;circuit cad binary decision diagrams;design automation;reordering;reordering time;distributed computing algorithms performance experimentation verification model checking bdd reordering;performance;bdd;distributed computing;application time;distributed dynamic bdd reordering;binary decision diagrams;model checking;sifting algorithm;experimental model;algorithms;distribution dynamics;circuit cad;binary decision diagrams data structures boolean functions application software distributed computing design automation runtime computer applications heuristic algorithms scalability;experimentation;sifting algorithm distributed dynamic bdd reordering reordering time application time rudell algorithm;rudell algorithm	Dynamic BDD reordering is usually a computationally-demanding process, and may slow down BDD-based applications. We propose a novel algorithm for distributing this process over a number of computers, improving both reordering time and application time. Our algorithm is based on Rudell's popular sifting algorithm, and takes advantage of a few empirical observations we make regarding Rudell's algorithm. Experimental results show the efficiency and scalability of our approach, when applied within an industrial model checker.	algorithm;computer;model checking;scalability	Ziv Nevo;Monica Farkash	2006	2006 43rd ACM/IEEE Design Automation Conference	10.1145/1146909.1146969	model checking;parallel computing;verification;electronic design automation;performance;computer science;theoretical computer science;algorithm	EDA	1.4743368995966017	41.140554700355985	37225
288f5d268d6d3c5b546f972847014db6a612575d	software-controlled fault tolerance	reliability;fault tolerant;software controlled fault tolerance;perforation;fault detection;profitability	Traditional fault-tolerance techniques typically utilize resources ineffectively because they cannot adapt to the changing reliability and performance demands of a system. This paper proposes software-controlled fault tolerance, a concept allowing designers and users to tailor their performance and reliability for each situation. Several software-controllable fault-detection techniques are then presented: SWIFT, a software-only technique, and CRAFT, a suite of hybrid hardware/software techniques. Finally, the paper introduces PROFiT, a technique which adjusts the level of protection and performance at fine granularities through software control. When coupled with software-controllable techniques like SWIFT and CRAFT, PROFiT offers attractive and novel reliability options.	fault tolerance;swift (programming language)	George A. Reis;Jonathan Chang;Neil Vachharajani;Ram Rangan;David I. August;Shubhendu S. Mukherjee	2005	TACO	10.1145/1113841.1113843	fault tolerance;parallel computing;real-time computing;simulation;computer science;operating system;reliability;fault detection and isolation;software fault tolerance;profitability index	Arch	5.124906731045543	58.5175362083675	37242
89ce0e11276fbc99936a81d0c18c7ca6e469c4b1	a graph-theoretical approach to boolean interpolation of non-boolean functions	graph theory;interpolation;boolean functions nonboolean function boolean interpolation graph theoretical methods nonboolean function approximation boolean algebra graph minimal chromatic decompositions graph vertices;boolean function;optimal interpolation;function approximation boolean algebra graph theory interpolation;boolean algebra;function approximation;interpolation boolean algebra boolean functions mathematics circuits	We introduce a graph-theoretical approach to the study of approximation of non-Boolean functions on Boolean algebra. We show that optimal interpolations of non-Boolean functions by Boolean functions are linked to minimal chromatic decompositions of graphs attached to these functions and we study special vertices in these graphs.	algorithm;approximation;boolean algebra;boolean circuit;graph theory;independent set (graph theory);interpolation;maximal independent set;maximal set;table (information)	Sergiu Rudeanu;Dan A. Simovici	2004	Proceedings. 34th International Symposium on Multiple-Valued Logic	10.1109/ISMVL.2004.1319949	boolean algebra;boolean circuit;and-inverter graph;combinatorics;circuit minimization for boolean functions;mathematical analysis;discrete mathematics;reed–muller expansion;boolean network;boolean domain;boolean expression;function approximation;product term;interpolation;standard boolean model;computer science;graph theory;maximum satisfiability problem;karp–lipton theorem;stone's representation theorem for boolean algebras;boolean algebras canonically defined;mathematics;boolean function;complete boolean algebra;binary decision diagram;algorithm;two-element boolean algebra;free boolean algebra;parity function;algebra	Logic	22.835182951030646	37.5815724250994	37257
401e226c0bde2ecc09e290050a8f2e6a15a19933	a fixed point exponential function accelerator for a neuromorphic many-core system		Many models of spiking neural networks heavily rely on exponential waveforms. On neuromorphic multiprocessor systems like SpiNNaker, they have to be approximated by dedicated algorithms, often dominating the processing load. Here we present a processor extension for fast calculation of exponentials, aimed at integration in the next-generation SpiNNaker system. Our implementation achieves single-LSB precision in a 32bit fixed-point format and 250Mexp/s throughput at 0.44nJ/exp for nominal supply (1.0V), or 0.21nJ/exp at 0.7V supply and 77Mexp/s, demonstrating a throughput multiplication of almost 50 and 98% energy reduction at 2% area overhead per processor on a 28nm CMOS chip.	32-bit;approximation algorithm;artificial neural network;cmos;computation;exptime;fixed point (mathematics);hardware acceleration;least significant bit;mpsoc;manycore processor;maximum throughput scheduling;multiprocessing;neuromorphic engineering;neuron;overhead (computing);prototype;spinnaker;spiking neural network;synapse;time complexity	Johannes Partzsch;Sebastian Höppner;Matthias Eberlein;René Schüffny;Christian Mayr;David R. Lester;Steve B. Furber	2017	2017 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2017.8050528	mpsoc;spiking neural network;chip;parallel computing;throughput;computer science;neuromorphic engineering;real-time computing;fixed point;multiprocessing;cmos	Arch	4.831823805174851	42.836184228819526	37269
0d26d19e0a1fe1ccbfe1c5a2410391aada45d566	dapr: design automation for partially reconfigurable fpgas.	design automation;partial reconfiguration	reconfiguration (PR) enhances traditional FPGA-based high-performance reconfigurable computing by providing additional benefits such as reduced area and memory requirements, increased performance, and increased functionality. However, since leveraging these additional benefits requires specific designer expertise, which increases design time, PR has not yet gained widespread usage. Even though Xilinx's PR design flow significantly eases PR design, to fully leverage PR benefits designers require extensive PR design flow knowledge, as well as low-level architectural details of the target FPGA device. In this paper, we present a PR design flow and associated tool to automate PR design intricacies and design space exploration. Our design flow and tool can significantly reduce PR design time effort and make PR designs more accessible and amenable to a wider range of PR designers.	algorithm;american and british english spelling differences;automation;bitstream;cordic;clock rate;dicom;design closure;design flow (eda);design space exploration;fast fourier transform;fastest;field-programmable gate array;floorplan (microelectronics);high- and low-level;iteration;iterative method;pareto efficiency;production rule representation;rapid prototyping;reconfigurable computing;requirement;xilinx ise	Shaon Yousuf;Ann Gordon-Ross	2010			embedded system;computer architecture;real-time computing	EDA	2.4163845978428786	50.056635662135406	37278
4edecd7026b2eb2497da37e862530616b309edd1	exploring the use of gpus in constraint solving		This paper presents an experimental study aimed at assessing the feasibility of parallelizing constraint propagationwith particular focus on arc-consistencyusing Graphical Processing Units (GPUs). GPUs support a form of data parallelism that appears to be suitable to the type of processing required to cycle through constraints and domain values during consistency checking and propagation. The paper illustrates an implementation of a constraint solver capable of hybrid propagations (i.e., alternating CPU and GPU), and demonstrates the potential for competitiveness against sequential implementations.	constraint satisfaction problem;graphics processing unit	Federico Campeotto;Alessandro Dal Palù;Agostino Dovier;Ferdinando Fioretto;Enrico Pontelli	2014		10.1007/978-3-319-04132-2_11	mathematical optimization;parallel computing;computer science;theoretical computer science	Logic	-3.776135010410443	42.211491887183435	37309
1300ff92155b6e3b9cea479da30f48706c4a56f6	analysis of multigrid algorithms on massively parallel computers: architectural implications	equation derivee partielle;fixed cost;iterative method;algoritmo paralelo;equation differentielle;partial differential equation;ecuacion derivada parcial;estacion trabajo;interconnection;parallel algorithm;algorithm performance;computer systems design;algorithm analysis;three dimensions;cost function;optimization technique;station travail;reseau ordinateur;massively parallel processors;differential equation;finite difference;funcion coste;parallel computation;interconnection network;computer networks;algorithme parallele;computer network;paralelismo masivo;ecuacion diferencial;metodo iterativo;workstation;calculo paralelo;multiprocessing computers;finite difference theory;efficient implementation;structured grids mathematics;massively parallel computer;community networks;resultado algoritmo;methode iterative;interconnexion;interprocessor communication;performance algorithme;red ordenador;parallel computer;message passing;multigrid methods;communication cost;parallel processing computers;fonction cout;performance prediction;analyse algorithme;procesador;processeur;iteration method;architecture computers;calcul parallele;parallelisme massif;fine grain parallelism;massive parallelism;analisis algoritmo;processor;real time operation;interconeccion	demands of the problem. We study an important class of algorithms, multigrid methods, with the intent of better understanding how the demands of multigrid applications can be met by the current generation of massively parallel computers. Multigrid methods, a set of techniques for accelerating the convergence of iterative processes, can potentially allow the solution of a broad and rich set of scientific applications. They accomplish this by speeding the propagation of information across the domains of physical problems. Instead of iterating to termination accuracy on a fine grid, multigrid methods move computation within a hierarchy of grids. The grid hierarchy has a pyramidal structure in which each coarser grid increases the distance between points by a small constant factor. When multigrid methods are used with explicit iterative schemes every point on each grid can be computed concurrently within an approximation sweep. While this data independence creates a natural domain parallelism, there is a data dependence between the grids which forces the computation to traverse the grid hierarchy serially. In addition to this high degree of natural parallelism, there are several other reasons that these algorithms are compelling candidates for massively parallel execution. First, they are widely used and they work. Applications as diverse as the design of vehicles including aircraft, spacecraft, boats, and cars, the design of structures, weather prediction, and problems in oceanography, astrophysics, molecular biology, cosmology, and geology can potentially use these methods to solve problems which would otherwise be computationally intractable. Second, they have a virtually unlimited demand for computational resources. Multigrid applications are often part of a design process requiring both high accuracy and rapid solution to effectively support interactive techniques. The current generation of massively parallel computers exhibits a convergence towards a common set of architectural characteristics. This convergence allows us to study the class of machines as a whole to determine whether the class can present efficient platforms for the solution of various tasks. Here we use this convergence to study these platforms for the solution of multigrid algorithms. Our approach is to develop a set of models of parallel computaJOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING 33, 33–43 (1996) ARTICLE NO. 0022	algorithm;approximation;computation;computational complexity theory;computational resource;computer;data dependency;distributed computing;iterative method;multigrid method;parallel computing;software propagation;traverse	Lesley R. Matheson;Robert E. Tarjan	1996	J. Parallel Distrib. Comput.	10.1006/jpdc.1996.0022	parallel computing;computer science;theoretical computer science;operating system;massively parallel;distributed computing;data parallelism;iterative method;algorithm	HPC	-3.5062937293473078	35.88716147218387	37338
8e73159bea626d8c2ce1623efffc137db1f55fd3	a cabac encoding core with dynamic pipeline for h.264/avc main profile	80 mbits s;binary codes;cabac;adaptive codes;avc;video coding;arithmetic codes;video coding adaptive codes arithmetic codes binary codes video codecs;150 mhz;data dependence;0 35 micron;h 264;encoder core;video codecs;pipeline cabac h 264 architecture;bit wise processing;architecture;encoding pipelines automatic voltage control context modeling arithmetic throughput delay frequency estimation codecs data analysis;context based adaptive binary arithmetic coding;pipeline;0 35 micron context based adaptive binary arithmetic coding cabac encoder core bit wise processing pipeline latency reduction h 264 avc 80 mbits s 150 mhz;pipeline latency reduction	This paper presents an encoder core architecture for context-based adaptive binary arithmetic coding (CABAC) in H.264/AVC main profile. The throughput of CABAC encoder is usually limited due to i) bit-wise processing, ii) complicated data dependency, and iii) variant iteration times for each binary symbol. This paper adopts dynamic pipeline scheme to improve the performance of CABAC encoder. The characteristics of CABAC algorithm are utilized to reduce pipeline latency. Meanwhile, pipeline bypass scheme is applied to eliminate the possible memory conflict. Proposed encoder core is implemented under ROHM 0.35mum technology. Results show that the equivalent gate counts is 4.57k when the maximum frequency is 150MHz. It is estimated that the proposed CABAC encoding core can process the input binary symbol at a bit-rate of 80Mb/s	algorithm;binary number;context-adaptive binary arithmetic coding;data dependency;encoder;h.264/mpeg-4 avc;intel core (microarchitecture);iteration;throughput	Lingfeng Li;Yang Song;Takeshi Ikenaga;Satoshi Goto	2006	APCCAS 2006 - 2006 IEEE Asia Pacific Conference on Circuits and Systems	10.1109/APCCAS.2006.342119	parallel computing;real-time computing;computer science;theoretical computer science;architecture;context-adaptive variable-length coding;pipeline	EDA	12.614084428186683	39.827546547407856	37367
ee6c8b73108c7281a067946b0d9db559e5801a22	overview of carbon-based circuits and systems	cmos integrated circuits;carbon nanotubes;nano technology;comunicacion de congreso;nanoteknik;graphene;logic gates;integrated circuit modeling;field effect transistors;graphene devices carbon nanotube field effect transistors carbon nanotubes;elektro och systemteknik;c carbon based circuits carbon based systems carbon nanotubes graphene transistors analog circuits digital circuits carbon based field effect devices main sizing parameters cutting edge integrated circuits;graphene integrated circuit modeling carbon nanotubes cmos integrated circuits field effect transistors logic gates;electrical engineering	This paper presents an overview of the state of the art on carbon-based circuits and systems made up of carbon nanotubes and graphene transistors. A tutorial description of the most important devices and their potential benefits and limitations is given, trying to identify their suitability to implement analog and digital circuits and systems. Main electrical models reported so far for the design of carbon-based field-effect devices are surveyed, and the main sizing parameters required to implement such devices in practical integrated circuits are analyzed. The solutions proposed by cutting-edge integrated circuits and devices are discussed, identifying current trends, challenges and opportunities for the circuits and systems community.	analog signal;boolean algebra;cmos;classical nucleation theory;digital data;digital electronics;flexible electronics;graphene;integrated circuit;radio frequency;sensor;static random-access memory;transistor;very-large-scale integration	Saul Rodriguez Duenas;Ana Rusu;Jose M. de la Rosa	2015	2015 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2015.7169296	field-effect transistor;electronic engineering;electronics;carbon nanotube;logic gate;engineering;graphene;cmos	EDA	12.397741183658184	57.8806055512061	37383
988cd01fabd87c145efd70e9871a7127975dc14e	high-level automatic pipelining for sequential circuits	cycle time;formal specification;and forward;sequential circuits;stall;formal specification sequential circuits pipeline processing parallel algorithms;forward;speculation;optimizations sequential circuits critical path speculation stalling forwarding clock cycle time pipelined specification pipelining algorithm;critical path;modular;pipeline;pipeline processing sequential circuits throughput laboratories computer science prototypes synthesizers clocks logic circuits permission;pipeline processing;parallel algorithms	This paper presents a new approach for automatically pipelining sequential circuits. The approach repeatedly extracts a computation from the critical path, moves it into a new stage, then uses speculation to generate a stream of values that keep the pipeline full. The newly generated circuit retains enough state to recover from incorrect speculations by flushing the incorrect values from the pipeline, restoring the correct state, then restarting the computation.We also implement two extensions to this basic approach: stalling, which minimizes circuit area by eliminating speculation, and forwarding, which increases the throughput of the generated circuit by forwarding correct values to preceding pipeline stages. We have implemented a prototype synthesizer based on this approach. Our experimental results show that, starting with a non-pipelined or insufficiently pipelined specification, this synthesizer can effectively reduce the clock cycle time and improve the throughput of the generated circuit.	clock signal;computation;critical path method;pipeline (computing);prototype;speculative execution;throughput	Maria-Cristina V. Marinescu;Martin C. Rinard	2001		10.1145/500001.500053	parallel computing;real-time computing;computer science;theoretical computer science;pipeline	EDA	5.99024579938113	54.27898321508031	37429
f0c3f202b07ac0f0c54bb3fe1a346ad11ff09e4e	low power logic for statistical inference	probability programming language;fault tolerant;generic model;programming language;gibbs sampling;stochastic circuits;stochastic circuits probabilistic graphical model generative model probability programming language belief propagation markov chain monte carlo gibbs sampling device physics;bayesian methods;probabilistic graphical model;statistical physics;computational modeling;low power;logic gates;markov chain monte carlo;belief propagation;probability distribution;fault tolerance;mathematical model;statistical inference;generative model;logic gates mathematical model bayesian methods computational modeling probabilistic logic monte carlo methods algorithm design and analysis;computer application;probabilistic logic;device physics;hardware implementation;algorithm design and analysis;monte carlo methods;vlsi architecture	Efficient hardware implementations of statistical inference continue to grow in importance for a wide range of computing applications. While CPU cycles are increasingly being used for statistical inference, transistors are also becoming increasingly statistical. For implementing statistical algorithms, could it be that statistical electronic substrates are a feature rather than a bug?  We show that inference models can often be built from local constraints, and explain the gate-level mathematical functions required for the resulting inference solver. We suggest that signals should consist of probabilistic populations of particles representing samples from a probability distribution, with gate functions acting to transform these ensembles. Using this mapping from statistical physics to statistical inference, we present Bayesian logic circuits as highly efficient alternatives to digital standard cell libraries. Novel VLSI architectures based on Bayesian logic circuits promise to consume orders of magnitude less power and silicon area compared to conventional digital processors.	algorithm;central processing unit;inference engine;library (computing);logic gate;microprocessor;population;solver;standard cell;transistor;very-large-scale integration	Benjamin Vigoda;David Reynolds;Jeffrey Bernstein;Theophane Weber;Bill Bradley	2010	2010 ACM/IEEE International Symposium on Low-Power Electronics and Design (ISLPED)	10.1145/1840845.1840918	fault tolerance;computer science;theoretical computer science;machine learning;algorithmic inference;probabilistic logic network;statistics	Arch	16.331576157229932	46.54239915237581	37443
a89e998c0336e285f865d87d45b4e2db5809ab07	nanoprism: a tool for evaluating granularity vs. reliability trade-offs in nano architectures	triple modular redundant;fault tolerant;prism;nanotechnology;probabilistic model checking;granularity;defect tolerant architecture;defect tolerance;tmr;ctmr	It is expected that nano-scale devices and interconnections will introduce unprecedented level of defects, noise and interferences in the substrates. This consideration motivates the search for new architectural paradigms based on redundancy based defect-tolerant designs. However, redundancy is not always a solution to the reliability problem, and often too much or too little redundancy may cause lack of reliability. The key challenge is in determining the granularity at which defect tolerance is designed, and the level of redundancy to achieve optimal reliability. Various forms of redundancy such as NAND multiplexing, Triple Modular Redundancy (TMR), Cascaded Triple Modular Redundancy (CTMR) have been considered in the fault-tolerance literature. Also, redundancy has been applied at different levels of granularity, such as gate level, logic block level, logic function level, unit level etc. The questions we try to answer in this paper is what level of granularity and what redundancy levels result in optimal reliability for specific architectures. In this paper, we extend previous work on evaluating reliability-redundancy trade-offs for NAND multiplexing to granularity vs. redundancy vs. reliability trade-offs for other redundancy mechanisms, and present our automation mechanism using the probabilistic model checking tool PRISM. We illustrate the power of this automation by pointing out certain anomalies of these trade-offs which are counter intuitive and can only be obtained by designers through automation, thereby providing better insight into defect-tolerant design decisions.	boolean algebra;complex network;fault tolerance;gnu nano;library (computing);logic block;logic gate;markov chain;model checking;nand gate;prism (surveillance program);redundancy (engineering);software bug;statistical model;triple modular redundancy;wavelength-division multiplexing	Debayan Bhaduri;Sandeep K. Shukla	2004		10.1145/988952.988980	logic redundancy;triple modular redundancy;dual modular redundancy;reliability engineering;embedded system;fault tolerance;electronic engineering;real-time computing;granularity;computer science;engineering;theoretical computer science;operating system;prism;redundancy	EDA	10.355812429650735	59.165630080261955	37490
ffb2c86883e1d8ac89f334e4e33c4da01ae99b19	intelligent network-on-chip with online reinforcement learning for portable hd object recognition processor	cmos integrated circuits;object recognition;reinforcement learning application specific integrated circuits network on chip object recognition;network on chip;resource allocation;bandwidth allocation;object recognition feature extraction high definition video bandwidth learning artificial intelligence streaming media;resource allocation bandwidth allocation cmos integrated circuits feature extraction learning artificial intelligence multiprocessing systems network on chip object recognition;feature extraction;voltage 1 2 v intelligent reinforcement learning network on chip heterogeneous manycore processor portable hd object recognition rl noc bandwidth adjustment resource allocation feature detection cmos process hd video stream size 65 nm power 235 mw frequency 200 mhz;multiprocessing systems;learning artificial intelligence	An intelligent Reinforcement Learning (RL) Network-on-Chip (NoC) is proposed as a communication architecture of a heterogeneous many-core processor for portable HD object recognition. The proposed RL NoC automatically learns bandwidth adjustment and resource allocation in the heterogeneous many-core processor without explicit modeling. By regulating the bandwidth and reallocating cores, the throughput performances of feature detection and description are increased by 20.4% and 11.5%, respectively. As a result, the overall execution time of the object recognition is reduced by 38%. The proposed processor with RL NoC is implemented in a 65 nm CMOS process, and it successfully demonstrates the real-time object recognition for a 720 p HD video stream while consuming 235 mW peak power at 200 MHz, 1.2 V.	cmos;central processing unit;explicit modeling;feature detection (computer vision);feature detection (web development);intelligent network;manycore processor;network on a chip;outline of object recognition;performance;real-time clock;reinforcement learning;router (computing);run time (program lifecycle phase);scheduling (computing);shortest job next;streaming media;throughput	Junyoung Park;Injoon Hong;Gyeonghoon Kim;Byeong-Gyu Nam;Hoi-Jun Yoo	2014	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2013.2284188	embedded system;parallel computing;real-time computing;feature extraction;resource allocation;computer science;cognitive neuroscience of visual object recognition;network on a chip;cmos;bandwidth allocation	Robotics	3.835437218146341	44.468073934309736	37497
16d1b1474c9560e9a1de3df11373bd257bc96e9f	efficient implementation of floating-point reciprocator on fpga	computers;look up table;binomial expansion;reciprocator;double precision;fpga;look up tables;floating point reciprocator;fpga implementation;accuracy;efficient implementation;partial block multipliers floating point reciprocator fpga single precision floating point numbers double precision floating point numbers look up tables;field programmable gate arrays hardware signal processing algorithms delay floating point arithmetic table lookup very large scale integration costs signal synthesis dynamic range;floating point;approximation methods;double precision floating point numbers;floating point arithmetic;field programmable gate arrays;table lookup field programmable gate arrays floating point arithmetic;magnetic cores;table lookup;binomial expansion floating point arithmetic reciprocator fpga double precision partial block multipliers;partial block multipliers;single precision floating point numbers;hardware	In this paper we have presented an efficient FPGA implementation of a  reciprocator for both IEEE single-precision and double-precision  floating point numbers. The method is based on the use of look-up tables and partial block multipliers. Compared with previously reported work, the modules occupy less area with a higher performance and less latency. The designs trade off either 1 unit in last-place (ulp) or 2 ulp of accuracy (for double or single precision respectively), without rounding, to obtain a better implementation. Rounding can also be added to the design to restore some accuracy at a slight cost in area.	block cipher;double-precision floating-point format;field-programmable gate array;least significant bit;lookup table;perturbation theory;pipeline (computing);requirement;rounding;signal processing;single-precision floating-point format	Manish Kumar Jaiswal;Nitin Chandrachoodan	2009	2009 22nd International Conference on VLSI Design	10.1109/VLSI.Design.2009.12	electronic engineering;parallel computing;lookup table;computer hardware;computer science;floating point;operating system;field-programmable gate array	EDA	12.240505578492504	44.36389527493569	37536
6e7c0f475ecbaf7031dac6b49f902457782aeaf7	a negative-overhead, self-timed pipeline	timing uncertainty;power supplies;fabrication;logic simulation;multiplier;pulsed power supplies;latency negative overhead self timed pipeline wave pipelining surfing timing uncertainty timing pulse spice simulations logic elements delays multiplier fabrication parameter variation power supply noise;logic simulation pipeline processing timing delays multiplying circuits integrated circuit noise spice;multiplying circuits;logic design;uncertainty;fabrication parameter variation;logic element;noise robustness;surfing;logic gates;negative overhead self timed pipeline;power supply noise;timing pipeline processing logic design delay uncertainty logic gates pulsed power supplies noise robustness fabrication power supplies;timing pulse;spice simulations;logic elements;latency;integrated circuit noise;spice;wave pipelining;pipeline processing;delays;timing	This paper presents a novel variation of wave pipelining that we call “surfing.” In previous wave pipelined designs, timing uncertainty grows monotonically as events propagate through gates or other logic elements. We bound this dispersion by propagating a timing pulse along with the data values. Our logic elements have delays that are smaller in the presence of the pulse than in its absence. This produces a “surfing” effect: events are bound in close proximity to the timing pulse. We demonstrate this approach with the design of a 4 12 multiplier. Spice simulations from the extracted layout indicate that this design is robust in the presence of fabrication parameter variation and power supply noise. Because timing is maintained by accelerating the logic, our designs achieve lower latency than their purely combinational equivalents. Thus, the control overhead for these designs is indeed negative. Appeared in Proceedings of Eighth International Symposium on Advanced Research in Asynchronous Circuits and Systems, April 2002.	combinational logic;critical path method;electronic circuit;experiment;handshaking;modulation;noise shaping;overhead (computing);pipeline (computing);pipelines;power supply;pulse shaping;robustness (computer science);spice;simulation;software propagation	Mark R. Greenstreet;Brian D. Winters	2002		10.1109/ASYNC.2002.1000294	embedded system;electronic engineering;real-time computing;engineering	EDA	19.64721452272526	56.679817296944535	37581
8da7c11023cf4f53cdda32085032b2036b246d03	variable sampling window flip-flop for low-power application	power supplies;signal sampling low power electronics flip flops cmos logic circuits integrated circuit noise;switching activity;clocks;signal sampling;flip flops;inverters;low power;clock swing reduced flip flop;variable sampling window flip flop;clock swing reduced flip flop variable sampling window flip flop low power application robustness latching operation power consumption input noise rejection;energy consumption;cmos logic circuits;digital systems;voltage;low power electronics;low power application;robustness;input noise rejection;power delay product;power consumption;sampling methods;sampling methods flip flops clocks energy consumption digital systems inverters power supplies voltage timing delay;integrated circuit noise;low power consumption;high power;latching operation;flip flop;timing	This paper describes novel flip-flops for achieving improved robustness and low power consumption. Variable sampling window flip-flop (VSWFF) improves robustness during latching operation by varying the width of the sampling window according to input data. It also reduces overall power consumption for higher input switching activities, and provides shorter hold time and better input noise rejection. Clock swing-reduced variable sampling window flip-flop (CSR-VSWFF) further reduces the power consumption by allowing use of a small swing clock. As compared to conventional reduced clock swing flip-flops, CRS-VSWFF requires no extra high power supply voltage. The simulation results indicate that VSWFF significantly improves robustness during latching operation with 10% reduction on the maximum power consumption, while CSR-VSWFF improves the power-delay product by about 64% as compared to the conventional flip-flops.	flops;flip-flop (electronics);low-power broadcasting;sampling (signal processing)	Sang-Dae Shin;Hun Choi;Bai-Sun Kong	2003		10.1109/ISCAS.2003.1206247	embedded system;sampling;electronic engineering;real-time computing;voltage;computer science;engineering;low-power electronics;robustness	EDA	17.679008145198885	56.84005366701035	37607
7ea5c6ccf5e6817fb728985aa9ff2def1110d32b	new integer-fft multiplication architectures and implementations for accelerating fully homomorphic encryption		This paper proposes a new hardware architecture of Integer-FFT multiplier for super-size integer multiplications. Firstly, a basic hardware architecture, with the feature of low hardware cost, of the Integer-FFT multiplication algorithm using the serial FFT architecture, is proposed. Next, a modified hardware architecture with a shorter multiplication latency than the basic architecture is presented. Thirdly, both architectures are implemented, verified and compared on the Xilinx Virtex-7 FPGA platform using 256, 512, 1024, 2048 and 8192 point Integer-FFT algorithm respectively with multiplication operands ranging from bits to bits in size. Experimental results show that the hardware cost of the proposed architecture is no more than 1/10 of the prior FPGA solution, and is perfectly within the implementable range of the Xilinx Virtex-7 FPGA platform, and outperforms the software implementations of the same bit-length operand multiplication on the Core-2 Q6600 and Core-i7 870 platforms. Finally, the proposed implementations are employed to evaluate the super-size multiplication in an encryption primitive of fully homomorphic encryption (FHE) over the integers. The analysis shows that the speed improvement factor is up to 26.2 compared to the corresponding integer-based FHE software implementation on the Core-2 Duo E8400 platform.	bit-length;field-programmable gate array;homomorphic encryption;list of intel core 2 microprocessors;multiplication algorithm;operand;processor register	Xiaolin Cao;Ciara Moore	2013	IACR Cryptology ePrint Archive		aes implementations;implementation;fast fourier transform;parallel computing;theoretical computer science;multiplication;homomorphic encryption;computer science;integer	Arch	9.224976120411494	44.49258881361635	37714
fa9fcb79ef3765d6762c44d6da1bb6d0abec21c4	tram: a design methodology for high-performance, easily testable, multimegabit ram's	design for testability;random access memory;design methodology read write memory automatic testing random access memory degradation failure analysis partitioning algorithms design for testability very large scale integration charge transfer;degradation;reliability;architecture systeme;metodologia;memoria acceso directo;integrated circuit;multimegabit dynamic rams;reliability tram design methodology high performance multimegabit dynamic rams testability performance yield;very large scale integration;integrated memory circuits;automatic testing;performance;layout problem;circuit vlsi;probleme agencement;conception;circuito integrado;charge transfer;methodologie;yield;testability;interconnection network;chip;failure analysis;vlsi circuit;tram;memoire acces direct;random access storage integrated memory circuits;random access memory ram;diseno;problema disposicion;random access storage;design;arquitectura sistema;read write memory;circuito vlsi;methodology;system architecture;high performance;divide and conquer;circuit integre;partitioning algorithms;design methodology	An architecture is proposed for multimegabit dynamic RAMs (random-access memories) that achieves higher testability and performance than the conventional four-quadrant RAMs. Applying the principle of divide and conquer, the RAM is partitioned into modules, each appearing as the leaf node of a binary interconnect network. Such a network carries the address/data/control bus, permitting the nodes to communicate among themselves as well as with the outside world. This architecture is shown to be easily testable. Parallelism in testing and partial self-test result in a large savings of testing time; the savings is independent of the test algorithm used. Unlike other testability schemes, this approach promises improved performance with only a small increase in chip area. It is also shown that the architecture is easily partionable and restructurable, with potential for yield and reliability improvement. >	random-access memory	Najmi T. Jarwala;Dhiraj K. Pradhan	1988	IEEE Trans. Computers	10.1109/12.5985	chip;testability;embedded system;design;yield;failure analysis;parallel computing;degradation;performance;computer science;electrical engineering;operating system;integrated circuit;methodology;reliability;algorithm;systems architecture	EDA	13.827899546113413	50.3184481987386	37733
e17ea4a595519ef6fd19f9bbdaa344948d065a81	efficient algorithms for exact two-level hazard-free logic minimization	libraries;graph theory;compacted state graphs;hazard constraints;single cube cover algorithms;extended burst mode fsm synthesis;generalized c element implementations;logic minimization methods hazards circuit synthesis libraries jacobian matrices space exploration cities and towns integrated circuit synthesis asynchronous circuits;efficient algorithm;minimisation of switching nets;logic;integrated logic circuits minimisation of switching nets asynchronous circuits logic cad finite state machines graph theory hazards and race conditions;two level hazard free logic minimization;hazards;divide and merge algorithm;space exploration;minimization methods;indexing terms;finite state machines;state graph exploration;asynchronous logic synthesis;finite state machine synthesis;hazards and race conditions;cover tables;cities and towns;asynchronous circuits;integrated circuit synthesis;integrated logic circuits;logic cad;jacobian matrices;asynchronous logic synthesis two level hazard free logic minimization extended burst mode fsm synthesis finite state machine synthesis hazard constraints generalized c element implementations two level standard gate implementations state graph exploration single cube cover algorithms compacted state graphs cover tables divide and merge algorithm;finite state machine;two level standard gate implementations;circuit synthesis	This paper presents a new approach to two-level hazard-free logic minimization in the context of extended burst-mode finite-state machine synthesis. The approach achieves fast single-output logic minimization that yields solutions that are exact in the number of literals. This paper presents algorithms and hazard constraints targeting both generalized C-element and two-level standard gate implementations. The logic minimization approach presented in this paper is based on state graph exploration in conjunction with single-cube cover algorithms. The algorithm achieves fast logic minimization by using compacted state graphs, cover tables, and a divide-and-merge algorithm for efficient single output minimization. The exact two-level hazard-free logic minimizer presented in this paper finds a minimal number of literal solutions and is several orders of magnitude faster than existing literal exact methods for the largest benchmarks available to date. This includes a benchmark that has never been possible to solve exactly in number of literals before.	benchmark (computing);c-element;circuit minimization for boolean functions;finite-state machine;literal (computer programming);literal (mathematical logic);merge algorithm	Hans M. Jacobson;Chris J. Myers	2002	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/TCAD.2002.804103	embedded system;mathematical optimization;electronic engineering;discrete mathematics;logic optimization;computer science;theoretical computer science;space exploration;mathematics;finite-state machine;logic;algorithm	EDA	16.852415416759403	48.256105560488436	37800
173ecf280ef87d67521582afa241e4e69819df87	a high throughput/gate aes hardware architecture by compressing encryption and decryption datapaths - toward efficient cbc-mode implementation		This paper proposes a highly efficient AES hardware architecture that supports both encryption and decryption for the CBC mode. Some conventional AES architectures employ pipelining techniques to enhance the throughput and efficiency. However, such pipelined architectures are frequently unfit because many practical cryptographic applications work in the CBC mode, where block-wise parallelism is not available for encryption. In this paper, we present an efficient AES encryption/decryption hardware design suitable for such block-chaining modes. In particular, new operation-reordering and register-retiming techniques allow us to unify the inversion circuits for encryption and decryption (i.e., SubBytes and InvSubBytes) without any delay overhead. A new unification technique for linear mappings further reduces both the area and critical delay in total. Our design employs a common loop architecture and can therefore efficiently perform even in the CBC mode. We also present a shared key scheduling datapath that can work on-the-fly in the proposed architecture. To the best of our knowledge, the proposed architecture has the shortest critical path delay and is the most efficient in terms of throughput per area among conventional AES encryption/decryption architectures with tower-field S-boxes. We evaluate the performance of the proposed and some conventional datapaths by logic synthesis results with the TSMC 65-nm standard-cell library and NanGate 45and 15-nm open-cell libraries. As a result, we confirm that our proposed architecture achieves approximately 53–72% higher efficiency (i.e., a higher bps/GE) than any other conventional counterpart.	block cipher mode of operation;critical path method;datapath;encryption;key schedule;library (computing);logic synthesis;overhead (computing);parallel computing;pipeline (computing);retiming;s-box;scheduling (computing);standard cell;symmetric-key algorithm;throughput;unification (computer science);whirlpool (cryptography)	Rei Ueno;Sumio Morioka;Naofumi Homma;Takafumi Aoki	2016	IACR Cryptology ePrint Archive	10.1007/978-3-662-53140-2_26	embedded system;parallel computing;computer hardware	Arch	9.433602430673734	45.11894753439637	37802
15e07d60d8364df6a863aa5c6d1e63475bd08a28	minimization of fractional wordlength on fixed-point conversion for high-level synthesis	fixed-point conversion;floating-point variable;word length;high-level synthesis;hardware synthesis;speed optimization;fractional wold length;huge data;fractional wordlength minimization problem;data flow graph;optimization method;bit length;floating point;fixed point;high level language;fixed point arithmetic;nonlinear programming;high level synthesis;non linear programming	In the hardware synthesis from high-level language such as C, bit length of variables is one of the key issues on the area and speed optimization. Usually, designers are required to specify the word length of each variable manually, and verify the correctness by the simulation on huge data. In this paper, we propose an optimization method of fractional wold length of floating-point variables in the floating to fixed-point conversion of variables. The amount of round-off erros are formulated with parameters and propagated via data flow graphs. The non-linear programming is used to solve the fractional wordlength minimization problem. The method does not require the simulation on huge data, and is very fast compared to ones based on the simulation. We have shown the effect on several programs.	bit-length;correctness (computer science);dataflow;fixed point (mathematics);high- and low-level;high-level programming language;high-level synthesis;linear programming;mathematical optimization;nonlinear programming;nonlinear system;round-off error;simulation	Nobuhiro Doi;Takashi Horiyama;Masaki Nakanishi;Shinji Kimura	2004	ASP-DAC 2004: Asia and South Pacific Design Automation Conference 2004 (IEEE Cat. No.04EX753)	10.1145/1015090.1015111	arithmetic;mathematical optimization;nonlinear programming;computer science;floating point;theoretical computer science;operating system;data-flow analysis;mathematics;fixed point;fixed-point arithmetic;high-level synthesis;programming language;q;high-level programming language;algorithm	EDA	6.86041506470984	46.985226380054534	37809
3d38777043586adfce244e41a69800ddd347b075	zambezi: a parallel pattern parallel fault sequential circuit fault simulator	parallel pattern simulator;multiple faults simulation;multiple vectors zambezi parallel pattern simulator parallel fault simulation sequential circuit fault simulator multiple faults simulation;fault simulation;multiple vectors;sequential circuits;sequential circuit fault simulator;vlsi fault diagnosis logic testing sequential circuits circuit analysis computing integrated logic circuits parallel algorithms;circuit faults sequential circuits circuit simulation computational modeling logic testing computer simulation acceleration parallel processing computer science circuit testing;logic testing;control flow;parallel fault simulation;vlsi;integrated logic circuits;circuit analysis computing;zambezi;fault diagnosis;parallel algorithms	Sequential circuit fault simulators use the multiple bits in a computer data word to accelerate simulation. We introduce, and implement, a new sequential circuit fault simulator, a parallel pattern parallel fault simulator, ZAMBEZI, which simultaneously simulates multiple faults with multiple vectors in one data word. ZAMBEZI is developed by enhancing the control flow, of existing parallel pattern algorithms. For a very wide range of benchmark circuits, compared to parallel fault and parallel pattern simulators, ZAMBEZI offers either the best, or very close to the best, uniprocessor performance. ZAMBEZI also offers superior performance when parallelized.	fault simulator;sequential logic;simulation	Minesh B. Amin;Bapiraju Vinnakota	1996		10.1109/VTEST.1996.510890	electronic engineering;parallel computing;real-time computing;computer science;stuck-at fault;theoretical computer science;sequential logic;parallel algorithm;very-large-scale integration;control flow	EDA	19.995159525827606	49.70702061461093	37818
799d411b32951b4403da7e608c9bc25f17dd80f0	image processing library for reconfigurable computers (abstract only)	field programmable gate array;image processing;process capability;reconfigurable computing;chip;general purpose processor;parallelism;speculation;design and implementation;parallel systems;networking;reconfigurable hardware	Reconfigurable Computers (RCs) are parallel systems that are designed around multiple general-purpose processors and multiple field programmable gate array (FPGA) chips. These systems can leverage the synergism between conventional processors and FPGAs to provide low-level hardware functionality at the same level of programmability as general-purpose computers. RCs have proposed very high processing capabilities for computationally intensive applications such as Image Processing. This is due to the inherently parallel operation paradigm of the FPGA hardware.In this paper we present the design and implementation of image processing kernels for RCs. This library of kernels have been tested and verified for performance on one of the state-of-the-art reconfigurable computers, SRC-6E. This paper shows that RCs are between 8 to 400 times faster than comparable Pentiums for image based tasks.	central processing unit;computer;field-programmable gate array;general-purpose markup language;high- and low-level;image processing;programming paradigm;reconfigurable computing	Mohamed Taher;Esam El-Araby;Tarek A. El-Ghazawi;Kris Gaj	2005		10.1145/1046192.1046256	embedded system;computer architecture;parallel computing;computer hardware;image processing;reconfigurable computing;computer science;operating system	Arch	2.77665432813558	47.050252119752294	37832
502015d60a078da746ba01b5de36d7435066bb8e	an evaluation of the parallel shift-and-invert lanczos method	distributed data;general and miscellaneous mathematics computing and information science;eigenvalues;linear system;eigenvalue;distributed data processing eigenvalue;lanczos method;evaluation;99 general and miscellaneous mathematics computing and information science;parallel processing	When the Lanczos method is used to compute eigenvalues, it is often restarted or used with the shift-and-invert scheme. The restarted scheme usually uses less memory but the shift-andinvert scheme is more robust. In addition, the shiftand-invert Lanczos method requires accurate solutions of a series of linear systems. Parallel software packages suitable for these linear systems are only started to become available. In this talk, we will present our evaluation of two such packages and brie y exam when it is necessary to use the shift-		Kesheng Wu;Horst D. Simon	1999			computational science;parallel processing;mathematical optimization;parallel computing;lanczos algorithm;eigenvalues and eigenvectors;lanczos approximation;computer science;theoretical computer science	Visualization	-2.815027504146145	38.383530243782424	37848
41841875e4f4d576c57c7506e138c65249a0f01b	resistive memory device requirements for a neural algorithm accelerator	write noise standard deviation resistive memory device requirements neural algorithm accelerator dramatic energy reductions general purpose neural architecture backpropagation read noise standard deviation;neuromorphic computing resistive memory memristor backpropagation neural networks noise;storage management chips backpropagation neural net architecture resistive ram;analog digital conversion backpropagation digital analog conversion memory management acceleration laboratories	Resistive memories enable dramatic energy reductions for neural algorithms. We propose a general purpose neural architecture that can accelerate many different algorithms and determine the device properties that will be needed to run backpropagation on the neural architecture. To maintain high accuracy, the read noise standard deviation should be less than 5% of the weight range. The write noise standard deviation should be less than 0.4% of the weight range and up to 300% of a characteristic update (for the datasets tested). Asymmetric nonlinearities in the change in conductance vs pulse cause weight decay and significantly reduce the accuracy, while moderate symmetric nonlinearities do not have an effect. In order to allow for parallel reads and writes the write current should be less than 100 nA as well.	algorithm;artificial neural network;backpropagation;computer data storage;conductance (graph);crossbar switch;dixon's factorization method;experiment;image noise;network architecture;neural networks;nonlinear system;numerical analysis;requirement;resistive random-access memory;resistive touchscreen;speculative execution;white noise	Sapan Agarwal;Steven J. Plimpton;David R. Hughart;Alexander H. Hsia;Isaac Richter;Jonathan A. Cox;Conrad D. James;Matthew J. Marinella	2016	2016 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2016.7727298	parallel computing;computer hardware;computer science	ML	4.314395653837833	42.234843356735524	37881
44c9165bad8fdad244b0492481c21954319de3ab	node normalization and decomposition in low power technology mapping	switching activity;boolean functions;binary decision diagrams node normalization algorithm node decomposition algorithm low power technology mapping cmos technology circuit power dissipation netlist switching activity reduction robdd uncorrelated signal probabilities reduced order bdd;high level synthesis;integrated circuit design;cmos technology energy consumption circuit synthesis delay permission switching circuits circuit simulation data structures boolean functions signal synthesis;low power;combinational circuits cmos logic circuits circuit cad high level synthesis boolean functions integrated circuit design;decomposition algorithm;cmos logic circuits;power dissipation;circuit cad;technology mapping;combinational circuits	In CMOS technology the decomposition of the nodes of a circuit can signi cantly reduce the circuit power dissipation. We present a normalization algorithm which extracts the largest nodes of the given netlist. Then we examine a known node decomposition algorithm and propose a new one which is provable optimal and tractable for moderate node sizes. Reduction of the overall switching activity on standard benchmark circuits is shown for exact (ROBDD) and uncorrelated signal probabilities.	algorithm;benchmark (computing);binary decision diagram;cmos;cobham's thesis;database normalization;netlist;provable security	Winfried Nöth;Reiner Kolla	1997		10.1145/263272.263351	equivalent circuit;boolean circuit;circuit minimization for boolean functions;electronic engineering;real-time computing;adiabatic circuit;asynchronous circuit;computer science;dissipation;theoretical computer science;combinational logic;boolean function;circuit extraction;high-level synthesis;integrated circuit design	EDA	16.500434819583894	48.83313762654293	37883
251a7be2515c5dcaee487cab282f444a57a7dde9	ams and rf design for reliability methodology	stress;reliability engineering;oscillations;digitally controlled oscillator;degradation;design for reliability concept;oscillation frequency;phase noise;reliability methodology;circuit design;radiofrequency oscillators circuit reliability nor circuits;aging;ams circuit design;circuit reliability;nor circuits;integrated circuit modeling;rf circuit design;radiofrequency oscillators;nor interpolative digital controlled oscillator;power consumption;integrated circuit reliability;digital circuits;radio frequency design methodology degradation circuit synthesis digital circuits digital control digital controlled oscillators phase noise energy consumption aging;f osc ageing degradation reliability methodology digital circuits design for reliability concept rf circuit design ams circuit design nor interpolative digital controlled oscillator oscillation frequency power consumption;f osc ageing degradation	The design for reliability concept is already in use on digital circuits, but not systematically in use on AMS or RF circuits. A reliable circuit design demands knowledge of the physical degradation and models to analyze the reliability in earlier stages. Also, it needs to be simple enough to be used on the redesign. In this work, we propose and validate an AMS and RF circuit design for reliability method. In order to investigate our method, we have designed a 5–3 NOR interpolative Digital Controlled Oscillator (DCO) near 1 GHz applications. This design example has presented 1.4% decrease of oscillation frequency, 0.2% decrease of phase noise for a 1 MHz off-set, and 2.1% decrease of power consumption after 10 years of degradation. According with the trends presented in Table I, we estimate that the fosc ageing degradation was improved of 13 % by applying the design for reliability method.	circuit design;device configuration overlay;digital electronics;elegant degradation;phase noise;rf modulator;radio frequency;reliability engineering;software aging;spatial variability;verilog-ams	Pietro Maris Ferreira;Hervé Petit;Jean-François Naviner	2010	Proceedings of 2010 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2010.5537771	control engineering;electronic engineering;degradation;engineering;electrical engineering;circuit design;stress;digitally controlled oscillator;oscillation;phase noise;digital electronics	EDA	21.156022435671506	57.49501539945665	37894
1e5edd7ea7642694c7383ae9fc368cb3393d0ff0	enabling fpga support in matlab based heterogeneous systems	kernel;fluid dynamics application fpga support matlab based heterogeneous systems cpu gpu absolute performance energy efficiency cholesky decomposition matrix inversion fft computational requirements application development hardware kernel implementation low level hardware logic high speed network processor connection parallel code generation performance gains compute intensive application medical imaging;parallelising compilers computational fluid dynamics fast fourier transforms field programmable gate arrays graphics processing units mathematics computing matrix inversion medical image processing multiprocessing systems parallel processing;graphics processing units;field programmable gate arrays;field programmable gate arrays graphics processing units kernel matlab instruction sets hardware;matlab;instruction sets;hardware	FPGAs have been shown to provide orders of magnitude improvement over CPUs and GPUs in terms of absolute performance and energy efficiency for various kernels such as Cholesky decomposition, matrix inversion, and FFT among others. Despite this, the overall performance of many applications suffer when implemented entirely in FPGAs. Combining FPGAs with CPUs and GPUs provides the range of capabilities needed to support diverse computational requirements of applications. Integrating FPGAs into these systems challenges application developers with constructing hardware kernel implementations and interfacing from the low level hardware logic in the FPGA to the high speed networks that connect processors in the system. In this work we extend the compute capabilities of Matlab by incorporating support for FPGAs and automating the parallel code generation. We characterize the system and evaluate the performance gains that can be achieved by adding the FPGA for two compute intensive applications. We present performance results for medical imaging and fluid dynamics applications implemented in a CPU+GPU+FPGA system and achieved up to 40× improvement compared to the standard Matlab CPU+GPU environment.	algorithm;central processing unit;cholesky decomposition;code generation (compiler);debugging;electronic hardware;fast fourier transform;field-programmable gate array;graphics processing unit;high-level programming language;kernel (operating system);matlab;medical imaging;numerical analysis;parallel computing;requirement	Sam Skalicky;Tyler Kwolek;Sonia López;Marcin Lukowiak	2014	2014 International Conference on ReConFigurable Computing and FPGAs (ReConFig14)	10.1109/ReConFig.2014.7032515	embedded system;computer architecture;parallel computing;kernel;reconfigurable computing;computer science;theoretical computer science;operating system;instruction set;field-programmable gate array	HPC	0.1982700993253604	45.79706216938674	37920
b010366e2e9142087fd8812e239bf9cf51129579	a provably good algorithm for the two module routing problem	construccion modular;concepcion asistida;hierarchical system;evaluation performance;circuito lsi;computer aided design;optimisation;interconnection;performance evaluation;optimizacion;integrated circuit;wire routing;routing;evaluacion prestacion;systeme hierarchise;probleme np dur;circuito integrado;algorithme;algorithm;sistema jerarquizado;algorritmo;lsi circuit;construction modulaire;worst case performance bound;interconnexion;conception assistee;channel routing;optimization;encaminamiento;modular construction;np hard;circuit lsi;circuit integre;interconeccion;acheminement	"""In the Mead-Conway design methodology for LSI, modules are designed and then connected by wires to form larger modules in a hierarchical fashion. It would be helpful to have a design aid that would do the routing automatically and be guaranteed of coming within some fixed percentage of the size of an optimal routing. With this goal in mind, we investigate the problem of routing two-terminal nets between two modules of the same width but possibly different heights, assuming that the sides are aligned vertically. The terminals may lie on any of the sides of either module. Wires must be routed according to the """"Manhattan"""" reserved-layer model, in which all wires must lie on a rectilinear grid, and wires running the same direction must be separated by at least unit distance. Finding an optimal routing for this problem is NP-hard, where the measure of performance is the perimeter of the bounding box around the whole routing region. We describe an algorithm whose worst-case performance is asymptotically at most 19/10 times that of an optimal routing. The algorithm runs in O(n log n) time, where n is the number of nets. One of the problems encountered in routing is how to evaluate a routing when the optimal routing is not available for comparison. The techniques given here can be used to calculate lower bounds on the size of an optimal routing. Thus, these techniques may be useful in evaluating routings produced by methods other than the algorithm in this paper. Key words, wire routing, channel routing, NP-hard, worst-case performance bound"""	algorithm;best, worst and average case;channel router;conway's game of life;integrated circuit;mind;minimum bounding box;perimeter;regular grid;routing (electronic design automation)	Brenda S. Baker	1986	SIAM J. Comput.	10.1137/0215012	routing table;mathematical optimization;routing;combinatorics;static routing;simulation;equal-cost multi-path routing;computer science;destination-sequenced distance vector routing;integrated circuit;interconnection;computer aided design;np-hard;hierarchical control system;routing;algorithm	Theory	18.905767513798274	38.16131074788462	37924
1f17c4bc64685b2797e614d2aaf6c76b5a2ac990	energy- and time-efficient matrix multiplication on fpgas	field programmable gate array;evaluation performance;matrix multiplication field programmable gate arrays integrated circuit design logic design;performance evaluation;image processing;dissipation energie;logic design;performance estimation algorithm design configurable hardware energy delay tradeoff field programmable gate array fpga linear array matrix multiplication;performance estimation;evaluacion prestacion;barreta lineal;linear array;energy efficient matrix multiplication;punto caliente;barrette lineaire;energy dissipation;configurable hardware;red puerta programable;field programmable gate array fpga;design space;reseau porte programmable;linear array energy efficient matrix multiplication time efficient matrix multiplication field programmable gate array energy hot spots energy profiling system wide energy dissipation fpga device configurable hardware energy delay tradeoff;hot spot;algorithme;etat actuel;algorithm;integrated circuit design;energy performance;field programmable gate arrays delay energy dissipation algorithm design and analysis signal processing algorithms signal processing mobile computing image processing predictive models hardware;signal processing;fpga device;state of the art;point chaud;estado actual;performance prediction;predictive models;disipacion energia;matrix multiplication;temps retard;energy delay tradeoff;delay time;energy hot spots;power consumption;field programmable gate arrays;consommation energie electrique;signal processing algorithms;time efficient matrix multiplication;mobile computing;energy profiling;tiempo retardo;algorithm design;algorithm design and analysis;system wide energy dissipation;hardware;algoritmo	"""We develop new algorithms and architectures for matrix multiplication on configurable devices. These have reduced energy dissipation and latency compared with the state-of-the-art field-programmable gate array (FPGA)-based designs. By profiling well-known designs, we identify """"energy hot spots"""", which are responsible for most of the energy dissipation. Based on this, we develop algorithms and architectures that offer tradeoffs among the number of I/O ports, the number of registers, and the number of PEs. To avoid time-consuming low-level simulations for energy profiling and performance prediction of many alternate designs, we derive functions to represent the impact of algorithm design choices on the system-wide energy dissipation, area, and latency. These functions are used to either optimize the energy performance or provide tradeoffs for a family of candidate algorithms and architectures. For selected designs, we perform extensive low-level simulations using state-of-the-art tools and target FPGA devices. We show a design space for matrix multiplication on FPGAs that results in tradeoffs among energy, area, and latency. For example, our designs improve the energy performance of state-of-the-art FPGA-based designs by 29%-51% without any increase in the area-latency product. The latency of our designs is reduced one-third to one-fifteenth while area is increased 1.9-9.4 times. In terms of comprehensive metrics such as Energy-Area-Time, our designs exhibit superior performance compared with the state-of-the-art by 50%-79%."""	algorithm design;field-programmability;field-programmable gate array;high- and low-level;matrix multiplication;memory-mapped i/o;performance prediction;simulation	Ju-wook Jang;Seonil B. Choi;Viktor K. Prasanna	2005	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2005.859562	embedded system;algorithm design;electronic engineering;real-time computing;image processing;computer science;operating system;signal processing;mobile computing;algorithm;field-programmable gate array	Arch	0.4082744608405086	54.222649931448444	37925
e1be207e5300250b73903bcafd437fe63d92f08a	an eco routing algorithm for eliminating coupling-capacitance violations	engineering change order eco;optimization strategies;design automation;design process;integrated circuit layout;routing;network routing capacitance circuit layout cad integrated circuit layout integrated circuit noise;coupling capacitance violation elimination;postlayout timing;multiple layer routing design;indexing terms;design optimization;engineering change order eco routing algorithm coupling capacitance violation elimination design process cve problem multiple layer routing design signal wire segments postlayout timing noise analysis optimization strategies;design space;network routing;engineering change order;design rules;eco routing algorithm;cve problem;routing coupling capacitance engineering change order eco;routing algorithm;circuit layout cad;capacitance;time to market;coupling capacitance;noise analysis;integrated circuit noise;signal wire segments;routing rails wires capacitance chaos signal design semiconductor device noise algorithm design and analysis crosstalk	Engineering change order changes are almost inevitable in the late stages of a design process. Based on an existing design, incremental change is favored since it can avoid considerable efforts of redoing the whole process and can minimize the disturbance on the existing converged design. The coupling-capacitance violation elimination (CVE) problem is addressed. Due to the changes in the multiple layer routing design, the total coupling capacitance on some signal wire segments on a layer may be larger than their allowable bounds after postlayout timing/noise analysis. The target is to find a new routing solution without coupling-capacitance violations under certain constraints, which helps to keep the new design close to the original one. This paper proposes a two-stage algorithm to solve CVE problems, and present optimization strategies to speed up the execution. Experimental results demonstrate the efficiency and effectiveness of this algorithm	algorithm;common vulnerabilities and exposures;coupling (electronics);engineering change order;mathematical optimization;routing;speedup	Hua Xiang;Kai-Yuan Chao;Martin D. F. Wong	2006	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2005.857396	routing;electronic engineering;real-time computing;electronic design automation;computer science;engineering;engineering drawing;computer network	EDA	16.217718420996814	52.99950317915356	38012
2e6e1f6d4865eda1f95ef7cfde54f20554857e85	heuristic for two-level cache hierarchy exploration considering energy consumption and performance	evaluation performance;performance evaluation;integrated circuit;evaluacion prestacion;heuristic method;metodo heuristico;cache memory;circuito integrado;antememoria;antememoire;energy consumption;hierarchie memoire;methode heuristique;memory hierarchy;power consumption;consommation energie electrique;jerarquia memoria;circuit integre	In this work is presented an automated method for adjusting two-level cache memory hierarchy in order to reduce energy consumption in embedded applications. The proposed heuristic, TECH-CYCLES (Two-level Cache Exploration Heuristicconsidering CYCLES), consists of making a small search in the space of configurations of the two-level cache hierarchy, analyzing the impact of each parameter in terms of energy and number of cycles spent for a given application. Experiments show an average reduction of about 41% in the energy consumption by using our heuristic when compared with the existing heuristic (TCaT), also for two-level caches. Besides the energy improvement, this method also reduces the number of cycles needed to execute a given application by about 25%. In order to validate the proposed heuristic, twelve benchmarks from the MiBench suite have been used.	cpu cache;heuristic	Abel G. Silva-Filho;Filipe R. Cordeiro;Remy Eskinazi Sant'Anna;Manoel Eusebio de Lima	2006		10.1007/11847083_8	embedded system;cache-oblivious algorithm;parallel computing;real-time computing;cpu cache;computer science;electrical engineering;integrated circuit;cache algorithms;cache pollution;algorithm	EDA	-2.3497120518345804	54.45802936574872	38020
d500beb789970390f62b164d0f93984ab288f98b	tessellation aspect of combinational cellular array testing	logic arrays;tessellation;combinational logic;cellular array;fault detection;logic testing;tessera;cellular array combinational logic fault detection tessera tessellation;fault diagnosis	This paper introduces procedures which enable one to settle the tessellation problem for the class of combinational cellular arrays with each cell having a binary horizontal input and a binary vertical input. Where all input combinations are applicable to all cells in the array in this class, the necessary number of tests is obtained from necessary prime as well as composite tessellations.	combinational logic	Chia-Hsiaing Sung;Clarence L. Coates	1974	IEEE Transactions on Computers	10.1109/T-C.1974.223951	computer science;theoretical computer science;mathematics;tessellation;combinational logic;fault detection and isolation;algorithm	Visualization	22.631879952753998	48.57057175084068	38022
b70acceb6d9f40760f6bcf8fdf61346fd43c6c3f	architectural-level power optimization of microcontroller cores in embedded systems	microcontrollers embedded system macrocell networks cmos technology driver circuits clocks performance loss design automation silicon automotive engineering;switching activity;automotive electronics;microcontrollers;computer aided design;power saving;hardware design languages hdls;power efficiency;system on chip automotive electronics cmos digital integrated circuits embedded systems logic cad microcontrollers;clock gating;embedded system;sensor control architectural level power optimization microcontroller cores embedded systems power saving electronic systems clustered clock gating power efficiency optimal cluster organization cmos digital circuits computer aided design tools automotive electronics sensor interface;microcontrollers automotive electronics cmos embedded systems hardware design languages hdls low power;embedded systems;low power;cmos digital integrated circuits;system on chip;number of clusters;power optimization;hardware design;logic cad;cmos	Power saving is becoming one of the major design drivers in electronic systems embedding microcontroller cores. Known microcontrollers typically save power at the expense of reduced computational capability. With reference to an 8051 core, this paper presents a novel clustered clock gating to increase power efficiency at architectural level without performance loss and preserving the reusability of the macrocell. Different from known clustered-gating strategies where the number of clusters is fixed a priori, the optimal cluster organization is derived, considering both the macrocell complexity and switching activity. When implementing the 8051 core in CMOS technology, the proposed approach leads to a 37% power saving, which is higher than the 29% permitted by automatic-clock-gating insertion in commercial computer-aided design tools or the 10% of state-of-the-art clustered-gating strategies. To assess its full functionality, the power-optimized cell has been proved in silicon that is embedded in an automotive system for sensors interface/control.	cmos;clock gating;computer-aided design;embedded system;intel mcs-51;microcontroller;performance per watt;power optimization (eda);sensor	Sergio Saponara;Luca Fanucci;Pierangelo Terreni	2007	IEEE Trans. Industrial Electronics	10.1109/TIE.2006.885450	system on a chip;microcontroller;embedded system;electronic engineering;real-time computing;electrical efficiency;computer science;engineering;computer aided design;clock gating;cmos;power optimization	EDA	3.5277273790778354	54.993308045599456	38094
507c4365a8e14afde3726da221e6ab03684074c4	design migration from peripheral asic design to area-i/o flip-chip design by chip i/o planning and legalization	modelizacion;area i o flip chip design;microprocessor;diseno circuito;optimisation;arquitectura red;area array flip chip;puce a bosses;haute performance;design migration;optimizacion;integrated circuit;integrated circuit layout;packaging electronico;input output count;microprocessor designs;area array architecture;flip chip devices;circuit design;circuito integrado;packaging consideration;architecture reseau;chip i o planning;deep submicrometer regime;power supply;circuit a la demande;conception circuit integre;chip;input output;algorithme;packaging electronique;modelisation;algorithm;miniaturisation;input output i o planning and legalization;packaging consideration design migration peripheral application specific integrated circuit design area i o flip chip design chip i o planning input output count power delivery problem deep submicrometer regime area array architecture microprocessor designs peripheral bonding design i o planning algorithm;integrated circuit design;i o planning algorithm;custom circuit;integrated circuit bonding;circuito integrato personalizado;peripheral bonding design;alimentation electrique;application specific integrated circuits bonding costs electrostatic discharge technology planning microprocessors load flow load flow analysis packaging integrated circuit technology;application specific integrated circuits;application specific integrated circuit;flip chip;electronic packaging;proceedings paper;peripheral application specific integrated circuit design;power delivery problem;alto rendimiento;implantation circuit integre;optimization;conception circuit;network architecture;microprocesseur;miniaturization;miniaturizacion;electronics packaging;alimentacion electrica;integrated circuit design application specific integrated circuits electronics packaging flip chip devices integrated circuit bonding;modeling;high performance;article;microprocesador;input output i o planning and legalization area array flip chip design migration;circuit integre	Due to higher input/output (I/O) count and power delivery problem in deep submicrometer (DSM) regime, flip-chip technology, especially for area-array architecture, has provided more opportunities for adoption than traditional peripheral bonding design style in high-performance application-specific integrated circuit and microprocessor designs. However, it is hard to tell which technique can provide better design cost edge in usually concerned perspectives. In this paper, we present a methodology to convert a previous peripheral bonding design to an area-I/O flip-chip design. It is based on an I/O buffer modeling and an I/O planning algorithm to legalize I/O buffer blocks with core placement without sacrificing much of the previous optimization in the original core placement. The experimental results have shown that we have achieved better area and I/O wirelength in area-IO flip-chip configuration (especially for pad-limit designs), compared with peripheral bonding configuration in packaging consideration.	algorithm;application-specific integrated circuit;automated planning and scheduling;bump mapping;cluster analysis;flip chip;input/output;mathematical optimization;microprocessor;peripheral;power supply;signal integrity	C.-Y. Chang;H.-M. Chen	2008	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2007.912202	embedded system;electronic engineering;computer science;engineering;electrical engineering;operating system;application-specific integrated circuit;electronic packaging	EDA	18.219436454178155	55.04094635699579	38128
ad76d2568de82cb2aa39c4535617cd3dfd23ee5a	power optimization techniques for associative processors		Abstract The toughness and complexity of the computational problems which human beings tackle rise faster than the computational platforms themselves. Moreover, the dark silicon era negatively effects the traditional computational platforms and contributes unfavorably to this gap. These situations require the alternative computing paradigms, ranging from multi-core CPUs to GPUs and even untraditional paradigms such as in-memory computing. Associative processing (AP) is a promising candidate for in-memory computing where the computation is performed on the memory rows without moving the data. Even though APs propose a good solution for the memory bottleneck, their power density poses an issue because of the huge switching activity on the rows happens during the operations. In this study, we seek a low-power AP implementation by proposing architectural and instructional improvements to decrease the switching activity. The simulations on various benchmarks from different domains show that the proposed low-power AP methods provide energy reduction up to 48% with a negligible impact on the area and performance.	central processing unit;mathematical optimization	Hasan Erdem Yantir;Ahmed M. Eltawil;Smaïl Niar;Fadi J. Kurdahi	2018	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2018.08.006	power density;parallel computing;computational problem;computation;unconventional computing;dark silicon;ranging;computer science;power optimization;bottleneck	EDA	-3.0603872497542715	47.94710184461561	38142
cf9884c45e3ca22051814f952a1b1d942e4a26f2	a hardware/software prototyping system for driving assistance investigations	driver assistance;hardware acceleration;image processing;design flow;fpga;prototyping;system on chip;hardware software system	A holistic design and verification environment to investigate driving assistance systems is presented, with an emphasis on system-on-chip architectures for video applications. Starting with an executable specification of a driving assistance application, subsequent transformations are performed across different levels of abstraction until the final implementation is achieved. The hardware/software partitioning is facilitated through the integration of OpenCV and SystemC in the same design environment, as well as OpenCV and Linux in the run-time system. We built a rapid prototyping, FPGA-based camera system, which allows designs to be explored and evaluated in realistic conditions. Using lane departure and the corresponding performance speedup, we show that our platform reduces the design time, while improving the verification efforts.	64-bit computing;abstraction layer;embedded system;executable;field-programmable gate array;hardware acceleration;holism;image processing;linux;opencv;pixel;principle of abstraction;rapid prototyping;real-time transcription;runtime system;sensor;software architecture;software prototyping;software versioning;speedup;system on a chip;systemc;transmitter	Jakob Anders;Michael Mefenza;Christophe Bobda;Franck Yonga;Zeyad Aklah;Kevin Gunn	2013	Journal of Real-Time Image Processing	10.1007/s11554-013-0351-4	system on a chip;embedded system;computer architecture;real-time computing;hardware acceleration;image processing;computer science;design flow;hardware architecture;prototype;fpga prototype;field-programmable gate array	EDA	3.723971073801773	52.402020863457814	38168
b18757da18e7ffc179dc1014597c79c54c2b669c	a segmented parallel-prefix vlsi circuit with small delays for small segments	vlsi layout parallel prefix circuit;vlsi layout	I present a VLSI circuit for segmented parallel prefix with gate delay O (log S ) and wire delay.	propagation delay;very-large-scale integration	Bradley C. Kuszmaul	2005		10.1145/1073970.1074002	computer architecture;parallel computing;computer science;circuit extraction	EDA	14.63227844705151	50.842999731766405	38194
07a239c69217eea2b48d5da926732fc2721787d8	dynamically reconfigurable multi-processor arrays	spatiotemporal reconfiguration;multi core processor;partial reconfiguration;dynamic reconfiguration;scoc;fpga;multiprocessor array;embedded supercomputing;soc;dataflow supercomputing;parallel processor	In this paper, the FPGA-based Multi-Processor Array architectural form is augmented with capability for hardware partial reconfiguration on processor element I/D-space memory components. Reconfiguration overhead is effectively masked on the process schedule timeline with use of an associated pipelining mechanism. In this manner, a maximally-parallel processing gain may be achieved. The resulting structure is then leveraged in support of a spatiotemporal processing model applicable to arbitrarily large dataflow graphs.	dataflow;field-programmable gate array;multiprocessing;overhead (computing);parallel computing;pipeline (computing);process gain;processor array;reconfigurability;reconfigurable computing;timeline	James Glenn-Anderson	2014	2014 48th Asilomar Conference on Signals, Systems and Computers	10.1109/ACSSC.2014.7094790	computer architecture;parallel computing;real-time computing;computer science	HPC	-0.44027857146807914	50.25117487413509	38202
0b55130fb7a8c4eea212f3d77dde99433da41b05	an updated multilayer printed wiring c-a-d capability	computer program;computer aided design;automatic generation	In 1966 the Communications and Electronics Division of the Philco-Ford Corporation developed a Computer-Aided-Design (C-A-D) Capability for multilayer printed interconnection wiring.1 At that time improvements were planned to enhance the efficiency and usefulness of the series of computer programs involved. Since then additions and changes have been incorporated which allow for the slope classifying of nets, provide data for the automatic generation of artwork, and adapt the series for use on revisions as well as original runs.  This paper addresses itself to describing the scope of these additions and changes, how they were implemented, and the resulting effects on the overall C-A-D capability	computer program;interconnection;printing;routing;subroutine;time complexity;user experience;wiring	Gerald L. Ginsberg;Calvin R. Maurer;E. H. Whitley	1969		10.1145/800260.809022	embedded system;electronic engineering;telecommunications;computer science;engineering;electrical engineering;computer aided design;engineering drawing	EDA	11.051598673519676	51.119868189614515	38248
6b01e0b668737f0b9acdf64ef0ede498d063a4de	accelerating viability kernel computation with cuda architecture: application to bycatch fishery management	dynamic programming;gpu;cuda;fishery management;viability kernel	Computing a viability kernel consumes time and memory resources which increase exponentiallywith the dimension of the problem.This curse of dimensionality strongly limits the applicability of this approach, otherwise promising. We report here an attempt to tackle this problem with Graphics Processing Units (GPU). We design and implement a version of the viability kernel algorithm suitable for General Purpose GPU (GPGPU) computing using Nvidia’s architecture, CUDA (Computing Unified Device Architecture). Different parts of the algorithm are parallelized on the GPU device and we test the algorithm on a dynamical system of theoretical population growth. We study computing time gains as a function of the number of dimensions and the accuracy of the grid covering the state space. The speed factor reaches up to 20 with the GPU version compared to the Central Processing Unit (CPU) version, making the approachmore applicable to problems in 4 to 7dimensions.Weuse theGPUversion of the algorithm to compute viability kernel of bycatch fishery management problems up to 6 dimensions.	algorithm;cuda;central processing unit;computation;curse of dimensionality;dynamical system;general-purpose computing on graphics processing units;graphics processing unit;kernel (operating system);parallel computing;state space	Antoine Brias;Jean-Denis Mathias;Guillaume Deffuant	2016	Comput. Manag. Science	10.1007/s10287-015-0246-x	mathematical optimization;parallel computing;computer science;fisheries management;theoretical computer science;dynamic programming;mathematics	HPC	-4.088606819021511	40.68993563228291	38287
76fb3152c97528dc0505a457966bb9e3b2c6cea4	highly-reliable integer matrix multiplication via numerical packing	single threaded software realization highly reliable integer matrix multiplication numerical packing generic matrix multiply routine gemm memory intensive part compute intensive part information retrieval systems relevance ranking systems object recognition systems transient hardware faults error control coding dual modular redundancy approaches fault detection ecc approaches dmr approaches off the shelf 64 bit floating point gemm routine intel i7 3632qm 2 2ghz processor;multiprocessing systems fault diagnosis floating point arithmetic matrix multiplication microprocessor chips;numerical packing integer matrix multiplication sum of products fault tolerance soft errors;error correction codes circuit faults fault detection fault tolerant systems redundancy;matrix multiplication;multiprocessing systems;floating point arithmetic;fault diagnosis;microprocessor chips	The generic matrix multiply (GEMM) routine comprises the compute and memory-intensive part of many information retrieval, relevance ranking and object recognition systems. Because of the prevalence of GEMM in these applications, ensuring its robustness to transient hardware faults is of paramount importance for highly-efficientlhighly-reliable systems. This is currently accomplished via error control coding (ECC) or via dual modular redundancy (DMR) approaches that produce a separate set of “parity” results to allow for fault detection in GEMM. We introduce a third family of methods for fault detection in integer matrix products based on the concept of numerical packing. The key difference of the new approach against ECC and DMR approaches is the production of redundant results within the numerical representation of the inputs rather than as a separate set of parity results. In this way, high reliability is ensured within integer matrix products while allowing for: (i) in-place storage; (ii) usage of any off-the-shelf 64-bit floating-point GEMM routine; (iii) computational overhead that is independent of the GEMM inner dimension. The only detriment against a conventional (i.e. fault-intolerant) integer matrix multiplication based on 32-bit floating-point GEMM is the sacrifice of approximately 30.6% of the bitwidth of the numerical representation. However, unlike ECC methods that can reliably detect only up to a few faults per GEMM computation (typically two), the proposed method attains more than “12 nines” reliability, i.e. it will only fail to detect 1 fault out of more than 1 trillion arbitrary faults in the GEMM operations. As such, it achieves reliability that approaches that of DMR, at a very small fraction of its cost. Specifically, a single-threaded software realization of our proposal on an Intel i7-3632QM 2.2GHz processor (Ivy Bridge architecture with AVX support) incurs, on average, only 19% increase of execution time against an optimized, fault-intolerant, 32-bit GEMM routine over a range of matrix sizes and it remains more than 80% more efficient than a DMR-based GEMM.	32-bit;64-bit computing;advanced vector extensions;computation;double-precision floating-point format;dual modular redundancy;error detection and correction;fault detection and isolation;in-place algorithm;information retrieval;ivy bridge (microarchitecture);matrix multiplication;numerical analysis;outline of object recognition;overhead (computing);redundancy (engineering);relevance;requirement;run time (program lifecycle phase);set packing;thread (computing)	Ijeoma Anarado;Mohammad Ashraful Anam;Davide Anastasia;Fabio Verdicchio;Yiannis Andreopoulos	2013	2013 IEEE 19th International On-Line Testing Symposium (IOLTS)	10.1109/IOLTS.2013.6604045	embedded system;electronic engineering;parallel computing;real-time computing;matrix multiplication;computer science;floating point;theoretical computer science;operating system;algorithm	Arch	5.761304833616182	58.18115744152637	38315
43f960a9ebe83986beeb2bf5e333595147b58c02	: a parallel algorithm for fault simulation on the connection machine	single fault propagation;c language logic testing automatic testing parallel algorithm fault simulation connection machine data level parallelism single fault propagation;microcontrollers;parallel algorithm;circuit faults;fault simulation;routing;parallel algorithms circuit faults routing circuit simulation computational modeling microcontrollers parallel processing circuit testing computer architecture logic testing;automatic testing;computer architecture;circuit simulation;c language;computational modeling;connection machine;fast algorithm;logic testing;parallel processing automatic testing c language fault location logic testing;circuit testing;parallel processing;data level parallelism;parallel algorithms;fault location	A fast algorithm for fault simulation, using data-level parallelism, is presented for the Connection Machine. The algorithm is of the parallel pattern single-fault-propagation type. An implementation in C language has been completed and results are presented. >	connection machine;parallel algorithm;simulation	Vinod Narayanan;Vijay Pitchumani	1988		10.1109/TEST.1988.207784	parallel processing;computer architecture;parallel computing;computer science;theoretical computer science;parallel algorithm	EDA	20.01247256349809	49.658634816169595	38365
a99133d4b5bd686c1b502db3852aecdcc71918ec	openmp-based synergistic parallelization and hw acceleration for on-chip shared-memory clusters	system on chip embedded systems integrated circuit design multiprocessing systems parallel processing;shared memory clustered architectures hw acceleration mpsocs design flow openmp;accelerator based sw architectures openmp based synergistic parallelization hw acceleration on chip shared memory clusters modern embedded mpsoc designs hardware accelerators processing cores energy efficiency streamline accelerator definition streamline accelerator instantiation architectural templates run time techniques processors to accelerator communication costs tightly coupled processors tightly coupled accelerators zero copy communication extended openmp programming model code regions parallel cores accelerator based hw architectures;acceleration program processors programming kernel hardware computational modeling;design flow;shared memory clustered architectures;embedded systems;integrated circuit design;system on chip;mpsocs;openmp;multiprocessing systems;hw acceleration;parallel processing	Modern embedded MPSoC designs increasingly couple hardware accelerators to processing cores to trade between energy efficiency and platform specialization. To assist effective design of such systems there is the need on one hand for clear methodologies to streamline accelerator definition and instantiation, on the other for architectural templates and run-time techniques that minimize processors-to-accelerator communication costs. In this paper we present an architecture featuring tightly-coupled processors and accelerators, with zero-copy communication. Efficient programming is supported by an extended OpenMP programming model, where custom directives allow to specialize code regions for execution on parallel cores, accelerators, or a mix of the two. Our integrated approach enables fast yet accurate exploration of accelerator-based HW and SW architectures.	address space;adobe streamline;automatic parallelization;central processing unit;clustered file system;compiler;computer cluster;design space exploration;discrete cosine transform;embedded system;field-programmable gate array;hardware acceleration;high- and low-level;high-level synthesis;jpeg;mpsoc;openmp;parallel computing;partial template specialization;pipeline (computing);programming model;scale-invariant feature transform;shared memory;shattered world;simulation;synergy;systemc;toolchain;universal instantiation;wait state;zero-copy	Paolo Burgio;Andrea Marongiu;Dominique Heller;Cyrille Chavet;Philippe Coussy;Luca Benini	2012	2012 15th Euromicro Conference on Digital System Design	10.1109/DSD.2012.97	system on a chip;embedded system;parallel processing;computer architecture;parallel computing;real-time computing;computer science;design flow;operating system;integrated circuit design	EDA	-1.5733429853583685	49.28333467926867	38367
56ec33d0e71fcdeb69ca853dcb759cef2406fbdc	music an event-flow computer for fast simulation of digital systems		The Munich Simulation Computer (MuSiC), a special-purpose, highly-parallel programmable machine, is an approach to transfer concepts (developed for data flow computers) to fast, mixed-design-level simulation of digital systems. To gain high performance, however, the operation principle is modified from data flow computation to event flow computation. This paper presents the event flow computation scheme and its implementation by the MUSIC organisation. Parameters, which influence performance, are discussed and it is shown that MuSiC can simulate more than 8 million gates and flipflops at a speed of up to some billion events per second.	computation;computer;dataflow architecture;digital electronics;simulation	Winfried Hahn;Kristian Fischer	1985	22nd ACM/IEEE Design Automation Conference	10.1145/317825.317882	computer simulation;embedded system;data flow diagram;electronic engineering;logic synthesis;simulation;simulation software;computer science;theoretical computer science;discrete event simulation;multiple signal classification;computer music;computational model	EDA	7.674169937019537	50.52699022514018	38369
0b97636f95a379aa8a1c5aff1860664c6f8ec399	automatic tlm generation for c-based mpsoc design	functional verification;decoding;api;transaction level model;hardware description languages;graphical net list;automatic programming;automatic generation;system on chip;system on chip application program interfaces automatic programming decoding hardware description languages multiprocessing systems;application program interfaces;h 264 c based mpsoc design transaction level model c code graphical net list api systemc simulator mp3 decoder;mp3 decoder;embedded computing bridges application software code standards digital audio players decoding computer applications concurrent computing computer architecture microarchitecture;c code;h 264;industrial application;c based mpsoc design;multiprocessing systems;systemc simulator;high speed;transaction level	This paper presents a tool for automatic generation of transaction level models (TLMs)for MPSoC designs using only C-code and graphical capture. The MPSoC platform is captured as a graphical net-list of components, busses and bridge elements. The application is captured as C processes mapped to the platform components. Once the platform is decided, a set of transaction level communication APIs is automatically generated for each process. After the C code is input, an executable SystemC TLM of the design is automatically generated using our tool. This TLM can be executed using standard SystemC simulators for early functional verification of the design. Although, several TLM styles and standards have been proposed in the past, our approach differs in the fact that the designers do not need to understand the underlying SystemC code or TLM modeling style to verify that their application executes on the selected platform. Moreover, the platform can be easily modified and a new TLM for that platform can be automatically generated. Our experimental results demonstrate that for large industrial applications such as MP3 decoder and H.264, high-speed TLMs can be generated for a wide variety of platforms in a few seconds.	computation;executable;game boy camera;graphical user interface;h.264/mpeg-4 avc;mp3;mpsoc;mesa;simulation;systemc;transaction-level modeling	Lucky L. Chi Yu Lo;Samar Abdi	2007	2007 IEEE International High Level Design Validation and Test Workshop	10.1109/HLDVT.2007.4392781	system on a chip;embedded system;computer architecture;parallel computing;real-time computing;application programming interface;computer science;operating system;hardware description language;programming language;functional verification	EDA	4.0563137858030425	51.90624291815522	38381
d24670e6bde3afcf52d647f62ed16facd0bda2fe	burst-mode asynchronous controllers on fpga		FPGAs have been mainly used to design synchronous circuits. Asynchronous design on FPGAs is difficult because the resulting circuit may suffer from hazard problems. We propose a method that implements a popular class of asynchronous circuits, known as burst mode, on FPGAs based on look-up table architectures. We present two conditions that, if satisfied, guarantee essential hazard-free implementation on any LUT-based FPGA. By doing that, besides all the intrinsic advantages of asynchronous over synchronous circuits, they also take advantage of the shorter design time and lower cost associated with FPGA designs.		Duarte Lopes de Oliveira;Marius Strum;Sandro S. Sato	2008	Int. J. Reconfig. Comp.	10.1155/2008/926851	asynchronous system;embedded system;parallel computing;real-time computing;reconfigurable computing;computer science	EDA	6.561176497817548	54.49493171505795	38545
d16b0047a21911fb775f92707a3d0bf193926a53	construction of speculative optimization algorithms	control flow graph;control flow;speculative execution;data flow;optimal algorithm;control dependence	In modern processors, instructions to perform operations are often produced before it becomes known that this is required. Such an expedient, which is called speculative execution, helps to reveal parallelism at the instruction level. In the EPIC architectures, the speculative execution is completely controlled by the compiler, which makes it possible to avoid using complex hardware mechanisms for supporting speculative instruction production. Moreover, the idea of the speculative execution can be used by the compiler in machine-independent optimizations. The paper describes a scheme of construction of the speculative optimization that is based on the selection of properties of the control flow and data flow that are important from the optimization standpoint and on the estimation of the probabilities of their fulfillment. The probabilities found are used for searching and constructing advantageous speculative and bookkeeping transformations. For optimizations that include only speculative movements of instructions upwards along the control flow graph, on the basis of the suggested scheme, a method has been developed that includes algorithms for finding probabilities of data and control dependences, for estimating benefit of speculative movements, and for constructing a recovery code. On the basis of this method, an algorithm for the speculative scheduling of instructions for the Intel Itanium architecture has been developed and implemented. Specific features of its implementation and experimental results are described.	algorithm;central processing unit;compiler;control flow graph;dataflow;itanium;mathematical optimization;parallel computing;scheduling (computing);speculative execution	A. A. Belevantsev;S. S. Gaisaryan;Victor Ivannikov	2008	Programming and Computer Software	10.1134/S036176880803002X	data flow diagram;parallel computing;real-time computing;computer science;theoretical computer science;programming language;control flow;slipstream;speculative multithreading;control flow graph;speculative execution	PL	-3.6412727839461887	51.65173594269717	38598
ce5b9602a4c742adeb782b15f4235384a6db73fb	the tightly super 2-extra connectivity and 2-extra diagnosability of locally twisted cubes		Connectivity plays an important role in measuring the fault tolerance of an interconnection network G = (V,E). A faulty set F ⊆ V is called a g-extra faulty set if every component of G − F has more than g nodes. A g-extra cut of G is a g-extra faulty set F such that G− F is disconnected. The minimum cardinality of g-extra cuts is said to be the g-extra connectivity of G. G is super g-extra connected if every minimum g-extra cut F of G isolates one connected subgraph of order g+ 1. If, in addition, G−F has two components, one of which is the connected subgraph of order g + 1, then G is tightly |F | super g-extra connected. Diagnosability is an important metric for measuring the reliability of G. A new measure for fault diagnosis of G restrains that every fault-free component has at least (g+1) fault-free nodes, which is called the g-extra diagnosability of G. The locally twisted cube LTQn is applied widely. In this paper, it is proved that LTQn is tightly (3n − 5) super 2-extra connected for n ≥ 5, and the 2-extra diagnosability of LTQn is 3n−3 under the PMC model (n ≥ 5) and MM∗ model (n ≥ 6).	cubes;fault tolerance;interconnection;twisted	Yunxia Ren;Shiying Wang	2017	Journal of Interconnection Networks	10.1142/S0219265917500062	cube;distributed computing;theoretical computer science;computer science	Theory	23.899229801735686	34.647444087132946	38614
c47927bfaf913ff031746c76ee9925d8876284ea	a memory-based hardware accelerator for real-time mpeg-4 audio coding and reverberation	cmos integrated circuits;reverberation;44 1 khz;cmos technology;decoding;real time;hardware engine;real time processing;hardware accelerator;unified imdct fft ifft algorithm;220 mhz;video coding;audio coding;video coding audio coding cmos integrated circuits discrete cosine transforms fast fourier transforms reverberation;mpeg 4 standard;engines;discrete cosine transforms;channel audio decoding;0 18 micron;real time mpeg 4 audio coding;fast fourier transforms;transform coefficients;sampling methods;memory based hardware accelerator;algorithm design and analysis;256 2048 point imdct;audio reverberation;hardware mpeg 4 standard audio coding reverberation costs cmos technology algorithm design and analysis engines decoding sampling methods;0 18 micron memory based hardware accelerator real time mpeg 4 audio coding 256 2048 point imdct fft based reverberation hardware engine unified imdct fft ifft algorithm channel audio decoding audio reverberation transform coefficients cmos technology 44 1 khz 220 mhz;fft based reverberation;hardware	In this paper we propose a memory-based hardware accelerator for MPEG-4 audio coding and reverberation to achieve both high quality and reality of audio. The proposed design can realize both the computation-intensive component of 256/2048-point IMDCT and the 1024-point FFT-based reverberation through the same hardware engine by adopting a unified IMDCT/FFT/IFFT algorithm, which greatly reduces the hardware cost. The proposed design can achieve both the real-time 5.1 channel audio decoding at the sampling rate of 44.1 KHz and audio reverberation with the hardware cost of 26,633 gates and 4.6K words of local memory for storing transform coefficients and temporary results. The maximum working frequency achieves 220 MHz when implemented by UMC 0.18mum CMOS technology, which can fit the real-time processing requirement of many high quality MPEG-4 audio coding applications	advanced audio coding;algorithm;cmos;coefficient;computation;display resolution;electronic circuit;fast fourier transform;hardware acceleration;mpeg-4 part 3;modified discrete cosine transform;monkey's audio;real-time cmix;real-time clock;sampling (signal processing);system on a chip	Guo-An Jian;Chih-Da Chien;Jiun-In Guo	2007	2007 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2007.378712	embedded system;electronic engineering;real-time computing;digital audio;computer science;speech coding;audio signal flow;cmos	EDA	11.84958777233928	41.760595190984716	38626
f1870e01a84d4dcd27da2ec0f4cf369d9bc36862	integrated circuit and system design. power and timing modeling, optimization and simulation		Complex System on Chip (SoC) ICs require a high development effort and extensive testing in order to meet performance and power consumption specifications. Those requirements are usually achieved through laborious and iterative procedures which engage synthesis, timing, power analysis and back-end tools. Traditional design methodologies deal with the above issues from a synthesis or technology point of view, trying to optimize the propagation delay of primitive cells, simplifying the logic and disabling the clocks when possible. This presentation describes how a design methodology can be combined with an innovative architecture, called COMBA, for system on chip ICs in order to help the designer to achieve timing closure and meet performance and power consumption requirements in short time. This methodology is supported by tools that produce the gate level description of the system using pre-optimized building blocks for the COMBA architecture, leading to minimum development and testing time while preserving the performance and power requirements.	biasing;cmos;central processing unit;charge pump;complex system;dynamic random-access memory;integrated circuit;iterative method;lecture notes in computer science;library (computing);low-power broadcasting;mathematical optimization;microelectronics and computer technology corporation;network processor;peripheral;power supply;propagation delay;p–n junction;requirement;silicon on insulator;simulation;software propagation;spectral leakage;springer (tank);standard cell;static random-access memory;system on a chip;timing closure;transistor;very-large-scale integration	Gerhard Goos;Juris Hartmanis;Jan van Leeuwen;David Hutchison;Enrico Macii;Vassilis Paliouras;Odysseas Koufopavlou	2004		10.1007/b100662		EDA	7.851750783212171	54.10988824438484	38796
bc2b7e6486a8c41694110288251ecb711969ab99	owaru: free space-aware timing-driven incremental placement with critical path smoothing		This paper presents an incremental timing-driven placement tool, named OWARU. It optimizes timing critical paths through a free space-aware path smoothing: the gates on such paths are relocated to free spaces around the smoothed paths, while incremental static timing analysis is involved to accurately assess timing changes due to the relocation. OWARU is extended to accommodate gate sizing and layer assignment to demonstrate the effectiveness of unified physical synthesis optimizations and incremental placement. The goal is to show that OWARU is an ideal platform for timing closure at later stages of a physical design flow. OWARU is applied on a set of test circuits from 14-nm high-performance commercial microprocessors, which originally failed in timing closure. On average, the worst slack is improved by 63.6%, which corresponds to 5.0% of the clock period; total negative slack is improved by 69.1%.	clock rate;critical path;microprocessor;physical design (electronics);relocation (computing);slack variable;smoothing;static timing analysis;timing closure	Jinwook Jung;Gi-Joon Nam;Lakshmi N. Reddy;Iris Hui-Ru Jiang;Youngsoo Shin	2018	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2017.2774277	computer science;critical path method;real-time computing;relocation;physical design;electronic circuit;logic gate;static timing analysis;smoothing;timing closure	EDA	16.076785352131818	53.39104738798454	38805
22d36b799384e8b78ad67316915de47edc296512	parallel quadrant interlocking factorization on hypercube computers	parallel calculus;tratamiento paralelo;sistema lineal;linear algebra;algoritmo paralelo;hypercube;analisis numerico;parallel algorithm;traitement parallele;multiprocessor;sistema informatico;computer system;calculateur simd;linear system;matrice mathematique;analyse numerique;algorithme parallele;resolucion sistema ecuacion;resolution systeme equation;factorization;calculo paralelo;mathematical matrix;numerical analysis;hypercubo;factorizacion;simd computer;algebre lineaire;factorisation wz;factorisation;matriz matematica;algebra lineal;systeme informatique;equation system solving;systeme lineaire;multiprocesador;calcul parallele;factorisation lu;parallel processing;multiprocesseur	In this article we present an adaptation of the QIF (Quadrant Interlocking Factorization) algorithm, which solves systems of linear equations, for implementation in SIMD hypercube computers with distributed memory. This method is based in the WZ decomposition of the system's matrix. The parallel algorithm developed is general in the sense that there is no restriction imposed on the size of the problem and that it is independent of the dimension of the hypercube. The comparison of this algorithm with the parallel algorithms based on the LU factorization show that the execution time is divided by a factor of two, approximately.	computer	Inmaculada García;Juan Julián Merelo Guervós;Javier D. Bruguera;Emilio L. Zapata	1990	Parallel Computing	10.1016/0167-8191(90)90033-6	parallel processing;combinatorics;parallel computing;computer science;linear algebra;calculus;mathematics;factorization;algorithm;algebra	HPC	-2.3358905925259825	37.119895585784334	38845
4e4f525fa85446e29ad2833d9445ad5b7bf8dca4	exploring the unified design-space of custom-instruction selection and resource sharing	instruction sets data analysis;instruction latency;measurement;resource management;space exploration;unified design space;global constraint;design space;resource sharing process;data path synthesis;data analysis;instruction set extension;hardware software partitioning;resource sharing process unified design space custom instruction selection data path synthesis area latency instruction latency instruction set extensions selection process hardware software partitioning framework;resource sharing;instruction set extensions selection process;merging;area latency;profitability;hardware software partitioning framework;program processors;resource management merging hardware space exploration program processors measurement;instruction sets;hardware;custom instruction selection	Resource sharing can be applied during data-path synthesis of Instruction-Set Extensions (ISEs) in order to obtain flexibility and area efficiency. The design space of resource sharing solutions can be explored in order to find the trade-offs between area and instruction latency that suit the design goals. On the other hand, area is a proven global constraint that should be considered in the ISE selection process, since maximizing speedup as a unique goal assumes the availability of unlimited resources. Thus, a selection process should be aware of the area requirements of a subset of ISE candidates. However, when resource sharing is used for ISE data-path synthesis, the area and profitability of the subset cannot be known until resource sharing is attempted. This paper proposes a hardware/software partitioning framework in which the selection of ISEs interacts with the resource sharing process in order drive the exploration of the selection design space towards implementation alternatives that are likely to increase the utilization of the given area resources. On the benchmarks analyzed in this paper, our techniques find solutions that under a fixed area constraint, achieve speedups from 8% to 238% higher than previous selection techniques. Furthermore, unlike previous approaches, the proposed framework allows the exploration, at the selection level, of the design space of trade-offs between speedup and area that are available to the designer.	american and british english spelling differences;electronic design automation;experiment;heart rate variability;instruction selection;problem domain;requirement;speedup;xilinx ise	Marcela Zuluaga;Nigel P. Topham	2010	2010 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation	10.1109/ICSAMOS.2010.5642056	parallel computing;real-time computing;computer science;theoretical computer science	EDA	1.1655332824362379	51.22202357878577	38860
74d389497ef0e470260f1480e7eed7986f56f590	multi-user fpga co-simulation over tcp/ip	digital signal processing;matlab multiuser fpga tcp ip fpga co simulation ip core ip development system development xilinx system generator dsp 3 1 xsg multiple users hardware co simulation fpga board encryption cores camellia aes 128 advanced encryption standard;design engineering;fuses;prototypes;fpga co simulation;tcpip;design flow;multiuser fpga;multi user;multi access systems microcomputers field programmable gate arrays transport protocols cryptography;aes 128;transport protocols;xsg;xilinx;tcp ip;hardware co simulation;cryptography;workstations;multi access systems;system generator;ip core;system development;dsp 3 1;field programmable gate arrays;advanced encryption standard;matlab;microcomputers;encryption cores;field programmable gate arrays tcpip hardware cryptography prototypes matlab workstations fuses digital signal processing design engineering;fpga board;multiple users;ip development;hardware;camellia	FPGA co-simulation of an IP core is an important design flow step in IP and system development. In this paper, we discuss how, with Xilinx's system generator for DSP 3.1 (XSG), it is possible for multiple-users to hardware co-simulate IP cores over any distance via TCP/IP, sharing only one FPGA board resource. The hardware co-simulation strategy is mutually exclusive in that only one user at any one time can hardware co-simulate on the FPGA board. We demonstrate this with the use of two encryption cores, Camellia and AES-128 (advanced encryption standard), which have both been generated using the block-based tool. The sharing of the FPGA board is handled with a set of Matlab function commands.	co-simulation;encryption;field-programmable gate array;internet protocol suite;matlab;multi-user;semiconductor intellectual property core;shared web hosting service;simulation	Daniel Denning;James Irvine;Derek Stark;Malachy Devlin	2004	Proceedings. 15th IEEE International Workshop on Rapid System Prototyping, 2004.	10.1109/RSP.2004.31	advanced encryption standard;embedded system;parallel computing;real-time computing;computer science;operating system;internet protocol suite;fpga prototype	EDA	3.769207718970213	48.07031860032232	38875
785ff07930e859cf4d559ae34820f4e570e1c6b8	efficient distributed-memory parallel matrix-vector multiplication with wide or tall unstructured sparse matrices		This paper presents an efficient technique for matrix-vector and vector-transpose-matrix multiplication in distributed-memory parallel computing environments, where the matrices are unstructured, sparse, and have a substantially larger number of columns than rows or vice versa. Our method allows for parallel I/O, does not require extensive preprocessing, and has the same communication complexity as matrix-vector multiplies with column or row partitioning. Our implementation of the method uses MPI. We partition the matrix by individual nonzero elements, rather than by row or column, and use an “overlapped” vector representation that is matched to the matrix. The transpose multiplies use matrix-specific MPI communicators and reductions that we show can be set up in an efficient manner. The proposed technique achieves a good work per processor balance even if some of the columns are dense, while keeping communication costs relatively low.		Jonathan Eckstein;Gyorgy Matyasfalvi	2018	CoRR			HPC	-1.691078601373866	38.74628284164327	38937
fd179dfd9b171541751df8e751d86e863a71e7db	memory architecture exploration framework for cache based embedded soc	integrated approach;cache storage;cycle time;tight time to market constraints;system on chip cache storage memory architecture;supercomputer education research centre;multimedia application;embedded system;data partitioning;chip;short design cycle time;graph partitioning;critical system;embedded multimedia applications multilevel multiobjective memory architecture exploration embedded soc system on chip tight time to market constraints short design cycle time spram cache;embedded system design;complex system;system on chip;energy consumption;memory architecture;memory architecture runtime embedded system system on a chip data structures read write memory energy consumption very large scale integration instruments supercomputers;optimal design;hybrid architecture;time to market;data layout;embedded multimedia applications;market segmentation;embedded soc;high performance;multilevel multiobjective memory architecture exploration;spram cache;exhaustive search	Today's feature-rich multimedia products require embedded system solution with complex System-on-Chip (SoC) to meet market expectations of high performance at a low cost and lower energy consumption. The memory architecture of the embedded system strongly influences critical system design objectives like area, power and performance. Hence the embedded system designer performs a complete memory architecture exploration to custom design a memory architecture for a given set of applications. Further, the designer would be interested in multiple optimal design points to address various market segments. However, tight time-to-market constraints enforces short design cycle time. In this paper we address the multi-level multi-objective memory architecture exploration problem through a combination of exhaustive-search based memory exploration at the outer level and a two step based integrated data layout for SPRAM-Cache based architectures at the inner level. We present a two step integrated approach for data layout for SPRAM-Cache based hybrid architectures with the first step as data-partitioning that partitions data between SPRAM and Cache, and the second step is the cache conscious data layout. We formulate the cache-conscious data layout as a graph partitioning problem and show that our approach gives up to 34% improvement over an existing approach and also optimizes the off-chip memory address space. We experimented our approach with 3 embedded multimedia applications and our approach explores several hundred memory configurations for each application, yielding several optimal design points in a few hours of computation on a standard desktop.	address space;cpu cache;complex system;computation;computer memory;critical system;desktop computer;embedded system;exploration problem;graph partition;key;mpsoc;memory address;optimal design;pareto efficiency;partition problem;real-time transcription;requirement;software feature;system on a chip;systems design	T. S. Rajesh Kumar;C. P. Ravikumar;Renganayaki Govindarajan	2008	21st International Conference on VLSI Design (VLSID 2008)	10.1109/VLSI.2008.113	chip;memory address;system on a chip;uniform memory access;distributed shared memory;shared memory;embedded system;interleaved memory;complex systems;computer architecture;parallel computing;real-time computing;cycle time variation;computer science;physical address;graph partition;optimal design;operating system;brute-force search;memory protection;overlay;flat memory model;market segmentation;cache-only memory architecture;memory map;non-uniform memory access;memory management	EDA	1.53163638819513	54.39917575783397	38949
d8429b4eec6e798dc14d82429f20401980384ec0	a high performance matrix manipulation algorithm for mpps	high performance matrix manipulation;massively parallel processing;matrix computation;matrix multiplication;2 dimensional;parallel algorithm;3 dimensional;three dimensions	A 3-dimensional (3-D) matrix multiplication algorithm for massively parallel processing systems is presented. Performing the product of two matrices C= C+ A B is viewed as solving a 2-dimensional problem in the 3-dimensional computational space. The three dimensions correspond to the matrices dimensions m, k, and n: A  Rm×k, B  R k×n, and C  R m×n. The p processors are configured as a virtual processing cube with dimensions p 1, P 2, and p 3. The cube's dimensions are proportional to the matrices' dimensions-m, n, and k. Each processor performs a local matrix multiplication of size m/p 1 × n/p 2 × k/p 3, on one of the sub-cubes in the computational space. Before the local computation can be carried out, each sub-cube needs to receive sub-matrices corresponding to the planes where A and B reside. After the single matrix multiplication has completed, the sub-matrices of C have to be reassigned to their respective processors. The 3-D parallel matrix multiplication approach has, to the best of our knowledge, the least amount of communication among all known parallel algorithms for matrix multiplication. Furthermore, the single resulting sub-matrix computation gives the best possible performance from the uni-processor matrix multiply routine. The 3-D approach achieves high performance for even relatively small matrices and/or a large number of processors (massively parallel). This algorithm has been implemented on IBM Power-parallel SP-2 systems (up to 216 nodes) and have yielded close to the peak performance of the machine. For large matrices, the algorithm can be combined with Winograd's variant of Strassen's algorithm to achieve super-linear speed-up. When the Winograd approach is used, the performance achieved per processor exceeds the theoretical peak of the system.	algorithm	Ramesh C. Agarwal;Fred G. Gustavson;Susanne M. Balle;Mahesh V. Joshi;Prasad V. Palkar	1995		10.1007/3-540-60902-4_1	three-dimensional space;parallel computing;computer science;theoretical computer science;distributed computing;algebra	Robotics	-1.403356719633502	38.407682127724414	39022
312001ff1f82938897020a419a31dc46c37b4a52	cellular nanocomputers: a focused review	computers;fault tolerance;cellular automata;nanocomputers;asynchronous timing	Research into nanocomputer architectures have increasingly attracted attention in recent years, driven by the realization that improvements in integration densities can only be sustained at an unchanged pace if new approaches are adopted. As top-down fabrication methods like optical lithography are gradually facing their technological and economical limits, alternatives are called for. Bottom-up fabrication methods are still in their early stages of development, but, being based on the self-assembling properties inherent in molecules, they offer much promise for nanocomputers. With expected changes in fabrication method, there will also be changes in the architectures of the resulting computers: it is unlikely that the complicated structures of von Neumann computer architectures can be produced by top-down methods in the nanometer-scale regime. Rather, future computers are expected to ABSTRACT	computer architecture;nanocomputer;top-down and bottom-up design;von neumann architecture	Ferdinand Peper;Jia Lee;Susumu Adachi;Teijiro Isokawa	2009	IJNMC	10.4018/jnmc.2009010103	cellular automaton;nanocomputer;fault tolerance;real-time computing;computer science;theoretical computer science;nanotechnology;distributed computing;algorithm	Vision	11.117220526095794	57.977434420570326	39100
79d68db415c56f5641cd645173f7d3f0b5307035	multigraph: efficient graph processing on gpus		High-level GPU graph processing frameworks are an attractive alternative for achieving both high productivity and high performance. Hence, several high-level frameworks for graph processing on GPUs have been developed. In this paper, we develop an approach to graph processing on GPUs that seeks to overcome some of the performance limitations of existing frameworks. It uses multiple data representation and execution strategies for dense versus sparse vertex frontiers, dependent on the fraction of active graph vertices. A two-phase edge processing approach trades off extra data movement for improved load balancing across GPU threads, by using a 2D blocked representation for edge data. Experimental results demonstrate performance improvement over current state-of-the-art GPU graph processing frameworks for many benchmark programs and data sets.	benchmark (computing);data (computing);edge detection;graph (abstract data type);graphics processing unit;high- and low-level;load balancing (computing);multigraph;sparse matrix;two-phase locking	Changwan Hong;Aravind Sukumaran-Rajam;Jinsung Kim;P. Sadayappan	2017	2017 26th International Conference on Parallel Architectures and Compilation Techniques (PACT)	10.1109/PACT.2017.48	memory management;parallel computing;instruction set;multigraph;graph database;theoretical computer science;thread (computing);external data representation;load balancing (computing);computer science;data structure	HPC	-3.556711338441957	42.979343899704546	39138
a5dd7859bc3a0dc3d7127625669cb0db2b53ca71	unifying vlsi array designs with geometric transformations				Peter R. Cappello;Kenneth Steiglitz	1983			parallel computing;very-large-scale integration;computer science;transformation geometry	EDA	5.888263306776147	49.87403052601597	39204
c046de3784c1508c9221efd3f8a3209a50aff5d4	mullet - a parallel multiplier generator	xst tools mullet parallel multiplier generator partial product generator partial product summer searching algorithm xilinx fpga devices xilinx coregenerator;multiplying circuits;logic design;search algorithm;digital arithmetic;field programmable gate arrays adders timing computer science very large scale integration integrated circuit technology application specific integrated circuits encoding logic devices tree data structures;logic design digital arithmetic multiplying circuits	A module generator called Mullet for producing near-optimal parallel multipliers in a technology independent manner is presented. Using this tool, a large number of candidate designs can be generated in order to find combinations of primitive elements which produce the best multiplier. The process of multiplication is broken down into a partial product generator (PPG) and a partial product summer (PPS). Both of these tasks can be done in a number of different ways and the best solution depends on the size of the required multiplier as well as the technology used. Mullet can be combined with a searching algorithm to find the best multiplier based on some objective. It can generate high quality multipliers for irregular architectures such as FPGAs and use features such as the dedicated multipliers. The tool can also be used to explore tradeoffs between architectures and to calibrate timing models of the primitive components. Synthesized examples using Xilinx FPGA devices are comparisons are made with those produced by the Xilinx CoreGenerator and XST tools.	display resolution;field-programmable gate array;search algorithm	Kuen Hung Tsoi;Philip Heng Wai Leong	2005	International Conference on Field Programmable Logic and Applications, 2005.	10.1109/FPL.2005.1515814	embedded system;logic synthesis;computer science;theoretical computer science;operating system;algorithm;search algorithm	EDA	10.345787738900688	47.27874616148638	39239
29984a360dc8a07dceba68d38a109531dfb44417	modular performance analysis of embedded real-time systems: improving modeling scope and accuracy		A fundamental aspect of the design of an embedded system is the prediction of its performance in terms of timing, memory, or energy early in the design process. The objective of this task, typically referred to as systemlevel performance evaluation, is twofold. On one hand, it is instrumental for pre-validating a system design before any resources are invested for the actual implementation and, on the other hand, the performance evaluation is a central driver for the exploration of the design space. For systems with strict performance requirements such as hard real-time systems the performance evaluation needs to be provably correct, that is, it has to cover the worst-case performance scenarios. Furthermore, the evaluation should be fast such that it can be employed for the exploration of large design spaces. Recent research efforts have led to analytical and modular methods for worst-case performance evaluation at the system level. These methods ensure the correctness of the performance evaluation and are fast even for large-scale systems. However, they suffer from limited modelling scope and analysis accuracy. As a consequence, when applying these methods to complex systems, one often experiences considerable abstraction losses, which lead to overly pessimistic performance results. This thesis introduces several formal models and methods that refine the modelling capabilities of analytical performance evaluation and prevent abstraction losses. The results build on the existing framework for Modular Performance Analysis (MPA), but apply also to other analytical formalisms. The main contributions of this thesis can be summarized as follows: • The modelling scope of analytical performance evaluation is extended to systems with cyclic dependencies. • New models and methods are introduced for handling structured event or data streams in analytical performance evaluation. • A novel hybrid analysis methodology is presented that combines analytical and state-based system evaluation. • New design methods for energy-efficient real-time systems are introduced.	best, worst and average case;complex systems;correctness (computer science);embedded system;experience;performance evaluation;profiling (computer programming);real-time clock;real-time computing;requirement;systems design	Simon Perathoner	2011		10.3929/ethz-a-006492384	real-time computing;modular design;computer science	Embedded	3.3655934653912882	55.77309982344548	39262
9850edcf456684174d08d585dc7b714e9dfdaef4	two-sided single-detour untangling for bus routing	pins;sorting;routing;board level routing;wires;network routing;cpu time two sided single detour untangling bus routing hierarchical bubble sorting single layer bus routing;bus routing board level routing;heuristic algorithms;periodic structures;bus routing;sorting network routing;circuit testing;computer science;routing pins sorting algorithm design and analysis timing wires computer science educational institutions circuit testing central processing unit;algorithm design and analysis;central processing unit;timing	In this paper, based on the optimality of hierarchical bubble sorting, the problem of two-sided single-detour untangling for single-layer bus routing is firstly formulated. Compared with an optimal O(n3) algorithm[4] for one-sided single-detour untangling without capacity consideration, an optimal O(n2) algorithm is proposed to solve the two-sided single-detour untangling problem without capacity consideration. For two-sided single-detour untangling with capacity consideration, an efficient O(n2) algorithm is proposed and the experimental results show that our proposed algorithm can successfully untangle all the twisted nets for the tested examples in less CPU time.	algorithm;central processing unit;routing;sorting;twisted	Jin-Tai Yan;Zhi-Wei Chen	2010	Design Automation Conference	10.1145/1837274.1837325	routing;parallel computing;real-time computing;computer science;distributed computing;algorithm;computer network	EDA	15.167075706397657	51.720231299980234	39264
89c88ff7acae4b3bd6f558a3cc841436a43e810e	innovative test solutions for pin-limited microcontrollers	scan-based testing;test mode;test method;high quality testing;correct mode entry;scan-based test methodology;dedicated test mode pin;6-pin package;pin-limited microcontrollers;chain testing;innovative test solutions;8-pin package;automatic test pattern generation;test methods;scan;packaging;dft;design for test;microcontroller;testing;test;design for testability;logic design;microcontrollers	"""A scan-based test methodology was adopted for the Freescale S08 and RS08 (8-bit) families of microcontrollers (MCUs) several years ago. This methodology has been shown to provide high quality testing and is an important part of Freescale's """"Zero Defect"""" initiative. One of the difficult restrictions placed on these families of products is the requirement for applications requiring very few pins. For instance, the 9S08QG8 MCU is available in an 8-pin package, and the 9S08KA2 MCU is available in a 6- pin package. Because of these packaging requirements, several test methods have been developed to implement scan-based testing using very few pins. The methods include full single scan chain testing, an internally-generated reset (no dedicated reset pin), test mode latching with no dedicated test mode pins, IDDQ measurement using a single power supply, an indirect method for verifying correct mode entry, and pin multiplexing that combines several signals required for scan-based testing."""	8-bit;display resolution;freescale rs08;freescale s08;iddq testing;microcontroller;multiplexing;power supply;requirement;software bug;verification and validation	Matthew G. Stout;Kenneth P. Tumin	2008	9th International Symposium on Quality Electronic Design (isqed 2008)	10.1109/ISQED.2008.84	microcontroller;embedded system;electronic engineering;computer hardware;computer science;engineering;design for testing;software testing	EDA	21.791995563371586	53.257758689037296	39296
1cd6e55eb0bca789770cdee17b2fa2de6b6cc9f5	a test evaluation technique for vlsi circuits using register-transfer level fault modeling	design for testability;fault simulation;circuit testing very large scale integration circuit faults control system synthesis integrated circuit synthesis sampling methods application specific integrated circuits hardware design languages registers circuit simulation;very large scale integrated;sampling technique;weighted sums;application specific integrated circuit;logic testing;integrated circuit testing;vlsi;verilog hardware description language test evaluation technique vlsi circuits register transfer level fault modeling test patterns dft stratified fault sampling rtl fault simulation fault injection algorithms rtl fault modeling rtl fault list collapsed gate level stuck at fault set rtl coverage gate level coverage statistical error bounds very large scale integration system vlsi system rtl module coverages system timing controller asic application specific integrated circuit;fault coverage;design for testability vlsi fault simulation logic testing integrated circuit testing;error bound;hardware description language;fault model;fault injection;register transfer level;logic gate	Stratified fault sampling is used in register transfer level (RTL) fault simulation to estimate the gate-level fault coverage of given test patterns. RTL fault modeling and fault-injection algorithms are developed such that the RTL fault list of a module can be treated as a representative fault sample of the collapsed gate-level stuck-at fault set of the module. The RTL coverage for the module is experimentally found to track the gate-level coverage within the statistical error bounds. For a very large scale integration system, consisting of several modules, the level of description may differ from module to module. Therefore, the stratified fault sampling technique is used to determine the overall coverage as a weighted sum of RTL module coverages. Several techniques are proposed to determine these weights, known as stratum weights. For a system timing controller application specific integrated circuit, the stratified RTL coverage of verification test-benches is estimated to be within 0.6% of the actual gate-level coverage of the synthesized circuit. This ASIC consists of 40 modules (consisting of 9000 lines of Verilog hardware description language) that are synthesized into 17 126 equivalent logic gates by a commercial synthesis tool. Similar results on two other systems are eported.	algorithm;application-specific integrated circuit;experiment;fault coverage;fault injection;hardware description language;logic gate;register-transfer level;sampling (signal processing);simulation;stuck-at fault;test card;verilog;very-large-scale integration;weight function	Pradip A. Thaker;Vishwani D. Agrawal;Mona E. Zaghloul	2003	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/TCAD.2003.814958	embedded system;sampling;computer architecture;electronic engineering;real-time computing;fault coverage;logic gate;computer science;stuck-at fault;design for testing;fault model;application-specific integrated circuit;very-large-scale integration;hardware description language;register-transfer level	EDA	20.2390445791978	49.882500238719096	39370
20b824ca16fc1ae41f490d5893bedbbab9638416	a hardware/software reconfigurable architecture for adaptive wireless image communication	compression algorithm;architectural design;hardware software reconfigurable architecture;image coding;hardware software codesign;land mobile radio hardware software codesign reconfigurable architectures adaptive signal processing image coding data compression internet multimedia communication;data compression;image communication;web and internet services;reconfigurable architectures;compression algorithms;power efficiency;high energy;home appliances;image compression hardware software reconfigurable architecture adaptive wireless image communication mobile internet mobile multimedia services next generation appliances adaptive multimedia compression algorithms dynamic channel conditions;wireless communication;adaptive algorithm;reconfigurable architecture;adaptive;adaptive signal processing;adaptive wireless image communication;internet;land mobile radio;image compression;batteries;multimedia communication;wireless multimedia;hardware reconfigurable architectures image coding wireless communication image communication web and internet services home appliances batteries compression algorithms software algorithms;next generation appliances;next generation;dynamic channel conditions;software algorithms;adaptive multimedia compression algorithms;mobile internet;multimedia services;mobile multimedia services;hardware;design methodology	With the projected significant growth in mobile internet and multimedia services, there is a strong demand for next-generation appliances capable of wireless image communication. One of the major bottlenecks in enabling wireless image communication is the high energy requirement, which may surpass the current and future capabilities of battery technologies. Past studies have shown that the bottlenecks can be overcome by developing adaptive multimedia compression algorithms which can adapt to dynamic channel conditions and service requirements {tyrh00,taylor01}.In this paper, we present an application-specific hardware/software reconfigurable architecture to support adaptive image compression algorithms. We present a design methodology which considers co-design between adaptive algorithms and architectural design leading to a reconfigurable architecture for image compression algorithms. Co-design of the proposed architecture aims not only at performance and power efficient implementation, but also towards fast and efficient run-time adaptation of an adaptive image compression algorithm. Finally, we present experimental results demonstrating that the proposed architecture provides a low cost (performance, energy) implementation for the adaptive image compression algorithm, and necessary run-time adaptation to current wireless conditions and requirements with very low overhead.	algorithm;data compression;image compression;overhead (computing);requirement	Debashis Panigrahi;Clark N. Taylor;Sujit Dey	2002		10.1109/ASPDAC.2002.994979	data compression;embedded system;real-time computing;computer science;theoretical computer science;algorithm;statistics;computer network	Arch	1.8433460712796366	57.94583402083579	39378
b0636b35d58b9b40c5868281a399019a0e9e9076	analysis of multidimensional loops with non-uniform dependences	janus test multidimensional loops nonuniform dependences parallelizing compiler loop transformations dependence information irregular dependences preprocessing phase integer simplex resolution integer simplex integer linear fractional programming;parallelizing compilers;multidimensional systems information analysis vectors integer linear programming testing performance evaluation program processors parallel processing computational efficiency;fractional programming;multi dimensional;general methods;integer programming;loop transformation;parallelising compilers;linear programming;parallelising compilers integer programming linear programming	For a parallelixing compiler, mainly based on loop transformations, dependence information that is as complete and precise as possible is required. In this paper, we propose a generalzzed method for computing, in any multi-damensional loop, information which proved to be useful in the case of irregular dependences. Firstly, we solve the basic problem of the existence of a dependence with an algorithm composed of a preprocessang phase of reduction and of a n integer simplex resolution. If a solution exists, we compute by integer simplex the bounds of the distances associated with loop indices. Dependang on the values of these bounds, we finally define problems consisting in evaluating the bounds of slopes of dependence vectors, which we solve by integer linear fractional programming. The amount of computation for each new problem is very low. This algorithm has been implemented as an extension of the Janus Test, which was presented in a previous work.	algorithm;compiler;computation;fractional programming;janus;linear-fractional programming	Jean-Claude Sogno	1997		10.1109/APDC.1997.574056	mathematical optimization;discrete mathematics;integer programming;branch and price;theoretical computer science;mathematics;branch and cut	HPC	4.279951733622821	37.5183022591926	39396
14df24736f9dae0c844006b50bc6800f446a8375	an accurate system architecture refinement methodology with mixed abstraction-level virtual platform	architecture exploration;complicated system;design space;architecture refinement;system performance profiling;virtual system based-on;dual dsp cores virtual system;mixed abstraction-level virtual platform;system validation;system-on-a-chip design;system performance;architecture refinement flow;system-on-chip;digital signal processing chips;design engineer;accurate system architecture refinement;logic design;system-level architecture refinement flow;transaction-level modeling (tlm);design methodology;soc design;electronic system-level (esl);electronic engineering computing;electronic system-level;space exploration;error rate;application software;digital signal processing;software performance;decoding;computational modeling;hardware;system on chip;accuracy;system on a chip;computer architecture;electronic system level;system architecture	The increasing complexity of today's system-on-a-chip (SoC) design is challenging the design engineers to evaluate the system performance and explore the design space. Electronic system-level (ESL) design methodology is of great help for attacking the challenges in recent years. In this paper, we present a system-level architecture refinement flow and implement a dual DSP cores virtual system based-on the highly accurate mixed abstraction-level modeling methodology. The constructed virtual platform can run various multimedia applications and achieve high accuracy. Compared with the traditional RTL simulation, the error rate is less than 5% and the simulation speed is around 100 times faster. Using the architecture refinement flow, the system performance profiling and architecture exploration is also realized for the software and hardware engineers to scrutinize the complicated system.	abstraction layer;digital signal processor;electronic system-level design and verification;profiling (computer programming);refinement (computing);simulation;system on a chip;systems architecture;virtual machine	Zhe-Mao Hsu;Jen-Chieh Yeh;I-Yao Chuang	2010	2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)		system on a chip;embedded system;computer architecture;real-time computing;computer science;computer performance;systems architecture	EDA	3.9445849453370894	52.90524459359961	39410
89a70ad6e68bd52c1a3bd7d6dfc707bae7ab710e	an order based segmentation algorithm for low power implementation of digital filters	multiplication operator;digital filters capacitance finite impulse response filter digital signal processing hardware energy consumption filtering cmos logic circuits clocks switching circuits;fir filters switched capacitor filters;digital filter;switched capacitor filters;low power;fir filters;it evaluation;power consumption;correlation order based segmentation algorithm low power implementation digital filters power consumption switched capacitance multiplier circuit primitive components	The paper presents a new algorithm for low power implementation of digital filters. The algorithm reduces power consumption through a two phased strategy, which targets reducing switched capacitance within the multiplier circuit. The first phase involves the segmentation of Coefficients into more primitive components which could in turn be processed through a single shift and a more primitive mutiplication operations. The second phase exploits the correlation among the new set of coefficients at the coefficient input of the multiplier for more reduction in switched capacitance. The paper describes the algorithm and its evaluation environment, and provides results with a number of filter examples demonstrating up to 65% reduction in power compared to conventional filtering.	algorithm;coefficient;digital filter;expression (computer science)	Ahmet T. Erdogan;Tughrul Arslan	2000		10.1109/ICASSP.2000.860094	network synthesis filters;multiplication operator;digital filter;switched capacitor;finite impulse response;control theory;mathematics	EDA	13.728120048836276	46.34751205560043	39411
c50a9446a62c2c05c1a94a382f9e9832e2d467c0	automated extraction of model parameters for noise coupling analysis in silicon substrates	silicon;elemental semiconductors;system on chip electronic engineering computing elemental semiconductors integrated circuit noise silicon substrates;si model parameters automated extraction noise coupling analysis silicon substrates seamless substrate noise simulation package;silicon coupling circuits integrated circuit noise doping profiles noise generators parameter extraction calibration circuit testing circuit simulation radio frequency;system on chip;electronic engineering computing;substrates;integrated circuit noise;substrate noise	An automated process, requiring the fabrication of only a few simple test structures, can efficiently characterize a silicon substrate by extracting the process constants of a Z-parameter based macromodel. The resulting model is used to generate a resistive substrate network that can be used in noise coupling simulations. This process has been integrated into the Cadence DFII environment to provide a seamless substrate noise simulation package which alleviates the need for pre-characterized libraries.	doping (semiconductor);library (computing);macromodel;parameter (computer programming);randomness extractor;seamless3d;simulation	Brett Peterson;Kartikeya Mayaram;Terri S. Fiez	2007	2007 IEEE Custom Integrated Circuits Conference	10.1109/CICC.2007.4405862	system on a chip;embedded system;substrate coupling;electronic engineering;computer science;electrical engineering;silicon	EDA	24.57320506394589	54.73547001237754	39415
a1700f85cf3b664c1191ef1e567d72fc045c3aca	subsampling-based compression and flow visualization	particles;visualization	As computational capabilities increasingly outpace disk speeds on leading supercomputers, scientists will, in turn, be increasingly unable to save their simulation data at its native resolution. One solution to this problem is to compress these data sets as they are generated and visualize the compressed results afterwards. We explore this approach, specifically subsampling velocity data and the resulting errors for particle advection-based flow visualization. We compare three techniques: random selection of subsamples, selection at regular locations corresponding to multi-resolution reduction, and introduce a novel technique for informed selection of subsamples. Furthermore, we explore an adaptive system which exchanges the subsampling budget over parallel tasks, to ensure that subsampling occurs at the highest rate in the areas that need it most. We perform supercomputing runs to measure the effectiveness of the selection and adaptation techniques. Overall, we find that adaptation is very effective, and, among selection techniques, our informed selection provides the most accurate results, followed by the multi-resolution selection, and with the worst accuracy coming from random subsamples.	adaptive system;algorithm;chroma subsampling;data compression;data-flow analysis;distributed memory;memory footprint;native resolution;run time (program lifecycle phase);simulation;supercomputer;surround sound;velocity (software development)	Alexy Agranovsky;David Camp;Kenneth I. Joy;Hank Childs	2015		10.1117/12.2083251	particle;simulation;visualization;computer science;theoretical computer science;data mining;physics;quantum mechanics	HPC	-3.8548491621904017	35.062052449420186	39417
0771a90a38ef237e13a6af466032921856e895c0	a time-optimal parallel algorithm for three-dimensional convex hulls	parallel algorithm;three dimensional;timing optimization;convex hull;divide and conquer	In this paper we present an O(1/α logn)-time parallel algorithm for computing the convex hull ofn points in ℜ3. This algorithm usesO(@#@ n1+a) processors on a CREW PRAM, for any constant 0 < α ≤1. So far, all adequately documented parallel algorithms proposed for this problem use time at least O(log2 n). In addition, the algorithm presented here is the first parallel algorithm for the three-dimensional convex hull problem that is not based on the serial divide-and-conquer algorithm of Preparata and Hong, whose crucial operation is the merging of the convex hulls of two linearly separated point sets. The contributions of this paper are therefore (i) an O(logn)-time parallel algorithm for the three-dimensional convex hull problem, and (ii) a parallel algorithm for this problem that does not follow the traditional paradigm.	central processing unit;convex hull;linear separability;parallel algorithm;preparata code;programming paradigm	Nancy M. Amato;Franco P. Preparata	1995	Algorithmica	10.1007/BF01293667	convex analysis;subderivative;three-dimensional space;frank–wolfe algorithm;mathematical optimization;combinatorics;discrete mathematics;divide and conquer algorithms;convex optimization;convex polytope;convex combination;orthogonal convex hull;ramer–douglas–peucker algorithm;linear matrix inequality;convex conjugate;computer science;gift wrapping algorithm;convex hull;dykstra's projection algorithm;mathematics;geometry;parallel algorithm;convex set;proper convex function;output-sensitive algorithm	Theory	12.594224599297553	33.50524758338869	39420
db1842ef52578dd3ec9cd101e87d54c6dd368b48	case study of 1149.01 microprocessor implementations		Abstract   This paper describes selected implementation issues of IEEE 1149.1 Std. Test Access Port and Boundary-Scan Architecture for three Motorola VLSI CMOS microprocessors. Problems and solutions that arose during implementation of standard features and additional public instructions are discussed.		W. C. Bruce;Michael G. Gallup;Grady Giles;Tom Munns	1993	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(93)90003-P	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system	EDA	6.080612178332977	49.3276918142121	39436
607c36da64c9a6eadd2fe17fc293779c123648f0	the complexity of the reliable connectivity problem	algoritmo paralelo;fiabilidad;reliability;parallel algorithm;plenitud;metodo combinatorio;combinatorial problems;methode combinatoire;algorithme parallele;graph connectivity;reliable connection;fiabilite;conectividad grafo;combinatorial method;completeness;completude;connectivite graphe;p completeness;parallel algorithms	Abstract   Let  G =( V , E ) be a graph together with two distinguished nodes  s  and  t , and suppose that to every node  f ( v )∈ V , a nonnegative integer  f ( v )⩽degree( v ) is assigned. Suppose, moreover, that each node υ can cause at most  f ( v ) of its incident edges to “fail” (these  f ( v ) edges can be arbitrarily chosen). The Reliable Connectivity Problem is to test whether node  s  remains connected to  t  by a path of non-failed edges for all possible choices of the failed edges. Although, as expected, the general Reliable Connectivity Problem is co-NP-complete and this remains true even if  G  is restricted to the class of directed, acyclic graphs, we show that the problem is in P for directed, acyclic graphs if the edges caused to fail by each node υ are chosen only from the edges  incoming  to v. Concerning the parallel complexity of the latter version of the problem, it turns out that it is P-complete. Moreover, approximating the maximum  d  such that for any choice of failed edges there is a directed path of non-failed edges that starts from  s  and has length  d  turns out to be P-complete as well, for any given degree of relative accuracy of the approximation. Furthermore, given that every node υ will cause  at least   f ( v ) incoming edges to fail, the question whether there is a choice of failed edges such that  s  remains connected to  t  via non-failed edges turns out to be in NC, even for general graphs.		Dimitris Kavadias;Lefteris M. Kirousis;Paul G. Spirakis	1991	Inf. Process. Lett.	10.1016/0020-0190(91)90023-B	combinatorics;discrete mathematics;multiple edges;computer science;mathematics;parallel algorithm;path;algorithm	DB	23.484301992769215	33.689211824588455	39492
a2ff8074ca6613731ea0f9fb0afdaa1df058949d	optimized code generation for programmable digital signal processors	digital signal processors;digital signal processing;software portability;signal generators;application software;prototypes;rule based;code generation;iron;drives;artificial intelligent;feasibility;assembly;software portability artificial intelligence digital signal processing chips program compilers;programmable digital signal processors;signal generators digital signal processors assembly artificial intelligence digital signal processing prototypes modems application software iron drives;digital signal processor;artificial intelligence;feasibility programmable digital signal processors compilation model optimized code generation artificial intelligence pattern library rule base;compilation model;digital signal processing chips;modems;optimized code generation;program compilers;rule base;pattern library	The authors present a novel compilation model for optimized code generation for programming digital signal processors (PDSPs). The model uses artificial intelligence techniques to yield output code that is comparable (in some cases superior) to that of handwritten assembly codes by DSP experts. Porting the code generator to different PDSPs requires modifying only two processor-dependent modules: a pattern library and a rule base. A prototype code generator has been implemented, targeting a subset of the TMS32020's architecture and instruction set. Several case studies are presented, which demonstrate the feasibility of the proposed approach. >		Kin H. Yu;Yu Hen Hu	1993		10.1109/ICASSP.1993.319155	rule-based system;dead code;feasibility study;digital signal processor;computer architecture;parallel computing;computer science;redundant code;code generation;unreachable code;source code	EDA	8.402732404680133	48.06924873896797	39495
bcf4c8ba59a62283f489fdc4967e4d4fce9be9bf	the era of high bandwidth memory	market research;bandwidth;through silicon vias	■ HBM is a breakthrough memory solution for performance, power and form-factor constrained systems by delivering high bandwidth, Low effective power & Small form factor ■ HBM device provide various mechanisms to ensure quality/reliability at pre and post SiP assembly ■ HBM is an industry standard solution with multiple supply sources	form factor (design);high bandwidth memory;small form factor;technical standard	Kevin Tran	2016	2016 IEEE Hot Chips 28 Symposium (HCS)	10.1109/HOTCHIPS.2016.7936171	embedded system;electronic engineering;engineering;electrical engineering	Arch	11.627422197836095	57.12496046066251	39517
f889ba1127ca376ef7700e9e718e4eacbae654a1	reconfigurable sparse matrix-vector multiplication on fpgas	index terms —fpga;sparse matrix-vector multiply;sparse matrix;floating point;indexing terms	executing memory-intensive simulations, such as those required for sparse matrix-vector multiplication. This effect is due to the memory bottleneck that is encountered with large arrays that must be stored in dynamic RAM. An FPGA core designed for a target performance that does not unnecessarily exceed the memory imposed bottleneck can be distributed, along with multiple memory interfaces, into a scalable architecture that overcomes the bandwidth limitation of a single interface. Interconnected cores can work together to solve a computing problem and exploit a bandwidth that is the sum of the bandwidth available from all of their connected memory interfaces. This work demonstrates this concept of scalability with two memory interfaces through the use of an available FPGA prototyping platform. It is shown that our reconfigurable approach is scalable as performance roughly doubles when two FPGAs are used for computation instead of one.	bandwidth (signal processing);computation;dynamic random-access memory;exploit (computer security);fpga prototyping;field-programmable gate array;matrix multiplication;scalability;simulation;sparse matrix;von neumann architecture	Russell Tessier;Salma Mirza;J. Blair Perot	2010			parallel computing;field-programmable gate array;floating point;sparse matrix;sparse matrix-vector multiplication;computer science	HPC	-1.8946790851614266	47.17343470573308	39523
4667a4081465ee58a155e75b6eb9a8fb84258711	low power scheduling method using multiple supply voltages	power supplies;power comsumption;power saving;power supply circuits;control graph;data flow graphs;adjusting phase;data flow graph;scheduling data flow graphs low power electronics power supply circuits;low power;control data flow graph;scheduling;critical path;low power electronics;scheduling phase;scheduling problem;power comsumption multiple supply voltage scheduling control graph data flow graph scheduling phase adjusting phase level shifters;power consumption;delay scheduling algorithm energy consumption processor scheduling power dissipation voltage control flow graphs high level synthesis power system reliability circuits;level shifters;multiple supply voltage scheduling	In this paper, we propose a method to solve the multiple supply voltage scheduling problem which is to assign the operational nodes of a control/data flow graph to a voltage level to minimize the average power consumption within a given computation time. Different from the previous researches focused on the operational nodes in the critical path and utilized the slack time to change the voltage of other nodes, our method can deal with all nodes without considering whether the node is in the critical path or not, and the benefit is that the voltage assignment of each node becomes more flexible. The proposed method consists of two phases, the scheduling phase and the adjusting phase, and considers both the power (delay) of the computational components and the power (delay) of the level shifters. Experimental result shows that using three voltages on a number of standard benchmarks, an average power saving of 34.23% can be obtained if the delay overhead is set as 0, and 48.07% can be obtained if the total delay is set as 1.6 times of the original delay	benchmark (computing);computation;critical path method;dataflow;overhead (computing);scheduling (computing);slack variable;time complexity	Kun-Lin Tsai;Ju-Yueh Lee;Shanq-Jang Ruan;Feipei Lai	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1693828	job shop scheduling;electronic engineering;parallel computing;real-time computing;computer science;operating system;critical path method;data-flow analysis;scheduling;low-power electronics	EDA	16.51587229093595	54.70763313027439	39549
34093708ffa996d888c074d271b0dd9d2ac99070	a unified codesign run-time environment for the ultrasonic reconfigurable computer	reconfiguration;conception conjointe;diseno circuito;reconfiguracion;reconfigurable computing;diseno conjunto;reconfigurable architectures;real time;circuit design;technique video;tecnica video;codesign;scheduling;temps reel;tiempo real;video technique;conception circuit;task management;architecture reconfigurable;ordonnancement;reconfigurable processor;reglamento	This paper presents a codesign environment for the UltraSONIC reconfigurable computing platform which is designed specifically for real-time video applications. A codesign environment with automatic partitioning and scheduling between a host microprocessor and a number of reconfigurable processors is described. A unified runtime environment for both hardware and software tasks under the control of a task manager is proposed. The practicality of our system is demonstrated with an FFT application.	reconfigurable computing	Theerayod Wiangtong;Peter Y. K. Cheung;Wayne Luk	2003		10.1007/978-3-540-45234-8_39	co-design;embedded system;parallel computing;real-time computing;reconfigurable computing;computer science;control reconfiguration;operating system;circuit design;scheduling	EDA	-0.3436870054369553	54.27958522690336	39572
18db11b2339f33510b92fb4947420fa80a545dc9	a fuzzy logic-based system for indoor localization using wifi in ambient intelligent environments	q science general;ieee 802 11 standards fuzzy logic hardware noise artificial intelligence indoor environments continuing professional development;online learning ambient intelligence fuzzy logic systems localization systems;fuzzy logic;qa75 electronic computers computer science;signal processing;wireless lan fuzzy logic learning artificial intelligence signal processing;wireless lan;learning artificial intelligence;ispace fuzzy logic based system indoor localization ambient intelligent environments information paradigm digital environment aie user privacy protection nonintrusive sensors outdoor spaces satellite signals triangulation outdoor localization ultrasound emitters ultrasound antennas radio frequency identification rfid antennas wifi signals domestic spaces access points locations zero cost localization system offline learning process living lab intelligent apartment	Ambient intelligence is a new information paradigm, where people are empowered through a digital environment that is “aware” of their presence and context and is sensitive, adaptive, and responsive to their needs. Hence, one of the important requirements for ambient intelligent environments (AIEs) is the ability to localize the whereabouts of the user in the AIE to address her/his needs. In order to protect user privacy, the use of cameras is not desirable in AIEs, and hence, there is a need to rely on nonintrusive sensors. There are various localization means that are available for outdoor spaces such as those which rely on satellite signals triangulation. However, these outdoor localization means cannot be used in indoor environments. The majority of nonintrusive and noncamera-based indoor localization systems require the installation of extra hardware such as ultrasound emitters/antennas, radio-frequency identification (RFID) antennas, etc. In this paper, we propose a novel indoor localization system that is based on WiFi signals which are free to receive, and they are available in abundance in the majority of domestic spaces. However, free WiFi signals are noisy and uncertain, and their strengths and availability are continuously changing. Hence, we present a fuzzy logic-based system which employs free available WiFi signals to localize a given user in AIEs. The proposed system receives WiFi signals from a large number of existing WiFi access points (up to 170 access points), where no prior knowledge of the access points locations and the environment is required. The system employs an incremental lifelong learning approach to adjust its behavior to the varying and changing WiFi signals to provide a zero-cost localization system which can provide high accuracy in real-world living spaces. We have compared our system in both simulated and real environments with other relevant techniques in the literature, and we have found that our system outperforms the other systems in the offline learning process, whereas our system was the only system which is capable of performing online learning and adaptation. The proposed system was tested in real-world spaces from a living lab intelligent apartment (iSpace) to a town center apartment to a block of offices. In all these experiments, our system has been highly accurate in detecting the user in the given AIEs, and the system was able to adapt its behavior to changes in the AIE or the WiFi signals. We envisage that the proposed system will play an important role in AIEs, especially for privacy concerned situations like elderly care scenarios.	ambient intelligence;digital environment;experiment;fuzzy logic;indoor positioning system;intelligent environment;living lab;offline learning;online and offline;privacy;programming paradigm;radio frequency;radio-frequency identification;requirement;sensor;wireless access point	Teresa García-Valverde;Alberto García-Sola;Hani Hagras;James Dooley;Victor Callaghan;Juan A. Botía Blaya	2013	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2012.2227975	fuzzy logic;embedded system;simulation;telecommunications;computer science;artificial intelligence;signal processing	Mobile	-0.25418573973458897	32.78474266617397	39673
7a94c7afd36ad524574b33fae7f56e40f46028bf	multiple design error diagnosis and correction in digital vlsi circuits	logic design errors;text;digital vlsi circuits;logic design;multiple design error diagnosis;very large scale integration;vlsi circuit design;digital integrated circuits error correction fault diagnosis integrated circuit design vlsi logic cad;integrated circuit design;circuit simulation;binary decision diagrams;digital integrated circuits;test vector simulation;error correction;error diagnosis;vlsi;explosions;circuit testing;iscas 85 benchmark circuits multiple design error diagnosis error correction digital vlsi circuits vlsi circuit design logic design errors test vector simulation error space;computer science;error space;logic cad;error correction very large scale integration circuit synthesis logic design design methodology circuit testing circuit simulation binary decision diagrams explosions benchmark testing;benchmark testing;circuit synthesis;iscas 85 benchmark circuits;fault diagnosis;design methodology	With the increase of circuit size and complexity, logic design errors can occur. Logic design errors are functional mismatches between the speciication and gateelevel implementation. Once a veriication tool has found that the design is erroneous, logic debug-ging must be performed. The research presented in this thesis provides a methodology for multiple design error diagnosis and correction. To diagnose an erroneous design, two algorithms based on testtvector simulation are presented. The rst algorithm is exhaustive on the error space as it exhaustively enumerates the set of all possible error lines and returns the lines of this set that a correction can be applied and rectify the design. The proposed approach exhibits good runntime performance when the number of design errors is less than or equal to two. The second diagnosis approach, uses the results of a testtvector simulation procedure to build a graph. Diierent operations on the graph allow us to explore the error space without performing an explicit enumeration of all error candidates. This makes the method runntime and space eecient for designs corrupted with a larger number of errors. To correct the design, we propose two techniques, one based on testtvector simulation, and one based on Boolean function manipulation techniques. Both correction approaches are based on a design error dictionary which is an extension of the one proposed by Abadir et al. 22. iii Our experimental results show that our algorithms have good error resolution and runntime performance as they are able to rectify designs with one, two and three errors within minutes of CPU time. In addition, our experiments suggest that diagnosis and correction of multiple design errors with input testtvector simulation is an attractive alternative t o s y m bolic techniques. This makes our testtvector simulation based methods applicable to designs where a symbolic representation might n o t b e a vailable. iv To m y parents for their unconditional love, faith, patience, and support v ACKNOWLEDGMENTS I w ould like to begin by expressing my gratitude to my advisor, Professor Ibrahim N. Hajj, for giving me the freedom, allowance, guidance and support throughout the course of my graduate years. who served as the chairpersons of my nal and preliminary doctoral exams, respectively. I w ould also like to take the opportunity to thank Professors Dennis Mickunas, Naresh Shanbhag and Douglas West for treating me with kindness and sharing their experience with me. A …	algorithm;central processing unit;complexity;debugging;dictionary;experiment;gnu variants;graph (discrete mathematics);logic synthesis;open road tolling;rectifier;simulation;subscriber identity module;test vector;very-large-scale integration	Andreas G. Veneris;Ibrahim N. Hajj;Srikanth Venkataraman;W. Kent Fuchs	1999		10.1109/VTEST.1999.766647	electronic engineering;computer science;electrical engineering;theoretical computer science;very-large-scale integration;computer engineering	EDA	20.044420116926194	48.90414856169156	39732
ee06645a641e3253d35c8f133248b13c1718e25c	optimal timing for skew-tolerant high-speed domino logic	sths;timing delay clocks logic gates threshold voltage energy consumption very large scale integration degradation circuit noise strontium;logic simulation;degradation;circuit noise;clocks;very large scale integration;power efficiency;contention free skew tolerant window;delay logic gates;strontium;circuit simulation;mos logic circuits;circuit simulation high speed integrated circuits integrated circuit noise logic gates timing vlsi mos logic circuits logic simulation;logic gates;energy consumption;threshold voltage;vlsi;power efficiency skew tolerant high speed domino logic threshold voltage noise margin sths dual keeper structure delay logic gates timing analysis contention free skew tolerant window clock delay control logic signal skew;timing analysis;clock delay control logic;dual keeper structure;signal skew;skew tolerant high speed domino logic;integrated circuit noise;logic gate;noise margin;high speed;high speed integrated circuits;timing	When low threshold voltage (Vt) is applied to domino logic to improve the performance, the tradeoff between performance and noise margin is a major design issue. To resolve the tradeoff, we propose Skew-Tolerant High-Speed (STHS) domino logic, which incorporates a dual keeper structure and delay logic gates. Detailed timing analysis of STHS domino logic induces optimal timing conditions wherein contention-free skew-tolerant window is maximized. We show that dual keeper structure increases innate noise-tolerance, and clock delay control logic fortifies signal skew-tolerance. Simulation results show that STHS domino logic is more robust to noise and signal skew than High-Speed (HS) domino logic, while presenting better performance and power efficiency.	domino logic;electronic circuit;keeper (password manager);logic gate;logic level;nmos logic;noise margin;performance per watt;power supply;simulation;static timing analysis;transistor	Seong-ook Jung;Ki-Wook Kim;Sung-Mo Kang	2002		10.1109/ISVLSI.2002.1016871	electronic engineering;parallel computing;real-time computing;logic optimization;logic level;computer science;pass transistor logic;sequential logic	EDA	17.702884343131593	58.79504483011734	39844
aba3c97e13048f7be00bc703fbcb9d82bf833376	fpga prototyping of a multi-million gate system-on-chip (soc) design for wireless usb applications	field programmable gate array;fpga synthesis;functional verification;synthesis constraints;fpga;clock gating;asic application specific integrated circuits;ecma 368;system on chip;application specific integrated circuit;software development;time to market;fpga physical implementation;verification and validation;soc system on chip	The complexity and costs involved in today's SoC designs makes Field Programmable Gate Arrays (FPGA's) prototyping of ASIC's as means of pre-silicon SoC validation, to accelerate system software development and to meet the time-to-market requirements. In this paper, we present a FPGA prototyping used in the implementation, verification and validation of a multimillion gate SoC designed for wireless USB application. The purpose of the prototyping was to serve as a method for architectural validation which reduces development cost and avoid duplication of design effort.	application-specific integrated circuit;fpga prototyping;field-programmable gate array;mpsoc;prototype;requirement;software development;system on a chip;verification and validation;wireless usb	P. Subramanian;Jagonda Patil;Manish Kumar Saxena	2009		10.1145/1582379.1582676	embedded system;computer architecture;computer science;fpga prototype;field-programmable gate array	EDA	4.557702045938305	51.37057852468058	39859
abb368d16a6e420c5985da786bb7c26f340ab606	analysis of multiple parallel block coding in jpeg2000	parallel processing image coding;image coding;hardware software codesign;data compression;block codes discrete wavelet transforms hardware image coding transform coding switches software libraries software standards communication switching fabrics;parallel processing multiple parallel block coding jpeg2000 compression standard system on a chip soc software hardware codesign platform parallel coding scheduling granularity altera nios ii processor flexible integrated peripheral image coding;indexing terms;system on a chip;system on chip block codes data compression hardware software codesign image coding parallel processing;system on chip;block codes;parallel processing	We present the analysis and results for a system on a chip (SoC) software/hardware codesign platform, for parallel coding in JPEG2000 compression standard. We show that there are optimum numbers of parallel block coders and scheduling granularity per row of codeblocks. The system was implemented on an Altera NIOS II processor with flexible integrated peripheral.	code::blocks;jpeg 2000;motherboard;peripheral;scheduling (computing);system on a chip	Michael Dyer;Saeid Nooshabadi;David S. Taubman	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379793	system on a chip;embedded system;parallel processing;parallel computing;real-time computing;computer science;context-adaptive binary arithmetic coding;statistics	Robotics	11.170243333906557	40.76024864124707	39860
2d3ecd387392f58fc69d8f5e3150ee0e45bc03f0	a standard cell initial placement strategy	sandia national laboratories;placement scheme;adverse trait;initial placement;standard cell;constructive placement method;ic layout;standard cell placement;initial placement strategy;placement strategy;advantageous property;global connectivity information characteristic;algorithms exhibit;top-down partitioning scheme;integrated circuit layout;bottom up;top down;microelectronics;electronics industry;integrated circuit;computer aided design;process design;design automation	A standard cell initial placement strategy has been developed that incorporates characteristics of both the class of algorithms that is constructive in nature (i.e., bottom-up) and the class that utilizes a top-down partitioning scheme. This approach has been pursued recognizing the fact that while both of these types of algorithms exhibit some rather adverse traits both also possess advantageous properties. Specifically, the placement strategy described in this paper incorporates both the simplicity of constructive placement methods and the global connectivity information characteristic of placement schemes involving partitioning. The specified algorithm has been implemented within the microelectronics computer-aided design facility at Sandia National Laboratories.	algorithm;bottom-up proteomics;computer-aided design;standard cell;top-down and bottom-up design	Bill D. Richard	1984	21st Design Automation Conference Proceedings		electronic engineering;electronic design automation;engineering;electrical engineering;computer aided design;top-down and bottom-up design;integrated circuit layout;engineering drawing;placement;computer engineering	EDA	14.637318559110275	49.88762104116265	39884
e7784a637474408937bb18c1ca4dded47a6ee836	on exploiting partitioning-based placement approach for performances improvement of 3d fpga		Three-dimensional Field Programmable Gate Arrays (3D FPGAs) represent a viable alternative to overcome challenges of integration complexity in modern embedded systems. Mapping applications into 3D FPGAs requires a set of accompanying suite of Computer-Aided Design (CAD) tools. One of critical issue of a 3D FPGA-based implementation is the quality and efficiency of associated CAD algorithms. In this paper, we are interested to investigate placement algorithms aspect to optimize proposed 3D FPGA performances. In fact, the way we distribute clusters between 3D FPGA layers has an important impact on performances. We present partitioning- based placement algorithm for 3D FPGA. The circuit is first divided into two layers with limited number of inter-layer interconnections, and then placed on individual layers. Placement solution of each layer is then gradually improved using adapted simulated annealing algorithm. We conduct experiments using exploration platform to compare partitioning-based and simulated annealing based placement approaches for proposed 3D FPGA architecture. Exploration results show that using partitioning-based placement algorithm achieves a saving in terms of power consumption, area and performance by an average of 15%, 18% and 10% Unlike DFPGA, MS-FPGA can deal with complex circuits.	algorithm;computer-aided design;embedded system;experiment;field-programmable gate array;performance;simulated annealing	Sonda Chtourou;Mohamed Abid;Zied Marrakchi;Emna Amouri;Habib Mehrez	2017	2017 International Conference on High Performance Computing & Simulation (HPCS)	10.1109/HPCS.2017.91	parallel computing;field-programmable gate array;architecture;placement;reconfigurable computing;electronic circuit;real-time computing;simulated annealing;computer science	EDA	2.770569496723141	59.18930949858758	39893
1415c481153c4815d8191fb950d52a076d1b2900	solving systems of linear equations on the cell processor using cholesky factorization	linear algebra;linear systems;linear systems parallel algorithms numerical linear algebra;65 numerical analysis;processor architecture;symmetric positive definite;parallel algorithm;iterative algorithms;cell processor;lightweight decentralized synchronization;linear system;cell broadband engine;short vector single instruction multiple data cores;cholesky factorization;synchronisation;equations registers parallel processing arithmetic iterative algorithms linear algebra engines lifting equipment bandwidth performance gain;fine grained task granularity;engines;sony toshiba ibm cell processor;registers;task analysis instruction sets parallel algorithms synchronisation;task analysis;numerical algorithm;performance gain;arithmetic;bandwidth;68 computer science;lifting equipment;numerical linear algebra;linear equations;lightweight decentralized synchronization linear equations cholesky factorization sony toshiba ibm cell processor processor architecture short vector single instruction multiple data cores fine grained task granularity;parallel processing;instruction sets;parallel algorithms	The Sony/Toshiba/IBM (STI) CELL processor introduces pioneering solutions in processor architecture. At the same time it presents new challenges for the development of numerical algorithms. One is effective exploitation of the differential between the speed of single and double precision arithmetic; the other is efficient parallelization between the short vector SIMD cores. The first challenge is addressed by utilizing the well known technique of iterative refinement for the solution of a dense symmetric positive definite system of linear equations, resulting in a mixed-precision algorithm, which delivers double precision accuracy, while performing the bulk of the work in single precision. The main contribution of this paper lies in addressing the second challenge by successful thread-level parallelization, exploiting fine-grained task granularity and a lightweight decentralized synchronization. The implementation of the computationally intensive sections gets within 90 percent of peak floating point performance, while the implementation of the memory intensive sections reaches within 90 percent of peak memory bandwidth. On a single CELL processor, the algorithm achieves over 170~Gflop/s when solving a symmetric positive definite system of linear equation in single precision and over 150~Gflop/s when delivering the result in double precision accuracy.	algorithm;care-of address;cell (microprocessor);cholesky decomposition;double-precision floating-point format;entity–relationship model;iterative method;iterative refinement;kilobyte;linear algebra;linear equation;memory bandwidth;numerical analysis;parallel computing;refinement (computing);simd;scheduling (computing);single-precision floating-point format;system of linear equations;whole earth 'lectronic link	Jakub Kurzak;Alfredo Buttari;Jack J. Dongarra	2008	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2007.70813	parallel processing;parallel computing;real-time computing;computer science;theoretical computer science;linear algebra;operating system;database;distributed computing;extended precision;parallel algorithm;linear system;programming language;algorithm	HPC	-1.4280515888246408	39.169591606928854	39896
31d59ddc207e644deb2e3aa84101ccf107edd032	scalar multiplication in elliptic curve cryptosystems: pipelining with pre-computations			computation;pipeline (computing);scalar processor	Pradeep Kumar Mishra	2004	IACR Cryptology ePrint Archive			Crypto	8.966131258991426	42.75507655468042	39898
e6e4cc789461ec4ea2014010cbd0ae29bddee560	decimal-binary conversions in cordic	application software;clocks;binary codes;shift registers;digital arithmetic;shift registers digital arithmetic binary codes application software goniometers clocks equations;goniometers	A special-purpose, binary computer called CORDIC 90° (COordinate Rotation DIgital Computer) contains a unique arithmetic 0.10 unit composed of three shift registers, three adder-subtractors, and suitable interconnections for efficiently performing calculations in13545 volving trigonometric functions. A technique is formulated for using 0.11 0 01 the CORDIC arithmetic unit to convert between angles expressed in degrees and minutes in the 8, 4, 2, 1 code and angles expressed in POSITIVE binary fractions of a half revolution. Decimal-to-binary conversion is accomplished through the generation of an intermediate binary o code in which the variable values are + 1 and -1. Each of these 180 0	adder (electronics);arithmetic logic unit;cordic;shift register	Dan H. Daggett	1959	IRE Trans. Electronic Computers	10.1109/TEC.1959.5222694	arithmetic;binary code;application software;goniometer;computer science;theoretical computer science;mathematics;shift register;algorithm	Graphics	14.623527214806126	43.78767211360642	39910
35996ee133b68b0683e8dd238a37cddafb10f64c	a genetic algorithm for register allocation	optimal solution;register allocation;logic cad genetic algorithm register allocation merge operator population matrix offspring parents linear time complexity;computational complexity;linear time;genetic algorithm;genetic algorithms;circuit optimisation genetic algorithms logic cad computational complexity;genetic algorithms hamming distance registers biological cells genetic engineering petroleum minerals testing pattern classification machine learning;circuit optimisation;logic cad	In this paper we introduce a new genetic algorithm for register allocation. A merge operator is used to generate new individual solutions. The number of steps required to examine all pairs in the population matrix to generate n (n is the population matrix size). Generating an offspring from the parents needs m steps ( m number of nodes). The total number of steps required by the algorithm is n m 2 , that is, the genetic algorithm has a linear time complexity in terms of number of nodes. The experimental results show optimal solutions in many of the graphs used for testing.	genetic algorithm;matrix multiplication;merge sort;register allocation;time complexity	Khaled M. Elleithy;E. G. Abd-El-Fattah	1999		10.1109/GLSV.1999.757416	mathematical optimization;genetic algorithm;computer science;theoretical computer science;genetic operator;machine learning;genetic representation;algorithm;population-based incremental learning	Logic	16.680051330546533	48.413599487209446	39930
511c11c4bcf3028c46990a799d18284a014c7c43	parallel prefix on mesh of trees and otis mesh of trees		$ The corresponding author, Member, IEEE and IEEE Computer Society Abstract: In this paper, we first develop a parallel algorithm for prefix computation on an n × n mesh of trees (MOT). For n data elements, the algorithm requires 4 log n + O(1) time using n2 processors. Using the MOT prefix, we next propose a prefix algorithm on an n × n OTIS mesh of trees. This algorithm for n4 data elements is shown to map in 13 log n + O(1) electronic moves + 2 OTIS moves using n processors.	central processing unit;computation;parallel algorithm	Dheeresh K. Mallick;Prasanta K. Jana	2008			parallel computing;computer science;computation;parallel algorithm;distributed computing;binary logarithm;prefix	Theory	13.001824433253859	32.44961275424906	39935
301e7d8483bd55c51638878cedcf786b16547efa	sizing power/ground meshes for clocking and computing circuit components	timing relationsbetween;bythe computing block;ground mesh;circuit component delay;terms ofthe power;area improvement;sizing power;ground ir-drops;ir-droprelated timing violation;computing circuit components;power andground node potential;ground meshes;computing component;drams;power dissipation;nonlinear programming;voltage;logic;linear programming;cost function;integrated circuit layout	This paper presents a new formulation and an efficientsolution of the power and ground mesh sizing problem. Weuse the key observations that (1) the drops in power andground node potentials are due not only to currents drawn bythe computing blocks, but also to those drawn by the clockbuffers, and (2) changes of circuit component delays arelinearly proportional to the power/ground IR-drops. Thisleads to a linear quantification of the timing relationsbetween the clocking and computing components in terms ofthe power/ground IR-drops. Our method removes all IR-droprelated timing violations that occur in about 2% of pathswhen grids are sized using the existing methods that satisfythe maximum IR-drop constraints. In addition, we achievesupply mesh area improvements of the order of 30% whilesimultaneously reducing the power dissipated in the circuitsby about 6.6% compared to traditional grid sizing methods.	clock rate;synchronous circuit;transponder timing	Arindam Mukherjee;Kai Wang;Lauren Hui Chen;Malgorzata Marek-Sadowska	2002			control engineering;embedded system;mathematical optimization;electronic engineering;real-time computing;nonlinear programming;engineering;linear programming;electrical engineering;dissipation;integrated circuit layout;dram	EDA	16.080798199679315	53.534612224979966	39974
6b3865fd86181756c93d913fe8bde1d2d52e0558	reconfigurable hardware from programmable logic devices	programmable logic device;emerging technology;field programmable logic;reconfigurable hardware	Reconfigurable hardware is an emerging technology that utilises SRAM based field programmable logic devices to implement functions in hardware to accelerate processing functions. Using the reconfigurable aspect of the programmable logic device this hardware function can then be changed allowing alternative functions to be implemented. This article will describe a hardware platform — the Altera Re configurable Interconnect Peripheral Processor (RIPP 10) that is now available to support research in this area.	field-programmable gate array;programmable logic device	Nigel Toon	1994		10.1007/3-540-58419-6_101	erasable programmable logic device;computer architecture;logic synthesis;macrocell array;logic gate;logic family;reconfigurable computing;programmable logic array;computer science;programmable logic device;complex programmable logic device;hardware description language;simple programmable logic device;emerging technologies;digital electronics;programmable array logic;register-transfer level;field-programmable gate array	EDA	5.2794051920705245	49.37876076453048	39975
25eafc88e1dce7daa20227a475a4908b27ed1d12	towards efficient arithmetic for lattice-based cryptography on reconfigurable hardware	ideal lattices;fft;fpga implementation;lattice based cryptography;ntt;reconfigurable hardware	In recent years lattice-based cryptography has emerged as quantum secure and theoretically elegant alternative to classical cryptographic schemes (like ECC or RSA). In addition to that, lattices are a versatile tool and play an important role in the development of efficient fully or somewhat homomorphic encryption (SHE/FHE) schemes. In practice, ideal lattices defined in the polynomial ring Zp[x]/〈x + 1〉 allow the reduction of the generally very large key sizes of lattice constructions. Another advantage of ideal lattices is that polynomial multiplication is a basic operation that has, in theory, only quasi-linear time complexity of O(n logn) in Zp[x]/〈x +1〉. However, few is known about the practical performance of the FFT in this specific application domain and whether it is really an alternative. In this work we make a first step towards efficient FFT-based arithmetic for lattice-based cryptography and show that the FFT can be implemented efficiently on reconfigurable hardware. We give instantiations of recently proposed parameter sets for homomorphic and public-key encryption. In a generic setting we are able to multiply polynomials with up to 4096 coefficients and a 17-bit prime in less than 0.5 milliseconds. For a parameter set of a SHE scheme (n=1024,p=1061093377) our implementation performs 9063 polynomial multiplications per second on a mid-range Spartan-6.	application domain;automated planning and scheduling;chebyshev polynomials;coefficient;coprocessor;corner case;ecc memory;fast fourier transform;field-programmable gate array;homomorphic encryption;ideal lattice cryptography;key size;lattice-based cryptography;learning with errors;like button;polynomial ring;post-quantum cryptography;public-key cryptography;sampling (signal processing);sparse matrix;spectral leakage;time complexity	Thomas Pöppelmann;Tim Güneysu	2012		10.1007/978-3-642-33481-8_8	lattice-based cryptography;combinatorics;discrete mathematics;theoretical computer science;mathematics	Crypto	9.109489460849215	43.40608370022327	40019
54a41dcd6c38d6afefeee2ce33047b0e3cf1f6e9	automatic opencl device characterization: guiding optimized kernel design	benchmarking;paper;tesla c2050;ati;cell processor;nvidia;algorithms;nvidia geforce gtx 275;optimization;computer science;nvidia geforce gtx 460;opencl;ati radeon hd 5870	The OpenCL standard allows targeting a large variety of CPU, GPU and accelerator architectures using a single unified programming interface and language. While the standard guarantees portability of functionality for complying applications and platforms, performance portability on such a diverse set of hardware is limited. Devices may vary significantly in memory architecture as well as type, number and complexity of computational units. To characterize and compare the OpenCL performance of existing and future devices we propose a suite of microbenchmarks, uCLbench. We present measurements for eight hardware architectures – four GPUs, three CPUs and one accelerator – and illustrate how the results accurately reflect unique characteristics of the respective platform. In addition to measuring quantities traditionally benchmarked on CPUs like arithmetic throughput or the bandwidth and latency of various address spaces, the suite also includes code designed to determine parameters unique to OpenCL like the dynamic branching penalties prevalent on GPUs. We demonstrate how our results can be used to guide algorithm design and optimization for any given platform on an example kernel that represents the key computation of a linear multigrid solver. Guided manual optimization of this kernel results in an average improvement of 61% across the eight platforms tested.	algorithm design;application programming interface;benchmark (computing);central processing unit;compiler;computation;graphics processing unit;linear programming relaxation;mathematical optimization;memory bandwidth;microarchitecture;multigrid method;opencl api;software portability;solver;throughput	Peter Thoman;Klaus Kofler;Heiko Studt;John Thomson;Thomas Fahringer	2011		10.1007/978-3-642-23397-5_43	computer architecture;parallel computing;computer hardware;computer science;operating system;algorithm;benchmarking	Arch	-1.41424189532643	45.525538780760854	40087
b9fc1d042273b64f5a3f23f6daf7783607dd9c5a	a gpu algorithm for greedy graph matching.	paper;tesla c2050;cuda;package;nvidia;algorithms;computer science	Greedy graph matching provides us with a fast way to coarsen a graph during graph partitioning. Direct algorithms on the CPU which perform such greedy matchings are simple and fast, but offer few handholds for parallelisation. To remedy this, we introduce a fine-grained shared-memory parallel algorithm for maximal greedy matching, together with an implementation on the GPU, which is faster (speedups up to 6.8 for random matching and 5.6 for weighted matching) than the serial CPU algorithms and produces matchings of similar (random matching) or better (weighted matching) quality.	central processing unit;graph (discrete mathematics);graph partition;graphics processing unit;greedy algorithm;matching (graph theory);maximal set;memory bandwidth;multi-core processor;parallel algorithm;parallel computing;shared memory;speedup;technical support	B. O. Fagginger Auer;Rob H. Bisseling	2011		10.1007/978-3-642-30397-5_10	greedy randomized adaptive search procedure;mathematical optimization;computer science;3-dimensional matching;theoretical computer science;machine learning	Theory	-2.7277092338596245	42.18914494905181	40094
824fe3f843fa212e189249f735b64a23d664e80b	an investigation of power delay trade-offs for dual v/sub t/ cmos circuits	design automation;cmos technology;low leakage power dissipation;hspice simulation;availability;cad;circuit design;logic cad cmos logic circuits circuit optimisation spice digital simulation logic gates;cmos process;cmos gates;circuit simulation;deep submicron technology;leakage power;logic gates;delay cmos process power dissipation availability cmos technology digital circuits circuit simulation software design design automation circuit optimization;power delay trade offs;cmos logic circuits;dual v t cmos circuits;power dissipation;software development;delay analysis;cad power delay trade offs dual v sub t cmos circuits low leakage power dissipation deep submicron technology digital circuits delay analysis hspice simulation cmos gates experimental results circuit optimization;process model;digital circuits;circuit optimisation;software design;logic cad;experimental results;high performance;spice;digital simulation;circuit optimization	The availability of the dual V/sub t/ CMOS process provides a practical way to achieve high performance and low leakage power dissipation for current deep submicron technology. Early work on leakage power optimization of digital circuits utilizing dual V/sub t/ devices show some promising results (Kao et al., 1997). However, due to the lack of real dual V/sub t/ process models and parameters, these works are based on simple power and delay analysis of dual V/sub t/ devices. For example, the impact of dual V/sub t/ on the short circuit power dissipation is ignored in all these works. We provide extensive HSPICE simulation results on CMOS gates and circuits from a commercial dual V/sub t/ CMOS process. The experimental results show that optimization of dual V/sub t/ circuits involves complex trade-offs between leakage power, short circuit power and performance. For example, it is observed that using lower V/sub t/ devices does not always result in a faster circuit. One of the main contributions of this paper is that it reveals some new challenges and opportunities offered by the dual V/sub t/ technology to both circuit designers and CAD software developers for circuit optimization.		Qi Wang;Sarma B. K. Vrudhula	1999		10.1109/ICCD.1999.808595	availability;electronic engineering;real-time computing;electronic design automation;logic gate;engineering;electrical engineering;software design;dissipation;software development;circuit design;process modeling;cad;cmos;digital electronics	EDA	21.122248171273753	58.12531504958347	40214
a390688d3581b0c9bc23555d04a894fa685f9fe0	power and area optimization for multiple restricted multiplication	power saving;multiplying circuits;optimization technique;multiple restricted multiplication problem;linear programming multiplying circuits circuit optimisation field programmable gate arrays integer programming;integer linear program power optimization area optimization optimization technique multiple restricted multiplication problem fpga architecture time step based optimization high level power modelling;integer programming;fpga architecture;time step based optimization;power optimization;power engineering and energy filters;linear programming;high level power modelling;field programmable gate arrays;circuit optimisation;integer linear program;area optimization	This paper presents a design and optimization technique for the multiple restricted multiplication problem [N. Sidahao, G. A. Constantinides, and F. Y. Cheung (2004)]. This refers to a situation where a single variable is multiplied by several coefficients which, while not constant, are drawn from a finite set of constants that change with time. The approach exploits dedicated registers in FPGA architecture for further time-step based optimization over previous approaches [N. Sidahao, G. A. Constantinides, and F. Y. Cheung. S. S. Demirsoy, A. G. Dempster, and I. Kale (2003)]. It is also combined with an effective technique, based on high-level power modelling, for power optimization. The problem is formulated into an integer linear program for finding solutions to the minimum-costs. The new approach results up to 22% area saving compared to the optimal non-register approach in [N. Sidahao, G. A. Constantinides, and F. Y. Cheung (2004)], and 80% of all results also show 21%-48% power savings.	coefficient;field-programmable gate array;high- and low-level;linear programming;mathematical optimization;power optimization (eda)	Nalin Sidahao;George A. Constantinides;Peter Y. K. Cheung	2005	International Conference on Field Programmable Logic and Applications, 2005.	10.1109/FPL.2005.1515708	mathematical optimization;integer programming;computer science;linear programming;power optimization;algorithm;field-programmable gate array	EDA	0.09783736926735224	52.55359191141255	40226
27723d24610556603e3c8a53ed9d59ba7f6a0bd5	balancing exploration and exploitation in an adaptive three-dimensional cellular genetic algorithm via a probabilistic selection operator	exploration tradeoff adaptive three dimensional cellular genetic algorithm probabilistic selection operator adaptive gradual algorithm silicon chip technology search space exploitation tradeoff;topology;topology algorithm design and analysis adaptation model power capacitors space exploration genetics probabilistic logic;probability;image processing;system configuration;network on chip;design flow;power capacitors;space exploration;multimedia application;three dimensional;genetics;adaptation model;complex system;system on chip;search problems genetic algorithms probability;cache performance;genetic algorithm;genetic algorithms;search problems;memory hierarchy;probabilistic logic;algorithm design and analysis	Pre-fetching in a memory hierarchy is known to alleviate the “memory wall” paradigm but its use is impeded because of the difficulty to estimate efficiency when used in a complex system such as a SoC (System on Chip) or NoC (Network on Chip). Therefore, some methods are needed to evaluate the benefit of pre-fetching at the earliest possible stage in a design flow to help the designer choose architectural parameters or transform the application algorithm. In this paper we show that the emulation platform implementing the nD-AP Cache (n-Dimensional Adaptive and Predictive Cache) allows to perform a platform-independent measurement of this cache efficiency. The nD-AP Cache performs pre-fetching in multidimensional arrays which are commonly used in image processing and multimedia applications. The obtained metric can be used to extrapolate the cache performance in a much broader system configuration. The method to compute this metric is the calibration process. The performed benchmarks show that the calibration process is confident. Also, we measured that the nD-AP Cache is two times faster than a standard PowerPC 2-way set associative cache in the context of an image processing kernel.	benchmark (computing);cpu cache;cellular evolutionary algorithm;complex system;design flow (eda);emulator;extrapolation;genetic algorithm;image processing;memory hierarchy;network on a chip;powerpc;programming paradigm;random-access memory;symmetric multiprocessing;system configuration;system on a chip	Asmaa Al-Naqi;Ahmet T. Erdogan;Tughrul Arslan;Yves Mathieu	2010	2010 NASA/ESA Conference on Adaptive Hardware and Systems	10.1109/AHS.2010.5546248	embedded system;pipeline burst cache;cache-oblivious algorithm;real-time computing;cache coloring;genetic algorithm;image processing;cache;computer science;artificial intelligence;theoretical computer science;cache invalidation;operating system;machine learning;cache algorithms;cache pollution;statistics	HPC	1.5082473863405839	55.895778503629344	40269
1d3bc6d95ed1e673e5d81d3892d0338a97bacfb4	case: a reliability simulation tool for analog ics		With the evolution in the scale of integration in ICs, aging-related problems are becoming more important and, nowadays, solutions to cope with these issues are not yet mature enough, especially in the field of analog circuit simulation. CASE, the novel simulator presented in this paper, can evaluate the impact of reliability effects in analog circuits through a stochastic physic-based model. The implemented simulation flow is accurate and efficient in terms of CPU. The two main improvements over currently reported and commercial tools, is that the simulator can simultaneously take into account both time-zero and time-dependent variability, and that an adaptive method, to account for the strong link between biasing and stress, can improve the accuracy while keeping acceptable CPU times.	adaptive algorithm;analogue electronics;biasing;central processing unit;computer-aided software engineering;electronic circuit simulation;graphical user interface;heart rate variability;performance;usability	P. Martín-Lloret;A. Toro-Frias;R. Castro-López;Elisenda Roca;Francisco V. Fernández;Javier Martín-Martínez;Rosana Rodríguez;Montserrat Nafría	2017	2017 14th International Conference on Synthesis, Modeling, Analysis and Simulation Methods and Applications to Circuit Design (SMACD)	10.1109/SMACD.2017.7981588	computer-aided software engineering;control engineering;analogue electronics;biasing;engineering;electronic engineering	EDA	24.257996343054096	57.440934898045086	40335
8a3be17cce86fd80b8cc64411713700d2eaf8870	designing hardware with dynamic memory abstraction	bluespec;upper bound;high level synthesis;dynamic memory;c to gates;program analysis;hardware description language;parallel execution;automatic parallelization	Recent progress in program analysis has produced tools that are able to compute upper bounds on the use of dynamic memory. This opens up a space for the use of dynamic memory abstraction in high-level synthesis. In this paper, we explain how to design hardware using C programs with malloc() and free(). A compilation process is outlined for transforming C programs with heap operations into a hardware description language. As demonstrated by our experiments, this approach is feasible. Further, automatic parallelization of the generated circuits improves by a factor up to 1.9 in terms of clock frequency and a factor up to 2.7 in terms of clock cycles over the previous work.	automatic parallelization;c dynamic memory allocation;clock rate;clock signal;compiler;experiment;hardware description language;high- and low-level;high-level synthesis;memory management;parallel computing;program analysis	Jirí Simsa;Satnam Singh	2010		10.1145/1723112.1723125	program analysis;embedded system;computer architecture;parallel computing;dynamic random-access memory;computer science;theoretical computer science;operating system;hardware description language;c dynamic memory allocation;upper and lower bounds;high-level synthesis;automatic parallelization	PL	1.1479776135761832	50.536845442280374	40336
1e9c9910409af7cc5d1829f81e6165329e36dc90	soda: a high-performance dsp architecture for software-defined radio	simd;protocols;embedded processor software defined radio dsp simd multicore soda;broadband networks;software defined radio;data processing;digital signal processing wireless application protocol hardware mobile communication throughput application software signal processing computer architecture multiaccess communication process control;software radio;computer architecture;code division multiple access;multicore;signal processing;mobile radio;digital signal processing chips;soda;wireless lan;wireless lan broadband networks code division multiple access computer architecture digital signal processing chips mobile radio protocols software radio;dsp architecture software defined radio signal processing on demand architecture programmable architecture w cdma 802 11a power performance requirement ultrawide simd execution;high performance;embedded processor;mobile terminal;dsp	Software-defined radio (SDR) belongs to an emerging class of applications with the processing requirements of a supercomputer but the power constraints of a mobile terminal. The authors developed the signal-processing on-demand architecture (SODA), a fully programmable architecture that supports SDR, by examining two widely differing protocols, W-CDMA and 802.11A. It meets power-performance requirements by separating control and data processing and by employing ultrawide SIMD execution	digital signal processor;etsi satellite digital radio;mobile phone;requirement;simd;signal processing;supercomputer	Yuan Lin;Hyunseok Lee;Mark Woh;Yoav Harel;Scott A. Mahlke;Trevor N. Mudge;Chaitali Chakrabarti;Krisztián Flautner	2007	IEEE Micro	10.1109/MM.2007.22	embedded system;parallel computing;real-time computing;data processing;computer science;operating system;signal processing;software-defined radio	Arch	3.0173660393000477	48.05572197919834	40340
5b9271c27acab2a9dcc9feada884f3df380a9286	open cores for digital signal processing.		This paper presents the design and implementation of three System on Chip (SoC) cores, which implement the Digital Signal Processing (DSP) functions: Finite Impulse Response (FIR) filter, Infinite Impulse Response (IIR) filter and Fast Fourier Transform (FFT). The FIR filter core is based on the symmetrical realization form, the IIR filter core is based on the Second Order Sections (SOS) architecture and the FFT core is based on the Radix 2 Single Delay Feedback (R2SDF) architecture. The three cores are compatible with the Wishbone SoC bus and they were described using generic and structural VHDL. In system hardware verification was performed by using an OpenRisc-based SoC synthesized on an Altera FPGA, the tests showed that the designed DSP cores are suitable for building SoC based on the OpenRisc processor and the Wishbone bus.	cyclone;digital signal processing;fast fourier transform;field-programmable gate array;finite impulse response;generic programming;infinite impulse response;opencores;openrisc;stratix;system on a chip;the moon is a harsh mistress;vhdl;wishbone (computer bus)	Juan Camilo Valderrama-Cuervo;Alexander López-Parrado	2014	CoRR		system on a chip;parallel computing;field-programmable gate array;architecture;openrisc;finite impulse response;digital signal processing;infinite impulse response;wishbone;electronic engineering;computer science	EDA	7.401059042237051	48.725784317103205	40404
c1e38c113b86f8f84f3c54be2998aa26c2f05e81	electrostatic discharge (esd) protection of rf integrated circuits	cmos integrated circuits;size 22 nm electrostatic discharge protection esd electrostatic discharge induced failures semiconductor industry cmos technology rf integrated circuits;radiofrequency integrated circuits cmos integrated circuits electrostatic discharge integrated circuit reliability;electrostatic discharge;electrostatic discharges cmos integrated circuits radio frequency cmos technology discharges electric silicon;integrated circuit reliability;radiofrequency integrated circuits	Electrostatic discharge (ESD) induced failures continue to be a major reliability concern in the semiconductor industry. Such a concern will in fact be intensified as the CMOS technology is scaling toward the 22-nm and beyond. This paper covers the issues and challenges pertinent to the design of electrostatic discharge (ESD) protection solutions of CMOS-based RF integrated circuits.	cmos;discharger;electrostatic loudspeaker;image scaling;integrated circuit;rfic;radio frequency;relevance;semiconductor industry	Juin J. Liou;Chang Jiang;Feng Chia	2012	2012 IEEE Asia Pacific Conference on Circuits and Systems	10.1109/APCCAS.2012.6419071	embedded system;electronic engineering;electrostatic discharge;computer science;engineering;electrical engineering;cmos	EDA	12.352190365912062	57.55454348766014	40442
3e9c8684b6fb01dabfe61f95f8703d15982abda5	accelerating sparse arithmetic in the context of newton’s method for small molecules with bond constraints	computer engineering;datorteknik	Molecular dynamics is used to study the time evolution of systems of atoms. It is common to constrain bond lengths in order to increase the time step of the simulation. Here we accelerate Newton’s method for solving the constraint equations for a system consisting of many identical small molecules. Starting with a modular and generic base code using a sequential data layout, we apply three different optimization techniques. The compiled code approach is used to generate subroutines equivalent to a single step of Newton’s method for a user specified molecule. Differing from the generic subroutines, these specific routines contain no loops and no indirect addressing. Interleaving the data describing different molecules generates vectorizable loops. Finally, we apply task fusion. The simultaneous application of all three techniques increases the speed of the base code by a factor of 15 for single precision calculations.	addressing mode;advanced vector extensions;algorithm;cholesky decomposition;code;compiler;forward error correction;mathematical optimization;molecular dynamics;newton;newton's method;simulation;single-precision floating-point format;solver;sparse matrix;speedup;subroutine	Carl Christian Kjelgaard Mikkelsen;Jesús Alastruey-Benedé;Pablo Ibáñez;Pablo García-Risueño	2015		10.1007/978-3-319-32149-3_16	mathematical optimization;computer science;theoretical computer science;algorithm	Comp.	-4.508130762429689	37.304358302171146	40460
4e6a9f79289d089768aea8b54e97b444e54269d5	mpidepqbf: towards parallel qbf solving without knowledge sharing		Inspired by recent work on parallel SAT solving, we present a lightweight approach for solving quantified Boolean formulas (QBFs) in parallel. In particular, our approach uses a sequential state-of-the-art QBF solver to evaluate subformulas in working processes. It abstains from globally exchanging information between the workers, but keeps learnt information only locally. To this end, we equipped the state-ofthe-art QBF solver DepQBF with assumption-based reasoning and integrated it in our novel solver MPIDepQBF as backend solver. Extensive experiments on standard computers as well as on the supercomputer Tsubame show the impact of our approach.	boolean satisfiability problem;computer;coprocessor;experiment;magnetic-core memory;manycore processor;multi-core processor;solver;stemming;supercomputer;true quantified boolean formula;tsubame (supercomputer)	Charles Jordan;Lukasz Kaiser;Florian Lonsing;Martina Seidl	2014		10.1007/978-3-319-09284-3_32	theoretical computer science;discrete mathematics;computer science;conjunctive normal form;true quantified boolean formula;knowledge sharing;solver;supercomputer	AI	0.8830940871455885	44.5975170369658	40496
2c62f67a5b424777312eefc2a16fc0b7808e3664	sequential circuit delay optimization using global path delays	network synthesis;constraint optimization;clocks;optimization technique;sequential circuits;flip flops;satisfiability;logic synthesis;integrated circuit interconnections;delay sequential circuits constraint optimization clocks circuit synthesis optimization methods integrated circuit interconnections flip flops costs network synthesis;network flow;flip flop;circuit synthesis;optimization methods;partial order	We propose a novel sequential delay optimization technique based on network flow methods that simultaneously exploits delays on all paths in the circuit. We view the sequential circuit as an interconnection of path segments with pre-specified delays. Path segments are bounded by flip-flops, primary inputs or primary outputs. Recognizing that a delay optimizer can satisfy certain delay constraints more easily than others, we first propose a measure of difficulty for the delay optimizer. Our measure is based on explicit path delays to be satisfied by the delay optimizer. Also, our measure induces a partial order on the set of possible delay constraints. We then compute a set of delay constraints that is optimal with respect to our measure. The delay constraint set is optimal in the sense that it is the easiest constraint that can be specified to the delay optimizer. We formulate the delay constraint calculation problem as a minimum cost network flow problem. If the delay optimizer satisfies the optimal delay constraint set, then the resynthesized circuit may have several paths exceeding the desired clock period. However, we show that the resynthesized circuit can always be retimed to achieve the desired clock period. Experimental results on MCNC synthesis benchmarks show that our method improves the performance of circuits beyond what is achievable using optimal retiming and conventional combinational logic synthesis.	clock rate;combinational logic;flops;flip-flop (electronics);flow network;interconnection;logic synthesis;mathematical optimization;retiming;sequential logic	Srimat T. Chakradhar;Sujit Dey;Miodrag Potkonjak;Steven G. Rothweiler	1993	30th ACM/IEEE Design Automation Conference	10.1145/157485.164991	partially ordered set;network synthesis filters;mathematical optimization;constrained optimization;electronic engineering;logic synthesis;real-time computing;flow network;asynchronous circuit;delay calculation;computer science;elmore delay;mathematics;sequential logic;satisfiability	EDA	17.041596797360626	51.58790892276351	40641
3d67253623702a5c98912c1a95f4e63e6c6f4f17	algebraic topology and distributed computing: a primer	distributed computing;concurrent process;lower bound	"""they are elementary, being fully covered in the rst chapter of Munkres' standard textbook [18]. Our discussion focuses on a class of problems called decision tasks, described in Section 2. In Section 3, we show how decision tasks can be modeled using simplicial complexes, a standard combinatorial structure from elementary topology. In Section 4, we review the notion of a chain complex, which provides an algebraic vocabulary for describing the topological properties of simplicial complexes. In Section 5, we show how these combinatorial and algebraic notions can be applied to prove a variety of lower bounds for a well-known problem in distributed computing, the k-set agreement task [7]. 2 Model A set of n + 1 sequential threads of control, called processes, communicate by applying operations to objects in shared memory. Examples of shared objects include message queues, read/write variables, test-and-set variables, or objects of arbitrary abstract type. Processes are asynchronous: they run at arbitrarily varying speeds. Up to t processes may fail. Because the processes are asynchronous, a protocol cannot distinguish a failed process from a slow process. To distill the notion of a distributed computation to its simplest interesting form, we focus on a simple but important class of problems called decision tasks. We are given a set of n+ 1 sequential processes P0; : : : ; Pn. Each process starts out with a private input value, typically subject to task-speci c constraints. The processes communicate for a while, then each process chooses a private output value, also subject to task-speci c constraints, and then halts. Decision tasks are intended to model \reactive"""" systems such as databases, le systems, or ight control systems. An input value represents information entering the system from the outside world, such as a character typed at a keyboard, a message from another computer, or a signal from a sensor. An output value models an e ect on the outside world, such as an irrevocable decision to commit a transaction, to dispense cash, or to launch a missile. Perhaps the simplest example of a decision task is consensus [9]. Each process starts with an input value and chooses an output value. All output values must agree, and each output value must have been some process's input value. If the input values are boolean, the task is called binary consensus. The consensus task was originally studied as an idealization of the transaction commitment problem, in which a number of database sites must agree on whether to commit or abort a distributed transaction. A natural generalization of consensus is k-set agreement [7]. Like consensus, each process's output value must be some process's input value. Unlike consensus, which requires that all processes agree, k-set agreement requires that no more than k distinct output values be chosen. Consensus is 1-set agreement. A program that solves a decision task is called a protocol. A protocol is tresilient if any non-faulty process will nish the protocol in a xed number of steps, regardless of failures or delays by up to t other processes. A protocol is wait-free if it tolerates failures or delay by all but one of the processes. 3 Combinatorial Structures Formally, an initial or nal state of a process is a vertex, v = hPi; vii, a pair consisting of a process id and a value (either input or output). A set of d+1 mutually compatible initial or nal process states is modeled as a d-dimensional simplex , (or d-simplex). A simplex is properly colored if each vertex is labeled with a distinct process id. The complete set of possible initial (or nal) process states is represented by a set of properly colored simplexes, closed under containment, called a simplicial complex (or complex). The dimension of C is the dimension of a simplex of largest dimension in C. Where convenient, we use superscripts to indicate dimensions of simplexes and complexes. The k-th skeleton of a complex, skelk(Cn), is the subcomplex consisting of all simplexes of dimension k or less. The set of process ids associated with simplex Sn is denoted by ids(Sn), and the set of values by vals(Sn). Sm is a (proper) face of Sn if the vertexes of Sm are a (proper) subset of the vertices of Sn. If K and L are complexes, a simplicial map : K ! L carries vertexes of K to vertexes of L so that simplexes are preserved. It is often convenient to visualize vertexes, simplexes, and complexes as point sets in Euclidian space. A vertex is simply a point, and an n-simplex is the convex hull of n+1 a nely-independent5 vertexes. A complex is represented by a set of (geometric) simplexes arranged so that that each pair of simplexes intersects either in a common face, or not at all. The point set occupied by such a complex is called its polyhedron. Although we use this geometric interpretation for illustrations and informal discussions, we do not use it in our formal treatment. A decision task for n+ 1 processes is given by an input complex I, an output complex O, and a map carrying each input n-simplex of I to a set of nsimplexes of O. This map associates with each initial state of the system (an input n-simplex) the set of legal nal states (output n-simplexes). It is convenient to extend to simplexes of lower dimension: when n t m < n, (Sm) is the set of legal nal states in executions where only the indicated m + 1 processes take steps. For example, the input complex for binary consensus is constructed by assigning independent binary values to n+1 processes. We call this complex the binary n-sphere, because its polyhedron is homeomorphic to an n-sphere (exercise left to the reader). The output complex consists of two disjoint n-simplexes, corresponding to decision values 0 and 1. Figure 1 illustrates the input and output complexes for two-process binary consensus. As an example of an interesting output complex, consider the renaming task [1], in which each process is given a unique input name taken from a large name space, and must choose a unique output name taken from a much smaller name space. Figure 2 shows the output complex for the three-process renaming task 5 v0; : : : ;vn are a nely independent if v1 v0; : : : ;vn v0 are linearly independent. Input Complex Output Complex ∆ P 0 Q 0 P 1 P 0 Q 0 P 1 Q 1 Q 1 Fig. 1. Input and Output Complexes for 2-Process Consensus P 0 Q 3 R 0 P 3 Q 0 R 3 P 2 R 1 Q 2 Q 1 P 1 R 2 R 0 P 3 Q 0 R 3 P 0 Q 2 P 0 P 1 Q 3 R 2 P 0 A A B B Fig. 2. Output Complex for 3-Process Renaming with 4 Names using four output names. Notice that the two edges marked A are identical, as are the two edges marked B. By identifying these edges, we can see that this complex has a polyhedron homeomorphic to a torus. At the end of a protocol, the process's local state is its view of the computation: its input value followed by the sequence of operations (including arguments and results) applied to shared objects. It is convenient to view a process as executing a protocol for a xed number of steps, and then choosing its output value by applying a task-speci c decision map to its local state. We can treat any protocol as an \uninterpreted"""" protocol simply by treating each process's local state as its decision value (i.e., omitting the task-speci c decision map ). This uninterpreted protocol itself de nes a complex P, called its protocol complex. Each vertex v in this complex is labeled with a process id and a local state such that there exist some execution of the protocol in which process id(v) nishes the protocol with local state val(v). A simplex Tm = (t0; : : : ; tm) is in this complex if there is an execution of the protocol in which each process id (ti) nishes the protocol with local state val (ti) (i.e., the vertexes of any simplex are compatible local states). For an input simplex Sm, let P(Sm) be the subcomplex of P generated by executions in which only the processes in ids(Sm) take steps, starting with input values from vals(Sm). P 0"""	abstract type;asynchronous i/o;computation;consensus (computer science);control system;convex hull;database;distributed computing;distributed transaction;existential quantification;input/output;linear algebra;local variable;message queue;non-blocking algorithm;primer;parameter (computer programming);polyhedron;process identifier;process state;shared memory;simplicial complex;test-and-set;thread (computing);vertex (geometry);vertex (graph theory);vii;vocabulary;while	Maurice Herlihy;Sergio Rajsbaum	1995		10.1007/BFb0015245	distributed algorithm;discrete mathematics;computer science;theoretical computer science;distributed computing;upper and lower bounds;distributed concurrency control	Theory	17.79278993983572	35.00257420520765	40656
6fdda58807d19ab8f7350d39dba75c91ec288d09	complexity reduction in an nrerl microprocessor	microprocessor;energy recovery;651 khz nrerl microprocessor adiabatic microprocessor reversible logic buffers recovery circuits energy consumption clocked power generator cmos circuit complexity 0 25 micron 2 4 v;logic circuits;circuit complexity;chip;integrated circuit design;reversible logic;complexity reduction;cmos digital integrated circuits;energy consumption;buffer skipping;reversibility breaking;power generation;nmos reversible energy recovery logic nrerl;microprocessors clocks energy consumption cmos technology cmos logic circuits logic circuits power generation integrated circuit technology energy measurement semiconductor device measurement;clocked power generator cpg;cmos digital integrated circuits microprocessor chips integrated circuit design logic circuits;microprocessor chips	We describe an adiabatic microprocessor implemented with a reversible logic, nRERL [1]. We employed an 8-phase clocked power instead of 6-phase one to reduce the number of buffers required for the phase aligning in the adiabatic microprocessor. Furthermore, by breaking the logic reversibility with self-energy recovery circuits, we also reduced its complexity as well as its energy consumption.We integrated an 8-bit nRERL microprocessor with an 8-phase clocked power generator into a chip with 0.25mm CMOS technology. Its minimum energy consumption of 4.67μA/MHz was measured at Vdd=2.4V and f=651kHz, which was about 40% compared to the previous 6-phase version. Its circuit complexity was also reduced down to 65% that of its 6-phase version.	8-bit;cmos;circuit complexity;clock rate;microprocessor;reduction (complexity);reversible computing	Seokkee Kim;Soo-Ik Chae	2005	ISLPED '05. Proceedings of the 2005 International Symposium on Low Power Electronics and Design, 2005.	10.1145/1077603.1077649	chip;energy recovery;circuit complexity;electricity generation;embedded system;electronic engineering;real-time computing;logic gate;computer science;engineering;reduction;integrated circuit design	Arch	17.457810311888245	57.76120742655067	40678
ec4b55fc6b5cd9add7e6cb4e4e380e935a98ee36	the computer system graphograph	graph theory	The aim of this talk is to draw the attention of the scientific community to an ambitious project to develop multipurpose software for graph theory. The basic idea of this computer system is to verify graph-theoretic properties for sets of graphs. A user specifies a property using a simple Formalised Graph Language. No programming skills are required. The next step is to specify a graphbase, a set of graphs that can be chosen either randomly or from a huge built-in database of graphs, or specified by the user in Graph Editor. A computer then verifies the property for each graph from the graphbase and constructs a new set consisting of graphs satisfying the property. The resulting set of graphs is visualised and can be analysed by the user. One of the possible applications of this computer system is to verify whether a certain conjecture is true for all graphs of small order. The software would also be helpful in proving theoretical results. Some examples will be demonstrated during the talk. In order to provide the ability to specify a wide range of graph properties, a bank of basic graph algorithms will be developed. To achieve this, the algorithmic part of the project is divided into modules designed for different branches of graph theory. Any graph theorist interested in taking part in developing one of the modules is invited to cooperate (please contact: vadim.zverovich@uwe.ac.uk). All such developers will become co-authors of the software.	algorithm;canonical account;computer;graph property;graph theory;randomness;volume rendering	Vadim E. Zverovich	2006		10.1016/j.endm.2006.08.079	graph power;edge-transitive graph;factor-critical graph;combinatorics;null graph;graph property;graph labeling;clique-width;graph theory;simplex graph;cubic graph;mathematics;voltage graph;graph;windmill graph;butterfly graph;quartic graph;line graph;strength of a graph;coxeter graph;friendship graph	Theory	20.22434410819236	32.64053923958494	40688
c1d8709fca13bb9d0add4ae4a1a0b127e151da05	a systolic array structure for matrix multiplication in the residue number system	systolic array;residue number system;matrix multiplication	"""This paper describes a new scheme for matrix multiplication in the residue number system (RNS) by a VLSI systolic structure. The basic goal is to accelerate matrix multiplication by exploiting the """"double parallelism"""" involved in the systolic structure and in the RNS. The scheme consists of a N × N rhombus-like array of residue-based processors for RNS N × N matrix multiplication. Each processor operates in parallel, pipeline or hybrid modes using an arrangement of q moduli adders and multipliers, q>1. The operations of residue addition and multiplication are performed by associative table lookup processing, which has been shown to be particularly efficient for fast residue arithmetic implementations in VLSI technology."""		Christos A. Papachristou;Suntae Hwang	1987		10.1007/3-540-18991-2_40	arithmetic;residue number system;systolic array;matrix multiplication;computer science	HPC	11.747813616038465	44.09728709937786	40694
95dcf9f14807c44deefc9e73a3e7eca4a9daa796	hardware implementation for a new design of the vbsme used in h.264/avc	video coding field programmable gate arrays logic design motion estimation;motion estimation video coding vectors computer architecture clocks adders standards;variable block size motion estimation vbsme video compression h 264 avc motion estimation;vhdl design hardware implementation vbsme h 264 avc motion estimation video coding standard variable block size high compression rates	Motion estimation (ME) in video coding standard H.264/AVC adopts variable block size (VBSME) which provides high compression rates but requires much higher computation compared to the previous coding standards. To overcome this complexity, this paper describes a VHDL design and an implementation of VBSME. The design is based on partitioning each 16×16 macroblock into sixteen 4×4 non overlapping subblocks. The motion estimation of these subblocks is performed in parallel in order to use them to form the 41 subblocks of different sizes specified by the standard. As a result, this new design has in consideration low latency and high throughput with a maximum frequency which reaches over than 277 MHz on a Xilinx-Vittex5-LX110T FPGA.	computation;data compression;h.264/mpeg-4 avc;macroblock;motion estimation;throughput;vhdl;video coding format	Amira Yahi;Kamel Messaoudi;Salah Toumi;El-Bey Bourennane	2014	2014 International Conference on Control, Decision and Information Technologies (CoDIT)	10.1109/CoDIT.2014.6996974	scalable video coding;electronic engineering;real-time computing;quarter-pixel motion;computer science;theoretical computer science;block-matching algorithm;rate–distortion optimization;context-adaptive binary arithmetic coding;motion compensation	EDA	12.40119539019394	40.59691012074704	40705
b98619935a76efb56f3890a51c21478760f545fd	an efficient two-dimensional blocking strategy for sparse matrix-vector multiplication on gpus	spmv;brc;gpu;cuda	Sparse matrix-vector multiplication (SpMV) is one of the key operations in linear algebra. Overcoming thread divergence, load imbalance and non-coalesced and indirect memory access due to sparsity and irregularity are challenges to optimizing SpMV on GPUs.  In this paper we present a new blocked row-column (BRC) storage format with a novel two-dimensional blocking mechanism that effectively addresses the challenges: it reduces thread divergence by reordering and grouping rows of the input matrix with nearly equal number of non-zero elements onto the same execution units (i.e., warps). BRC improves load balance by partitioning rows into blocks with a constant number of non-zeros such that different warps perform the same amount of work. We also present an efficient auto-tuning technique to optimize BRC performance by judicious selection of block size based on sparsity characteristics of the matrix. A CUDA implementation of BRC outperforms NVIDIA CUSP and cuSPARSE libraries and other state-of-the-art SpMV formats on a range of unstructured sparse matrices from multiple application domains. The BRC format has been integrated with PETSc, enabling its use in PETSc's solvers.	bioinformatics resource centers;block size (cryptography);blocking (computing);cuda;execution unit;graphics processing unit;library (computing);linear algebra;load balancing (computing);matrix multiplication;petsc;self-tuning;sparse matrix;the matrix;warp (information security)	Arash Ashari;Naser Sedaghati;John Eisenlohr;P. Sadayappan	2014		10.1145/2597652.2597678	parallel computing;computer hardware;computer science;theoretical computer science;operating system	HPC	-1.859331456454116	39.52110804277668	40751
585d5f1403b77eb5673131e2d680cb6592692832	bist architecture to detect defects in tsvs during pre-bond testing	silicon;circuit faults;open fault;pre bond test;tsvs;defect 3 d ics tsvs pre bond test open fault bridging fault;resistance;three dimensional integrated circuits built in self test integrated circuit testing;built in self test;bridging fault;conference report;defect;integrated circuit modeling;integrated circuit testing;through silicon vias circuit faults built in self test resistance integrated circuit modeling silicon;prebond bist architecture defect detection prebond testing through silicon vias three dimensional integrated circuits 3d ic defective tsv fabrication process;3 d ics;three dimensional integrated circuits;through silicon vias;built in self testintegrated circuit testingthree dimensional integrated circuits	Through Silicon Vias (TSVs) are critical elements in three dimensional integrated circuits (3-D ICs). The detection of defective TSVs in the earliest process step is of major concern. Hence, testing TSVs is usually done at different stages of the fabrication process. In this context, this work proposes a simple pre-bond GIST architecture to improve the detection of hard and weak defects.	built-in self-test;gist;integrated circuit;semiconductor device fabrication;wafer (electronics)	Daniel Arumí;Rosa Rodríguez-Montañés;Joan Figueras	2013	2013 18th IEEE European Test Symposium (ETS)	10.1109/ETS.2013.6569389	structural engineering;embedded system;electronic engineering;engineering;silicon;resistance	Embedded	22.61037153937607	53.69087569731319	40773
7bfdb2bf2dfa0297a6793feb1d70f58a1486ccbc	nemo: a massively parallel discrete-event simulation model for neuromorphic architectures		Neuromorphic computing is a broad category of non–von Neumann architectures that mimic biological nervous systems using hardware. Current research shows that this class of computing can execute data classification algorithms using only a tiny fraction of the power conventional CPUs require. This raises the larger research question: How might neuromorphic computing be used to improve application performance, power consumption, and overall system reliability of future supercomputers? To address this question, an open-source neuromorphic processor architecture simulator called NeMo is being developed. This effort will enable the design space exploration of potential heterogeneous compute systems that combine traditional CPUs, GPUs, and neuromorphic hardware. This article examines the design, implementation, and performance of NeMo. Demonstration of NeMo’s efficient execution using 2,048 nodes of an IBM Blue Gene/Q system, modeling 8,388,608 neuromorphic processing cores is reported. The peak performance of NeMo is just over ten billion events-per-second when operating at this scale.	algorithm;blue gene;central processing unit;design space exploration;graphics processing unit;neuromorphic engineering;open-source software;simulation;supercomputer	Mark Plagge;Christopher D. Carothers;Elsa Gonsiorowski;Neil McGlohon	2018	ACM Trans. Model. Comput. Simul.	10.1145/3186317	massively parallel;parallel computing;mathematical optimization;discrete event simulation;computer science;design space exploration;microarchitecture;neuromorphic engineering	Arch	-3.344678927870444	46.29462737021656	40809
bcaaadad4842d1225072e12a68466e0bcad8a472	mapping computation kernels to clustered programmable-reconfigurable processors	processor architecture;reconfigurable computing;reconfigurable architectures;data flow graphs;data flow graphs reconfigurable architectures finite state machines program compilers;datapath synthesis tool mapping computation kernel clustered programmable reconfigurable processor reconfigurable computing systems processor architecture compilation time amalgam reconfigurable clusters gated singular assignment program parallel intermediate program representation fsm finite state machines;finite state machines;program compilers;reconfigurable processor;kernel registers reconfigurable logic computer architecture program processors clustering algorithms programming profession size control network synthesis system on a chip;program dependence graph	Reconfigurable computing systems have shown the potential to surpass conventional processor architectures in performance for a growing range of applications. That performance, however, must be attained without significantly changing the design effort on the programmer’s part, and without drastically increasing compilation time. In this paper, we present our compiler framework for mapping computation kernels to the reconfigurable clusters of Amalgam, a clustered programmable-reconfigurable processor. We first promote the use of the gated singular-assignment program dependence graph, a parallel intermediate program representation, to represent computation kernels. We then present an algorithm for mapping a computation kernel into the control FSM and datapath for a reconfigurable cluster. Finally, we describe our fast datapath synthesis tool-flow which preserves regularity and reduces the problem size by not flattening the datapath to gates.	algorithm;analysis of algorithms;central processing unit;compiler;computation;datapath;kernel (operating system);program dependence graph;programmer;reconfigurable computing	Jeffrey J. Cook;Lee Baugh;Derek B. Gottlieb;Nicholas P. Carter	2003		10.1109/FPT.2003.1275796	embedded system;computer architecture;parallel computing;real-time computing;microarchitecture;reconfigurable computing;computer science;operating system;finite-state machine	HPC	-0.09032300335419811	50.26576866062438	40831
2439400203842ab22d43e4b401d8a4c29ba1dc03	a programmable and scalable technique to design spintronic logic circuits based on magnetic tunnel junctions	magnetic tunnel junction;look up table;magnetic tunnel junctions;low power;energy consumption;programmable spintronics logic	Exciting developments are taking place in the field of spintronics, particularly with the advances in the fabrication and characterization of devices such as Magnetic Tunnel Junctions (MTJ). The distinction of spintronic devices from conventional electronic devices makes it challenging to design efficient, scalable and low power logic circuits with MTJs. We propose a programmable and scalable technique to design MTJ-based logic circuits that are capable of implementing any 2-input logic truth table. We present the energy-delay trade-offs of this design with respect to circuit parameters. We also demonstrate that this circuit can be scaled to a 6-input logic function without incurring an increase in the energy consumption.	boolean algebra;logic gate;scalability;spintronics	Shruti R. Patil;David. J. Lilja	2011		10.1145/1973009.1973012	electronic engineering;tunnel magnetoresistance;lookup table;logic family;computer science;engineering;electrical engineering;pass transistor logic;algorithm	EDA	15.577021288064321	58.3827653216042	40918
05d1750db0896fd51e8143b4f16301adce85aefe	a new approach to test generation and test compaction for scan circuits	automatic test pattern generation;flip-flops;integrated circuit testing;integrated logic circuits;sequential circuits;limited scan operations;scan circuits;test application times;test compaction;test generation;test sequences	We propose a new approach to test generation and test compaction for scan circuits that eliminates the distinction between scan operations and application of primary input vectors. Under this approach, the scan-in, scan-select and scan-out lines are treated as conventional primary inputs or primary outputs of the circuit. As a result, limited scan operations, where scan chains are shifted a number of times smaller than their lengths, are incorporated naturally into the test sequences generated by this approach. This leads to very aggressive compaction, resulting in test sequences with the lowest known test application times for benchmark circuits.	benchmark (computing);curve-fitting compaction;data compaction;markov chain	Irith Pomeranz;Sudhakar M. Reddy	2003			embedded system;automatic test equipment;electronic engineering;scan chain;real-time computing;boundary scan;engineering;automatic test pattern generation;test compression;sequential logic	EDA	21.191727935523485	51.42682426339627	40977
116c663abd011e4da5d0ed6a3315a7a9974c25ae	improved wire length-driven placement technique for minimizing wire length, area and timing		The placement of cells in Integrated Circuit Design Automation has a major influence on overall design cycle. The existing popular quadratic placement techniques suffer from overlaps, large placement effort and time. In order to lower the placement overhead and to avoid the overlaps with reduced wire length, we propose a grouping and merging based placement methodology that is simpler than existing placers and easier to integrate into timing-closure flows. As a proof of concept, the proposed methodology is extensively tested on standard benchmark circuits. The proposed methodology resulted in 5× placement time reduction, 13% reduction in wire-length and 11% reduction in area with zero overlap.		Srinivas Sabbavarapu;Basireddy Karunakar Reddy;Amit Acharyya;S. Saqib Khursheed	2017	J. Low Power Electronics	10.1166/jolpe.2017.1506	merge (version control);electronic engineering;automation;real-time computing;proof of concept;electronic circuit;engineering;placement;integrated circuit design	EDA	15.911433761513473	53.45181533447034	40982
cc1501644e9b3e9296193e728bc8d12ae064b4bc	e-pipeline: elastic hardware/software pipelines on a many-core fabric	public key encryption;pipelines;cloning;throughput;hardware;benchmark testing	On-chip many-core systems are expected to be in common use in the future. A set of homogeneous processors in a many-core system can be used to implement multiple pipelines which execute simultaneously. Pipelines of processors use varying numbers of cores when their workloads vary at run time. In this paper, we show how such a system executing multiple pipelines with varying workloads can be implemented. We further show how the system can switch cores within a pipeline (intra-elasticity) and between pipelines (inter-elasticity). The method is named E-pipeline, and is implemented and evaluated in a commercial tool suite. Compared to reference design methods with clock gating, E-pipeline achieves the same power savings, maintains the throughput to meet throughput constraints and reduces core usage by an average of 37.7%. The adaptation overhead for switching cores is approximately 2μs.	benchmark (computing);central processing unit;clock gating;elasticity (cloud computing);elasticity (data store);experiment;manycore processor;multi-core processor;overhead (computing);performance per watt;pipeline (computing);pipeline (software);reference design;run time (program lifecycle phase);throughput	Xi Zhang;Haris Javaid;Muhammad Shafique;Jorgen Peddersen;Jörg Henkel;Sri Parameswaran	2015	2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;electronic engineering;parallel computing;real-time computing;computer science;operating system;public-key cryptography	EDA	-2.227072491593258	50.251071426744595	40983
2accfde27b4105083c5a43b6843b0506cce262a1	accelerating the iterative linear solver for reservoir simulation on multicore architectures	iterative solver;8 thread multicore architecture iterative linear solver multicore architectures petroleum reservoir simulation reservoir production management reservoir production planning jacobian nonlinear partial differential equations energy conduction energy conservation incomplete lu factorization ilu factorization data dependency sparse iterative solver parallelized ilu triangular solve block wise data structure industrial reservoir simulation matrices data flow graph task scheduling topological order;incomplete lu factorization;scheduling data flow graphs data structures hydrocarbon reservoirs iterative methods matrix algebra nonlinear differential equations partial differential equations petroleum industry production engineering computing production planning;reservoirs sparse matrices mathematical model runtime multicore processing jacobian matrices computational modeling;multicore architecture iterative solver sparse matrix incomplete lu factorization;multicore architecture;sparse matrix	Modern petroleum reservoir simulation serves as a primary tool for quantitatively managing reservoir production and planning new fields. It involves repeatedly solving the Jacobian of a set of strong nonlinear partial differential equations governing the mass and energy conduction and conservation. Most of the existing reservoir simulators adopt iterative solver with multiple stages of preconditioners, in which the incomplete LU (ILU) factorization is an outstanding universal smoother. However, it turns out that when the degree of freedom of each grid grows, ILU usually becomes the bottleneck of the solver. Moreover, ILU is difficult to parallelize due to its inherent data dependency. In this paper, we developed a sparse iterative solver with parallelized ILU and triangular solve using block-wise data structure. Compared with the state of art iterative solver on 14 industrial reservoir simulation matrices, the proposed ILU is 5.2x faster (on average) than the state of art iterative solver because of the block-wise data structure, which leads to 2.2x speedup on the total solver runtime. In addition, parallel ILU and triangular solve are developed to further accelerate the solver. To tackle the strong data dependency in ILU and triangular solve, we first partition the algorithm into separated tasks and construct a data flow graph to represent the data dependency. Then, tasks are scheduled in parallel according to the topological order of the data flow graph. On an 8-thread multicore architecture, we achieved another 3.6x speedup on ILU factorization, and 3.3x on triangular solve with good scalability.	algorithm;data dependency;data structure;dataflow;incomplete lu factorization;iterative method;jacobian matrix and determinant;multi-core processor;nonlinear system;numerical linear algebra;parallel computing;preconditioner;qr decomposition;scalability;simulation;solver;sparse matrix;speedup	Wei Wu;Xiang Li;Lei He;Dongxiao Zhang	2014	2014 20th IEEE International Conference on Parallel and Distributed Systems (ICPADS)	10.1109/PADSW.2014.7097817	mathematical optimization;parallel computing;sparse matrix;incomplete lu factorization;computer science;theoretical computer science	HPC	-3.4549572464740472	38.91085830995217	41002
1350fa8c28fa7a848a723d46dfd1720ed057390a	built-in tpg with designed phaseshifts	cellular automata automatic test pattern generation built in self test phase shifters;phase shifters hardware test pattern generators logic testing network synthesis built in self test linear feedback shift registers automata very large scale integration;phase shifters;automatic test pattern generation;pattern generation;built in self test;phase shifter;built in test;test pattern generation tpg;test pattern generator;compact structure tpg phaseshifts built in test pattern generation prescribed exact set channel separations bit sequences hardware overhead linear dependencies pseudorandom tpg pseudoexhaustive tpg two dimensional tpg architecture;built inself test bist;cellular automata	In this paper, we present built–in test pattern generation (TPG) mechanisms that can enforce a prescribed exact set of phaseshifts, or channel separations, on the bit sequences produced by their successive stages, while still requiring low hardware overhead. Such mechanisms are used in controlling the amount of correlations and/or linear dependencies that are problematic for pseudorandom and pseudoexhaustive TPG in a two–dimensional TPG architecture. The reduction in hardware overhead is achieved by a new technique that merges the logic of the original TPG mechanism with that of the required phase shifter network in order to yield an improved compact structure.	cellular automaton;electronic circuit;finite-state machine;overhead (computing);pseudorandomness;test card	Dimitrios Kagaris	2003		10.1109/VTEST.2003.1197676	cellular automaton;electronic engineering;parallel computing;real-time computing;computer science;engineering;phase shift module;automatic test pattern generation;algorithm;quantum mechanics	Arch	20.335016460221023	48.654403403356454	41004
8dc84df03cc56aa310fa90ff1e14fc7a1d044f47	gmm foreground segmentation processor based on address free pixel streams	custom dsp processor;multi resolution transform;streaming;video streaming;memory management;image segmentation;image resolution;video signal processing;gaussian processes;video streaming application specific integrated circuits buffer storage digital signal processing chips field programmable gate arrays gaussian processes high definition television image resolution image segmentation power aware computing transforms video signal processing;transforms registers field programmable gate arrays memory management process control hdtv real time systems;buffer storage;power aware computing;gaussian mixture model;registers;application specific integrated circuits;transforms;hdtv;fpga prototype;process control;gaussian mixture model gmm foreground segmentation processor address free pixel streams hdtv signals hardware streaming memory capacity reduction multiresolution spatial transform temporal segmentation i o buffers fpga prototype chip tablets smart phone asic implementation power reduction;streaming custom dsp processor multi resolution transform gaussian mixture model fpga prototype;digital signal processing chips;field programmable gate arrays;high definition television;real time systems	A compact implementation of a foreground segmentation processor in a multi-resolution transform domain has been proposed for HDTV signals. The proposed architecture is designed to simplify system controls by the hardware streaming and to reduce required memory capacities. It enables flowing pixels through all functional units in order, including multi-resolution spatial transform and temporal segmentation. The resultant architecture does not use memories except I/O buffers. Therefore, memory modules as well as complex address manipulation over the multiple global transforms and spatial/temporal interface is not required. The FPGA prototype chip dissipates 150 mW of power. This approach can be used for tablets and smart-phone by an ASIC implementation which will reduce the operation power to about 1/6.	application-specific integrated circuit;dimm;field-programmable gate array;google map maker;input/output;mobile app;pixel;prototype;resultant;smartphone;spectral leakage	Ryo Yagi;Tomohito Kajimoto;Takao Nishitani	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288213	embedded system;real-time computing;image resolution;computer science;process control;mixture model;gaussian process;image segmentation;application-specific integrated circuit;processor register;fpga prototype;field-programmable gate array;memory management	Robotics	10.578110784051752	39.854485501064275	41019
5e0b3a651bf6937b52b55b322d3b68089f8e87aa	statistical analysis of sram cell stability	process variation;reliability;theoretical framework;sram chips circuit stability integrated circuit modelling integrated circuit noise integrated circuit reliability integrated circuit yield monte carlo methods;integrated circuit yield;dc noise;marginal models;sram cell stability;memory cell;performance;circuit stability;yield estimation;stability;statistical analysis;integrated circuit modelling;monte carlo simulations statistical analysis sram cell stability dc noise memory cell cell failure;modeling performance design reliability sram reliability stability noise margin;integrated circuit modeling;performance analysis;design;sram;statistical analysis random access memory stability analysis inverters threshold voltage fluctuations error correction codes design optimization semiconductor process modeling random variables;integrated circuit reliability;monte carlo simulation;integrated circuit noise;modeling;monte carlo simulations;noise margin;cell failure;monte carlo methods;failure probability;reading and writing;sram chips	The impact of process variation on SRAM yield has become a serious concern in scaled technologies. In this paper, we propose a methodology to analyze the stability of an SRAM cell in the presence of random fluctuations in the device parameters. We provide a theoretical framework for characterizing the DC noise margin of a memory cell and develop models for estimating the cell failure probabilities during read and write operations. The proposed models are verified against extensive Monte-Carlo simulations and are shown to match well over the entire range of the distributions well beyond the 3-sigma extreme.	cell (microprocessor);memory cell (binary);monte carlo method;noise margin;simulation;static random-access memory	Kanak Agarwal;Sani R. Nassif	2006	2006 43rd ACM/IEEE Design Automation Conference	10.1145/1146909.1146928	electronic engineering;real-time computing;computer science;electrical engineering;statistics;monte carlo method	EDA	21.077010442023006	58.73032722622456	41052
a12bee93834a00ceca8fa49f36005a98c8e6ea01	scheduling and optimizing stream programs on multicore machines by exploiting high-level abstractions	energy;computer engineering;streaming;computer science scheduling and optimizing stream programs on multicore machines by exploiting high level abstractions university of california;model based;dai nguyen;programming model;computer science;language;berkeley edward a lee bui;electrical engineering;dsp	Scheduling and Optimizing Stream Programs on Multicore Machines by Exploiting High-Level Abstractions by Dai Nguyen Bui Doctor of Philosophy in Engineering Electrical Engineering & Computer Sciences University of California, Berkeley Professor Edward A. Lee, Chair Real-time streaming of HD movies and TV via YouTube, Netflix, Apple TV and Xbox Live is gaining popularity. Stream programs often consume considerable amounts of energy due to their compute-intensive nature. Making stream programs energy-efficient is important, especially for energy-constrained computing devices such as mobile phones and tablets. The first part of this thesis focuses on exploiting the popular Synchronous Dataflow (SDF) high-level abstraction of stream programs to design adaptive stream programs for energy reduction on multicore machines. Observing that IO rates of stream programs can vary at runtime, we seek to make stream programs adaptive by transforming their internal structures to adapt required occupied computing resources, e.g., cores and memory, to workload changes at runtime. Our experiments show that adapting stream programs to IO rate changes can lead to significant energy reduction. In addition, we also show that the modularity and static attributes of stream programs’ abstraction not only help map stream programs on multicore machines more easily but also enable energy-efficient routing schemes of high-bandwidth stream traffic on the interconnection fabric, such as networks on-chip. While SDF abstractions can help optimize stream programs on multicore machines, SDF is more suitable for describing stream data-intensive computations such as FFT, DCT, and FIR and so on. Modern stream operations such as MPEG2 or MP3 encoders/decoders are often more sophisticated and composed of multiple such computations. Enabling operation synchronization between different such computations with different semantics leads to the need for control messaging. We extend previous work on control messaging and give a formal definition for control message latency via the semantics of information wavefronts. This control-operation-integrated SDF (COSDF) is able to model sophisticated stream programs more precisely. However, the conventional scheduling method developed for SDF is not sufficient to schedule COSDF applications. To schedule COSDF applications, we develop a scheduling method using dependency graphs and applying a periodic graph theory, based on reduced dependency graphs (RDG). This RDG scheduling method also helps extract	apple tv;browser user interface;codec;computation;computer science;data-intensive computing;dataflow architecture;discrete cosine transform;electrical engineering;encoder;experiment;fast fourier transform;finite impulse response;graph theory;high- and low-level;interconnection;mp3;mpeg-2;mobile phone;multi-core processor;optimizing compiler;periodic graph (graph theory);real-time transcription;routing;run time (program lifecycle phase);scheduling (computing);tablet computer	Dai Nguyen Bui	2013			parallel computing;real-time computing;computer science;distributed computing	Arch	0.18428486694346125	54.81182874985962	41081
51ed7e7494c0cfb34a879edb57d4c2a8db92434b	task scheduling for exploiting parallelism and hierarchy in vlsi cad algorithms	parallelisme;concepcion asistida;vlsi circuit cad circuit layout cad integrated circuit technology parallel algorithms scheduling;computer aided design;hierarchy;integrated circuit;gestion labor;routing;shared memory multiprocessor vlsi cad algorithms computer aided design parallel processing task scheduling parallel hierarchical circuit extractor parallel hierarchical global router;circuit design;circuit vlsi;circuito integrado;parallelism;vlsi circuit;gestion tâche;paralelismo;particion;integrated circuit technology;scheduling;scheduling theory;jerarquia;partition;vlsi;conception assistee;circuit layout cad;circuit cad;encaminamiento;circuito vlsi;task scheduling;very large scale integration scheduling algorithm design automation processor scheduling parallel processing routing concurrent computing circuit synthesis algorithm design and analysis;hierarchie;parallel processing;circuit integre;acheminement;shared memory multiprocessor;parallel algorithms	Two approaches to handling the computational requirements of computer-aided design problems are considered. One approach is to take advantage of the hierarchical nature of circuit design and develop hierarchical CAD algorithms. Another involves the use of parallel processing and development of parallel CAD algorithms. How these two approaches can be combined to speed up various CAD applications is discussed. Toward this goal, two general problems in scheduling are solved: parallelizable independent task scheduling (PITS) and parallelizable dependent task scheduling (PDTS). The PITS scheduling theory is applied to a parallel hierarchical circuit extractor, and the PDTS scheduling theory is applied to a parallel hierarchical global router. Both implementations show speedups of about six on eight processors of a shared-memory multiprocessor. >	algorithm;computer-aided design;parallel computing;scheduling (computing)	Krishna P. Belkhale;Randall J. Brouwer;Prithviraj Banerjee	1993	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.277604	partition;fair-share scheduling;embedded system;parallel processing;routing;computer architecture;parallel computing;real-time computing;dynamic priority scheduling;computer science;electrical engineering;integrated circuit;computer aided design;circuit design;two-level scheduling;parallel algorithm;very-large-scale integration;round-robin scheduling;scheduling;hierarchy	EDA	14.062013112292302	50.37769433070542	41114
04bc510d1b4205dff43e08cf09534c1f1d2a28f1	the total delay fault model and statistical delay fault coverage	statistical delay fault coverage;integrated circuit;circuit retard;sistema informatico;metric;circuito integrado;computer system;modele recouvrement;circuito logico;indexing terms;delay faults delay fault model delay fault coverage delay testing statistical delay fault coverage defect level model;test;ensayo;essai;delay faults;operating system;propagacion;circuit logique;delay testing;propagation delay;delay fault model;logic testing;defaillance;metrico;fault coverage;systeme informatique;failures;circuit faults system testing delay effects propagation delay circuit testing timing clocks logic testing fault detection delay systems;circuito retardo;fault model;figure of merit;logic circuit;delay circuit;defect level model;fallo;metrique;propagation;circuit integre;delay fault coverage;timing	Delay testing at the operational system clock rate can detect system timing failures caused by delay faults. However, delay fault coverage in terms of the percentage of the number of tested faults may not be an effective measure of delay testing because, unlike a stuck-at-fault, the impact of a delay fault on proper system operation is dependent on its delay defect size rather than on its existence. The effectiveness of a delay test is dependent on the propagation delay of the path to be tested, the delay defect size, and the system clock interval. This paper presents a quantitative delay fault coverage model to provide a figure of merit for delay testing. System sensitivity of a path to a delay fault along that path and the effectiveness of a delay test are described in terms of the propagation delay of the path under test and the delay defect size. A new statistical delay fault coverage (SDFC) model is established. A new defect level model is also proposed as a function of the yield of a manufacturing process and the new statistical delay fault coverage. Finally, a new delay testing strategy driven by the defect level for delay faults is proposed.	clock rate;fault coverage;fault model;operational system;propagation delay;software bug;software propagation	Eun Sei Park;M. Ray Mercer;Thomas W. Williams	1992	IEEE Trans. Computers	10.1109/12.144621	embedded system;propagation delay;figure of merit;real-time computing;index term;fault coverage;delay calculation;logic gate;metric;computer science;stuck-at fault;elmore delay;operating system;integrated circuit;fault model;software testing;contamination delay;computer network	EDA	22.857163076201243	50.952310049886336	41116
6e4411dc824963e38d45aaad0f47f0a633c8405e	scalable vectorless power grid current integrity verification	multigrid power grid electromigration;integrated circuit reliability electromigration integrated circuit design integrated circuit interconnections;integrated circuit design;power grids wires vectors current density power dissipation sensitivity analysis;integrated circuit interconnections;multigrid;power grid;electromigration;integrated circuit reliability;large power grid designs scalable vectorless power grid current integrity verification electromigration phenomenon reliable power delivery network large scale power grid current geometric power grid reduction method geometric property electrical property hot wire multilevel power grid verification algorithm	"""To deal with the growing phenomenon of electromigration (EM), power grid current integrity verification becomes indispensable to designing reliable power delivery networks (PDNs). Unlike previous works that focus on vectorless voltage integrity verification of power grids, in this work, for the first time we present a scalable vectorless power grid current integrity verification framework. By taking advantage of multilevel power grid verifications, large-scale power grid current integrity verification tasks can be achieved in a very efficient way. Additionally, a novel EM-aware geometric power grid reduction method is proposed to well preserve the similar geometric and electrical properties of the original grid on the coarse-level power grids, which allows to quickly identify the potential """"hot wires"""" that may carry greater-than-desired currents in a given power grid design. The proposed multilevel power grid verification algorithm provides flexible tradeoffs between the current integrity verification cost and solution quality, while the desired upper/lower bounds for worst case currents flowing through a wire can also be computed efficiently. Extensive experimental results show that our current integrity verification approach can efficiently handle very large power grid designs with good solution quality."""	algorithm;best, worst and average case;electromigration;scalability	Zhuo Feng	2013	2013 50th ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2463209.2488840	electromigration;electronic engineering;parallel computing;computer science;electrical engineering;engineering drawing;multigrid method;integrated circuit design	EDA	23.794941396193945	60.39006284287477	41135
0738c2629ae21a8fde15bc7036a16ca6dcdc057c	using indexing functions to reduce conflict aliasing in branch prediction tables	cache storage;processor architecture;branch prediction;indexing terms;parallel architectures cache storage;parallel architectures;indexation;parallel architectures cache memories;prediction accuracy;processor architecture indexing functions conflict aliasing branch prediction table high performance processor conflict miss elimination memory hierarchy;memory hierarchy;processor architectures;high performance	High-accuracy branch prediction is crucial for high-performance processors. Inspired by the work on indexing functions to eliminate conflict-misses in memory hierarchy, this paper explores different indexing approaches to reduce conflict aliasing in branch-prediction tables. Our results show that indexing functions provide a highly complexity-effective way to enhance prediction accuracy	aliasing;bitwise operation;branch predictor;central processing unit;displacement mapping;exclusive or;kerrison predictor;memory hierarchy;perceptron	Yi Ma;Hongliang Gao;Huiyang Zhou	2006	IEEE Transactions on Computers	10.1109/TC.2006.133	computer architecture;parallel computing;index term;microarchitecture;computer science;theoretical computer science;operating system;branch predictor	HPC	6.00669422816547	39.50915978529752	41159
ebe0396acddbb343ca7e489a5f78c5065737d0e8	a programmable 3.2-gops merged dram logic for video signal processing	model design;programmable circuit;programmable logic devices;traitement signal;evaluation performance;dram chips video signal processing digital signal processing chips vlsi integrated logic circuits programmable logic devices discrete cosine transforms motion compensation integrated circuit design;largeur bande;transformation cosinus;transformacion discreta;performance evaluation;motion compensation;etude theorique;circuit programmable;video signal processing;methode mesure;inverse discrete cosine transform;clocks;gollete estrangulamiento;puce memoire acces direct dynamique;motion compensated;chip;modelisation;integrated circuit design;goulot etranglement;circuito programable;signal video;design guideline;discrete cosine transforms;signal processing;memoire acces direct;transformacion coseno;measuring methods;discrete transformation;vlsi;bandwidth;random access storage;horloge;digital signal processing chips;transformation inverse;processeur programme;video signals;integrated logic circuits;theoretical study;cosine transform;inverse transformation;high performance;bottleneck;transformation discrete;program processors;random access memory logic video signal processing clocks guidelines signal analysis signal design design optimization signal processing discrete cosine transforms;8 bit programmable 3 2 gops merged dram logic video signal processing programmable high performance architecture datapath mdl total required clock cycles dram access rate design guidelines optimized video signal processor inverse discrete cosine transform motion compensation trcc dar data bandwidth processing units maximum performance 8 bit video signals decoding performance multimedia signal processing 3 2 gflops 200 mhz;dram chips;modeling and analysis;transformacion inversa	This paper proposes a programmable high-performance architecture of datapath in the merged DRAM logic (MDL) for video signal processing. A model of a datapath in the programmable MDL is generated, and two basic parameters, total required clock cycles (TRCC) and DRAM access rate (DAR), are defined by analysis of the model. Design guidelines are suggested for the optimized video signal processor based on the modeling and analysis of the MDL. The inverse discrete cosine transform (IDCT) and motion compensation (MC) of the video signal processing are analyzed in the MDL architecture. Two measures, TRCC and DAR, are determined such that the data bandwidth between DRAM and logic is not a bottleneck in the MDL architecture. The efficient datapath is designed based on these design guidelines. The datapath has processing units (ALU, MAC, and Barrel Shifter) with splittabilities of data and multi-port SRAM. The maximum performance of the proposed datapath with 200-MHz clock frequency is 3.2 GOPS for 8-bit video signals, which can deal with decoding high-level (192	8-bit;arithmetic logic unit;barrel shifter;clock rate;clock signal;datapath;discrete cosine transform;dynamic random-access memory;high- and low-level;mdl (programming language);motion compensation;signal processing;static random-access memory	Sunho Chang;Bum-Sik Kim;Lee-Sup Kim	2000	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.867935	chip;embedded system;parallel computing;telecommunications;computer science;programmable logic device;signal processing;discrete cosine transform;very-large-scale integration;motion compensation;bandwidth;algorithm;integrated circuit design	Arch	13.878965240188185	40.61995505508912	41191
ba19c9351edb1f0b35e72b5052db3d79689bad6d	low-power hypercube divided memory fft engine using 3d integration	thermal analysis;3dic;fft;3d integration;memory access;scaling;low power;floating point;power consumption;tsv	In this article we demonstrate a floating point FFT processor that leverages both 3D integration and a unique hypercube memory division scheme to reduce the power consumption of a 1024 point FFT down to 4.227μJ. The hypercube memory division scheme lowers the energy per memory access by 59.2% and increases the total required area by 16.8%. The use of 3D integration reduces the logic power by 5.2%. We describe the tool flow required to realize the 3D implementation and perform a thermal analysis of it.	fast fourier transform;random-access memory	Thorlindur Thorolfsson;Samson Melamed;William Rhett Davis;Paul D. Franzon	2010	ACM Trans. Design Autom. Electr. Syst.	10.1145/1870109.1870114	fast fourier transform;parallel computing;computer hardware;scaling;computer science;floating point;operating system;through-silicon via;thermal analysis	Arch	11.36360028626308	44.674872499933464	41193
d791ef968ce14be568ac34d2dab868c46703e532	a universal method for designing low-power carbon nanotube fet-based multiple-valued logic circuits	energy efficiency;size 32 nm;p type devices;low power carbon nanotube fet based multiple valued logic circuit design;cmos architecture;threshold logic carbon nanotube field effect transistors carrier mobility circuit simulation cmos logic circuits logic design low power electronics multivalued logic circuits nanoelectronics performance evaluation power aware computing spice ternary logic;threshold voltages;cntfet device;cntfet based ternary circuits;synopsys hspice;binary gates;cntfet based mvl circuits;nanoelectronics;high performance multiple vth circuits;power consumption;state of the art quaternary circuits;n type devices;size 32 nm state of the art quaternary circuits power consumption energy efficiency synopsys hspice static power dissipation cntfet based ternary circuits binary gates cmos architecture high performance multiple vth circuits p type devices n type devices carrier mobility threshold voltages cntfet device cntfet based mvl circuits low power mvl circuits nanoelectronics low power carbon nanotube fet based multiple valued logic circuit design;static power dissipation;carrier mobility;low power mvl circuits	This study presents new low-power multiple-valued logic (MVL) circuits for nanoelectronics. These carbon nanotube field effect transistor (FET) (CNTFET)-based MVL circuits are designed based on the unique characteristics of the CNTFET device such as the capability of setting the desired threshold voltages by adopting correct diameters for the nanotubes as well as the same carrier mobility for the Pand N-type devices. These characteristics make CNTFETs very suitable for designing high-performance multiple-Vth circuits. The proposed MVL circuits are designed based on the conventional CMOS architecture and by utilising inherently binary gates. Moreover, each of the proposed CNTFET-based ternary circuits includes all the possible types of ternary logic, that is, negative, positive and standard, in one structure. The method proposed in this study is a universal technique for designing MVL logic circuits with any arbitrary number of logic levels, without static power dissipation. The results of the simulations, conducted using Synopsys HSPICE with 32 nm-CNTFET technology, demonstrate improvements in terms of power consumption, energy efficiency, robustness and specifically static power dissipation with respect to the other state-of-the-art ternary and quaternary circuits.	cmos;electron mobility;field effect (semiconductor);logic gate;low-power broadcasting;spice 2;simulation;three-valued logic;transistor	Mohammad Hossein Moaiyeri;Reza Faghih Mirzaee;Akbar Doostaregan;Keivan Navi;Omid Hashemipour	2013	IET Computers & Digital Techniques	10.1049/iet-cdt.2013.0023	nanoelectronics;electronic engineering;electron mobility;engineering;electrical engineering;pass transistor logic;efficient energy use	EDA	16.056123195151997	58.487342221696515	41199
8c833ee0fe4c9215419373edcfbd81f73fcb8255	message-driven parallel computations on the meiko cs-2 parallel supercomputer	distributed memory;scientific application;parallel computer;gaussian elimination;matrix multiplication;parallel programs	In this paper we focus on how to efficiently program and use the resources of a distributed-memory parallel and vector supercomputer  MEIKO CS-2 for scientific applications using CHARM message-driven parallel programming. Distributed-memory parallel computers  have communication advantage and thus perform better on applications requiring large amount of communication. We show that  with the CHARM message-driven parallel programming one can efficiently overlap communication and computation. Performance  data on applications such as matrix multiplication and gaussian elimination show that we achieve good performance.  	computation;supercomputer	Vikram A. Saletore;Tony F. Neff	1995		10.1007/BFb0046634	computational science;gaussian elimination;parallel computing;distributed memory;embarrassingly parallel;matrix multiplication;computer science;theoretical computer science;massively parallel;algebra	HPC	-3.5493159005936303	38.851033448384804	41225
9d219fb0289b297b4bddc945e0bea76b7660fc36	semu: a parallel processing system for timing simulation of digital cmos vlsi circuits	cmos integrated circuits;global communication;hardware accelerated approach;parallel processing system;full custom floating point processors;sun vme board;design engineering;building block;32 bit;emu software;very large scale integration;digital cmos vlsi circuits;instruction set;parallel ports;hardware accelerator;smoke processor array;runtime library;acceleration;circuit simulation;simulation software;20 mhz;floating point integer unit;workstations;sun;vlsi;floating point;integrated logic circuits;parallel architecture;timing simulation;logic cad;integrated logic circuits parallel processing logic cad vlsi cmos integrated circuits;inter processor communication;parallel processing timing circuit simulation very large scale integration workstations hardware acceleration design engineering sun runtime library;20 mhz semu parallel processing system timing simulation digital cmos vlsi circuits hardware accelerated approach full custom floating point processors sun vme board runtime library emu software smoke processor array floating point integer unit inter processor communication parallel ports global communication instruction set 32 bit;parallel processing;semu;hardware;timing	Describes a hardware accelerated approach to MOS VLSI timing simulation. Accurate timing simulations are crucial to the design and verification of MOS VLSI circuits, but can take prohibitively large amounts of time on an engineering workstation. The SEMU system consists of a 4/spl times/4 array of full custom floating point processors on a single SUN/VME board, runtime library, and the EMU timing simulation software. The basic building block of this parallel architecture is a processor called Smoke that contains a fully integrated 32-bit floating point/integer unit, four parallel ports for inter-processor communication, a parallel port for global communication, and a small but powerful instruction set. Performance of a 20 MHz system on a 4/spl times/4 Smoke processor array is 25-30 times faster than EMU on a 40 MHz Sparc2 Workstation. >	cmos;parallel processing (dsp implementation);simulation;timing closure;very-large-scale integration	Abhaya Asthana;Mike Laznovsky;Boyd Mathews	1994		10.1109/ICVD.1994.282634	embedded system;parallel processing;computer architecture;electronic engineering;parallel computing;computer science;operating system;very-large-scale integration	EDA	7.61400789378535	50.49208806690774	41263
121901404e1268ff37ed553980dbe3e7b644d7e9	fine-grain leakage power reduction method for m-out-of-n encoded circuits using multi-threshold-voltage transistors	multithreshold voltage transistor;protocols;mos devices;asynchronous self timed circuit;crosstalk;threshold logic asynchronous circuits low power electronics;mtcmos asynchronous leakage power;power gating;leakage power reduction;energy dissipation;threshold logic;sleep;total power;four phase handshake protocol fine grain leakage power reduction method m out of n encoded circuits multithreshold voltage transistor asynchronous self timed circuit;fine grain leakage power reduction method;leakage power;logic gates;threshold voltage;transistors;power dissipation;voltage;low power electronics;mtcmos;asynchronous circuits;delay protocols energy dissipation voltage power dissipation crosstalk asynchronous circuits transistors laboratories combinational circuits;four phase handshake protocol;m out of n encoded circuits;benchmark testing;asynchronous;combinational circuits	Asynchronous self-timed circuits which tolerate any delay variations are a feasible solution to timing-related problems while they suffer large energy dissipation due to large amount of their circuits. In  future process technologies, it has been recognized that leakage power is a large factor in the total power dissipation.  In this paper, we propose two leakage power reduction methods for m-out-of-n encoded circuits which operate based on the four-phase handshake protocol using multi-threshold-voltage transistors. One method is based on the traditional power gating technique which reduces leakage power by shutting off the power of idle circuits. Another method is to apply high-threshold-voltage transistors into the off-state transistors whose gate input signals are inactive in the idle phase. The evaluation results show that about 94% and 81% of the leakage power in the idle phase can be reduced.	asynchronous circuit;power gating;spectral leakage;transistor	Masashi Imai;Kouei Takada;Takashi Nanya	2009	2009 15th IEEE Symposium on Asynchronous Circuits and Systems	10.1109/ASYNC.2009.11	electronic engineering;real-time computing;engineering;electrical engineering	EDA	17.918699705566446	56.73332179465501	41337
f40b88cde230a9d8bc8caec157a1cc38f39c9c5e	a java processor ip design for embedded soc	dynamic class loading;application processor soc;performance;embedded systems;design;experimentation;article;java accelerator	In this article, we present a reusable Java processor IP for application processors of embedded systems. For the Java microarchitecture, we propose a low-cost stack memory design that supports a two-fold instruction folding pipeline and a low-complexity Java exception handling hardware. We also propose a mapping between the Java dynamic class loading model and the SoC platform-based design principle so that the Java core can be encapsulated as a reusable IP. To achieve this goal, a two-level method area with two on-chip circular buffers is proposed as an interface between the RISC core and the Java core. The proposed architecture is implemented on a Xilinx Virtex-5 FPGA device. Experimental results show that its performance has some advantages over other Java processors and a Java VM with JIT acceleration on a PowerPC platform.	central processing unit;circular buffer;embedded system;exception handling;field-programmable gate array;java classloader;java platform, enterprise edition;java processor;java virtual machine;just-in-time compilation;microarchitecture;platform-based design;powerpc 600	Chun-Jen Tsai;Han-Wen Kuo;Zi-Gang Lin;Zi-Jing Guo;Jun-Fu Wang	2015	ACM Trans. Embedded Comput. Syst.	10.1145/2629649	embedded system;design;computer architecture;parallel computing;real-time computing;java concurrency;performance;computer science;operating system;strictfp;embedded java;real time java;java	EDA	-0.11862302123859388	48.21987239180153	41352
8e2f61f6e12eaff6aadb2a0ecb2547e3cf478406	a programmable processor for approximate string matching with high throughput rate	dynamic programming;database system;approximate string matching;query processing;information retrieval;0 6 micron programmable processor approximate string matching high throughput rate multimedia applications information retrieval applications dynamic programming procedure string to string correction problem full text search database system wildcards cmos technology character comparisons text processor asic 132 mhz;edit distance;database machines;dynamic program;multimedia systems;cmos digital integrated circuits;application specific integrated circuits;vlsi;throughput information retrieval delay multimedia databases database systems acceleration application software cmos process cmos technology clocks;vlsi cmos digital integrated circuits microprocessor chips string matching multimedia systems query processing database machines application specific integrated circuits dynamic programming high speed integrated circuits;string matching;high throughput;cmos;high speed integrated circuits;microprocessor chips	In this paper we present the algorithm and architecture of a processor for approximate string matching with high throughput rate. The processor ist dedicated for multimedia and information retrieval applications working on huge amounts of mass data where short response times are necessary. The algorithm used for the approximate string matching is based on a dynamic programming procedure known as the string-to-string correction problem. It has been extended to fulfil the requirements of full text search in a database system, including string matching with wildcards and handling of idiomatic turns of some languages. The processor has been fabricated in a 0.6-μm CMOS technology. It performs a maximum of 8.5 Billion character comparisons per second when operating at the specified clock frequency of 132 MHz.	application-specific integrated circuit;approximate string matching;approximation algorithm;cmos;clock rate;database;datapath;dynamic programming;full custom;information retrieval;programming idiom;reconfigurable computing;requirement;scalability;string searching algorithm;string-to-string correction problem;throughput;wildcard character	Hans-Martin Blüthgen;Tobias G. Noll	2000		10.1109/ASAP.2000.862401	high-throughput screening;embedded system;computer architecture;parallel computing;real-time computing;edit distance;approximate string matching;commentz-walter algorithm;computer science;theoretical computer science;operating system;dynamic programming;application-specific integrated circuit;very-large-scale integration;programming language;cmos;string searching algorithm	DB	7.330520831761064	41.5265448180502	41361
4ae5973504cbfb9c0b7b9fa9db8defb7ecf8d4e1	a multi-stage thermal management strategy for 3d multicores	processing elements multistage thermal management strategy 3d multicores 3d integration technology ic performance integrated circuits heat generation power dissipation thermal aware design temperature power thermal model temperature gradient;silicon;temperature multiprocessing systems power aware computing;heat sinks silicon three dimensional displays integrated circuit modeling thermal conductivity approximation methods;three dimensional displays;integrated circuit modeling;thermal conductivity;approximation methods;heat sinks	3D integration technology has the potential to enhance IC performance, improve functionality and lessen wiring of ICs. However, it poses several challenges, where the key challenge is heat generation from internal active layers due to power dissipation. To mitigate this challenge, thermal aware design has become a necessity. Towards thermal aware design, this paper proposes a two stage design technique. In the first stage, a temperature-power thermal model is created to calculate power dissipated by an IC at an input temperature. The proposed model calculates power dissipated by 2D and 3D ICs with an average error of 0.37% and 25% respectively. Power calculation helps in process variation, validation of power models and minimization of temperature gradients. In the second stage, thermal aware mapping is performed for the ICs. For thermal aware mapping, three mapping algorithms are proposed to account for different resource (processor) availability scenarios. Each algorithm utilizes temperature-power thermal model (from the first design stage) to map applications to processing elements in a 3D IC. The proposed two stage design technique performs faster temperature to power calculations than existing techniques. It provides a simplified approach to mapping compared to existing techniques by utilizing power dissipated by processing elements to map applications.	algorithm;gradient;map;mathematical optimization;thermal management of high-power leds;three-dimensional integrated circuit;transient state;wiring	Dipika Suresh;Amit Kumar Singh;Akash Kumar	2014	2014 25nd IEEE International Symposium on Rapid System Prototyping	10.1109/RSP.2014.6966896	embedded system;electronic engineering;engineering;electrical engineering;heat sink;silicon;thermal conductivity	EDA	15.055156089990666	54.80668596106618	41392
7b6c696d36d2f8f4518c3d434d1146ed7aa17bc3	test generation for cyclic combinational circuits	circuito combinatorio;concepcion circuito;integrated circuit;automatic testing;circuit design;circuito integrado;circuito logico;test;ensayo;algorithme;combinatory circuit;algorithm;essai;circuit logique;automatic testing combinational circuits logic testing timing;circuit combinatoire;logic testing;test generation;fault coverage;circuit testing combinational circuits circuit faults circuit topology redundancy sufficient conditions history feedback test pattern generators circuit analysis;conception circuit;test pattern generator;timing analysis test generation cyclic combinational circuits untestable faults classification testing algorithm ram program;combinational circuit;formal analysis;logic circuit;circuit integre;algoritmo;combinational circuits;timing	F Abstract-Circuits that have an underlying acyclic topology are guaranteed to be combinational since feedback is necessary for sequential behavior. However, the reverse is not true, i.e., feedback is not a sufficient condition since there do exist combinational logic circuits that are cyclic. In fact, such combinational circuits occur often in bus structures in data paths. This class of circuits has largely been ignored by conventional combinational single-stuck-at fault test pattern generators which assume that the circuit topology is acyclic. There has not been a formal study of the test generation problem for these circuits. Also, no algorithms and tools exist for this purpose. In practice, test generation for these circuits is handled in an awkward manner, typically with poor fault coverage. This work provides, for the first time, a formal analysis of the test generation problem for these circuits. This analysis leads to a clear insight into generation of tests, as well as a classification of untestable faults for such circuits. We demonstrate that cyclic combinational circuits may have untestable faults that do not correspond to redundancies. This insight is then translated to a testing algorithm which has been implemented in the program RAM. RAM has been successful in providing complete or near complete coverage on a range of typical examples, which is significantly higher than that provided by conventional techniques. t	algorithm;circuit topology;combinational logic;directed acyclic graph;fault coverage;feedback;logic gate;random-access memory;stuck-at fault;test card	Anand Raghunathan;Pranav Ashar;Sharad Malik	1995	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.469666	electronic engineering;real-time computing;computer science;engineering;combinational logic;algorithm	EDA	20.809735625272953	49.21201940169445	41502
fe2f99b00fa1f7f8225beece2c6e7358411bb7ac	exploiting dark fluorophore states to implement resonance energy transfer pre-charge logic		As lithographic feature sizes approach fundamental scaling limits, a variety of computational domains remain incompatible with integrated circuits merely due to their operating principles. Resonance energy transfer (RET) logic offers a molecular-scale solution for accessing these untapped domains. This emerging technology uses self-assembled networks of fluorescent molecules to perform computation at scales far below the diffraction limit and in environments that preclude silicon electronics. In this article, the authors propose a new form of RET logic design that yields a library of nonlinear logic gates that can be cascaded to build more complex integrated molecular circuits.	computation;image scaling;integrated circuit;logic gate;nonlinear system;resolution enhancement technology;resonance	Craig D LaBoda;Chris Dwyer;Alvin R. Lebeck	2017	IEEE Micro	10.1109/MM.2017.3211112	real-time computing;electronics;resonance;logic synthesis;computation;logic gate;electronic circuit;integrated circuit;computer science;dna nanotechnology;electronic engineering	Arch	12.318869542829676	57.94064695068542	41530
7acd5bc78dfbf1c41a6ec8fbd76ed712ff15d234	an efficient tlm/t modeling and simulation environment based on conservative parallel discrete event principles	circuit simulation;discrete event simulation;multiprocessing systems;system-on-chip;bca simulation;systemc sc-thread;tlm simulation;cache effects;hardware architecture;interconnects;multi-clusters multi-processors soc;parallel discrete event simulation;transaction level model	The paper presents an innovative simulation scheme to speed-up simulations of multi-clusters multi-processors SoCs at the TLM/T (transaction level model with time) abstraction level. The hardware components of the SoC architecture are written in standard SystemC. The goal is to describe the dynamic behavior of a given software application running on a given hardware architecture (including the dynamic contention in the interconnect and the cache effects), in order to provide the system designer with the same reliable timing information as a cycle accurate simulation, with a simulation speed similar to a TLM simulation. The key idea is to apply parallel discrete event simulation (PDES) techniques to a collection of communicating SystemC SC-THREAD. Experimental results show a simulation speedup of a factor up to 50 versus a BCA simulation (bus cycle accurate), for a timing error lower than 10-3	abstraction layer;central processing unit;design space exploration;logic simulation;performance evaluation;simulation;speedup;system on a chip;systemc;systems design;thread (computing)	Emmanuel Viaud;François Pêcheux;Alain Greiner	2006	Proceedings of the Design Automation & Test in Europe Conference		system on a chip;embedded system;computer architecture;application software;parallel computing;real-time computing;computer science;discrete event simulation;operating system;dynamic web page;logic simulation;hardware architecture;modeling and simulation;bandwidth;systems design	EDA	-0.04430304557552695	56.16864972283383	41559
6346f33fdda9d0bb62ef0f6365d893348c270bbc	performance-based modeling and restructuring of object oriented software for pipeline architecture	pipeline architecture;object oriented software;software restructuring;performance modeling and analysis;distributed systems	Pipelining is the suitable architecture to adopt applications that are naturally divided into stages. Recently, applications tend to be Object Oriented (OO). Within the context of OO, there are a lot of interactions among different objects that result in many communication activities. Besides the feed-forward communication activities, many bypassing activities are generated in the pipeline structure.#R##N##R##N#In this paper, we present a performance model that analyzes and evaluates the execution and communication times of OO software that runs on pipeline architecture. The model realizes both the feed-forward and the bypassing communication. We utilize the model to restructure the target software to achieve better performance. The restructuring algorithm has two phases; the first phase is concerned with maximizing the throughput. The second phase aims to minimize the latency and fully exploit the system resources.	pipeline (computing)	Amal Abdel-raouf;Reda A. Ammar;Tahany A. Fergany	2006	J. Comput. Meth. in Science and Engineering		reference architecture;parallel computing;real-time computing;computer science;operating system;software engineering;database;distributed computing;programming language	SE	-3.162995089718321	49.58778551431703	41562
9552a7e8220fa7a2ea1ded964671990e68bd0c5c	fault tolerance techniques in the neax61 digital switching system and its field performance	active methods;switched system;fault tolerant;quality improvement;quality control electronic switching systems fault location;fault detection;electronic switching systems;fault tolerant systems switching systems control systems maintenance availability protection national electric code multiplexing fault detection communication system control;system availability neax61 digital switching system field performance fault tolerance techniques operation modes fault detection systems abnormality supervision system restart functions service interruptions quality improvement activities;quality control;fault location	The fault-tolerance techniques applied to the NEAX61 digital switching system are described, focusing on its operation modes, fault detection systems, abnormality supervision, and system restart functions. Also discussed is the improved field performance of NEAX61 systems over three years in terms of service interruptions, which were reduced as a result of quality improvement activities. Methods to further improve system availability are described. >	fault tolerance	Yukio Itoh;Hiroshi Kawashima;Yuichi Shimojo;Kenichiro Kawase	1988	IEEE Journal on Selected Areas in Communications	10.1109/49.7876	embedded system;quality control;fault tolerance;quality management;real-time computing;fault coverage;fault indicator;computer science;stuck-at fault;fault detection and isolation;software fault tolerance	Visualization	7.0325977437132074	56.41385719658162	41581
5e65f76c260265dfd2dde7fb9ed0f902a56b113c	a single chip 1024 bits rsa processor	institutional repositories;fedora;vital;chip;power consumption;vtls;high performance;ils	A new carry-free division algorithm will be described; it is based on the properties of RSD arithmetic to avoid carry propagation  and uses the minimum hardware per bit i.e. one full-adder. Its application to a 1024 bits RSA cryptographic chip will be presented.  Thanks to the features of this new algorithm, high performance (8 kbits/s for 1024 bits words) was obtained for relatively  small area and power consumption (80 mm2 in a 2 μm CMOS process and 500 mW at 25 MHz).  		André Vandermeulebroecke;Etienne Vanzieleghem;Tony Denayer;Paul G. A. Jespers	1989		10.1007/3-540-46885-4_24	chip;embedded system;parallel computing;computer science;operating system	Theory	9.494997874629105	44.46313448664724	41588
bb120703179ed9df74d439d3e4caf1fe0d04b294	optimal and efficient parallel algorithms for summing and prefix summing	distributed memory;noncommutative binary operators efficient parallel algorithms optimal parallel algorithms summing prefix summing optimal algorithms latency dependent distributed memory model optimal summing algorithm inherent structure near optimal algorithms commutative binary operators;distributed memory systems;parallel algorithm;non commutative;mathematical operators;parallel algorithms algorithm design and analysis phase change random access memory costs delay commutation computer science binary trees computational modeling concurrent computing;distributed memory systems parallel algorithms parallel algorithms mathematical operators mathematical operators distributed memory systems;optimal algorithm;parallel algorithms	W e consider the problem of designing ef ic ient parallel algorithms for summing and prefix summing. In this paper, we present optimal algorithms f o r summing on a latency-dependent distributed-memory model and show that any optimal summing algorithm must have an inherent structure. Moreover, we present optimal or near-optimal algorithms f o r prefix summing for both non-commutative and commutative binary operators. Furthermore, we show that the optimal algorithms f o r prefix summing for these two types of operators are not equivalent.	distributed memory;parallel algorithm	Eunice E. Santos	1996		10.1109/SPDP.1996.570375	parallel computing;computer science;theoretical computer science;parallel algorithm;algorithm	Theory	4.117511444973184	38.77124413744447	41689
77be5632180cf1235e29cf44db6747545a16aeec	work in progress — design and delivery of the graduate course on electronic systems integration	computers;software;printed circuits;circuit manufacture;network synthesis;integrated circuit;system analysis and design;analysis and design;software system analysis and design printed circuits layout computers circuit simulation educational institutions;software systems;design and integration;layout;input output;circuit simulation;integrated project;system design;system integration;circuit manufacture electronic systems design and integration circuit simulation;electronic engineering education;complex circuits graduate course electronic systems integration electronics industry electronic systems design input output characteristics circuit simulation software printed circuit board design software printed circuit board maker printed circuit board based electronic circuits;printed circuits circuit simulation electronic engineering education network synthesis;printed circuit board;student learning;electrical engineering;work in progress;graduate student;electronic systems	This paper discusses the approach to improve the preparation of graduate students in Electrical Engineering for projects in the electronics industry by introducing the graduate course: Electronic Systems Design and Integration. The course emphasizes the understanding and skills necessary for students to achieve competencies at the system and sub-system level of electronic project design, test, and validation. Basic and advanced electronic circuits are studied and modeled in terms of their input-output characteristics. In this course, the circuit simulation software, the printed circuit board design software, printed circuit board maker and its related software are introduced to students. Students are required to complete several subsystem level and system level projects by employing the above software and hardware to design, build, test, and validate the final products. Through this procedure, students learn the required software and hardware for the electronic systems design and become familiar with the process to manufacture printed circuit board based electronic circuits. The purpose of offering this course is to (a) enable graduate students to apply what they learn in electronics courses during undergraduate studies to design complex circuits for electronic systems integration, and (b) to prepare students for electronic systems design and integration projects in industry.	data validation;electrical engineering;electronic circuit simulation;emoticon;printed circuit board;printing;simulation software;system integration;systems design	Qing Zheng;Ramakrishnan Sundaram;Fong Mak	2011	2011 Frontiers in Education Conference (FIE)	10.1109/FIE.2011.6142825	electronic engineering;electronic design automation;computer science;engineering;electrical engineering;printed circuit board;electronic circuit simulation;computer engineering	EDA	9.849293115082505	52.19089840550546	41710
2b7893958a5d54ffc31633a968379289b8052aa2	a discrete pso for multi-objective optimization in vlsi floorplanning	floorplanning;multi objective optimization;physical design;discrete pso;very large scale integrated;chip;mop;discrete particle swarm optimization;multi objective optimization problem	Floorplanning is a critical step in the physical design of Very Large Scale Integrated (VLSI) circuits. Its main target is optimizing the layout area and interconnection wire length of chips, which can be transformed into a Multi-objective Optimization Problem (MOP). In this paper, we propose a discrete Particle Swarm Optimization (PSO) algorithm for MOP which could take many key objectives into consideration and give a good compromise between them. The experiments on MCNC benchmarks show that the proposed algorithm is effective, and gives out many optional results for user's choice according to partialness, which can not be finished by traditional methods.		Jinzhu Chen;Guolong Chen;Wenzhong Guo	2009		10.1007/978-3-642-04843-2_43	mathematical optimization;multi-swarm optimization;electronic engineering;engineering;engineering drawing	EDA	14.184187743510961	52.46070810782215	41785
7e01126cc3396a8a30af27a9068b1e3c5dc11c14	power and area reduction in multi-stage addition using operand segmentation	digital signal processing;finite impulse response filters;registers;adders;fir filters adders digital signal processing chips;proceedings paper;adders delays finite impulse response filters hardware registers digital signal processing;power area reduction power reduction multistage addition operand segmentation carry bypass segmented adders 16 tap fir filter delay;delays;hardware	This paper presents an architectural technique to efficiently implement multi-stage additions through operand segmentation. Carry bypass is leveraged to break the dependency between the two segmented adders, reducing the delay of the critical path. This allows for power- and area-efficient hardware implementation due to the increased timing margin for architectural transformations at the cost of one extra clock cycle. Compared to existing segmented-adders, the proposed architecture has the least hardware overhead with near execution time. An accumulator and a 16-tap FIR filter are used to demonstrate the delay, power, and area improvements of the proposed technique. The synthesis results show that the delay is improved by up to 42% and 28.1%. Given the same timing constraint, the adder area is reduced by 27.4% and 12.4%.	accumulator (computing);adder (electronics);carry-select adder;carry-skip adder;clock signal;critical path method;filter design;finite impulse response;mathematical optimization;multiple encryption;operand;overhead (computing);run time (program lifecycle phase);software propagation	Ching-Da Chan;Wei-Chang Liu;Chia-Hsiang Yang;Shyh-Jye Jou	2013	2013 International Symposium onVLSI Design, Automation, and Test (VLSI-DAT)	10.1109/VLDI-DAT.2013.6533879	electronic engineering;parallel computing;real-time computing;computer science	Arch	13.396467961186861	47.90560152470297	41795
0292f9e8c29aa779db664cdb6f6d0223e42cac28	dram-cell-based multiple-valued logic-in-memory vlsi with charge addition and charge storage	mos capacitors;electronic mail;cmos technology;hspice simulation;mosfets;very large scale integration;multivalued logic circuits;multiple valued threshold literal function;logic circuits;communication bottleneck;logic in memory vlsi;system on a chip;acceleration;nonvolatile memory;very large scale integration cmos technology mosfets combinational circuits nonvolatile memory mos capacitors electronic mail logic circuits system on a chip acceleration;hspice simulation multiple valued logic in memory charge addition charge storage fast reprogrammability multiple valued threshold literal function logic circuits multiple valued inputs logic in memory vlsi;functional pass gate;logic in memory;charge storage;multiple valued inputs;fast reprogrammability;threshold operation;multiple valued;dram;dram chips multivalued logic circuits;charge addition;dram chips;combinational circuits;vlsi architecture;multiple valued logic	A multiple-valued logic-in-memory VLSI with fast reprogrammability is proposed to realize transfer-bottleneck-free VLSI systems. A basic component, in which a dynamic storage function and a multiple-valued threshold-literal function are merged, can be simply implemented by charge addition and charge storage with a DRAM-cell-based circuit structure. Any logic circuits with multiple-valued inputs and binary outputs can be realized by the combination of the basic components and logic-value conversion. As a typical example, a fully parallel magnitude comparator between three-valued input and stored words is designed by using the proposed logic-in-memory VLSI architecture. Its performance is superior to that of a corresponding binary implementation by using HSPICE simulation under a 0.5-/spl mu/m CMOS technology.	dynamic random-access memory;very-large-scale integration	Takahiro Hanyu;Hiromitsu Kimura;Michitaka Kameyama	2000		10.1109/ISMVL.2000.848652	acceleration;system on a chip;embedded system;electronic engineering;parallel computing;non-volatile memory;logic gate;computer science;theoretical computer science;very-large-scale integration;combinational logic;cmos;dram	EDA	16.849926384018232	57.80297308415639	41797
df604f8719499e896dede53ed73d2bd9826cf561	vector extraction for average total power estimation	leakage power;large-scale design;power consumption;vector extraction;average tpower vector extraction;power estimation;average total power estimation;tpower vector extraction;large-scale designs;integrated circuit design;dynamic power;design level;distribution analysis;total power;specified power property;circuit simulation;tdm;interconnect	Power consumption has become a primary constraint of integrated circuit design. Many models have been proposed to evaluate dynamic and leakage power in every design level. However, how to accurately predict TPower, the total power with dynamic and leakage power included, for large-scale designs within reasonable time remains unsolved. In this paper, a new topic of vector extraction for power estimation is brought forward based on the distribution analysis of power consumption of different types. After extracted, a large number of vectors are compacted into much fewer without significant influence on the specified power property, which makes the application of accurate and fast simulator possible. For the purpose of validation, we use the method on average TPower vector extraction and obtain good experimental results.		Yongjun Xu;Jinghua Chen;Zuying Luo;Xiaowei Li	2005		10.1109/ASPDAC.2005.1466529	control engineering;physical design;power module;power budget;electronic engineering;telecommunications;power factor;computer science;engineering;electrical engineering;interconnection;constant power circuit;switched-mode power supply;circuit extraction;volt-ampere;power optimization;time-division multiplexing;low-power electronics;integrated circuit design	EDA	19.289906849965902	55.50162498357387	41835
5f19aaaf026a9331c23d7dca70304d2df7206d29	a self-stabilizing algorithm for graph searching in trees	graph search;distributed system;on the fly;tree network	Graph searching games have been extensively studied in the past years. The graph searching problem involves a team of searchers who are attempting to capture a fugitive moving along the edges of the graph. In this paper we consider the graph searching problem in a network environment, namely a tree network. Searchers are software programs and the fugitive is a virus that spreads rapidly. Every node of the network which the virus may have reached, becomes contaminated. The purpose of the game is to clean the network. In real world distributed systems faults can occur and thus it is desirable for an algorithm to be able to facilitate the cleaning of a network in an optimal way, and also to reconfigure on the fly.#R##N##R##N#In this paper we give the first self-stabilizing algorithm for solving the graph searching problem in trees. Our algorithm stabilizes after  O  ( n  3) time steps under the distributed adversarial daemon. Our algorithm solves the node searching variant of the graph searching problem, but can with small modifications also solve edge and mixed searching.	algorithm;self-stabilization	Rodica Mihai;Morten Mjelde	2009		10.1007/978-3-642-05118-0_39	spqr tree;feedback arc set;graph bandwidth;computer science;connectivity;theoretical computer science;machine learning;distributed computing;graph;moral graph;complement graph;tree decomposition	ML	18.882438165728228	33.57240781588759	41883
2e53229085908343d03784a9795f3c9a1acfb9fa	fridge: a fixed-point design and simulation environment	circuit cad;circuit analysis computing;digital arithmetic;digital integrated circuits;high level synthesis;integrated circuit design;interpolation;ansi-c;fridge;compile-time analyses;fixed-point architectures;fixed-point design/simulation environment;fixed-point implementation;floating-point description;hardware/software codesign;interactive automated transformation;interpolative approach;seamless design flow	Digital systems, especially those for mobile applications are sensitive to power consumption, chip size and costs. Therefore they are realized using fixed-point architectures, either dedicated HW or programmable DSPs. On the other hand, system design starts from a floating-point description. These requirements have been the motivation for FRIDGE (Fixed-point pRogrammIng DesiGn Environment), a design environment for the specification, evaluation and implementation of fixed-point systems. FRIDGE offers a seamless design flow from a floating- point description to a fixed-point implementation. Within this paper we focus on two core capabilities of FRIDGE: (1) the concept of an interactive, automated transformation of floating-point programs written in ANSI-C into fixed-point specifications, based on an interpolative approach. The design time reductions that can be achieved make FRIDGE a key component for an efficient HW/SW-CoDesign. (2) a fast fixed-point simulation that performs comprehensive compile-time analyses, reducing simulation time by one order of magnitude compared to existing approaches.	ansi c;algorithm;cognitive dimensions of notations;compile time;compiler;fixed point (mathematics);fixed-point arithmetic;hybris;mobile app;operand;requirement;seamless3d;simulation;systems design	Holger Keding;Markus Willems;Martin Coors;Heinrich Meyr	1998			chip;embedded system;design;computer architecture;electronic engineering;real-time computing;ansi c;quantization;interpolation;computer science;floating point;design flow;operating system;digital signal processing;fixed point;integrated circuit design	EDA	4.548056703282131	53.081040309125704	41890
5850e025714a34c31d7b14942fd58228a166d307	predictable design of low power systems by pre-implementation estimation and optimization	circuit optimisation;electronic design automation;low-power electronics;systems analysis;algorithmic-level;architecture-level;design iteration;high level design decision;low power system design;microelectronics industry;optimization;power estimation tool;preimplementation estimation;system-level design flow	Each year tens of billions of Dollars are wasted by the microelectronics industry because of missed deadlines and delayed design projects. These delays are partially due to design iterations many of which could have been avoided if the low level remifications of high level design decisions, at the Architecture- and Algorithmic-level would have been known before the time consuming and tedious RT- and lower level implementation started. In this contribution we present a System-level design flow and respective EDA support tools for low power designs. We analyze the requirements for such a design technology, which shifts more responsibility to the system architect. We exemplify this approach with a design flow for low power systems. The architecture of an Algorithm-level power estimation tool will be presented together with some use cases based on a EDA product which has been commercially developed from the research results of several collaborative projects funded by the Commission of the European Community.	algorithm;computer architecture;design flow (eda);electronic design automation;electronic system-level design and verification;exemplification;high-level programming language;ibm power systems;iteration;level design;mathematical optimization;requirement;unicom system architect	Wolfgang Nebel	2004	ASP-DAC 2004: Asia and South Pacific Design Automation Conference 2004 (IEEE Cat. No.04EX753)	10.1145/1015090.1015095	use case;iterative design;physical design;embedded system;systems analysis;electronic engineering;real-time computing;probabilistic design;electronic design automation;computer science;engineering;design flow;electrical engineering;computer-automated design;common power format;electronic system-level design and verification;design technology;power optimization;low-power electronics;systems design	EDA	8.239625774009845	54.82665248363591	41948
bc5bad9fe377bb613470c7c68026b8975301f4d9	comparison of parallel particle swarm optimizers for graphical processing units and multicore processors	graphic processing units;multilevel inverters;cuda;particle swarm optimization;mpi;parallel implementation	In this paper, we present a parallel implementation of the particle swarm optimization (PSO) on graphical processing units (GPU) using CUDA. By fully utilizing the processing power of graphic processors, our implementation (CUDA-PSO) provides a speedup of 167× compared to a sequential implementation on CPU. This speedup is significantly superior to what has been reported in recent papers and is achieved by four optimizations we made to better adapt the parallel algorithm to the specific architecture of the NVIDIA GPU. However, because today's personal computers are usually equipped with a multicore CPU, it may be unfair to compare our CUDA implementation to a sequential one. For this reason, we implemented a parallel PSO for multicore CPUs using MPI (MPI-PSO) and compared its performance against our CUDA-PSO. The execution time of our CUDA-PSO remains 15.8× faster than our MPI-PSO which ran on a high-end 12-core workstation. Moreover, we show with statistical significance that the results obtained using our CUDA-PSO are of equal quality as the results obtained by the sequential PSO or the MPI-PSO. Finally, we use our parallel PSO for real-time harmonic minimization of multilevel power inverters with 20 DC sources while considering the first 100 harmonics and show that our CUDA-PSO is 294× faster than the sequential PSO and 32.5× faster than our parallel MPI-PSO.	multi-core processor;swarm	Vincent Roberge;Mohamed Tarbouchi	2013	International Journal of Computational Intelligence and Applications	10.1142/S1469026813500065	mathematical optimization;computer architecture;parallel computing;computer science;message passing interface;theoretical computer science;particle swarm optimization	Arch	-1.8297039689005958	43.39922187902755	41962
ca7dbf98f51b84e820ca3915ff092d8ae3465dcc	whirlpool plas: a regular logic structure and their synthesis	wpla synthesis;finite state machines;finite state machine;logic cad;combinational circuits;programmable logic arrays;whirlpool plas;regular logic structure;logic arrays;compact layout;delay trade-off;four-level logic minimization algorithm;minimisation of switching nets;multivalued logic circuits;wpla;four-level boolean;logic array;combinational logic;four-level boolean nor network;whirlpool pla;boolean functions;regular circuit structure;equivalence checking	A regular circuit structure called a Whirlpool PLA (WPLA) is proposed. It is suitable for the implementation of finite state machines as well as combinational logic. A WPLA is logically a four-level Boolean NOR network. By arranging the four logic arrays in a cycle, a compact layout is achieved. Doppio-ESPRESSO, a four-level logic minimization algorithm is developed for WPLA synthesis. No technology mapping, placement or routing is necessary for the WPLA. Area and delay trade-off is absent, because these two goals are usually compatible in WPLA synthesis.	whirlpool (cryptography)	Fan Mo;Robert K. Brayton	2002		10.1109/ICCAD.2002.1167585	boolean circuit;and-or-invert;circuit minimization for boolean functions;electronic engineering;discrete mathematics;logic synthesis;logic optimization;diode–transistor logic;logic level;logic gate;logic family;programmable logic array;computer science;programmable logic device;pass transistor logic;formal equivalence checking;mathematics;sequential logic;combinational logic;finite-state machine;boolean function;simple programmable logic device;digital electronics;register-transfer level;algorithm;resistor–transistor logic	Logic	15.975611053517596	49.266207219504324	42019
34da1b76b73439efefd96b3fa3f2a7d1bfeb4624	improving locality using a graph-based technique for detecting memory layouts of arrays			locality of reference	Mahmut T. Kandemir;Alok N. Choudhary;J. Ramanujam;Prithviraj Banerjee	1999			locality;theoretical computer science;computer science;graph	Logic	-2.4255419616032783	43.76384619973078	42027
09ab52ad5139afdd7310cc75a1454bf08318b3e7	silicon compaction/defragmentation for partial runtime reconfiguration	design flow;fpga;power efficiency;field programmable gate arrays;place and route	"""The effective use of Run Time Reconfiguration (RTR) in modern FPGAs opens up new avenues to design area and power efficient high performance architectures. However the current design flow for exploiting RTR in designs, leads to the problem of silicon Defragmentation. We propose a silicon compaction/defragmentation technique which works on already placed and routed modules to generate partial bitstreams (programming files) for the device. We have outlined a method which generates these partial bitstreams very fast taking into account the size and position of the """"free"""" silicon when the device is in operation. The other advantage of this method is that the changes in the basic FPGA fabric needed to implement this defragmentation strategy are (almost) trivial."""	algorithm;data compaction;embedded system;field-programmable gate array;real-time recovery;routing;run time (program lifecycle phase)	Kolin Paul;Joel Porquet;Josep Llosa	2007	10th Euromicro Conference on Digital System Design Architectures, Methods and Tools (DSD 2007)	10.1109/DSD.2007.4341487	embedded system;parallel computing;real-time computing;computer science;operating system;field-programmable gate array	EDA	6.992191763455005	55.50786984395509	42032
d4eed607bb2cd9f52b07d9f5d8ef7be2a2c21220	energy-efficient 4t sram bitcell with 2t read-port for ultra-low-voltage operations in 28 nm 3d monolithic coolcube™ technology		This paper presents a 4T-based SRAM bitcell optimized both for write and read operations at ultra-low voltage (ULV). The proposed bitcell is designed to respond to the requirements of energy constrained systems, as in the case of most IoT-oriented circuits and applications. The use of 3D CoolCubeTM technology enables the design of a stable 4T SRAM bitcell by using data-dependent back biasing. The proposed bitcell architecture provides a major reduction of the write operation energy consumption compared to a conventional 6T bitcell. A dedicated read port coupled to a virtual GND (VGND) ensures a full functionality at ULV of read operations. Simulation results show reliable operations down to 0.35 V close to six sigma (6 σ) without any assist techniques (e.g. negative bitlines), achieving in worst case corner 300 ns and 125 ns in write and read access time, respectively. A 6x energy consumption reduction compared to a ULV ultra-low-leakage (ULL) 6T bitcell is demonstrated.		Reda Boumchedda;Jean-Philippe Noel;Bastien Giraud;Adam Makosiej;Marco Antonio Rios;Eduardo Esmanhotto;Emilien Bourde-Cic&#233;;Mathis Bellet;David Turgis;Edith Beigné	2018	2018 IEEE/ACM International Symposium on Nanoscale Architectures (NANOARCH)	10.1145/3232195.3232210	electronic engineering;computer science;static random-access memory;access time;transistor;energy consumption;efficient energy use;electronic circuit;low voltage;logic gate	Arch	17.69502600689448	58.79545250431844	42075
bded1d2d1947c4df3fbd3cdc4d2c422065b931ee	design of ternary d flip-flop with pre-set and pre-reset functions based on resonant tunneling diode literal circuit	resonant tunneling diode rtd;mi lin wei feng lv ling ling sun design of ternary d flip flop with pre set and pre reset functions based on resonant tunneling diode literal circuit;resonant tunneling diode;ternary logic;literal circuit;d flip flop;multiple valued;flip flop;multiple valued logic	The problems existing in the binary logic system and the advantages of multiple-valued logic (MVL) are introduced. A literal circuit with three-track-output structure is created based on resonant tunneling diodes (RTDs) and it has the most basic memory function. A ternary RTD D flip-flop with pre-set and pre-reset functions is also designed, the key module of which is the RTD literal circuit. Two types of output structure of the ternary RTD D flip-flop are optional: one is three-track and the other is single-track; these two structures can be transformed conveniently by merely adding tri-valued RTD NAND, NOR, and inverter units after the three-track output. The design is verified by simulation. Ternary flip-flop consists of an RTD literal circuit and it not only is easy to understand and implement but also provides a solution for the algebraic interface between the multiple-valued logic and the binary logic. The method can also be used for design of other types of multiple-valued RTD flip-flop circuits.	diode;flops;flip-flop (electronics);literal (mathematical logic);logic gate;memory bound function;power inverter;simulation;split tunneling;triangular function;tunneling protocol;zhi-li zhang	Mi Lin;Wei-feng Lü;Lingling Sun	2011	Journal of Zhejiang University SCIENCE C	10.1631/jzus.C1000222	embedded system;electrical engineering;mathematics;resonant-tunneling diode;algorithm	EDA	18.969838633855215	44.94894627286568	42086
306976c50a7383ebc370f4964859b30045369cb7	combinational access tunnel fet sram for ultra-low power applications		In this paper, a novel combinational access topology of Tunnel FET (TFET) SRAM is proposed for ultra-Low Power applications. Since forward p-i-n current of TFET could cause serious damage to SRAM circuit performance, the proposed topology can avoid the forward bias applied to the p-i-n junction, thus increasing SRAM cell read and hold static noise margin (SNM) and decreasing its static power consumption dramatically. At 0.6 V supply voltage, the combinational access TFET SRAM topology presents 26% hold SNM larger than traditional TFET SRAM topologies, 8 orders of magnitude lower static power consumption, and 2 order of magnitude lower power delay product, demonstrating its great potential for ultra-low power applications.	cell (microprocessor);combinational logic;noise margin;pin diode;programmed data processor;p–n diode;static random-access memory;value-driven design	Libo Yang;Jiadi Zhu;Cheng Chen;Zhixuan Wang;Zexue Liu;Qianqian Huang;Le Ye;Ru Huang	2018	2018 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2018.8351297	static random-access memory;noise (radio);order of magnitude;voltage;electronic engineering;network topology;computer science;power–delay product	Arch	17.413020366793333	59.06147878057997	42095
7c47007c12b7f08c9b77fb7b905c7bf41ba4b865	efficient fpga implementation of sharp fir filters using the frm technique	field programmable gate array;systolic array;frequency response masking technique;fpga implementation;fir filter	A high-performance field programmable gate array (FPGA) implementation of full pipelined computation structure is proposed for sharp finite-impulse -response (FIR) filters using the frequency response masking (FRM) technique. The FRM-based FIR (FFIR) filter consists of a novel symmetrical systolic array of a interpolated FIR (IFIR) filter in cascade to a pair of nonsymmetrical systolic arrays of masking FIR filters mainly. These filters are designed based on innerproduct computation involving MAC operation which can be realized by the DSP block in the latest FPGA device efficiently. The realization results on a Xilinx Virtex-5 chip show that the proposed FPGA implementation can obtain higher throughput but consumes less resource compared to the equivalent conventional sharp FIR (CSFIR) filter that developed by the Core Generator software tool.	computation;digital signal processor;field-programmable gate array;finite impulse response;frequency response;interpolation;programming tool;systolic array;throughput	Shuguo Li;Jian Zhang	2009	IEICE Electronic Express	10.1587/elex.6.1656	embedded system;electronic engineering;computer hardware;systolic array;computer science;engineering;finite impulse response;field-programmable gate array	EDA	12.587642043321903	43.287915932591154	42128
146e82b64eda1d39f17143405d93a92ecd5d8d01	space-time trade-off optimization for a class of electronic structure calculations	programa paralelo;arithmetic operation;fiabilidad;reliability;general and miscellaneous mathematics computing and information science;coupled cluster;electronic structure calculation;operation arithmetique;computability;boucle programme;structure electronique;supercomputer;space time;calculation methods loop fusion loop transformation tile size selection;bucle programa;estructura electronica;loop fusion;supercomputador;loop transformation;tile size selection;fiabilite;compromis;calculabilite;computerized simulation;program loop;algorithms;electronic structure;trade off;operation aritmetica;parallel program;quantum chemistry;superordinateur;calculabilidad;programme parallele;pacific northwest	The accurate modeling of the electronic structure of atoms and molecules is very computationally intensive. Many models of electronic structure, such as the Coupled Cluster approach, involve collections of tensor contractions. There are usually a large number of alternative ways of implementing the tensor contractions, representing different trade-offs between the space required for temporary intermediates and the total number of arithmetic operations. In this paper, we present an algorithm that starts with an operation-minimal form of the computation and systematically explores the possible space-time trade-offs to identify the form with lowest cost that fits within a specified memory limit. Its utility is demonstrated by applying it to a computation representative of a component in the CCSD(T) formulation in the NWChem quantum chemistry suite from Pacific Northwest National Laboratory.	algorithm;computation;contraction mapping;coupled cluster;electronic structure;fits;nwchem	Daniel Cociorva;Gerald Baumgartner;Chi-Chung Lam;P. Sadayappan;J. Ramanujam;Marcel Nooijen;David E. Bernholdt;Robert J. Harrison	2002		10.1145/512529.512551	coupled cluster;loop fusion;supercomputer;simulation;trade-off;computer science;theoretical computer science;space time;reliability;computability;quantum chemistry;algorithm;electronic structure	PL	-4.417613806410577	36.99314221229935	42141
5760e7e5a83e32785259856539dfd30d1a5f70fc	instruction design to minimize program size	program size;instruction design;instructional design	With the introduction of dynamic microprogrammer computers, it becomes practical to reconfigure the architecture of a computer to more efficiently represent programs. A number of methods have been proposed and investigated for reducing the redundancy of computer op codes, address and data items. The greatest gains have come from frequency based encoding. Wilner 9 reports 40 to 70 percent reduction of program size using frequency based encoding for instructions and addresses combined with tailored instruction sets. This paper examines the effect on program size of various architectural features assuming frequency based encoding. It is primarily oriented toward the reduction of program memory by selection of instructions and features to microcode, but applicable to the structure of the underlying micro machine and the compressed storage of any type of symbol string.	computer;microcode;qr code;read-only memory;redundancy (engineering)	James F. Wade;Paul D. Stigall	1974		10.1145/642089.642097	computer science	Arch	6.479244837180587	45.919952914615045	42237
b4d17bc316a489dcaaa719701849ad8047ef4b2c	a new combinatorial approach to optimal embeddings of rectangles	parallel computer;compression ratio;parallel machines;graph embedding;vlsi layout;doubly stochastic matrix	An important problem in graph embeddings and parallel computing is to embed a rectangular grid into other graphs. We present a novel, general, combinatorial approach to (one-to-one) embedding rectangular grids into their ideal rectangular grids and optimal hypercubes. In contrast to earlier approaches of Aleliunas and Rosenberg, and Ellis, our approach is based on a special kind of doubly stochastic matrix. We prove that any rectangular grid can be embedded into its ideal rectangular grid with dilation equal to the ceiling of the compression ratio, which is bothoptimal up to a multiplicative constant and a substantial generalization of previous work. We also show that any rectangular grid can be embedded into its nearly ideal square grid with dilation at most 3. Finally, we show that any rectangular grid can be embedded into itsoptimal hypercube withoptimal dilation 2, a result previously obtained, after much research, through anad hoc approach. Our results also imply optimal simulations of two-dimensional mesh-connected parallel machines by hypercubes and mesh-connected machines, where each processor in the guest machine is simulated by exactly one processor in the host.	bruce ellis;dilation (morphology);doubly stochastic model;embedded system;hoc (programming language);one-to-one (data model);parallel computing;regular grid;simulation;square tiling;stochastic matrix	Shou-Hsuan Stephen Huang;Hongfei Liu;Rakesh M. Verma	1996	Algorithmica	10.1007/BF01940645	mathematical optimization;combinatorics;discrete mathematics;graph embedding;doubly stochastic matrix;compression ratio;mathematics	HPC	23.440163474079778	35.691109277841775	42242
64f3602344154dae208ba80f675f06a604e62860	an experimental perspective for computation-efficient neural networks training		Nowadays, as the tremendous requirements of computationefficient neural networks to deploy deep learning models on inexpensive and broadly-used devices, many lightweight networks have been presented, such as MobileNet series, ShuffleNet, etc. The computationefficient models are specifically designed for very limited computational budget, e.g., 10–150 MFLOPs, and can run efficiently on ARM-based devices. These models have smaller CMR than the large networks, such as VGG, ResNet, Inception, etc. However, it is quite efficient for inference on ARM, how about inference or training on GPU? Unfortunately, compact models usually cannot make full utilization of GPU, though it is fast for its small size. In this paper, we will present a series of extensive experiments on the training of compact models, including training on single host, with GPU and CPU, and distributed environment. Then we give some analysis and suggestions on the training.	arm architecture;artificial neural network;central processing unit;computation;computer cluster;deep learning;experiment;graphics processing unit;memory bound function;requirement	Lujia Yin;Xiaotao Chen;Zheng Qin;Zhaoning Zhang;Jinghua Feng;Dongsheng Li	2018		10.1007/978-981-13-2423-9_13	flops;deep learning;artificial neural network;computation;computer architecture;residual neural network;artificial intelligence;computer science	Security	2.0072681831210004	42.76070164501445	42265
d43a43db386939d1dc84064e05df0ac163730f29	investigation of process impact on soft error susceptibility of nanometric srams using a compact critical charge model	cmos memory circuits;sram chips;integrated circuit modelling;nanoelectronics;radiation hardening (electronics);6t sram cell;cmos process;mim capacitors;spice simulations;cell supply voltage;compact critical charge model;decoupling technique;injected current parameters;manufacturing defects;nanometric sram;nonlinearly coupled storage nodes;packing density;particle induced soft error;process impact;process variations;resistive contacts;size 90 nm;soft error susceptibility;transistor parameters;sram;soft error;critical charge;process variation	Nanometric SRAMs are more vulnerable to experiencing particle induced soft error due to lower operating voltages coupled with higher packing density and increased process variations. In this paper, we present a compact model for critical charge of a 6T SRAM cell for estimating the effects of process variations on its soft error susceptibility. The model is based on dynamic behavior of the cell and a simple decoupling technique for the non-linearly coupled storage nodes. The model describes the critical charge in terms of transistor parameters, cell supply voltage, and injected current parameters. Consequently, it enables investigating the spread of critical charge due to process induced variations in these parameters and to manufacturing defects, such as, resistive contacts or vias. In addition, the model can estimate the improvement in critical charge when MIM capacitors are added to the cell in order to improve the soft error robustness. The critical charge calculated by the model is in good agreement with SPICE simulations for a commercial 90 nm CMOS process with a maximum discrepancy of less than 5%.	cmos;cell (microprocessor);coupling (computer programming);discrepancy function;spice;set packing;simulation;soft error;transistor model;via (electronics)	Shah M. Jahinuzzaman;Mohammad Sharifkhani;Manoj Sachdev	2008	9th International Symposium on Quality Electronic Design (isqed 2008)	10.1109/ISQED.2008.48	nanoelectronics;embedded system;electronic engineering;static random-access memory;soft error;computer science;electrical engineering;nanotechnology;process variation;sphere packing	EDA	20.237206193303983	58.855542197496895	42284
f299324b5afbd11a44525e931a979ac5bf1b0c01	a write-back-free 2t1d embedded dram with local voltage sensing and a dual-row-access low power mode	amplifiers;dram chips amplifiers cmos memory circuits;cmos memory circuits;memory size 64 kbyte write back free 2t1d embedded dram local voltage sensing dual row access low power mode gain cell embedded dram edram lp process random read access frequency write back operation read bitline swing short local bitline local voltage sense amplifier low overhead dual row access mode cmos size 65 nm frequency 1 ghz;computer architecture random access memory microprocessors sensors couplings timing transistors;dram chips	A gain cell embedded DRAM (eDRAM) in a 65nm LP process achieves a 1.0 GHz random read access frequency by eliminating the write-back operation. The read bitline swing of the 2T1D cell is improved by employing short local bitlines connected to local voltage sense amplifiers. A low-overhead dual-row access mode improves the worst-case cell retention time by 3X, minimizing refresh power at times when only a fraction of the entire memory is utilized. Measurement results from a 64kb eDRAM test chip in 65nm CMOS demonstrate the effectiveness of the proposed circuit techniques.	amplifier;best, worst and average case;cmos;cache (computing);dynamic random-access memory;edram;embedded system;experiment;overhead (computing);random access	Wei Zhang;Ki Chul Chun;Chris H. Kim	2012	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/CICC.2012.6330623	electronic engineering;parallel computing;real-time computing;memory rank;sense amplifier;memory refresh;computer science;engineering;electrical engineering;amplifier	EDA	17.262647520457772	59.001378665894386	42365
4b8907e99f94f0b84be9edd5758d2e3dd92846ef	on the use of a gpu-accelerated mobile device processor for sound source localization		Abstract The growing interest to incorporate new features into mobile devices has increased the number of signal processing applications running over processors designed for mobile computing. A challenging signal processing field is acoustic source localization, which is attractive for applications such as automatic camera steering systems, human-machine interfaces, video gaming or audio surveillance. In this context, the emergence of systems-on-chip (SoC) that contain a small graphics accelerator (or GPU), contributes a notable increment of the computational capacity while partially retaining the appealing low-power consumption of embedded systems. This is the case, for example, of the Samsung Exynos 5422 SoC that includes a Mali-T628 MP6 GPU. This work evaluates an OpenCL-based implementation of a method for sound source localization, namely, the Steered-Response Power with Phase Transform (SRP-PHAT) algorithm, on GPUs of this type. The results show that the proposed implementation, given the audio samples, is able to perform audio localization in real time with high-resolution spatial grids using up to 12 microphones.	graphics processing unit;mobile device	Jose A. Belloch;José M. Badía;Francisco D. Igual;Maximo Cobos;Enrique S. Quintana-Ortí	2017		10.1016/j.procs.2017.05.037	embedded system;real-time computing;computer science;audio signal processing;signal processing;acoustic source localization;mobile device;mobile computing	Arch	2.2165182265102814	47.55292048035555	42384
71abef75ad423c6c94d5c7e53343b760506476e7	advanced dma controller for 16-bit microcomputer systems		Abstract   The features of a DMA device designed for use with 286, 186, 086 and 088 Intel-type buses are given. The SAB 82258 is also an I/O controller. The principles of data transfer in various modes are detailed, with some examples of data transfer which may be controlled or modified to suit various environments. Methods of interfacing with a CPU are outlined. This includes programming and the use of the multiplexer channel.	16-bit;channel i/o;direct memory access;microcomputer	Werner Boening	1984	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(84)90436-8	embedded system;parallel computing;real-time computing;computer hardware;computer science;operating system	EDA	6.520766977829348	48.782801183683844	42390
aa979c6a3ea328c9f209b5bfc6c0dbf5b793fa1b	combining advanced process technology and design for systems level integration	libraries;silicon;design automation;gdsii algorithms;design process;spice modelling;margin analysis;prototypes;design flow;unit process quality;process module quality;process integration;process design;integrated circuit design;manufacturing processes;statistical analysis;library elements;integrated circuit modelling;cmp planarization;semiconductor device modeling;computer aided manufacturing;semiconductor process modelling integrated circuit design integrated circuit modelling spice circuit cad statistical analysis;advanced process design;semiconductor process modelling;process specific post layout features;statistical capability measurement;circuit cad;cad flow;planarization;process characterisation;systems level integration;spice;process design libraries semiconductor device modeling spice planarization design automation computer aided manufacturing manufacturing processes silicon prototypes;advanced process technology;process characterisation advanced process technology advanced process design systems level integration unit process quality process module quality margin analysis statistical capability measurement gdsii algorithms spice modelling process specific post layout features cmp planarization cad flow library elements	Recent advances in process and integration are enabling systems level integration for numerous applications. The quality of the systems depends directly on the quality of the processes and effectiveness of the process integration, and on the quality of the designs and libraries employed, as well as on the completeness and accuracy of the models used to link the process and designs. Unit process and process module quality is ensured through the use of designed experimentation, margin analysis, and statistical capability measurement. The link between the processes and the libraries and designs is formed through such models as SPICE and interconnect, with quality implications associated with the extraction and implementation. GDSII algorithms to incorporate process specific post layout features such as OPC and fill patterns for CMP planarization are integrated into the CAD flow prior to final verification, reticle manufacturing and silicon prototyping. Foundry specific challenges in providing process and library elements include multiple design flows, tool providers and library suppliers. Examples of approaches to quality designs, processes and systems are presented using advanced cores and systems level integration.		Ana Hunter;C. K. Lau;John Martin	2000		10.1109/ISQED.2000.838879	process design;electronic engineering;semiconductor device modeling;design process;electronic design automation;systems engineering;engineering;design flow;prototype;silicon;chemical-mechanical planarization;process integration;manufacturing engineering;computer-aided manufacturing;integrated circuit design	EDA	23.826837628958295	56.231967399660206	42460
0f3ecfd45abdba23112a8caa66a765cab56435d2	timing-driven partitioning for two-phase domino and mixed static/domino implementations	logic network;logic cad;high-performance circuit configuration;timing-driven partitioning;logic network partitioning;domino implementation;domino logic;logic partitioning;timing;static logic environment;static mapping algorithm;clocks;two-phase clock;clocking scheme;timing-driven partitioning algorithm;two-phase domino;efficient static mapping algorithm;mixed static/domino implementation;two-phase domino implementation	Domino logic is a high-performance circuit configuration that is usually embedded in static logic environment and tightly coupled with the clocking scheme. In this paper, the timing-driven partitioning algorithms that partition a logic network between (1) static and domino implementations, and (2) the phases of a two-phase clock, are provided. In addition, an efficient static mapping algorithm is described.	algorithm;clock rate;clock signal;domino logic;dynamic logic (digital electronics);embedded system;two-phase commit protocol	Min Zhao;Sachin S. Sapatnekar	1999	1999 IEEE/ACM International Conference on Computer-Aided Design. Digest of Technical Papers (Cat. No.99CH37051)		computer architecture;parallel computing;logic synthesis;real-time computing;logic optimization;logic level;asynchronous circuit;logic gate;logic family;computer science;programmable logic device;pass transistor logic;sequential logic;digital electronics;static timing analysis;register-transfer level	EDA	16.79037091811545	50.057442514574596	42491
6432fbf9a286a54d13274b2aee10dbfcf7f9dfbb	timing error prediction avfs with detection window tuning for wide-operating-range ics		In-situ timing monitoring-based adaptive voltage frequency scaling (AVFS) is effective in eliminating the timing margin. However, for wide-operating-range applications, one big challenge is that severe process, voltage, and temperature variations causes varying need for detection window. Therefore, a timing error prediction based AVFS method is proposed with on-chip detection window tuning to solve this problem. In addition, to cope with the area overhead problem, an activation-oriented monitoring paths selection method is proposed to decrease the insertion rate. Implemented on a 45-nm CMOS Bitcoin miner chip working across 0.6 V–1.1 V, it increases the throughput by 190.8%, 83.3%, 67.9%, and 25.4% at 0.6 V, 0.72 V, 0.9 V, and 1.1 V, respectively, for a typical die at 25 °C, compared to baseline frequencies. Measurement results demonstrate that our AVFS system is suitable for wide-operating-range applications.	baseline (configuration management);bitcoin;cmos;frequency scaling;image scaling;kerrison predictor;overhead (computing);real-time clock;requirement;throughput;value-driven design;window function	Weiwei Shan;Xinchao Shang;Longxing Shi;Wentao Dai;Jun Yang	2018	IEEE Transactions on Circuits and Systems II: Express Briefs	10.1109/TCSII.2017.2735445	throughput;chip;system on a chip;voltage;frequency scaling;mathematics;electronic engineering;timing margin;real-time computing;cmos	EDA	19.044281624957726	58.88752036396344	42495
9e5d6ac904b44bd06365c40dfb99aab29daf79d3	multi-bit non-volatile spintronic flip-flop		As leakage increases proportionally with the technology downscaling, it becomes extremely challenging to manage to meet the total power budget. This is because, CMOS-based logic blocks can not be completely power-gated as their flip-flops always require a retention supply to hold the system states. Alternatively, their data can be stored in a separate memory during the standby mode, however, that results in a huge area and energy overhead. Spin Transfer Torque (STT) based nonvolatile flip-flops can offer normally-off/instant-on computing features to reduce leakage by complete power shut-down without the need to transfer and restore system states separately. The non-volatile component of such flip-flops can be easily shared for the overall design optimizations. In this paper, we design a unique multi-bit non-volatile flip-flop architecture using STT devices to reduce the area and energy costs associated with nonvolatile components. This architecture is developed based on the resource sharing principle using a custom design that enables the optimization for the area and energy consumption. Moreover, we have developed a framework in which we have replaced the conventional neighbor flipflops in the layout with our proposed multi-bit non-volatile designs. Results show that using our multi-bit flip-flop architecture, we improve the system-level area and energy by 26% and 14% in average, respectively, compared to the standard single-bit non-volatile flip-flop design.	ambiguous name resolution;cmos;downscaling;flops;flip-flop (electronics);instant-on;mathematical optimization;non-volatile memory;overhead (computing);physical design (electronics);sleep mode;spectral leakage;spintronics	Christopher Munch;Rajendra Bishnoi;Mehdi Baradaran Tahoori	2018	2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)	10.23919/DATE.2018.8342203	computer science;real-time computing;computer hardware;leakage (electronics);standby power;non-volatile memory;power budget;energy consumption;flip-flop;shared resource;cmos	EDA	15.987451211142806	57.38192926980011	42519
eb760f6d01de2c00bb75026e6a393d5bc600e878	implementing vrtx on the series 32000		VRTX (versatile realtime executive) is a multitasking realtime kernel for embedded microprocessor applications. Production versions of VRTX for use with the series 32000 microprocessors are now available. VRTX components reside in PROMs installed in the target system. The executive can be used alone or in combination with other components to build a more complete operating system. This paper explains the facilities of VRTX-in particular task scheduling, intertask communication and services supporting I/(9 and interrupts. Configuring the executive in a system environment is discussed.	computer multitasking;embedded system;environment variable;interrupt;microprocessor;operating system;poweredge vrtx;programmable read-only memory;scheduling (computing)	Guenther Hausmann	1987	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(87)90527-8	embedded system;parallel computing;real-time computing;operating system	Embedded	-2.3808448429684717	48.44275596022727	42626
704024955e8e7a4985b7862b5a42affe038cd84e	power supply noise aware workload assignment for multi-core systems	integrated circuit design;integrated circuit reliability;microprocessor chips;power supply circuits;design reliability;multicore processor;multicore system;power supply noise aware workload assignment;single processor	As the industry moves from single- to multi-core processors, the challenges of how to reliably design and analyze power delivery for such systems also arise. We study various workload assignments to cores and their impact on the global power grid noise. We develop metrics to estimate the amount of noise propagated from core to core and propose a power supply noise aware workload assignment method. In our experiments, we show that performance loss can be significant if workload assignment is not properly made.	algorithm;assignment (computer science);central processing unit;experiment;ibm notes;multi-core processor;power supply;software propagation	Aida Todri;Malgorzata Marek-Sadowska;Joseph N. Kozhaya	2008	2008 IEEE/ACM International Conference on Computer-Aided Design	10.1145/1509456.1509534	multi-core processor;embedded system;electronic engineering;real-time computing;computer science;electrical engineering;operating system	EDA	20.339918570051708	57.81707471151571	42696
5ea1f6ffe2a3fa9244e30471e22efd14373da4af	architecture exploration based on ga-pso optimization, ann modeling, and static scheduling	analytical model;dsp application;cycle-accurate simulation;design space;static scheduling;ga-pso optimization;ann modeling;different architecture;high computation power;architecture exploration framework;commercial embedded processor;different digital signal processing;embedded system	Embedded systems are widely used today in different digital signal processing (DSP) applications that usually require high computation power and tight constraints. The design space to be explored depends on the application domain and the target platform. A tool that helps explore different architectures is required to design such an efficient system. This paper proposes an architecture exploration framework for DSP applications based on Particle Swarm Optimization (PSO) and genetic algorithms (GA) techniques that can handle multiobjective optimization problems with several hybrid forms. A novel approach for performance evaluation of embedded systems is also presented. Several cycle-accurate simulations are performed for commercial embedded processors. These simulation results are used to build an artificial neural network (ANN) model that can predict performance/power of newly generated architectures with an accuracy of 90% compared to cycle-accurate simulations with a very significant time saving. These models are combined with an analytical model and static scheduler to further increase the accuracy of the estimation process. The functionality of the framework is verified based on benchmarks provided by our industrial partner ON Semiconductor to illustrate the ability of the framework to investigate the design space.	genetic algorithm;particle swarm optimization;scheduling (computing)	Ahmed Elhossini;Shawki Areibi;Robert D. Dony	2013	VLSI Design	10.1155/2013/624369	embedded system;electronic engineering;real-time computing;simulation;engineering;operating system	EDA	1.0782934486681406	55.53513156059869	42709
7bab77de88a1ee3ac7daf131e94b51fcebb37389	design-space exploration of low power coarse grained reconfigurable datapath array architectures	reconfiguration;concepcion asistida;computer aided design;concepcion circuito;estimacion;reconfiguracion;arquitectura circuito;comparative analysis;interconnection;integrated circuit;dissipation energie;circuit design;circuit architecture;circuito integrado;procesador panel;energy dissipation;array processor;interconexion;processeur tableau;reconfigurable architecture;low power;estimation;power dissipation;interconnexion;mobile communication;architecture circuit;puissance faible;conception assistee;disipacion energia;conception circuit;design space exploration;coarse grained;circuit integre;potencia debil	Coarse-grain reconfigurable architectures promise to be more adequate for computational tasks due to their better efficiency and higher speed. Since the coarse granularity implies also a reduction of flexibility, a universal architecture seems to be hardly feasible, especially under consideration of low power applications like mobile communication. Based on the KressArray architecture family, a design-space exploration system is being implemented, which supports the designer in finding an appropriate architecture featuring an optimized performance / power trade-off for a given application domain. By comparative analysis of the results of a number of different experimental application-to-array mappings, the explorer system derives architectural suggestions. This paper proposes the application of the exploration approach for low power KressArrays. Hereby, both the interconnect power dissipation and the operator activity is taken into account.	application domain;datapath;qualitative comparative analysis;reconfigurable computing;systolic array	Reiner W. Hartenstein;Thomas Hoffmann;Ulrich Nageldinger	2000		10.1007/3-540-45373-3_12	embedded system;real-time computing;telecommunications;computer science;engineering;dissipation;computer aided design;thermodynamics	EDA	13.348440008688504	57.26710107716044	42717
e5976ec9eff1496ce3e22325ed0a80300dfb4115	design of residue generators with cla/compressor trees and multi-bit eac		We propose a new approach to designing residue generators for an arbitrary moduli. Its novelty is that the carry-out bit is fed as multi-bit word back to the adder tree, while using reduction algorithms similar to the concepts of the compressor trees. For any modulus, reduction of multiple operands can be done using a carry-save adder (CSA) tree down to two vectors, which are then handled by an arbitrary 2-operand modular adder. The compressor trees built using full-adders and 5:3 compressors are analyzed. The number of reduction stages is similar to the positional CSA tree (without any carry-out values fed back) followed by the reduction circuit. The RNS applications of the proposed circuits include forward- and reverse converters as well as constant- and variable multipliers.	adder (electronics);algorithm;booth's multiplication algorithm;carry-save adder;global optimization;lossy compression;mathematical optimization;modulo operation;modulus robot;operand;residue number system	Piotr Patronik;Stanislaw J. Piestrak	2017	2017 IEEE 8th Latin American Symposium on Circuits & Systems (LASCAS)	10.1109/LASCAS.2017.7948058	operand;electronic engineering;gas compressor;algorithm design;residue (complex analysis);electronic circuit;serial binary adder;adder;modular design;mathematics	Logic	15.986081161980067	44.088749167823856	42728
785c4a48b765179ab79c9bd8f907ef2f2d81e8c9	the segbus platform - architecture and communication mechanisms	fpga;platform design;chip;performance improvement;system on chip;segmented bus;multiple clock domain;power consumption;on chip communication	In this study, we introduce the SegBus architecture, a synchronous segmented bus platform for systems on chip. We present the envisioned structure in detail, and also address aspects of communication on the platform. The motivation behind SegBus is the search for performance improvements, in several directions, such as global throughput, power consumption, modularity, adaptability. By means of an example, we illustrate the capabilities of the described architecture. The implementation strategy targets FPGA technology, and allows for the utilization of multiple clock domains. The platform emerges as a highly design-time configurable system, adaptable to various design constraints. 2006 Elsevier B.V. All rights reserved.	application-specific integrated circuit;arbiter (electronics);emergence;field-programmable gate array;kernel (operating system);mathematical optimization;network on a chip;operating system;simulation;stan;switch;system bus;system on a chip;throughput	Tiberiu Seceleanu	2007	Journal of Systems Architecture	10.1016/j.sysarc.2006.07.002	chip;system on a chip;embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;field-programmable gate array	EDA	1.4805522373552251	50.04062650169581	42763
83b808198a264514eb5bc65e35ebf28702c2a26f	data-flow aware cnn accelerator with hybrid wireless interconnection		Deep convolution neural networks (CNNs) are computationally intensive machine learning algorithms with a large amount of data that impose various challenges for their hardware implementation. To meet the high computing demands of CNNs, many accelerator designs are proposed that revolve around achieving high parallelization, increasing on-chip data reuse and efficient memory hierarchy, etc. However, very few works have attempted to address the communication challenges in these massively parallel accelerators architectures, which is the most anticipated performance bottleneck. Traditional interconnections like bus, crossbar and even Network-on-Chip ($N$ o $C$) topologies like mesh fail to achieve the peak performance required by the large number of processing elements on accelerators. In this work, we address the communication bottlenecks of accelerators by extensively studying the application data-flow. We propose an efficient accelerator architecture that employs broadcast enabled low latency wireless links along with traditional wired links to efficiently support the data-flow of accelerators and achieve high communication performance. Evaluation of the proposed design shows that it achieves 28 % latency reduction, 19x bandwidth improvement and 35% network energy saving as compared to baseline wired networks.	algorithm;artificial neural network;baseline (configuration management);convolution;convolutional neural network;crossbar switch;dataflow;interconnection;machine learning;memory hierarchy;network on a chip;parallel computing;scalability	Mitali Sinha;Gade Narayana Sri Harsha;Wazir Singh;Sujay Deb	2018	2018 IEEE 29th International Conference on Application-specific Systems, Architectures and Processors (ASAP)	10.1109/ASAP.2018.8445126	latency (engineering);massively parallel;computer science;parallel computing;artificial neural network;network topology;latency (engineering);architecture;memory hierarchy;bottleneck	Arch	3.422164317018151	43.59478229189155	42785
06ab54c9b10f86a0755fff8fe741a4606ba64a01	a run-time reconfigurable network-on-chip for streaming dsp applications	limiting factor;network on chip;mobile phone;chip;run time reconfigurable;system on chip	With the advance of semiconductor technology, global on-chip wiring is becoming a limiting factor for the overall performance of large System-on-Chip (SoC) designs. In this thesis we propose a global communication architecture that avoids this limitation byrnstructuring and shortening of the global wires. The communication architecture is used in a multiprocessor SoC for streaming DSP applications. The SoC is intended as a platform for wireless multimedia devices, such as PDAs, mobile phones, mobile medical systems, car infotainment systems, etc.	digital signal processor;network on a chip	Nikolay Kavaldjiev	2007			chip;system on a chip;limiting factor;network on a chip	EDA	3.3407594519691073	54.25147105857714	42897
64cade319bb27b4188787870c40081b8be22ae04	current-mode cmos galois field circuits	many valued logics;logic circuits;current mode;galois fields cmos logic circuits polynomials voltage detectors switches application software current mode circuits circuit synthesis signal synthesis;many valued logics logic circuits;galois field;mvl circuits current mode cmos circuits multiple valued logic polynomial representations galois fields;multiple valued logic	Use of current-mode CMOS circuits for implementation of multiple-valued logic (MVL) functions has been considered in a number of recent papers. In this paper, we present an application of these circuits in realization of Galois field operations. We also give a new algorithm for determination of polynomial representations for arbitrary functions over a class of Galois fields implementable with presently available MVL circuits. 1.0 Current-Mode MVL Functions Recent experience shows that current-mode circuits are attractive for implementation of MVL functions, particularly when the radix is greater than 3. Most of the circuits and synthesis techniques in the literature have been intended for the 4-valued environment [2], [4], [5]. Current-mode circuits offer several advantages, but they also have some disadvantages. Perhaps the most important of these are the ease of summation of signals and the difficulty in distribution of signals caused by the fanout being equal to one. In practice, it is useful to augment the current-mode circuits with some intermediate voltage-mode circuits, which often results in more effective designs. Figure 1 gives the basic blocks used in our design. All of these blocks have been used before [2], [4], [5]. Here, we will only summarize their characteristics. Currents are summed by means of a simple wired connection. Current sources are realized as an N-type or a Ptype transistor with the gate connected to a reference voltage. The amount (value) of current is proportional to the ratio of W/L. Signal distribution can be performed using current mirrors, which can be of either N-type or P-type. We will use both types. The current produced at the output is determined by the input current and the ratio between output and input W/L values. In addition to signal distribution, the current mirror will be used for multiplication with a constant and for sign reversal. FIGURE 1. Basic Current Mode Blocks A threshold detector block takes a multiple-valued input current signal and produces a voltage signal. The output signal is High if the input exceeds a predefined reference value; otherwise the output signal is Low. Threshold detectors generate signals that can be used with normal binary logic gates and for control of pass-transistor networks. A pass transistor is a voltage-controlled device, which acts as a switch, depending on the gate voltage. We will use both N and P-type pass transistors. Name Log. Expression Symbol Circuit Realization	algorithm;basic block;cmos;comparator applications;current mirror;current source;fan-out;logic gate;network switch;polynomial;sensor;transistor;transistor–transistor logic	Zeljko Zilic;Zvonko G. Vranesic	1993		10.1109/ISMVL.1993.289552	galois theory;electronic engineering;discrete mathematics;logic level;logic gate;logic family;theoretical computer science;pass transistor logic;mathematics;integrated injection logic;algebra	EDA	18.73227754336235	44.68111843738588	42905
1b7062cf21f76b5b583812b93a86d6f560bb6ab7	a hybrid power estimation technique to improve ip power models quality	computational modeling;hybrid power systems;logic gates;estimation;integrated circuit modeling;switches;power demand	Nowadays, power consumption is the one key factor that hinders System-on-Chip (SoC) performance. In order to reduce the power consumption, accurate and efficient power models have to be introduced early in the design flow, when most of the optimization potential is obtained. However, early accuracy cannot be ensured because of the lack of precise knowledge of the circuit structure. Current SoC design paradigm relies on Intellectual Property (IP) reuse, and low-level information about circuit components and structure is usually available. Thus, if we use this information and develop an estimation methodology that fits IP power modeling needs, the estimation accuracy at system level will be improved. This paper presents a Hybrid Power Estimation Technique (HPET). It is based on an effective library characterization methodology and an efficient hybrid power modeling approach to accurately and quickly assess gate-level power consumption. The aim is to give valuable and accurate physical information to design teams so they can ensure that the correct optimization techniques are implemented. Our approach can be used to compute both realistic instantaneous power and average power on a single simulation time. We performed experiments on different benchmark circuits synthesized using the 28nm FDSOI technology. To validate the proposed technique, we correlated our results with SPECTRE and PrimeTime-PX simulations. Our results showed that we can achieve up to 144× speedup on the simulation runtime with a mean error of about 6% and 13% for the instantaneous and average power components respectively.	benchmark (computing);combinational logic;design flow (eda);die shrink;experiment;fits;high precision event timer;high- and low-level;logic simulation;mpsoc;mathematical optimization;physical information;pixel;programming paradigm;semiconductor intellectual property core;simulation;speedup;system on a chip;transistor	Alejandro Nocua;Arnaud Virazel;Alberto Bosio;Patrick Girard;Cyril Chevalier	2016	2016 IFIP/IEEE International Conference on Very Large Scale Integration (VLSI-SoC)	10.1109/VLSI-SoC.2016.7753582	electronic engineering;real-time computing;simulation;engineering;power optimization	EDA	20.977785262336983	57.59965806904618	42926
1107e5a90fbfafb7987f26a502a0543c619662c0	hardware implementation of distributed speech recognition system front end	speech recognition embedded systems field programmable gate arrays;hardware speech recognition application software telecommunication standards embedded system computer architecture field programmable gate arrays software prototyping prototypes application specific integrated circuits;third party reference designs distributed speech recognition system front end embedded systems handheld devices etsi aurora standard etsi es 201 108 fpga platform prototype structured asic mass production	Modern speech recognition applications are heading towards embedded systems and hand-held devices. Distributed speech recognition (DSR) system architecture emerged to address this kind of applications. Most of the existing implementations of this system are presented in software fashion, with little consideration to the end product platform in which the system will be deployed. In this paper, an optimized hardware implementation of the front end part of the DSR specified in the basic ETSI Aurora standard ETSI ES 201 108 is presented in FPGA platform prototype, with consideration of migration to structured ASIC in case of mass-production. Main design issues and tips are highlighted. Results are presented in terms of hardware resources utilization, comparison of some basic system components to third party reference designs and compliance to the Aurora standard.	ansi escape code;algorithm;application-specific integrated circuit;aurora;cordic;course (navigation);cyclone;embedded system;fast fourier transform;field-programmable gate array;fixed point (mathematics);mobile device;prototype;reference architecture;reference implementation;significant figures;speech recognition;standard streams;systems architecture;technical standard	Ahmad A. Al Sallab;Hossam Fahmy;Mohsen Rashwan	2009	2009 4th International Design and Test Workshop (IDT)	10.1109/IDT.2009.5404138	embedded system;electronic engineering;real-time computing;engineering	EDA	6.014550268069206	47.917134519179584	42946
92fc65c6cb32a2e6459be6f328d41c9702348631	energy efficient multi-player smartphone gaming using 3d spatial subdivisioning and pvs techniques	energy;mobile;wireless;game;smartphone;power	With the advent of feature rich smartphone platforms such as Android and iOS, people can now enjoy a wide variety of applications on-the-go. Among these applications, games are one of the most desired types. However, as bigger screens, faster CPUs and interfaces supporting higher bandwidth (WiFi, 3G, LTE) consume more power, battery lifetime becomes a bottleneck on such devices.  In this paper, we present novel techniques that combine 3D spatial subdivisioning, Potentially Visible Set (PVS) and Visual Perception based Localisation (VPL) methods to estimate the non-critical game states to save wireless interface energy with minimum processing penalty. Our techniques and algorithms are realised in a commercial game and can save up to 57% of wireless interface energy (which is, about 22.8% overall system energy) while retaining game play quality.	algorithm;android;central processing unit;compaq lte;potentially visible set;smartphone;usb on-the-go;vpl research;ios	Anand Bhojan;Zeng Qiang;Akkihebbal L. Ananda	2013		10.1145/2505483.2505491	embedded system;simulation;engineering;computer security	HCI	-1.8811869420049363	59.21652378077563	42968
e04e74de8c044f74e134a80c500d1c576af89064	knowledge-based vlsi routing system - wirex	knowledge base		routing;very-large-scale integration	Hajimu Mori;Keiko Mitsumoto;Tomyyuki Fujita;Satoshi Goto	1984				EDA	9.438926429703448	50.46833782880459	42979
4f3b3706778b8a2c374f1ec5c87901848b96cd61	power minimization of functional units by partially guarded computation	circuit optimisation minimisation of switching nets high level synthesis;minimisation of switching nets;optimal location;partially guarded computation;area overhead power minimization functional units partially guarded computation msp lsp most significant part power consumption least significant part optimal location high level synthesis;high level synthesis;low power;signal processing algorithms power engineering computing high level synthesis power dissipation minimization circuits signal design permission dynamic range speech;power reduction;power consumption;functional unit;circuit optimisation;power minimization	This paper deals with power minimization problem for data-dominated applications based on a novel concept called partially guarded computation. We divide a functional unit into two parts - MSP (Most Significant Part) and LSP (Least Significant Part) - and allow the functional unit to perform only the LSP computation if the range of output data can be covered by LSP. We dynamically disable MSP computation to remove unnecessary transitions thereby reducing power consumption. We also propose a systematic approach for determining optimal location of the boundary between the two parts during high-level synthesis. Experimental results show about 10~44% power reduction with about 30~36% area overhead and less than 3% delay overhead in functional units.	computation;execution unit;high- and low-level;high-level synthesis;max;overhead (computing)	Junghwan Choi;Jinhwan Jeon;Kiyoung Choi	2000		10.1145/344166.344549	electronic engineering;real-time computing;computer science;engineering;high-level synthesis;engineering drawing	EDA	13.723744536247231	51.866142133372335	42993
7bbb1a7e7459dd2b61a510b80a89821384162cbb	composite arithmetic: proposal for a new standard	proposals floating point arithmetic programming profession calculators digital arithmetic hardware scientific computing ansi standards computer graphics coupling circuits;280502 data storage representations;standards;number representation;standard;integer arithmetic;information and communication services;number format general purpose arithmetic standard general computation stability reliability composite arithmetic formatting scheme number storage number display inexact numbers exact numbers extended arithmetic;computer arithmetic;computer software not elsewhere classified;291601 arithmetic and logic structures;information and communication services not elsewhere classified;standards digital arithmetic floating point arithmetic;scientific computing;arithmetic;digital arithmetic;floating point;computer software;230116 numerical analysis;floating point arithmetic;information and computing sciences;rational arithmetic;other information and communication services;symmetric level indexed arithmetic	65 posite arithmetic combines aspects of traditional integer and floating-point arithmetics with less familiar aspects of rational and logarithmic arith-metics to complement the binary floating-point standard and satisfy more diverse computational needs. I describe a formatting scheme for storage and display of exact and inexact numbers and an extended arithmetic with a number format as its basis. I also introduce possibilities for implementing the arithmetic and discuss the interface between the representations and the arithmetic. No matter how varied, computer arithmetics, and the digital forms of the numbers they use, follow a traditional pattern. Fixed-point arithmetic, more correctly called integer arithmetic outside the computing industry, is meant to represent and compute with integers exactly. In integer arithmetic, calculation with fractions is not done directly and must be carried out via subterfuges, such as scaling, which may deliver inexact results. Instead, fractions can be handled by an arithmetic called floating slash, which has been proposed 5 but not widely adopted. A similar scheme is built into the composite arithmetic I propose. Fixed-point arithmetic can handle a relatively limited range of numbers. More than one length of representation is often provided so that the programmer can choose a length to cope with the expected range of numbers. But very soon the numbers become too large to store, a condition called arithmetic overflow , which a program must deal with specially to prevent wrong results. Floating-point arithmetic was designed to circumvent this overflow problem but at the cost of exactness. Floating-point arithmetic can cope with a large range of numbers, but it does so only by approximating. More than one length of representation lets the programmer choose a length that produces suitable precision, yet overflow can still occur, A general-purpose arithmetic standard could give general computation the kind of reliability and stability that the floating-point standard brought to scientific computing. The author describes composite arithmetic as a possible starting point. E ver since their early days, digital computers and their arithmetics have made different kinds of complex computation possible. Frustrated, however, by the limitations of integer arithmetic (originally intended for counting loops and calculating addresses), scientists and engineers developed a floating-point number representation. Floating-point arithmetic, or more properly, arithmetic using scaled numbers, was variously implemented , first in software and later in hardware. Scientific computing was greatly enhanced by the worldwide adoption in 1985 of the ANSI/IEEE standard for binary floating-point arithmetic. 1 …	computation;computational science;computer;fixed-point arithmetic;general-purpose modeling;ieee 754-1985;image scaling;programmer;slash (cms);ver (command)	W. Neville Holmes	1997	IEEE Computer	10.1109/2.573666	arbitrary-precision arithmetic;binary scaling;computer science;floating point;theoretical computer science;operating system;arithmetic circuit complexity;machine epsilon;algorithm	PL	7.86635675132942	42.64739519252384	43000
56575ef0fb10937dfd5c186ad63000fa0d35b457	an evaluation of move-based multi-way partitioning algorithms	partitioning algorithms flexible manufacturing systems simulated annealing algorithm design and analysis very large scale integration silicon graphics design automation application software circuits;vlsi circuit layout cad integrated circuit testing;integrated circuit testing;vlsi;circuit layout cad;vlsi move based multi way partitioning algorithms iscas benchmarks	This paper presents a thorough analytical and experimental comparison of four move-based multi-way partitioning algorithms. Modifications are considered to the algorithm with the best solution quality, partitioning by free moves. These modifications allow a tradeoff to be made between solution quality and execution time. Results are given for ISCAS and other benchmarks.	algorithm;benchmark (computing);run time (program lifecycle phase)	Elie Yarack;Joan E. Carletta	2000		10.1109/ICCD.2000.878309	computer architecture;parallel computing;real-time computing;computer science;electrical engineering;very-large-scale integration	EDA	14.286931019765188	51.654736226458766	43020
114da085698cbeb70d46967165a56e0eeb36b1de	heresy: a hybrid approach to automatic schematic generation (for vlsi)	control systems;design automation;levelization algorithm;logic design;rule based approaches;very large scale integration;automatic schematic generation system;system buses;circuit diagrams;hybrid power systems;computational efficiency automatic schematic generation automatic schematic generation system heresy rule based approaches evaluation criteria levelization algorithm arbitrary cyclic structures;evaluation criteria;hybrid power systems very large scale integration circuit synthesis logic design design automation silicon compiler system buses hardware pattern recognition control systems;pattern recognition;vlsi;arbitrary cyclic structures;circuit cad;vlsi circuit cad circuit diagrams;silicon compiler;computational efficiency;circuit synthesis;heresy;hardware;automatic schematic generation	An automatic schematic generation system called HERESEY is presented, which represents a well-engineered combination of algorithmic and rule-based approaches. Equipped with a set of carefully-chosen evaluation criteria, HERSEY is able to generate high-quality, reasonably general classes of schematic diagrams in an efficient way. A novel levelization algorithm that can detect and resolve arbitrary cyclic structures of a circuit is described. An example schematic generated by HERESY, together with its computational efficiency is also presented.	algorithm;computation;diagram;logic programming;schematic	Tzi-cker Chiueh	1991		10.1109/EDAC.1991.206438	layout versus schematic;computer science;theoretical computer science;engineering drawing;algorithm;schematic	HCI	11.811556069119286	49.68164248720654	43101
eedc4469a4f0b44011bb84e4b287edf628714212	case study: deployment of the 2d noc on 3d for the generation of large emulation platforms	three dimensional integrated circuits field programmable gate arrays network on chip;network on chip;hermes noc noc multi component 3d emulation platform;field programmable gate arrays;field programmable gate arrays emulation routing switches registers;three dimensional integrated circuits;soc network on chip 2d noc 3d noc large emulation platforms system on chip field programmable gate array multi fpga platform 3d technology	The evaluation of Network-On-Chip (NoC) architectures is an up to date problem in the design of System-on-Chip. Emulation on FPGA (Field Programmable Gate Array) is used to cover all possible NoC solutions in a reduced exploration time. Emulation requires multi-FPGA platform as the resources for large NoC is important and cannot be handling by one FPGA. In the same time, SoC community is exploring 3D technology for the next generation of large SoC with 3D NoC, making emulation more complex. This paper presents a case study of the deployment of the 2D NoC structure to 3D. A design flow is proposed for the automatic generation of a NoC targeting 3D on multi-FPGAs. The flow integrates emulation blocks used for the validation and exploration on the NoC. With this automatic aided tool, the designer can evaluate and explore the NoC architecture and extract performances of the NoC regardless of the multi-component platform. One may expect a communication performance improvement using an adapted partitioning of the NoC, as highlighted by the results given in this paper.	3d computer graphics;algorithm;emulator;field-programmable gate array;longest path problem;mpsoc;network on a chip;next-generation network;performance;routing;software deployment;system on a chip	Virginie Fresse;Zhiwei Ge;Junyan Tan;Frédéric Rousseau	2012	2012 23rd IEEE International Symposium on Rapid System Prototyping (RSP)	10.1109/RSP.2012.6380686	embedded system;computer architecture;electronic engineering;computer science;engineering;network on a chip;field-programmable gate array;hardware emulation	EDA	2.865465868123102	59.735162355436216	43102
8a97d180bc214c77a0c5a7c0284ef79cda076bc5	an efficient dct-iv-based ecg compression algorithm and its hardware accelerator design	data compression;decoding;compression algorithm the type iv of discrete cosine transform dct iv electrocardiogram ecg;hardware resource efficient dct iv based ecg compression algorithm hardware accelerator design quality score compressing ratio mit bit arrhythmia database percent rms difference prd mit bih database qs value unified transform kernel ecg signal encoding decoding;electrocardiography;electrocardiography compression algorithms hardware databases transforms algorithm design and analysis encoding;discrete cosine transforms;medical signal processing data compression decoding discrete cosine transforms electrocardiography encoding;encoding;medical signal processing	This paper presents a new efficient DCT-IV-based ECG compression algorithm with a higher Quality Score (QS) and a better Compressing Ratio (CR). The ECG signals sourced from MIT-BIT arrhythmia database with a sampling rate of 360 Hz are employed to be the test patterns for the evaluating the proposed compression algorithm. The simulation results show that the averages of CR, Percent RMS Difference (PRD), and QS are, respectively, 5.267, 0.187, and 28.223 for all 48 lead-V1 patterns of MIT-BIH database. Compared with Lee et al.'s algorithm, the QS value of the proposed method has a great improvement by 25.1%. Additionally, we use DCT-IV to be a unified transform kernel for ECG signal encoding and decoding because the formula of forward DCT-IV is same to its inverse. Also, we realize it to be a compact hardware accelerator with a fewer hardware resources. Therefore, it would be a better choice for realizing the ECG compressor in the future.	bounding interval hierarchy;data compression;dijkstra's algorithm;discrete cosine transform;field-programmable gate array;hardware acceleration;kernel (operating system);mobile device;product requirements document;sampling (signal processing);simulation;test card	Shin-Chi Lai;Wei-Che Chien;Chien-Sheng Lan;Meng-Kun Lee;Ching-Hsing Luo;Sheau-Fang Lei	2013	2013 IEEE International Symposium on Circuits and Systems (ISCAS2013)	10.1109/ISCAS.2013.6572091	data compression;electronic engineering;speech recognition;computer science;theoretical computer science;encoding;statistics	EDA	11.677590917138314	42.396642378478106	43153
52c3ace892d74a902b12121d6014fb80fb6ea4b7	service dependency graph: an efficient model for hardware/software interfaces modeling and generation for soc design	design automation;hardware software interfaces;laser sintering;computer aided instruction;hardware middleware costs design automation time to market laboratories laser sintering mpeg 4 standard computer aided instruction computer interfaces;mpeg 4 standard;service based model;middleware;systems on chip hardware software interfaces interface design automation service based model;time to market;systems on chip;computer interfaces;hardware;interface design automation	Complex systems-on-chip are designed by interconnecting pre-designed hardware (HW) and software (SW) components. During the design cycle, a global model of the SoC may be composed of HW and SW models at different abstraction levels. Designing HW/SW interfaces to interconnect SoC components is a source of design bottlenecks. This paper describes a service-based model enabling systematic design and co-simulation of HW/SW interfaces for SoC design. This model, called Service dependency graph (SDG) allows modeling of complex and application-specific interfaces. We present also a model generator that can automatically build HW/SW interfaces based on service and resource requirements described by the SDG. This approach has been applied successfully on the design of an MPEG-4 encoder. Additionally the SDG seems to be an excellent intermediate representation for the design automation of HW/SW interfaces.	co-simulation;complex systems;computer hardware;encoder;intermediate representation;requirement;shattered world;simulation	Adriano Sarmento;Lobna Kriaa;Arnaud Grasset;Mohamed-Wassim Youssef;Aimen Bouchhima;Frédéric Rousseau;Wander O. Cesário;Ahmed Amine Jerraya	2005	2005 Third IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS'05)	10.1145/1084834.1084900	embedded system;computer architecture;real-time computing;computer science	EDA	4.108101199930626	52.54323532832947	43357
a50db20fda07692a33d4fab6dc369d1a071cdc7d	automated multi-device placement, i/o voltage supply assignment, and pin assignment in circuit board design	design space exploration automated multidevice placement i o voltage supply assignment embedded systems multiple field programmable gate arrays printed circuit board design computer aided design tools fast pcb design framework rapid prototyping short production run pcbs freely available open source framework fpga pin assignments trace length minimization trace length estimations;printed circuit design circuit cad field programmable gate arrays;printed circuit design;rapid prototyping printed circuit board design capture pin assignment field programmable gate array high level specification computer aided design electronic design automation;pins field programmable gate arrays layout design automation wires joining processes simulated annealing;circuit cad;field programmable gate arrays	Embedded systems often contain many components, some with multiple Field Programmable Gate Arrays (FPGAs). Designing Printed Circuit Boards (PCBs) for these systems can be a complex process that is often tedious, error-prone, and time-intensive. Existing computer-aided design tools require designers to manually insert components and explicitly define the connections between every component on the PCB - a cumbersome process. A fast PCB design framework requiring reduced designer time and effort would be particularly advantageous for rapid prototyping and short production run PCBs. Therefore, this paper proposes a novel, freely-available open-source framework to capture design intent and automatically implement the design details. Designers express connectivity at a higher level of abstraction than enumerating or drawing each individual trace between components. Given the components and connection requirements, the proposed framework automatically generates component placements, I/O voltage supply assignments, and FPGA pin assignments to minimize trace length. We also propose a novel method to improve trace length estimations during placement, before FPGA pins have actually been assigned to those connections. The proposed framework quickly explores large solution spaces, enabling rapid prototyping and design space exploration, and can lead to lower costs in design time and other non-recurring expenses. We demonstrate that it produces favorable results for various design requirements, which suggests the framework will be especially appreciated by designers of systems with multiple FPGAs having large numbers of flexible pins.	cognitive dimensions of notations;component placement;computer-aided design;design space exploration;embedded system;field-programmable gate array;high-level programming language;input/output;open-source software;printed circuit board;rs-232;rapid prototyping;requirement	Daniel P. Seemuth;Katherine Morrow	2013	2013 International Conference on Field-Programmable Technology (FPT)	10.1109/FPT.2013.6718363	physical design;embedded system;computer science;operating system;circuit design;routing;field-programmable gate array	EDA	12.994845314516517	51.84599292709456	43368
d25dd8307bbabf2c9d2abe379f52272ef06bdf70	reducing power consumption in fault tolerant asics		Fault tolerance techniques have been traditionally used for specific applications such as space, avionic, or nuclear where the reliability is of utmost importance. Moreover, nowadays the reliability of nanoscale CMOS processes is reduced, and the power optimization goes beyond the standard worst-case boundaries. As a consequence, fault tolerance became the important factor also for the main stream industry. Handling faults is connected with some sort of redundancy either in hardware, timing, information, or by combination of previously defined methods. The immediate consequence of the redundancy is increased power consumption. As an example, the traditional N-modular redundancy techniques, such as TMR (triple modular redundancy) end up in the power overhead of about 300-400% of original non-protected system. In order to limit the overhead the additional methods and techniques need to be applied. In this presentation the relevant state-of-the-art methods for reducing power consumption in fault tolerant chips will be presented. This will include the global overview of the traditional methods (such as improvements of TMR/DMR schemes) and novel methods for low-power ASICs (such as RAZOR). Moreover, the methods at different abstraction layers proposed at IHP for handling the power optimization in fault-tolerant ASICs will be presented. At the RTL level, selective fault tolerance can be applied limiting overhead by 50% with still protecting 20% of the inputs. Additionally, for the memories the overhead can be reduced by utilizing specific error correction codes. Finally, the fault tolerance can be introduced on application demand, dramatically reducing the power consumption when it is not needed. This can be performed on the pipeline level or even at the level of multi-processors. Such an example will be shown and IHP’s FMP adaptive multi-core platform will be presented.	application-specific integrated circuit;best, worst and average case;cmos;central processing unit;code;dual modular redundancy;ecc memory;error detection and correction;fault tolerance;finite model property;low-power broadcasting;mathematical optimization;multi-core processor;overhead (computing);power optimization (eda);razor;redundancy (engineering);timing closure;triple modular redundancy	Milos Krstic	2015			real-time computing;fault tolerance;computer science	HPC	6.146415534977278	58.9907041846548	43372
7e478b1d942352d530c7c3dd96313d9051eb4a52	automatic generation of hardware self-organizing map for fpga implementation	unsupervised learning;vectors registers;computer program;vhsic hardware description language code hardware selforganizing map automatic generation fpga implementation neural network unsupervised leaning multidimensional vector classification hardware som configuration computer program;hardware description languages;fpga;automatic generation;generator;fpga implementation;self organising feature maps;vhdl;generator self organizing map fpga hardware vhdl;vhsic hardware description language;self organizing map;pattern classification;self organized map;field programmable gate arrays;unsupervised learning field programmable gate arrays hardware description languages pattern classification self organising feature maps;neural network;hardware	Self-organizing map (SOM) proposed by T. Kohonen is a neural network with unsupervised leaning to classify multidimensional vectors. The performance of the SOM implemented in software decreases as the number of neurons or vector dimention increases. Thus, performance acceleration of the SOM by the custom hardware is highly desired. However, compared to the SOM implemented in software, the modification of the hardware SOM design is still time-consuming task. This paper proposes a flexible hardware SOM configuration, and a computer program that generates VHSIC hardware description language (VHDL) code of the hardware SOM was developed.	artificial neural network;code;computer program;field-programmable gate array;hardware acceleration;hardware description language;logic synthesis;organizing (structure);self-organization;self-organizing map;simulation;unsupervised learning;vhdl;vhsic	Kota Yamamoto;Yoshiro Oba;Zuiko Rikuhashi;Hiroomi Hikawa	2011	2011 International Symposium on Intelligent Signal Processing and Communications Systems (ISPACS)	10.1109/ISPACS.2011.6146080	computer architecture;computer science;theoretical computer science;machine learning	Arch	9.186789007159222	48.5006625961897	43400
28af7241b11a695a7fa57c93ff71d09d0f46a5ce	uncertainty-aware dynamic power management in partially observable domains	silicon;cmos integrated circuits;silicon cmos integrated circuits decision making markov processes microprocessor chips nanoelectronics power aware computing;evaluation performance;optimisation;semimarkov decision process;procesador risc;cmos technology;parametro circuito;performance evaluation;echelle nanometrique;optimizacion;fluctuations;integrated circuit;systeme aide decision;proceso markov;circuit parameter;uncertainty;evaluacion prestacion;pomdp;manufacturing process;si uncertainty aware dynamic power management nanoscale cmos design technology silicon structure stochastic framework decision making manufacturing process semimarkov decision process risc processor size 65 nm;circuito integrado;cmos process;sistema ayuda decision;tecnologia mos complementario;parametre circuit;technology management;stochastic framework;optimization problem;power aware computing;decision support system;stochastic processes;mathematical programming;procedimiento fabricacion;power system management;semi markov decision process;processus markov;processus semi markovien;nanoelectronics;silicon structure;fluctuation temperature;power management;markov process;risc processor;proceso semi markoviano;uncertainty aware dynamic power management;temperature fluctuation;nanometer scale;fluctuacion temperatura;stochastic control;optimization;partial observation;si;markov processes;energy management uncertainty technology management cmos technology power system management cmos process temperature fluctuations silicon stochastic processes;power consumption;processeur risc;temperature;nanoscale cmos design technology;consommation energie electrique;procede fabrication;technologie mos complementaire;dynamic power management;dynamic power management dpm;size 65 nm;semimarkovian process;circuit integre;complementary mos technology;energy saving;uncertainty dynamic power management dpm pomdp stochastic control;energy management;microprocessor chips	This paper tackles the problem of dynamic power management (DPM) in nanoscale CMOS design technologies that are typically affected by increasing levels of process and temperature variations and fluctuations due to the randomness in the behavior of silicon structure. This uncertainty undermines the accuracy and effectiveness of traditional DPM approaches. This paper presents a stochastic framework to improve the accuracy of decision making during dynamic power management, while considering manufacturing process and/or environment induced uncertainties. More precisely, variability and uncertainty at the system level are captured by a partially observable semi-Markov decision process with interval-based definition of states while the policy optimization problem is formulated as a mathematical program based on this model. Experimental results with a RISC processor in 65-nm technology demonstrate the effectiveness of the technique and show that the proposed uncertainty-aware power management technique ensures system-wide energy savings under statistical circuit parameter variations.	algorithm;cmos;iteration;iterative method;markov chain;markov decision process;mathematical optimization;online and offline;optimization problem;partially observable system;power management;process modeling;randomness;semiconductor industry;spatial variability;uncertainty quantification	Hwisung Jung;Massoud Pedram	2009	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2008.2009014	embedded system;electronic engineering;real-time computing;simulation;decision support system;computer science;engineering;electrical engineering;technology management;operating system;markov process;cmos;statistics	EDA	23.392782230023887	57.525071151591895	43410
2e29bd7506df56802afd0d93d9f643c16ee5031c	faster group operations on elliptic curves	side channel attack;efficient elliptic curve arithmetic;unified addition	This paper improves implementation techniques of Elliptic Curve Cryptography. We introduce new formulae and algorithms for the group law on Jacobi quartic, Jacobi intersection, Edwards, and Hessian curves. The proposed formulae and algorithms can save time in suitable point representations. To support our claims, a cost comparison is made with classic scalar multiplication algorithms using previous and current operation counts. Most notably, the best speedup is obtained in the case of Jacobi quartic curves which also lead to one of the most efficient scalar multiplications benefiting from the proposed 2M+5S+1D (i.e. 2 multiplications, 5 squarings, and 1 multiplication by a curve constant) point doubling and 7M+3S+1D point addition algorithms. Furthermore, the new addition algorithm provides an efficient way to protect against side channel attacks which are based on simple power analysis (SPA).	elliptic curve cryptography;hessian;jacobi method;multiplication algorithm;period-doubling bifurcation;side-channel attack;speedup	Hüseyin Hisil;Kenneth Koon-Ho Wong;Gary Carter;Ed Dawson	2007			combinatorics;discrete mathematics;jacobian curve;counting points on elliptic curves;side channel attack;mathematics;elliptic curve cryptography;hessian form of an elliptic curve;elliptic curve point multiplication;algebra	Crypto	9.251055507955659	42.90513060013933	43436
7ef515dd832b95c37114dc8a7338a3adf6132296	fpga implementation of an adaptive filter robust to impulsive noise: two approaches	hardware design languages;echo cancellation;digital signal processing;field programmable gate array;field programmable gate arrays adaptive filters noise robustness noise cancellation hardware design languages high level synthesis logic testing echo cancellers system identification distortion;design tool;logic occupation fpga implementation adaptive filter impulsive noise signal distortion hardware platforms field programmable gate arrays timing requirements hardware description language high level synthesis design tool adaptive algorithm;design process;signal distortion;real time;timing requirements;impulse noise;hardware description languages;high level synthesis hls;logic circuits;field programmable gate array fpga;noise robustness;distortion;fpga implementation;high level synthesis;adaptive algorithm;logic occupation;adaptive filters;system identification;embedded system design;noise cancellation;logic testing;adaptive system;impulsive noise;hardware design;hardware platforms;field programmable gate arrays;hardware description language;high level synthesis design tool;logic circuits adaptive filters field programmable gate arrays hardware description languages impulse noise;impulsive noise adaptive filters field programmable gate array fpga hardware design languages high level synthesis hls;echo cancellers;real time application;algorithm design;hardware implementation;adaptive filter	Adaptive filters are used in a wide range of applications such as echo cancellation, noise cancellation, system identification, and prediction. Its hardware implementation becomes essential in many cases where real-time execution is needed. However, impulsive noise affects the proper operation of the filter and the adaptation process. This noise is one of the most damaging types of signal distortion, not always considered when implementing algorithms, particularly in specific hardware platforms. Field-programmable gate arrays (FPGAs) are used widely for real-time applications where timing requirements are strict. Nowadays, two main design processes can be followed for embedded system design, namely, a hardware description language (e.g., VHDL) and a high-level synthesis design tool. This paper proposes the FPGA implementation of an adaptive algorithm that is robust to impulsive noise using these two approaches. Final comparison results are provided in order to test accuracy, performance, and logic occupation.	adaptive algorithm;adaptive filter;central processing unit;clock rate;clock signal;design tool;digital signal processor;distortion;echo suppression and cancellation;embedded system;experience;field-programmable gate array;filter design;hardware description language;high- and low-level;high-level programming language;high-level synthesis;level design;library (computing);low-level design;mathematical optimization;real-time clock;requirement;sampling (signal processing);simulation;simulink;system identification;systems design;vhdl	Alfredo Rosado Muñoz;Manuel Bataller-Mompeán;Emilio Soria-Olivas;Claudio Scarante;Juan Guerrero-Martínez	2011	IEEE Transactions on Industrial Electronics	10.1109/TIE.2009.2023641	adaptive filter;embedded system;electronic engineering;real-time computing;computer science;adaptive system;hardware description language;field-programmable gate array	EDA	5.57948083556878	47.51311630620798	43456
3c569d604c676af69cd0059b4745433a29945264	a cabac encoder design of h.264/avc with rdo support	h 264 audio video coding;optimisation;pipeline structure;pipeline structure hardware cabac encoder architecture h 264 audio video coding rate distortion optimization memory resource context based adaptive binary arithmetic coding;hardware cabac encoder architecture;binary codes;adaptive codes;null;rate distortion theory;memory access;video coding;audio coding;arithmetic codes;data dependence;video coding adaptive codes arithmetic codes audio coding binary codes optimisation rate distortion theory;context management;rate distortion optimization;automatic voltage control costs delay pipelines bit rate encoding arithmetic rate distortion video coding clocks;memory resource;context based adaptive binary arithmetic coding	In this paper, a HW CABAC encoder architecture is proposed targeting H.264/A VC main profile. CABAC and rate distortion optimization (RDO) are two important coding tools that enhance coding efficiency of H.264/AVC How ever, coding speed of CABAC encoder is limited by the coding data dependence and its serial coding procedure, and RDO requires large memory resource and costs long delay of memory access to backup and restore CABAC context state data. The CABAC encoder of this paper utilizes pipeline structure at top level to reduce data dependence, and coding speed of one symbol per cycle is achieved. A context managing mechanism is designed that fully supports RDO coding of H.264/AVC encoder. It significantly reduces context memory cost and operation delay for context backup and restore. 85% of context memory resource is reduced comparing to the reference design. Synthesis result shows that the encoder can work at 620 MHz targeting 0.13 um CMOS process.	algorithmic efficiency;backup and restore;cmos;clock rate;context-adaptive binary arithmetic coding;data dependency;distortion;encoder;h.264/mpeg-4 avc;logic gate;mathematical optimization;pipeline (computing);random-access memory;raster document object;rate–distortion optimization;rate–distortion theory;reference design;remote data objects;unified model	Xiaohua Tian;Thinh M. Le;B. L. Ho;Yong Lian	2007	18th IEEE/IFIP International Workshop on Rapid System Prototyping (RSP '07)	10.1109/RSP.2007.5	binary code;parallel computing;real-time computing;rate–distortion theory;computer science;theoretical computer science;context-adaptive variable-length coding;rate–distortion optimization;context-adaptive binary arithmetic coding	HCI	12.430466415711471	39.989922583836304	43561
3870bdd199eac867608e1b108cede3f657381f2a	high-level energy estimation in the sub-v$_{{\rm t}}$ domain: simulation and measurement of a cardiac event detector	detectors;subthreshold;elektroteknik och elektronik;integrated circuit;sub threshold;clocks;energy efficient;cardiology;standard deviation;pacemakers;cardiac pacemaker;energy dissipation;r wave;circuit simulation;model error;logic gates;qrs detection;integrated circuit modeling;energy model;capacitance;high level energy estimation;switches;logic gate;spice;energy estimate;biomedical measurement;biomedical equipment	This paper presents a flow that is suitable to estimate energy dissipation of digital standard-cell based designs which are determined to operate in the subthreshold regime. The flow is applicable on gate-level netlists, where back-annotated toggle information is used to find the minimum energy operation point, corresponding maximum clock frequency, as well as the dissipated energy per clock cycle. The application of the model is demonstrated by exploring the energy efficiency of pipelining, retiming, and register balancing. Simulation results, which are obtained during a fraction of SPICE simulation time, are validated by measurements on a wavelet-based cardiac event detector that was fabricated in 65-nm low-leakage high-threshold technology. The mean of the absolute modeling error is calculated as 5.2%, with a standard deviation of 6.6% over the measurement points. The cardiac event detector dissipates 0.88 pJ/sample at a supply voltage of 320 mV.	application-specific integrated circuit;artificial cardiac pacemaker;clock rate;clock signal;detector device component;detectors;equilibrium;extravasation;feature toggle;high- and low-level;ll parser;pipeline (computing);retiming;spice;simulation;spectral leakage;standard cell;standard deviation;wavelet;orders - hl7publishingdomain;voltage	Omer Can Akgun;Joachim Neves Rodrigues;Yusuf Leblebici;Viktor Öwall	2012	IEEE Transactions on Biomedical Circuits and Systems	10.1109/TBCAS.2011.2157505	electronic engineering;real-time computing;logic gate;engineering;electrical engineering	EDA	20.739711029793472	60.19931544618924	43588
6bdb0d0cd0b681ecec20c04a2cc96965c1651635	partitioned systolic architecture for modular multiplication in gf(2m)	red sistolica;systolic architecture;lpgs partition method;theorie communication;first in first out;lpgs partition;systolic array;cle publique;teoria comunicacion;architecture systolique;public key;particion;systolic network;criptografia;cryptography;communication theory;llave publica;partition;reseau systolique;modular multiplier;arquitectura;cryptographie;locally parallel globally sequential partitioned method;theorie information;fifo;architecture;modular multiplication;information theory;teoria informacion	Abstract   This letter proposes a partitioned systolic array for MSB-first approach multiplication in   GF(2     m   )  . When compared to the related multiplier presented by Wang, the proposed systolic array requires a significantly smaller number of basic cells. It requires only   m/2   basic cells and exhibits the same latency when it has two bands.		Hyun-Sung Kim;Sung-Woo Lee;Kee-Young Yoo	2000	Inf. Process. Lett.	10.1016/S0020-0190(00)00130-7	partition;arithmetic;systolic array;information theory;computer science;cryptography;theoretical computer science;architecture;mathematics;algorithm;communication theory	Logic	13.191103471434705	36.620247450353986	43743
0630e9feb2bc0d4a2a4be54b96b2904b2d59eed2	fast buffer insertion for yield optimization under process variations	higher yield;large case;process variations;process variation;buffer insertion algorithm;timing optimization;statistical buffering algorithm;fast buffer insertion;exiting approach;yield optimization;buffer insertion;traditional corner-based timing optimization;deterministic buffering;random variable;statistical distributions	With the emerging process variations in fabrication, the traditional corner-based timing optimization techniques become prohibitive. Buffer insertion is a very useful technique for timing optimization. In this paper, we propose a buffer insertion algorithm with the consideration of process variations. We use the solutions from the deterministic buffering that sets all the random variables at their nominal values to guide the statistical buffering algorithm. Our algorithm keeps the solution lists short, and always achieves higher yield than the deterministic buffering. The experimental results demonstrate that the exiting approaches cannot handle large cases efficiently or effectively, while our algorithm handles large cases very efficiently, and improves the yield more than 12% on average.	genetic algorithm;insertion sort;mathematical optimization;rule 184	Ruiming Chen;Hai Zhou	2007	2007 Asia and South Pacific Design Automation Conference		probability distribution;random variable;electronic engineering;real-time computing;computer science;process variation;engineering drawing;statistics	EDA	23.127444220573583	57.53812934605085	43831
0c9976920bc75a3ef21b0ec57f6937d3818bf3f3	concept for a safety-controller based on uncertified hardware	libraries;iec standards;safety;safety hardware iec standards encoding program processors libraries;encoding;program processors;hardware	This work suggests new solutions for safe systems in industrial applications. Nowadays automation systems get more complex, so the microcontroller has to be replaced with a more powerful CPU. The preferred solution is to use commercial off-the-shelf (COTS) general purpose CPUs. The hardware has to be analyzed in detail to estimate the behavior in case of a fault. With state-of-the art processors this is not possible anymore. A concept to avoid this is “coded processing” as mentioned in the stan-dard for industrial safety systems IEC 61508 [1]. The goal of this research is to analyze a concept which meets the demands of the IEC 61508 safety integrity level 3 (SIL 3) only based on software techniques to avoid any hardware analysis and dependencies. The evaluation of the concept is done by theoretical analysis based on fault models found in literature. The practical tests are done by a fault injection software which is developed in the course of this research.	central processing unit;computation;computer;control theory;fault injection;fault model;level of detail;mandatory integrity control;microcontroller;pci express;read-only memory;redundancy (information theory);requirement;stan (fan);type signature;watchdog timer	Bernd Thiemann;Andreas Platschek	2014	2014 10th IEEE Workshop on Factory Communication Systems (WFCS 2014)	10.1109/WFCS.2014.6837588	embedded system;real-time computing;computer science;safety instrumented system;computer engineering	Embedded	5.586758451838196	55.58607646210773	43837
44eb7425ab27d9776f0d8decd159b54dad23b2f5	combined intra- and inter- block analysis of balanced ternary designs			balanced ternary	Sobita Sapam;B. K. Sinha;N. K. Mandal	2018	JSTA	10.2991/jsta.2018.17.1.7	mathematical optimization;balanced ternary;mathematics	EDA	24.190838415428026	39.797341807568806	43877
3cd99d177d0b6c0a884676854c28ff8fb18c70b6	an efficient method for analyzing on-chip thermal reliability considering process variations	process variation;reliability;thermal analysis;electrothermal simulation;performance;simulation;thermal reliability;chip temperature;algorithms;design;article	This work provides an efficient statistical electrothermal simulator for analyzing on-chip thermal reliability under process variations. Using the collocation-based statistical modeling technique, first, the statistical interpolation polynomial for on-chip temperature distribution can be obtained by performing deterministic electrothermal simulation very few times and by utilizing polynomial interpolation. After that, the proposed simulator not only provides the mean and standard deviation profiles of on-chip temperature distribution, but also innovates the concept of thermal yield profile to statistically characterize the on-chip temperature distribution more precisely, and builds an efficient technique for estimating this figure of merit. Moreover, a mixed-mesh strategy is presented to further enhance the efficiency of the developed statistical electrothermal simulator.  Experimental results demonstrate that (1) the developed statistical electrothermal simulator can obtain accurate approximations with orders of magnitude speedup over the Monte Carlo method; (2) comparing with a well-known cumulative distribution function estimation method, APEX [Li et al. 2004], the developed statistical electrothermal simulator can achieve 215× speedup with better accuracy; (3) the developed mixed-mesh strategy can achieve an order of magnitude faster over our baseline algorithm and still maintain an acceptable accuracy level.	algorithm;approximation;baseline (configuration management);collocation;monte carlo method;polynomial interpolation;simulation;speedup;statistical model	Yu-Min Lee;Pei-Yu Huang	2013	ACM Trans. Design Autom. Electr. Syst.	10.1145/2491477.2491485	design;simulation;performance;computer science;reliability;process variation;statistics;thermal analysis	EDA	22.365584906764486	58.71297051266126	43905
a707007d5cd57f04baba53a330b23261b5b9f142	acyclic modeling of combinational loops	original feedback path;cycles;acyclic circuit;feedback logic;monotonic loops;combinational loop;gate-level combinational loop;nested feedback path;combinational circuits.;feedback;acyclic combinational logic;original stateful feedback logic;cyclic combinational logic;combinational loops breach design;acyclic modeling;upper bound;equivalent circuit;combinational circuits;combinational circuit;logic design;design methodology;oscillations;equivalent circuits;integrated circuit design	"""This paper presents a method to convert gate-level combinational loop into an acyclic circuit, if the combinational loop is not oscillatory. Combinational loops breach design methodologies, because they can involve undesirable circuit behavior and can possibly lead to oscillations based on the external stimuli to the loops. However, for designs compiled using automated synthesis-compiler, these loops are very likely to appear in the generated gate-level designs. We present a modeling of combinational loops as state holding elements and break non oscillatory loops using a level sensitive latch. Apart from modeling combinational loops consisting of gates, the algorithm also converts the loops through design latches. The increase in design area, due to the loop conversion, has an upper bound of twice the size of the original feedback path. However, in case of multiply nested feedback paths, each path is treated separately. Unlike previous work that converts cyclic combinational logic where the feedback is not exercised, this paper presents an algorithm to identify the stateful """"latch"""" behavior in a class of feedback logic (non-oscillatory, monotonic). A conversion algorithm replaces such feedback logic by an equivalent circuit comprising explicit latches and acyclic combinational logic. The replacement circuit has an identical behavior as the original stateful feedback logic."""	algorithm;combinational logic;compiler;directed acyclic graph;equivalent circuit;lambda calculus;negative feedback;state (computer science)	Amit Gupta;Charles Selvidge	2005	ICCAD-2005. IEEE/ACM International Conference on Computer-Aided Design, 2005.		equivalent circuit;control engineering;electronic engineering;real-time computing;computer science;sequential logic;combinational logic;quantum mechanics	EDA	20.343241839475144	47.81064145759495	43916
5ef95115209f8e9b6dc7f9b4b1e55494b44b2d10	code-c: a novel two-level hardware/software co-design framework	hardware software co design;reconfigurable architectures;simulated annealing;c language;hardware partitioning algorithms memory management computer languages computational modeling simulated annealing fractals workstations concurrent computing sliding mode control;development systems;source code;development systems reconfigurable architectures logic partitioning program compilers simulated annealing c language;logic partitioning;program compilers;simulated annealing code x two level hardware software co design framework xputer universal accelerator reconfigurable datapath hardware profiling driven host accelerator partitioning resource driven sequential structural partitioning program compilers;performance optimization	Notice: This document has been provided by the contributing authors as a means to ensure timely dissemination of scholarity and technical work on a noncommercial basis. Copyright and all rights therein are maintained by the authors or by other copyright holders, notwithstanding that they have offered their works here electronically. It is understood that all persons copying this information will adhere to the terms and constraints invoked by each author's copyright. These works may not be reposted without the explicit permission of the copyright holder. Xputer Lab Abstract	xputer	Reiner W. Hartenstein;Jürgen Becker;Rainer Kress;Helmut Reinig	1996		10.1109/ICVD.1996.489461	computer architecture;parallel computing;real-time computing;simulated annealing;computer science;programming language;source code	AI	-2.2743566038410394	48.99405446992314	43942
6bc2fb9893232843049ee2f64bfc5faeaeb4bda2	on-line bist for testing analog circuits	bist;circuit simulation analogue circuits built in self test circuit testing current mode circuits online operation;current window comparator;automatic testing;simulation;filters;high speed current mode circuits;mixed signal integrated circuits;signal integrity;analog circuits;circuit simulation;built in self test;circuit testing built in self test analog circuits automatic testing integrated circuit testing mixed analog digital integrated circuits current mode circuits filters vehicles circuit simulation;analogue circuit testing;integrated circuit testing;current mode circuit;current mode circuits;current based checker circuits;analogue circuits;circuit testing;vehicles;leapfrog filter;high speed;bist online built in self test analogue circuit testing current window comparator current based checker circuits test response processing mixed signal integrated circuits high speed current mode circuits leapfrog filter simulation;online operation;test response processing;online built in self test;mixed analog digital integrated circuits	In this paper, we present a new online built-in self-test (BIST) approach for testing analog circuits. It uses a current window comparator and current-based checker circuits for processing the test response of the analog parts in mixed-signal integrated circuits. Online analog BIST capability is achieved by using high-speed current-mode circuits. A leapfrog filter has been considered as a test vehicle, and simulation results show the feasibility and effectiveness of the proposed BIST approach.	built-in self-test;integrated circuit	Jaime Velasco-Medina;Iyad Rayane;Michael Nicolaidis	1999		10.1109/ICCD.1999.808563	embedded system;electronic engineering;analogue electronics;computer science;signal integrity;engineering;electrical engineering	EDA	24.19666746181454	52.06777361490548	43964
faa7db461e6c5eee103eb5eaa086eb3139827464	on the solution of block hessenberg systems	queue;block hessenberg matrix;linear system;fast fourier transform;on the fly;technical report;block toeplitz matrix;divide and conquer;sparse matrices;toeplitz matrices	This paper describes a divide-and-conquer strategy for solving block Hessenberg systems. For dense matrices the method is as efficient as Gaussian elimination; however, because it works almost entirely with the original blocks, it is much more efficient for sparse matrices or matrices whose blocks can be generated on the By. For Toeplitz matrices, the algorithm can be combined with the fast Fourier transform.	fast fourier transform;gaussian elimination;karl hessenberg;sparse matrix;toeplitz hash algorithm	G. W. Stewart	1995	Numerical Lin. Alg. with Applic.	10.1002/nla.1680020309	fast fourier transform;mathematical optimization;combinatorics;divide and conquer algorithms;sparse matrix;technical report;toeplitz matrix;mathematics;linear system;queue;block matrix;hessenberg matrix;algebra	HPC	-2.363500588245338	38.33448918969025	44033
0df08b8c5119007b1e14ff94d96cdd9970d4525f	designing customized isa processors using high level synthesis	kernel;instruction sets c language field programmable gate arrays high level synthesis;arrays;hls generated hardware customized isa processors high level synthesis soft processor design instruction set architecture software kernels full isa soft processor cores c language c language functional hardware processor architectures basic linear algebra application cryptographic application xilinx microblaze;registers;program processors;pipeline processing;program processors registers kernel hardware pipeline processing arrays;hardware	In this paper we propose a new degree of flexibility for soft processor design in which only the instructions relevant to the task at hand are implemented as a subset of the Instruction Set Architecture (ISA). These customized processors execute software kernels in the usual way, yet can be implemented with a fraction of the hardware resources used by other full- ISA soft processor cores. We present a design methodology for such customized ISA processors where the functional description of the processor is written in a high level language and the hardware implementation is obtained using high level synthesis (HLS) tools. We investigate the potential and limitations of the HLS tools to take processor simulator-like implementations in C/C++ and produce functional hardware processor architectures. We apply this methodology to two relevant applications -a basic linear algebra application and a cryptographic application- due to their distinct features. Our results demonstrate that these customized processors utilize significantly less hardware resources than those required by Xilinx MicroBlaze with reasonable performance degradation using HLS generated hardware exclusively.	c++;central processing unit;cryptographic hash function;cryptography;elegant degradation;high-level programming language;high-level synthesis;linear algebra;natural language processing;processor design;spim	Sam Skalicky;Tejaswini Ananthanarayana;Sonia López;Marcin Lukowiak	2015	2015 International Conference on ReConFigurable Computing and FPGAs (ReConFig)	10.1109/ReConFig.2015.7393299	embedded system;computer architecture;parallel computing;kernel;real-time computing;computer science;operating system;processor register	EDA	0.5585396682988087	48.70879292229235	44047
34eff1927eef5e8eb052c752583a6821fd284332	distributed-function computer architectures	computers;software;biological system modeling;computer architecture;computational modeling;computer architecture computational modeling computers software hardware conferences biological system modeling;conferences;hardware	Recent advances in solid-state technology coupled with marked decreases in the cost of hardware have provided the impetus to reexamine computer architectures. The von Neuman computer and other early stored-program computers were constrained to single data-stream/single-instruction-stream sequential organization primarily because of economic considerations (flip flops were of the order of $100 each). Today, however, with flip flops priced under $0.25 each, hardware costs are rapidly approaching the point where both horizontally distributed (array structures) and vertically distributed (hierarchical structures) information processing architectures are not only feasible, but economically practical.	computer architecture;flops;flip-flop (electronics);information processing;solid-state drive;stored-program computer	Ragnar N. Nilsen	1974	Computer	10.1109/MC.1974.6323469	computer architecture;computing;computer science;theoretical computer science;operating system;software engineering;computational model	Arch	8.370526843877089	55.79387237911004	44061
db29b9c62f7b11363aeabc5c34a176a8f76ec6e3	fpga implementation of reversible watermarking in digital images using reversible contrast mapping	reversible watermarking;fpga;reversible contrast mapping	Reversible contrast mapping (RCM) and its various modified versions are used extensively in reversible watermarking (RW) to embed secret information into the digital contents. RCM based RW accomplishes a simple integer transform applied on pair of pixels and their least significant bits (LSB) are used for data embedding. It is perfectly invertible even if the LSBs of the transformed pixels are lost during data embedding. RCM offers high embedding rate at relatively low visual distortion (embedding distortion). Moreover, low computation cost and ease of hardware realization make it attractive for real-time implementation. To this aim, this paper proposes a field programmable gate array (FPGA) based very large scale integration (VLSI) architecture of RCM-RW algorithm for digital images that can serve the purPGA pose of media authentication in real-time environment. Two architectures, one for block size (8 × 8) and the other one for (32 × 32) block are developed. The proposed architecture allows a 6-stage pipelining technique to speed up the circuit operation. For a cover image of block size (32 × 32), the proposed architecture requires 9881 slices, 9347 slice flip-flops, 11291 number 4-input LUTs, 3 BRAMs and a data rate of 1.0395 Mbps at an operating frequency as high as 98.76 MHz. © 2014 Elsevier Inc. All rights reserved.	algorithm;authentication;block size (cryptography);clock rate;computation;data rate units;digital image;digital watermarking;distortion;emoticon;flops;field-programmable gate array;flip-flop (electronics);integrated circuit;least significant bit;pipeline (computing);pixel;read-write memory;real-time transcription;reliability-centered maintenance;very-large-scale integration	Hirak Kumar Maity;Santi Prasad Maity	2014	Journal of Systems and Software	10.1016/j.jss.2014.05.079	electronic engineering;computer science;theoretical computer science;field-programmable gate array;computer graphics (images)	EDA	12.448791331704625	42.36601118733088	44063
018c389ea37b9dec236bc62445851bd299cfb951	in search of the optimal walsh-hadamard transform	optimisation;automatic generation;program optimization;walsh hadamard transform;signal processing algorithms iterative algorithms packaging tensile stress matlab algorithm design and analysis mathematics computer science signal generators timing;hadamard transforms;signal processing;spiral project optimal walsh hadamard transform fast signal transforms signal processing algorithms symbolic expressions programs matlab wht algorithms;optimisation hadamard transforms signal processing	This paper describes an approach to implementing and optimizing fast signal transforms. Algorithms for computing signal transforms are expressed by symbolic expressions, which can be automatically generated and translated into programs. Optimizing an implementation involves searching for the fastest program obtained from one of the possible expressions. In this paper we apply this methodology to the implementation of the Walsh-Hadamard transform. An environment, accessible from MATLAB, is provided for generating and timing WHT algorithms. These tools are used to search for the fastest WHT algorithm. The fastest algorithm found is substantially faster than standard approaches to implementing the WHT. The work reported in this paper is part of the SPIRAL project (see http://www.ece.cmu.edu/∼spiral), an ongoing project whose goal is to automate the implementation and optimization of signal processing algorithms.	algorithm;fastest;hadamard transform;matlab;mathematical optimization;optimizing compiler;s-expression;signal processing	Jeremy R. Johnson;Markus Püschel	2000		10.1109/ICASSP.2000.860117	multidimensional signal processing;hadamard transform;computer science;theoretical computer science;program optimization;signal processing;algorithm	PL	6.074051940055118	46.60130123614568	44097
8313bb79112dcda93d6fdbd3509613cb68044d3b	a method/approach leading to controlled randomization in validation of an ip	silicon;stress;software;microcontrollers;registers;ip networks;computer bugs	Post-silicon validation is the last level of inspecting the silicon before it is delivered to the customer. Automotive microcontrollers use Direct Memory Access (DMA) extensively in safety critical applications. This article explains how post-silicon validation can be improved to address the needs of the growing complexity of microcontrollers with a large number of Intellectual Property (IP). With increasing design complexity, aggressive scaling, and decreasing time to market, it is imperative to test the robustness of the microcontroller. Traditional test cases follow directed approach to testing and do not guarantee complete functional coverage. The proposed methodology uses the concept of constraint based randomization that is used in pre-silicon verification. The main advantage of using constraint based randomization in post-silicon validation is that millions of seeds can be executed in a very short time. This also stresses the silicon, increasing the likelihood of uncovering a bug which would not have been humanly possible to uncover at the pre-silicon stage.	data validation;direct memory access;image scaling;imperative programming;microcontroller;software bug;test case	Meghashyam Ashwathanarayan;Guddeti Jayakrishna	2017	2017 12th International Conference on Design & Technology of Integrated Systems In Nanoscale Era (DTIS)	10.1109/DTIS.2017.7930165	embedded system;real-time computing;computer science;theoretical computer science	EDA	20.81311658839525	53.241505925946456	44185
949778c9a2e9a4bea940aec27b1bdec7a890f07c	ultra low-power visual odometry for nano-scale unmanned aerial vehicles		One of the fundamental functionalities for autonomous navigation of Unmanned Aerial Vehicles (UAVs) is the hovering capability. State-of-the-art techniques for implementing hovering on standard-size UAVs process camera stream to determine position and orientation (visual odometry). Similar techniques are considered unaffordable in the context of nano-scale UAVs (i.e. few centimeters of diameter), where the ultra-constrained power-envelopes of tiny rotor-crafts limit the onboard computational capabilities to those of low-power microcontrollers. In this work we study how the emerging ultra-low-power parallel computing paradigm could enable the execution of complex hovering algorithmic flows onto nano-scale UAVs. We provide insight on the software pipeline, the parallelization opportunities and the impact of several algorithmic enhancements. Results demonstrate that the proposed software flow and architecture can deliver unprecedented GOPS/W, achieving 117 frame-per-second within a power envelope of 10 mW.	aerial photography;autonomous robot;gnu nano;low-power broadcasting;microcontroller;parallel computing;pipeline (software);programming paradigm;r.o.t.o.r.;unmanned aerial vehicle;visual odometry	Daniele Palossi;Andrea Marongiu;Luca Benini	2017	Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017		embedded system;computer vision;navigation;kernel;simulation;visualization;computer science;engineering	EDA	0.4284375480906104	46.471102021778336	44186
5721fba2f8a25082510253b4c58b88af9cc69316	distributed computation of wasserstein barycenters over networks		We propose a new class-optimal algorithm for the distributed computation of Wasserstein Barycenters over networks. Assuming that each node in a graph has a probability distribution, we prove that every node reaches the barycenter of all distributions held in the network by using local interactions compliant with the topology of the graph. We provide an estimate for the minimum number of communication rounds required for the proposed method to achieve arbitrary relative precision both in the optimality of the solution and the consensus among all agents for undirected fixed networks.		César A. Uribe;Darina Dvinskikh;Pavel Dvurechensky;Alexander Gasnikov;Angelia Nedic	2018	2018 IEEE Conference on Decision and Control (CDC)		mathematical optimization;probability distribution;computation;computer science;algorithm;graph	Vision	17.90676889908427	34.51133531245762	44271
7a19e22e2a8747bd72186b73372c98aad4bfb967	chain grouping: a method for partitioning loops onto mesh-connected processor arrays	processor scheduling parallel processing computer architecture parallel architectures tiles computer society delay effects vectors concurrent computing vliw;uniform chains of iterations;uniform chains of iterations partitioning loops mesh connected processor arrays chain grouping partitioning the loop iteration space mesh connected architectures loop grouping orthogonal projection hyperplane method;low complexity;parallel algorithms parallel architectures;parallel architectures;hyperplane method;orthogonal projection;loop grouping;communication delay;mesh connected architectures;space mapping;discrete group;parallel algorithms	This paper presents Chain Grouping, a new low complexity method for the problem of partitioning the loop iteration space into groups with little intercommunication requirements, for mapping onto mesh-connected architectures. First, the iterations are scheduled in time, according to the hyperplane method, taking into consideration the minimum time displacement. Then, the iteration space is divided into discrete groups of related iterations, which are assigned to different processors, while preserving the optimal completion time. Chain Grouping is based on clustering together neighboring uniform chains of iterations, formed by a particular dependence vector. This vector will be proven as the best among all to reduce the total communication requirements. Inside every group, the optimal hyperplane scheduling is preserved and references to intragroup iterations are considerably increased. The partitioned groups are afterward assigned to meshes of processors. The resulting space mapping maximizes processor utilization and cuts down overall communication delays while preserving the optimal hyperplane time schedule.		Panayiotis Tsanakas;Nectarios Koziris;George K. Papakonstantinou	2000	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.879777	mathematical optimization;parallel computing;space mapping;computer science;distributed computing;parallel algorithm;orthographic projection;discrete group	Visualization	-0.5960250635296822	38.3913477246409	44349
252baaee9b11ce057b0aab59f3f47c612e503e37	defect-aware high-level synthesis targeted at reconfigurable nanofabrics	reconfiguration;defect prone nanofabrics;nanotechnologies;kernel;reliability;helium;high level synthesis hls;defect density;design optimization;design space;nanofabrics;high level synthesis;defect aware high level synthesis;nanoelectronics;merging;reconfigurable nanofabrics;fabrics;scalability;reconfiguration based defect avoidance methodology;defect tolerance;algorithm design and analysis;buildings;design methodology	Entering the nanometer era, a major challenge to current design methodologies and tools is how to effectively address the high defect densities projected for nanoelectronic technologies. To this end, we proposed a reconfiguration-based defect-avoidance methodology for defect-prone nanofabrics. It judiciously architects the nanofabric, using probabilistic considerations, such that a very large number of alternative implementations can be mapped into it, enabling defects to be circumvented at configuration time, in a scalable way. Building on this foundation, in this paper we propose a synthesis framework aimed at implementing this new design paradigm. A key novelty of our approach with respect to traditional high level synthesis is that, rather than carefully optimizing a single (‘deterministic’) solution, our goal is to simultaneously synthesize a large family of alternative solutions, so as to meet the required probability of successful configuration, or yield, while maximizing the average performance of the family of synthesized solutions. Experimental results generated for a set of representative benchmark kernels, assuming different defect regimes and target yields, empirically show that our proposed algorithms can effectively explore the complex probabilistic design space associated with this new class of high level synthesis problems.	algorithm;benchmark (computing);best, worst and average case;cluster analysis;experiment;high- and low-level;high-level programming language;high-level synthesis;programming paradigm;reconfigurable computing;scalability;software bug	Chen He;Margarida F. Jacome	2007	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/TCAD.2006.884401	nanoelectronics;embedded system;algorithm design;electronic engineering;kernel;scalability;multidisciplinary design optimization;simulation;design methods;computer science;engineering;control reconfiguration;theoretical computer science;reliability;mathematics;helium;high-level synthesis;algorithm	EDA	9.89476773663247	58.00657107704908	44389
0aad0ad8a9ba382febeb7bd57454ecd944a10885	standard cell like via-configurable logic block for structured asics	libraries;configurable logic block;logic arrays;metals;logic design;routing;flip flops;design flow;routing fabrics application specific integrated circuits logic functions libraries programmable logic arrays flip flops logic arrays field programmable gate arrays costs;programmable logic arrays;size 0 18 micron;logic design application specific integrated circuits integrated logic circuits;size 0 18 micron via configurable logic block structured asic cell library;application specific integrated circuits;transistors;logic functions;vlsi;via configurable logic block;fabrics;standard cell;integrated logic circuits;vlsi structured asic programmable logic standard cell regular fabric;field programmable gate arrays;programmable logic;structured asic;cell library;regular fabric	A structured ASIC has some arrays of pre-fabricated yet configurable logic blocks (CLBs) with/without a regular routing fabric. In this paper, we propose a standard cell like via-configurable logic block (VCLB). We design a 0.18 um standard cell library based on our VCLB and establish a design flow using as many commercial tools as possible. We also propose a method to evaluate the viability of a structured ASIC fabric. Our structured ASIC fabric with programmable metals for routing achieves a delay of 2.7 times, an area of 3 times, and a power of 1.5 times that attained by the designs using a commercial cell library.	application-specific integrated circuit;logic block;routing;standard cell;unified model	Mei-Chen Li;Hui-Hsiang Tung;Chien-Chung Lai;Rung-Bin Lin	2008	2008 IEEE Computer Society Annual Symposium on VLSI	10.1109/ISVLSI.2008.50	computer architecture;electronic engineering;computer science;computer engineering	EDA	12.58204601429576	52.05340203669926	44427
e4bcaaffcb5e81ea29b18312153afbe161fa533b	the time-varying nature of cache utilization: a case study on the mantevo and apex benchmarks		Computer architects have been utilizing cache hierarchies to improve performance by minimizing latency of main memory accesses via caching frequently used data closer to the core. The majority of prior research focuses on measuring cache hit rates and data movement for the entire run of an application. We present cache performance metrics as they vary with the execution progression of the application, and show how these metrics can provide improved insight about application behavior. In this work, we present runtime cache utilization, as well as, conventional performance metrics that illustrate a holistic understanding of cache behavior. We built and incorporated a memory simulator into the Structural Simulation Toolkit (SST). We measure and analyze the performance for several scientific mini-applications from the APEX and Mantevo suites. This characterization can help identify the critical program points in benchmarks that may reduce the overall simulation time while testing new designs. Moreover, this introductory work enables us to quantify cache behavior for our work that proposes an on-chip memory with dynamic adaptive cache line size that is expected to reduce power consumption.	benchmark (computing);cpu cache;cache (computing);color gradient;computer data storage;experiment;holism;locality of reference;memory hierarchy;multi-core processor;requirement;run time (program lifecycle phase);scratchpad memory;simulation;software metric;supercomputer;symmetric multiprocessing;vii	Nafiul Alam Siddique;Patricia A. Grubel;Abdel-Hameed A. Badawy	2017	2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)	10.1109/UIC-ATC.2017.8397629	latency (engineering);system on a chip;distributed computing;memory management;computer science;cache;real-time computing;benchmark (computing);apex (geometry);cpu cache	HPC	-3.85080784915576	54.32191551187528	44429
f9d63c005288bf9332cfedc61d9ba4e5d81e0e0e	languages and compilers for parallel computing		Heterogeneous computing is being realized in modern SoCs, because power and performance are key to delivering multimedia experiences in mobiles devices that are limited by physical thermal limits. Programming these devices is challenging lack of shared virtual address spaces, different ISAs, different performance capabilities, are just some of the factors that complicate any programming model. Heterogeneous programming can be pretty ugly in places and is not easy for the first time developer, however, it has also been proven on multiple devices, ranging from tiny mobile devices that fit a pocket, to supercomputers, providing its users access to huge possibilities. In this talk I will reflect on the good, the bad, and the ugly of design and feature capabilities of OpenCL 2.0 and it new formalized foundations for shared memory model programming and support for dataand irregularparallel workloads. With a particular focus on OpenCL 2.0’s low-level features for high-level programming abstractions we will discuss capabilities of Qualcomm’s task based programming model, MARE, and how it might be integrated into a heterogenous platform supporting OpenCL.	address space;crosstalk;experience;heterogeneous computing;high- and low-level;high-level programming language;mobile device;opencl api;parallel computing;programming model;shared memory;supercomputer;system on a chip;windows mobile	Wei Ding;Oscar Hernández;Tony Curtis;Barbara Chapman	2013		10.1007/978-3-319-09967-5		HPC	-2.3559002800818556	48.82659508618998	44442
22d9cbed6db132aa3685cbedee70d8f7710186d2	a lifting-based discrete wavelet transform and discrete wavelet packet processor with support for higher order wavelet filters		Andre Guntoro, Manfred Glesner. A Lifting-Based Discrete Wavelet Transform and Discrete Wavelet Packet Processor with Support for Higher Order Wavelet Filters. Christian Piguet; Ricardo Reis; Dimitrios Soudris. VLSI-SoC: Design Methodologies for SoC and SiP, 313, Springer, pp.154-173, 2010, IFIP Advances in Information and Communication Technology, 978-3-64212266-8. <10.1007/978-3-642-12267-5 9>. <hal-01054539>	16-bit;central processing unit;discrete wavelet transform;international federation for information processing;lambda lifting;lifting scheme;locality of reference;polynomial;springer (tank);very-large-scale integration	Andre Guntoro;Manfred Glesner	2008		10.1007/978-3-642-12267-5_9	wavelet;electronic engineering;speech recognition;harmonic wavelet transform;second-generation wavelet transform;continuous wavelet transform;theoretical computer science;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;wavelet transform	EDA	8.373015603206563	39.37462815098633	44496
43181583d0e569d5a9e5d6c84648a88900733592	opportunities for system and user features in a new apl interpreter	unique opportunity;apl implementation;sam i architecture;parallel processing;user feature;structured architecture machine;new vlsi chip;new apl interpreter;high-level language interpretation;high level language;chip	SAM I (Structured Architecture Machine) is the prototype of a new VLSI chip set based computer. SAM was designed with High-Level Language interpretation and parallel processing in mind. SAM's first language is a subset of APL. Our paper introduces the SAM I architecture and discusses some unique opportunities for APL implementation made possible by this architecture.	apl;chipset;parallel computing;prototype;very-large-scale integration	Richard F. Hobson;J. D. Hoskin;R. W. Spilsbury	1989		10.1145/75144.75171	chip;parallel processing;computer architecture;parallel computing;computer science;programming language;high-level programming language	Arch	1.4212195216880477	47.64520139657411	44528
c2a68fb5d2f0677c72f3a3b183ae2a9af42c25d3	a new four-moduli set with high speed rns arithmetic unit and efficient reverse converter	residue arithmetic;reverse converter;vlsi architecture	In this paper, a new four-moduli set {22n, 22n+1-1, 2n/2+1, 2n/2-1} for even n is introduced. This moduli set has 5n-bit dynamic range and well-formed moduli which result in a high speed RNS arithmetic unit. Furthermore, an efficient adder-based reverse converter is proposed for this moduli set by using New Chinese remainder theorem 2 (New CRT-II) algorithm. The proposed moduli set has a high speed RNS arithmetic unit and a reverse converter with lower hardware cost and delay in comparison to the recently introduced moduli sets {2n-1, 2n, 2n+1, 22n+1-1} and {2n, 22n+1-1, 2n/2-1, 2n/2+1, 2n+1} with the same dynamic range.	arithmetic logic unit;residue number system	Mohammadreza Noorimehr;Mehdi Hosseinzadeh;Reza Farshidi	2010	IEICE Electronic Express	10.1587/elex.7.1584	arithmetic;electronic engineering;discrete mathematics;mathematics	HCI	15.840696954572566	43.88525586571682	44568
94c430639de852fe704dddc0ca3b1b4fe022586f	stack-based sorting algorithms	programme tri;estudio comparativo;stack;programa ordenacion;pila;algorithme;etude comparative;algorithm;sort routine;analyse performance;comparative study;performance analysis;sorting algorithm;pile memoire;algoritmo;analisis eficacia	The emergence of stacks as a hardware device in stack machines implies the recognition of the importance of using stacks in different computer applications and the need to make use of them in others. This paper uses stacks to solve the sorting problem. Two stack-based sorting algorithms are introduced. The first is based upon sorting by the insertion technique, whereas the second is based upon sorting by the exchange technique. Their analysis and performance are derived when stack computers are used to run them. A comparison study with other sorting algorithms is presented. This study shows that both algorithms have the best performance with a wide margin relative to other sorting algorithms when stack computers are used.	computer;emergence;sorting algorithm;stack (abstract data type);stack machine;stack-oriented programming language	Reda A. Ammar	1989	Journal of Systems and Software	10.1016/0164-1212(89)90042-3	bogosort;parallel computing;stack;sorting network;computer science;external sorting;sorting algorithm;comparative research;programming language;engineering drawing;algorithm;quantum sort	Theory	11.97783347846742	34.86335638126992	44590
3512d72860cfcad68bd8fb8409cc5dbff6ab04e8	rvc: a mechanism for time-analyzable real-time processors with faulty caches	cache;real time;embedded system;time analysis;faults;conference report;timing analysis;embedded	Geometry scaling due to technology evolution as well as Vcc scaling lead to failures in large SRAM arrays such as caches. Faulty bits can be tolerated from the average performance perspective, but make critical realtime embedded systems non time-analyzable or worstcase execution time (WCET) estimations unacceptably large.  This paper proposes a mechanism to tolerate faulty bits in caches while still providing safe and tight WCET. Our solution is based on adapting structures such as the victim cache, cache eviction buffers or miss state handle registers to serve as replacement for faulty cache storage. We show how modest modifications in the hardware help providing safe and tight WCET on the face of permanent faulty bits with negligible impact in power and performance.	best, worst and average case;cpu cache;central processing unit;embedded system;image scaling;real-time clock;run time (program lifecycle phase);static random-access memory;worst-case execution time	Jaume Abella;Eduardo Quiñones;Francisco J. Cazorla;Yiannakis Sazeides;Mateo Valero	2011		10.1145/1944862.1944878	embedded system;parallel computing;real-time computing;cache;computer science;operating system;fault;static timing analysis	Embedded	-3.103887467235173	56.61081334606	44601
f4e208a9235edb297d1e69913b2747799ff0ac3e	lower bounds for distributed coin-flipping and randomized consensus	consensus;impossibility;randomization;distributed algorithm;lower bound	We examine a class of <italic>collective coin-flipping games</italic> that arises from randomized distributed algorithms with halting failures. In these games, a sequence of <italic>local coin flips</italic> is generated, which must be combined to form a single <italic>global coin flip</italic>. An adversary monitors the game and may attempt to bias its outcome by hiding the result of up to <italic>t</italic> local coin flips. We show that to guarantee at most constant bias, ω(<italic>t</italic><supscrpt>2</supscrpt>) local coins are needed, even if (a) the local coins can have arbitrary distributions and ranges, (b) the adversary is required to decide immediately wheter to hide or reveal each local coin, and (c) the game can detect which local coins have been hidden. If the   adversary is permitted to control the outcome of the coin except for cases whose probability is polynomial in <italic>t</italic>, ω(<italic>t</italic><supscrpt>2</supscrpt>/log<supscrpt>2</supscrpt><italic>t</italic>) local coins are needed. Combining this fact with an extended version of the well-known Fischer-Lynch-Paterson impossibility proof of deterministic consensus, we show that given an adaptive adversary, any <italic>t</italic>-resilient asynchronous consensus protocol requires ω(<italic>t</italic><supscrpt>2</supscrpt>/log<supscrpt>2</supscrpt><italic>t</italic>) local coin flips in any model that can be simulated deterministically using atomic registers. This gives the first nontrivial lower bound on the total work required by wait-free consensus and is tight to within   logarithmic factors.	adversary (cryptography);deterministic algorithm;distributed algorithm;emoticon;halting problem;michael j. fischer;non-blocking algorithm;polynomial;randomized algorithm;whole earth 'lectronic link	James Aspnes	1998	J. ACM	10.1145/278298.278304	randomization;distributed algorithm;combinatorics;group testing;consensus;computer science;mathematics;distributed computing;upper and lower bounds;algorithm	Theory	16.783350850110693	34.11587271686688	44647
a83dff1465585c9b361988ab8228e1d14c8dcf0f	an improved reconfigurable finite impulse response filter using common subexpression elimination algorithm for cognitive radio	cognitive radio cr;reconfigurability;spectrum sensing;binary common subexpression elimination bcse;application specific integrated circuits asic		algorithm;cognitive radio;common subexpression elimination	Venkatachalam Nithish Kumar;Koteswara Rao Nalluri;Gopalakrishnan Lakshminarayanan;Mathini Sellathurai	2015	J. Low Power Electronics	10.1166/jolpe.2015.1375	electronic engineering;parallel computing;real-time computing	EDA	12.48428057298039	46.94966399396943	44664
eed05ea58f8ee580ad02fdc3e996fa120b40c2dd	a new bank sensitive drampower model for efficient design space exploration		In systems ranging from mobile devices to servers, Dynamic Random Access Memories (DRAM) have a large impact on performance and contribute a significant part to the total consumed power. Therefore, it is crucial to have an accurate DRAM power model for exhaustive design space explorations, which can handle different types of DRAM devices. In this paper, we present an improved version of the well known DRAMPower model. Our enhanced model is derived and calibrated from real measurements and outperforms pessimistic state-of-the-art DRAM power estimators like the widely used spread sheet provided by Micron.	design space exploration;dynamic random-access memory;experiment;mobile device;random access;repository (version control);simulation;spreadsheet;whole earth 'lectronic link	Matthias Jung;Deepak M. Mathew;Éder F. Zulian;Christian Weis;Norbert Wehn	2016	2016 26th International Workshop on Power and Timing Modeling, Optimization and Simulation (PATMOS)	10.1109/PATMOS.2016.7833700	embedded system;electronic engineering;real-time computing;telecommunications;engineering;operating system	EDA	-0.8530196639841062	60.161697388476995	44709
7a960129b7d7ebb74ee55f11c53655583f6fee1e	rendezvous of many agents with different speeds in a cycle	mobile agents;rendezvous;cycle;speeds	Rendezvous is concerned with enabling $$k \ge 2$$ mobile agents to move within an underlying domain so that they meet, i.e., rendezvous, in the minimum amount of time. In this paper we study a generalization from $$2$$ to $$k$$ agents of a deterministic rendezvous model first proposed by [5] which is based on agents endowed with different speeds. Let the domain be a continuous as opposed to discrete ring cycle of length $$n$$ and assume that the $$k$$ agents have respective speeds $$s_1, \ldots , s_k$$ normalized such that $$\min \{ s_1, \ldots , s_k \} = 1$$ and $$\max \{ s_1, \ldots , s_k \} = c$$. We give rendezvous algorithms and analyze and compare the rendezvous time in four models corresponding to the type of distribution of agents' speeds, namely Not-All-Identical, One-Unique, Max-Unique, All-Unique. We propose and analyze the Herding Algorithm for rendezvous of $$k \ge 2$$ agents in the Max-Unique and All-Unique models and prove that it achieves rendezvous in time at most $$\frac{1}{2}\left \frac{c+1}{c-1}\right n$$, and that this rendezvous is strong in the All-Unique model. Further, we prove that, asymptotically in $$k$$, no algorithm can do better than time $$\frac{2}{c+3}\left \frac{c+1}{c-1}\right n$$ in either model. We also discuss and analyze additional efficient algorithms using different knowledge based on either $$n, k, c$$ as well as when the mobile agents employ pedometers.		Evan Huus;Evangelos Kranakis	2015		10.1007/978-3-319-19662-6_14	real-time computing;simulation;distributed computing;speed	AI	17.745960763859557	34.51139053063579	44759
fb06b063b79076b1901dcd15a314be30bc8f9689	norx8 and norx16: authenticated encryption for low-end systems		This paper presents NORX8 and NORX16, the 8-bit and 16-bit versions of the authenticated cipher NORX, one of the CAESAR candidates. These new versions are better suited for low-end systems—such as “internet of things” devices—than the original 32-bit and 64-bit versions: whereas 32-bit NORX requires 64 bytes of RAM or cache memory, NORX8 and NORX16 require just 16 and 32 bytes, respectively. Both of the low-end variants were designed to retain the security properties of the initial NORX and be fast on small CPUs. Keywords-authenticated encryption, lightweight, CAESAR	16-bit;32-bit;64-bit computing;8-bit;algorithm;authenticated encryption;authentication;byte;cpu cache;central processing unit;cipher;internet of things;random-access memory	Jean-Philippe Aumasson;Philipp Jovanovic;Samuel Neves	2015	IACR Cryptology ePrint Archive		key wrap;56-bit encryption;sponge function;disk encryption theory;parallel computing;client-side encryption;ocb mode;filesystem-level encryption;link encryption;computer science	Crypto	8.315151543600102	44.90004047938214	44777
e23fb232996ab2cb8cb315c0fbae83b0200622e5	ferroelectric fets-based nonvolatile logic-in-memory circuits		Among the beyond-complementary metal–oxide–semiconductor (CMOS) devices being explored, ferroelectric field-effect transistors (FeFETs) are considered as one of the most promising. FeFETs are being studied by all major semiconductor manufacturers, and experimentally, FeFETs are making rapid progress. FeFETs also stand out with the unique hysteretic $I_{text {ds}}$ - $V_{text {gs}}$ characteristic that allows a device to function as both a switch and a nonvolatile (NV) storage element. We exploit this FeFET property to build two categories of fine-grained logic-in-memory (LiM) circuits: 1) ternary content addressable memory (TCAM) which integrates efficient and compact logic/processing elements into various levels of memory hierarchy; 2) basic logic function units for constructing larger and more complex LiM circuits. Two writing schemes (with and without negative supply voltages respectively) for FeFETs are introduced in our LiM designs. The resulting designs are compared with existing LiM approaches based on CMOS, magnetic tunnel junctions (MTJs), resistive random access memories (ReRAMs), ferrorelectric tunnel junctions (FTJs), etc., that afford the same circuit-level functionality. Simulation results show that FeFET-based NV TCAMs offer lower area overhead than MTJ (79%) and CMOS (42% less) equivalents, as well as better search energy-delay products (EDPs) than TCAM designs based on MTJ ( $149times $ ), ReRAM ( $1.7times $ ), and CMOS ( $1.3times $ ) in array evaluations. NV FeFET-based LiM basic circuit blocks are also more efficient than functional equivalents based on MTJs in terms of propagation delay ( $4.2times $ ) and dynamic power ( $2.5times $ ). A case study for an FeFET-based LiM accumulator further demonstrates that by employing FeFET as both a switch and an NV storage element, the FeFET-based accumulator can save area (36%) and power consumption (40%) when compared with a conventional CMOS accumulator with the same structure.		Xunzhao Yin;Xiaoming Chen;Michael T. Niemier;Xiaobo Sharon Hu	2019	IEEE Trans. VLSI Syst.	10.1109/TVLSI.2018.2871119	electronic engineering;content-addressable memory;transistor;propagation delay;mosfet;electronic circuit;computer science;resistive random-access memory;capacitance;cmos	EDA	16.642951699370713	59.418954336703365	44797
3c296792797046631b212aa7f8da72f04d09c7f9	safety-oriented mixed-signal verification of automotive power devices in a uvm environment	automotive engineering;standards;circuit faults;analog safety analysis automotive power devices uvm environment presilicon verification functional safety standards regression based mixed signal verification;shape;photonic band gap;safety;integrated circuit modeling;regression analysis automotive electronics electrical safety mixed analogue digital integrated circuits;circuit faults safety automotive engineering shape photonic band gap integrated circuit modeling standards	The complexity of automotive applications is continuously increasing, leading to a growing demand for methodologies that offer comprehensive mixed-signal verification. However, compared to the highly automated verification methodologies in the digital domain, pre-silicon verification in the analog domain still implies a substantial amount of manual work and computational effort. Apart from this, automotive applications most often have to comply with functional safety standards and therefore their robustness concerning safety-critical faults needs to be proven. This is normally ensured by performing safety verification with faults being purposefully injected into the designs. In this paper we present a methodology that enables a regression-based mixed-signal verification combined with an existing approach for analog safety analysis. Both concepts are applied to an automotive design, in which faults have been injected, in order to demonstrate their capabilities.	bandgap voltage reference;computation;fault injection;mixed-signal integrated circuit;power semiconductor device;requirement;verification and validation	Sebastian Simon;Ozlem Karaca;Jérôme Kirscher;Alexander W. Rath;Georg Pelz;Linus Maurer	2016	2016 13th International Conference on Synthesis, Modeling, Analysis and Simulation Methods and Applications to Circuit Design (SMACD)	10.1109/SMACD.2016.7520718	reliability engineering;embedded system;electronic engineering;engineering	EDA	23.913631256790275	54.94066418666712	44800
ee91c989ccbbd73b635dcab02060dfa3225f9bc7	parallel processing of multichannel video based on multicore architecture	digital signal processing;parallel processing digital signal processing embedded systems image processing multi core architecture;image processing;video signal processing;embedded systems;video signal processing embedded systems multiprocessing systems object detection parallel processing;multiprocessing systems;multi core architecture;parallel processing;digital signal processing multicore processing parallel processing process control streaming media algorithm design and analysis;object detection;embedded systems multichannel video parallel processing multicore architecture nonreal time systems black screen detection algorithm digital image processing	Parallel processing and multi-core architectures are being accepted in all segments of industry caused by the need for better performance in real-time and non-real-time systems. This paper presents an implementation of parallel processing system for multichannel video on a multicore architecture using different building blocks. Black Screen Detection algorithm is used for digital image processing. The implemented system was validated by means of a particular case study. Experimentally obtained results are related to analysis of the system scalability, in terms of processing speed up as a function of the number of cores that participate in the processing. Also, due to the specific memory architecture, the influence of a ping-pong mechanism has been analyzed. Based on these results, the use of multi-core architecture for parallel processing to achieve significantly better performance of the target class of embedded systems is justified.	algorithm;digital image processing;embedded system;experiment;intel core (microarchitecture);multi-core processor;parallel computing;parallel processing (dsp implementation);real-time clock;real-time computing;scalability	Branislav Kordic;Vladimir Marinkovic;Miroslav Popovic;Vukota Pekovic	2013	2013 3rd Eastern European Regional Conference on the Engineering of Computer Based Systems	10.1109/ECBS-EERC.2013.29	embedded system;parallel processing;parallel computing;real-time computing;image processing;computer science;digital signal processing;digital image processing	HPC	1.568671485228658	46.85383059003637	44801
1461164762890041633d9de86d7ff7c8f1a204bd	187 mhz subthreshold-supply charge-recovery fir	digital signal processing;cmos integrated circuits;voltage 0 16 v to 0 36 v finite impulse response filter chip charge recovery logic family cmos process two phase power clock dc supply frequency 5 mhz to 187 mhz size 0 13 mum voltage 0 40 v;performance evaluation;two phase power clock;punto funcionamiento;integrated circuit;filtre reponse impulsion finie;clocks;energy efficient;finite impulse response filter energy efficiency clocks frequency energy consumption voltage circuit testing power supplies circuit synthesis boosting;finite impulse response filter;low power vlsi digital signal processing;circuit vlsi;frequency 5 mhz to 187 mhz;circuito integrado;charge recovery logic family;cmos process;tecnologia mos complementario;digital filter;finite impulse response filter chip;chip;nmos technology;consumo electricidad;filtro respuesta impulsion acabada;vlsi circuit;finite impulse response;low power;technologie nmos;voltage 0 40 v;logic gates;low power electronics clocks cmos logic circuits fir filters;filtro numerico;energy consumption;cmos logic circuits;fir filter;tecnologia nmos;low power vlsi;electric power consumption;low power electronics;mathematical model;digital signal processor;horloge;operating point;voltage 0 16 v to 0 36 v;processeur signal numerique;fir filters;circuito vlsi;procesador senal numerica;technologie mos complementaire;electronique faible puissance;clock;point fonctionnement;consommation electricite;reloj;circuit integre;size 0 13 mum;complementary mos technology;dc supply;filtre numerique	This paper presents a finite impulse response (FIR) filter chip that relies on a charge-recovery logic family to achieve multi-MHz clock frequencies with subthreshold DC supply levels. Fabricated in a 0.13 ¿m CMOS process with Vth,nmos = 0.40 V, the FIR operates with a two-phase power-clock in the 5 MHz-187 MHz range and with DC supplies in the 0.16 V-0.36 V range. Using a single DC supply, the chip achieves its most energy-efficient operating point when resonating at 20 MHz with a 0.27 V supply. Recovering 89 % of the energy supplied to its 57 pF per-phase load, it consumes 15.57 pj per cycle and yields 17.37 nW/Tap/MHz/InBit/ CoeffBit. Using two subthreshold DC supplies at 20 MHz, energy per cycle can be further reduced by 17.1 %, yielding 14.40 nW/Tap/ MHz/InBit/CoeffBit.	8-bit;cmos;clock network;clock rate;computation;crowbar (circuit);electronic circuit;finite impulse response;logic family;nmos logic;operating point;overhead (computing);reversible computing;spice;simulation;two-phase commit protocol	Wei-Hsiang Ma;Jerry C. Kao;Visvesh S. Sathe;Marios C. Papaefthymiou	2010	IEEE Journal of Solid-State Circuits	10.1109/JSSC.2010.2042247	embedded system;electronic engineering;computer science;engineering;electrical engineering;finite impulse response	Mobile	18.176225754400583	55.58554543942871	44841
d40ab34c87af30c05428a2da4972021345e10bef	circuit design techniques for the high-performance cmos ibm s/390 parallel enterprise server g4 microprocessor	microprocessor;optimisation;synthese circuit;concepcion circuito;haute performance;metodologia;optimizacion;circuit design;serveur informatique;tecnologia mos complementario;methodologie;alto rendimiento;servidor informatico;sintesis circuito;optimization;conception circuit;microprocesseur;systeme parallele;parallel system;methodology;technologie mos complementaire;high performance;microprocesador;circuit synthesis;sistema paralelo;complementary mos technology;computer server	This paper describes the circuit design techniques used for the IBM S/390® Parallel Enterprise Server G4 microprocessor to achieve operation up to 400 MHz. A judicious choice of process technology and concurrent top-down and bottom-up design approaches reduced risk and shortened the design time. The use of timing-driven synthesis/placement methodologies improved design turnaround time and chip timing. The combined use of static, dynamic, and self-resetting CMOS (SRCMOS) circuits facilitated the balancing of design time and performance return. The use of robust PLL design, floorplanning, and clock distribution minimized clock skew. Innovative latch designs permitted performance optimization without adding risk. Microarchitecture optimization and circuit innovations improved the performance of timing-critical macros. Full custom array design with extensive use of SRCMOS circuit techniques resulted in an on-chip L1 cache having 2.0-ns cycle time.	cmos;circuit design;microprocessor	Leon J. Sigal;James D. Warnock;Brian W. Curran;Yuen H. Chan;Peter J. Camporese;Mark D. Mayo;William V. Huott;Daniel R. Knebel;Ching-Te Chuang;James P. Eckhardt;Philip T. Wu	1997	IBM Journal of Research and Development	10.1147/rd.414.0489	physical design;embedded system;electronic engineering;real-time computing;computer science;engineering;electrical engineering;operating system;circuit design;methodology;server	EDA	18.23054137015163	55.06023102885645	44867
36ab1ee8544e89865c1c4ee3d23cfbde9338c3a8	discrete event simulation on a mimd parallel computer: algorithm optimization or hardware acceleration?	electronic circuits;analytical models;hardware acceleration;silicon efficient implementation;optimisation;design of algorithms;concurrent computing;transputer systems;mimd parallel computer;application software;event list;abstract data types;vlsi implementation specification;hardware accelerator;abstract data type;transputer;acceleration;design tuning discrete event simulation mimd parallel computer algorithm optimization hardware acceleration silicon efficient implementation resource intensive applications software based approach performance optimization critical abstract data type event list sequential computer parallel computer architectures parallel algorithms general purpose processor transputer application specific circuit asic vlsi implementation specification occam2 architecture verification;general purpose processor;parallel architectures discrete event simulation parallel algorithms parallel machines optimisation transputer systems application specific integrated circuits vlsi abstract data types;circuit simulation;sequential computer;resource intensive applications;computational modeling;parallel computer architecture;parallel architectures;efficient implementation;parallel computer architectures;architecture verification;application specific integrated circuits;queueing model;design tuning;parallel computer;discrete event simulation concurrent computing hardware acceleration application software computational modeling analytical models circuit simulation queueing analysis electronic circuits;vlsi;software based approach;parallel machines;critical abstract data type;occam2;asic;algorithm optimization;application specific circuit;high performance;performance optimization;queueing analysis;hardware;discrete event simulation;parallel algorithms	Discrete event simulation is a widely used technique to analyze systems ranging from queueing models to digital electronic circuits. Except for very sample problems, this type of application is very demanding on computing resources. This problem has traditionally been addressed in two ways. A first software-based approach is aimed at optimizing the performance of the algorithm that implements the critical abstract data type (the event list) on a sequential computer. The second approach is the design of algorithms that are aimed at exploiting the opportunities offered by new (parallel) computer architectures. A comparison of both approaches is made, resulting in the identification of a parallel computer architecture that will significantly improve the performance of discrete event simulation codes. Each node in this architecture consists of a general-purpose processor (a transputer) and an application-specific circuit (ASIC). The specification of the VLSI implementation in Occam2 not only allows an easy verification of the architecture, but also allows one to tune the design so that the goals of high performance and a silicon-efficient implementation can be achieved at the same time. >	algorithm;hardware acceleration;mimd;mathematical optimization;parallel computing;simulation	Erik Dirkx	1993		10.1109/EMPDP.1993.336360	computer architecture;parallel computing;real-time computing;computer science	HPC	1.996830259164948	50.2932426040413	44874
4e392b37ebb51009ff8ea82ae9621090ac51e8a4	a stochastic local hot spot alerting technique	stochastic local hot spot alert;stochastic processes;temperature sensors;kalman filters;integrated circuit modelling;local hot spot;chip junction temperature;stochastic local hot spot;power density;temperature report;thermal management (packaging);power state;chip temperature;markovian decision process;hot spot alert;nanoscale devices;thermal management;markov processes;temperature sensor;hot spot;nanoelectronics;kalman filtering;chip failure;thermal sensors;formal verification;manufacturing;chip;kalman filter;algorithm;uncertainty	With the increasing levels of variability in the behavior of manufactured nano-scale devices and dramatic changes in the power density on a chip, timely identification of hot spots on a chip has become a challenging task. This paper addresses the questions of how and when to identify and issue a hot spot alert. There are important questions since temperature reports by thermal sensors may be erroneous, noisy, or arrive too late to enable effective application of thermal management mechanisms to avoid chip failure. This paper thus presents a stochastic technique for identifying and reporting local hot spots under probabilistic conditions induced by uncertainty in the chip junction temperature and the system power state. More specifically, it introduces a stochastic framework for estimating the chip temperature and the power state of the system based on a combination of Kalman Filtering (KF) and Markovian Decision Process (MDP) model. Experimental results demonstrate the effectiveness of the framework and show that the proposed technique alerts about thermal threats accurately and in a timely fashion in spite of noisy or sometimes erroneous readings by the temperature sensor.	gnu nano;hotspot (wi-fi);junction temperature;kalman filter;sensor;spatial variability;stochastic process;thermal management of high-power leds	Hwisung Jung;Massoud Pedram	2008	2008 Asia and South Pacific Design Automation Conference		kalman filter;control engineering;stochastic process;embedded system;electronic engineering;simulation;computer science;engineering;statistics	EDA	22.209595701555127	58.52376941010042	44884
f4a6250c60cbb10fcb0f900f472e82ff3009bf48	improving the dependability of amr sensors used in automotive applications		Electronic systems are replacing mechanical parts to improve the safety and performance of the cars. However, the electronic components should be dependable, meaning they should be trusted to work properly over time. AMR sensors are widely used in automotive for angle measurements. Nevertheless, they are affected by performance degradation and catastrophic faults. Both should be handled to guarantee the correct operation of the sensor over time. This paper proposed two modules, fault-tolerant, and self-calibration. Results show they allow to improve the dependability of the sensor.	adaptive multi-rate audio codec;catastrophic interference;dependability;electronic component;elegant degradation;fault tolerance;sensor	Andreina Zambrano;Hans G. Kerkhoff	2017	2017 22nd IEEE European Test Symposium (ETS)	10.1109/ETS.2017.7968243	fault tolerance;real-time computing;computer science;automotive industry;dependability;reliability engineering;electronic component	Embedded	7.026275508387217	58.35465588156228	44920
2d2d1989a15f132694aef1e5c4a7af3c41033998	new power index model for switching power analysis from adder graph of fir filter	dynamic power simulations;switching activity;power analysis;fir filters adders application specific integrated circuits circuit optimisation;power efficiency;finite impulse response filter;design flow;switching power index;fluid flow measurement;design optimization;computational modeling;multiplier block;global positioning system;application specific integrated circuits;glitch path count;adders;fir filter;power dissipation;indexation;asic design flow;integrated circuit modeling;optimization;glitch path score;capacitance;fir filters;adder graph;digital circuits;circuit optimisation;switching power analysis;switches;optimal algorithm;load modeling;power modeling;algorithm design and analysis;asic design flow fir filter switching power index switching power analysis adder graph digital circuits optimization multiplier block glitch path count glitch path score dynamic power simulations;power measurement;finite impulse response filter adders algorithm design and analysis global positioning system digital circuits design optimization capacitance application specific integrated circuits power measurement fluid flow measurement	Efficient power modeling of a generic class of digital circuits is crucial to the analysis and development of optimization algorithms of power efficient design. This paper proposes a new switching power index model for the power analysis of multiplier-block based FIR filter. Unlike the existing Glitch Path Count (GPC) and Glitch Path Score (GPS), the proposed Power Index (PI) model takes into account correlated input switching activity propagations of full adders within and across adders of different widths, as well as the variation of load capacitances due to sharing of adders in reduced adder graph. Dynamic power simulations of several benchmark filters in an ASIC design flow show that this PI measure is more closely correlated with the actual dynamic power dissipation than the existing GPC and GPS models.	adder (electronics);algorithm;application-specific integrated circuit;benchmark (computing);cmos;computer;digital electronics;finite impulse response;glitch;global positioning system;mathematical optimization;simulation	Jiajia Chen;Chip Hong Chang;Hanhua Qian	2009	2009 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2009.5118233	electronic engineering;real-time computing;computer science;electrical engineering;finite impulse response;control theory;mathematics	Arch	16.898208381743583	54.78002197849954	44936
3a09612d2e173f33e23fef4236a97219936d48d3	redefine: runtime reconfigurable polymorphic asic	resource utilization;polymorphic asic;honeycomb;supercomputer education research centre;data exchange;logic element;application synthesis;chip;fpga implementation;instruction set extension;low power;runtime reconfiguration;polymorphism;router;handheld device;dataflow software pipeline;software pipelining;noc;stream processing;custom instruction extension;high level language;communication pattern;data transfer	Emerging embedded applications are based on evolving standards (e.g., MPEG2/4, H.264/265, IEEE802.11a/b/g/n). Since most of these applications run on handheld devices, there is an increasing need for a single chip solution that can dynamically interoperate between different standards and their derivatives. In order to achieve high resource utilization and low power dissipation, we propose REDEFINE, a polymorphic ASIC in which specialized hardware units are replaced with basic hardware units that can create the same functionality by runtime re-composition. It is a “future-proof” custom hardware solution for multiple applications and their derivatives in a domain. In this article, we describe a compiler framework and supporting hardware comprising compute, storage, and communication resources.  Applications described in high-level language (e.g., C) are compiled into application substructures. For each application substructure, a set of compute elements on the hardware are interconnected during runtime to form a pattern that closely matches the communication pattern of that particular application. The advantage is that the bounded CEs are neither processor cores nor logic elements as in FPGAs. Hence, REDEFINE offers the power and performance advantage of an ASIC and the hardware reconfigurability and programmability of that of an FPGA/instruction set processor.  In addition, the hardware supports custom instruction pipelining. Existing instruction-set extensible processors determine a sequence of instructions that repeatedly occur within the application to create custom instructions at design time to speed up the execution of this sequence. We extend this scheme further, where a kernel is compiled into custom instructions that bear strong producer-consumer relationship (and not limited to frequently occurring sequences of instructions). Custom instructions, realized as hardware compositions effected at runtime, allow several instances of the same to be active in parallel. A key distinguishing factor in majority of the emerging embedded applications is stream processing. To reduce the overheads of data transfer between custom instructions, direct communication paths are employed among custom instructions.  In this article, we present the overview of the hardware-aware compiler framework, which determines the NoC-aware schedule of transports of the data exchanged between the custom instructions on the interconnect. The results for the FFT kernel indicate a 25% reduction in the number of loads/stores, and throughput improves by log(n) for n-point FFT when compared to sequential implementation. Overall, REDEFINE offers flexibility and a runtime reconfigurability at the expense of 1.16× in power and 8× in area when compared to an ASIC. REDEFINE implementation consumes 0.1× the power of an FPGA implementation. In addition, the configuration overhead of the FPGA implementation is 1,000× more than that of REDEFINE.	application-specific integrated circuit;basic block;cmos;cpu power dissipation;central processing unit;code::blocks;compiler;control flow;customer relationship management;directed acyclic graph;embedded system;fabric os;faraday cage;fast fourier transform;field-programmable gate array;high- and low-level;high-level programming language;instruction pipelining;interconnection;interoperability;iteration;library (computing);microarchitecture;mobile device;network on a chip;overhead (computing);pipeline (computing);producer–consumer problem;reconfigurability;router (computing);run time (program lifecycle phase);stream processing;streaming media;thread-local storage;throughput;xiii	Mythri Alle;Keshavan Varadarajan;Alexander Fell;C. Ramesh Reddy;Joseph Nimmy;Saptarsi Das;Prasenjit Biswas;Jugantor Chetia;Adarsha Rao;S. K. Nandy;Ranjani Narayan	2009	ACM Trans. Embedded Comput. Syst.	10.1145/1596543.1596545	chip;data exchange;software pipelining;honeycomb;embedded system;polymorphism;computer architecture;in situ resource utilization;parallel computing;real-time computing;stream processing;computer science;operating system;mobile device;network operations center;programming language;high-level programming language	Arch	1.7991581708911193	48.81175143883416	44960
68b6682aceebc432138027f8e373d7e97032a6bf	applying graphics processor acceleration in a software defined radio prototyping environment	libraries;benchmarking;nvidia geforce gtx 260;protocols;graphics processing unit acceleration pipelines kernel multicore processing libraries parallel processing;kernel;paper;software defined radio;gnu radio;design flow;stand alone gpu accelerated library;design space;software defined radio prototyping environment;coprocessors;acceleration;software radio;cuda;software radio coprocessors protocols;dataflow foundation;development environment;stand alone gpu accelerated library graphics processor acceleration software defined radio prototyping environment protocols multicore platforms gnu radio dataflow foundation;signal processing;multicore processing;pipelines;graphics processors;nvidia;multicore platforms;graphics processing unit;parallel processing;graphics processor acceleration	With higher bandwidth requirements and more complex protocols, software defined radio (SDR) has ever growing computational demands. SDR applications have different levels of parallelism that can be exploited on multicore platforms, but design and programming difficulties have inhibited the adoption of specialized multicore platforms like graphics processors (GPUs). In this work we propose a new design flow that augments a popular existing SDR development environment (GNU Radio), with a dataflow foundation and a stand-alone GPU accelerated library. The approach gives an SDR developer the ability to prototype a GPU accelerated application and explore its design space fast and effectively. We demonstrate this design flow on a standard SDR benchmark and show that deciding how to utilize a GPU can be non-trivial for even relatively simple applications.	benchmark (computing);central processing unit;dataflow;design flow (eda);etsi satellite digital radio;gnu radio;graphics processing unit;integrated development environment;multi-core processor;parallel computing;prototype;requirement;scheduling (computing)	William Plishker;George F. Zaki;Shuvra S. Bhattacharyya;T. Charles Clancy;John Kuykendall	2011	2011 22nd IEEE International Symposium on Rapid System Prototyping	10.1109/RSP.2011.5929977	embedded system;parallel processing;computer architecture;parallel computing;computer science;operating system;signal processing;software-defined radio	Arch	-0.6600663913210987	46.06980664544442	44967
a0be1cd89e152dd52e906833b9165f9fbb287457	concurrent error detection in a polynomial basis multiplier over gf(2m)	multiplier;tecnologia electronica telecomunicaciones;fault tolerant;concurrent error detection;systolic array;finite fields arithmetic;fault tolerant computing;cryptography;fault detection;finite field arithmetic;tecnologias;grupo a;polynomial basis	Eliminating cryptographic computation errors is vital for preventing attacks. A simple approach is to verify the correctness of the cipher before outputting it. The multiplication is the most significant arithmetic operation among the cryptographic computations. Hence, a multiplier with concurrent error detection ability is urgently necessary to avert attacks. Employing the re-computing shifted operand concept, this study presents a semi-systolic array polynomial basis multiplier with concurrent error detection with minimal area overhead. Moreover, the proposed multiplier requires only two extra clock cycles while traditional multipliers using XOR trees consume at least ⌈ log2 m ⌉ extra XOR gate delays in GF(2m) fields.	binary logarithm;cipher;clock signal;computation;correctness (computer science);cryptography;error detection and correction;exclusive or;operand;overhead (computing);polynomial basis;semiconductor industry;systolic array;xor gate	Chiou-Yng Lee;Che Wun Chiou;Jim-Min Lin	2006	J. Electronic Testing	10.1007/s10836-006-7446-9	embedded system;finite field arithmetic;electronic engineering;real-time computing;systolic array;computer science;cryptography;theoretical computer science;mathematics;multiplier;algorithm;statistics	Logic	13.872301402914072	45.11531551214255	44985
d14c7d1871c0b48ca7dce7b4f927382a9e855019	video coding for decoding power-constrained embedded devices	multimedia;video coding;data storage;computer hardware;video	ABSTRACT Low power dissipation and fast processing time are crucial requirements for embedded multimedia devices. This paper presents a technique in video coding to decrease the power consumption at a standard video decoder. Coupled with a small dedicated video internal memory cache on a decoder, the technique can substantially decrease the amount of data traffic to the external memory at the de coder. A decrease in data traf fic to the external memory at decoder will result in multiple benefits: faster real-time processing and power savings. The encoder, given prior knowledge of the decoders dedicated video internal memory cache management scheme, regulates its choice of motion compensated predictors to reduce the decoders external memory accesses. This technique can be used in any standard or proprietary encoder scheme to generate a compliant output bit stream decodable by standard CPU-based and dedicated hardware-based decoders for power savings with the best quality-power cost trade off. Our simulation results show that with a relatively small amount of dedicated video internal memory cache, the technique may decrease the traffic between CPU and external memory over 50%. Keywords : Video Coding, Low Power, Embedded Systems	embedded system	Ligang Lu;Vadim Sheinin	2004		10.1117/12.538250	embedded system;real-time computing;computer hardware;computer science;video processing	ECom	13.302368315672203	40.36375074273753	45079
a249545c486e4bf285f1422d413c76de322b10b3	towards new metrics for high-performance computing resilience		Ensuring the reliability of applications is becoming an increasingly important challenge as high-performance computing (HPC) systems experience an ever-growing number of faults, errors and failures. While the HPC community has made substantial progress in developing various resilience solutions, it continues to rely on platform-based metrics to quantify application resiliency improvements. The resilience of an HPC application is concerned with the reliability of the application outcome as well as the fault handling efficiency. To understand the scope of impact, effective coverage and performance efficiency of existing and emerging resilience solutions, there is a need for new metrics. In this paper, we develop new ways to quantify resilience that consider both the reliability and the performance characteristics of the solutions from the perspective of HPC applications. As HPC systems continue to evolve in terms of scale and complexity, it is expected that applications will experience various types of faults, errors and failures, which will require applications to apply multiple resilience solutions across the system stack. The proposed metrics are intended to be useful for understanding the combined impact of these solutions on an application's ability to produce correct results and to evaluate their overall impact on an application's performance in the presence of various modes of faults.	algorithm;computation;correctness (computer science);dependability;fault model;numerical linear algebra;programming paradigm;rf modulator;radio frequency;solver;supercomputer;transistor	Saurabh Hukerikar;Rizwan A. Ashraf;Christian Engelmann	2017		10.1145/3086157.3086163	fault tolerance;psychological resilience;systems engineering;reliability engineering;engineering;supercomputer	HPC	4.854908537797257	58.31683975233508	45107
c80e0048a0975b394efd2daf01f3d926972e2f1b	a fast scalar multiplication algorithm based on alternate-zeckendorf representation.		This paper proposed a new method to point scalar multiplication on elliptic curve and it is defined in a finite field with a characteristic greater than 3. It is based on the Transformed Fibonacci type sequence like (2P + Q) and it can resist the Simple Power Attack (SPA). Although the sequence quite easier to calculate, expressing any k using the sequence remains a very difficult problem, so we proposed the Alternate-Zeckendorf representation and given the proof of this view.The NewADD algorithm is also added to the new algorithm and in meanwhile we also listed (2P + Q) results as a table to reduce the computation cost.The performance comparisons show that our algorithm is less costly than other algorithms 12.7 % to 27.9 % at least.	computation;multiplication algorithm;scalar processor	Shuang-Gen Liu;Xue-Jing Sun	2018	I. J. Network Security		computer network;theoretical computer science;scalar multiplication;computer science	Comp.	9.173834610404866	42.6689754723894	45134
8cdb198d05d3d17ed275fb8b75cbd74b42695afd	triangulating molecular surfaces on multiple gpus	gpu;gaussian surfaces;triangulations;molecular surfaces	Current GPU-based workstations are inadequate to triangulate and rendering large molecular datasets with thousands and hundreds of thousands, not to say millions, of atoms. The problem is not so the lack of processing power, but the memory limitations of current GPU graphics cards. For example, the NVidia GeForce GTX 590 graphics card comes with two 1.5GB GPUs. We tackle here this problem using a OpenMP-CUDA solution that runs on a loosely-coupled GPU cluster. Basically, we propose a fast, scalable, parallel triangulation algorithm for molecular surfaces that takes advantage of multicore processors of CPUs and GPUs of modern hardware architectures, where each CPU core works as the master of a single GPU, being the processing burden distributed over the CPU cores available in a single computer or a cluster. As much as we know, this is the first marching cubes algorithm that triangulates molecular surfaces on multiple GPUs using CUDA and OpenMP.	accessible surface area;algorithm;cuda;central processing unit;gpu cluster;geforce 500 series;geforce 600 series;graphics processing unit;marching cubes;multi-core processor;olap cube;openmp;scalability;video card;workstation	Sérgio Dias;Abel J. P. Gomes	2013		10.1145/2488551.2488582	parallel computing;computer science;theoretical computer science;computer graphics (images)	HPC	-1.714749295713108	41.630881898073895	45157
5abbd7406dbe5c38840e36d7b1aac95737c9b43f	mapping reference code to irregular dsps within the retargetable, optimizing compiler cogen(t)	code generation;optimizing compiler;embedded systems;genetic algorithms	Generating high quality code for embedded processors is made difficult by irregular architectures and highly encoded parallel instructions. Rather than deal with the target machine at every stage of the compilation, a promising new methodology employs generic algorithms to optimize code for an idealized abstraction of the true target machine. This code, called reference code, is then mapped to the real instruction set by enhanced genetic algorithms. One perturbs the original schedule to find a number of alternative (parallel) instruction sequences, and the other evolves feasible register assignments, if possible, for each sequence. This paper describes the strategy for mapping idealized code into actual code. The COGEN(T) system employs this methodology to produce good code for different commercial DSPs and ASIPs.	application-specific instruction set processor;artificial neural network;central processing unit;code generation (compiler);computation;cryptographic service provider;daniel goossens;digital signal processor;display resolution;embedded system;enhanced graphics adapter;genetic algorithm;high-level synthesis;lagrangian relaxation;mathematical optimization;michael garey;motorola 68000;np-completeness;neural networks;optimizing compiler;parallel computing;processor design;processor register;qr code;scheduling (computing);shake;strong generating set	Gary William Grewal;Thomas Charles Wilson	2001		10.1145/563998.564024	dead code;self-modifying code;computer architecture;parallel computing;real-time computing;genetic algorithm;object code;computer science;operating system;redundant code;programming language;code generation;threaded code;unreachable code;source code	Arch	0.039151501980163414	51.691195536218544	45163
db967e93b1d3fca5e9cd5bb7a21142d608f581ed	clock buffer polarity assignment utilizing useful clock skews for power noise reduction	libraries;clocks;space exploration;inverters;clocks time factors inverters libraries switches space exploration search problems;logic design buffer circuits clocks delay circuits;time factors;search problems;switches;clock pa algorithm clock buffer polarity assignment clock skews power noise reduction clock trees clock signal clock sink clock polarity assignment techniques clock noise skew constraints delay variation environment ispd 10 benchmark circuits	Clock trees, which deliver the clock signal to every clock sink in the whole system, are one of the most active components on a chip which makes them one of the most dominant sources of noise. While many clock polarity assignment (PA) techniques were proposed to mitigate the clock noise, no attention has been paid to the PA under useful skew constraints. In this work, we show that the clock PA problem under useful skew constraints is intractable and propose a comprehensive and scalable clique search based algorithm to solve the problem effectively. In addition, we demonstrate the applicability of our solution by effectively extending it for PA under delay variation environment. Through experiments with ISPD'10 benchmark circuits, it is shown that our proposed clock PA algorithm is able to reduce the peak noise by 10.9% further over that of the conventional global skew bound constrained PA.	algorithm;benchmark (computing);clock signal;clock skew;experiment;noise reduction;pa-risc;scalability	Deokjin Joo;Taewhan Kim	2016	2016 21st Asia and South Pacific Design Automation Conference (ASP-DAC)	10.1109/ASPDAC.2016.7428015	clock synchronization;embedded system;electronic engineering;real-time computing;asynchronous circuit;clock angle problem;vector clock;clock domain crossing;clock skew;network switch;computer science;space exploration;self-clocking signal;timing failure;clock drift;synchronous circuit;matrix clock;clock gating;digital clock manager;clock signal;cpu multiplier	EDA	17.218741685869986	53.028387344582754	45236
41a9642ab9100591fb0f7a057869c3c5256d26b6	fault-tolerant processor interconnection networks	fault tolerant;interconnection network	A s suitable topology for interconnection networks of multiprocessors, De Bruijn graphs have been proposed and a number of investigations have been conducted on their fault tolerance. A De Bruijn graph is a directed graph with maximum degree d (the maximum number of links that can be connected to one processor), diameter k (maximum number of repeaters between two processors) and number of nodes dk (number of processors). graph with maximum degree d , diameter k and number of nodes dk + dk-1. smallest diameter among the graphs with maximum degree d. following: If d 2 nodes in a De Bruijn graph are removed (if d 2 processors are faulty), its diameter becomes d + 1, which is only 1 larger than the original diameter. @ moved, its diameter becomes k + 1 and if d 1 nodes are removed, the diameter is k + 2 or less, which is at most 2 larger than the original diameter. @ The values of @ and @ are at most 1 larger than the lower bound under the limitation of degree d. @ A fault-tolerant routing algorithm can be realized easily. This algorithm is better than any existing algorithm because of its short routes. A Kautz graph is a directed	central processing unit;de bruijn graph;degree (graph theory);dijkstra's algorithm;directed graph;fault tolerance;interconnection;routing	Makoto Imase;Terunao Soneoka;Keiji Okada	1986	Systems and Computers in Japan	10.1002/scj.4690170803	fault tolerance;combinatorics;discrete mathematics;degree;graph toughness;computer science;mathematics;distributed computing	Theory	23.351506641125216	34.788579066176446	45316
a8ac36c28b6a7334346a08f734ef11e76be513b5	an improved fft architecture optimized for reconfigurable application specified processor	computer architecture signal processing algorithms algorithm design and analysis discrete fourier transforms digital signal processing radar applications software algorithms;reconfigurable architectures digital arithmetic fast fourier transforms optimisation;radix 2 4 8 butterfly unit fast fourier transformation fft architecture optimization reconfigurable application specified processor radix 8 fft algorithm	This paper presents an efficient architecture for computing 16 points to 1M points FFT(Fast Fourier Transformation) with a new FFT architecture based on mixed radix 2/4/8 butterfly unit. Taking advantage of the radix-8 FFT algorithm the proposed FFT architecture reduced the computation level while remaining compatible with sequences whose source data length is 2n. Furthermore, some optimizations for reconfigurable application specified processor is developed. First, we propose a separated radix 2/4/8 butterfly unit which is more flexible than an entire radix 2/4/8 butterfly unit; Second, for the sequences longer than 128k points, an efficient 2D FFT computation solution is proposed. This FFT architecture is implemented in a prototype chip of reconfigurable application specified processor. Our architecture requires only 676 us and 7.4 ms for 128k points FFT and 1M points FFT respectively. Compared to the existing DSP processor GPGPU, the proposed performance approach improved in different degrees.	algorithm;cmos;computation;digital signal processor;fast fourier transform;general-purpose computing on graphics processing units;parallel computing;prototype;reconfigurable computing;scalability;source data;tik;tik-tok;twiddle factor	Feng Han;Li Li;Kun Wang;Fan Feng;Hongbing Pan;Dong Yu	2015	2015 IEEE 11th International Conference on ASIC (ASICON)	10.1109/ASICON.2015.7517201	fast fourier transform;computer architecture;parallel computing;split-radix fft algorithm;computer science;theoretical computer science;prime-factor fft algorithm	EDA	11.570816480995838	44.4026616348347	45317
69beecfc6ec130ebf2bb0e6e886956956f571af0	mixed gates: leakage reduction techniques applied to switches for networks-on-chip	network on chip		network switch;spectral leakage;system on a chip	Frank Sill;Claas Cornelius;Stephan Kubisch;Dirk Timmermann	2006			chip;parallel computing;leakage (electronics);network on a chip;computer science	Logic	14.138718012805546	56.40687432418073	45330
51ac6520ff383dc3063b6a01d122b266f2330cc3	an improved constant-time algorithm for computing the radon and hough transforms on a reconfigurable mesh	radon transforms;time complexity constant time algorithm reconfigurable mesh data bus o 1 time;image processing;time complexity;efficient algorithm;image processing transforms aerospace electronics computer science computer vision nasa very large scale integration equations international collaboration councils;indexing terms;image processing radon transforms hough transforms computational complexity;computer vision;computational complexity;hough transforms;hough transform	The Hough transform is an important problem in image processing and computer vision. Recently, an efficient algorithm for computing the Hough transform has been proposed on a reconfigurable array [12]. For a problem with an p N p N image and an n n parameter space, the algorithm runs in a constant time on a threedimensional (3-D)n n N reconfigurable mesh where the data bus is N1=c-bit wide. To our best knowledge, this is the most efficient constanttime algorithm for computing the Hough transform on a reconfigurable mesh. In this paper, an improved Hough transform algorithm on a reconfigurable mesh is proposed. For the same problem, our algorithm runs in constant time on a 3-Dn n p N p N reconfigurable mesh, where the data bus is onlylogN -bit wide. In most practical situations, n = O( p N). Hence, our algorithm requires much less VLSI area to accomplish the same task. In addition, our algorithm can compute the Radon transform (a generalized Hough transform) in O(1) time on the same model, whereas the algorithm in [12] cannot be adapted to computing Radon transform easily.	algorithm;computer vision;generalised hough transform;image processing;reconfigurable computing;time complexity;very-large-scale integration	Yi Pan;Keqin Li;Mounir Hamdi	1999	IEEE Trans. Systems, Man, and Cybernetics, Part A	10.1109/3468.769762	hough transform;time complexity;computer vision;index term;image processing;computer science;theoretical computer science;computational complexity theory	Theory	11.621348770763676	35.5897324767766	45369
8800929df0b576f308c16f87f9f6c08c962eb5d2	switch-level timing models in the mos simulator brasil	cmos integrated circuits;circuit analysis computing;cmos;nmos;clock skew;dynamic hazards;faulty aspect ratios;races;signal waveforms;switch level timing models;timing faults	New timing models have been developed and implemented in the switch-level timing simulator BRASIL which are able to give fairly accurate signal waveforms. This enables the detection of faulty aspect ratios, dynamic hazards and races and timing faults caused by clock skew in NMOS and CMOS circuits. In contrast to most existing switch-level timing simulators the algorithm is not restricted to tree structures of the active subnetworks.	algorithm;cmos;clock skew;nmos logic;simulation	H. Warmers;D. Sass;Ernst-Helmut Horneber	1990			embedded system;electronic engineering;real-time computing;computer science;engineering;cmos;static timing analysis	EDA	20.797321158926536	55.444902482213884	45375
70e0ca0e02cb751481d0f9f423426aac1af87022	system support for micro-harvester powered mobile sensing	glove device system support microharvester powered mobile sensing microharvesting self sustainable sensing systems mobile healthcare applications renewable sources sensing systems indoor light driven wearable glove device accelerometers flex sensors hand gesture recognition two fold contribution microharvester driven mobile sensing systems indoor light scavenging hardware logic wakeup controllers microcontroller bluetooth device ultra low energy constraints gesture recognition hand gesture driven home automation system;microcontrollers;sensors;paralysis patients micro harvesting tiered architecture gesture recognition;sensors accelerometers bluetooth energy harvesting gesture recognition microcontrollers;energy harvesting;micro harvesting;tiered architecture;sensors bluetooth gesture recognition lighting batteries computer architecture hardware;bluetooth;accelerometers;paralysis patients;gesture recognition	Micro-harvesting from sources such as indoor light can enable a plethora of self-sustainable sensing systems for mobile healthcare applications. However, given the minuscule and variable amount of energy harvested from these renewable sources, practical sensing systems powered by micro-harvesting is today limited to light driven motion sensing. In this paper, we design, implement, and evaluate an indoor light driven wearable glove device that uses flex sensors and accelerometers for hand gesture recognition. Through the design, we make a two-fold contribution to micro-harvester driven mobile sensing systems. First, motivated by extensive profiling of panels for indoor light scavenging, we design a harvester that multiplexes panels of different compositions to maximally scavenge energy as a function of lighting conditions. Second, we present a tiered architecture composed of application specific hardware logic, wakeup controllers, a general purpose micro-controller, and a bluetooth device that can adapt to variable and ultra-low energy constraints, and at the same time provide high responsiveness and compute capability for gesture recognition. We evaluate the glove device in the context of a hand gesture driven home automation system for the elderly.	bioinformatic harvester;bluetooth;cyber-physical system;electronic hardware;end-to-end principle;gesture recognition;home automation;microcontroller;multiplexing;network switch;responsiveness;sensor;smartphone;transmitter;wearable computer	Alexander Nelson;Jackson Schmandt;William Wilkins;James P. Parkerson;Nilanjan Banerjee	2013	2013 IEEE 34th Real-Time Systems Symposium	10.1109/RTSS.2013.33	microcontroller;embedded system;simulation;computer hardware;computer science;sensor;operating system;gesture recognition;bluetooth;energy harvesting;accelerometer	Embedded	2.94673791084616	34.55846430576067	45394
544e049685b442849992bacfdc933fd9ac699d9c	design flow of a digital ic: the role of digital ic\/soc design in ce products	digital circuits digital ic design digital icisoc design consumer electronics products ce products analog mixed signal soc ams soc;routing;resource management;layout;multiplexing;integrated circuits layout ip networks resource management multiplexing routing;ip networks;integrated circuits;system on chip integrated circuit design integrated circuit reliability mixed analogue digital integrated circuits	The Central Module of Consumer Electronics (CE) products is a miniature-sized IC that finds wide-spectrum applicability from kitchen appliances to automobiles, aircraft, or any embedded systems. The system in these modern CE products is built as an analog/mixed-signal SOC (AMS-SOC). In a typical case, the digital circuits are the main computational modules, whereas the analog or mixed-signal components are interfacing circuits. Proficient design of digital ICs has become more important because it is one of the significant driving factors of efficient system design in this current mobile electronics era. The various factors involved during IC design, such as speed, power, and reliability, are shown in Figure 1.	design flow (eda);digital electronics;embedded system;integrated circuit design;mixed-signal integrated circuit;system on a chip;systems design;verilog-ams	Anirban Sengupta	2016	IEEE Consumer Electronics Magazine	10.1109/MCE.2016.2516108	mixed-signal integrated circuit;layout;embedded system;routing;real-time computing;ic layout editor;computer science;resource management;multiplexing;integrated circuit design	EDA	9.80692025552394	55.130992647912535	45428
8a8b1a6c4aa75302c0e0b94ccb70048b6fc36c96	majority logic gate synthesis approaches for post-cmos logic circuits: a review	logic gates cmos logic circuits logic design;logic gates boolean functions computers quantum dots inverters standards benchmark testing;majority logic gate synthesis approaches dense circuit integration levels tunneling phase logic single electron transistor quantum dot cellular automata fundamental logic gates post cmos logic circuits	Majority gates are fundamental logic gates that can be implemented in several potential post-CMOS technologies such as Quantum-dot Cellular Automata, Single Electron Transistor, and Tunneling Phase Logic. It is expected that these technologies provide more dense circuit integration levels than current CMOS technology. In this paper, a review of existing synthesis techniques targeted for majority logic gates are presented and compared.	cmos;computer-aided design;electron;electronic circuit;integrated circuit;logic gate;logic synthesis;majority function;quantum dot cellular automaton;transistor;tunneling protocol	Srinivasa Vemuru;Mohammed Y. Niamat	2014	IEEE International Conference on Electro/Information Technology	10.1109/EIT.2014.6871778	and-or-invert;electronic engineering;nmos logic;logic synthesis;logic optimization;diode–transistor logic;logic level;logic gate;logic family;three-input universal logic gate;programmable logic array;theoretical computer science;programmable logic device;pass transistor logic;mathematics;sequential logic;diode logic;digital electronics;pmos logic;algorithm;resistor–transistor logic	EDA	15.557298663775097	57.88135840856338	45431
9c9b014e6f65b0b39db3811d84fdc08e5afc9105	going parallel over the rainbow	multi-threading;multiprocessing systems;parallel algorithms;actual scaling;multicore machines;multithreaded programming;multithreaded syntax;multivariate binomial lattices;multivariate lattices models;multivariate trinomial lattices;parallel algorithms;parallel computation;parallelization algorithms;rainbow options;aptech gauss;openmp;derivatives pricing;multivariate binomial/trinomial lattices;pthreads	Some parallel algorithms are proposed to speed up computation of multivariate lattices used to evaluate rainbow options. These novel algorithms have been programmed using multi-threaded syntax in Gausstm of Aptech honing codes for 32 and 128 cores machines. Actual scaling is linear or superlinear in excess of Amdahl theoretical thresholds. Rainbow options have been valued with several multivariate lattices models programmed according to our parallelization algorithms providing results not found in extant literature in terms of granularity and convergence. Keywords: Rainbow Options, multivariate binomial lattices, multivariate trinomial lattices, parallel computation, multithreaded programming, multicore machines.	amdahl's law;binomial options pricing model;code;computation;control flow;flynn's taxonomy;graphics processing unit;http 404;image scaling;iteration;multi-core processor;offset binary;parallel algorithm;parallel computing;scalability;speedup;thread (computing);trinomial;value (ethics)	Giuseppe Alesii	2014	2014 International Conference on High Performance Computing & Simulation (HPCS)	10.1109/HPCSim.2014.6903764	thread;supercomputer;parallel computing;computer science;posix threads;theoretical computer science;operating system;distributed computing;programming language	HPC	-3.7089873513586884	40.94538619739858	45502
9dfc5dbe4f88c040d4b27af05730036d5860cde9	technology mapping with all spin logic		This work is the first to propose a technology mapping algorithm for All Spin Logic (ASL) device. The ASL is the most actively-pursed one among spintronic devices which themselves fall under emerging post-CMOS nano-technologies. We identify the shortcomings of directly applying classical technology mapping with ASL devices, and propose techniques to extend it to handle these shortcomings. Our results show that our ASL-aware technology mapping algorithm can achieve on-average 9.15% and up to 27.27% improvement in delay (when optimizing delay) with slight improvement in area, compared to the solution generated by classical technology mapping. In a broader sense, our results show the need for developing circuit-level CAD tools that are aware of and optimized for emerging nano-technologies in order to better assess their promise as we move to the post-CMOS era.	algorithm;cmos;computer-aided design;gnu nano;spintronics	Boyu Zhang;Azadeh Davoodi	2017	Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017		embedded system;electronic engineering;logic gate;computer science;electrical engineering;pattern matching;programming language;algorithm	EDA	15.39993990738539	58.00175699466696	45520
04d7187857606e802368884b07548c33c20a9ebf	constrained piecewise polinomial approximation for hardware implementation of elementary functions	look up table;interpolation;piecewise linear approximation;piecewise polynomial techniques;hardware description languages;polynomial interpolation;linear approximation;mixed integer linear programming optimization algorithm;hardware polynomials piecewise linear approximation table lookup signal processing algorithms application software piecewise linear techniques design engineering matlab approximation algorithms;polynomials;polynomial interpolators;function approximation;integer programming;linear interpolators;elementary functions;linear programming;quadratic interpolators;approximation methods;table lookup;table lookup hardware description languages integer programming interpolation linear programming piecewise polynomial techniques;hardware implementation;constrained piecewise polynomial approximation;polynomial approximation;hardware;mixed integer linear programming optimization algorithm constrained piecewise polynomial approximation hardware implementation elementary functions polynomial interpolators look up table linear interpolators quadratic interpolators	This paper presents a novel technique for designing piecewise polynomial interpolators for hardware implementation of elementary functions. In the proposed approach, we impose special constraints between polynomial coefficients of adjacent segments. This allows to significantly reduce look-up table size with respect to standard, unconstrained piecewise polynomial approximations, with negligible reduction in accuracy. The reduction of look-up table size improves performances in terms of area and speed. Implementations of linear and quadratic interpolators for the reciprocal function f(x)=1/x are presented and analyzed as an application example in the paper.	approximation;coefficient;elementary function;lookup table;performance;polynomial	Antonio Giuseppe Maria Strollo;Davide De Caro;Nicola Petra;Ettore Napoli;Valeria Garofalo	2008	2008 15th IEEE International Conference on Electronics, Circuits and Systems	10.1109/ICECS.2008.4674949	mathematical optimization;combinatorics;discrete mathematics;mathematics;piecewise	EDA	14.0447438337341	44.281280160503435	45533
043754504ad818a0fcc76b8d6c6104f92039f7e2	partial gathering of mobile agents in asynchronous unidirectional rings	distributed system;gathering problem;mobile agent;partial gathering	In this paper, we consider the partial gathering problem of mobile agents in asynchronous unidirectional rings equipped with whiteboards on nodes. The partial gathering problem is a new generalization of the total gathering problem. The partial gathering problem requires, for a given integer g, that each agent should move to a node and terminate so that at least g agents should meet at the same node. The requirement for the partial gathering problem is weaker than that for the (well-investigated) total gathering problem, and thus, we have interests in clarifying the difference on the move complexity between them. We propose three algorithms to solve the partial gathering problem. The first algorithm is deterministic but requires unique ID of each agent. This algorithm achieves the partial gathering in O(gn) total moves, where n is the number of nodes. The second algorithm is randomized and requires no unique ID of each agent (i.e., anonymous). This algorithm achieves the partial gathering in expected O(gn) total moves. The third algorithm is deterministic and requires no unique ID of each agent. For this case, we show that there exist initial configurations in which no algoThe conference version of this paper is published in the proceedings of 16th International Conference on Principles of Distributed Systems (OPODIS 2012). This work was supported by JSPS KAKENHI Grant Numbers 24500039, 24650012, 25104516, 26280022, and 26330084. ∗Corresponding author. Tel.:+81 6 6879 4117. Fax: +81 6 6879 4119. Email addresses: m-sibata@ist.osaka-u.ac.jp (Masahiro Shibata), f-oosita@ist.osaka-u.ac.jp (Fukuhito Ooshita), kakugawa@ist.osaka-u.ac.jp (Hirotsugu Kakugawa), masuzawa@ist.osaka-u.ac.jp (Toshimitsu Masuzawa) Preprint submitted to Theoretical Computer Science June 1, 2015 rithm can solve the problem and agents can achieve the partial gathering in O(kn) total moves for solvable initial configurations, where k is the number of agents. Note that the total gathering problem requires Ω(kn) total moves, while the partial gathering problem requires Ω(gn) total moves in each model. Hence, we show that the move complexity of the first and second algorithms is asymptotically optimal.	asymptotically optimal algorithm;decision problem;email;emoticon;existential quantification;fax;leader election;lexicographical order;mobile agent;node (computer science);randomized algorithm;software agent;terminate (software);theoretical computer science	Masahiro Shibata;Shinji Kawai;Fukuhito Ooshita;Hirotsugu Kakugawa;Toshimitsu Masuzawa	2016	Theor. Comput. Sci.	10.1016/j.tcs.2015.09.012	computer science;theoretical computer science;mobile agent;distributed computing;algorithm	Theory	17.556459467144816	33.70405885196651	45556
e0ed3eb4fc12cf395f49bc92814f6a7e48f5d6bd	a fault simulation method based on stem regions	benchmark circuits circuit area reduction processing steps reduction fault simulation method stem regions combinational circuits static reduction dynamic reduction fault coverage parallel pattern evaluation technique efficient implementation;fault location circuit analysis computing combinatorial circuits digital simulation;fault simulation;combinatorial circuits;efficient implementation;fault coverage;circuit faults circuit simulation combinational circuits fault detection electrical fault detection laboratories analytical models performance analysis;combinational circuit;circuit analysis computing;digital simulation;fault location	The concept of stem regions has been used as a framework for a fast fault simulator for combinational circuits. The concept allows a static reduction of the circuit area of explicit analysis, for single-output as well as multiple-output circuits. A dynamic reduction of processing steps is also achieved as the fault simulation progresses and fault coverage increases. Both the static and dynamic reductions are fully compatible with the parallel pattern evaluation technique, resulting in a very efficient implementation. The simulation algorithm is described, and experimental results for well-known benchmark circuits are shown. >	simulation	Fadi Maamari;Janusz Rajski	1988		10.1109/ICCAD.1988.122487	electronic engineering;parallel computing;real-time computing;fault coverage;fault indicator;computer science;stuck-at fault;automatic test pattern generation;combinational logic;algorithm	Logic	19.999853626239457	49.650701729175644	45588
c3d17942cbd591407b725838d680d30f87d48e16	on the program size of perfect and universal hash functions	silicon;finite element methods;computer languages;complexity theory;cost function;time measurement;time complexity;measurement units;length measurement;upper bound;manganese;artificial neural networks;large scale integration;dictionaries;cost function dictionaries length measurement measurement units time measurement computer languages upper bound;upper and lower bounds;hash function;radio access networks	We address the question of program size of of perfect and universal hash functions. We prove matching upper and lower bounds (up to constant factors) on program size. Furthermore, we show that minimum or nearly minimum size programs can be found efficiently. In addition, these (near) minimum size programs have time complexity at most O(log* N) where N is the size of the universe in the case of perfect hashing, and time complexity 0(1) in the case of universal hashing. Thus for universal hashing programs of minimal size and minimal time complexity have been found.	perfect hash function;time complexity;universal hashing	Kurt Mehlhorn	1982	23rd Annual Symposium on Foundations of Computer Science (sfcs 1982)	10.1109/SFCS.1982.80	hash table;combinatorics;discrete mathematics;hash function;linear hashing;perfect hash function;dynamic perfect hashing;computer science;consistent hashing;theoretical computer science;universal hashing;mathematics;k-independent hashing;upper and lower bounds;2-choice hashing;artificial neural network;algorithm	Theory	15.516648615400937	32.493935093219655	45607
1d1b0195b28cf0a3ebc0a515675e2af6913aacdb	cycle-efficient lineary feedback shift register implementation on word-based micro-architecture	microcontrollers;generators;scrambler linear feedback shift register iteration bound vector processing look ahead transformation;linear feedback shift registers;linear feedback shift register;polynomials;iteration bound;vectors;circuit feedback;shift registers;polynomials generators throughput parallel processing linear feedback shift registers vectors program processors;scrambler;look ahead transformation;arm 9 simulator cycle efficient lineary feedback shift register word based microarchitecture term preserving look ahead transformation teplat bit serial linear feedback shift register algorithm lfsr algorithm bit parallel formulation look ahead algorithm transformation approach word based microprocessor development platforms texas instrument c6416 code composition simulator;program processors;shift registers circuit feedback microcontrollers microprocessor chips;vector processing;parallel processing;microprocessor chips;throughput	A novel algorithm transformation method, called term-preserving look-ahead transformation (TePLAT) is proposed to transform the bit-serial linear feedback shift register (LFSR) algorithm into a bit-parallel formulation which promises order of magnitudes improvement of execution speed compared to the traditional look-ahead algorithm transformation approach. TePLAT is applied to 26 commonly used LFSRs and tested on two popular word-based micro-processor development platforms: a Texas Instrument C6416 Code Composition Simulator and an ARM-9 Simulator. In all 26 cases, TePLAT transformed implementations consistently deliver much higher throughput than those implementations based on traditional look-ahead algorithm transformation.	algorithm;linear-feedback shift register;microarchitecture;microprocessor;serial communication;throughput	Jui-Chieh Lin;Sao-Jie Chen;Yu Hen Hu	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288203	microcontroller;parallel processing;throughput;vector processor;parallel computing;real-time computing;computer science;theoretical computer science;shift register;linear feedback shift register;polynomial	EDA	9.976809091438628	44.88335538179652	45626
9f631cd564b38d87eb33de12657b2b7de69119a7	towards accelerating generic machine learning prediction pipelines		Machine Learning models are often composed by sequences of transformations. While this design makes easy to decompose and accelerate single model components at training time, predictions requires low latency and high performance predictability whereby end-to-end runtime optimizations and acceleration is needed to meet such goals. This paper shed some light on the problem by using a production-like model, and showing how by redesigning model pipelines for efficient execution over CPUs and FPGAs performance improvements of several folds can be achieved.	baseline (configuration management);central processing unit;design space exploration;dictionary;end-to-end principle;field-programmable gate array;high-level synthesis;lookup table;machine learning;norm (social);pipeline (computing);scheduling (computing);throughput	Alberto Scolari;Yunseong Lee;Markus Weimer;Matteo Interlandi	2017	2017 IEEE International Conference on Computer Design (ICCD)	10.1109/ICCD.2017.76	latency (engineering);real-time computing;field-programmable gate array;parallel computing;predictability;computer science;machine learning;artificial intelligence	EDA	-0.6369280204896944	49.58649479310952	45631
214ceb8059387402a60b6f755e3d02ad824e316b	netrace: dependency-driven trace-based network-on-chip simulation	performance monitoring;network on chip;chip multiprocessor;network simulator;chip;evaluation methodology;system on chip;networks on chip;systems on chip;system simulation;hardware counters;analytical model	Chip multiprocessors (CMPs) and systems-on-chip (SOCs) are expected to grow in core count from, a few today to hundreds or more. Since efficient on-chip communication is a primary factor in the performance of large core-count systems, the research community has directed substantial attention to networks-on-chip (NOCs). Current NOC evaluation methodologies include analytical modeling, network simulation, and full-system simulation. However, as core count and system complexity grow, the deficiencies of each of these methods will limit their ability to meet the demands of developers and researchers. Developing efficient NOCs requires high-fidelity, low-overhead NOC evaluation techniques and metrics. To address these challenges, this paper describes a new trace-based network simulation methodology that captures dependencies between network messages observed in full-system simulation of multithreaded applications. We also introduce Netrace, a library of tools and traces that enables targeted NOC simulators to track and replay network messages and their dependencies.	computer architecture simulator;network on a chip;overhead (computing);simulation;system on a chip;thread (computing);tracing (software)	Joel Hestness;Boris Grot;Stephen W. Keckler	2010		10.1145/1921249.1921258	embedded system;parallel computing;real-time computing;computer science;network on a chip	Arch	-0.45943063098665343	56.80217754387239	45692
bd8a44f6c8507f3909e1403736d0d12590045fca	energy consumption breakdown of a modern mobile platform under various workloads	integrated circuit;energy efficient;resource allocation;mobile computing system;power breakdown;systems analysis mobile computing power aware computing resource allocation;power aware computing;workload analysis mobile computing system high power dissipation integrated circuits energy efficiency on power source component wise energy consumption breakdown energy contribution modern mobile platform;mobile platform;systems analysis;energy consumption;workload analysis;benchmark testing electric breakdown energy consumption batteries graphics power demand mobile communication;workload analysis power breakdown energy consumption mobile platform;mobile computing;high power	Advancements in today's mobile computing systems are mainly constrained by the high power dissipation of integrated circuits. Enhancing the energy efficiency of these systems has become a pressing challenge due to their dependency on batteries as the main power source. In this paper, we present a component-wise energy consumption breakdown of a modern platform under a variety of applications and benchmarks. This is an essential step in directing future research towards the power-hungry components and in providing a better understanding of the system behavior. Our results demonstrate a substantial variation in the energy contribution of various components as well as the total system energy depending on the nature of the workload.	benchmark (computing);integrated circuit;mobile computing;mobile operating system	Faisal Hamady;Ali Chehab;Ayman I. Kayssi	2011	2011 International Conference on Energy Aware Computing	10.1109/ICEAC.2011.6136684	embedded system;electronic engineering;real-time computing;engineering	EDA	-3.7900178979604076	56.43164146183413	45775
5a13ae4b0bb7ba6ca4b68d7dbc8c4890ae3cc2bd	an interconnect-aware dynamic voltage scaling scheme for dsm vlsi	voltage control;cmos integrated circuits;dynamic voltage scaling very large scale integration voltage control delay power system modeling clocks cmos logic circuits semiconductor device modeling batteries power system interconnection;dsm vlsi;interconnections;dynamic voltage scaling;performance of systems;wire;low power;energy consumption;low power electronics;interconnect aware dynamic voltage scaling scheme;vlsi cmos integrated circuits interconnections low power electronics;supply voltage selection;mathematical model;vlsi;4 section global clock distribution network;capacitance;deep sub micron;clock distribution network;high performance;4 section global clock distribution network interconnect aware dynamic voltage scaling scheme dsm vlsi deep sub micron cmos supply voltage selection;deep sub micron cmos	Dynamic Voltage Scaling (DVS) is a successful design solution that addresses the challenges associated with low-power/energy and high-performance design in Deep Sub Micron (DSM) CMOS. In DSM, VLSI systems have become interconnect-centric; correspondingly, the associated design solutions should be adapted to preserve their functionality. In reference to this concern, and with respect to DVS, we propose a DVS scheme that takes interconnect effects into account. The proposed DVS scheme is a generalization of existing methods that treat systems as pure logic. To support this DVS scheme, two design metrics are introduced. These metrics model the performance of system components subject to DVS, based on the proportion of their delay due to interconnects. Based on the proposed design metrics, a compact delay model and a method for supply voltage selection are proposed. The limit of scaling for hazard-free system operation in VLSI systems is further formulated. It is shown that this limit can be smaller than the one dictated by the process technology. The proposed DVS scheme is applied to a 4-section global clock distribution network. Reported results show that this scheme improves both the timing accuracy and energy consumption aspects of DVS by 25% and 30% on average, respectively.	cmos;clock signal;dynamic voltage scaling;image scaling;low-power broadcasting;mpsoc;programmable logic device;spice 2;scaling limit;speaker wire;very-large-scale integration	Houman Zarrabi;Asim J. Al-Khalili;Yvon Savaria	2010	Proceedings of 2010 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2010.5537106	embedded system;electronic engineering;real-time computing;computer science;engineering;electrical engineering;mathematical model;capacitance;very-large-scale integration;cmos;low-power electronics	EDA	16.75314680305336	56.43537190380946	45793
237aa40c930b8a1e4fb0039582b68e988f153a22	hardware accelerated constrained random test generation	software;evaluation performance;systeme passif;interconnection;methode essai;performance evaluation;integrated circuit;canal bus;hardware accelerated simulation;logiciel;evaluacion prestacion;hardware description languages;accelerated test;systemverilog;canal colector;test bench;circuito integrado;hardware accelerator;banco prueba;random testing;logic testing hardware description languages logic cad;satisfiability;design verification language;interconexion;layered structure;ensayo acelerado;ibm coreconnect protocol systemverilog design verification language constrained random test generation hardware accelerated simulation random value selection arm amba bus;interconnexion;logic testing;arm amba bus;bus channel;ibm coreconnect protocol;logicial;constrained random test generation;passive system;random value selection;test method;essai accelere;banc essai;logic cad;circuit integre;sistema pasivo;metodo ensayo	Recent design and verification languages, such as SystemVerilog, support a rich test bench language, which provides significant support towards developing layered, structured, constrained random test bench architectures. Typically, the test bench language offers many features that are not synthesisable and therefore cannot be carried into the hardware for hardware accelerated simulation. One of the main challenges in improving the performance of hardware accelerated simulation is to run the task of random value selection under specified constraints in hardware. This problem (possibly for the first time) is addressed and a two-step approach is presented. In the first step, the constraints are pre-processed in software to generate a set of entailed regions. In the second step, random value selection is performed in hardware using the entailed regions pre-computed in the first step. It is shown that this method has modest area overhead and produces constraint satisfying random valuations within very few cycles. Results on test bench architectures for the ARM AMBA Bus and IBM CoreConnect protocol suites have been reported.	coreconnect;overhead (computing);precomputation;protocol stack;simulation;systemverilog;test bench	Bhaskar Pal;Arnab Sinha;Pallab Dasgupta;P. P. Chakrabarti;Kaushik De	2007	IET Computers & Digital Techniques	10.1049/iet-cdt:20070016	random testing;embedded system;parallel computing;real-time computing;hardware acceleration;telecommunications;computer science;operating system;integrated circuit;interconnection;hardware description language;test method;algorithm;statistics;satisfiability	EDA	3.655749896554137	51.08299863884305	45809
5584a056af027225bf0b75919e0598e7ac0dc333	a memory efficient fine grain scalability coefficient encoding method for h.264/avc scalable video extension	scalable video;memory management;h 264 avc scalable video extension;simulation;video coding;macroblock fine grain scalability coefficient encoding method h 264 avc scalable video extension frame based mechanism hardware implementation;automatic voltage control;scalability encoding automatic voltage control bandwidth hardware video coding static var compensators entropy displays spatial resolution;proceedings paper;mobile communication;frame based mechanism;scalability;video coding encoding;external memory;encoding;hardware implementation;macroblock;fine grain scalability coefficient encoding method;hardware	In this paper, a memory efficient Fine Grain Scalability (FGS) coefficient encoding method is proposed to reduce the external memory access requirement. In the H.264/AVC Scalable Video Extension, the FGS coefficients encoding is frame based. However, the frame based mechanism results in the difficulty of hardware implementation due to large internal memory requirements and external memory accesses. Therefore, a non-uniform memory size design which can achieve low external memory access is proposed to realize the macroblock based FGS coefficients encoding. Compared to previous work, our proposed method can save at least 38KB external memory accesses per frame in average.	coefficient;computer data storage;h.264/mpeg-4 avc;kilobyte;macroblock;requirement;scalability;simulation;whole genome sequencing	Meng-Wei Shen;Gwo-Long Li;Tian-Sheuan Chang	2009	2009 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2009.5118442	uniform memory access;interleaved memory;parallel computing;real-time computing;scalability;mobile telephony;computer science;theoretical computer science;operating system;macroblock;encoding;memory management	EDA	12.522601849761507	39.6246514919144	45810
ad826f2f2354219ccc92254b1d60d0c4e60a5648	computational balance in real-time cyclic spectral analysis	general and miscellaneous mathematics computing and information science;real time;spectrum;computational bottlenecks computational balance real time cyclic spectral analysis computational complexity complex multipliers multiprocessor cyclic spectrum analyzers hardware;fast fourier transforms computational complexity spectral analysis real time systems;spectral analysis data flow computing arithmetic distributed computing delay computational complexity signal analysis signal detection parameter estimation computer architecture;data analysis;computational complexity;computer calculations;algorithms 990200;fast fourier transforms;spectral analysis;parallel processing;mathematics and computers;real time systems;spectra unfolding	Real-time cyclic spectral analysis is useful in many applications, but is difficult to achieve because of its computational complexity. This paper studies the distribution of complex multipliers in multiprocessor cyclic spectrum analyzers, with the objective of obtaining computational balance. Computationally balanced implementations efficiently use hardware so that computational bottlenecks are reduced and a smooth flow of data between computational sections of the analyzer is maintained. Tables are presented that give the number of complex multipliers required in each section of the analyzer to obtain computational balance.	computation;computational complexity theory;dataflow;multiprocessing;real-time clock;spectral density estimation;spectrum analyzer	Randy S. Roberts;Herschel H. Loomis	1994		10.1109/ICASSP.1994.389846	spectrum;parallel processing;fast fourier transform;computer science;theoretical computer science;computational resource;data analysis;computational complexity theory;computational model;asymptotic computational complexity;algorithm	HPC	8.480095291481344	38.939508928817794	45851
4b2edce32abf3b6460cc15f72582948087071f53	real-time kernel in hardware rtu: a step towards deterministic and high-performance real-time systems	processor architecture;kernel;performance evaluation;application software;hardware real time unit;time model;real time;real time kernel;computer architecture;registers;application specific integrated circuits;circuit testing;high performance real time systems;real time kernel coprocessor;asic;kernel hardware real time systems delay application specific integrated circuits circuit testing application software registers field programmable gate arrays switches;field programmable gate arrays;time model real time kernel hardware real time unit high performance real time systems real time kernel coprocessor asic spl mu s domain;operating system kernels;switches;high performance;μs domain;hardware;real time systems	Demands on real-time kernels increase every year: as applications grow larger and become more complex, real-time kernels must give short and predictable response. The RTU is a real-time kernel coprocessor implemented in an ASIC, which is intended to meet the growing expectations on real-time kernels. Since the RTU gives fast response times, it is necessary to define a detailed time-model to improve the understanding of the real-time kernel behaviour in the /spl mu/S-domain. In this paper, we describe how to use the RTU in a single-or multi-processor architecture and a time-model for a RTU based real-time system is defined. The time-model is considered with regards to determinism and performance.	benchmark (computing);central processing unit;context switch;diff utility;embedded system;interrupt;kernel (operating system);memory management;multiprocessing;peripheral;real-time clock;real-time computing;real-time operating system;real-time transcription;remote terminal unit	Joakim Adomat;Johan Furunäs;Lennart Lindh;Johan Stärner	1996		10.1109/EMWRTS.1996.557849	embedded system;real-time computing;computer hardware;computer science;operating system;application-specific integrated circuit	Embedded	-3.611526387741635	51.49378831575404	45884
4a698a7d573ce9c51726cd3c113a799281579b98	generalized rotate sort on mesh-connected computers with multiple broadcasting using fewer processors	rotate sort;global bus systems;multiple broadcasting;high dimension;mesh connected computers;parallel algorithms	In this paper, we first present an O(log n) time sorting algorithm on 3-D mesh-connected computers with multiple broadcasting (abbreviated to MCCMB) using n1/2×n1/2×n1/2 processors. Our algorithm is derived from rotate sort. Further, we also show that the result can be extended to k-dimensional MCCMB of size to sort n data items in O(7k−3 log n) time, for k≥3. The algorithm proposed is optimal speed-up while k is any constant. The contribution of this paper is to show that the proposed algorithm can be run in a higher dimensional MCCMB and using fewer processors but keeps the same time complexity as O(log n).		Chin Fu Lin;Shi-Jinn Horng;Tzong-Wann Kao	1995	International Journal of High Speed Computing	10.1142/S0129053395000282	merge sort;proxmap sort;pigeonhole sort;counting sort;parallel computing;computer science;theoretical computer science;sorting algorithm;block sort;in-place algorithm;comparison sort;distributed computing;parallel algorithm;stooge sort	Arch	12.865104535267786	33.1940495381041	45912
01d826a9a9a739fd7fe43604190b101c2fa1af80	error correction based on verification techniques	error correction circuit simulation permission signal processing automatic test pattern generation combinational circuits computer errors data structures boolean functions binary decision diagrams;formal verification;error correction;fully sis optimized benchmark circuits verification techniques error correction combinational circuit multiple errors internal equivalent pairs dynamic support back substitution circuit equivalence;combinational circuit	In this paper, we address the problem of correcting a combinational circuit that is an incorrect implementation of a given specification. Most existing error-correction approaches can only handle circuits with certain types of errors. Here, we propose a general approach that can correct a circuit with multiple errors without assuming any error model. We identify internal equivalent pairs to narrow down the possible error locations using local BDD’s with dynamic support. We also employ a technique called back-substitution to correct the circuit incrementally. This approach can also be used to verify circuit equivalence. The experimental results of correcting fully SISoptimized benchmark circuits with a number of injected errors will be presented.	benchmark (computing);combinational logic;error detection and correction;logic gate;triangular matrix;turing completeness	Shi-Yu Huang;Kuang-Chien Chen;Kwang-Ting Cheng	1996		10.1145/240518.240566	equivalent circuit;boolean circuit;electronic engineering;error detection and correction;formal verification;computer science;theoretical computer science;combinational logic;circuit extraction;programming language;algorithm	EDA	19.644123275836638	48.337387303423945	45932
c4d648ee8cc85fd002f47890a349def16d84d3d7	criticality and sensitivity analysis for incremental performance optimization of asynchronous pipelines		Asynchronous methodologies gain increasing adoption in modern IC design to overcome synchronization limitations. There has been recent work optimizing asynchronous pipeline performance based on static performance analysis (SPA). Despite its linear-time complexity, SPA remains inefficient for large designs that undergo an excessive number of incremental optimization iterations. In this paper, we investigate the performance criticality and sensitivity of asynchronous pipelines, and propose incremental SPA for iterative optimization with buffer insertion. Experimental results show that our method achieves average runtime improvement by two orders of magnitude. For circuits with a wide range of delay distributions among their pipeline modules, our method can reduce pipeline cycle time to a level not achievable by prior methods while inserting significantly fewer buffers.	iteration;mathematical optimization;pipeline (computing);pipeline (software);pre-charge;profiling (computer programming);scalability;self-organized criticality;speedup;time complexity	Chun-Hong Shih;Jie-Hong Roland Jiang	2017	2017 23rd IEEE International Symposium on Asynchronous Circuits and Systems (ASYNC)	10.1109/ASYNC.2017.17	parallel computing;order of magnitude;synchronization;electronic circuit;real-time computing;criticality;asynchronous communication;integrated circuit design;engineering;pipeline transport	EDA	16.387491607384987	53.32860992162246	45962
7cf323c0dfcce6ee0f28438be7009799ad34084e	alternative least mean square adaptive filter architectures for implementation on field programmable gate arrays	adaptive filters;digital filters;field programmable gate arrays;least mean squares methods;low-power electronics;fpga implementation;fpga resource usage;least mean square adaptive filter architectures;power consumption	The Least Mean Square (LMS) adaptive filter is a simple well behaved algorithm which is commonly used in applications where a system has to adapt to its environment. Architectures including the direct, transposed and hybrid forms are examined in terms of the following criteria: speed, power consumption and FPGA resource usage. Both the transposed and hybrid forms, which are derived from the delayed LMS, allow for higher speeds without significant increases in power or area. Results for both these adaptations are independent of filter length with the maximum speed of the 16 tap transposed form being over 4 times greater than the speed of a 16 tap direct form implementation. For FPGA implementation, the transposed form is optimal, as power and area are not significantly greater than values found for the direct form, despite the higher maximum frequency. Even at greater numbers of taps, the maximum frequency of the transposed form is not degraded, despite the input data bus driving an increased number of multipliers.	adaptive filter;algorithm;field-programmable gate array;least mean squares filter	Sinead Mullins;Conor Heneghan	2002	2002 11th European Signal Processing Conference		embedded system;electronic engineering;computer hardware;computer science	HPC	13.63132684816205	45.90082235304404	45998
76d46df9200b6bf5e3813e2bc2f0e262da1a7e25	on designing universal logic blocks and their application to fpga design	concepcion asistida;field programmable gate array;look up table;computer aided design;concepcion circuito;realisation circuit;puerta logica;metodologia;boolean functions;implementation;logic design field programmable gate arrays table lookup logic gates boolean functions logic functions wiring prototypes logic programming routing;circuit design;red puerta programable;reseau porte programmable;methodologie;cmos logic circuits field programmable gate arrays logic gates logic cad boolean functions circuit cad integrated circuit design;porte logique;ejecucion;integrated circuit design;logic gates;circuit realization;realizacion circuito;cmos logic circuits;fpga architecture;conception assistee;three input lookup table cells universal logic blocks fpga design logic function programmable cell universal logic gate actel 2;conception circuit;circuit cad;field programmable gate arrays;methodology;logic cad;logic gate	We present a general methodology to determine the logic function of a programmable cell. It is based on the concept of universal logic gate (ULG) that is capable of being configured to a given set of functions. The cells studied here can be configured to the desired functionality by applying input permutation, negation, bridging or constant assignment, or output negation. One application of this technique is to select an appropriate programmable cell structure for FPGA architecture. The Actel 2 and the three-input look-up table cells are studied and compared to the cell that has been designed using the approach described here. Experimental results suggest that the new cell behaves as well as the Actel 2 cell in terms of logic power, but requires substantially less area and wiring overhead.	boolean algebra;bridging (networking);cell (microprocessor);field-programmable gate array;logic gate;lookup table;overhead (computing);wiring	Chih-Chang Lin;Malgorzata Marek-Sadowska	1997	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.631214	electronic engineering;logic gate;programmable logic array;computer science;theoretical computer science;computer aided design;simple programmable logic device;algorithm;field-programmable gate array	EDA	12.902514872377713	49.349757845154016	46103
8849caffcd547a10bbb8b74c8cdc98f20a1a7fe3	an effective probability distribution sat solver on reconfigurable hardware		Boolean Satisfiability (SAT) is an important problem both theoretically and for a variety of practical applications. While the general SAT problem is NP complete, advanced solver algorithms and heuristics can provide fast and efficient solving of otherwise intractable problems. While much advancement has been made with Conflict Driven Clause Learning (CDCL) based sequential solvers, Stochastic Local Search (SLS) solvers such as WalkSAT, Sparrow and probSAT have proven effective for certain instance types. SLS solvers are well suited to parallelization and hardware implementation due to the simplified control flow and lack of data dependencies between solver instances started with different seeds. This paper presents a hardware implementation of the probSAT algorithm using High-Level Synthesis (HLS) for rapid porting of the design from the original C implementation. Specifically, the presented approach shows very strong performance on the class of small, but difficult SAT problems with speedups between 89–828x over MiniSAT and 5–99x over the software implementation of probSAT on such problems.	algorithm;analysis of algorithms;benchmark (computing);boolean satisfiability problem;clock rate;computer memory;conflict-driven clause learning;control flow;correctness (computer science);data dependency;field-programmable gate array;hardware description language;heuristic (computer science);high-level synthesis;hybrid memory cube;np-completeness;parallel computing;profiling (computer programming);prototype;reconfigurable computing;solver;standard sea level;thread (computing);throughput;walksat	Ali Asgar Sohanghpurwala;Peter M. Athanas	2016	2016 International Conference on ReConFigurable Computing and FPGAs (ReConFig)	10.1109/ReConFig.2016.7857150	parallel computing;computer science;theoretical computer science;algorithm	EDA	1.955534915827401	40.57918311596098	46237
424adf2456825dfb961d98b8475354bc8c4a98d4	balancing performance and reliability in the memory hierarchy	cache storage;reliability benchmark testing cache storage error analysis microprocessor chips performance evaluation;reliability;performance evaluation;unprotected first level caches;mean time to failure;cache memory;spec2000 benchmark suite cosmic ray induced soft errors cache memories microprocessor based systems mean time to failure unprotected first level caches;error analysis;cosmic ray induced soft errors;spec2000 benchmark suite;error rate;microprocessor based systems;memory hierarchy;soft error;benchmark testing;cache memory redundancy error correction codes error analysis hardware protection single event upset read write memory reliability engineering computer errors;microprocessor chips;cache memories;cosmic ray	Cosmic-ray induced soft errors in cache memories are becoming a major threat to the reliability of microprocessor-based systems. In this paper, we present a new method to accurately estimate the reliability of cache memories. We have measured the MTTF (mean-time-to-failure) of unprotected first-level (L1) caches for twenty programs taken from SPEC2000 benchmark suite. Our results show that a 16 KB first-level cache possesses a MTTF of at least 400 years (for a raw error rate of 0.002 FIT/bit.) However, this MTTF is significantly reduced for higher error rates and larger cache sizes. Our results show that for selected programs, a 64 KB first-level cache is more than 10 times as vulnerable to soft errors versus a 16 KB cache memory. Our work also illustrates that the reliability of cache memories is highly application-dependent. Finally, we present three different techniques to reduce the susceptibility of first-level caches to soft errors by two orders of magnitude. Our analysis shows how to achieve a balance between performance and reliability	benchmark (computing);cosmic;cpu cache;mean time between failures;memory hierarchy;microprocessor;soft error	Hossein Asadi;Vilas Sridharan;Mehdi Baradaran Tahoori;David R. Kaeli	2005	IEEE International Symposium on Performance Analysis of Systems and Software, 2005. ISPASS 2005.	10.1109/ISPASS.2005.1430581	least frequently used;benchmark;cache-oblivious algorithm;parallel computing;cache coloring;mean time between failures;cpu cache;soft error;cosmic ray;word error rate;computer science;cache invalidation;reliability;cache algorithms;cache pollution	Arch	7.06006360761318	60.28272224358795	46300
42b07dc48b94cb56f8fe82c30d3f3b6dc71bb9e5	incremental, iterative data processing with timely dataflow		We describe the timely dataflow model for distributed computation and its implementation in the Naiad system. The model supports stateful iterative and incremental computations. It enables both low-latency stream processing and high-throughput batch processing, using a new approach to coordination that combines asynchronous and fine-grained synchronous execution. We describe two of the programming frameworks built on Naiad: GraphLINQ for parallel graph processing, and differential dataflow for nested iterative and incremental computations. We show that a general-purpose system can achieve performance that matches, and sometimes exceeds, that of specialized systems.	batch processing;computation;dataflow;distributed computing;general-purpose modeling;graph (abstract data type);high-throughput computing;iterative and incremental development;iterative method;state (computer science);stream processing;throughput	Derek Gordon Murray;Frank McSherry;Michael Isard;Rebecca Isaacs;Paul Barham;Martín Abadi	2016	Commun. ACM	10.1145/2983551	parallel computing;real-time computing;computer science;distributed computing;programming language	OS	-4.489417261916075	42.67070749181882	46330
96799331cac559de5cd32487b041446952168e60	battery-free sensing platform for wearable devices: the synergy between two feet	foot biomedical monitoring footwear backscatter sensors legged locomotion energy harvesting;bluetooth battery free sensing platform wearable devices synergy kinetic energy ambient backscatter communication energy harvesting power management circuits ambient backscatter module;energy harvesting bluetooth	Recent years have witnessed the prevalence of wearable devices. Wearable devices are intelligent and multifunctional, but they rely heavily on batteries. This greatly limits their application scope, where replacement of battery or recharging is challenging or inconvenient. We note that wearable devices have the opportunity to harvest energy from human motion, as they are worn by the people as long as being functioning. In this study, we propose a battery-free sensing platform for wearable devices in the form-factor of shoes. It harvests the kinetic energy from walking or running to supply devices with power for sensing, processing and wireless communication, covering all the functionalities of commercial wearable devices. We achieve this goal by enabling the whole system running on the harvested energy from two feet. Each foot performs separate tasks and two feet are coordinated by ambient backscatter communication. We instantiate this idea by building a prototype, containing energy harvesting insoles, power management circuits and ambient backscatter module. Evaluation results demonstrate that the system can wake up shortly after several seconds' walk and have sufficient Bluetooth throughput for supporting many applications. We believe that our framework can stir a lot of useful applications that were infeasible previously.	ambient backscatter;bluetooth;form factor (design);kinesiology;multi-function printer;overhead (computing);power management;power network design (ic);prototype;shoes;smartphone;synergy;throughput;wearable computer;wearable technology	Qianyi Huang;Yan Mei;Wei Wang;Qian Zhang	2016	IEEE INFOCOM 2016 - The 35th Annual IEEE International Conference on Computer Communications	10.1109/INFOCOM.2016.7524543	embedded system;simulation	Mobile	2.7749391110936177	34.15297491838811	46333
84de5f18a07cba0eb255fa1f15ea6bc49f2ebf93	the giotto system: a parallel computer for image processing	vision ordenador;architecture systeme;parallel algorithm;image processing;ordinateur parallele;procesamiento imagen;procesador panel;array processor;traitement image;computer vision;single instruction multiple data;processeur tableau;robot vision;machine vision;industrial robots;performance analysis;ordenador paralelo;parallel computer;arquitectura sistema;arquitectura modular;vision ordinateur;system architecture;modular architecture;architecture modulaire	This paper presents the GIOTTO system, a parallel computer based on a scalable single instruction, multiple data (SIMD) array of processors specially conceived for image processing. The system is characterized by a reduced-size array and a novel organization of the memory subsystem, designed to support transparent processing of images larger than the array. The system is designed to meet the computational requirements of machine vision, together with the compactness, ease of integration and flexibility called for by industrial robotic environments. The paper describes the system architecture in detail, focussing on original solutions conceived to endow the system with flexibility and performance. As proof of GIOTTO's suitability for robotic application, its use in a robot vision experiment is presented, showing the approach to the vision problem, the parallel algorithms, and performance analysis.	image processing;parallel computing	Rita Cucchiara;Luigi di Stefano;Massimo Piccardi;Tullio Salmon Cinotti	1997	Real-Time Imaging	10.1006/rtim.1996.0069	computer vision;real-time computing;simulation;simd;machine vision;image processing;computer science;parallel algorithm;algorithm;computer graphics (images)	Graphics	10.764581134405685	35.97812323813403	46339
310794208dbbeda2900096609d408540b16792fe	a distributed algorithm for the detection of local cycles and knots	distributed algorithms;graph theory;diffusing computation;detection algorithms;deadlock detection;speed up;computational method;2m messages;system recovery;blocking paradigm;deadlock breaking distributed algorithm cycles knots 2m messages speed up diffusing computation deadlock detection parallel simulation blocking paradigm;graph theory distributed algorithms concurrency control;cycles;distributed algorithms system recovery clustering algorithms computer science detection algorithms discrete event simulation database systems;database systems;detection algorithm;concurrency control;communication cost;clustering algorithms;computer science;knots;distributed algorithm;deadlock breaking;parallel simulation;discrete event simulation	The purpose of this paper is to present an efficient distributed cycle/knot detection, algorithm for general graphs which will determine whether a given node is a member of a knot or a cycle. This is relevant to an application such as parallel simulation in which (1) cycles and knots can arise frequently, (2) the size of the graph is very large and (3) it is necessary to know if a given node is in a cycle or a knot. The algorithm is based on a diffusing computation. It requires less communication cost than preceding algorithms and is the first algorithm capable of detecting both cycles and knots. The algorithm differs from the classical diffusing computation methods through its use of incomplete search messages to speed up the computation. The algorithm requires a total of at most 2m messages, where m is the number of links. This is compared to Chandy-Misra's algorithm (1982) which requires at least (3m+n), where n is a number of nodes and m is the number of links. The algorithm. Requires O(log(n)) bits of memory. Various applications for the cycle/knot detection algorithm are presented. In particular, we demonstrate its importance to deadlock detection to algorithms for parallel simulation which employ a blocking paradigm and a deadlock breaking technique known as TNE/DLTNE. >	distributed algorithm	Azzedine Boukerche;Carl Tropper	1995		10.1109/IPPS.1995.395923	computer science;theoretical computer science;distributed computing;algorithm	Vision	1.2845806470276713	37.62131573325613	46365
183112d9c4dead27b0f8e9d3672d8f6c25b7ae9b	a novel watermarking technique for lut based fpga designs	filigranage;watermarking;field programmable gate array;look up table;diseno circuito;logic design;filigrana;direct manipulation;cryptanalyse;circuit design;tabla dato;logic element;red puerta programable;reseau porte programmable;cryptanalysis;criptoanalisis;conception logique;table donnee;information embedding;conception circuit;table lut;data table;concepcion logica	Although methods for watermarking Field Programmable Gate Arrays (FPGA) have been proposed, they require a high-level design approach whereby additional circuitry, or information embedded in unused logic elements indicate design ownership. The method proposed in this paper is unique in that: it is applied directly to the bit-stream used to configure Look Up Table (LUT) -based FPGAs; has no effect on the operation of the device; can be applied retrospectively to existing designs; attacks require both detailed knowledge of device architecture and direct manipulation of the design at a bitstream level.	field-programmable gate array	Dylan Carline;Paul Coulton	2002		10.1007/3-540-46117-5_129	embedded system;cryptanalysis;logic synthesis;lookup table;digital watermarking;computer science;theoretical computer science;circuit design;table;algorithm;field-programmable gate array	EDA	12.426177956749898	49.245123803983724	46378
4aee60d6346662aa2a0469a56e23aefb2855be24	variability inspired implementation selection problem	circuit optimisation;dynamic programming;electrical faults;network analysis;probability;dynamic-programming;effective pruning criteria;fabrication variability;implementation selection problem;leakage optimization;node costs;node delays;probabilistic approach;probability density functions;probability distributions	Given a directed acyclic graph and different possible implementations for each node, the implementation selection problem (ISP) selects the appropriate implementation for each node such that a given global design objective is optimized, ISP is a generic formulation that is explicitly or implicitly solved in several design automation problems like leakage optimization using dual V/sub th/, gate sizing, etc. An implementation of a node results in an associated delay and perhaps cost for the node. In the presence of different sources of uncertainty and fabrication variability, fixed estimates of delays and costs of a node are extremely erroneous. We investigate a probabilistic approach to solve ISP by considering probability density functions for delays and costs of a node. We propose a dynamic-programming based approach in a probabilistic sense and introduce effective pruning criteria when dealing with probability distributions for identifying co-optimal solution at each stage. A case study of leakage optimization using dual V/sub th/ is presented where we show the effectiveness of a probabilistic approach considering V/sub th/ variability over a traditional deterministic one.	directed acyclic graph;dynamic programming;heart rate variability;mathematical optimization;optimization problem;selection algorithm;semiconductor device fabrication;spatial variability;spectral leakage	Azadeh Davoodi;Vishal Khandelwal;Ankur Srivastava	2004	IEEE/ACM International Conference on Computer Aided Design, 2004. ICCAD-2004.	10.1145/1112239.1112308	probability distribution;mathematical optimization;probability density function;electronic design automation;network analysis;computer science;theoretical computer science;machine learning;dynamic programming;probability;mathematics;directed acyclic graph;statistics	EDA	23.34982140987634	57.563696581327356	46381
eaa2bfaeece658ed141329b66d405db142143ec1	fin prin: analysis and optimization of finfet logic circuits under pvt variations	timing circuits;logic gates finfets delay integrated circuit modeling computational modeling optimization;timing circuits cmos logic circuits delay circuits logic gates monte carlo methods mosfet scaling circuits statistical analysis;pvt variations;finfets;ssta finfet leakage power pvt variations;computational modeling;statistical analysis;leakage power;logic gates;cmos logic circuits;ssta;delay circuits;scaling circuits;integrated circuit modeling;mosfet;optimization;size 22 nm optimization finfet logic circuit pvt variation bulk scaling cmos technology process voltage temperature variation statistical static timing analysis ssta gate output slope gate delay logic gate quasimonte carlo simulation qmc statistical leakage dynamic power analysis;monte carlo methods;finfet	Continued scaling of bulk CMOS technology is facing formidable challenges. As an alternative, FinFETs offer a promising solution for the 22nm technology node and beyond though they still suffer from process, voltage, and temperature (PVT) variations. Thus, in order to analyze the delay of FinFET logic circuits, statistical static timing analysis (SSTA) is more suitable than traditional static timing analysis (STA). In this paper, we consider voltage and temperature variations in addition to process variations. We propose a simplified FinFET timing model with an average absolute error of 3.4% and 4.4%, respectively, for gate output slope and gate delay over all logic gates and sizes, compared to accurate quasi Monte-Carlo (QMC) simulations. We extend an existing SSTA algorithm to statistical leakage and dynamic power analysis as well, and evaluate its performance relative to Monte-Carlo (MC) simulation. Finally, we show that FinFET logic circuits need to be carefully optimized with temperature taken into consideration, since the ratio between the leakage and dynamic power of a circuit can vary drastically depending on the operating temperature assumed.	and gate;algorithm;approximation error;cmos;image scaling;logic gate;monte carlo method;propagation delay;quantum monte carlo;quasi-monte carlo method;semiconductor device fabrication;simulation;spectral leakage;statistical machine translation;statistical static timing analysis	Yang Yang;Niraj K. Jha	2013	2013 26th International Conference on VLSI Design and 2013 12th International Conference on Embedded Systems	10.1109/VLSID.2013.213	electronic engineering;real-time computing;delay calculation;logic gate;computer science;electrical engineering;computational model;statistics;monte carlo method	EDA	22.121425926823285	58.46212218170073	46384
78aa52bc25d32725238d3c25a6d5adab94f7c2ec	multiprocessor-based placement by simulated annealing	exhibit different speedup;multiprocessor environment;simulated annealing change;multiprocessor-based placement;physical design application;standard cell placement;different temperature range;parallel placement;multiprocessor partitioning strategy;simulated annealing method;annealing algorithm;random testing;hardware;temperature control;physical design;algorithm design and analysis;computational modeling;application software;hill climbing;simulated annealing	Simulated annealing methods have proven to be particularly successful in physical design applications, but often require burdensome, long run times. This paper studies the design and analysis of standard cell placement by annealing in a multiprocessor environment. Annealing is not static: we observe that the temperature parameter which controls hill-climbing in simulated annealing changes the behavior of an annealing algorithm as it runs, and strongly influences the choice of multiprocessor partitioning strategy. We introduce the idea of adaptive strategies that exhibit different speedups across different temperature ranges. Measured performance of parallel placement algorithms running on a multiprocessor demonstrate practical speedups consistent with our predictions.	computer-aided design;electrical engineering;experiment;gate array;graceful exit;hardware acceleration;hill climbing;ieee journal of solid-state circuits;ieee software;mach;mathematical optimization;multi language virtual machine;multiprocessing;operating system;parallel algorithm;parallel computing;parallel processing (dsp implementation);physical design (electronics);routing;shared memory;simulated annealing;standard cell;timberwolf web browser	Saul A. Kravitz;Rob A. Rutenbar	1986	23rd ACM/IEEE Design Automation Conference	10.1145/318013.318104	random testing;physical design;algorithm design;mathematical optimization;application software;real-time computing;simulation;simulated annealing;computer science;theoretical computer science;hill climbing;temperature control;computational model;adaptive simulated annealing	EDA	14.121401377322801	50.75454090755655	46399
7863c221e9e47c53da7e8383e545499559e0e41e	an effecient level-shifter floorplanning method for multi-voltage design	chirp;voltage island;optimal method;two stage optimization method level shifter floorplanning method multivoltage design low power design;circuit layout;simulated annealing;its sequences;application specific integrated circuits;application specific integrated circuit;low power electronics;multi voltage design;level shifter;optimization;low power design;optimization chirp application specific integrated circuits;voltage island multi voltage design level shifter;simulated annealing circuit layout low power electronics	Nowdays, Low-power design, especially Multi-voltage design becomes a popular and efficient way to reduce both dynamic power and static power. In this paper, we propose an efficient method of level-shifter floorplanning for a given multi-voltage design. This method is a two stage optimization method. First, for a given voltage island and its sequence pair representation, we greedily pre-place level-shifters into white-spaces of multi-voltage island based sequence-pair representation. Then, we employ a modified IARFP [1] algorithm to re-optimize the positions of level-shifters. Experimental results show that, the proposed two stage level-shifter floorplanner is efficient for post multi-voltage island optimization.	comparator;floorplan (microelectronics);greedy algorithm;logic level;mathematical optimization;white spaces (radio)	Xiaolin Zhang;Zhi Lin;Song Chen;Takeshi Yoshimura	2011	2011 9th IEEE International Conference on ASIC	10.1109/ASICON.2011.6157211	electronic engineering;real-time computing;engineering;electrical engineering	EDA	16.188092187568223	54.579806091685896	46506
2492f3b208885b9523ad9dcae88855be084b2d54	cellular automata to more efficiently compute the collatz map		The Collatz, or 3x+ 1, Conjecture claims that for every positive integer n, there exists some k such that T (n) = 1, where T is the Collatz map. We present three cellular automata (CA) that transform the global problem of mimicking the Collatz map in bases 2, 3, and 4 into a local one of transforming the digits of iterates. The CAs streamline computation first by bypassing calculation of certain parts of trajectories: the binary CA bypasses division by two altogether. In addition, they allow for multiple trajectories to be calculated simultaneously, representing both a significant improvement upon existing sequential methods of computing the Collatz map and a demonstration of the efficacy of using a massively parallel approach with cellular automata to tackle iterative problems like the Collatz Conjecture.	adobe streamline;automata theory;cellular automaton;computation;division by two;heuristic;iteration;iterative method;java;parallel computing;sorting;workaround	Sitan Chen	2014	IJUC		massively parallel;mathematical analysis;discrete mathematics;conjecture;iterated function;cellular automaton;binary number;collatz conjecture;division by two;mathematics;integer	Robotics	7.934983111496295	42.05016461054571	46551
7ca9c80e85861636dd93b9c58aeaf07d87ced2cf	super connectivity of kronecker products of graphs	tolerancia falta;graphe biparti;calcul tolerant les pannes;procesamiento informacion;fault tolerant;algorithm analysis;grafo bipartido;vertex;connected graph;68m15;cartesian product;graph connectivity;fault tolerant computing;producto grafo;informatique theorique;fault tolerance;ensemble contour;super connectivity;information processing;conectividad grafo;edge set;analyse algorithme;vertice;traitement information;graphe produit;bipartite graph;connectivite graphe;article;graph product;graphe connexe;tolerance faute;analisis algoritmo;produit graphe;kronecker product;computer theory;grafo conexo;informatica teorica	"""Let G""""1 and G""""2 be two connected graphs. The Kronecker product G""""1xG""""2 has vertex set V(G""""1xG""""2)=V(G""""1)xV(G""""2) and the edge set E(G""""1xG""""2)={(u""""1,v""""1)(u""""2,v""""2):u""""1u""""2@?E(G""""1),v""""1v""""2@?E(G""""2)}. In this paper, we show that if G is a bipartite graph with @k(G)=@d(G), then GxK""""n(n>=3) is super-@k."""		Litao Guo;Chengfu Qin;Xiaofeng Guo	2010	Inf. Process. Lett.	10.1016/j.ipl.2010.05.013	fault tolerance;combinatorics;discrete mathematics;information processing;connectivity;mathematics;algorithm	DB	24.111674575720663	34.19561037864541	46577
f4983b85ff6b1f5eaaf0faea15fb662fb7a5b754	nonvolatile logic-in-memory lsi using cycle-based power gating and its application to motion-vector prediction	nonvolatile memory magnetic tunneling large scale integration integrated circuit modeling computer architecture microprocessors educational institutions;microprocessors;computer architecture;large scale integration;size 300 mm nonvolatile logic in memory lsi cycle based power gating motion vector prediction magnetic tunnel junction hardware accelerator lsi mtj mos process wafer fabrication line parallel motion vector prediction wasted power dissipation automated design environment logic circuit peripheral assistant tools fine temporal granularity random logic lsi size 90 nm;nonvolatile memory;integrated circuit modeling;spin transfer torque random access memory stt ram automated design environment magnetic tunnel junction mtj motion vector prediction nonvolatile logic in memory nv lim power gating;magnetic tunneling;random access storage cmos logic circuits large scale integration magnetic tunnelling	A magnetic tunnel junction (MTJ)-based logic-in-memory hardware accelerator LSI with cycle-based power gating is fabricated using a 90 nm MTJ/MOS process on a 300 mm wafer fabrication line for practical-scale, fully parallel motion-vector prediction, without wasted power dissipation. The proposed nonvolatile LSI is designed by establishing an automated design environment with MTJ-based logic-circuit IPs and peripheral assistant tools, as well as a precise MTJ device model produced by the fabricated test chips. Through the measurement results of the fabricated LSI, this study shows both the impact of the power-gating technique in a fine temporal granularity utilizing the non-volatility of the MTJ device and the effectiveness of the established automated design environment for designing random logic LSI using nonvolatile logic-in-memory.	cmos;circuit design;electronic circuit;embedded system;hardware acceleration;hybrid integrated circuit;in-memory database;logic gate;non-volatile memory;nonvolatile bios memory;nv network;overhead (computing);peripheral;power gating;quantum fluctuation;random logic;scalability;spatial variability;spectral leakage;transistor;volatility;wafer fabrication	Masanori Natsui;Daisuke Suzuki;Noboru Sakimura;Ryusuke Nebashi;Yukihide Tsuji;Ayuka Morioka;Tadahiko Sugibayashi;Sadahiko Miura;Hiroaki Honjo;Keizo Kinoshita;Shoji Ikeda;Tetsuo Endoh;Hideo Ohno;Takahiro Hanyu	2015	IEEE Journal of Solid-State Circuits	10.1109/JSSC.2014.2362853	embedded system;electronic engineering;non-volatile memory;computer hardware;computer science;engineering	EDA	19.674136483628264	57.19416516920643	46601
